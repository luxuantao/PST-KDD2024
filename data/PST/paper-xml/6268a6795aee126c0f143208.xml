<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding The Robustness in Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-26">26 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<email>&lt;zhidingy@nvidia.com&gt;.</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><surname>Caltech</surname></persName>
						</author>
						<title level="a" type="main">Understanding The Robustness in Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-26">26 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.12451v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies show that Vision Transformers (ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state-of-the-art 87.1% accuracy and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code will be available at https://github.com/NVlabs/FAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in visual recognition are marked by the rise of Vision Transformers (ViTs) <ref type="bibr" target="#b11">(Dosovitskiy et al., 2020)</ref> as state-of-the-art models. Unlike ConvNets <ref type="bibr" target="#b22">(LeCun et al., 1989;</ref><ref type="bibr" target="#b20">Krizhevsky et al., 2012)</ref> that use a "sliding window" strategy to process visual inputs, the initial ViTs feature a design that mimics the Transformers in natural language processing -An input image is first divided into a sequence of patches (tokens), followed by self-attention (SA) <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>  Input size is set to 448 ? 448 following <ref type="bibr" target="#b6">(Caron et al., 2021)</ref>.</p><p>good performance in many visual recognition tasks.</p><p>Unlike ConvNets, ViTs incorporate the modeling of nonlocal relations using self-attention, giving it an advantage in several ways. An important one is the robustness against various corruptions. Unlike standard recognition tasks on clean images, several works show that ViTs consistently outperform ConvNets by significant margins on corruption robustness <ref type="bibr" target="#b3">(Bai et al., 2021;</ref><ref type="bibr" target="#b44">Xie et al., 2021;</ref><ref type="bibr">Zhu et al., 2021;</ref><ref type="bibr" target="#b28">Paul &amp; Chen, 2022;</ref><ref type="bibr">Naseer et al., 2021)</ref>. The strong robustness in ViTs is partly attributed to their self-attention designs, but this hypothesis is recently challenged by an emerging work ConvNeXt <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, where a network constructed from standard ConvNet modules without self-attention competes favorably against ViTs in generalization and robustness. This raises an interesting question on the actual role of self-attention in robust generalization.</p><p>Our approach: In this paper, we aim to find an answer to the above question. Our journey begins with the intriguing observation that meaningful segmentation of objects naturally emerge in ViTs during image classification <ref type="bibr" target="#b6">(Caron et al., 2021)</ref>. This motivates us to wonder whether selfattention promotes improved mid-level representations (and thus robustness) via visual grouping -a hypothesis that echoes the odyssey of early computer vision (U.C. Berkeley). As a further examination, we analyze the output tokens from each ViT layer using spectral clustering <ref type="bibr" target="#b27">(Ng et al., 2002)</ref>, where the significant 1 eigenvalues of the affinity matrix correspond to the main cluster components. Our study shows an interesting correlation between the number of significant eigenvalues and the perturbation from input corruptions: both of them decrease significantly over midlevel layers, which indicates the symbiosis of grouping and robustness over these layers.</p><p>To understand the underlying reason for the grouping phenomenon, we interpret SA from the perspective of information bottleneck (IB) <ref type="bibr" target="#b36">(Tishby et al., 2000;</ref><ref type="bibr" target="#b35">Tishby &amp; Zaslavsky, 2015)</ref>, a compression process that "squeezes out" unimportant information by minimizing the mutual information between the latent feature representation and the target class labels, while maximizing mutual information between the latent features and the input raw data. We show that under mild assumptions, self-attention can be written as an iterative optimization step of the IB objective. This partly explains the emerging grouping phenomenon since IB is known to promote clustered codes (Cite here).</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref> (a), previous Vision Transformers often adopt a multi-head attention design, followed by an 1 eigenvalues are larger than a predefined threshold .</p><p>MLP block to aggregate the information from multiple separate heads. Since different heads tend to focus on different components of objects, the multi-head attention design essentially forms a mixture of information bottlenecks. As a result, how to aggregate the information from different heads matters. We aim to come up with an aggregation design that strengthens the symbiosis of grouping and robustness. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> (b), we propose a novel attentional channel processing design which promotes channel selection through reweighting. Unlike the static convolution operations in the MLP block, the attentional design is dynamic and content-dependent, leading to more compositional and robust representations. The proposed module results in a new family of Transformer backbone, coined Fully Attentional Networks (FANs) after their designs.</p><p>Our contributions can be summarized as follows:</p><p>? Instead of focusing on empirical studies, this work provides an explanatory framework that unifies the trinity of grouping, information bottleneck and robust generalization in Vision Transfomrers. ? We also conduct extensive experiments in semantic segmentation and object detection. We show that the significant gain in robustness from our proposed design is transferrable to these downstream tasks.</p><p>Our study indicates the non-trivial benefit of attention representations in robust generalization, and is in line with the recent line of research observing the intriguing robustness in ViTs. We hope our observations and discussions can lead to a better understanding of the representation learning in ViTs and encourage the community to go beyond standard recognition tasks on clean images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fully Attentional Networks</head><p>In this section, we examine some emerging properties in ViTs and interpret these properties from an information bottleneck perspective. We then present the proposed Fully Attentional Networks (FANs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries on Vision Transformers</head><p>A standard ViT first divides an input image into n patches uniformly and encodes each patch into a token embedding x i ? R d , i = 1, . . . , n. Then, all these tokens are fed into a stack of transformer blocks. Each transformer block leverages self-attention for token mixing and MLPs for channelwise feature transformation. The architecture of a transformer block is illustrated in the left of Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Token mixing. Vision transformers leverage self-attention to aggregate global information. Suppose the input token embedding tensor is</p><formula xml:id="formula_0">X = [x 1 , . . . , x n ] ? R d?n , SA ap- plies linear transformation with parameters W K , W Q , W V to embed them into the key K = W K X ? R d?n , query Q = W Q X ? R d?n and value V = W V X ? R d?n respec- tively.</formula><p>The SA module then computes the attention matrix and aggregates the token features as follows:</p><formula xml:id="formula_1">Z = SA(X) = Softmax Q K ? d V WL,<label>(1)</label></formula><p>where W L ? R d?d is a linear transformation and Z = [z 1 , . . . , z n ] is the aggregated token features and ? d is a scaling factor. The output of the SA is then normalized and fed into the MLP to generate the input to the next block.</p><p>Channel processing. Most ViTs adopt an MLP block to transform the input tokens into features Z:</p><formula xml:id="formula_2">Z = MLP(Z).</formula><p>(2)</p><p>The block contains two Linear layers and a GELU layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intriguing Properties of Self-Attention</head><p>We begin with the observation that meaningful clusters emerge on ViT's token features z. We examine such phenomenon using spectral clustering <ref type="bibr" target="#b27">(Ng et al., 2002)</ref>, where the token affinity matrix is defined as S ij = z i z j . Since the number of major clusters can be estimated by the multiplicity of significant eigenvalues <ref type="bibr" target="#b48">(Zelnik-Manor &amp; Perona, 2004)</ref> of S, we plot the number of (in)significant eigenvalues across different ViT-S blocks (Figure <ref type="figure">3 (a)</ref>). We observe that by feeding Gaussian noise x ? N (0, 1), the resulting perturbation (measured the by normalized feature norm) decreases rapidly together with the number of significant eigenvalues. Such observation indicates the symbiosis of grouping and improved robustness over middle blocks.</p><p>We additionally visualize the same plot for FAN-S-ViT in Figure <ref type="figure">3</ref> (b) where similar trend holds even more obviously. The noise decay of ViT and FAN is further compared to ResNet-50 in Figure <ref type="figure">3</ref> (c). We observe that: 1) the robustness of ResNet-50 tends to improve upon downsampling but plateaus over regular convolution blocks.</p><p>2) The final noise decay of ResNet-50 less significant. Finally, we visualize the grouped tokens obtained at different blocks in Figure <ref type="figure">4</ref>, which demonstrates the process of visual grouping by gradually squeezing out unimportant components. Additional visualizations on different features (tokens) from different backbones are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">An Information Bottleneck Perspective</head><p>The emergence of clusters and its symbiosis with robustness in Vision Transformers draw our attention to early pioneer works in visual grouping (U.C. <ref type="bibr" target="#b39">Berkeley;</ref><ref type="bibr" target="#b4">Buhmann et al., 1999)</ref>. In some sense, visual grouping can also be regarded as some form of lossy compression <ref type="bibr" target="#b45">(Yang et al., 2008)</ref>. We thus present the following explanatory framework from an information bottleneck perspective.</p><p>Given a distribution X ? N (X , ) with X being the observed noisy input and X the target clean code, IB seeks a mapping f (Z|X) such that Z contains the relevant information in X for predicting X . This goal is formulated as the following information-theoretic optimization problem:</p><formula xml:id="formula_3">f * IB (Z|X) = arg min f (Z|X) I(X, Z) -I(Z, X ),<label>(3)</label></formula><p>Here the first term compresses the information and the second term encourages to maintain the relevant information.</p><p>In the case of an SA block, Z = [z 1 , . . . , z n ] ? R d?n denote the output features and X = [x 1 , . . . , x n ] ? R d?n the input. Assuming i is the data point index, we have:</p><p>Proposition 2.1. Under mild assumptions, the iterative step to optimize the objective in Eqn.</p><p>(3) can be written as:</p><formula xml:id="formula_4">zc = n i=1 log[nc/n] n det ? exp ? c ? -1 x i 1/2 n c=1 exp ? c ? -1 x i 1/2 xi,<label>(4)</label></formula><p>or in matrix form:</p><formula xml:id="formula_5">Z = Softmax(Q K/d)V ,<label>(5)</label></formula><formula xml:id="formula_6">with V = [x 1 , . . . , x N ] log[nc/n] n det ? , K = [? 1 , . . . , ? N ] = W K X, Q = ? -1 [x 1 , . . . , x N ] and d = 1/2.</formula><p>Here n c , ? and W K are learnable variables.</p><p>Remark. We defer the proof to the appendix. The above proposition establishes an interesting connection between the vanilla self-attention (1) and IB (3), by showing that SA aggregates similar inputs x i into representations Z with cluster structures. Self-attention updates the token features following an IB principle, where the key matrix K stores the temporary cluster center features ? c and the input features x are clustered to them via soft association (softmax). The new cluster center features z are output as the updated token features. The stacked SA modules in ViTs can be broadly regarded as an iterative repeat of this optimization which promotes grouping and noise filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Self-attention (MHSA). Many current Vision</head><p>Transformer architectures adopt an MHSA design where each head tends to focus on different object components. In some sense, MHSA can be interpreted as a mixture of information bottlenecks. We are interested in the relation between the number of heads versus the robustness under a fixed total number of channels. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, having more heads leads to improved expressivity and robustness. But the reduced channel number per head also causes decreased clean accuracy. The best trade-off is achieved with 32 channels per head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fully Attentional Networks</head><p>With the above mixture of IBs interpretation, we intend to design a channel processing module that strengthens robust representation through the aggregation across different heads. Our design is driven by two main aspects: 1) To pro- mote more compositional representation, it is desirable to introduce channel reweighting since some heads or channels do capture more significant information than the others.</p><p>2)</p><p>The reweighting mechanism should involve more spatially holistic consideration of each channel to leverage the promoted grouping information, instead of making "very local" channel aggregation decisions.</p><p>A starting point towards the above goals is to introduce a channel self-attention design similar to XCiT (El-Nouby et al., 2021). As shown in Figure <ref type="figure" target="#fig_4">6</ref> (a), the channel attention (CA) module adopts a self-attention design which moves the MLP block into the self-attention block, followed by matrix multiplication with the D ?D channel attention matrix from the channel attention branch.</p><p>Attentional feature transformation. A FAN block introduces the following channel attention (CA) to perform feature transformation which is formulated as:</p><formula xml:id="formula_7">CA(Z) = Softmax (W Q Z)(W K Z) ? n MLP(Z),<label>(6)</label></formula><p>Here W Q ? R d?d and W K ? R d?d are linear transformation parameters. Different from SA, CA computes the attention matrix along the channel dimension instead of the token dimension (recall Z ? R d?n ), which leverages the feature covariance (after linear transformation W Q , W K ) for feature transformation. Strongly correlated feature channels with larger correlation values will be aggregated while outlier features with low correlation values will be isolated. This aids the model in filtering out irrelevant information.</p><p>With the help of CA, the model can filter irrelevant features and thus form more precise token clustering for the foreground and background tokens. We will give a more formal description on such effects in the following section.</p><p>We will verify the improved robustness from CA over existing ViT models in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Efficient Channel Self-attention</head><p>There are two limits of applying the conventional selfattention calculation mechanism along the channel dimension. The first one is the computational overhead. The computational complexity of CA introduced in Eqn 6 is quadratically proportional to D 2 , where D is the channel dimension. For modern pyramid model designs <ref type="bibr" target="#b41">(Wang et al., 2021;</ref><ref type="bibr" target="#b23">Liu et al., 2021)</ref>, the channel dimension becomes larger and larger at the top stages. Consequently, direct applying CA can cause a large computational overhead. The second one is the low parameter efficiency. In conventional SA module, the attention distribution of the attention weights is sharpened via a Softmax operation. Consequently, only a partial of the channels could contribute to the representation learning as most of the channels are diminished by being multiplied with a small attention weights. To overcome these, we explore a novel self-attention like mechanism that is equipped with both the high computational efficiency and parameter efficiency. Specifically, two major modifications are proposed. First, instead of calculating the co-relation matrix between the tokens features, we first generate a token prototype, Z, Z ? R n?1 , by averaging over the channel dimension. Intuitively, Z aggregates all the channel information for each spatial positions represented by tokens. Thus, it is informative to calculate the co-relation matrix between the token features and token prototype Z, resulting in learn complexity with respect to the channel dimension. Secondly, instead of applying a Softmax function, we use a Sigmoid function for normalizing the attention weights and then multiply it with the token features instead of using MatMul to aggregate channel information. Intuitively, we do not force the channel to select only a few of the "important" token features but re-weighting each channel based on the spatial co-relation. Indeed, the channel features are typically considered as independent. A channel with large value should not restrain the importance of other channels. By incorporating those two design concepts, we propose a novel channel self-attention and it is calculated via Eqn. ( <ref type="formula" target="#formula_8">7</ref>):</p><formula xml:id="formula_8">ECA(Z) = Norm (W Q ?(Z)) ?(Z) ? n MLP(Z),<label>(7)</label></formula><p>Here, ? denotes the Softmax operation along the token dimension and Z denotes the token prototype (Z ? R 1?N ).We use sigmoid as the Norm. The detailed block architecture design is also shown in Figure <ref type="figure" target="#fig_4">6</ref>. We verify that the novel efficient channel self-attention takes consumes less computational cost while improve the performance significantly.</p><p>The detailed results will be shown in Sec. 3.2. In the experiments, we evaluate the performance with both the clean accuracy on ImageNet-1K (IN-1K) and the robustness accuracy on these out-of-distribution benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Results &amp; Analysis</head><p>To quantify the resilience of a model against corruptions, we propose to calibrate with the clean accuracy. We use retention rate (Ret R) as the robustness metric, defined as R = Robust Acc. Clean Acc. = IN-C IN-1K . We also report the mean corruption error (mCE) following <ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref>. For more details, please refer to Appendix A.2. For Cityscapes, we take the average mIoU for three severity levels for the noise category, following the practice in SegFormer <ref type="bibr" target="#b44">(Xie et al., 2021)</ref>. For all the rest of the datasets, we take the average of all five severity levels.</p><p>Model selection. We design four different model sizes (Tiny, Small, Base and large) for our FAN models, abbreviated as '-T', '-S', '-B' and '-L' respectively. Their detailed configurations are shown in Table <ref type="table" target="#tab_1">1</ref>. For ablation study, we use ResNet-50 as a representative model for CNNs and ViT-S as a representative model for the conventional vision transformers. ResNet-50 and ViT-S have similar model sizes and computation budget as FAN-S. When comparing with SOTA models, we take the most recent vision transformer and CNN models as baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis</head><p>In this section, we present a series of ablation studies to analyze the contribution of self-attention in model robustness.</p><p>Since multiple advanced training recipes have been recently introduced, we first investigate their effects in improving model robustness. We then compare ViTs and CNNs with exactly the same training recipes to exclude factors other than architecture design that might affect model robustness.</p><p>Effects of advanced training tricks. We empirically evaluate how different training recipes could be used to improve the robustness, with the results reported in Table <ref type="table" target="#tab_2">2</ref>. Interestingly, it is observed that widely used tricks such as knowledge distillation (KD) and large dataset pretraining do improve the absolute accuracy. However, they do not significantly reduce the performance degradation when transferred to ImageNet-C. The main improvement comes from the advanced training recipe such as the CutMix and RandAugmentation adopted in DeiT training recipe. In the following comparison, we use the ViT-S trained with DeiT recipe and increased block number with reduced channel dimension, denoted as ViT-S * . In addition, to make fair comparison, we first apply those advanced training techniques to reproduce the ResNet-50 performance. Adding new training recipes to CNNs. We make a step by step empirical study on how the robustness of ResNet-50 model changes when adding advanced tricks. We examine three design choices: training recipe, attention mechanism and down-sampling methods. For the training recipe, we adopt the same one as used in training the above ViT-S model. We use Squeeze-and-Excite (SE) attention <ref type="bibr" target="#b18">(Hu et al., 2018)</ref> and apply it along the channel dimension for the feature output of each block. We also investigate different downsampling strategies, i.e., average pooling (ResNet-50 default) and strided convolution. The results are reported in Table <ref type="table" target="#tab_3">3</ref>. As can be seen, adding attention (Squeeze-and-Excite (SE) attention) and using more advanced training recipe do improve the robustness of ResNet-50 significantly.</p><p>We take the best-performing ResNet-50 with all these tricks, denoted as ResNet-50 * , for the following comparison.     FAN-Hybrid. From the clustering process as presented in Figure <ref type="figure">3</ref>, we find that the clustering mainly emerges at the top stages of the FAN model, implying the bottom stages to focus on extracting local visual patterns. Motivated by this, we propose to use convolution blocks for the bottom two stages with down-sampling and then append FAN blocks to the output of the convolutional stages. Each stage includes 3 convolutional blocks. This gives the FAN-Hybrid model.</p><p>In particular, we use the ConvNeXt <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, a very recent CNN model, to build the early stages of our hybrid model. As shown in Table <ref type="table" target="#tab_7">7</ref>, we find original ConvNeXt exhibits strong robustness than SWIN transformer, but performs less robust than FAN-ViT and FAN-Swin models. However, the FAN-Hybrid achieves comparable robustness as FAN-ViT and FAN-SWIN and presents higher accuracy for both clean and corrupted datasets, implying FAN can also effectively strengthen the robustness of a CNN-based model. Similar to FAN-SWIN, FAN-Hybrid enjoys efficiency for processing large-resolution inputs and dense prediction tasks, making it favorable for downstream tasks. Thus, for all downstream tasks, we use FAN-Hybrid model to compare with other state-of-the-art models. More details on the FAN-Hybrid and FAN-SWIN architecture can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to SOTAs on various tasks</head><p>In this subsection, we evaluate the robustness of FAN with other SOTA methods against common corruptions on different downstream tasks, including image classification (ImageNet-C), semantic segmentation (Cityscapes-C) and object detection (COCO-C). Additionally, we evaluate the robustness of FAN on various other robustness benchmarks including ImageNet-A and ImageNet-R to further show its non-trivial improvements in robustness.</p><p>Robustness in image classification. We first compare the robustness of FAN with other SOTA models by directly applying them (pre-trained on ImageNet-1K) to the ImageNet-C dataset <ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref> without any finetuning. We divide all the models into three groups according to their model size for fair comparison. The results are shown in Table <ref type="table" target="#tab_8">8</ref> and the detailed results are summarized in Table <ref type="table" target="#tab_2">12</ref>. From the results, one can clearly observe that all the transformer-based models show stronger robustness than CNN-based models. Under all the models sizes, our proposed FAN models surpass all other models significantly.</p><p>They offer strong robustness to all the types of corruptions. Notably, FANs perform excellently robust for bad weather conditions and digital noises, making them very suitable for vision applications in mobile phones and self-driving cars. We also evaluate the zero-shot robustness of the Swin transformer and the recent ConvNeXt. Both of them demonstrate weaker robustness than the transformers with global selfattention. However, adding FAN to them improves their robustness, enabling the resulted FAN-SWIN and FAN-Hybrid variants to inherit both high applicability for downstream tasks and strong robustness to corruptions. We will use FAN-Hybrid variants in the applications of segmentation and detection.</p><p>Robustness in semantic segmentation.  Robustness against out-of-distribution. The FAN encourages token features to form clusters and implicitly selects the informative features, which would benefit generalization performance of the model. To verify this, we directly test our ImageNet-1K trained models for evaluating their robustness, in particular for out-of-distribution samples, on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head><p>Vision Transformers <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> are a family of transformer-based architectures on computer vision tasks. Unlike CNNs relying on certain inductive biases (e.g., locality and translation invariance), ViTs perform the global interactions among visual tokens via self-attention, thus having less inductive bias about the input image data. Such designs have offered significant performance improvement on various vision tasks including image classification <ref type="bibr" target="#b11">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b47">Yuan et al., 2021;</ref><ref type="bibr">Zhou et al., 2021a;</ref><ref type="bibr">b)</ref>, object detection <ref type="bibr" target="#b5">(Carion et al., 2020;</ref><ref type="bibr" target="#b55">Zhu et al., 2020;</ref><ref type="bibr">Dai</ref>   <ref type="bibr" target="#b42">(Wang et al., 2020;</ref><ref type="bibr" target="#b23">Liu et al., 2021;</ref><ref type="bibr" target="#b51">Zheng et al., 2020)</ref>. The success of vision transformers for vision tasks triggers broad debates and studies on the advantages of self-attention versus convolutions <ref type="bibr" target="#b29">(Raghu et al., 2021;</ref><ref type="bibr" target="#b34">Tang et al., 2021)</ref>. Compared to convolutions, an important advantage is the robustness against observable corruptions. Several works <ref type="bibr" target="#b3">(Bai et al., 2021;</ref><ref type="bibr" target="#b44">Xie et al., 2021;</ref><ref type="bibr">Zhu et al., 2021;</ref><ref type="bibr" target="#b28">Paul &amp; Chen, 2022;</ref><ref type="bibr">Naseer et al., 2021)</ref> have empirically shown that the robustness of ViTs against corruption consistently outperforms ConvNets by significant margins. However, how the key component (i.e. self-attention) contributes to the robustness is under-explored. In contrast, our work conducts empirical studies to reveal intriguing properties (i.e., token grouping and noise absorbing) of self-attention for robustness and presents a novel fully attentional architecture design to further improve the robustness.</p><p>There exists a large body of work on improving robustness of deep learning models in the context of adversarial examples by developing robust training algorithms <ref type="bibr" target="#b21">(Kurakin et al., 2016;</ref><ref type="bibr" target="#b32">Shao et al., 2021)</ref>, which differs from the scope of our work. In this work, we focus the zero-shot robustness to the natural corruptions and mainly study improving model's robustness from the model architecture perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we verified self-attention as a contributor of the improved robustness in vision transformers. Our study shows that self-attention promotes naturally formed clusters in tokens, which exhibits interesting relation to the extensive early studies in vision grouping prior to deep learning. We also established an explanatory framework from the perspective of information bottleneck to explain properties of self-attention. To push the boundary of robust representation learning with self-attention, we introduced a family of fully-attentional network (FAN) architectures, where self-attention is leveraged in both token mixing and channel processing. FAN models demonstrate significantly improved robustness over their CNN and ViT counterparts.</p><p>Our work provides a new angle towards understanding the working mechanism of vision transformers, showing the potential of inductive biases going beyond convolutions. Our work can benefit wide real-world applications, especially safety-critical ones such as autonomous driving.</p><p>Table <ref type="table" target="#tab_9">13</ref>. Comparison of Model Robustness on Cityscapes-C (%). FAN shows stronger robustness than both CNN and transformer models, for all the image corruption settings. "DLv3+" refer to DeepLabv3+ <ref type="bibr" target="#b8">(Chen et al., 2018)</ref>. The mIoUs of compared CNN models are adopted from <ref type="bibr" target="#b19">(Kamann &amp; Rother, 2020)</ref>. The mIoU of ConvNeXt, DeiT, Swin and SegFormer models are our reproduced results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Main results on ImageNet-C (top figure) and clustering visualization (bottom row). Retention rate is defined as robust accuracy / clean accuracy. Left to right in bottom row: input image contaminated by corruption (snow) and the visualized clusters. Visualization is conducted on the output features (tokens) of the second last layers. All models are pretrained on ImageNet-1K.Input size is set to 448 ? 448 following<ref type="bibr" target="#b6">(Caron et al., 2021)</ref>.</figDesc><graphic url="image-2.png" coords="1,386.06,355.31,76.75,58.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between conventional ViT block and the proposed FAN block. (a) ViT block: Input tokens are first aggregated by self-attention, followed by a linear projection and an MLP is appended to the self attention block for feature transformation. (b) FAN block: both token self-attention and channel attention are applied, which makes the entire network fully attentional. The linear projection layer after the channel attention is removed.</figDesc><graphic url="image-4.png" coords="2,179.70,120.24,111.55,62.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 4. Clustering visualization for different blocks. The visualization is based on our proposed FAN-S model as detailed in Table1. The cluster visualizations are generated by applying spectral clustering on token features from each FAN block.</figDesc><graphic url="image-6.png" coords="4,57.80,318.11,73.76,55.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Impacts of head number on model robustness.</figDesc><graphic url="image-9.png" coords="4,216.31,251.71,73.90,55.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison among channel attention designs. (a) CA: a channel self attention design similar to XCiT (El-Nouby et al., 2021), but differently applied on the output of the MLP block. (b) The proposed efficient channel attention (ECA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>metrics. We verify the model robustness on Imagenet-C (IN-C), Cityscape-C and COCO-C without extra corruption related fine-tuning. The suffix '-C' denotes the corrupted images based on the original dataset with the same manner proposed in<ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref>. To test the generalization to other types of out-of-distribution (OOD) scenarios, we also evaluate the accuracy on ImageNet-A<ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref> (IN-A) and ImageNet-R (IN-R)<ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Segmentation visualization on corrupted images with impulse noise (severity 3) and snow (severity 3). We select the recent state-of-the-art Segformer model (Xie et al., 2021) as a strong baseline. FAN-S-H denotes our hybrid model. Under comparable model size and computation, FAN achieve significantly improved segmentation results over ResNet-50 and SegFormer-B2 model. A video demo is available via external players and in Figure 8 in the appendix.</figDesc><graphic url="image-13.png" coords="9,-55.79,70.95,227.45,113.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-18.png" coords="16,55.68,448.65,485.54,242.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Details and abbreviations of different FAN variants.</figDesc><table><row><cell cols="5">Model #Blocks Channel Dim. #Heads Param. FLOPs</cell></row><row><cell>FAN-T</cell><cell>12</cell><cell>192</cell><cell>4</cell><cell>7.3M 1.4G</cell></row><row><cell>FAN-S</cell><cell>12</cell><cell>384</cell><cell>8</cell><cell>28.3M 5.3G</cell></row><row><cell>FAN-B</cell><cell>18</cell><cell>448</cell><cell>8</cell><cell>54.0M 10.4G</cell></row><row><cell>FAN-L</cell><cell>24</cell><cell>480</cell><cell>10</cell><cell>80.5M 15.8G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Impacts of various performance improvement tricks on model robustness (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ViT-S</cell><cell>77.9 54.2</cell><cell>70</cell><cell>63.5</cell></row><row><cell>+ DeiT Recipe</cell><cell>79.3 57.1</cell><cell>72</cell><cell>57.1</cell></row><row><cell cols="2">+ #Blocks (8 ?12) 79.9 58.0</cell><cell>72</cell><cell>56.2</cell></row><row><cell>+ KD</cell><cell>81.3 59.6</cell><cell>73</cell><cell>54.0</cell></row><row><cell>+ IN22K w/o KD</cell><cell>81.8 59.7</cell><cell>73</cell><cell>54.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Robustness of ResNet-50 with various performance improvement tricks (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ResNet-50</cell><cell>76.0 38.8</cell><cell>51</cell><cell>76.7</cell></row><row><cell cols="2">+ DeiT Recipe 79.0 43.9</cell><cell>46</cell><cell>69.7</cell></row><row><cell>+ SE</cell><cell>79.8 50.1</cell><cell>63</cell><cell>63.1</cell></row><row><cell cols="2">+ Strided Conv 80.2 52.1</cell><cell>65</cell><cell>61.6</cell></row></table><note><p><p><p>Advantages of ViTs over CNNs on robustness. To make fair comparison, we use all the above validated training tricks to train the ViT-S and ResNet-50 to their best performance. Specifically, ResNet-50 * is trained with DeiT recipe, SE and strided convolution; ViT-S * is also trained with DeiT recipe and has 12 blocks with 384 embedding dimension for matching the model size as ResNet-50. Results in Table</p>4</p>show that even with the same training recipe, ViTs still outperform ResNet-50 in robustness. These results indicate that the improved robustness in ViTs may come from their architectural advantages with self-attention. This motivates us to further improve the architecture of ViTs by leveraging self-attention more broadly to further strengthen the model's robustness.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Robustness comparison between ResNet-50 and ViT-S (%).Difference among ViT, SWIN-ViT and ConvNeXt. Very recent CNN models has shown superiority of the robustness over the recent state-of-the-art transformer based models SWIN transformer. We here interpret this from the view of information bottleneck. As explained in Sec. 2.3, the SA module is forming an IB to select essential tokens. As SWIN transformer deploys a window based local self-attention mechanism, it forces the model to select information from a predefined window area. Such a local window IB forces each window to select tokens from a local constrained features. Intuitively, when a selected window contains no essential information, a local SA is forced to select some key tokens and thus resulting a set of sub-optimal clusters. Thus, the robustness of SWIN transformer is worse than the recent SOTA CNN model ConvNeXt. However, as shown in Table5, DeiT achieve better robustness with 24.1% less number of parameters, compared to ConvNeXt model. We thus argue that transformers with global SA module are still more robust than the state-of-the-art ConvNeXt model.</figDesc><table><row><cell>Model</cell><cell cols="4">Param IN-1K IN-C Retention mCE (?)</cell></row><row><cell cols="2">ResNet-50  *  25M</cell><cell>80.2 52.1</cell><cell>65</cell><cell>61.6</cell></row><row><cell>ViT-S  *</cell><cell>22M</cell><cell>79.9 58.0</cell><cell>72</cell><cell>56.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Robustness comparison among Swin, ConvNeXt, DeiT and FAN. The mIoU of ConvNeXt, DeiT, Swin and Seg-Former models are our reproduced results.</figDesc><table><row><cell>Model</cell><cell>Param.</cell><cell cols="3">ImageNet Clean Corrupt Reten. Clean Corrupt Reten. Cityscapes</cell></row><row><cell cols="3">ConvNeXt (Liu et al.) 29M 82.1 59.1</cell><cell>72.0 79.0 54.2</cell><cell>68.6</cell></row><row><cell>SWIN (Liu et al.)</cell><cell cols="2">28M 81.3 55.4</cell><cell>68.1 78.0 47.3</cell><cell>61.7</cell></row><row><cell cols="3">DeiT-S (Touvron et al.) 22M 79.9 58.1</cell><cell>72.7 76.0 55.4</cell><cell>72.9</cell></row><row><cell cols="3">FAN-Hybrid-S (Ours) 26M 83.5 64.7</cell><cell>78.2 81.5 66.4</cell><cell>81.5</cell></row><row><cell cols="3">3.3. Fully Attentional Networks</cell><cell></cell></row><row><cell cols="5">In this subsection, we investigate how the new FAN archi-</cell></row><row><cell cols="5">tecture improves the model's robustness among different</cell></row><row><cell>architectures.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Impacts of efficient channel attention We first ablate the</cell></row><row><cell cols="5">impacts of different forms of channel attentions in terms</cell></row><row><cell cols="5">of GPU memory consumption, clean image accuracy and</cell></row><row><cell cols="5">robustness. The results are shown in Table 6. Compared to</cell></row><row><cell cols="5">the original self-attention module, SE attention consumes</cell></row><row><cell cols="5">less memory and achieve comparable clean image accuracy</cell></row><row><cell cols="5">and model robustness. By taking the spatial relationship into</cell></row><row><cell cols="5">consideration, our proposed CSA produces the best model</cell></row><row><cell cols="5">robustness with comparable memory consumption to the SE</cell></row><row><cell>attention.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Effects of different channel attentions on model robustness (%). This is possibly because their local attention hinders global clustering and the IB-based information extraction, as detailed in Section 3.2. The drop in robustness can be effectively remedied by using the FAN block. By adding the ECA to the feature transformation of SWIN models, we obtain the FAN-SWIN, a new FAN model whose spatial self-attention is augmented by the shifted window attention in SWIN. As shown in Table7, adding FAN block improves the accuracy on ImageNet-C by 5%. Such a significant improvement shows that our proposed CSA does have significant effectiveness on improving the model robustness.</figDesc><table><row><cell>Model</cell><cell cols="4">Mem.(M) IN-1K IN-C Retention mCE (?)</cell></row><row><cell>FAN-ViT-S-SA</cell><cell>235</cell><cell>81.3 61.7</cell><cell>76</cell><cell>51.4</cell></row><row><cell>FAN-ViT-S-SE</cell><cell>126</cell><cell>81.2 62.0</cell><cell>76</cell><cell>50.0</cell></row><row><cell>FAN-ViT-S-ECA</cell><cell>127</cell><cell>82.5 64.6</cell><cell>78</cell><cell>47.7</cell></row><row><cell cols="5">FAN-ViT &amp; FAN-Swin. Using the FAN block to replace</cell></row><row><cell cols="5">the conventional transformer block forms the FAN-ViT.</cell></row><row><cell cols="5">FAN-ViT significantly enhances the robustness. However,</cell></row><row><cell cols="5">compared to ViT, the robustness of Swin architecture (Liu</cell></row><row><cell cols="5">et al., 2021) (which uses shifted window attention) drops.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Effects of architectural changes on model robustness (%).</figDesc><table><row><cell>Model</cell><cell cols="3">IN-1K IN-C Retention mCE (?)</cell></row><row><cell>ViT-S  *</cell><cell>79.9 58.1</cell><cell>73</cell><cell>56.2</cell></row><row><cell>+ FAN</cell><cell>81.3 61.7</cell><cell>76</cell><cell>51.4</cell></row><row><cell>Swin-T</cell><cell>81.4 55.4</cell><cell>68</cell><cell>59.6</cell></row><row><cell>+ FAN</cell><cell>81.9 59.4</cell><cell>73</cell><cell>54.5</cell></row><row><cell cols="2">ConvNeXt-T 82.1 59.1</cell><cell>72</cell><cell>54.8</cell></row><row><cell>+ FAN</cell><cell>82.5 60.8</cell><cell>74</cell><cell>53.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Main results on image classification. FAN models show improved performance in both clean accuracy and robustness than other models. ? denotes models are pretrained on ImageNet-22K.</figDesc><table><row><cell>Model</cell><cell>Param./FLOPs IN-1K IN-C Retention</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 13</head><label>13</label><figDesc>We further eval-uate robustness of our proposed FAN model for the segmentation task. We use the Cityscapes-C for evaluation, which expands the Cityscapes validation set with 16 types of natural corruptions. We compare our model to variants of DeeplabV3+ and latest SOTA models. The results are summarized in Table9and by category results are summarized in</figDesc><table><row><cell>. Our model significantly outperforms previous</cell></row><row><cell>models. FAN-S-Hybrid surpasses the latest SegFormer-a</cell></row><row><cell>transformer based segmentation model-by 6.8% mIoU</cell></row><row><cell>with comparable model size. The results indicate strong</cell></row><row><cell>robustness of FAN.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Main results on semantic segmentation.</figDesc><table><row><cell>'R-' and 'X-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Main results on object detection. FAN shows stronger clean accuracy and robustness than other models. ' ?' denotes the accuracy pretrained on ImageNet-22K.</figDesc><table><row><cell>Model</cell><cell cols="4">Encoder Size COCO COCO-C Retention</cell></row><row><cell>ResNet50 (He et al.)</cell><cell>25.4M</cell><cell>39.9</cell><cell>21.3</cell><cell>53.3%</cell></row><row><cell>ResNet101 (He et al.)</cell><cell>44.1M</cell><cell>41.8</cell><cell>23.3</cell><cell>55.7%</cell></row><row><cell>DeiT-S (Touvron et al.)</cell><cell>22.1M</cell><cell>40.0</cell><cell>26.9</cell><cell>67.3%</cell></row><row><cell>SWIN-T (Liu et al.)</cell><cell>28.0M</cell><cell>46.0</cell><cell>29.3</cell><cell>63.7%</cell></row><row><cell>FAN-T-Hybrid</cell><cell>7.0M</cell><cell>45.8</cell><cell>29.7</cell><cell>64.8%</cell></row><row><cell>FAN-S-Hybrid</cell><cell>26.3M</cell><cell>49.1</cell><cell>35.5</cell><cell>72.3%</cell></row><row><cell>FAN-B-Hybrid</cell><cell>50.4M</cell><cell>53.5</cell><cell>39.0</cell><cell>72.9%</cell></row><row><cell>FAN-B-Hybrid  ?</cell><cell>50.4M</cell><cell>54.2</cell><cell>40.6</cell><cell>74.9%</cell></row><row><cell>FAN-L-Hybrid</cell><cell>76.8M</cell><cell>54.1</cell><cell>40.6</cell><cell>75.0%</cell></row><row><cell>FAN-L-Hybrid  ?</cell><cell>76.8M</cell><cell>55.1</cell><cell>42.0</cell><cell>76.2%</cell></row><row><cell cols="5">ImageNet-A and ImageNet-R. The experiment results are</cell></row><row><cell cols="5">summarized in Table 11. Among these models, ResNet-50</cell></row><row><cell cols="5">(Liu et al.) presents weakest generalization ability while the</cell></row><row><cell cols="5">recent ConvNeXt substantially improves the generalization</cell></row><row><cell cols="5">performance of CNNs. The transformer-based models, Swin</cell></row><row><cell cols="5">and RVT performs comparably well as ConvNeXt and much</cell></row><row><cell cols="5">better than ResNet-50. Our proposed FANs outperform all</cell></row><row><cell cols="5">these models significantly, implying the fully-attentional ar-</cell></row><row><cell cols="5">chitecture aids generalization ability of the learned represen-</cell></row><row><cell cols="5">tations as the irrelevant features are effectively processed.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Main results on out-of-distribution generalization. FAN models show improved generalization across all datasets. '</figDesc><table><row><cell>Model</cell><cell cols="3">Params (M) Clean IN-A IN-R IN-C</cell></row><row><cell></cell><cell cols="2">ImgNet-1k pretrain</cell><cell></cell></row><row><cell>XCiT-S12 (El-Nouby et al.)</cell><cell>26.3</cell><cell>81.9</cell><cell>25.0 45.5 51.5</cell></row><row><cell>XCiT-S24 (El-Nouby et al.)</cell><cell>47.7</cell><cell>82.6</cell><cell>27.8 45.5 49.4</cell></row><row><cell>RVT-S* (Mao et al.)</cell><cell>23.3</cell><cell>81.9</cell><cell>25.7 47.7 51.4</cell></row><row><cell>RVT-B* (Mao et al.)</cell><cell>91.8</cell><cell>82.6</cell><cell>28.5 48.7 46.8</cell></row><row><cell>Swin-T (Liu et al.)</cell><cell>28.3</cell><cell>81.2</cell><cell>21.6 41.3 59.6</cell></row><row><cell>Swin-S (Liu et al.)</cell><cell>50</cell><cell>83.4</cell><cell>35.8 46.6 52.7</cell></row><row><cell>Swin-B (Liu et al.)</cell><cell>87.8</cell><cell>83.4</cell><cell>35.8 64.2 54.4</cell></row><row><cell>ConvNeXt-T (Liu et al.)</cell><cell>28.6</cell><cell>82.1</cell><cell>24.2 47.2 53.2</cell></row><row><cell>ConvNeXt-S (Liu et al.)</cell><cell>50.2</cell><cell>82.1</cell><cell>31.2 49.5 51.2</cell></row><row><cell>ConvNeXt-B (Liu et al.)</cell><cell>88.6</cell><cell>83.8</cell><cell>36.7 51.3 46.8</cell></row><row><cell>FAN-S-ViT (Ours)</cell><cell>28.0</cell><cell>82.5</cell><cell>29.1 50.4 47.7</cell></row><row><cell>FAN-B-ViT (Ours)</cell><cell>54.0</cell><cell>83.6</cell><cell>35.4 51.8 44.4</cell></row><row><cell>FAN-L-ViT (Ours)</cell><cell>80.5</cell><cell>83.9</cell><cell>37.2 53.1 43.3</cell></row><row><cell>FAN-S-Hybrid (Ours)</cell><cell>26.0</cell><cell>83.6</cell><cell>33.9 50.7 47.8</cell></row><row><cell>FAN-B-Hybrid (Ours)</cell><cell>50.0</cell><cell>83.9</cell><cell>39.6 52.9 45.2</cell></row><row><cell>FAN-L-Hybrid (Ours)</cell><cell>76.8</cell><cell>84.3</cell><cell>41.8 53.2 43.0</cell></row><row><cell></cell><cell cols="2">ImgNet-22k pretrain</cell><cell></cell></row><row><cell>ConvNeXt-B  ? (Liu et al.)</cell><cell>88.6</cell><cell>86.8</cell><cell>62.3 64.9 43.1</cell></row><row><cell>FAN-L-Hybrid (Ours)</cell><cell>76.8</cell><cell>86.5</cell><cell>60.7 64.3 35.8</cell></row><row><cell>FAN-L-Hybrid  ? (Ours)</cell><cell>76.8</cell><cell>87.1</cell><cell>74.5 71.1 36.0</cell></row><row><cell cols="4">et al., 2021; Zheng et al., 2020) and segmentation</cell></row></table><note><p>? ' denotes results with finetuning on 384 ? 384 image resolution. IN-C is measured by mCE (?). All metrics are scaled by (%).</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Details</head><p>A.1. Proof on the relationship between the Information Bottleneck and Self-Attention Suppose we are given a joint distribution P (X, Y ). The information bottleneck (IB) approach seeks a mapping q(z|x) such that Z contains the relevant information in X for predicting Y . This goal is formulated as the following information-theoretic optimization problem q * IB (z|x) = arg min q(z|x)</p><p>subject to the Markov constraint Z ? X ? Y . ? is a free parameter that trades-off the information compression by the first term and the relevant information maintaining by the second.</p><p>The information bottleneck approach can be applied for solving unsupervised clustering problems. To apply IB to a clustering problem, we must specify how to define the variables X and Y . Here we choose X to be the data point index i to be clustered into cluster indices c. As for the target variable Y , we aim to maintain its information and thus we choose it to be the data features x.</p><p>Following previous works, we assume the following data distribution:</p><p>where s is a smoothing parameter. We assume the marginal to be p(i) = 1 N , where N is the number of data points. Using the above notations, the t-th step in the iterative IB for clustering is formulated as</p><p>Here Z(x, ?) is the normalizing factor and S c denotes the set of indices of data points assigned to cluster c.</p><p>We choose to replace q(x|c) with a Gaussian approximation g(x|c) = N (x|? c , ? c ) and assume s is sufficiently small. Then,</p><p>where B denotes terms not dependent on the assignment of data points to clusters and thus irrelevant for the objective. Thus the above cluster update can be written as:</p><p>The next step is to update ? c to minimize the KL-divergence between g(x|c) and p(x|c):</p><p>Minimizing the above w.r.t. ? c gives:</p><p>By properly re-arranging the above terms and writing them into a compact matrix form, the relationship between the IB approach and self-attention would become clearer. Assume ? c = ? is shared across all the clusters. Assume ? c are normalized w.r.t.</p><p>Then the above update (15) can be written as:</p><p>Here the softmax normalization is applied along the row direction. Thus we conclude the proof for Proposition 2.1.</p><p>Proposition 2.1 can be proved by following the above road map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation details</head><p>ImageNet classification For all the experiments and ablation studies, the models are pretrained on ImageNet-1K if not specified additionally. The training recipes follow the one used in <ref type="bibr">(Touvron et al., 2021a)</ref> for both the baseline model and our proposed FAN model family. Specifically, we train FAN for 300 epochs using AdamW with a learning rate of 2e-3. We use 5 epochs to linearly warmup the model. We adopt a cosine decaying schedule afterward. We use a batch size of 2048 and a weight decay of 0.05. We adopt the same data augmentation schemes as <ref type="bibr">(Touvron et al., 2021a)</ref> including Mixup, Cutmix, RandAugment, and Random Erasing. We use Exponential Moving Average (EMA) to speed up the model convergence in a similar manner as timm library <ref type="bibr" target="#b43">(Wightman, 2019)</ref>. For the image classification tasks, we also include two class attention blocks at the top layers as proposed by Touvron et al..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic segmentation and object detection</head><p>For FAN-ViT, we follow the same decoder proposed in semantic transformer (SETR) <ref type="bibr" target="#b52">(Zheng et al., 2021)</ref> and the same training setting used in Segformer <ref type="bibr" target="#b44">(Xie et al., 2021)</ref>. For object detection, we finetune the faster RCNN <ref type="bibr" target="#b30">(Ren et al., 2015)</ref> with 2x multi-scale training. The resolution of the training image is randomly selected from 640?640 to 896 ? 896. We use a deterministic image resolution of size 896? 896 for testing.</p><p>For FAN-Swin and FAN-Hybrid, We finetune Mask R-CNN <ref type="bibr" target="#b13">(He et al., 2017)</ref> on the COCO dataset. Following Swin Transformer <ref type="bibr" target="#b23">(Liu et al., 2021)</ref>, we use multi-scale training, AdamW optimizer, and 3x schedule. The codes are developed using MMSegmentation <ref type="bibr" target="#b9">(Contributors, 2020)</ref> and MMDetection <ref type="bibr" target="#b7">(Chen et al., 2019)</ref> toolbox.</p><p>Corruption dataset preparation For ImageNet-C, we directly download it from the mirror image provided by Hendrycks &amp; Dietterich. For Cityscape-C and COCO-C, we follow Kamann &amp; Rother and generate 16 algorithmically generated corruptions from noise, blur, weather and digital categories.</p><p>Evaluation metrics For ImageNet-C, we use retentaion as a main metric to measure the robustness of the model which is defined as ImageNet-C Acc. ImageNet Clean Acc . It measures how much accuracy can be reserved when evaluated on ImageNet-C dataset. When comparing with other models, we also report the mean corruption error (mCE) in the same manner defined in the ImageNet-C paper <ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref>. The evaluation code is based on timm library <ref type="bibr" target="#b43">(Wightman, 2019)</ref>. For semantic segmentation and object detection, we load the ImageNet-1k pretrained weights and finetune on Cityscpaes and COCO clean image dataset. Then we directly evaluate the performance on Cityscapes-C and COCO-C. We report semantic segmentation performance using mean Intersection over Union (mIoU) and object detection performance using mean average precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Detailed benchmark results on corrupted images on classification, segmentation and detection</head><p>The by category robustness of selected models and FAN models are shown in Tab. 12, Tab. 13 and Tab. 14 respectively. As shown, the strong robustness of FAN is transferrable to all downstreaming tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Architecture details of FAN-Swin and FAN-Hybrid</head><p>For FAN-Swin architecture, we follow the same macro architecture design by only replacing the conventional self-attention module with the efficient window shift self-attention in the same manner as proposed in the Swin transformer <ref type="bibr" target="#b23">(Liu et al., 2021)</ref>. For the FAN-Hybrid architecture, we use three convolutional building blocks for each stage in the same architecture as proposed in ConvNeXt <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Feature clustering and visualizations</head><p>To cluster the token features, we first normalize the tokens taken from the second last block's output with a SoftMax function. We then calculate a self-correlation matrix based on the normalized tokens and use it as the affinity matrix for spectral clustering. Figure <ref type="figure">9</ref> provides more visualization on clustering results of token features from our FAN, ViT and CNN models. The visualization on Cityscape is shown in Figure <ref type="figure">8</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deit-B (touvron</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Swin-B (</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Convnext-B (</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are transformers more robust than cnns?</title>
		<author>
			<persName><forename type="first">References</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image recognition: Visual grouping, recognition, and learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="14203" to="14204" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8828" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">A convnet for the 2020s. CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. Naseer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision transformers are robust learners</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks? NeurIPS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the adversarial robustness of visual transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15670</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sparse mlp for image recognition: Is self-attention really necessary?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05422</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. physics/0004057</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reorganization: Grouping, contour detection, segmentation, ecological statistics</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
		</author>
		<ptr target="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Pytorch image models</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of natural images via lossy data compression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="225" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet. ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Icnet for realtime semantic segmentation on high-resolution images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">End-toend object detection with adaptive clustering transformer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Deepvit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Towards deeper vision transformer</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Refiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03714</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021b. Zhu,. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Refining self-attention for vision transformers</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Contr Satur JPEG Pixel Bright</title>
		<author>
			<persName><forename type="first">Model</forename><surname>Param</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Snow Fog Frost Mobile Setting</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mobilenetv</surname></persName>
		</author>
		<idno>M 35.0 33.4 29.6 21.3 32.9 24.4 21.5 23.7 32.9 57.6 49.6 38.0 62.5 28.4 45.2 37.6 28.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fan-T-Vit-P16</forename></persName>
		</author>
		<idno>M 57.5 52.4 48.3 37.4 51.5 54.8 54.7 53.1 60.2 66.6 72.8 62.7 56.7 74.3 55.5 61.4 53.6 FAN-T-Hybrid-P16</idno>
		<imprint/>
	</monogr>
	<note>Ours) 8M 57.4 52.6 46.7 34.3 50.3 55.5 55.8 54.5 61.4 65.8 73.3 63.8 47.9 74.5 55.0 61.4 52.8 GPU Setting (20M+</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-S (</forename><surname>Vit</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>22M 54.2 49.7 45.2 38.4 48.0 50.2 47.6 49.0 57.5 58.4 70.1 61.6 57.3 72.5 51.2 50.6 57.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">73.6 55.1 61.1 60.7 FAN-S-ViT (Ours)</title>
		<author>
			<persName><surname>Deit-S (touvron</surname></persName>
		</author>
		<idno>28M 64.5 61.4 56.3 45.6 58.7 62.1 63.0 61.1 67.1 70.9 77.1 69.4 63.5 78.4 63.5 68.2 61.2 FAN-S-Hybrid (Ours) 26M 64.7 60.8 56.0 44.5 58.6 65.6 66.2 64.8 69.7 67.5 77.4 68.7 61.0 78.4 63.2 66.1 62.4 GPU Setting (50M+</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-B * (</forename><surname>Vit</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>88M 59.7 60.2 55.6 50.0 57.6 54.9 52.9 53.2 62.0 52.3 71.5 68.7 71.7 74.9 52.8 57.1 41.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deit-B (touvron</surname></persName>
		</author>
		<idno>89M 62.7 56.7 52.2 43.6 55.1 64.9 63.5 61.2 65.7 68.2 74.6 66.9 61.7 76.2 59.7 68.2 64.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">FAN-B-ViT (Ours) 54M 67.0 64.2 58</title>
		<author>
			<persName><forename type="first">Swin-S (</forename><surname>Liu</surname></persName>
		</author>
		<idno>7 60.8 66.0 67.3 65.0 69.8 72.9 78.1 71.2 66.9 79.3 64.5 70.9 62.8 FAN-B-Hybrid (Ours) 50M 66.4 62.5 58.0 47.2 60.9 67.6 67.9 67.1 71.2 70.8 78.0 69.3 62.1 78.9 64.8 69.8 63.3 FAN-B-Hybrid-IN22K (Ours) 50M 70.5 67.4 62.9 55.6 65.4 70.3 71.6 70.1 73.8 74.1 79.8 74.3 79.8 81.0 70.2 72.2 65.4 GPU Setting (80M+</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deit-B (touvron</surname></persName>
		</author>
		<idno>86M 59.7 60.22 55.6 50.0 57.6 54.9 52.9 53.2 62.0 52.3 71.5 68.7 71.7 74.9 52.9 57.1 54.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Swin-B-In22k (</forename><surname>Liu</surname></persName>
		</author>
		<idno>88M 68.6 66.1 62.1 48.2 63.2 67.3 66.2 66.4 70.5 71.7 77.8 73.5 74.0 80.3 66.2 74.0 66.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">FAN-L-ViT (Ours) 81M 67</title>
		<author>
			<persName><forename type="first">Convnext-B (</forename><surname>Liu</surname></persName>
		</author>
		<idno>6 61.1 66.8 68.5 65.6 70.1 72.5 78.4 71.3 69.8 79.7 66.5 71.5 64.8 FAN-L-Hybrid (Ours) 77M 68.3 65.1 59.2 49.2 61.9 70.1 71.1 69.4 72.7 72.4 77.6 71.8 66.6 79.6 65.6 71.3 65.7 FAN-L-Hybrid-IN22K (Ours) 77M 73.6 71.2 67.5 58.9 69.3 73.9 75.1 73.4 76.6 76.4 81.6 76.8 74.0 82.5 73.6 74.3 69.6</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Model Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost DLv3+</title>
		<imprint/>
	</monogr>
	<note>R50</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Icnet</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Pspnet</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno>34.5 59.8 53.2 44.4 53.9 11.0 15.4 15.4 34.2 60.4 51.8 30.6 21.4 8.4 42.7 34.4 16.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Convnext-T (</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Swin-T (</forename><surname>Liu</surname></persName>
		</author>
		<idno>47.5 62.1 61.0 48.7 62.2 22.1 24.8 25.1 42.2 75.8 62.1 75.7 33.7 19.9 56.9 72.1 30.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Segformer-B0 (</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Segformer-B1 (</forename><surname>Xie</surname></persName>
		</author>
		<idno>52.6 63.8 63.5 52.0 29.8 23.3 35.4 56.2 76.3 70.8 74.7 36.1 56.2 28.3 60.5 70.5 36.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Segformer-B2 (</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">FAN-B-Hybrid (Ours)</title>
		<idno>67.3 70.0 69.0 64.3 70.3 55.9 60.4 61.1 70.9 81.2 76.1 80.0 57.0 54.8 72.5 78.4 52.3 FAN-L-Hybrid (Ours) 68.5 70.0 69.9 65.3 71.6 60.0 64.5 63.3 71.6 81.4 76.2 80.1 62.3 53.1 73.9 78.9 54.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">FAN shows stronger robustness than other models. Model Average Blur Noise Digital Weather Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright</title>
	</analytic>
	<monogr>
		<title level="j">Contr Satur JPEG Snow Spatter Fog Frost ResNet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<pubPlace>Ren</pubPlace>
		</imprint>
	</monogr>
	<note>Comparison of model robustness on COCO-C (%). Faster-RCNN. et al.</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
	<note>ResNet-101 (Faster-RCNN</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fan-B-Hybrid</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Figure 8. Visualization on Cityscapes. Best viewed in Adobe Reader for video animation or view it with extrenal player</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
