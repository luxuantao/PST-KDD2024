<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HELAD: A Novel Network Anomaly Detection Model Based on Heterogeneous Ensemble Learning Journal Pre-proof HELAD: A Novel Network Anomaly Detection Model Based on Heterogeneous Ensemble Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ying</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Network Sciences</orgName>
								<orgName type="institution">Cyberspace at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Network Sciences</orgName>
								<orgName type="institution">Cyberspace at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Network Sciences</orgName>
								<orgName type="institution">Cyberspace at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science and Technology of Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yahui</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technology at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Yin</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technology at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingang</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Network Sciences</orgName>
								<orgName type="institution">Cyberspace at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahai</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Network Sciences</orgName>
								<orgName type="institution">Cyberspace at Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keqin</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York</orgName>
								<address>
									<region>New Paltz</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Networks</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HELAD: A Novel Network Anomaly Detection Model Based on Heterogeneous Ensemble Learning Journal Pre-proof HELAD: A Novel Network Anomaly Detection Model Based on Heterogeneous Ensemble Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18BA9761331C20FA6DF476E6EF53CF57</idno>
					<idno type="DOI">10.1016/j.comnet.2019.107049</idno>
					<note type="submission">Received date: 5 April 2019 Revised date: 11 November 2019 Accepted date: 9 December 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Traffic anomaly detection</term>
					<term>Ensemble Learning</term>
					<term>LSTM forecast</term>
					<term>Deep learning</term>
					<term>Adjustable threshold</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The importance of intrusion detection systems (IDS) is critical because networks can be vulnerable to attacks from internal and external intruders <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref>. Network traffic anomalies will lead to a decline in network communication performance and network service interruption. The definition of network anomalies is that the current network traffic is seriously deviating from normal traffic. Network anomalies are mainly *Zhiliang Wang is the author for correspondence. E-mail: wzl@cernet.edu.cn, zhongy18@mails.tsinghua.edu.cn. caused by malicious network attacks, e.g. Denial of Service (DoS), Distributed Denial of Service (DDoS), port scan, worm propagation, etc., as well as network configuration errors and other exception <ref type="bibr" target="#b2">[3]</ref> caused by the interruption of the line. As a detection system put in place to monitor computer networks, IDS has been in use since 1980s <ref type="bibr" target="#b3">[4]</ref>. By analysing patterns of captured data from a network, IDS helps to detect threats <ref type="bibr" target="#b4">[5]</ref>. Traffic anomaly detection has always been the research direction of network security academics and industry, and many related detection methods and systems have been developed.</p><p>The constant change of the attack mode makes it more difficult to solve the traffic anomaly detection problem. Traditional intrusion detection tools, such as rule-based Snort <ref type="bibr" target="#b5">[6]</ref>, are no longer able to meet the growing demand for network security. We need to design a smarter intrusion detection tool. This anomaly detection tool requires the ability to learn dynamically and requires environmental adaptation to defend against unknown attacks.</p><p>The current mainstream method of traffic anomaly detection is machine learning. Our design choices will be analyzed from different categories of machine learning.</p><p>1) Machine learning methods can be divided into shallow machine learning and deep learning according to the number of layers of neural networks involved: Shallow machine learning <ref type="bibr" target="#b6">[7]</ref> has the advantage of short training time, and deep learning <ref type="bibr" target="#b7">[8]</ref> has stronger representation ability. The trend of GPU <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> acceleration methods allow us to choose a deep learning approach.</p><p>2) Specific classification tasks can be divided into single classifiers and ensemble learning classifiers: The idea of ensemble learning is to improve machine learning performance by combining multiple models, which is better than a single model in common sense. Ensemble learning is the research hotspot of machine learning in the field of traffic anomaly detection <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. The ensemble learning model is superior to the single model in both predictive power and generalization ability, so ensemble learning is introduced in the design of our model.</p><p>3) Choose supervised learning or unsupervised learning: Supervised learning <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> interacts with the external environment through a label-guided approach, so that the trained model can better integrate into human domain knowledge. Therefore, it is necessary to incorporate a supervised learning model.</p><p>Based on the above design choices, we have the following challenges:</p><p>(1) Ensemble learning is well used in the field of anomaly detection. However, deep learning method has not been used as component learners <ref type="bibr" target="#b60">[61]</ref> of heterogeneous ensemble learning in network intrusion detection.</p><p>(2) Algorithms based on supervised learning require a large number of labeled training data to obtain good detection results. However, real network traffic data lacks of a large number of truly labeled data sets, which makes it difficult to use supervised deep learning.</p><p>(3) The attack environment of the network changes constantly. If the model does not have the ability to relearn, the performance of the detector will decrease.</p><p>(4) Many of the deep learning based anomaly detection models have not been evaluated with real traffic data. The main manifestation: the training data of anomaly detection model comes from the idealized data set, for example, the data set has been used for too long or it is just generated by the attack tool.</p><p>Inspired by the above observations, this paper attempts to absorb the advantages of heterogeneous ensemble learning and deep learning techniques and propose a more effective method.</p><p>To summarize, our main contributions in this paper are listed as follows:</p><p>• We have integrated various deep learning techniques and proposed the Heterogeneous Ensemble Learning Anomaly Detection (HELAD) algorithm framework. This framework is composed of four parts: feature dimension reduction, abnormal score generation, abnormal score prediction, and anomaly detection result combination. Each module can choose the appropriate technology according to its own design.</p><p>• We apply ensemble learning to anomaly detection. Specifically, the unsupervised Autoencoder and the supervised Long Short-Term Memory (LSTM) are combined in a heterogeneous way. The Autoencoder gains the profile of normal network traffic as one of the base learners, and provides learned RMSE as the label needed to train the LSTM. The LSTM can detect continuous attacks well, as it can record historical information and predict whether the attack will occur next time. In order to be able to use the supervised LSTM, we introduce the concept of a temporary label (TL), which is generated by an unsupervised Autoencoder.</p><p>• We introduce the concept of retraining time slices to retrain the model. This time slice is the time required to train the anomaly detection model in the previous round. We design dynamic thresholds and integrate learning parameters in the model so that the anomaly detection effect does not degrade.</p><p>• To evaluate the HELAD model , we conduct experiments on the latest data sets that reflect the real environment. And, we further evaluate our algorithm by comparing it with different state of the art algorithms. The experimental results consistently prove the superiority and competitiveness of our proposed model.</p><p>The rest of the paper is organized as follows. Section II presents the related work. We present our system model and problem formulation in Section III. The model training and strategy optimization are presented in Section IV. We do the experiment and evaluate the performance using real traffic trace data in Section V. We discuss some of the details of our models and experimental methods in Section VI. Finally, we conclude our work in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we review some literature work. Network intrusion detection is a classic network security issue. We focus on analyzing the related work from four aspects: traditional statistics, machine learning, deep learning and ensemble learning based methods. Next we discuss some of the literature for relearning.</p><p>We first discuss the traditional statistics method. There are several examples of statistical methods that are widely used in attack detection <ref type="bibr" target="#b14">[15]</ref>. Lee et al. <ref type="bibr" target="#b15">[16]</ref> proposed to use several information-theoretic measures, such as entropy, conditional entropy, relative conditional entropy, information gain, and information cost for anomaly detection. Other examples include the Cumulative Sum(CUSUM) algorithm <ref type="bibr" target="#b16">[17]</ref>, the exponentially weighted moving average (EWMA) algorithm <ref type="bibr" target="#b17">[18]</ref>, the Holt-Winter algorithm <ref type="bibr" target="#b18">[19]</ref>, and so on. Their advantage is that they do not need to know the prior knowledge of cyber attacks in advance. But most statistical methods rely on the assumption of a static detection process <ref type="bibr" target="#b19">[20]</ref>, which is not always realistic. The models in these references <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> are more suitable for anomaly detection of scenarios with small network environment changes. Our HELAD model incorporates prior knowledge and deep learning models to respond to changing environments. Such prior knowledge is embodied in the design of the features.</p><p>Next, we introduce some excellent methods based on machine learning. The purpose of machine learning is to create explicit or implicit models. In the field of anomaly detection, machine learning has the advantages of high detection rate and continuous learning and updating <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>.</p><p>Deep learning <ref type="bibr" target="#b23">[24]</ref> is the further development of neural networks. Deep learning uses a subsequent information processing layer in some hierarchies for classification or feature representation. For the strong presentation capabilities, existing IDS can be improved based on this latest technology. Many algorithms for anomaly detection of network traffic are based on deep learning. We will introduce the Autoencoder and Long Short-Term Memory (LSTM) that are most relevant to our work. About Autoencoder, Shone et al. <ref type="bibr" target="#b24">[25]</ref> proposed nonsymmetric deep Autoencoder (NDAE) for unsupervised feature learning. Furthermore, they also proposed novel deep learning classification model constructed using stacked NDAEs. Khan et al. <ref type="bibr" target="#b25">[26]</ref> proposed a novel two-stage deep learning model based on a stacked Autoencoder with a soft-max classifier for efficient network intrusion detection. Specifically, their proposed model is able to learn useful feature representations from large amounts of unlabeled data and classifies them automatically and efficiently. Mirsky et al. <ref type="bibr" target="#b26">[27]</ref> proposed a neural network based NIDS which has been designed to be efficient and plug-and-play. It accomplishes this task by efficiently tracking the behavior of all network channels, and by employing ensemble of Autoencoders for anomaly detection. On the other hand, Du et al. <ref type="bibr" target="#b27">[28]</ref> proposed DeepLog, a deep neural network model utilizing LSTM, to model a system log as a natural language sequence. Wang et al. <ref type="bibr" target="#b28">[29]</ref> proposed a novel IDS called the hierarchical spatialtemporal features-based intrusion detection system (HAST-IDS), which first learns the low-level spatial features of network traffic using deep convolutional neural networks (CNNs) and then learns high-level temporal features using LSTM. Jiang et al. <ref type="bibr" target="#b29">[30]</ref> proposed a novel multi-channel intelligent attack detection method based on long short term memory recurrent neural networks (LSTM-RNNs). The references <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> use Autoencoder for anomaly detection. Our HELAD model differs from these tasks. In our method, Autoencoder has two functions. One is to generate RMSE labels to train LSTM, and the other is to be one of the base classifiers. The references <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> all use LSTM as part of the model. They mainly consider LSTM for feature processing or time series establishment, but in our HELAD model, LSTM is used for RMSE prediction. As the labeled data set becomes larger, the prediction effect of LSTM will be better and better. Therefore, combining the LSTM and Autoencoder in our model to train the threshold of abnormal scores is better than a single deep learning model.</p><p>In addition, ensemble learning plays an important role in the field of anomaly detection <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. In these works, many excellent anomaly detection methods are based on homogeneous ensemble learning <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. Ensemble learning is also used in the most relevant paper in our research. However, this is a homogeneous learning based on stacking <ref type="bibr" target="#b26">[27]</ref>. Moreover, the advantages of heterogeneous ensemble learning in the field of classification are constantly being explored <ref type="bibr" target="#b36">[37]</ref>. For these reasons, we hope to seek better heterogeneous ways to make ensemble learning work well in the field of anomaly detection.</p><p>For relearning, we are mainly inspired by the following literatures. Papadimitriou et al. <ref type="bibr" target="#b52">[53]</ref> proposed arbitrary window stream modeling method (AWSOM), which allows sensors in remote or hostile environments to efficiently and effectively discover interesting patterns and trends. They developed a onepass algorithm to incrementally update the patterns. Ippoliti et al. <ref type="bibr" target="#b53">[54]</ref> developed an enhanced dynamic anomaly detector for network traffic, which use auxiliary set to give online feedback to the model. Its importance lies in providing anomaly detection based on general traffic and keep updating constantly to achieve online adaptation in the meanwhile. Viegas et al. <ref type="bibr" target="#b54">[55]</ref> presented BigFlow, a reliable stream learning intrusion detection engine that can maintain its accuracy over long periods of time. The labeled samples will be sent to an administrator periodically for judging, and the wrong-labeled ones will be used to re-train the model. Their solution evaluates the classification reliability, while it allows to incrementally update the intrusion detection engine. We conduct comparative analysis, finding that <ref type="bibr" target="#b52">[53]</ref> maintains a window and updates the model when new instances reach. <ref type="bibr" target="#b53">[54]</ref> uses false positives as an update condition. When the false positive rate increases, the auxiliary set is changed and the model is retrained. <ref type="bibr" target="#b54">[55]</ref> specifies the period of the update. In order to reduce the cost of the update, the models are incrementally updated only with instances that were previously rejected. Because we are dealing with the actual complex network environment, we are looking forward to updating our HELAD model as quickly as possible. So we take the training time as an interval and add as much of the latest labeled data as possible to the training. Relearning is part of our model to prevent model degradation.</p><p>To sum up, the developments of deep learning and ensemble learning have brought new opportunities to the development of anomaly detection. We adopt these advantages to design a more intelligent and adaptable anomaly detection tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HELAD: HETEROGENEOUS ENSEMBLE LEARNING ANOMALY DETECTION MODEL</head><p>In this section, first motivation of this paper are presented. Next, we provide a basic model description that includes the meaning of statistical features, the meaning of formulas, and the way in which features are expressed. After that, we build the submodel for each part of the HELAD model. Finally, the abnormality detection result can be obtained by discriminant formula. Fig. <ref type="figure" target="#fig_1">1</ref> details the training process of the HELAD model and the abnormality determination process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>In this section, we discuss our design choice of feature extraction and anomaly detection. Feature extraction has an important impact on the performance of the machine learning model. The network traffic has the following characteristics: 1) packets of different sessions are related; 2) there are many sessions at the same time; 3) the rate of packet arrival is high. In order to extract features efficiently, we decide to use the improved Damped Incremental Statistics algorithm <ref type="bibr" target="#b26">[27]</ref>.</p><p>For example, we analyze a TCP SYN packet in a network that does not adopt the SYN Cookies <ref type="bibr" target="#b57">[58]</ref> defense mechanism. This can be the network packet generated by the client when it normally accesses the server. This packet can also be one of millions of attack packets that cause DoS attack. It depends on the context of the information. Furthermore, for IP-based video streaming, although the packets are normal, if there is significant jitter in the traffic, then there may be a man-inthe-middle attack. If the feature can capture this jitter, then this attack can be detected. We need contextual information to make the decision and hope that our features can be incorporated into these metrics that reflect anomalies in traffic. The Damped Incremental Statistics algorithm can meet these conditions, which is why we choose it as feature extraction method.</p><p>The Damped Incremental Statistics algorithm can consider the relationship between network traffic packets, but cannot consider the relationship between consecutive attacks. Thus, we discuss the second question. Is there a way to detect the relationship between successive attacks. Based on the previous research <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, we found that an effective combination of Autoencoder and LSTM can achieve this idea. The specific analysis is as follows. First, if the anomaly score calculated by Autoencoder alone is directly classified, it is equivalent to completely ignoring the relationship of attack behavior between consecutive network traffic packet. This is a disadvantage of Autoencoder. Second, if a single LSTM network is used to train an anomaly detector, this network does not effectively use Autoencoder to characterize anomalous features and abandon the benefits of ensemble learning. However, the unsupervised nature of Autoencoder can improve the contextindependent detection. Third, LSTM has a recording function for historical samples. As the labeled data set becomes larger, the prediction effect of LSTM will be better and better. Therefore, combining the LSTM and Autoencoder networks to train the threshold of abnormal scores is better than a single neural network.  We use the mirai<ref type="foot" target="#foot_0">1</ref> dataset in Kitsune <ref type="bibr" target="#b26">[27]</ref> to explore the effects of LSTM predictions. Fig. <ref type="figure" target="#fig_3">2</ref> shows prediction effect of LSTM on mirai dataset. The x-axis represents the index of packet. The y-axis represents the value of the anomaly score. The solid line in dark green in the figure is the predicted value of LSTM, and the dashed line in blue is the value of the actual root mean square error (RMSE). The trend predicted by LSTM is similar to the actual value.</p><formula xml:id="formula_0">→ x0 -→ x0 = (f1, f2, • • • , fn 0 ) T , Original feature vector -→ xR -→ xR = (f1, f2, • • • , fn) T Feature vector after dimensionality reduction. -→ xL -→ xL = (f1, f2, • • • , fn 0 , RM SE)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>We take the Damped Incremental Statistics for feature extraction <ref type="bibr" target="#b26">[27]</ref>. A slight difference from the approach of Kitsune is that we remove the SrcMAC-IP field and we change the number of features per sample (packet). The element definitions of HELAD model are listed in Table <ref type="table" target="#tab_0">I</ref>.</p><p>The Damped Incremental Statistics algorithm treats one of the attributes of each packet as an unbounded stream. These attributes can be the count, size, or jitter (inter-packet delay) of the corresponding packets aggregated by the same SrcIP (source IP), Channel (source and destination IPs) and Socket (source and destination IPs and ports). For a packet attribute stream S, the mean, variance, and standard deviation of S can be updated by the tuple of IS = (N, LS, SS). N represents the total number of network packets arriving, LS is linear sum, and SS is squared sum. Each time a new network traffic packet p i arrives, feature extraction can be performed according to the feature extraction formula in the table I, and the tuple of IS can be updated by IS ← (N +1, LS +x i , SS +x i 2 ). x i is the statistic of p i . The statistics at any time are </p><formula xml:id="formula_1">µ s = LS N ,σ s 2 = SS N -( LS N ) 2 , and σ s = √ σ s 2 .</formula><formula xml:id="formula_2">(σs i ) 2 + (σs j ) 2 Covariance CovS i S j SR ij W i +W j Correlation Coefficient PS i S j</formula><p>Cov S i S j σs i σs j</p><p>In the case of using a sliding window, the memory and runtime complexity required for the IS method is O(n). For reducing the complexity to O(1), IS i,λ data structure is used to maintain the latest snapshot of the feature. The main idea of the Damped Incremental Statistics algorithm is that the weight of a sample decreases with time. The decay model for this weight uses the formula: d λ (t) = 2 -λt , where λ &gt; 0 is the decay factor, and the t is the timestamp difference between the current packet and the previous network packet.</p><p>Specifically, IS i,λ = (w, LS, SS, SR ij , T last ) is maintained in real time. w is the current weight and one of the onedimensional feature calculation methods, which is calculated by first adding one and then multiplying by decay value. (1 )</p><formula xml:id="formula_3">0 1 2 0 1 2</formula><p>pe p e pe pe 0</p><formula xml:id="formula_4">1 2 0 1 2 (1 ) (1 ) 0 1 2 0 1 2 0 1 2 y y y y 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2</formula><p>+ + y y y y y y y y</p><formula xml:id="formula_5">0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 (1 (1 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 y 1 2 3</formula><p>y y y</p><formula xml:id="formula_6">1 2 3 1 2 3 1 2 3 1 2 3 1 2 3</formula><p>Training and Execution The first three lines of Table <ref type="table" target="#tab_2">II</ref> are used to calculate onedimensional features. T last is the timestamp of the previous packet. SR ij is the sum of residual products between two attribute streams, which is used to calculate two-dimensional features. r i r j represents one of the two-dimensional feature calculation methods. The methods in the last four rows of Table <ref type="table" target="#tab_2">II</ref> are used to calculate two-dimensional features. For updating the IS i,λ in real time, Damped Incremental Statistics has the following update steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relearning</head><p>Step 1: Calculate the decay factor, φ ← d λ (t curt last );</p><p>Step 2: Calculate IS i,λ ← (φw, φLS, φSS, φSR, T cur ) and integrate newly arrived packet into data structures IS i,λ ← (w + 1, LS + x cur , SS + x i 2 , SR ij + r i r j , T cur );</p><p>Step 3: t last = t cur , goto Step 1.</p><p>Next we introduce the formation of features. For instance, op 1 represents statistical feature st 1 operates on the aggregation ag 1 in the case of the bandwidth of the outbound traffic. st 1 is from A 1 , where A 1 represents the statistical method. ag 1 is from B 1 , where B 1 represents what the packets are aggregated by. Therefore, the first feature can be represented as f 1 =&lt; λ 1 , µ i , SrcIP &gt;. This means that in the case of first decay factor λ 1 , statistic µ i operates on attribute SrcIP as a feature.</p><p>After selecting the original features -→  </p><formula xml:id="formula_7">x 0 = (f 1 , f 2 , • • • , f n0 ) T ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Dimension Reduction</head><p>The first is the HELAD-DBN dimension reduction submodel. There are two reasons why DBN technology is applied to our model. First, the DBN has the ability to find good features in incomplete information. Second, the DBN can achieve dimensionality reduction, so that the efficiency of training Autoencoder is improved. This submodel is essentially a multi-layer HELAD-RBM model. Therefore, the focus of our discussion is the training and solution of the original feature vector -→</p><p>x 0 in the HELAD-RBM model. The elements of the HELAD-RBM model are defined as follows. n 0 , m represent the number of neurons in the visible layer and the hidden layer, respectively. -→</p><formula xml:id="formula_8">x 0 = (f 1 , f 2 , • • • , f n0 ) T is</formula><p>the state vector of the visible layer, and</p><formula xml:id="formula_9">-→ h 0 = (h 1 , h 2 , • • • , h m ) T is the state vector of the hidden layer. The vectors - → a = (a 1 , a 2 , • • • , a n0 ) T , - → b = (b 1 , b 2 , • • • , b n0</formula><p>) T represent the offset vectors of the visible and hidden layers, respectively. The RBM model is an energy-based model. For a given set of states( -→</p><p>x 0 , -→ h 0 ), the system energy of HELAD-RBM can be defined as:</p><formula xml:id="formula_10">E( -→ x 0 , -→ h 0 | - → θ ) = - n i=1 a i f i - m j=1 b j h j - n i=1 m j=1 f i w ij h j<label>(1)</label></formula><p>In the formula, -→ θ represents the parameter set of HELAD-RBM. w ij is the connection weight between the i th neuron of the visible layer and the j th neuron of the hidden layer, b j represents the offset of the j th neuron of the hidden layer, and a i represents the offset of the i th neuron of the visible layer.</p><p>To solve this model, we can use the contrast divergence algorithm proposed by <ref type="bibr" target="#b38">[39]</ref>. The specific DBN dimension reduction solution can be referred to the literature <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. After HELAD-DBN dimension reduction we get a new feature vector</p><formula xml:id="formula_11">-→ x R = (f 1 , f 2 , • • • , f n ) T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Abnormal Score Generation</head><p>The second sub-model is the HELAD-Autoencoder anomaly score calculation model. This submodel calculates the anomaly score by the value of the loss function. Autoencoder <ref type="bibr" target="#b41">[42]</ref> tries to learn the function S(x), which satisfies: S w,w ,b1,b2 ( -→</p><p>x R ) = -→ x R . Where w, w , b 1 , and b 2 are model parameters. S(x) can be represented in two phases. The first is the coding phase from the input layer to the hidden layer. The second is the decoding phase from the hidden layer to the output layer.</p><formula xml:id="formula_12">h = f (w -→ x R + b 1 )<label>(2)</label></formula><formula xml:id="formula_13">-→ y R = g(w h + b 2 )<label>(3)</label></formula><p>The learning goal of this network is -→ y R ≈ -→ x R . In order to reconstruct the input -→</p><p>x R as much as possible, the root mean square error (RMSE) is used as the loss function:</p><formula xml:id="formula_14">RM SE( -→ x R , -→ y R ) = n i=1 (xi-yi) 2 n</formula><p>. Autoencoder is a special artificial neural network (ANN). Commonly used way for training an ANN is known as the back-propagation algorithm <ref type="bibr" target="#b42">[43]</ref> and the activation function we use is sigmoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Abnormal Score Prediction</head><p>The third sub-model is the HELAD-LSTM anomaly score prediction model. This sub-model predicts the anomaly scores of the next three samples based on the anomaly scores of the historical samples. Inspired by <ref type="bibr" target="#b43">[44]</ref>, we decide to use LSTM to perform timing prediction on anomalies. -→</p><formula xml:id="formula_15">x L = (f 1 , f 2 , • • • , f n0 , RM SE)</formula><p>T is the original feature vector plus the RMSE label (TL). This is a new feature vector for training HELAD-LSTM which can predict abnormal scores.</p><p>At training time t, the parameters of HELAD-LSTM are updated as follows:  </p><formula xml:id="formula_16">!"#$!% !"#$!% !"#$!% &amp;'() &amp;'() * + ! * + " # * , - + ! * + ! * , - + $ * + $ * + ! * . + ! * / + ! * , + ! * , - + ! * / - + ! * + " # * , - + " # * / - + " # !"#$!% !"#$!% !"#$!% &amp;'()</formula><formula xml:id="formula_17">i t = f (w xi -→ x L (t) + w hi b h t-1 + w ci b c t-1 + σ i )<label>(4)</label></formula><formula xml:id="formula_18">b f t = f (w xf -→ x L (t) + w hf b h t-1 + w cf b c t-1 + σ f ) (5) b c t = b f t × b c t-1 + b i t × f (w xc -→ x L (t) + w hc b c t-1 + σ c )<label>(6)</label></formula><formula xml:id="formula_19">b o t = f (w xo -→ x L (t) + w ho b h t-1 + w co b c t-1 + σ o ) (7) b h t = b o t × f (b c t )<label>(8)</label></formula><formula xml:id="formula_20">a k t = w k b h t (9) Since LSTM is a time series model, -→ x L (t) in the formula is the representation of -→ x L at time t. b i t , b f t , b c t , b o t</formula><p>represent the input value of the input gate, the forgetting gate, the memory state, and the output gate respectively at time t; a k t represents the output value of the network output layer at time t, which is the value of y (t) . We can calculate the values involved in the above formula (4)- <ref type="bibr" target="#b8">(9)</ref> in turn. Fig. <ref type="figure" target="#fig_4">3</ref> indicates the relationship between internal variables of LSTM unit. Finally, we will get the predicted RMSE values y (t+1) , y (t+2) , y (t+3) for the three moments t + 1, t + 2, and t + 3, and each prediction yields a predicted value. The specific LSTM network solving algorithm can be found in the literature <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Abnormal Detection Result Combination</head><p>In the final step of the model, we use the discriminant formula pe y0 + (1p)e y1+y2+y3 to calculate the anomaly detection score for each packet. We use the simulated annealing algorithm to optimally select the gp and p values. Specifically, it is shown in Algorithm 1. If the value of the anomaly detection score exceeds the threshold gp, this packet will be recognized as an abnormality. The specific details are shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis of Heterogeneous Ensemble of HELAD Model</head><p>Deep learning method has not been used as component learners of heterogeneous ensemble learning in network intrusion detection. In addition, HELAD model does not have a more common ensemble learning mechanism like stacking, voting, etc. We mainly analyse our heterogeneous model from the perspective of the ensemble strategy and base classifier selection based on reference <ref type="bibr" target="#b60">[61]</ref>.</p><p>About ensemble strategy, ensemble learning models can be defined as models using multiple learners to do classification, and there is no need to use specific ensemble methods. So although we don't use classic ensemble methods such as stacking, our model is still an ensemble learner. Our discriminant formula is actually a variant of weighted averaging method defined as combination methods in <ref type="bibr" target="#b60">[61]</ref>, which uses the weighted average of the Autoencoder and the LSTM outputs as the anomaly score. In terms of base classifier selection, we use Autoencoder and LSTM as component learners, and the advantage of HELAD model is that both base classifiers are deep learning methods.</p><p>In summary, we propose a new heterogeneous ensemble learning approach which is different from classic ensemble methods such as stacking, where we use the Autoencoder to model normal traffic profile and generate the label to train the LSTM, and use heuristic algorithm to do weighted averaging with results of the Autoencoder and LSTM in the final stage. This is essentially the voting method, which is a method of heterogeneous ensemble learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL TRAINING &amp; STRATEGY OPTIMIZATION</head><p>The previous section mainly introduces our model from the perspective of technology construction. This section is now elaborated from the perspective of specific training details and the optimization of the effects of the entire model. The training of the model is to obtain the values of the parameters gp and p in the discriminant function after the neural network is stable. The values of gp and p are detailed in Table <ref type="table" target="#tab_2">II</ref>. All specific training steps can be seen in Fig. <ref type="figure" target="#fig_1">1</ref>. We divide the overall training model into three stages as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initialization</head><p>The original small amount of expert annotated (abnormal or normal) sample dataset is acquired by Damped Incremental Statistics algorithm to obtain a high-dimensional feature vector -→ x 0 . Then, the feature reduction is performed using the DBN to obtain the feature vector -→</p><p>x R . Next, the feature vector -→ x R is taken as the input of the Autoencoder, and then the training of the Autoencoder is completed. During training, the root mean square error (RMSE) of each sample reconstruction is accurately recorded. This RMSE is used as an anomaly detection score. Next, the RMSE of each sample is added as a feature to the feature vector -→ x 0 to form a new feature vector -→</p><p>x L . then, The feature vector -→ x L is trained as an input to the LSTM network. After the Autoencoder network and LSTM network training is completed, the data set marked by the expert is used again. This time it is used to train the values of gp and p. Specifically, each sample collected will have an output y 0 through the Autoencoder, and then output y 1 , y 2 , y 3 through the LSTM. We establish the discriminant formula pe y0 + (1p)e y1+y2+y3 and then use the label (abnormal or normal) of each sample for parameter tuning. In this way, we can initialize the values of gp and p of the HELAD algorithm (the case where the detection rate is the highest) by the label marked by the expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real Time Traffic Detection</head><p>The HELAD anomaly detector has been initialized in the above stage. Next, the HELAD model can begin to receive actual network traffic (as indicated by the red arrow). Network traffic continues through the Autoencoder network and the LSTM network, and y 0 , y 1 , y 2 , y 3 are obtained for each sample taken online. </p><formula xml:id="formula_21">for k = 0 to L -1 do 8: x = x + (rand() -0.5, rand() -0.5) × 0.5 k ; 9:</formula><p>//the formula makes the whole solution domain is involved 10: -→ x 0 =Feature Extraction(pps i );//Extracting feature of a sample using the Damped Incremental Statistics algorithm; 5: </p><formula xml:id="formula_22">if x [0] &lt; 0 or x [0] &gt; 1 or x [1] &lt; 0 or x [1] &gt;</formula><formula xml:id="formula_23">-→ x R =DBN( -→ x 0 ); 6: -→ y R =Autoencoder( -→ x R ); 7: T L = RM SE( -→ x R , -→ y R ) = n i=1 (xi-yi)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end if 32: end while</head><p>Then we can get the value dv of the formula pe y0 + (1p)e y1+y2+y3 . If dv is greater than gp, then decision is yes, which is an abnormal. On the contrary, it is normal. In this way, online traffic data is continuously labeled and stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relearning HELAD Model</head><p>At this stage, we can set an environment timer K that the user can adjust (in the time of this K, the expert can modify the erroneous label in the log according to experience, if time and effort allow). When the cumulative time of K reaches a certain value, we merge the original expert data with the tagged data in the log. We then re-trained the HELAD model by replacing the expert annotation data for the first phase with this new data set. It is important to note that the anomaly detection detector formed by the last gp and p is used for detection before the next retraining is completed. The reason for this is that the training time and the detection time are parallel, and there is no waiting time for training. The specific details are shown in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS &amp; EVALUATION</head><p>This section covers our experimental results. Our codes are available at the open-source code repository <ref type="foot" target="#foot_1">2</ref> . In order to systematically evaluate our model, we want to check the following four points: <ref type="bibr" target="#b0">(1)</ref> In what circumstances can our algorithm achieve the best performance. (2) Whether ensemble learning and the re-learning function are effective. (3) How does our algorithm compare to the performance of other state of the arts algorithms on different data sets. (4) How does our algorithm compare to the state-of-the-art homogeneous ensemble learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Metrics for Evaluating Anomaly Detection Algorithm</head><p>Time consuming is one of the shortcomings of machine learning. Parallel computing, Graphics Processing Unit and other acceleration methods will be used in our future work. Therefore, the level we discuss is the effect of anomaly detection in the latest real network traffic and comparison with other machine learning algorithms.</p><p>The effectiveness of Machine Learning based anomaly detection algorithm can be evaluated by the following indicators: Precision (P): T P T P +F P , Recall (R): T P T P +F N , F1-Score (F1): 2×P ×R P +R , False Positive Rate (FPR): F P F P +T N , Area Under Curve (AUC): the area enclosed by the ROC curve and the coordinate axis. Among them, True Negative (TN): a measure of the number of normal events rightly classified as normal ones. True Positive (TP): a measure of the number of abnormal events rightly classified as abnormal ones. False Positive (FP): a measure of normal events misclassified as attacks. False Negative (FN): a measure of attacks misclassified as normal.</p><p>We use these five indicators to measure the performance of our HELAD algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and Experimental Settings 1) Datasets</head><p>Two data sets, MAWILab<ref type="foot" target="#foot_2">3</ref> and IDS 2017 <ref type="foot" target="#foot_3">4</ref> , are used in this paper. The experiments are all based on the MAWILab dataset. How ever, the IDS 2017 dataset will be tested in contrast experiment, which means comparison with other algorithms. MAWILab <ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref> is a database that assists researchers to evaluate their traffic anomaly detection methods. MAWILab annotates traffic anomalies in the MAWI archive with four different labels: anomalous, suspicious, notice, and benign. In order to use these anomaly detection labels marked by different anomaly detector. We classify the two categories anomalous and suspicious as abnormal and classify benign and notice into normal. Table <ref type="table" target="#tab_6">III</ref> are examples of the anomalies we use in our experiments. The first line identifies that network scan SY N is detected by the anomaly detectors. This is a SYN attack, and the original label is anomalous. Then, the label we give is an abnormal. The sixth line identifies that network scan ICM P ecrq is detected by the anomaly detector Hough. This is a ping flood, and the original label is suspicious. The label we give is abnormal.</p><p>The IDS2017 data set collection time is from July 3, 2017 to July 7, 2017. The data set contains benign traffic and some of the latest common attacks. The format of the data set includes real data (pcap file) and the results of analysis using CICFlowMeter. Generating real attack scenes is the main task of this data set. This data set includes attacks such as Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet and DDoS. We use the original packet and its corresponding label to our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experiment Environment</head><p>In order to use a variety of algorithms more effectively, we use Python to implement our model. The hardware and software configurations are shown in Table <ref type="table" target="#tab_7">IV</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results &amp; Implications</head><p>This section illustrates a group of experiments to verify the effectiveness of HELAD Model. We study the proposed anomaly detection using MAWILab data for the period of June 3, 2018. Table <ref type="table" target="#tab_8">V</ref> shows the distribution of the data set. Here, we have a total of 300,000 network traffic packets. All of these packets take about 15 minutes to collect. Among them, 125,374 network traffic packets are abnormal. Table VI illustrates number of labels in training set (including verification set) and testing set. In order to accurately predict the future real-predictive environment, the predictor must retain data for events that occur after the event that fits the model <ref type="bibr" target="#b58">[59]</ref>. Therefore, for time-series data such as network traffic, we do not use k-fold cross-validation, but use hold-out. The holdout method directly divides the data set D into two mutually exclusive sets. One of the sets is used as the training set S, and the other is used as the test set T, ie D=S∪T, S∩T=0. After training the model on S, the test error is evaluated by T, and as evaluation of generalization error. To assess the performance accurately, we use three partition methods to train and evaluate the data set. Table VII lists the comparison result of average P, R, F1 for three partition methods. Therefore, the latter experiments are based on partition 3 to train and evaluate the data set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) In what circumstances can our algorithm achieve the best performance</head><p>In order to confirm in what circumstances can our algorithm achieve the best performance, we analyze the influence of four factors (the dimension of the feature, the size of the data set, In the case of selecting 100 features: Damped Incremental Statistics algorithm is used to sample the features. We set the value of λ to [5, 3, 1, 0.1, 0.01] to get the 100-dimensional feature. After using DBN, our features are reduced to 30 dimensions. We use Algorithm 1 to get the optimal values for gp and p. For further detailed analysis, we use the growth step size method to observe the effect of p and gp on the anomaly detection effect. In the iteration we found that p is 0.8 or gp is 0.65, and the abnormality detection result is better. The method of controlling variables is used to determine the values of gp and p. As shown in Fig. <ref type="figure">4</ref>, we use 300,000 packets and set the value of gp to 0.65 to observe the effect of p-value on the detection effect in the HELAD algorithm. It can be seen that the Precision, Recall, and F1 value all reach the highest when the p value is 0.8.</p><p>In order to determine the effect of the value of gp on the detection effect in the HELAD algorithm, we conduct the further experiment. Fig. <ref type="figure">5</ref> shows that the F1 value reaches the highest when the gp value is 0.65.</p><p>After determining the values of gp and p, we verify the effect of the data set size on the detection of the HELAD algorithm. As can be seen from Table <ref type="table" target="#tab_11">VIII</ref>, when the over all data set size is 300,000, the average Precision is 0.871, the average recall is 0.864, and the average of the F1 value is 0.861. When the data set is smaller, the detection effect is even worse.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In the case of selecting 200 features:</head><p>In order to verify the effect of the number of features on the detection of the HELAD algorithm, we set 200 features for each sample, and the value of λ is [10,5,3,2,1.5,1,0.5,0.2,0.1,0.01]. After using DBN, our features are reduced to 50 dimensions.</p><p>When the feature is increased to 200 dimensions, the computing power of a single node is limited. So we selected 10,000, 30,000, and 100,000 data packets for analysis. Fig. <ref type="figure">6</ref> shows the performance of HELAD anomaly detection model under the conditions of 200 features with the varying p-value. As shown in Fig. <ref type="figure">6</ref>, from the comparison of each subgraph, as the data set expands, the effect of anomaly detection is getting better and better. This is because anomaly detection algorithms are based on machine learning algorithms, which depend on the data set. As shown in Fig. <ref type="figure">6</ref>(a), when the data packet is 10,000, the value of the Precision is 0.7, and the p value has not yet played a role. As shown in Fig. <ref type="figure">6</ref>(b), in the case where the data packet is 30,000, the Precision, Recall and F1 increase as the p value increases. This shows that the effects of Autoencoder come into play. So for the selection of p value, we use the evaluation index of F1 value. When the data packet shown in Fig. <ref type="figure">6(c</ref>) is 100,000, the detection effect is best when the p value is 0.8. This illustrates that as the further expansion of the data set, the role of LSTM begins to emerge. It also reveals that LSTM is a deep learning technology that requires a larger data set if it needs to work better. Fig. <ref type="figure" target="#fig_7">7</ref> reveals the performance of HELAD anomaly detection model under the conditions of 200 features with the varying gp value. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, from the comparison of each subgraph, the Precision increases as the gp value increases. However, the Recall is reversed because there are many false negatives due to an increase in the threshold. So for the selection of gp value, we use the evaluation index of F1 value. When the value gp is 0.65, the comprehensive detection effect is the best. The larger the data set, the better the detection effect, which is characteristic of the machine learning algorithm itself. Table <ref type="table" target="#tab_12">IX</ref> shows the effect of different data sets on the experiment under the 200-dimensional feature.</p><p>Based on the above experiments, we can draw the following conclusions. In the case of insufficient data (data size 100000), our model can improve the precision by increasing the feature dimension. In the case where the feature dimension is specified (100-dimensional feature), increasing the dataset size can improve the precision. We can see that the size of the data set has a greater impact than the feature. Therefore, if our model is to achieve good performance on the MAWILab dataset, the feature requires 200 dimensions, the dataset size is 300,000, the value of gp is 0.65, and the value of p is 0.8. If we change the network environment (dataset source), our model will also set the values of these variables according to the network environment to achieve the best detection effect.</p><p>2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Whether ensemble learning and the re-learning function are effective</head><p>In order to verify the effectiveness of ensemble learning. We use MAWILab's 300,000 data on June 1, 2018 for training and another 600,000 data for testing. Through training, we get three models: our model, our model without LSTM, and our model without RMSE. The experimental results are shown in Table <ref type="table" target="#tab_13">X</ref>. We can see that our ensemble model is better than a single model for anomaly detection.   For verifying the importance of re-learning, we conduct two sets of experiments on MAWILab, using data from the first three days of April, June, July, and September, and intercepting 300,000 data per day. The first set of experiments is a function of re-learning. We use the data of the 1st of each month for training, and the data of the 2nd and 3rd of the month is used for testing. The second set of experiments do not have the function of re-learning, that is to say, the training is carried out with the data of April 1st, and the latter model is not retrained, and the data of the 2nd and 3rd of the four months is used for testing.</p><p>In order to evaluate the detection effect of our HELAD model when the network flow characteristics change drastically, we add some analytical experiments. We analyze the experimental data for 6 days and 0402 stands for the date of collection of the MAWILab data set is April 2. For a better comparison, we count two sets of data. As shown in Table <ref type="table" target="#tab_14">XI</ref>, the first group is the #select data set. This is our experimental data, which contains 300,000 network packets. The second group is the #all data set, which is a whole day of data. It is about 90 million packets a day, and the total amount of data is different every day. Previous represents the number of IP addresses that have appeared in the previous date. New expresses a completely new IP address. It is important to note that each IP address corresponds to multiple network packets. Table <ref type="table" target="#tab_14">XI</ref> shows changes in the IP address of data sets on different dates, indicating that the network IP is almost always changing. Fig. <ref type="figure" target="#fig_8">8</ref> illustrates that traffic volumes vary widely from day to day (during the specified time period). The experimental results are shown in Fig. <ref type="figure">9</ref>. The model with re-learning is better than the model with no re-learning. It also shows that our re-learning model performs well in the case of severe network turbulence. Table <ref type="table" target="#tab_15">XII</ref> shows that FPR does not change much before and after re-learning. This shows that FPR is mainly related to the algorithm itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) How does our algorithm compare to the performance of other state-of-the-art algorithms on different data sets</head><p>we use Isolation Forests (IF) <ref type="bibr" target="#b47">[48]</ref> and Gaussian Mixture Models (GMM) <ref type="bibr" target="#b48">[49]</ref>. IF is an ensemble based method of outlier detection, and GMM is a statistical method based on the expectation maximization algorithm. Then we use support vector machine (SVM) from <ref type="bibr" target="#b49">[50]</ref>, sparse autoencoder finetuned neural network (SAE) from <ref type="bibr" target="#b50">[51]</ref>, restricted boltzmann machine fine-tuned neural network (RBM) from <ref type="bibr" target="#b51">[52]</ref> and kitsune from <ref type="bibr" target="#b26">[27]</ref>.     . This leads to the fact that the comprehensive evaluation index F1 value is not as good as SVM. Both SAE and IF performed better in the IDS 2017 data set. This also shows that SAE and IF perform differently for different environments (data sets). Next is the GMM algorithm, and the overall performance is not good. The Kitsune algorithm still performs very well. Our HELAD algorithm uses a contrasting approach. The first group (gp=0.65, p=0.80) uses the gp value and the p value trained directly on the MAWILab data set, and the second group (gp=0.60, p=0.85) uses the newly trained gp value and p value according to the IDS 2017 data set. Experiments show that the comprehensive performance of the second group is better than other algorithms, and the comprehensive evaluation index F1 can reach 0.985. Comparative experiments verify the environmental adaptability of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) How does our algorithm compare to the state-of-theart homogeneous ensemble learning algorithms</head><p>Next, we compare our algorithm with the anomaly detection algorithm for homogeneous ensemble learning. GradientTree-Boost <ref type="bibr" target="#b55">[56]</ref> integrates the gradient tree through the boost ensemble method. BaggingCR <ref type="bibr" target="#b56">[57]</ref> integrates the conjunctive rule (CR) classifier through the bagging ensemble method. At the same time, we have added bagging integration of various basic classifiers SVM, k-NearestNeighbor (KNN), Multilayer Perceptron (MLP). Then, considering the advantages of Adaboost, we also add the Adaboost ensemble method using Logistic Regression as the base classifier. For the sake of fairness, we all use the same features for training. The experimental data set is derived from the reselected 300,000 data sets from IDS2017. This data set is called IDS2017other. The purpose of re-selecting the data set is to make the experimental data more representative. Table XVI shows that our HELAD algorithm has higher F1 value and AUC, as well as lower FPR.</p><p>In order to measure the performance difference between the classification algorithms we use, Quade test and Quade post hoc test are used to weight how much our model and other state of the arts algorithms deviate from each other <ref type="bibr" target="#b61">[62]</ref>. These tests can find differences between classifiers <ref type="bibr" target="#b62">[63]</ref>. The F value is an intermediate process for calculating the p value, and the larger the F value, the smaller the p value. Based on the knowledge of hypothesis testing, we make two hypotheses HA and HB. HA means that there are no performance differences among the classifiers. HB indicates that there are performance differences among the classifiers. Table XVII reveals the result of Quade test with all classifiers, indicating that the performance of the classifiers is significantly different (p &lt; 0.05) in terms of Precision, Recall, F1-score, FPR, AUC. If the result of Quade test is highly significant, the null hypothesis HA (the performance of all classifiers is similar) could be rejected and hypothesis HB should be accepted. Table <ref type="table" target="#tab_21">XVIII</ref> shows the results of Quade post hoc test in terms of AUC, indicating that the difference between BaggingCR and HELAD is most significant. Because the lower the p-value, the greater the relative significant. This significant suggests that if our model continues to ensemble BaggingCR, our model will perform better statistically. This provides an expandable space for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>In this section, we will discuss some of the details of our models and experimental methods.</p><p>Q1: Whether e y0 is needed in the final discriminant formula?</p><p>The first question, in HELAD model, the abnormal score calculated by Autoencoder and the original feature are stitched into a new vector x L , which is used as an input to the LSTM. LSTM training uses the anomaly score and is affected by this abnormal score when forecasting. However, our discriminant formula also considers the abnormal score calculated by Autoencoder and the predicted value of LSTM. In other words, we want to determine whether e y0 is needed in the final discriminant formula. We do the following analysis. We refer to the anomaly score as the LSTM input as the ASA (anomaly score A) and the ASB (anomaly score B) as the    Q2: Whether the hold-out algorithm we use is overfitting, or is there a better evaluation method?</p><p>For second question, the hold-out data set partitioning method we use is consistent with the processing of time series data. Moreover, the same experimental method we use in all experiments is fair. To further demonstrate the effectiveness of our algorithm, we have added a set of experiments, which use Forward Chaining Cross-Validation <ref type="bibr" target="#b59">[60]</ref> based on IDS2017other. Forward Chaining Cross-Validation uses the front part of the data set as the training set and the latter part as the test set. And there are multiple split points, which can achieve the purpose of multiple training based on different partition. Finally, the errors on each partition are averaged to calculate a robust estimate of the model error. We define it as errorseeking method. We use the method which finds the average of the various indicators to replace the error-seeking method in the original paper. The two methods are equivalent. We set up three sets of data, and the number of training sets and test sets are: A (150,000 150,000), B (150,000 150,000), C (450,000 150,000). Then we average the experimental results of three groups. Table <ref type="table" target="#tab_23">XX</ref> shows that there is a small drop in Precision, Recall, F1-score, but our algorithm is still performing well. FPR is still the lowest. For the third question, we assume that Autoencoder gains the profile of normal network traffic as one of the base learners, and provides learned RMSE as the label needed to train the LSTM, while LSTM works well for continuous attacks. In this section we prove this hypothesis with experimental results.</p><p>We re-extract 400,000 network packets from IDS2017 for experiments to analyze sudden attacks and continuous attacks. We are using Wednesday's data, which contains different types of DoS attacks (DoS slowloris, DoS Slowhttptest, DoS Hulk, DoS GoldenEye). The time period of the selected sudden attack is 25,000-75,000, that is, the period during which the number of attacks gradually increases. The duration of the continuous attack is 100,000-150,000, and the attack frequency is very high. The red dots in Fig. <ref type="figure" target="#fig_9">10</ref> represent abnormal packets, and the blue dots represent normal packets. Fig. <ref type="figure" target="#fig_9">10</ref> shows selection of corresponding areas of sudden attacks and continuous attacks. The X axis represents the network packet sequence number, and the Y axis represents the RMSE value of each network packet. The data from 25,000 to 75,000 between the two vertical solid lines represent sudden attacks. The data from 100,000 to 150,000 between the two vertical dotted lines represent continuous attacks. A comparison of Table <ref type="table" target="#tab_25">XXI</ref> and Table <ref type="table" target="#tab_26">XXII</ref> shows that LSTM does perform better in continuous attack detection while Autoencoder performs better in sudden attack. Finally, our HELAD model works best and illustrates the need for our ensemble approach.   VII. CONCLUSION The network environment is increasingly complex, and as such, the form of attack is ever-changing. Many of the existing machine learning related anomaly detection models published previously are evaluated using data such as KDD, leading to the emergence of this problem that it is not practical in real-life environments. By introducing the idea of organic integration of various deep learning techniques, the HELAD model can better combine LSTM classifier and Autoencoder classifier. This provides a new idea for the application of heterogeneous ensemble learning in the field of anomaly detection. The advantage is to make it adaptable in the real environment. Then, latest raw packet data is used in our experiments, which provides a verification idea for the proof of the future anomaly detection algorithm. Next, in order to prevent the degradation of the anomaly detection model we design, we introduce a module for relearning. Experimental results compare Gaussian distribution, SVM, homogeneous ensemble learning and the latest Kitsune algorithm, which shows the superiority of our algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The process diagram of HELAD model.</figDesc><graphic coords="6,79.69,85.56,411.99,251.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we introduce the expression of each sub-model in the HELAD algorithm. The HELAD algorithm has four parts: feature dimension reduction, abnormal score generation, abnormal score prediction, and abnormal detection result combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Prediction effect of LSTM on mirai dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&amp;Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Relationship between internal variables of LSTM unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig.4: Performance of HELAD model under the conditions of 100 features, 300,000 packets. When the gp value is fixed, observe the effect of p value on the detection effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Performance of HELAD anomaly detection model under the conditions of 200 features. When the p value is fixed, observe the effect of gp value on the detection effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The traffic volumes statistics corresponding to relearning experiment under MAWILab dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Selection of corresponding areas of sudden attacks and continuous attacks.</figDesc><graphic coords="17,22.23,316.73,281.42,158.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Element Definitions of HELAD Model Ai ag ∈ Bi|tj , st k , ag k &gt;, The original i th feature, the feature is expressed as a triple.</figDesc><table><row><cell>Expression</cell><cell>Meaning</cell></row><row><cell>Si</cell><cell>Si represents a sequence, the elements of this sequence can be the size of the packet, the jitter value, etc.</cell></row><row><cell>d λ (t)</cell><cell>decay function, λ &gt; 0 is the decay factor, and t is the time elapsed since the last observation from stream Si</cell></row><row><cell>Statistics ST Aggregated AG Operation OP op1</cell><cell>Statistical characteristics of attributes of network traffic ST = {A1, A2, A3, A4} The way network traffic is aggregated AG = {B1, B2, B3, B4} The operation of the formula on the specified property OP = {opi ∈ OP, 1 ≤ i ≤ 4| &lt; Ai, Bi &gt;} op1 represents the bandwidth of the outbound traffic, and statistical feature st1 operates on the aggregation ag1.</cell></row><row><cell>op2</cell><cell>op2 represents Outbound and inbound traffic bandwidth, and statistical feature st2 operates on the aggregation ag2.</cell></row><row><cell>op3</cell><cell>op3 represents Outbound traffic packet rate, and statistical feature st3 operates on the aggregation ag3.</cell></row><row><cell>op4</cell><cell>op4 represents Inter-packet delay for outbound traffic, and statistical feature st4 operates on the aggregation ag4.</cell></row><row><cell cols="2">A1 A2 A3 A4 B1 B2 B3 B4 λ  *  fi fi =&lt; λi ∈ λ  f1 {µi, σi} { si, sj , Rsisj , Covs i s j , ps i s j } {wi} {wi, µi, σi} {SrcIP, Channel, Socket} {Channel, Socket} {SrcIP, Channel, Socket} {Channel} {λ1, λ2, • • • , λn 1 }, λi represents the i th decay factor. f1=&lt; λ1, µi, SrcIP &gt; Statistic µi operate on attribute SrcIP as a feature. -</cell></row></table><note><p>* , st ∈</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Incremental statistics index for IS i,λ in<ref type="bibr" target="#b26">[27]</ref>.</figDesc><table><row><cell>Statistic</cell><cell>Notation</cell><cell>Calculation</cell></row><row><cell>Weight</cell><cell>w</cell><cell>w</cell></row><row><cell>Mean</cell><cell>µs i</cell><cell>LS/W</cell></row><row><cell>Standard deviation Magnitude</cell><cell>σs i Si, Sj</cell><cell>SS/W -(LS/W ) 2 µs i 2 + µs j 2</cell></row><row><cell>Radius</cell><cell>RS i S j</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 1</head><label>1</label><figDesc>Searching p and gp by simulated annealing algorithm Input:Data; //Testing data HELAD; //The parameters are constant {T 0 , r, T f , L}; //The hyper-parameter set. T 0 is the initial value of temperature. r is the coefficient for reducing temperature. T f is the lowest temperature. L is the amount of searching for every value of temperature.</figDesc><table><row><cell>Output:</cell></row><row><cell>p;//Coordinate the weight between the predicted and de-</cell></row><row><cell>tected values.</cell></row><row><cell>gp;//Overall threshold, because it is an exponential func-</cell></row><row><cell>tion, only the upper limit</cell></row><row><cell>//Initial phase</cell></row><row><cell>1: T = T 0 ; 2: p 0 = rand();//The function, rand(), returns a random floating number in (0, 1)</cell></row><row><cell>3: gp 0 = rand(); 4: x=(p 0 , gp 0 );</cell></row></table><note><p>5: f = F 1score computed with Data, HELAD, x; //F1-score is the metric (mentioned in Section V-A) //Searching phase 6: while T &gt; T f do 7:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Example of The Anomalies inJune 3, 2018    </figDesc><table><row><cell>Taxonomy</cell><cell>Heuristic</cell><cell>Original Label</cell><cell>Detectors</cell><cell>Final Label</cell></row><row><cell>network scan SY N</cell><cell>SYN attack</cell><cell>anomalous</cell><cell>Hough, P CA</cell><cell>Abnormal</cell></row><row><cell>network scan SY N</cell><cell>SYN attack</cell><cell>anomalous</cell><cell>Hough, Gamma, P CA</cell><cell>Abnormal</cell></row><row><cell>network scan T CP RST ACK response</cell><cell>RST attack</cell><cell>anomalous</cell><cell>Hough, P CA</cell><cell>Abnormal</cell></row><row><cell>small network scan SY N</cell><cell>SYN attack</cell><cell>anomalous</cell><cell>Gamma</cell><cell>Abnormal</cell></row><row><cell>network scan SY N</cell><cell>SYN attack</cell><cell>anomalous</cell><cell>Hough, KL, PCA</cell><cell>Abnormal</cell></row><row><cell>network scan ICM P ecrq</cell><cell>Ping flood</cell><cell>suspicious</cell><cell>Hough</cell><cell>Abnormal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>The software and hardware configurations</figDesc><table><row><cell>Resource Type</cell><cell>Configuration</cell></row><row><cell></cell><cell>ubuntu 14.04,</cell></row><row><cell></cell><cell>conda 4.4.10,</cell></row><row><cell></cell><cell>python 3.5.0 ,</cell></row><row><cell></cell><cell>numpy 1.15.0 ,</cell></row><row><cell>Software environment</cell><cell>cython 0.28.5, scapy 2.4.0,</cell></row><row><cell></cell><cell>scipy 1.1.0,</cell></row><row><cell></cell><cell>keras 2.2.2,</cell></row><row><cell></cell><cell>tensorflow 1.9.0</cell></row><row><cell>CPU</cell><cell>i7-5500 2.40GHz</cell></row><row><cell>Memory</cell><cell>32G</cell></row><row><cell>Number of server</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Number of labels in the MAWILab data set</figDesc><table><row><cell>DataSet</cell><cell>Abnormal</cell><cell>Normal</cell></row><row><cell>300,000</cell><cell>125,374</cell><cell>174,626</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="4">: Number of labels in training set and testing set</cell></row><row><cell></cell><cell cols="2">(number/portion)</cell><cell></cell></row><row><cell>partition method</cell><cell>Tag</cell><cell>Training set</cell><cell>Testing set</cell></row><row><cell></cell><cell>Normal</cell><cell>115,253</cell><cell>59,373</cell></row><row><cell>partition 1</cell><cell>Abnormal (Sum)</cell><cell>82,747 198,000/66%</cell><cell>42,627 102,000/33%</cell></row><row><cell></cell><cell>Normal</cell><cell>130,969</cell><cell>43,657</cell></row><row><cell>partition 2</cell><cell>Abnormal (Sum)</cell><cell>94,031 225,000/75%</cell><cell>31,343 75,000/25%</cell></row><row><cell></cell><cell>Normal</cell><cell>152,798</cell><cell>21,828</cell></row><row><cell>partition 3</cell><cell>Abnormal (Sum)</cell><cell>109,702 262,500/87.5%</cell><cell>15,672 37,500/12.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison</figDesc><table><row><cell></cell><cell cols="3">of average P, R, F1 for different</cell></row><row><cell cols="4">partition method (200 features, 300,000 packets, gp=0.65,</cell></row><row><cell></cell><cell>p=0.8)</cell><cell></cell><cell></cell></row><row><cell>partition method</cell><cell>Precision</cell><cell>Average Recall</cell><cell>F1-Measure</cell></row><row><cell>partition 1</cell><cell>0.793</cell><cell>0.837</cell><cell>0.815</cell></row><row><cell>partition 2</cell><cell>0.821</cell><cell>0.860</cell><cell>0.840</cell></row><row><cell>partition 3</cell><cell>0.901</cell><cell>0.880</cell><cell>0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII :</head><label>VIII</label><figDesc>Evaluation indicators under different dataset sizes (100 features, gp=0.65, p=0.8).</figDesc><table><row><cell cols="2">Data set size Precision</cell><cell cols="2">Recall F1-score</cell></row><row><cell>30000</cell><cell>0.761</cell><cell>0.761</cell><cell>0.757</cell></row><row><cell>100000</cell><cell>0.851</cell><cell>0.855</cell><cell>0.853</cell></row><row><cell>300000</cell><cell>0.871</cell><cell>0.864</cell><cell>0.861</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE IX :</head><label>IX</label><figDesc>Evaluation indicators under different dataset sizes (200 features, gp=0.65, p=0.8).</figDesc><table><row><cell cols="2">Data set size Precision</cell><cell cols="2">Recall F1-score</cell></row><row><cell>30000</cell><cell>0.781</cell><cell>0.779</cell><cell>0.779</cell></row><row><cell>100000</cell><cell>0.873</cell><cell>0.879</cell><cell>0.875</cell></row><row><cell>300000</cell><cell>0.901</cell><cell>0.880</cell><cell>0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE X :</head><label>X</label><figDesc>Evaluation indicators for ensemble learning under MAWILab dataset (200 features, 300,000 packets, gp=0.65, p=0.8).</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1-score</cell></row><row><cell>RMSE</cell><cell>0.812</cell><cell>0.772</cell><cell>0.791</cell></row><row><cell>LSTM</cell><cell>0.782</cell><cell>0.663</cell><cell>0.717</cell></row><row><cell>RMSE+LSTM</cell><cell>0.901</cell><cell>0.880</cell><cell>0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI :</head><label>XI</label><figDesc>Changes in the IP address of datasets on different dates.</figDesc><table><row><cell>Date</cell><cell cols="4">Previous(#select) New(#select) Previous(#all) New(#all)</cell></row><row><cell>0403</cell><cell>3</cell><cell>66087</cell><cell>105414</cell><cell>14703225</cell></row><row><cell>0702</cell><cell>6</cell><cell>59543</cell><cell>214780</cell><cell>15492070</cell></row><row><cell>0703</cell><cell>124</cell><cell>59524</cell><cell>411137</cell><cell>15669049</cell></row><row><cell>0902</cell><cell>10</cell><cell>118285</cell><cell>474783</cell><cell>15788790</cell></row><row><cell>0903</cell><cell>16</cell><cell>94018</cell><cell>601222</cell><cell>15575598</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XII :</head><label>XII</label><figDesc>The FPR before and after relearning.</figDesc><table><row><cell>Date</cell><cell cols="2">Relearning-FPR Non-relearning-FPR</cell></row><row><cell>0402</cell><cell>0.072</cell><cell>0.072</cell></row><row><cell>0403</cell><cell>0.100</cell><cell>0.100</cell></row><row><cell>0702</cell><cell>0.100</cell><cell>0.104</cell></row><row><cell>0703</cell><cell>0.135</cell><cell>0.118</cell></row><row><cell>0902</cell><cell>0.162</cell><cell>0.197</cell></row><row><cell>0903</cell><cell>0.176</cell><cell>0.181</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XIII :</head><label>XIII</label><figDesc>Evaluation indicators under MAWILab dataset (200 features, 300,000 packets, gp=0.65, p=0.8).</figDesc><table><row><cell cols="2">Compared methods Precision</cell><cell cols="2">Recall F1-score</cell></row><row><cell>SVM</cell><cell>0.598</cell><cell>0.651</cell><cell>0.623</cell></row><row><cell>RBM</cell><cell>0.708</cell><cell>0.714</cell><cell>0.711</cell></row><row><cell>SAE</cell><cell>0.759</cell><cell>0.711</cell><cell>0.734</cell></row><row><cell>IF</cell><cell>0.756</cell><cell>0.724</cell><cell>0.739</cell></row><row><cell>GMM</cell><cell>0.645</cell><cell>0.668</cell><cell>0.657</cell></row><row><cell>kitsune</cell><cell>0.871</cell><cell>0.870</cell><cell>0.870</cell></row><row><cell>HELAD</cell><cell>0.901</cell><cell>0.880</cell><cell>0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XIV :</head><label>XIV</label><figDesc>Number of labels in the IDS 2017 data set Performance of HELAD anomaly detection model under the conditions of 200 features. When the gp value is fixed, observe the effect of p value on the detection effect.</figDesc><table><row><cell cols="2">gp=0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">gp=0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">gp=0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell></row><row><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-score</cell></row><row><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95</cell><cell cols="8">0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95</cell><cell cols="8">0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">(a) 10,000 packets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) 30,000 packets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) 100,000 packets</cell><cell></cell><cell></cell></row><row><cell>0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 1.00 1.05 Fig. 6: 0.50 p=0.8 0.95</cell><cell>0.55</cell><cell>0.60</cell><cell>0.65</cell><cell>0.70</cell><cell>0.75</cell><cell>0.80 Precision Recall F1-score</cell><cell>0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 1.00 1.05 0.95</cell><cell>0.50 p=0.8</cell><cell>0.55</cell><cell>0.60</cell><cell>0.65</cell><cell>0.70</cell><cell>0.75</cell><cell>0.80 Precision Recall F1-score</cell><cell>0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.05 1.10 1.00</cell><cell>0.50 p=0.8</cell><cell>0.55</cell><cell>0.60</cell><cell>0.65</cell><cell>0.70</cell><cell>0.75</cell><cell>0.80 Precision Recall F1-score</cell></row><row><cell></cell><cell></cell><cell></cell><cell>gp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>gp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>gp</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">(a) 10,000 packets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) 30,000 packets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) 100,000 packets</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DataSet</cell><cell></cell><cell cols="2">Abnormal</cell><cell cols="2">Normal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>300,000</cell><cell></cell><cell cols="2">226,618</cell><cell cols="2">73,382</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XV :</head><label>XV</label><figDesc>Evaluation indicators under IDS 2017 dataset (200 features, 300,000 packets). Evaluation indicators for relearning under MAWILab dataset (200 features, 300,000 packets, gp=0.65, p=0.8). This data set comes from different months. 0402 represents the detection effect on April 2. machine learning based approaches. To verify the robustness of the algorithm, we experiment on IDS 2017 data set. Our experiments on the IDS 2017 data set are based on partition 3. Table XIV shows the number of labels in the IDS 2017 data set. The experimental results are shown in Table XV. SVM's Precision and Recall are not high. The comprehensive evaluation index F1 value is only 0.768. The Precision of RBM can reach 1.000, but the corresponding Recall is very low</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relearning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relearning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-relearning</cell><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-relearning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relearning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell>Non-relearning</cell></row><row><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.80 0.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell>0.78 0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-score</cell><cell>0.80 0.82</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.78</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0402</cell><cell>0403</cell><cell>0702</cell><cell>0703</cell><cell>0902</cell><cell>0903</cell><cell></cell><cell>0402</cell><cell>0403</cell><cell>0702</cell><cell>0703</cell><cell>0902</cell><cell>0903</cell><cell>0402</cell><cell>0403</cell><cell>0702</cell><cell>0703</cell><cell>0902</cell><cell>0903</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Dates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dates</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Recall</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) F1-score</cell></row><row><cell cols="2">Fig. 9:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">working out the Precision, Recall and F1-score of these</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">models. We can see that the Precision of the SVM algorithm</cell></row><row><cell></cell><cell cols="4">Compared methods</cell><cell cols="3">Precision Recall</cell><cell>F1-score</cell><cell></cell><cell></cell><cell cols="8">is only 0.598. The Precision of the RBM is slightly better</cell></row><row><cell cols="5">SVM RBM SAE IF GMM kitsune HELAD (gp=0.65, p=0.80) HELAD (gp=0.60, p=0.85)</cell><cell></cell><cell>0.797 1.000 1.000 0.997 0.786 0.940 0.941 0.988</cell><cell>0.444 0.433 0.961 0.971 0.694 0.998 0.998 0.982</cell><cell>0.768 0.604 0.980 0.983 0.737 0.968 0.969 0.985</cell><cell></cell><cell></cell><cell cols="8">and can reach 0.708. Then there are SAE and IF, and the detection results are very similar. The detection effect of GMM is also not ideal. Kitsune still perform well in the 300,000 data packet, with a Precision of 0.871. Our HELAD algorithm has improved detection effect compared to the kitsune algorithm. The average performances of different methods are displayed in Table XIII. As we can see from the results, our HELAD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">model achieves significantly better results compared with other</cell></row><row><cell cols="10">We have compared the effectiveness of these methods by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE XVI :</head><label>XVI</label><figDesc>Performance comparison between HELAD and other ensemble learning models.</figDesc><table><row><cell>Compared methods</cell><cell>Precision</cell><cell>Recall</cell><cell cols="2">F1-score False Positive Rate</cell><cell>AUC</cell></row><row><cell>HELAD (gp=0.60, p=0.85)</cell><cell>0.9958</cell><cell>0.9958</cell><cell>0.9958</cell><cell>0.0215</cell><cell>0.9986</cell></row><row><cell>GradientTreeBoost</cell><cell>0.9573</cell><cell>0.9663</cell><cell>0.9618</cell><cell>0.0610</cell><cell>0.9657</cell></row><row><cell>BaggingCR</cell><cell>0.9340</cell><cell>0.9310</cell><cell>0.9320</cell><cell>0.0610</cell><cell>0.9370</cell></row><row><cell>AdaboostLogistic</cell><cell>0.9680</cell><cell>0.9199</cell><cell>0.9433</cell><cell>0.0432</cell><cell>0.9391</cell></row><row><cell>BaggingSVM</cell><cell>0.9646</cell><cell>0.9213</cell><cell>0.9425</cell><cell>0.0480</cell><cell>0.9591</cell></row><row><cell>BaggingKNN</cell><cell>0.9685</cell><cell>0.9495</cell><cell>0.9589</cell><cell>0.0438</cell><cell>0.9662</cell></row><row><cell>BaggingMLP</cell><cell>0.9649</cell><cell>0.9201</cell><cell>0.9419</cell><cell>0.0474</cell><cell>0.9539</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE XVII :</head><label>XVII</label><figDesc>The result of Quade test with all ensemble learning models.</figDesc><table><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell cols="2">F1-score False Positive Rate</cell><cell>AUC</cell></row><row><cell>F</cell><cell>17.6667</cell><cell>5.3636</cell><cell>6.7778</cell><cell>7.7500</cell><cell>7.7500</cell></row><row><cell>p value</cell><cell>0.0014</cell><cell>0.0302</cell><cell>0.0174</cell><cell>0.0125</cell><cell>0.0125</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE XVIII :</head><label>XVIII</label><figDesc>The p value of post hoc Quade test for AUC. 's anomaly score. The ASA is designed to train LSTM so that LSTM can predict abnormal scores based on past data. The ASB (that is y 0 in pe y0 + (1p)e y1+y2+y3 ) is to calculate the abnormal score of latest network packet and is one of the judgment conditions for ensemble learning. The functions of the two abnormal scores are different, so there is no redundancy. TableXIXshows the necessity of e y0 , because our HELAD model works better with considering ASB.</figDesc><table><row><cell></cell><cell>HELAD</cell><cell cols="2">GradientTreeBoost BaggingCR</cell><cell cols="3">AdaboostLogistic BaggingSVM BaggingKNN</cell></row><row><cell>GradientTreeBoost</cell><cell>0.2666</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BaggingCR</cell><cell>0.0015</cell><cell>0.0052</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AdaboostLogistic</cell><cell>0.0037</cell><cell>0.0151</cell><cell>0.3938</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BaggingSVM</cell><cell>0.0104</cell><cell>0.0498</cell><cell>0.1158</cell><cell>0.3938</cell><cell>-</cell><cell>-</cell></row><row><cell>BaggingKNN</cell><cell>0.1767</cell><cell>0.7698</cell><cell>0.0073</cell><cell>0.0222</cell><cell>0.0758</cell><cell>-</cell></row><row><cell>BaggingMLP</cell><cell>0.0330</cell><cell>0.1767</cell><cell>0.0330</cell><cell>0.1158</cell><cell>0.3938</cell><cell>0.2666</cell></row></table><note><p>discriminant</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE XIX :</head><label>XIX</label><figDesc>Whether e y0 is needed in the final discriminant formula (200 features, gp=0.65, p=0.8, IDS 2017 dataset-other).</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1-score</cell></row><row><cell>HELAD (with e y 0 )</cell><cell>0.996</cell><cell>0.996</cell><cell>0.996</cell></row><row><cell>HELAD (without e y 0 )</cell><cell>0.871</cell><cell>0.913</cell><cell>0.891</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE XX :</head><label>XX</label><figDesc>Verify the problem of overfitting (200 features, gp=0.65, p=0.8, IDS 2017 dataset-other).</figDesc><table><row><cell>Compared methods</cell><cell cols="2">Precision Recall</cell><cell>F1-score</cell><cell>FPR</cell></row><row><cell>HELAD</cell><cell>0.971</cell><cell>0.972</cell><cell>0.971</cell><cell>0.020</cell></row><row><cell>SVM</cell><cell>0.814</cell><cell>0.696</cell><cell>0.728</cell><cell>0.132</cell></row><row><cell>IF</cell><cell>0.907</cell><cell>0.610</cell><cell>0.678</cell><cell>0.234</cell></row><row><cell>RBM</cell><cell>0.874</cell><cell>0.875</cell><cell>0.869</cell><cell>0.411</cell></row><row><cell>SAE</cell><cell>0.987</cell><cell>0.887</cell><cell>0.933</cell><cell>0.091</cell></row><row><cell>GMM</cell><cell>0.800</cell><cell>0.682</cell><cell>0.709</cell><cell>0.154</cell></row><row><cell>Q3: Whether</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>LSTM performs better in continuous attack detection while Autoencoder performs better in sudden attack?</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE XXI :</head><label>XXI</label><figDesc>The experimental indicator of sudden attack (200 features, gp=0.65, p=0.8).</figDesc><table><row><cell>Compared methods</cell><cell>Precision</cell><cell cols="2">Recall F1-score</cell><cell>FPR</cell></row><row><cell>LSTM</cell><cell>0.975</cell><cell>0.952</cell><cell>0.964</cell><cell>0.033</cell></row><row><cell>Autoencoder</cell><cell>0.982</cell><cell>0.982</cell><cell>0.982</cell><cell>0.025</cell></row><row><cell>LSTM+Autoencoder</cell><cell>0.986</cell><cell>0.993</cell><cell>0.989</cell><cell>0.019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE XXII :</head><label>XXII</label><figDesc>The experimental indicator of continuous attack (200 features, gp=0.65, p=0.8).</figDesc><table><row><cell>Compared methods</cell><cell>Precision</cell><cell cols="2">Recall F1-score</cell><cell>FPR</cell></row><row><cell>Autoencoder</cell><cell>0.994</cell><cell>0.979</cell><cell>0.986</cell><cell>0.075</cell></row><row><cell>LSTM</cell><cell>0.996</cell><cell>0.996</cell><cell>0.996</cell><cell>0.043</cell></row><row><cell>LSTM+Autoencoder</cell><cell>0.998</cell><cell>0.997</cell><cell>0.998</cell><cell>0.020</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/ymirsky/KitNET-py/blob/master/dataset.zip.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/cdogemaru/CPIP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.fukuda-lab.org/mawilab/index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.unb.ca/cic/datasets/ids-2017.html.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by the National Key Research and Development Program of China under Grant No. 2018YFB1800205. And we also thank for the support of National Engineering Lab for Next Generation Internet Technologies (No. NGIT2019004).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DECLARATION OF COMPETING INTEREST</head><p>The authors declare that they have no conflicts of interest to this work. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Intrusion-Detection Model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="1987-02">Feb. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Outside the Closed World: On Using Machine Learning for Network Intrusion Detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Paxson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal volume anomaly detection and isolation in large-scale IP networks using coarse-grained measurements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fillatre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1750" to="1766" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evolutionary Design of Intrusion Detection Programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martinvide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Network Security</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varadharajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tupakula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Pilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="686" to="728" />
			<date type="published" when="2019">2019</date>
			<pubPlace>Firstquarter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Snort: Open Source Network Intrusion Prevention</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Carr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shepard Interpolation Neural Networks with K-Means: A Shallow Learning Method for Time Series Classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Deep and Wide: A Spectral Method for Learning Deep Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2303" to="2308" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GPU-accelerated parallel algorithms for linear rankSVM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4141" to="4171" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hybrid GPU cluster and volunteer computing platform for scalable deep learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kijsipongse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piyatumrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>-Ruekolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3236" to="3263" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of intrusion detection systems based on ensemble and hybrid classifiers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Aburomman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M B I</forename><surname>Reaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="135" to="152" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble based Collaborative and Distributed Intrusion Detection Systems: A Survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sabatino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved competitive learning neural networks for network intrusion and fraud detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="145" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mamun Bin Ibne Reaz. A novel weighted support vector machines multiclass classifier based on differential evolution for intrusion detection systems</title>
		<author>
			<persName><forename type="first">Abdulla</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aburomman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Security Data Collection and Data Analytics in the Internet: A Survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="586" to="618" />
			<date type="published" when="2019">2019</date>
			<pubPlace>Firstquarter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for anomaly detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="130" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Nonparametric Adaptive CUSUM Method and Its Application in Network Anomaly Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advancements in Computing Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="288" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sketch-based change detection: methods, evaluation, and applications</title>
		<author>
			<persName><forename type="first">Balachander</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement (IMC &apos;03)</title>
		<meeting>the 3rd ACM SIGCOMM conference on Internet measurement (IMC &apos;03)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="234" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aberrant behavior detection in time series for network service monitoring</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Brutlag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Usenix Conference on System Administration</title>
		<meeting>Usenix Conference on System Administration</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An overview of anomaly detection techniques: Existing solutions and latest technological trends</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3448" to="3470" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of techniques for internet traffic classification using machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Armitage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="76" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Buczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1153" to="1176" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Mahdavinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rezvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Adibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Internet of Things Data Analysis: A Survey</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach to Network Intrusion Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Ngoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Phai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TSDL: A Two-Stage Deep Learning Model for Efficient Network Intrusion Detection</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gumaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Derhab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="30373" to="30385" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection. Network and Distributed System Security Symposium</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doitshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elovici</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigsac Conference on Computer and Communications Security</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1285" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HAST-IDS: Learning Hierarchical Spatial-Temporal Features Using Deep Neural Networks to Improve Intrusion Detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1792" to="1806" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Learning based Multi-channel intelligent attack detection for Data Security</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Sustainable Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mamun Bin Ibne Reaz. A survey of intrusion detection systems based on ensemble and hybrid classifiers</title>
		<author>
			<persName><forename type="first">Abdulla</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aburomman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="135" to="152" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ensemble-learning Approaches for Network Security and Anomaly Detection</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Vanerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks</title>
		<meeting>the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ensemble based collaborative and distributed intrusion detection systems: A survey</title>
		<author>
			<persName><forename type="first">Gianluigi</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Sabatino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Binary PSO and random forests algorithm for probe attacks detection in a network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE congress on evolutionary computation</title>
		<imprint>
			<biblScope unit="page" from="662" to="668" />
			<date type="published" when="2011">2011. 2011</date>
			<publisher>CEC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ensemble-distributed approach in classification problem solution for intrusion detection systems. Intelligent data engineering and automated learning (IDEAL)</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bukhtoyarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhukov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A novel framework, based on fuzzy ensemble of classifiers for intrusion detection systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Masarat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharifian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The online performance estimation framework: heterogeneous ensemble learning for data streams</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="149" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="599" to="619" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A New Learning Algorithm for Mean Field Boltzmann Machines. International Conference on Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yee Whye Teh:A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">minimum description length and Helmholtz free energy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Autoencoders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="399" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to forget: continualprediction with LSTM</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">MAW-ILab: Combining diverse anomaly detectors for automated anomaly labeling and performance benchmarking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fontugne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Borgnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukuda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-12">2010. December 2010</date>
			<publisher>ACM CoNEXT</publisher>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling in Internet Traffic: A 14 Year and 3 Day Longitudinal Study, With Multiscale Analyses and Random Projections</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fontugne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2152" to="2165" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">IEEE International Conference On Data Mining, ICDM08</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
	<note>Isolation</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Reynolds</surname></persName>
		</author>
		<title level="m">Gaussian mixture models. Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="827" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K A</forename><surname>Jena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svm</forename><surname>Multiclass</surname></persName>
		</author>
		<title level="m">Classification Approach for Intrusion Detection. International Conference on Distributed Computing &amp; Internet Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Effective Feature Extraction via Stacked Sparse Autoencoder to Improve Intrusion Detection System</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="41238" to="41248" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Network Anomaly Detection with the Restricted Boltzmann Machine</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castiglione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Santis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adaptive, Hands-Off Stream Mining. 29th International Conference on Very Large Data Bases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brockwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Online Adaptive Anomaly Detection for Augmented Network Flows</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Ippoliti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Auton. Adapt. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">BigFlow: Real-time and reliable anomaly-based intrusion detection for high-speed networks</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Altair</forename><surname>Santin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alysson</forename><surname>Bessani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="485" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An in-depth experimental study of anomaly detection using gradient boosted machine</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Tama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="955" to="965" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TSE-IDS: A Two-Stage Classifier Ensemble for Intelligent Anomaly-Based Intrusion Detection System</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Tama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Comuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="94497" to="94507" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An Enhanced SYN Cookie Defence Method for TCP DDoS Attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JNW</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1206" to="1213" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Out-of-sample tests of forecasting accuracy: an analysis and review</title>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">J</forename><surname>Tashman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the use of crossvalidation for time series predictor evaluation</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><forename type="middle">M</forename><surname>Benłtez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="192" to="213" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ensemble Methods -Foundations and Algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Practical nonparametric statistics 3rd edition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Conove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>Michigan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">He is working towards the Ph.D. degree at the Institute for Network Sciences and Cyberspace at Tsinghua University. His research interests include the machine learning, big data, network security, data analysis and mining algorithms and their parallel implementation</title>
		<author>
			<persName><forename type="first">Salvador</forename><surname>Garcła</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="2044" to="2064" />
			<date type="published" when="2010">2010</date>
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Hunan University</orgName>
		</respStmt>
	</monogr>
	<note>Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Currently he is an Associate Professor in the Institute for Network Sciences and Cyberspace at Tsinghua University. His research interests include formal methods and protocol testing, next generation Internet</title>
	</analytic>
	<monogr>
		<title level="m">Zhiliang Wang received the B.E., M.E. and Ph.D. degrees in computer science from Tsinghua University, China in 2001, 2003 and 2006 respectively</title>
		<imprint/>
	</monogr>
	<note>Wenqi Chen is now working for an undergraduate degreee in computer science and technology at Tsinghua University. His research interests include Cyberspace security and network intrusion detection</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Yifan Chen is now a junior student of Beijing University of Posts and Telecommunications</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include Computer Network and Intrusion Detection System</title>
		<meeting><address><addrLine>Beijing, P.R.China</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
