<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Tero</forename><surname>Karras</surname></persName>
						</author>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Timo</forename><surname>Aila</surname></persName>
						</author>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Remedy</forename><surname>Entertainment</surname></persName>
						</author>
						<author>
							<persName><roleName>NVIDIA</roleName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aalto</forename><surname>University</surname></persName>
						</author>
						<title level="a" type="main">Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3072959.3073658</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computing methodologies → Animation</term>
					<term>Neural networks</term>
					<term>Supervised learning by regression</term>
					<term>Learning latent representations</term>
					<term>Facial animation, deep learning, audio</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Our deep neural network for inferring facial animation from speech. The network takes approximately half a second of audio as input, and outputs the 3D vertex positions of a fixed-topology mesh that correspond to the center of the audio window. The network also takes a secondary input that describes the emotional state. Emotional states are learned from the training data without any form of pre-labeling.</p><p>We present a machine learning technique for driving 3D facial animation by audio input in real time and with low latency. Our deep neural network learns a mapping from input waveforms to the 3D vertex coordinates of a face model, and simultaneously discovers a compact, latent code that disambiguates the variations in facial expression that cannot be explained by the audio alone. During inference, the latent code can be used as an intuitive control for the emotional state of the face puppet.</p><p>We train our network with 3-5 minutes of high-quality animation data obtained using traditional, vision-based performance capture methods. Even though our primary goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language, as we demonstrate with a user study. The results are applicable to in-game dialogue, low-cost localization, virtual reality avatars, and telepresence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Expressive facial animation is an essential part of modern computergenerated movies and digital games. Currently, vision-based performance capture, i.e., driving the animated face with the observed motion of a human actor, is an integral component of most production pipelines. While the quality obtainable from capture systems is steadily improving, the cost of producing high-quality facial animation remains high. First, computer vision systems require elaborate setups and often also labor-intensive cleanup and other processing steps. A second, less obvious issue is that whenever new shots are recorded, the actors need to be on location, and ideally also retain their appearance. This may be challenging if, for example, another role requires growing a beard.</p><p>While audio-based performance capture algorithms are unlikely to ever match the quality of vision systems, they offer complementary strengths. Most importantly, the tens of hours of dialogue spoken by in-game characters in many modern games is much too expensive to produce using vision-based systems. Consequently, common practice is to produce only key animations, such as cinematics, using vision systems, and rely on systems based on audio and transcript for producing the bulk of in-game material. Unfortunately, the quality of the animation produced by such systems currently leaves much to be desired. Further, emerging real-time applications in telepresence and virtual reality avatars present additional challenges due to the lack of transcript, wide variability in user voices and physical setups, and stringent latency requirements.</p><p>Our goal is to generate plausible and expressive 3D facial animation based exclusively on a vocal audio track. For the results to look natural, the animation must account for complex and codependent phenomena including phoneme coarticulation, lexical stress, and interaction between facial muscles and skin tissue <ref type="bibr" target="#b13">[Edwards et al. 2016]</ref>. Hence, we focus on the entire face, not just the mouth and lips. We adopt a data-driven approach, where we train a deep neural network in an end-to-end fashion to replicate the relevant effects observed in the training data.</p><p>At first, the problem may seem intractable because of its inherent ambiguity-the same sounds can be uttered with vastly different facial expressions, and the audio track simply does not contain enough information to distinguish between the different variations <ref type="bibr" target="#b36">[Petrushin 1998</ref>]. While modern convolutional neural networks have proven extremely effective in various inference and classification tasks, they tend to regress toward the mean if there are ambiguities in the training data.</p><p>To tackle these problems, we present three main contributions:</p><p>• A convolutional network architecture tailored to effectively process human speech and generalize over different speakers (Sections 3.1 and 3.2). • A novel way to enable the network to discover variations in the training data that cannot be explained by the audio alone, i.e., apparent emotional state (Section 3.3). • A three-way loss function to ensure that the network remains temporally stable and responsive under animation, even with highly ambiguous training data (Section 4.3).</p><p>Our method produces expressive 3D facial motion from audio in real time and with low latency. To retain maximal independence from the details of the downstream animation system, our method outputs the per-frame positions of the control vertices of a fixedtopology facial mesh. Alternative encodings <ref type="bibr" target="#b25">[Lewis et al. 2014</ref>] such as blend shapes or non-linear rigs can be introduced at later pipeline stages, if needed for compression, rendering, or editability. We train our model with 3-5 minutes of high-quality footage obtained using traditional, vision-based performance capture methods. While our goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language.</p><p>We see uses for this technology in in-game dialogue, low-cost localization, virtual reality, and telepresence. It could also prove useful in accommodating small script changes even in cinematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We will review prior art in systems whose input is audio or text and output is 2D video or 3D mesh animation. We group the approaches into linguistic and machine learning based models, and also review methods that support apparent emotional states.</p><p>Models based on linguistics. A large body of literature exists for analyzing and understanding the structure of language, and translating it to anatomically credible facial animation <ref type="bibr" target="#b24">[Lewis 1991;</ref><ref type="bibr" target="#b32">Mattheyses and Verhelst 2015]</ref>. Typically, an audio track is accompanied with a transcript that helps to provide explicit knowledge about the phoneme content. The animation is then based on the visual counterpart of phonemes called visemes <ref type="bibr" target="#b16">[Fisher 1968</ref>] through complex rules of coarticulation. A well-known example of such a system is the dominance model <ref type="bibr" target="#b7">[Cohen and Massaro 1993;</ref><ref type="bibr" target="#b31">Massaro et al. 2012</ref>]. In general, there is a many-to-many mapping between phonemes and visemes, as implemented in the dynamic visemes model of <ref type="bibr" target="#b42">Taylor et al. [2012]</ref> and in the recent work dubbed JALI <ref type="bibr" target="#b13">[Edwards et al. 2016]</ref>. JALI factors the facial animation to lip and jaw movements, based on psycholinguistic considerations, and is able to convincingly reproduce a range of speaking styles and apparent emotional states independently of the actual speech content.</p><p>A core strength of these methods is the explicit control over the entire process, which makes it possible to, e.g., explicitly guarantee that the mouth closes properly when the puppet is spelling out a bilabial (/m/,/b/,/p/) or that the lower lip touches the upper teeth with labiodentals (/f/,/v/). Both are difficult cases even for visionbased capture systems. The weaknesses include the accumulated complexity of the process, language-specific rules, need for a nearperfect transcript for good quality (typically done manually <ref type="bibr" target="#b13">[Edwards et al. 2016]</ref>), inability to react convincingly to non-phoneme sounds, and lack of a principled way to animate other parts of the face besides the jaw and lips.</p><p>FaceFX (www.facefx.com) is a widely used commercial package that implements the most widely used linguistic models, including the dominance model. Models based on machine learning. Here we will group the systems primarily based on how they use machine learning.</p><p>Voice puppetry <ref type="bibr" target="#b2">[Brand 1999</ref>] is driven exclusively using audio, and does not perform explicit analysis of the structure of the speech. In the training stage, it estimates a hidden Markov model (HMM) based on the observed dynamics of the face in a video. During inference, the HMM is sampled and the most probable sequence is synthesized through trajectory optimization that considers the entire utterance. Subsequent work has improved the trajectory sampling <ref type="bibr" target="#b0">[Anderson et al. 2013;</ref><ref type="bibr" target="#b48">Wang and Soong 2015]</ref> and replaced HMM, which does piecewise linear approximation, with alternative representations such as Gaussian process latent variable model <ref type="bibr" target="#b8">[Deena and Galata 2009;</ref><ref type="bibr" target="#b9">Deena et al. 2013]</ref>, hidden semi-Markov model <ref type="bibr" target="#b37">[Schabus et al. 2014]</ref>, or recurrent networks <ref type="bibr" target="#b15">[Fan et al. 2016]</ref>.</p><p>Alternatively, machine learning has been used for learning coarticulation <ref type="bibr" target="#b11">[Deng et al. 2006;</ref><ref type="bibr" target="#b15">Ezzat et al. 2002]</ref>, followed by a concatenation stage to synthesize animation, or for mapping between various stages, such as phoneme classification <ref type="bibr" target="#b23">[Kshirsagar and Magnenat-Thalmann 2000]</ref>, mapping text to phonemes and phonemes to visemes <ref type="bibr" target="#b28">[Malcangi 2010</ref>], or mapping input audio features to control parameters of a Gaussian mixture model <ref type="bibr" target="#b18">[Hofer and Richmond 2010]</ref>.</p><p>Given that our goal is to produce 3D animation based on audio, we are not inherently interested in the intermediate representations. Instead, we would like to formulate the entire mapping as an end-to-end optimization task. The early experiments with neural networks used audio to directly drive the control parameters of an animated 3D mesh <ref type="bibr" target="#b19">[Hong et al. 2002;</ref><ref type="bibr" target="#b30">Massaro et al. 1999;</ref><ref type="bibr" target="#b35">Öhman and Salvi 1999]</ref>, but the networks back then were necessarily of trivial complexity. We revisit this end-to-end formulation with deep convolutional networks, latest training methods, and problem-specific contributions.</p><p>It is unfortunately difficult to do apples-to-apples comparisons against previous systems that use machine learning. The majority of work has focused on reusing captured video frames with concatenation, blending, and warping. Such image-based methods (e.g., <ref type="bibr" target="#b0">[Anderson et al. 2013;</ref><ref type="bibr" target="#b9">Deena et al. 2013;</ref><ref type="bibr" target="#b15">Ezzat et al. 2002;</ref><ref type="bibr" target="#b15">Fan et al. 2016;</ref><ref type="bibr" target="#b27">Liu and Ostermann 2011;</ref><ref type="bibr" target="#b48">Wang and Soong 2015]</ref>) can produce realistic results, but typically need to store a large corpus of frames and are not directly applicable to applications such as games or VR that need to animate and render 3D models from free viewpoints, typically with flexible identities. The systems that do produce 3D are often based on text input instead of audio <ref type="bibr" target="#b37">[Schabus et al. 2014;</ref><ref type="bibr" target="#b47">Wampler et al. 2007</ref>]. We are not aware of any publicly available implementations of methods that would suit our needs, e.g. <ref type="bibr" target="#b11">[Deng et al. 2006</ref>], and it would not be fair to compare against the result videos of old methods since the quality standards have risen dramatically in sync with the available computing power.</p><p>Extracting and controlling the emotional state. The automatic separation of speech and emotional state has been studied by several authors. <ref type="bibr" target="#b6">Chuang et al. [2002]</ref> build on the work of <ref type="bibr" target="#b43">Tenenbaum and Freeman [2000]</ref> and use a bilinear model for separating the apparent emotional state from speech visemes. This was later generalized in closely related topics to multilinear <ref type="bibr" target="#b46">[Vasilescu and Terzopoulos 2003;</ref><ref type="bibr" target="#b47">Wampler et al. 2007</ref>] and non-linear models <ref type="bibr" target="#b14">[Elgammal and Lee 2004]</ref>, as well as independent component analysis <ref type="bibr" target="#b3">[Cao et al. 2003]</ref>. <ref type="bibr" target="#b4">Cao et al. [2005]</ref> extract emotions using support vector machines, and synthesize 3D animations based on speech and apparent emotional state. They compute mappings between a set of pre-defined emotional states, and let the user specify the state to be used for animation synthesis. <ref type="bibr" target="#b11">Deng et al. [2006]</ref> compute an eigenspace for expressions based on a pre-defined set of emotional states. <ref type="bibr" target="#b47">Wampler et al. [2007]</ref> also allow a user-specified emotional state. <ref type="bibr" target="#b0">Anderson et al. [2013]</ref> use cluster adaptive training to derive a basis for the emotional state so that it can be interpolated and extrapolated. They also present a user study rating the level of realism in emotion synthesis, covering several methods <ref type="bibr" target="#b4">[Cao et al. 2005;</ref><ref type="bibr" target="#b27">Liu and Ostermann 2011;</ref><ref type="bibr" target="#b33">Melenchon et al. 2009</ref>]. <ref type="bibr" target="#b21">Jia et al. [2014]</ref> use neural networks to learn a mapping from PAD (pleasure-displeasure, arousal-nonarousal, and dominance-submissiveness) parameters to facial expressions.</p><p>What distinguishes our method from all these efforts is that instead of having pre-defined categories for emotions, we let the network learn a latent, low-dimensional descriptor that allows it to explain the data. be later assigned any number of semantic meanings, e.g., "sad" or "happy", but those have no role in the learning process itself.</p><p>Residual motion. Almost, but not quite, human-like appearance and motion is often perceived as particularly creepy, an effect known as the uncanny valley <ref type="bibr" target="#b34">[Mori 1970</ref>]. Several authors have tried alleviating the effect by deducing additional motion, such as head movements, eye saccades or blinks from audio (e.g., <ref type="bibr" target="#b10">[Deng et al. 2004;</ref><ref type="bibr" target="#b29">Marsella et al. 2013]</ref>). In our work, we assume that such residual motion is driven by higher-level procedural controls, such as a game engine, and limit our scope to motion that is directly related to articulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">END-TO-END NETWORK ARCHITECTURE</head><p>We will now describe the architecture of our network, along with details on audio processing and the separation of emotional state from the speech content.</p><p>Given a short window of audio, the task of our network is to infer the facial expression at the center of the window. We represent the expression directly as per-vertex difference vectors from a neutral pose in a fixed-topology face mesh. Once the network is trained, we animate the mesh by sliding a window over a vocal audio track and evaluate the network independently at each time step. Even though the network itself has no memory of past animation frames, it produces temporally stable results in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture overview</head><p>Our deep neural network consists of one special-purpose layer, 10 convolutional layers, and 2 fully-connected layers. We divide it in three conceptual parts, illustrated in Figure <ref type="figure">1</ref> and Table <ref type="table">1</ref>.</p><p>We start by feeding the input audio window to a formant analysis network to produce a time-varying sequence of speech features that will subsequently drive articulation. The network first extracts raw formant information using fixed-function autocorrelation analysis (Section 3.2) and then refines it with 5 convolutional layers. Through training, the convolutional layers learn to extract shortterm features that are relevant for facial animation, such as intonation, emphasis, and specific phonemes. Their abstract, time-varying representation is the output of the 5th convolutional layer.</p><p>Next, we feed the result to an articulation network that consists of 5 further convolutional layers that analyze the temporal evolution of the features and eventually decide on a single abstract feature vector that describes the facial pose at the center of the audio window. As a secondary input, the articulation network accepts a (learned) description of emotional state to disambiguate between different facial expressions and speaking styles (Section 3.3). The emotional state is represented as an E-dimensional vector that we concatenate directly onto the output of each layer in the articulation network, enabling the subsequent layers to alter their behavior accordingly.</p><p>Each layer l outputs F l ×W l ×H l activations, where F l is the number of abstract feature maps, W l is dimension of the time axis, and H l is the dimension of the formant axis. We use strided 1×3 convolutions in the formant analysis network to gradually reduce H l while increasing F l , i.e., to push raw formant information to the abstract features, until we have H l = 1 and F l = 256 at the end. Similarly, we use 3×1 convolutions in the articulation network to decrease W l , i.e., to subsample the time axis by combining information from the temporal neighborhood. We chose the specific parameters listed in Table <ref type="table">1</ref> because we found them to consistently perform well in our datasets while leading to reasonable training times. The results are not hugely sensitive to the exact number of layers or feature maps, but we found it necessary to organize the convolutions in two distinct phases to avoid overfitting. Importantly, the formant analysis network performs the same operation at every point along the time axis, so that we can benefit from the same training samples at different time offsets.</p><p>The articulation network outputs a set of 256+E abstract features that together represent the desired facial pose. We feed these features to an output network to produce the final 3D positions of 5022 control vertices in our tracking mesh. The output network is implemented as a pair of fully-connected layers that perform a simple linear transformation on the data. The first layer maps the set of input features to the weights of a linear basis, and the second layer calculates the final vertex positions as a weighted sum over the corresponding basis vectors. We initialize the second layer to 150 precomputed PCA components that together explain approximately 99.9% of the variance seen in the training data. In principle, we could use a fixed basis to effectively train the earlier layers to output the 150 PCA coefficients. However, we have found that allowing the basis vectors themselves to evolve during training yields slightly better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio processing</head><p>The main input to our network is the speech audio signal that we convert to 16 kHz mono before feeding it to the network. In our experiments, we normalize the volume of each vocal track to utilize the full [-1,+1] dynamic range, but we do not employ any other kind of processing such as dynamic range compression, noise reduction, or pre-emphasis filter.</p><p>The autocorrelation layer in Table <ref type="table">1</ref> converts the input audio window to a compact 2D representation for the subsequent convolutional layers. Our approach is inspired by the source-filter model of speech production <ref type="bibr" target="#b1">[Benzeghiba et al. 2007;</ref><ref type="bibr" target="#b24">Lewis 1991]</ref>, where the audio signal is modeled as a combination of a linear filter (vocal tract) and an excitation signal (vocal cords). The resonance frequencies (formants) of the linear filter are known to carry essential information about the phoneme content of the speech. The excitation signal indicates the pitch, timbre, and other characteristics of the speaker's voice, which we hypothesize to be far less important for facial animation, and thus we focus primarily on the formants to improve the generalization over different speakers.</p><p>The standard method for performing source-filter separation is linear predictive coding (LPC). LPC breaks the signal into several short frames, solves the coefficients of the linear filter for each frame based on the first K autocorrelation coefficients, and performs inverse filtering to extract the excitation signal. The resonance frequencies of the filter are entirely determined by the autocorrelation coefficients, so we choose to skip most of the processing steps and simply use the autocorrelation coefficients directly as our representation of the instantaneous formant information. This makes intuitive sense, since the autocorrelation coefficients essentially represent a compressed version of signal whose frequency content approximately matches the power spectrum of the original signal. The representation is a natural fit for convolutional networks, as the layers can easily learn to estimate the instantaneous power of specific frequency bands.</p><p>In practice, we use 520ms worth of audio as input, i.e., 260ms of past and future samples with respect to the desired output pose. We chose this value to capture relevant effects like phoneme coarticulation without providing too much data that might lead to overfitting. The input audio window is divided into 64 audio frames with 2x overlap, so that each frame corresponds to 16ms (256 samples) and consecutive frames are located 8ms (128 samples) apart. For each audio frame, we remove the DC component and apply the standard Hann window to reduce temporal aliasing effects. Finally, we calculate K = 32 autocorrelation coefficients to yield a total of 64×32 scalars for the input audio window. Although much fewer autocorrelations, e.g. K = 12, would suffice to identify individual phonemes, we choose to retain more information about the original signal to allow the subsequent layers to also detect variations in pitch.</p><p>Our approach differs from most of the previous work on speech recognition, where the analysis step is typically based on a specialized techniques such as Mel-frequency cepstral coefficients (MFCC), perceptual linear prediction (PLP), and rasta filtering <ref type="bibr" target="#b1">[Benzeghiba et al. 2007</ref>]. These techniques have enjoyed wide adoption mainly because they lead to good linear separation of phonemes and consequently work well with hidden Markov models. In our early tests, we tried several different input data representations and observed that the autocorrelation coefficients were clearly superior for our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representation of emotional states</head><p>Inferring facial animation from speech is an inherently ambiguous problem, because the same sound can be produced with very different facial expressions. This is especially true with the eyes and eyebrows, since they have no direct causal relationship with sound production. Such ambiguities are also problematic for deep neural networks, because the training data will inevitably contain cases where nearly identical audio inputs are expected to produce very different output poses. Indeed, Figure <ref type="figure" target="#fig_0">2</ref> shows several examples of conflicting training data where the input audio clip consists entirely of silence. If the network has nothing else to work with besides the audio, it will learn to output the statistical mean of the conflicting outputs.</p><p>Our approach for resolving these ambiguities is to introduce a secondary input to the network. We associate a small amount of additional, latent data with each training sample, so that the network has enough information to unambiguously infer the correct output pose. Ideally, this additional data should encode all relevant aspects of the animation in the neighborhood of a given training sample that cannot be inferred from the audio itself, including different facial expressions, speaking styles, and coarticulation patterns. Informally, we wish the secondary input to represent the emotional state of the actor. Besides resolving ambiguities in the training data, the secondary input is also highly useful for inference-it allows us to mix and match different emotional states with a given vocal track to provide powerful control over the resulting animation.</p><p>One way to implement emotional states would be to manually label or categorize the training samples based on the apparent emotion <ref type="bibr" target="#b0">[Anderson et al. 2013;</ref><ref type="bibr" target="#b4">Cao et al. 2005;</ref><ref type="bibr" target="#b11">Deng et al. 2006;</ref><ref type="bibr" target="#b47">Wampler et al. 2007</ref>]. This approach is not ideal, however, because there is no guarantee that a pre-defined labeling actually resolves ambiguities in the training data to a sufficient degree. Instead of relying on predefined labels, we adopt a data-driven approach where the network automatically learns a succinct representation of the emotional state as a part of the training process. This allows us to extract meaningful emotional states even from in-character footage, as long as a sufficient range of emotions is present.</p><p>We represent the emotional state as an E-dimensional vector, where E is a tunable parameter that we set to 16 or 24 in our tests, and initialize the components to random values drawn from a Gaussian distribution. One such vector is allocated for each training sample, and we refer to the matrix that stores these latent variables as the emotion database. As illustrated in Figure <ref type="figure">1</ref>, the emotional state is appended to the list of activations of all layers of the articulation network. This makes it a part of the computation graph of the loss function (Section 4.3), and as a trainable parameter, it gets updated along with the network's weights during backpropagation. The dimensionality of E is a tradeoff between two effects. If E is too low, the emotional states fail to disambiguate variations in the training data, leading to weak audio response. If E is too high, all emotional states tend to become too specialized to be useful for general inference (Section 5.1).</p><p>One potential concern with the emotion database is that unless we constrain it in a meaningful way, it might learn to explicitly store information that is also present in the audio. In a pathological case, it could store E blend shape weights that determine much of the facial expression, thus diminishing the role of audio and making the network useless for processing material not seen during training.</p><p>The information provided by the audio is limited to short-term effects within the 520ms interval by design. Consequently, a natural way to prevent the emotional states from containing duplicate information is to forbid them from containing short-term variation.</p><p>Having the emotional states focus on longer-term effects is also highly desirable for inference-we want the network to produce reasonable animation even when the emotional state remains fixed. We can express this requirement by introducing a dedicated regularization term in our loss function to penalize quick variations in the emotion database, which leads to incremental smoothing of the emotional states over the course of training. One important limitation of our approach is that we cannot model blinking and eye motion correctly since they do not correlate with the audio and cannot be represented using the slowly varying emotional state.</p><p>While it may seem redundant to append the emotional state to all layers of the articulation network, we have found this to improve the results significantly in practice. We suspect that this is because the emotional state aims to control the animation on multiple abstraction levels, and the higher abstraction levels are generally more difficult to learn. Connecting to the earlier layers provides nuanced control over subtle animation features such as coarticulation, whereas connecting to the later layers provides more direct control over the output poses. It makes intuitive sense that the early stages of training will need to concentrate on the latter, while the later stages can concentrate on the former once the individual poses are reasonably well represented.</p><p>The learned emotional states are just data without semantic meanings ("sad", "happy", etc.). We defer the discussion about semantics to Section 5.1 because they do not play a role in the network architecture or training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING</head><p>We will now describe the aspects relevant to training our network: how the training targets were obtained, what our dataset consists of, dataset augmentation, loss function, and training setup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training targets</head><p>We obtained the 3D vertex positions used as training targets using the commercial DI4D PRO system (www.di4d.com) that employs nine synchronized video cameras at 30Hz to directly capture the nuanced interactions of the skull, muscles, fascia and skin of an actor, excluding high frequency details such as wrinkles. The benefit of this system is that it allows us to bypass complex and expensive facial rigging and tissue simulations for digital doubles. The input and output data is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>As the first step, an unstructured mesh with texture and optical flow data is reconstructed from the nine images captured for each frame. A fixed-topology template mesh, created prior to the capture work using a separate photogrammetry pipeline, is then projected on to the unstructured mesh and associated with the optical flow. The template mesh is tracked across the performance and any issues are fixed semi-automatically in the DI4DTrack software by a tracking artist. The position and orientation of the head are stabilized using a few key vertices of the tracking mesh. Finally, the vertex positions of the mesh are exported for each frame in the shot. These positionsor more precisely the deltas from a neutral pose-are the desired outputs of our network when given a window of audio during training.</p><p>One limitation of video-based capture setups is that they cannot capture the tongue since it is typically not visible in the images. While the exact control of the tongue is highly relevant for speech production, it is rarely visible so clearly that it would have to be animated in detail. We thus leave the tongue as well as wrinkles, eyes and residual head/body motion to higher level procedural animation controls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training dataset</head><p>For each actor, the training set consists of two parts: pangrams and in-character material. In general, the inference quality increases as the training set grows, but a small training set is highly desirable due to the cost of capturing high-quality training data. We feel that 3-5 minutes per actor represents a practical sweet spot.</p><p>Pangrams. This set attempts to cover the set of possible facial motions during normal speech for a given target language, in our case English. The actor speaks one to three pangrams, i.e., sentences that are designed to contain as many different phonemes as possible, in several different emotional tones to provide a good coverage of the range of expression.</p><p>In-character material. This set leverages the fact that an actor's performance of a character is often heavily biased in terms of emotional and expressive range for various dramatic and narrative reasons. In case of a movie or a game production, this material can be composed of the preliminary version of the script. Only the shots that are deemed to support the different aspects of the character are selected so as to ensure that the trained network produces output that stays in character even if the inference is not perfect, or if completely novel or out of character voice acting is encountered.</p><p>For Character 1, our training set consists of 9034 frames (5min 1s), of which 3872 come from pangrams and 5162 from in-character material. Additionally, 1734 frames are reserved for validation.</p><p>For Character 2, our training set consists of 6762 frames (3min 45s), of which 1722 come from pangrams and 5040 from in-character material. Additionally, we have 887 frames for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss function</head><p>Given the ambiguous nature of our training data, we must take special care to define a meaningful loss function that we wish to optimize. We use a specialized loss function that consists of three distinct terms: a position term to ensure that the overall location of each output vertex is roughly correct, a motion term to ensure that the vertices exhibit the right kind of movement under animation, and a regularization term to discourage the emotion database from containing short-term variation.</p><p>Simultaneous optimization of multiple loss terms tends to be difficult in practice, because the terms can have wildly different magnitudes and their balance may change in unpredictable ways during training. The typical solution is to associate a pre-defined weight with each term to ensure that none of them gets neglected by the optimization. However, choosing optimal values for the weights can be a tedious process of trial and error and it typically needs to be repeated whenever the training set changes. To overcome these issues, we introduce a normalization scheme that automatically balances the loss terms with respect to their relative importance. As a result, we automatically put an equal amount of effort into optimizing each term and there is consequently no need to specify any additional weights.</p><p>Position term. Our primary error metric is the mean of squared differences between the desired output y and the output produced by the network ŷ. For a given training sample x, we express this using the position term P(x):</p><formula xml:id="formula_0">P(x) = 1 3V 3V i=1 y (i) (x) − ŷ(i) (x) 2 (1)</formula><p>Here, V represents the total number of output vertices and y (i) denotes the ith scalar component of y = (y (1) , y (2) , . . . , y (3V ) ). The total number of output components is 3V , because our network outputs the full 3D position for each vertex.</p><p>Motion term. Even though the position term ensures that the output of our network is roughly correct at any given instant in time, it is not sufficient to produce high-quality animation. We have found that training the network with the position term alone leads to a considerable amount of temporal instability, and the response to individual phonemes is generally weak. This motivates us to optimize our network in terms of vertex motion as well: a given output vertex should only move if it also moves in the training data, and it should only move at the right time. We thus need a way to measure vertex motion as a part of our loss function, similar to Brand's work on HMMs <ref type="bibr">[1999]</ref>, where both position and velocity are optimized.</p><p>The standard approach for training neural networks is to iterate over the training data in minibatches, where each minibatch consists of B randomly selected training samples x 1 , x 2 , . . . , x B . To account for vertex motion, we draw the samples as B/2 temporal pairs, each consisting of two adjacent frames. We define operator m[•] as the finite difference between the paired frames. We can now define the motion term M(x):</p><formula xml:id="formula_1">M(x) = 2 3V 3V i=1 m y (i) (x) − m ŷ(i) (x) 2 (2)</formula><p>The factor 2 appears because M is evaluated once per temporal pair.</p><p>Regularization term. Finally, we need to ensure that the network correctly attributes short-term effects to the audio signal and longterm effects to the emotional state as described in Section 3.3. We can conveniently define a regularization term for our emotion database using the same finite differencing operator as above:</p><formula xml:id="formula_2">R ′ (x) = 2 E E i=1 m e (i) (x) 2 (3)</formula><p>e (i) (x) denotes the ith component stored by the emotion database for training sample x. Note that this definition does not explicitly forbid the emotion database from containing short-term variationit merely discourages excess variation on the average. This is important, because our training data contains legitimate short-term changes in the emotional state occasionally, and we do not want the network to incorrectly try to explain them based on the audio signal.</p><p>A major caveat with Eq. 3 is that R ′ (x) can be brought arbitrarily close to zero by simply decreasing the magnitude of e(x) while increasing the corresponding weights in the network. Drawing on the idea of batch normalization <ref type="bibr" target="#b20">[Ioffe and Szegedy 2015]</ref>, we remove this trivial solution by normalizing R ′ (x) with respect to the observed magnitude of e(x):</p><formula xml:id="formula_3">R(x) = R ′ (x) 1 EB E i=1 B j=1 e (i) (x j ) 2 (4)</formula><p>Normalization. To balance our three loss terms, we leverage the properties of the Adam optimization method [Kingma and Ba 2014] that we use for training our network. In effect, Adam updates the weights of the network according to the gradient of the loss function, normalized in a component-wise fashion according to a long-term estimate of its second raw moment. The normalization makes the training resistant to differences in the magnitude of the loss function, but this is only true for the loss function as a whole-not for the individual terms. Our idea is to perform similar normalization for each loss term individually. Using the position term as an example, we estimate the second raw moment of P(x) for each minibatch and maintain a moving average v P t across consecutive minibatches:</p><formula xml:id="formula_4">v P t = β • v P t −1 + (1 − β) • 1 B B j=1 P(x j ) 2 (5)</formula><p>Here, t denotes the minibatch index and β is a decay parameter for the moving average that we set to 0.99. We initialize v P t = 0 and correct our estimate to account for startup bias to get vP t = v P t /(1 − β t ). We then calculate the average P(x) over the current minibatch and normalize the value according to vP t :</p><formula xml:id="formula_5">ℓ P = 1 B B j=1 P(x j ) vP t + ϵ (6)</formula><p>ϵ is a small constant that we set to 10 −8 to avoid division by zero.</p><p>Repeating Equations 5 and 6 for M(•) and R(•), we express our final loss function as a sum over the individual terms ℓ = ℓ P + ℓ M + ℓ R . Although it would be possible to further fine-tune the importance of the loss terms through additional weights, we have not found this to be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training data augmentation</head><p>To improve temporal stability and reduce overfitting, we employ random time-shifting for our training samples. Whenever we present a minibatch to the network, we randomly shift the input audio window by up to 16.6ms in either direction (±0.5 frames at 30 FPS).</p><p>To compensate, we also apply the same shift for the desired output pose through linear interpolation. We shift both training samples in a temporal pair by the same amount, but use different random shift amounts for different pairs. We also tried cubic interpolation of outputs, but it did not work as well as linear.</p><p>As a crucial step to improve generalization and avoid overfitting, we apply multiplicative noise to the input of every convolutional layer <ref type="bibr" target="#b39">[Srivastava et al. 2014]</ref>. The noise has the same magnitude for every layer, and it is applied on a per-feature map basis so that all activations of a given feature map are multiplied by the same factor. We apply identical noise to paired training samples to get a meaningful motion term. The formula for our noise is 1.4 N (0, 1) .</p><p>We do not apply any other kind of noise or augmentation on our training samples besides the time-shifting of input/outputs and multiplicative noise inside the network. We experimented with adjusting the volume, adding reverb (both long and short), performing time-stretching and pitch-shifting, applying non-linear distortion, random equalization, and scrambling the phase information but none of these improved the results further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training setup and parameters</head><p>We have implemented our training setup using Theano <ref type="bibr">[Theano Development Team 2016]</ref> and Lasagne <ref type="bibr" target="#b12">[Dieleman et al. 2015</ref>] that internally use cuDNN <ref type="bibr" target="#b5">[Chetlur et al. 2014]</ref> for GPU acceleration.</p><p>We train the network for 500 epochs using Adam [Kingma and Ba 2014] with the default parameters. Each epoch processes all training samples in a randomized order in minibatches of 100 training samples (50 temporal pairs). The learning rate is ramped up tenfold using a geometric progression during the first training epoch, and it is then decreased gradually according to 1/ √ t schedule. During the last 30 epochs we ramp the learning rate down to zero using a smooth curve, and simultaneously ramp Adam's β 1 parameter from 0.9 to 0.5. The ramp-up removes an occasional glitch where the network does not start learning at all, and the ramp-down ensures that the network converges to a local minimum. The total training time is 2h 55min for Character 1 and 1h 30min for Character 2 on an NVIDIA Pascal Titan X. The network weights are initialized following <ref type="bibr" target="#b17">He et al. [2015]</ref>. The emotion database is initialized with zero-mean Gaussian noise with E = 16, σ = 0.01 for Character 1 and E = 24, σ = 0.002 for Character 2. We had to hand-tune these parameters per actor, while other parameters required no tuning. The differences are probably explained by the fact that Character 1 mostly had the same expression throughout the training set, while Character 2 was much more lively and did various extraneous movements with his face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFERENCE AND RESULTS</head><p>Once trained, the network can be evaluated at arbitrary points in time by selecting the appropriate audio window, leading to facial animation at the desired frame rate.</p><p>On our Theano implementation, inference takes 6.3ms for a single frame and 0.2ms/frame when a batch of 100 frames is processed at once. Given that Theano is known for its large overheads, we are confident that it would be possible to push the single frame performance, which matters for real-time use, below 1ms by using a more efficient framework, e.g., cuDNN</p><p>The latency of our method is determined by the audio window size, which currently reaches 260ms to the future. Coarticulation sets a lower bound for the look-ahead; we have confirmed experimentally that we can limit the look-ahead to 100ms during training with minor degradation in quality, even though some coarticulation effects are known to be longer <ref type="bibr" target="#b38">[Schwartz and Savariaux 2014]</ref>. Shortening the look-ahead further than this leads to a quick drop in perceived responsiveness, so a realistic lower bound for the latency of our method therefore seems to be around 100ms. Methods relying on explicit trajectory optimization (e.g. <ref type="bibr" target="#b2">[Brand 1999;</ref><ref type="bibr" target="#b9">Deena et al. 2013;</ref><ref type="bibr" target="#b15">Fan et al. 2016]</ref>) have substantially higher latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Emotional states</head><p>When inferring the facial pose for novel audio, we need to supply the network with an emotional state vector as a secondary input. As part of training, the network has learned a latent E-dimensional vector for each training sample, and our strategy is to mine this emotion database for robust emotion vectors that can be used during inference.</p><p>During training, the network attempts to separate out the latent information-i.e., everything that is not inferable from the audio alone-into the emotion database. However, this decomposition is not perfect and some amount of crosstalk remains between articulation and the overall expression. In practice, many of the learned emotion vectors are only applicable in the neighborhood of their corresponding training frames and are not necessarily useful for general inference. This is to be expected, because our training data will necessarily be too limited to cover all phonemes and coarticulation effects for every observed emotional state. The neural network itself is not interested in these semantic meanings; as far it is concerned, the emotion vector is just data that helps to disambiguate the audio.</p><p>We manually mine robust emotion vectors a three-step process. The main problem in most learned emotion vectors is that they deemphasize the motion of the mouth: when such a vector is used as a constant input when performing inference for novel audio, the apparent motion of the mouth is subdued. Our first step is therefore to pick a few audio windows from our validation set that contain bilabials and a few that contain vowels, for which the mouth should be closed and open, respectively. We then scan the emotion database for vectors that exhibit the desired behavior for all chosen windows. Performing this preliminary culling for Character 1 resulted in 100 candidate emotion vectors for further consideration. Figure <ref type="figure">4</ref> illustrates how the response varies with different emotion vectors. The depicted training shot contains one region of highly responsive emotion vectors, from which one candidate was chosen for further consideration.</p><p>The second step in the culling process is to play back the validation audio tracks and inspect the facial motion inferred with each of the candidate emotion vectors. At this stage, we discard emotion vectors that result in subdued or spurious, unnatural motion, indicating that the emotion vector is tainted with short-term effects. This stage narrowed the set to 86 candidate emotion vectors for Character 1. As the third and final step, we run inference on several seconds of audio from a different speaker and eliminate emotion vectors with muted or unnatural response. We have found that severely attenuated response to a different speaker is a sign of lack of generalization power, and tends to cause problems even with the same speaker under varied articulation styles. With Character 1, this step left 33 emotion vectors.</p><p>We then examine the output of the network for several novel audio clips with every remaining emotion vector, and assign a semantic meaning (e.g., "neutral", "amused", "surprised", etc.) to each of them, depending on the emotional state they convey (Figure <ref type="figure">5</ref>). Which semantic emotions remain depends entirely on the training material, and it will not be possible to extract, e.g., a "happy" emotion if the training data does not contain enough such material to be generalizable to novel audio. Figure <ref type="figure" target="#fig_3">6</ref> shows inferred facial poses for Character 1 using the same audio window but different emotion vectors. As can be seen, even after removing all but the best performing emotion vectors there is still substantial variation to choose from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance capture</head><p>Inference (single) Inference (ensemble) Interestingly, we found that emotion vectors that are mined in this way behave well under interpolation, i.e., sweeping from one emotion vector to another tends to produce natural-looking results. This means that it is possible to vary the emotional state during inference based on high-level information from a game engine, or by manual keyframing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Temporal stability</head><p>As can be seen from the accompanying video, the results are stable in animation. The primary sources of temporal stability are the motion term ℓ M and time-shift augmentation, but even with these techniques there is still a small amount of jitter left in the lip area at 4ms timescale for some inputs. This is likely due to aliasing between neighboring audio frames around stops and plosives. We fix this via ensembling: the network is evaluated twice for a given animation frame, 4 ms apart in time, and the predictions are averaged. Related approaches have been used before, at varying timescales, e.g. <ref type="bibr" target="#b26">[Lewis and Parke 1987;</ref><ref type="bibr" target="#b41">Taylor et al. 2016</ref>]. Figure <ref type="figure" target="#fig_4">7</ref> illustrates the effect.</p><p>Even with the motion term, time-shifting, and ensembling some ambiguities remain in how the eyebrows should move. We thus employ additional temporal smoothing for the upper part of the face using a Gaussian filter with σ t = 0.1 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Retargeting</head><p>When we train our model, the output network becomes specialized for a particular mesh. In many applications we would like to drive several different meshes with audio, and we support that via deformation transfer <ref type="bibr" target="#b40">[Sumner and Popović 2004]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>To assess the quality of our results, we conducted a blind user study with 20 participants who had no professional experience on animation techniques. In the study, we compared the output of our method ("Ours") against video-based performance capture from the DI4D system ("PC") and against dominance model based animation <ref type="bibr" target="#b7">[Cohen and Massaro 1993;</ref><ref type="bibr" target="#b31">Massaro et al. 2012</ref>] produced using FaceFX software ("DM"). The audio clips used as inputs were taken from the held-out validation dataset of the corresponding actor, i.e., they were not used as part of training data when training our network.</p><p>The study featured two animated characters with a total of 13 audio clips that were 3-8 seconds long. For each clip a video was rendered with each of the three methods. Out of these, three A vs. B pairs were created (PC vs. Ours, PC vs. DM, Ours vs. DM), totaling 39 pairs of videos presented to the participants. These pairs were presented as A vs. B choices in random order, also randomizing which method was A and which was B for each individual question. The participants, unaware of which videos originated from which method, progressed through the video pairs, choosing the more natural-looking animation of each pair before moving to the next one. The study took approximately 10-15 minutes to complete. All videos were presented at 30 frames per second with audio. For our method, we assigned each audio clip a constant emotion vector that was mined from the emotion database as explained in Section 5.1.</p><p>FaceFX creates the animation by interpolating between pre-defined target poses: the neutral pose, 6 targets for the mouth, 3 for the tongue, 8 for head rotation, and 3 for the eyes (blink, squint, eyebrow raise). Ignoring the tongue, head rotation, and blinking, we supplied FaceFX with 9 targets that we manually selected from our training set. We spent about 2 hours per character to find targets that matched the examples in FaceFX documentation as closely as  possible. For mouth-related target poses, the upper part of the face was smoothly masked out so that no unwanted motion occurred near the eyes, and the lower part of the face was similarly masked for the two eye-related poses. We supplied FaceFX with the transcript for each audio clip to be used as a basis for viseme analysis.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the full results of the study, summed over the 20 participants. As expected, the output of video-based performance capture was generally perceived as more natural than the animations synthesized by our method or the dominance model. The fifth clip of Character 1, where our method is on par with performance capture, is an interesting exception to this rule. Because both our method and dominance model reach their highest scores against performance capture in this clip, it appears that the surprising result is caused by unnatural look of the performance capture data instead of exceptionally good performance of our method.</p><p>Our method clearly outperforms the dominance model, winning 87% of the pairwise comparisons, and even in the worst-case clips 75% of the participants preferred our method (15 vs. 5 votes). Additionally, our method fares much better in comparisons against video-based performance capture. Estimating an objective quality ranking for each of the methods would require a more elaborate user study, but we can still observe that the vote between our method and performance capture is closer to a tie than the vote between our method and dominance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generalization</head><p>To assess the capability of our network to generalize over different speakers and languages, we conducted a second user study using 14 representative audio clips that we extracted from public domain audio books hosted by LibriVox (www.librivox.org). We did not look at the output of our network when selecting the audio clips, and we did not use them in any way when training our network, tuning the parameters, or mining the emotional states. We selected a total of 6 clips for English, and 8 for French, German, Italian, and Spanish. We further organized the clips by gender to form 7 pairs consisting of one male speaker and one female speaker each. We used a similar setup as in the first user study, and asked 20 participants to compare videos produced by our method ("Ours") and dominance model ("DM") and choose the more natural-looking one. There were a total of 28 pairs of videos presented to each participant, one for each audio clip and each character. The videos were 8-13 seconds long and the study took approximately 15-20 minutes to complete.</p><p>The results of the study are summarized in Table <ref type="table" target="#tab_4">3</ref>. For male speakers, the number of participants that preferred our method is roughly the same as in the first study. The number is somewhat lower for female speakers, however. The likely explanation is that our comparison videos featured a male character even for audio from a female speaker, and it is possible that several participants perceived the output of both methods as equally unnatural in this case. The results also indicate that the variation between different languages is considerably lower than the variation between different speakers of the same language. This suggests that our method is not overly sensitive to the input language per se-the capability to generalize to novel audio is more likely related to the voice, speaking style, and tempo of the particular speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accompanying video</head><p>Since it is impossible to demonstrate the quality of our animation results using text and images, we refer the reader to the accompanying video. The video includes comparisons with video-based performance capture and dominance model, as well as a final render from a game engine. We also compare our results against dynamic visemes and JALI using the original video and audio footage from <ref type="bibr" target="#b42">Taylor et al. [2012]</ref> and <ref type="bibr" target="#b13">Edwards et al. [2016]</ref>. Note that in these comparisons we drive our network using a speaker different from the training set. We observe that our results are fluent, expressive, and have good temporal stability in the entire face region.</p><p>Because our method generalizes well over different speakers, it can also be used with synthetic audio. As an experiment, we ran our method with audio that was synthesized using WaveNet <ref type="bibr" target="#b45">[van den Oord et al. 2016</ref>], a deep neural network that generates audio based on input text. As shown in the video, our method produces naturallooking animation from both male and female synthetic voices.</p><p>To further probe the limits our method, we ran it for the speakers and languages featured in Table <ref type="table" target="#tab_4">3</ref> with several different emotional states. As shown in the video, the English-trained network responds surprisingly well in most cases, but it can have trouble keeping up if the input audio has very different tempo compared to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE WORK</head><p>Our primary focus is on generating high-quality facial animation for in-game dialogue, and we plan to continue evaluating the practical value of our method in a production setting. We have also performed several informal experiments to gauge the suitability of our method for more general-purpose use, using audio clips recorded in a casual setting with consumer-grade equipment and varying levels of background noise. In general, our method appears to remain responsive as long as the input volume level is normalized to roughly match the training data, and the animation looks plausible as long as it is displayed in sync with the audio and the tempo of the speech is not too fast. In the future, we hope to see a more principled study of these and related effects in a realistic interactive setting with two or more speakers.</p><p>We feel that the main shortcoming of our method is the lack of fine detail present in the performance capture data. Combining our approach with generative neural networks might enable better synthesis of such detail and possibly also residual motion for, e.g., the eyes. While our method is able to produce plausible results for several different emotional states based on just 5min of training data, increasing the size of the dataset would likely improve the results even further. It would be particularly interesting to train the network simultaneously for several different characters in attempt to learn a latent, unified representation of character identity. Conceivably, one could also deduce the emotional state automatically during inference based on a longer-term audio context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. What does silence look like? These are example frames from our training set where the actor does not speak.</figDesc><graphic url="image-26.png" coords="5,51.81,80.62,59.95,77.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The training targets were obtained using a vision-based pipeline that uses streams from 9 HD cameras to output 3D positions of animated control vertices for each frame.</figDesc><graphic url="image-33.png" coords="6,237.56,94.98,58.14,67.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Visualization of the opening of the mouth during the course of an audio clip (x -axis) under different constant emotion vector inputs. Each point on the y-axis represents a different emotion vector extracted from the learned database for the same clip. Blue and red indicate that the mouth is closed and open, respectively.We observe that many emotion vectors have problems with opening or closing the mouth properly. As the first step of our database mining process, we rapidly cull emotion vectors that do not respond well to audio in this way.</figDesc><graphic url="image-34.png" coords="8,369.53,92.19,131.96,124.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The emotional state has a large effect on the animation, as shown on the accompanying video. These nine poses are inferred from the same audio window using different emotion vectors.</figDesc><graphic url="image-37.png" coords="9,57.86,80.62,230.13,307.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. The graph shows the y-coordinate of the vertex marked with the red dot as a function of time for performance capture and inference. We can see that an ensemble prediction helps to remove small-scale jitter from the inference results.</figDesc><graphic url="image-41.png" coords="9,439.48,250.47,59.95,71.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, as shown in Figure8.</figDesc><table><row><cell></cell><cell cols="2">PC vs. Ours</cell><cell cols="2">PC vs. DM</cell><cell cols="2">Ours vs. DM</cell></row><row><cell cols="2">Character 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clip 1</cell><cell>14</cell><cell>6</cell><cell>19</cell><cell>1</cell><cell>19</cell><cell>1</cell></row><row><cell>Clip 2</cell><cell>16</cell><cell>4</cell><cell>20</cell><cell>0</cell><cell>20</cell><cell>0</cell></row><row><cell>Clip 3</cell><cell>17</cell><cell>3</cell><cell>20</cell><cell>0</cell><cell>19</cell><cell>1</cell></row><row><cell>Clip 4</cell><cell>16</cell><cell>4</cell><cell>19</cell><cell>1</cell><cell>18</cell><cell>2</cell></row><row><cell>Clip 5</cell><cell>9</cell><cell>11</cell><cell>16</cell><cell>4</cell><cell>17</cell><cell>3</cell></row><row><cell>Clip 6</cell><cell>19</cell><cell>1</cell><cell>19</cell><cell>1</cell><cell>17</cell><cell>3</cell></row><row><cell>Clip 7</cell><cell>15</cell><cell>5</cell><cell>20</cell><cell>0</cell><cell>17</cell><cell>3</cell></row><row><cell>Clip 8</cell><cell>13</cell><cell>7</cell><cell>19</cell><cell>1</cell><cell>15</cell><cell>5</cell></row><row><cell cols="2">Character 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clip 1</cell><cell>17</cell><cell>3</cell><cell>19</cell><cell>1</cell><cell>15</cell><cell>5</cell></row><row><cell>Clip 2</cell><cell>15</cell><cell>5</cell><cell>18</cell><cell>2</cell><cell>15</cell><cell>5</cell></row><row><cell>Clip 3</cell><cell>15</cell><cell>5</cell><cell>19</cell><cell>1</cell><cell>20</cell><cell>0</cell></row><row><cell>Clip 4</cell><cell>18</cell><cell>2</cell><cell>19</cell><cell>1</cell><cell>16</cell><cell>4</cell></row><row><cell>Clip 5</cell><cell>17</cell><cell>3</cell><cell>19</cell><cell>1</cell><cell>17</cell><cell>3</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Votes</cell><cell>201</cell><cell>59</cell><cell>246</cell><cell>14</cell><cell>225</cell><cell>35</cell></row><row><cell>Ratio</cell><cell>77%</cell><cell>23%</cell><cell>95%</cell><cell>5%</cell><cell>87%</cell><cell>13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of the blind user study where we compare our method ("Ours") against video-based performance capture ("PC") and dominance model based animation ("DM") through pairwise quality comparisons. The input audio clips were taken from the held-out validation dataset of the corresponding actor. See text for details of the setup.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of the second user study where we compare our method ("Ours") against dominance model ("DM") with several different speakers and languages. Each row represents one male speaker and one female speaker, evaluated separately for Character 1 and Character 2.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Transactions on Graphics, Vol. 36, No. 4, Article 94. Publication date: July 2017.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We wish to thank Julian Kostov and Derek Hagen for acting, and Markus Holtmanns for German voice acting; Pauli Kemppinen for the ambient occlusion renderings, our colleagues at NVIDIA Helsinki for participating in the user studies, and the deep learning team for compute resources; JALI authors for comparison videos and helpful suggestions on evaluation, Dynamic Visemes authors for comparison videos, and WaveNet authors and LibriVox for audio clips; David Luebke for helpful comments and Lance Williams for pointers to related work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expressive visual text-to-speech using active appearance models</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3382" to="3389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic speech recognition and speech variability: A review</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Benzeghiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Deroo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodora</forename><surname>Erbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Jouvet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Laface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="763" to="786" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Alfred Mertins, Christophe Ris, and others</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voice Puppetry</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
				<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Learning for Speech Motion Editing</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SCA</title>
				<meeting>SCA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="225" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Expressive Speechdriven Facial Animation</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><forename type="middle">C</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1283" to="1302" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient Primitives for Deep Learning</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial expression space learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Graphics</title>
				<meeting>Pacific Graphics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling Coarticulation in Synthetic Visual Speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Massaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Models and Techniques in Computer Animation</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech-Driven Facial Animation Using a Shared Gaussian Process Latent Variable Model</title>
		<author>
			<persName><forename type="first">Salil</forename><surname>Deena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aphrodite</forename><surname>Galata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium on Advances in Visual Computing: Part I</title>
				<meeting>Symposium on Advances in Visual Computing: Part I</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual Speech Synthesis Using a Variable-Order Switching Shared Gaussian Process Dynamical Model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1755" to="1768" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Audio-based Head Motion Synthesis for Avatar-based Telepresence Systems</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shri</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Effective Telepresence</title>
				<meeting>Workshop on Effective Telepresence</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expressive Facial Animation Synthesis by Learning Speech Coarticulation and Expression Spaces</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae-Yong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1523" to="1534" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Søren Kaae Sønderby, and others</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eben</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lasagne: First release</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">JALI: An Animatorcentric Viseme Model for Expressive Lip Synchronization</title>
		<author>
			<persName><forename type="first">Pif</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Separating style and content on a nonlinear manifold</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan-Su</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep bidirectional LSTM approach for video-realistic talking head</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gadi</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5287" to="5309" />
			<date type="published" when="2002">2002. 2002. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>ACM Trans. Graph.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Confusions Among Visually Perceived Consonants</title>
		<author>
			<persName><forename type="first">Cletus</forename><forename type="middle">G</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSLHR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="1968">1968. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<title level="m">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparison of HMM and TMDN Methods for Lip Synchronisation</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korin</forename><surname>Richmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="454" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time Speech-driven Face Animation with Expressions Using Neural Networks</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="916" to="927" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Head and facial gestures synthesis using PAD model for an expressive talking avatar</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="439" to="461" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lip synchronization using linear predictive analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
				<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1077" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated lip-sync: Background and techniques</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="118" to="122" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practice and Theory of Blendshape Facial Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Anjyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehyun</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<title level="s">State of the Art Reports</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated Lip-synch and Speech Synthesis for Character Animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI/GI Conference on Human Factors in Computing Systems and Graphics Interface</title>
				<meeting>SIGCHI/GI Conference on Human Factors in Computing Systems and Graphics Interface</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="143" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Realistic facial expression synthesis for an image-based talking head</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
				<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text-driven avatars based on artificial neural networks and fuzzy logic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malcangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Virtual Character Performance from Speech</title>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaux</forename><surname>Lhommet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SCA</title>
				<meeting>SCA</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Picture my voice: Audio to visual speech synthesis using artificial neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AVSP. #23</title>
				<meeting>AVSP. #23</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Animated speech: Research progress and applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audiovisual Speech Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="309" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Audiovisual speech synthesis: An overview of the state-of-the-art</title>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Mattheyses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="182" to="217" />
			<date type="published" when="2015-02">2015. 2 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Emphatic Visual Speech Synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Melenchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Montero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bukimi no tani (The uncanny valley)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="35" />
			<date type="published" when="1970">1970. 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using HMMs and ANNs for mapping acoustic to visual speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Öhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How well can People and Computers Recognize Emotions in Speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valery</surname></persName>
		</author>
		<author>
			<persName><surname>Petrushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Fall Symp</title>
				<meeting>AAAI Fall Symp</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schabus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="336" to="347" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Savariaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deformation Transfer for Triangle Meshes</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jovan</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="399" to="405" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Audio-to-Visual Speech Conversion using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1482" to="1486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic Units of Visual Speech</title>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry-John</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SCA</title>
				<meeting>SCA</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Separating Style and Content with Bilinear Models</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A Generative Model for Raw Audio</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multilinear Subspace Analysis of Image Ensembles</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic, Expressive Speech Animation from a Single Mesh</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wampler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daichi</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SCA</title>
				<meeting>SCA</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">HMM trajectory-guided sample selection for photo-realistic talking head</title>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="9849" to="9869" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
