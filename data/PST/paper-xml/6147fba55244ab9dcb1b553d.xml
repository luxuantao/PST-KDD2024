<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Entity-Centric Questions Challenge Dense Retrievers</title>
				<funder>
					<orgName type="full">Apple and Amazon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-22">22 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Sciavolino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
							<email>zzhong@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
							<email>jinhyuklee@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Entity-Centric Questions Challenge Dense Retrievers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-22">22 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.08535v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entityrich questions based on facts from Wikidata (e.g., "Where was Arve Furset born?"), and observe that dense retrievers drastically underperform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent dense passage retrievers outperform traditional sparse retrieval methods like TF-IDF and BM25 <ref type="bibr" target="#b22">(Robertson and Zaragoza, 2009</ref>) by a large margin on popular question answering datasets <ref type="bibr" target="#b15">(Lee et al. 2019</ref><ref type="bibr" target="#b8">, Guu et al. 2020</ref><ref type="bibr" target="#b11">, Karpukhin et al. 2020</ref><ref type="bibr" target="#b28">, Xiong et al. 2021)</ref>. These dense models are trained using supervised datasets and the dense passage retriever (DPR) model <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> demonstrates that only training 1,000 supervised examples on top of BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>   <ref type="table">1</ref>: Top-20 retrieval accuracy for dense and sparse retrieval models on Natural Questions <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> and our EntityQuestions along with a set of sampled questions (full results in Appendix A). We test two DPR models: (1) trained on NQ only; (2) trained on 4 datasets (NQ, TQA, WebQ, TREC) combined. 2   In this work, we argue that dense retrieval models</p><p>are not yet robust enough to replace sparse methods, and investigate some of the key shortcomings dense retrievers still face. We first construct EntityQuestions, an evaluation benchmark of simple, entity-centric questions like "Where was Arve Furset born?", and show dense retrieval methods generalize very poorly. As shown in Table <ref type="table">1</ref>, a DPR model trained on either a single dataset Natural Questions (NQ) <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> or a combination of common QA datasets drastically underperforms the sparse BM25 baseline (49.7% vs 72.0% on average), with the gap on some question patterns reaching 60% absolute! Based on these results, we perform a deep dive into why a single dense model performs so poorly on these simple questions. We decouple the two distinct aspects of these questions: the entities and the question pattern, and identify what about these questions gives dense models such a hard time. We discover the dense model is only able to successfully answer questions based on common entities, quickly degrading on rarer entities. We also observe that dense models can generalize to unseen entities only when the question pattern is explicitly observed during training.</p><p>We end with two investigations of practical solutions towards addressing this crucial problem. First, we consider data augmentation and analyze the trade-off between single-and multi-task finetuning. Second, we consider a fixed passage index and fine-tune specialized question encoders, leading to memory-efficient transfer to new questions.</p><p>We find that data augmentation, while able to close gaps on a single domain, is unable to consistently improve performance on unseen domains. We also find that building a robust passage encoder is crucial in order to successfully adapt to new domains. We view this study as one important step towards building universal dense retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b30">Yang et al., 2019)</ref>. These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.</p><p>Dense retrieval On the contrary, dense models <ref type="bibr" target="#b15">(Lee et al., 2019;</ref><ref type="bibr" target="#b11">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b8">Guu et al., 2020)</ref> measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT. In this paper, we use the popular dense passage retriever (DPR) model <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> as our main evaluation, 3 and we also report the evaluation of REALM <ref type="bibr" target="#b8">(Guu et al., 2020)</ref> in Appendix A. DPR models the retrieval problem using two encoders, namely the question and the passage encoders, initialized using BERT. DPR uses a contrastive objective during training, with in-batch negatives and hard negatives mined from BM25. During inference, a pre-defined large set of passages (e.g., 21million passages in English Wikipedia) are encoded 3 The detailed experimental settings are in Appendix B. and pre-indexed-for any test question, the top passages with the highest similarity scores are returned. Recently, other advances have been made in improving dense retrieval, including incorporating better hard negatives <ref type="bibr" target="#b28">(Xiong et al., 2021;</ref><ref type="bibr" target="#b21">Qu et al., 2021)</ref>, or fine-grained phrase retrieval <ref type="bibr" target="#b28">(Lee et al., 2021)</ref>. We leave them for future investigation.</p><p>Generalization problem Despite the impressive in-domain performance of dense retrievers, their capability of generalizing to unseen questions still remains relatively under-explored. Recently, <ref type="bibr">Lewis et al. (2021a)</ref> discover that there is a large overlap between training and testing sets on popular QA benchmarks, concluding that current models tend to memorize training questions and perform significantly worse on non-overlapping questions. AmbER <ref type="bibr" target="#b2">(Chen et al., 2021)</ref> test sets are designed to study the entity disambiguation capacities of passage retrievers and entity linkers. They find models perform much worse on rare entities compared to common entities. Similar to this work, our results show dense retrieval models generalize poorly, especially on rare entities. We further conduct a series of analyses to dissect the problem and investigate potential approaches for learning robust dense retrieval models. Finally, another concurrent work <ref type="bibr" target="#b24">(Thakur et al., 2021)</ref> introduces the BEIR benchmark for zero-shot evaluation of retrieval models and shows that dense retrieval models underperform BM25 on most of their datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EntityQuestions</head><p>In this section, we build a new benchmark Enti-tyQuestions, a set of simple, entity-centric questions and compare dense and sparse retrievers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset collection</head><p>We select 24 common relations from Wikidata <ref type="bibr" target="#b26">(Vrande?i? and Kr?tzsch, 2014)</ref> and convert fact (subject, relation, object) triples into natural language questions using manually defined templates (Appendix A). To ensure the converted natural language questions are answerable from Wikipedia, we sample triples from the T-REx dataset <ref type="bibr" target="#b6">(Elsahar et al., 2018)</ref>, where triples are aligned with a sentence as evidence in Wikipedia. We select relations following the criteria: (1) there are enough triples (&gt;2k) in the T-REx; (2) it is easy enough to formulate clear questions for the relation; (3) we do not select relations with only a few answer candidates (e.g., gender), which may cause too many false negatives when we evaluate the retriever; (4) we include both person-related relations (e.g., place-of-birth) and non-person relations (e.g., headquarter). For each relation, we randomly sample up to 1,000 facts to form the evaluation set. We report the macro-averaged accuracy over all relations of EntityQuestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We evaluate DPR and BM25 on the En-tityQuestions dataset and report results in Table <ref type="table">1</ref> (see full results and examples in Appendix A). DPR trained on NQ significantly underperforms BM25 on almost all sets of questions. For example, on the question "Where was [E] born?", BM25 outperforms DPR by 49.9% absolute using top-20 retrieval accuracy. <ref type="foot" target="#foot_0">4</ref> Although training DPR on multiple datasets can improve the performance (i.e., from 49.7% to 56.7% on average), it still clearly pales in comparison to BM25. We note the gaps are especially large on questions about person entities.</p><p>In order to test the generality of our findings, we also evaluate the retrieval performance of REALM <ref type="bibr" target="#b8">(Guu et al., 2020)</ref> on EntityQuestions. Compared to DPR, REALM adopts a pre-training task called salient span masking (SSM), along with an inverse cloze task from <ref type="bibr" target="#b15">Lee et al. (2019)</ref>. We include the evaluation results in Appendix A. <ref type="foot" target="#foot_1">5</ref> We find that REALM still scores much lower than BM25 over all relations (19.6% on average). This suggests that incorporating pre-training tasks such as SSM still does not solve the generalization problem on these simple entity-centric questions.</p><p>In this section, we investigate why dense retrievers do not perform well on these questions. Specifically, we want to understand whether the poor generalization should be attributed to (a) novel entities, or (b) unseen question patterns. To do this, we study DPR trained on the NQ dataset and evaluate on three representative question templates: placeof-birth, headquarter, and creator.<ref type="foot" target="#foot_2">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dense retrievers exhibit popularity bias</head><p>We first determine how the entity [E] in the question affects DPR's ability to retrieve relevant passages. To do this, we consider all triples in Wikidata that are associated with a particular relation, and order them based on frequency of the subject entity in Wikipedia. In our analysis, we use the Wikipedia hyperlink count as a proxy for an entity's frequency. Next, we group the triples into 8 buckets such that each bucket has approximately the same cumulative frequency.</p><p>Using these buckets, we consider two new evaluation sets for each relation. The first (denoted "rand ent") randomly samples at most 1,000 triples from each bucket. The second (denoted "train ent") selects all triples within each bucket that have subject entities observed in questions within the NQ training set, as identified by ELQ <ref type="bibr" target="#b18">(Li et al., 2020)</ref>.</p><p>We evaluate DPR and BM25 on these evaluation sets and plot the top-20 accuracy in Figure <ref type="figure">1</ref>. DPR performs well on the most common entities but quickly degrades on rarer entities, while BM25 is less sensitive to entity frequency. It is also notable that DPR performs generally better on entities seen during NQ training than on randomly selected entities. This suggests that DPR representations are much better at representing the most common entities as well as entities observed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Observing questions helps generalization</head><p>We next investigate whether DPR generalizes to unseen entities when trained on the question pattern.</p><p>For each relation considered, we build a training set with at most 8, 000 triples. We ensure no tokens from training triples overlap with tokens from triples in the corresponding test set. In addition to using the question template used during evaluation to generate training questions, we also build a training set based on a syntactically different but semantically equal question template. 7 We fine-tune DPR models on the training set for each relation and test on the evaluation set of EntityQuestions for the particular relation and report results in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Clearly, observing the question pattern during training allows DPR to generalize well on unseen entities. On all three relations, DPR can match or even outperform BM25 in terms of retrieval accuracy. Training on the equivalent question pattern achieves comparable performance to the exact pattern, showing dense models do not rely on specific phrasing of the question. We also attempt fine-tuning the question encoder and passage encoder separately. As shown in Table <ref type="table" target="#tab_1">2</ref>, surprisingly, there is a significant discrepancy between only training the passage encoder (OnlyP) and only training the question encoder (OnlyQ): for example, on place-of-birth, DPR achieves 72.8% accuracy with the fine-tuned passage encoder, while it 7 place-of-birth: "What is the birthplace of [E]?"; headquarter: "Where is [E] headquartered?"; creator: "Who is the creator of [E]?". achieves 45.4% if only the question encoder is finetuned. This suggests that passage representations might be the culprit for model generalization.</p><p>To understand what passage representations have learned from fine-tuning, we visualize the DPR passage space before and after fine-tuning using t-SNE (Van der <ref type="bibr" target="#b25">Maaten and Hinton, 2008)</ref>. We plot the representations of positive passages sampled from NQ and place-of-birth in Figure <ref type="figure" target="#fig_1">2</ref>. Before fine-tuning, positive passages for place-of-birth questions are clustered together. Discriminating passages in this clustered space is more difficult using an inner product, which explains why only finetuning the question encoder yields minimal gains. After fine-tuning, the passages are distributed more sparsely, making differentiation much easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Towards Robust Dense Retrieval</head><p>Equipped with a clear understanding of the issues, we explore some simple techniques aimed at fixing the generalization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>We first explore whether fine-tuning on questions from a single EntityQuestions relation can help generalize on the full set of EntityQuestions as well as other QA datasets such as NQ. We construct a training set of questions for a single relation and consider two training regimes: one where we fine-tune on relation questions alone; and a second where we fine-tune on both relation questions and NQ in a multi-task fashion. We perform this analysis for three relations and report top-20 retrieval accuracy in Table <ref type="table" target="#tab_2">3</ref>.</p><p>We find that fine-tuning only on a single relation improves EntityQuestions meaningfully, but degrades performance on NQ and still largely falls behind BM25 on average. When fine-tuning on both relation questions and NQ together, most of the performance on NQ is retained, but the gains on EntityQuestions are much more muted. Clearly, fine-tuning on one type of entity-centric question does not necessarily fix the generalization problem for other relations. This trade-off between accuracy on the original distribution and improvement on the new questions presents an interesting tension for universal dense encoders to grapple with.</p><p>Specialized question encoders While it is challenging to have one retrieval model for all unseen question distributions, we consider an alternative approach of having a single passage index and adapting specialized question encoders. Since the passage index is fixed across different question patterns and cannot be adapted using fine-tuning, having a robust passage encoder is crucial.</p><p>We compare two DPR passage encoders: one based on NQ and the other on the PAQ dataset <ref type="bibr">(Lewis et al., 2021b)</ref>. 8 We expect the question encoder trained on PAQ is more robust because (a) 10M passages are sampled in PAQ, which is arguably more varied than NQ, and (b) all the plausible answer spans are identified using automatic tools. We fine-tune a question encoder for each relation in EntityQuestions, keeping the passage encoder fixed. As shown in Table <ref type="table">4</ref>, 9 finetuning the encoder trained on PAQ improves performance over fine-tuning the encoder trained on NQ. This suggests the DPR-PAQ encoder is more robust, nearly closing the gap with BM25 using a single passage index. We believe constructing a robust passage index is an encouraging avenue for 8 PAQ dataset sampling scheme is described in Appendix B. 9 Per-relation accuracy can be found in Appendix C. future work towards a more general retriever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we show that DPR significantly underperforms BM25 on EntityQuestions, a dataset of simple questions based on facts mined from Wikidata. We derive key insights about why DPR performs so poorly on this dataset. We learn that DPR remembers robust representations for common entities, but struggles to differentiate rarer entities without training on the question pattern. We suggest future work in incorporating entity memory into dense retrievers to help differentiate rare entities. Several recent works demonstrate retrievers can easily learn dense representations for a large number of Wikipedia entities <ref type="bibr" target="#b27">(Wu et al., 2020;</ref><ref type="bibr" target="#b18">Li et al., 2020)</ref>, or directly generate entity names in an autoregressive manner <ref type="bibr" target="#b4">(De Cao et al., 2021)</ref>. DPR could also leverage entity-aware embedding models like <ref type="bibr">EaE (F?vry et al., 2020)</ref> or LUKE <ref type="bibr" target="#b29">(Yamada et al., 2020)</ref> to better recall long-tail entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Our proposed dataset, EntityQuestions, is constructed by sampling (subject, relation, object) triples from Wikidata, which is dedicated to the public domain under the Creative Commons CC0 License. In general, machine learning has the ability to amplify biases presented implicitly and explicitly in the training data. Models that we reference in our study are based on BERT, which has been shown to learn and exacerbate stereotypes during training (e.g., <ref type="bibr" target="#b12">Kurita et al. 2019</ref><ref type="bibr" target="#b23">, Tan and Celis 2019</ref><ref type="bibr" target="#b20">, Nadeem et al. 2021)</ref>. We further train these models on Wikidata triples, which again has the potential to amplify harmful and toxic biases.</p><p>In the space of open-domain question answering, deployed systems leveraging biased pre-trained models like BERT will likely be less accurate or biased when asked questions related to stereotyped and marginalized groups. We acknowledge this fact and caution those who build on our work to consider and study this implication before deploying systems in the real world.</p><p>Table 5: Top-20 retrieval accuracy (percentage of retrieved passages that contain the answer) for dense and sparse retrieval models on different sets of questions of EntityQuestions. We test two DPR models: (1) trained on NQ only; (2) trained on 4 datasets (NQ, TQA, WebQ, TREC) combined.</p><p>A Full Results on EntityQuestions DPR vs. BM25 The evaluation results are shown in Table <ref type="table">5</ref>. BM25 significantly outperforms DPR models trained on either a single dataset NQ or a combination of common QA datasets.</p><p>REALM vs. BM25 We also evaluate he retrieval performance of REALM <ref type="bibr" target="#b8">(Guu et al., 2020)</ref> on En-tityQuestions. Specifically, we use REALM to retrieve 20 passages and check if the gold answer is a sub-string of the retrieved passages. We also evaluate BM25 on the same 288-token blocks that are used in REALM model. As shown in Table <ref type="table" target="#tab_3">6</ref>, the results show that REALM still significantly underperforms BM25 on EntityQuestions, even with the extra pre-training tasks.</p><p>Examples of DPR retrived passages Table <ref type="table">7</ref> shows examples of DPR retrieved results on three representative questions. DPR makes clear mistakes like confusing entities with similar names or missing the presence of an entity, causing it to retrieve irrelevant passages on these simple, entitycentric questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>Experimental settings of DPR In our experiments, we use either pre-trained DPR models released by the authors, or the DPR models re-trained by ourself (Table <ref type="table">4</ref>). All our experiments are carried out on 4? 11Gb Nvidia RTX 2080Ti GPUs. For all our fine-tuning experiments, we fine-tune for 10 epochs, with a learning rate 2 ? 10 -5 and a batch size of 24. When we retrain DPR from scratch, we train for 20 epochs with a batch size of 24 (the original DPR models were trained on 8? 32Gb GPUs with a batch size of 128 and we have to reduce the batch size due to the limited computational resources) and a learning rate of 2 ? 10 -5 .</p><p>Experimental settings of BM25 In our experiments, we use the Pyserini <ref type="bibr">(Lin et al., 2021)</ref> implementation of unigram BM25 with default parameters. We build an index using the same Wikipedia passage splits provided in the official DPR release.</p><p>PAQ dataset sampling <ref type="bibr">Lewis et al. (2021b)</ref> introduce Probably Asked Questions (PAQ), a large question repository constructed using a question generation model on Wikipedia passages. We</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Top-20 retrieval accuracy on (a) place-of-birth questions, (b) creator questions, grouped by the entity's frequency in Wikipedia. We group entities into 8 buckets according to their frequency in Wikipedia. rand ent: randomly selected entities from Wikidata; train ent: entities in the NQ training set.</figDesc><graphic url="image-2.png" coords="3,299.00,70.87,204.55,104.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of positive passage embeddings returned by DPR before and after fine-tuning on the placeof-birth questions. (a): Positive passage embeddings returned by DPR trained on NQ; (b) Positive passage embeddings returned by DPR after fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>top-20 retrieval accuracy on En-tityQuestions. We fix the passage encoder and finetune the question encoder. Per-relation FT: fine-tuning an individual question encoder for each relation, En-tityQuestions FT: fine-tuning on all questions in Enti-tyQuestions. ? : we re-train the models from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>already outperforms BM25, making it very appealing in practical use.</figDesc><table><row><cell></cell><cell cols="3">DPR DPR BM25</cell></row><row><cell></cell><cell cols="2">(NQ) (multi)</cell><cell>-</cell></row><row><cell>Natural Questions</cell><cell cols="3">80.1 79.4 64.4</cell></row><row><cell>EntityQuestions (this work)</cell><cell cols="3">49.7 56.7 72.0</cell></row><row><cell>What is the capital of [E]?</cell><cell cols="3">77.3 78.9 90.6</cell></row><row><cell>Who is [E] married to?</cell><cell cols="3">35.6 48.1 89.7</cell></row><row><cell>Where is the headquarter of [E]?</cell><cell cols="3">70.0 72.0 85.0</cell></row><row><cell>Where was [E] born?</cell><cell cols="3">25.4 41.8 75.3</cell></row><row><cell>Where was [E] educated?</cell><cell cols="3">26.4 41.8 73.1</cell></row><row><cell>Who was [E] created by?</cell><cell cols="3">54.1 57.7 72.6</cell></row><row><cell>Who is [E]'s child?</cell><cell cols="3">19.2 33.8 85.0</cell></row><row><cell>(17 more types of questions)</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row></table><note><p>Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-20 retrieval accuracy on EntityQuestions test sets after fine-tuning. FT: fine-tuning on each individual question pattern. w/ similar: fine-tuning on a similar, semantically equivalent question pattern. On-lyP and OnlyQ: fixing the weights of the question encoder and only updating the passage encoder, or vice versa.</figDesc><table><row><cell></cell><cell cols="3">p-of-birth headquarter creator</cell></row><row><cell>DPR-NQ</cell><cell>25.4</cell><cell>70.0</cell><cell>54.1</cell></row><row><cell>FT</cell><cell>73.9</cell><cell>84.0</cell><cell>80.0</cell></row><row><cell>FT w/ similar</cell><cell>74.7</cell><cell>79.9</cell><cell>76.2</cell></row><row><cell>FT OnlyP</cell><cell>72.8</cell><cell>84.2</cell><cell>78.0</cell></row><row><cell>FT OnlyQ</cell><cell>45.4</cell><cell>72.8</cell><cell>73.4</cell></row><row><cell>BM25</cell><cell>75.3</cell><cell>85.0</cell><cell>72.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-20 retrieval accuracy on NQ and Enti-tyQuestions. FT: fine-tuning. Rel: the performance on the relation that is used during fine-tuning.</figDesc><table><row><cell></cell><cell cols="3">NQ Rel EntityQ.</cell></row><row><cell>DPR-NQ</cell><cell cols="2">80.1 25.4</cell><cell>49.7</cell></row><row><cell>+ FT p-of-birth</cell><cell cols="2">62.8 74.3</cell><cell>56.2</cell></row><row><cell>+ FT NQ ? p-of-birth</cell><cell cols="2">70.8 52.0</cell><cell>47.4</cell></row><row><cell>DPR-NQ</cell><cell cols="2">80.1 70.0</cell><cell>49.7</cell></row><row><cell>+ FT headquarter</cell><cell cols="2">71.6 80.3</cell><cell>53.3</cell></row><row><cell cols="3">+ FT NQ ? headquarter 75.1 81.3</cell><cell>49.5</cell></row><row><cell>DPR-NQ</cell><cell cols="2">80.1 54.1</cell><cell>49.7</cell></row><row><cell>+ FT creator</cell><cell cols="2">70.8 80.8</cell><cell>52.3</cell></row><row><cell>+ FT NQ ? creator</cell><cell cols="2">72.6 72.3</cell><cell>44.1</cell></row><row><cell>BM25</cell><cell>64.4</cell><cell>-</cell><cell>72.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Top-20 retrieval accuracy of REALM and BM25 on different sets of questions of EntityQuestions. In this table, BM25 and REALM both only check for the answer in the passage text, excluding the title.</figDesc><table><row><cell>Relation</cell><cell>REALM BM25</cell></row><row><cell>P36 What is the capital of [E]?</cell><cell>91.7 91.9</cell></row><row><cell>P407 Which language was [E] written in?</cell><cell>81.9 92.0</cell></row><row><cell>P26 Who is [E] married to?</cell><cell>47.1 90.0</cell></row><row><cell>P159 Where is the headquarter of [E]?</cell><cell>70.4 90.7</cell></row><row><cell>P276 Where is [E] located?</cell><cell>77.1 89.5</cell></row><row><cell>P40 Who is [E]'s child?</cell><cell>39.7 87.1</cell></row><row><cell>P176 Which company is [E] produced by?</cell><cell>69.2 83.2</cell></row><row><cell>P20 Where did [E] die?</cell><cell>61.9 89.2</cell></row><row><cell>P112 Who founded [E]?</cell><cell>77.3 85.5</cell></row><row><cell>P127 Who owns [E]?</cell><cell>73.6 84.4</cell></row><row><cell>P19 Where was [E] born?</cell><cell>52.9 90.3</cell></row><row><cell>P740 Where was [E] founded?</cell><cell>50.9 77.5</cell></row><row><cell>P413 What position does [E] play?</cell><cell>53.8 90.4</cell></row><row><cell>P800 What is [E] famous for?</cell><cell>45.3 81.9</cell></row><row><cell>P69 Where was [E] educated?</cell><cell>38.6 84.1</cell></row><row><cell>P50 Who is the author of [E]?</cell><cell>77.2 76.2</cell></row><row><cell>P170 Who was [E] created by?</cell><cell>56.8 78.5</cell></row><row><cell>P106 What kind of work does [E] do?</cell><cell>53.6 83.4</cell></row><row><cell>P131 Where is [E] located?</cell><cell>63.9 86.8</cell></row><row><cell>P17 Which country is [E] located in?</cell><cell>70.6 76.0</cell></row><row><cell>P175 Who performed [E]?</cell><cell>53.1 65.8</cell></row><row><cell>P136 What type of music does [E] play?</cell><cell>42.6 53.4</cell></row><row><cell>P264 What music label is [E] represented by?</cell><cell>53.2 55.3</cell></row><row><cell>P495 Which country was [E] created in?</cell><cell>34.8 24.8</cell></row><row><cell>Macro-Average</cell><cell>59.9 79.5</cell></row><row><cell>Micro-Average</cell><cell>59.5 79.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>For our entire analysis, we consider top-20 retrieval accuracy for brevity. However, trends still hold for top-1, top-5, and top-100 retrieval accuracy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>  5  We cannot directly compare the retrieval accuracy of REALM to DPR, as the REALM index uses 288 BPE token blocks while DPR uses 100 word passages.4 Dissecting the Problem: Entities vs.Question Patterns</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>The question templates for these relations are: place-ofbirth: "Where was [E] born?"; headquarter: "Where is the headquarters of [E]?"; creator: "Who was [E] created by?".</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the members of the Princeton NLP group for helpful discussion and valuable feedback. This research is supported by gift awards from <rs type="funder">Apple and Amazon</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Retrieved results from DPR-NQ Q: Where was Arve Furset born?</p><p>Gold: [Title: Arve Furset] Arve Furset Arve Eilif Furset (born 5 December 1964 in Askvoll, Western Norway) is a Norwegian composer, jazz musician (piano, keyboards) and music producer Top-1: [Title: Gard Agdi] Gard Agdi ("Old Norse" Gar?r Ag?i) appears in the legendary genealogies of "Hversu Noregr byggdist" as one of the three sons of . . . Top-2: [Title: Yrsa] kidnapped the queen for a while during which time he made her pregnant. Having returned to her kingdom, the queen bore a child, a girl which she named Yrsa after her dog. Yrsa was sent. . . Top-3: [Title: Arvid Noe] Sailor" and the anagram "Arvid Noe" to conceal his identity; his true name, Arne Vidar R?ed, became known after his death. R?ed began his career as a sailor in 1961, when he. . .  group all of the questions asked about a particular passage and filter out any passages that have less than 3 generated questions. We then sample 100K such passages and sample one question asked about each. We split this dataset into 70K/15K/15K for train/dev/test splits, although we do not evaluate on this dataset. Following <ref type="bibr" target="#b11">Karpukhin et al. (2020)</ref>, we use BM25 to mine hard negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Per-relation Accuracy with Different Passage Encoders</head><p>We fine-tune DPR with the passage encoder fixed on either NQ or PAQ. Table 8: Top-20 retrieval accuracy on NQ and EntityQuestions (EQ). Per-rel FT: we fine-tune an individual question encoder for each relation. EQ FT: we fine-tune a single question encoder on all relations in EntityQuestions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling of the question answering task in the yodaqa system</title>
		<author>
			<persName><forename type="first">Petr</forename><surname>Baudi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>?ediv?</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24027-5_20</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating entity disambiguation and the role of popularity in retrieval-based NLP</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Gudipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.345</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4472" to="4485" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entities as experts: Sparse memory access with entity supervision</title>
		<author>
			<persName><surname>Thibault F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4937" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">REALM: Retrieval-augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver, Canada. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dense representations of phrases at scale</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6634" to="6647" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2021a. Question and answer test-train overlap in open-domain question answering datasets</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1000" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021b. PAQ: 65 million probably-asked questions and what you can do with them</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2102.07033</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient one-pass end-to-end entity linking for questions</title>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6433" to="6441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2102.10073</idno>
		<title level="m">Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations</title>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.466</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Elisa</forename><surname>Celis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13209" to="13220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<idno>abs/2104.08663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Abhishek Srivastava, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end open-domain question answering with BERTserini</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
	<note>Relation DPR DPR BM25 (NQ) (multi</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<idno>54.1 57.7 72.6</idno>
		<title level="m">P170 Who was</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
