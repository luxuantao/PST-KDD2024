<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Agent Deep Reinforcement Learning for Urban Traffic Light Control in Vehicular Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Wu</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<email>panzhou@hust.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Kai</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yali</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiumin</forename><surname>Wang</surname></persName>
							<email>xmwang@scut.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Huawei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Dapeng</forename><forename type="middle">Oliver</forename><surname>Wu</surname></persName>
							<email>wu@ece.ufl.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Information and Communication Engineer-ing</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Hubei Engineering Research Center on Big Data Security</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Chongqing University</orgName>
								<address>
									<postCode>400040</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Göttingen University</orgName>
								<address>
									<addrLine>37077 Goldschmidtstraße 7</addrLine>
									<settlement>Göttingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Agent Deep Reinforcement Learning for Urban Traffic Light Control in Vehicular Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F852C3DE9911DB130BC40CA353451035</idno>
					<idno type="DOI">10.1109/TVT.2020.2997896</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TVT.2020.2997896, IEEE Transactions on Vehicular Technology</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Traffic Light Control</term>
					<term>Deep Reinforcement Learning</term>
					<term>Deep Deterministic Policy Gradient Algorithm</term>
					<term>Markov Decision Process</term>
					<term>Vehicular Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As urban traffic condition is diverse and complicated, applying reinforcement learning to reduce traffic congestion becomes one of the hot and promising topics. Especially, how to coordinate the traffic light controllers of multiple intersections is a key challenge for multi-agent reinforcement learning (MARL). Most existing MARL studies are based on traditional Q-learning, but unstable environment leads to poor learning in the complicated and dynamic traffic scenarios. In this paper, we propose a novel multi-agent recurrent deep deterministic policy gradient (MARDDPG) algorithm based on deep deterministic policy gradient (DDPG) algorithm for traffic light control (TLC) in vehiclar networks. Specifically, the centralized learning in each critic network enables each agent to estimate the policies of other agents in the decision-making process and each agent can coordinate with each other, alleviating the problem of poor learning performance caused by environmental instability. The decentralized execution enables each agent to make decisions independently. We share parameters in actor networks to speed up the training process and reduce the memory footprint. The addition of LSTM is beneficial to alleviate the instability of the environment caused by partial observable state. We utilize surveillance cameras and vehicular networks to collect status information for each intersection. Unlike previous work, we have not only considered the vehicle but also considered the pedestrians waiting to pass through the intersection. Moreover, we also set different priorities for buses and ordinary vehicles. The experimental results in a vehicular network show that our method can run stably in various scenarios and coordinate multiple intersections, which significantly reduces vehicle congestion and pedestrian congestion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>W ITH the rapid development of the social economy and the improvement of people's material and spiritual life, the city is constantly developing and expanding. And private cars are becoming more and more common, which bring great pressure on traffic problems <ref type="bibr" target="#b0">[1]</ref>. Simultaneously, traffic congestion will also bring unnecessary resource loss and environmental pollution, and it will also increase the probability of traffic accidents. The most direct way to alleviate traffic congestion is to control traffic lights. Traditional fixed time traffic light control algorithms <ref type="bibr" target="#b1">[2]</ref> become inefficient with respect to alleviating congestion in a highly dynamic traffic environment. The longest queue first traffic light controls algorithm control traffic lights simply according to the length of the queue, and it still cannot solve the congestion at multiple intersections. Manual traffic control is sometimes effective, but it is a waste of manpower and time. Thus, we need computers to be able to learn to make a reasonable decision by interacting with the environment.</p><p>In recent years, with the sustainable development of the Internet and wireless communication, the acquisition of various data has become more convenient and faster than before. Rational use of various traffic data is also the key to alleviating traffic congestion. Among them, the Internet of Things (IoT) <ref type="bibr" target="#b2">[3]</ref>, as a key component of the new generation of information technology, plays an important role in our lives, where vehicular network is a branch of the IoT. Vehicular ad hoc networks (VANETs) <ref type="bibr" target="#b3">[4]</ref> offer direct communication between vehicles (V2V) and between vehicles and infrastructure (V2I). And vehicles can send and receive hazard warnings or current traffic information with minimal latency in VANETs. In this regard, we can utilize VANETs to capture real-time vehicle information and road condition information for traffic light control (TLC), similar to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>As the recent advance of communication and computational sensing, there are new opportunities for developing intelligent transportation. Many researchers have proposed various adaptive traffic control systems (ATCS). The traffic lights can be reasonably changed according to the current traffic congestion situation, such as SCOOT <ref type="bibr" target="#b6">[7]</ref>, SCATS <ref type="bibr" target="#b7">[8]</ref>, OPAC <ref type="bibr" target="#b8">[9]</ref>, RHODES <ref type="bibr" target="#b9">[10]</ref>. These systems have been applied in hundreds of cities around the world. Some researchers have applied neuro-fuzzy <ref type="bibr" target="#b10">[11]</ref>, neural networks <ref type="bibr" target="#b11">[12]</ref> and genetic algorithms <ref type="bibr" target="#b12">[13]</ref> to ATSC. As we know, genetic algorithms and neural networks require a lot of computation. Moreover, there exists premature convergence in genetic algorithms. And neurofuzzy is not suitable for complex or diverse intersections, as mentioned in <ref type="bibr" target="#b13">[14]</ref>. In contrast, the reinforcement learning algorithm is more concise and efficient for TLC.</p><p>In previous works, reinforcement learning <ref type="bibr" target="#b14">[15]</ref> has been widely used in robot control and game theory. Traffic light control process can be defined as MDP <ref type="bibr" target="#b15">[16]</ref>. Thus many researchers have applied reinforcement learning to TLC. However, the computational complexity of the traditional reinforcement learning system grows exponentially with the increase of the state space. The combination of the fast-developing deep learning and traditional reinforcement learning further solve this problem, i.e., Deep Reinforcement Learning (DRL) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The biggest difference from classic reinforcement learning is that it directly handles the pixel-level super-raw raw image state input instead of abstracting the state into a lowdimensional state. Although the DRL algorithms has achieved good results in single intersection scenarios, there are many problems when they are applied to multiple intersections. In the real world, it will make more sense to directly control multiple intersections with complex traffic dynamics.</p><p>In the case of multiple intersections, congestion at different intersections is different and the congestion levels at multiple intersections will affect each other. Each intersection cannot fully sense the traffic conditions of other intersections. This is because each agent can only observe part of the traffic situation, which will lead to a non-stationary environment. Many researchers have begun to apply the multi-agent reinforcement learning (MARL) algorithms <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> to TLC, so how to coordinate multiple traffic light controllers is a major challenge. However, the centralized algorithms <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> slow down the convergence of the entire system and are not suitable for practical traffic control. Although the decentralized algorithms <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> is robust and has a fast convergence speed, only part of the state of the adjacent intersection is considered in the training and execution process, so that the global optimal policy may not be learned. Moreover, the previous work did not combine the real traffic condition that not only the vehicles want to pass, but also the pedestrians want to pass at each intersection. And the number of passengers on the bus can reach several times that on other vehicles, treating the two as the same kind of vehicle is obviously unfair.</p><p>As noticed, when the scale of the traffic network grows, it will inevitably lead to insufficient utilization of the state of the intersection. And the area around local intersections will often have an impact on the traffic control of these junctions. In this paper, we mainly consider and focus on the mediumscale urban traffic light control in a vehicular network. The main contributions of our method are as follow:</p><p>1) We propose a novel MARDDPG algorithm to solve traffic congestion at multiple intersections. The centralized learning in each critic network enables each agent to estimate the policies of other agents in the decision-making process. The decentralized execution enables each agent to make decisions independently. Every intersection can compete or collaborate with each other, and ultimately reach the global optimal policy. We also utilize LSTM <ref type="bibr" target="#b26">[27]</ref> to capture hidden state information. Moreover, we shares parameter in actor networks to speed up the training process and reduce the memory footprint.</p><p>2) Since there are a large number of pedestrians in urban or commercial areas, sometimes traffic lights controlled by the state of the vehicle will cause long waiting time and inconvenience of pedestrians <ref type="bibr" target="#b27">[28]</ref>. To tackle this problem, we also take the total waiting time of pedestrians crossing the road into consideration. 3) Different from previous research, in order to make our model more realistic with traffic problems in the real world, we no longer regard all vehicles as the same priority. Buses are used as urban public transport, the number of passengers is more than the number of passengers on ordinary vehicles. Therefore, we give buses a higher priority to pass. 4) In order to make full use of various state information, we have defined more comprehensive states and rewards to improve learning. 5) We collect spatio-temporal traffic information via a vehicular network to better coordinate the cooperation among multiple agents, thereby controlling traffic lights at multiple intersections to obtain an optimal traffic light control policy. The rest of this paper is organized as follows. In Section II, we discuss the related work. The background on reinforcement learning is presented in Section III. In Section IV, we introduce system model and problem formulation. In Section V, we define the deep reinforcement learning model of vehicular networks. Section VI shows the details of our algorithm for TLC. We evaluate our method in Section VII. Finally, we conclude this paper in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>For complex traffic conditions, reinforcement learning methods can solve traffic congestion problems more effectively. The Q-learning <ref type="bibr" target="#b28">[29]</ref> algorithm is a model-free and off-policy reinforcement learning algorithm, and it is very suitable for TCL. However, in Q-learning algorithm <ref type="bibr" target="#b4">[5]</ref>, all Q-values are stored in a table, the rows and columns of the table are states and actions respectively. When our states and actions are very large, the Q-table will be so large that the memory of computer cannot support such a large Q-table. Therefore, many researchers have applied the Deep Q-learning (DQN) algorithm <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> to TLC at a single intersection. And the Deep Recurrent Q-learning (DRQN) algorithm <ref type="bibr" target="#b32">[33]</ref> has been proposed to solve traffic congestion under partially observable scenario. However, considering only one intersection in real world is definitely not enough. Since multiple intersections will affect each other, many researchers have proposed applying multi-agent DRL algorithm <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> to TLC, such as <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>.</p><p>The MARL algorithms used to solve traffic congestion at multiple intersections can be divided into three categories. The first category is fully isolated multi-agent reinforcement learning, which assumes that each agent is independent of each other, such as <ref type="bibr" target="#b36">[37]</ref>. Only observing the local traffic conditions and controlling the traffic lights at local intersection is simple, but it lacks cooperation among agents. Thus, it is difficult to converge to the optimal joint policy. The second category is multi-agent reinforcement learning with partial state cooperation, such as <ref type="bibr" target="#b24">[25]</ref>. This method often takes into account part of the state information of adjacent intersections, and combines the local traffic state and partial state information of neighboring agents to control local traffic lights. It will achieve better result than the first category. Only the partial state of the neighboring agent is considered and the intrinsic relationship between the actions is neglected. And most of the MARL algorithms are based on Q-learning, but the multiagent Q-learning algorithms do not perform well in multiagent scenarios. Moreover, each agent still uses the greedy algorithm to select action independently and the final policy may not converge to the balanced joint policy. The third category is multi-agent reinforcement learning of action joints, such as <ref type="bibr" target="#b23">[24]</ref>, which represents the actions of partial or all agents as a joint vector. Although this method fully considers the state of partial or all agents, which may lead to curse of dimensionality. Thus, this method is rarely used. The state, reward, and action set in these works mentioned above are different. In general, there are two types of action settings, one is the fixed phase sequence <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref> and the other is the variable phase sequence <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Although the variable phase sequence is more flexible, that will increase the possibility of traffic accidents caused by the uncertainty of the next phase, thus it is not suitable for the real world. In this paper, we apply advanced multi-agent deep reinforcement learning algorithm to control traffic lights of multiple intersections. Our work takes into account that the number of pedestrians in real traffic is also one of our optimization goals, in which buses should have higher priority than ordinary cars. In addition, we also define new states and rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM MODEL AND PROBLEM FORMULATION</head><p>In this paper, we apply deep reinforcement learning to control traffic lights in a vehicular network, thereby alleviating traffic congestion at multiple intersections. We collect traffic information at intersections via a vehicular network to control the traffic lights at each intersection according to different environments. As shown in Fig. <ref type="figure">1</ref>, assume that each intersection is a four-way intersection, and each road contains three lanes. The innermost lane is the left-turn lane, the middle is the straight lane, and the outermost is the straight and rightturn lanes. Every vehicle passing through the intersection will be controlled by traffic lights. And there will be a fixed period of yellow light between the green light and red light to ensure traffic safety. The left part of Fig. <ref type="figure" target="#fig_1">2</ref> shows the basic deep reinforcement learning model. We control the traffic flows by adjusting the duration of the red and green lights to minimize traffic congestion, and we get a state and feedback from the environment after each adjustment of the traffic light phase. We use two deep neural networks to estimate the Q-value and select action respectively. Before each selection action, we need to synthesize various aspects of the state information.</p><p>In the case of multiple intersections, we need to consider the global information to minimize total congestion at all intersections.</p><p>To reduce traffic congestion, the problem is how to make use of previous experience and the current intersection environment to make a reasonable decision on the duration of the red or green light. We deploy an agent at each intersection. Therefore, the problem can be defined as a multi-agent problem. We use "R" and "G" to indicate red and green lights respectively, and "E", "S", "W", and "N" to indicate East, South, West, and North respectively. For the situation that the congestion at different intersections is different and there will be interactions among the intersections, the coordination between intersections is important. Therefore, it is our ultimate goal to reduce global traffic congestion by coordinating multiple agents in combination with spatio-temporal relationships. The meaning of each notation is shown in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP REINFORCEMENT LEARNING MODEL</head><p>In order to solve traffic congestion in vehicular networks, we need to establish a model of deep reinforcement learning. We treat each intersection as an agent, and each agent only controls the local traffic light. First, we need to define the single intersection state s t , agent action a t and reward r t at discrete time steps, t = 0, 1, 2, ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intersection State</head><p>For a single intersection, we define the state using the location of the vehicle in each lane, the velocity of the vehicle, the phase of the traffic light, the queue length of each lane, and the number of pedestrians waiting to pass. To represent the location of each vehicle at the intersection, we divide each lane into discrete cells of equal length. The setting of the length of the discrete cell is also very important. If the setting is too long, the position information of the vehicle will be inaccurate. If the setting is too short, the state space will be too large. Thus we set the length of the discrete unit to be slightly larger than the average length of the ordinary vehicle. As shown in Fig. <ref type="figure">1</ref>, we assume that vehicles other than buses are ordinary vehicles. Each cell can only accommodate one ordinary vehicle. One bus occupies two cells. We use the position matrix P to indicate the position of all the vehicles at the intersection. We put the position matrix of the road on the same line together, which is beneficial to exploit the correlation among the roads. Since there are 4 roads at each intersection, the matrix P is given by:</p><formula xml:id="formula_0">P =     P 1 P 3 P 2 P 4     . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Matrix P i represents the position matrix of the vehicles on the i-th road. If a vehicle is present at a cell, the corresponding position of the matrix is set to 1, otherwise, it is 0. Similarly, the velocity matrix V is given by:</p><formula xml:id="formula_2">V =     V 1 V 3 V 2 V 4     . (2)</formula><p>Matrix V i represents velocity matrix of the vehicles on the i-th road. If a vehicle is present at a cell, the corresponding position of the matrix is set to the velocity of the vehicle, otherwise, it is 0. For the number of pedestrians, we use a vector M = (M SN , M EW ), where M SN indicates the number of the pedestrians who are going to pass from south to north </p><formula xml:id="formula_3">light SN-G 1 0 0 0 SNL-G 0 1 0 0 EW-G 0 0 1 0 EWL-G 0 0 0 1</formula><p>and from north to south, M EW indicates the number of the pedestrians who are going to pass from west to east and from east to west. As shown in Table <ref type="table" target="#tab_1">II</ref>, we use one-hot-coding of length 4 to indicate the traffic light phase L, which will be mentioned in the next subsection. Since there are three lanes in each direction and the direction in which each lane is allowed to pass is different. Thus, the length of the queue cannot be counted by naive averaging. The queue length matrix D can be expressed as:</p><formula xml:id="formula_4">D =     D 11 D 12 D 13 D 21 D 22 D 23 D 31 D 32 D 33 D 41 D 42 D 43     .<label>(3)</label></formula><p>In Equ. ( <ref type="formula" target="#formula_4">3</ref>), D ij represents the queue length on the j-th lane of the i-th road (1 ≤ i ≤ 4, 1 ≤ j ≤ 3). We can collect vehicle position information and velocity information for each intersection via a vehicular network. And we can also utilize the snapshot of surveillance cameras deployed at intersections combined with CrowdCount <ref type="bibr" target="#b37">[38]</ref> in the computer vision field to get the number of pedestrians. In summary, we define the intersection state as s t = (P, V, D, L, M ) at the discrete time step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Agent Action</head><p>Each agent selects the action a t at each time step and receive a new state S t+1 . According to traffic rules, vehicles can make a right turn whether it is a red or green light. Thus, we consider that there are only four phases of the traffic light: (a) N to S and S to N, (b) N to E and S to W (turn left), (c) E to W and W to E, (d) E to S and W to N (turn left). Pedestrians waiting to pass from north to south or from south to north can only pass through the intersection at phase (a), and pedestrians waiting to pass from east to west or from west to east can only cross the intersection at phase (c). As shown in the right part of Fig. <ref type="figure" target="#fig_1">2</ref>, each phase lasts for a time step and is cycled in the fixed order of (a) (b) (c) (d). When the duration of a phase exceeds t max , the phase of the next time step is forced to switch. Therefore, our action set is A = {0, 1}. If the agent chooses an action a t = 0, it means that the current traffic light phase is kept unchanged. And the other choice means to switch the current traffic light phase to the next phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reward</head><p>In order to make each agent to gradually learn to make the most reasonable decision on traffic lights, it is essential to choose the appropriate reward function as feedback of the action, for the reasonable reward has a guiding effect on each decision. The task of each agent is to minimize the congestion at the local intersection by taking appropriate policy. The total time taken by the vehicle to enter the road until it passes through the intersection (i.e., vehicle travel time) is a significant metric of the traffic light control policy. However, if the total travel time is used as feedback, it may cause delayed reward, which is obviously unreasonable. Thus we can use other metrics instead of the vehicle travel time as the reward. Assume that when the vehicle velocity is less than v min , the vehicle is considered as waiting. We define the following several rewards:</p><p>(1) The sum of the queue lengths D ij of all the roads leading to the intersection, i.e., the total number of vehicles waiting for all roads. The reward R D of queue length can be given by:</p><formula xml:id="formula_5">R D = 4 i=1 3 j=1 D ij .<label>(4)</label></formula><p>(2) The total waiting time for each vehicle on all roads at the intersection. When the driver is waiting, the driver's impatience is not proportional to the waiting time, that is, as the waiting time increases, the driver's impatience index rises. Therefore, in order to ensure the fairness of the vehicle and avoid a small number of vehicles waiting too long, we divide the waiting time of vehicles into three categories, that is, no waiting, waiting for less than 6 time steps and waiting for more than 6 time steps. The number of vehicles at the intersection is N t , and W j,t indicates the cumulative waiting time of vehicle j at time step t, where 1 &lt; j &lt; N t . R W indicates the reward of vehicle waiting time.</p><formula xml:id="formula_6">W j,t = W j,t-1 + ∆t, vehicle speed &lt; v min 0, other ,<label>(5)</label></formula><formula xml:id="formula_7">r j,t =    0, W j,t = 0 -W j,t , ∆t ≤ W j,t ≤ 6∆t -2W j,t , W j,t ≥ 7∆t ,<label>(6)</label></formula><formula xml:id="formula_8">R W = 1 N t Nt 1 r j,t .<label>(7)</label></formula><p>(3) The system delay represents the difference between the actual travel time of the vehicle and the travel time when the vehicle travels at the maximum velocity allowed. To simplify the expression, we can use the following equation to represent the system delay. d n represents the delay of vehicle j, v j represents the velocity of vehicle j, and v max represents the maximum velocity allowed to arrive at the road. The reward R s of system delay can be given by:</p><formula xml:id="formula_9">R s = 1 N t Nt 1 d j = 1 N t Nt 1 1 - v j v max . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>(4) The total number N c of vehicles that passed the intersection in a time step ∆t. The reward R c of the vehicles passing the intersection can be given by:</p><formula xml:id="formula_11">R c = N c .<label>(9)</label></formula><p>(5) We define the blinking condition of the traffic light as C. When C = 1, it indicates the current phase has changed. When C = 0, it indicates the current phase has not changed.</p><p>The setting of C is to prevent the flickering of the traffic light, for the frequent flashing traffic lights may reduce vehicle throughput and increase the likelihood of a traffic accident.</p><p>(6) The total waiting time of pedestrians waiting to pass in all directions at the intersection. M t indicates the number of pedestrians waiting to pass at time step t. The method of calculating r k,t is similar to Equ. <ref type="bibr" target="#b4">(5)</ref> and Equ. <ref type="bibr" target="#b5">(6)</ref>, where 1 &lt; k &lt; M t . And the reward R w of pedestrian waiting time can be given by:</p><formula xml:id="formula_12">R w = 1 M t Mt 1 r k,t .<label>(10)</label></formula><p>Different rewards have different importance. In other words, different rewards have distinct effects on each agent's policy. We multiply the above rewards by different weights and add them as our final reward. And we consider that buses and ordinary vehicles have different passing priorities, thus the total reward r is defined by the following equation:</p><formula xml:id="formula_13">r t = W 1 * R D + W 2 * R W + W 3 * R s + W 4 * R c + (1 + α) (W 2 * R W + W 3 * R s + W 4 * R c ) + W 5 * C + W 6 * R w ,<label>(11)</label></formula><p>where R W , R s , R c represent the reward of ordinary vehicles, R W , R s , R c indicates the reward of the bus. And α is the weighting factor of the bus reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DEEP REINFORCEMENT LEARNING ALGORITHM FOR TRAFFIC SIGNAL CONTROL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The MARDDPG Algorithm</head><p>In this paper, we propose the MARDDPG algorithm to control traffic lights at multiple intersections, inspired by <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The MARDDPG algorithm is the extension of the DDPG algorithm on the multi-agent condition. Considering that the traffic flow and people flow are time-continuous, we can utilize this time continuity to capture more potential information and the correlation among various state information. Experiments in <ref type="bibr" target="#b38">[39]</ref> show that adding recurrency to the Q-network is beneficial to Q-network to obtain more potential information from the current and previous partial observations, so as to better estimate the Q-value.</p><p>Inspired by the DRQN algorithm <ref type="bibr" target="#b38">[39]</ref> and RDPG algorithm <ref type="bibr" target="#b39">[40]</ref> proposed by previous researchers, we propose a Multi-agent Recurrent Deep Deterministic Policy Gradient Algorithm (MARDDPG) to TLC, our algorithm model is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. In our proposed method, no additional exchange of information and the communication channel for each agent. In order to prevent the environment from being non-stationary due to the fact that each agent does not know the policies of other agents during training process, each agent utilizes global states and actions during training process so that the policy of the partner or competitor can be estimated. Therefore, each agent can adjust the local policy according to the estimated policy of other agents to achieve the global optimal policy. In our method, since each agent is distributed in different spatial locations, each agent learns a distinct policy. Therefore, each agent has its own actor network and critic network. During off-line training process, each critic network can receive the observation o t,i and action a t,i of other agents at time step t, thus the Q-function can be given by Q ∅i i (o t,1 , . . . , o t,N , a t,1 , . . . , a t,N ) for all agent i ∈ {1, ..., N }. This solved the problem that the algorithm is difficult to achieve convergence caused by the non-stationary environment. Suppose there are N agents, and π i represents the policy of agent i, then: </p><formula xml:id="formula_14">P (o</formula><formula xml:id="formula_15">π i = π N . (<label>12</label></formula><p>) However, each actor network only selects the action according to the local observation and its own policy</p><formula xml:id="formula_16">µ θi i : O → A: a t,i = µ θi i (o t,i ) . (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>In this way, each agent can make independent decisions during on-line testing without critic network, namely centralized learning and distributed execution. During the training process, each critic updates the critic network by minimizing the private loss function L (∅ i ) parameterized by ∅ = {∅ 1 , . . . , ∅ N }:</p><formula xml:id="formula_18">L (∅ i ) = E st,at Q ∅i i (S t , a t ) -y t 2 ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_19">y t = r t + γQ ∅i i (S t+1 , a t )| at,i=µ θ i i (ot+1,i) ,<label>(15)</label></formula><p>S t = {o t,1 , . . . , o t,N } , a t = {a t,1 , . . . , a t,N } .</p><p>Each actor updates the actor network by maximizing the future expected cumulative rewards, i.e., updating the actor network target in the direction that maximizes the cumulative reward according to the objective function J (θ i ) parameterized by θ = {θ i , . . . , θ N }:</p><formula xml:id="formula_20">J (θ i ) = E st,at Q ∅i i (s t , a t )| at,i=µ θ i i (oi) .<label>(16)</label></formula><p>In order to effectively utilize the timing information, we extend the MADDPG algorithm <ref type="bibr" target="#b18">[19]</ref> into the MARDDPG algorithm, which adds the LSTM to the critic network and the actor network. h critic t and h actor t are the historical information in the critic network and the actor network, respectively. Each agent can select the action according to the previous state information: a t,i = µ θi i (h t,i ), and the Q-function becomes</p><formula xml:id="formula_21">Q ∅i i h critic t,1</formula><p>, . . . , h critic t,N , a t . Similarly, the loss function L (∅ i ) in the critic network becomes:</p><formula xml:id="formula_22">L (∅ i ) = E ht,at Q ∅i i h critic t,1 , . . . , h criric t,N , a t -y t,i 2 ,<label>(17)</label></formula><p>where  The objective function J (θ i ) and its gradient in the actor network become:</p><formula xml:id="formula_23">y t,i = r t,i + γQ ∅ i i h critic t+1,1 , . . . , h critic t+1,N , µ θ 1 1 h actor t+1,1 , . . . , µ θ N N h actor t+1,N . (<label>18</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">J (θ i ) = E ht,at Q ∅i i h critic t,1 , . . . , h critic t,N , µ θ1 1 h actor t,1 , . . . , µ θ N N h actor t,N ai=µ θ i i (h actor t,i</formula><p>) .</p><p>(19) During the training process, we store the experience trajectory in the replay buffer. At each training step, a minibatch of episodes is randomly sampled from the replay buffer instead of the expectation in the above equation to update the critic networks and actor networks simultaneously. ∅ and θ are parameters in the target-critic network and targetactor network, respectively, we use "soft updates" to update target networks, like <ref type="bibr" target="#b40">[41]</ref>. Then, our proposed MARDDPG algorithm of traffic light control for N intersections can be summarized as Algorithm 1. The detailed steps of the MARDDPG-TLC algorithm are as follows:</p><p>(1) The actor networks and critic networks initialize the parameterized policy µ θi i h actor t,i and the parameterized value function approximation</p><formula xml:id="formula_26">Q ∅i i h critic t,1</formula><p>, . . . , h critic t,N , a t,1 , . . . , a t,N respectively, all target networks are initialized with random weights θ i and ∅ i and the replay buffer is also initialized.</p><p>(2) In each episode, each agent selects action according to current policy µ θi i h actor t,i and a t,i = µ θi i h actor t,i + N t , where N t indicates exploration noise. Each agent receives feedback from the environment and new observation o t+1 after executing its own action. Then, the hidden layers of the LSTM in critic networks and actor networks are updated according to their respective rules.</p><p>(3) Store the current experience {o 1,i , a 1,i , r 1,i , o 2,i , a 2,i , r 2,i , . . .} in the replay buffer D as the data set for training actor networks and critic networks.</p><p>(4) For agent i, sample a random minibatch of S episodes o j 1,i , a j 1,i , r j 1,i , . . . for all i ∈ {1, . . . , N } from replay buffer D to train actor networks and critic networks <ref type="bibr" target="#b4">(5)</ref> For each agent, from t = T down to t = 1, for any sample j ∈ S, the critic estimates the Q-value aproximation</p><formula xml:id="formula_27">Q ∅i i h critic t,1</formula><p>, . . . , h critic t,N , a t,1 , . . . , a t,N , and calculates the TD error and updates its parameters ∅ i by minimizing the average loss function over the minibatch. Similarly, the actor calculates the policy gradient using the loss function, and the parameters θ i of actor network are updated using the averaged gradient over the minibatch.</p><p>(6) Update the parameters ∅ i and θ i of the target networks using current ∅ i and θ i , τ (0 &lt; τ &lt; 1) represents the rate at which the target network is updated according to the primary network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Structure</head><p>The network structure of the actor network is shown in the Fig. <ref type="figure" target="#fig_6">4</ref>, the input of the actor network is the local agent's observation o t,i = (P, V, D, L, M ). Since P , V , D, and L are matrices of different dimensions or vectors of different lengths. The five metrics should first be processed differently to extract features. First, we combine the position matrix and velocity matrix as a two-channel image, and the image is fed to a stacked sub-network. The first convolution layer of the sub-network contains 32 filters, where the size of each filter is 4 × 4 applied with strides (2, 2). The second convolution layer of the sub-network contains 64 filters, where the size of each filter is 2 × 2 applied with strides (2, 2). Similarly, the queue length matrix D is fed to a convolution layer containing 8 filters, where the size of each filter is 2×2 applied with strides (1, 1). We use two simple fully connected layers to encode the pedestrian vector M and traffic light phase vector L into a 8-dimensional vector respectively. Then, we concatenate the output of the above network into a vector. At last, we send this composite vector into the LSTM with 64 hidden units, and the output of the LSTM is fed to a fully connected layer with 16 output and a fully connected layer with two outputs. Finally, the last layer uses softmax as the activation function. Similarly, the network structure of the critic network is similar to the network structure of the actor network, but the input of the critic network includes not only the global states of all intersections but also the global actions of all agents. In the critic network, actions of all agents are concatenated with other feature vectors that the convolutional layers and the fully connected layers output as input to the LSTM. And the last layer is replaced by a fully connected layer with one output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Sharing in Actor Networks</head><p>Each agent has a unique actor network and critic network, and there are multiple agents in our model. This will result in a lot of parameters that need to be trained in the entire model. In order to reduce the number of trainable parameters in the training process, speed up training process and reduce the memory footprint, we propose to share parameters <ref type="bibr" target="#b20">[21]</ref> in the actor network or critic network of different agents. In general, sharing the parameters of all layers in the network end while store episode {o 1,i , a 1,i , r 1,i , o 2,i , a 2,i , r 2,i , . . .} for all i ∈ {1, . . . , N } 6:</p><formula xml:id="formula_28">for agent i = 1 to N do 7:</formula><p>sample a random minibatch of S episodes o j 1,i , a j 1,i , r j 1,i , . . . for all i ∈ {1, . . . , N } from replay buffer D 8:</p><p>for t = T down to 1 do 9: set y j t,i = r j t,i + γQ</p><formula xml:id="formula_29">∅ i i</formula><p>h critic,j , . . . , h critic,j t+1,N , µ </p><formula xml:id="formula_30">L (∅ i ) = 1 s j Q ∅i i h critic,j t,1</formula><p>, . . . , h critic,j t,N , a j t,1 , . . . , a j t,N -y j t,i 2 update actor using the sampled policy gradient: end for update the target networks:</p><formula xml:id="formula_31">∇ θi J (θ i ) ≈ 1 S j ∇ θi µ θi i h actor,j t,i ∇ ai Q ∅i i ( h critic,j t,1 , h critic,j t,N , µ θ1</formula><formula xml:id="formula_32">∅ i ← τ ∅ i + (1 -τ )∅ i θ i ← τ θ i + (1 -τ )θ i 12:</formula><p>end for will result in the same policy for each agent. The location and spatial relationship of each agent is different, thus the policy of each agent should not be identical. In the actor network and critic network, the lower layers are typically used to extract the features of the input state, while the higher layers are used to select the action and output the estimated Q-value. Therefore, the best option is to share the sub-networks of the actor networks or critic networks that is used to extract the features of each agent's state, so that each agent can do the same processing for input observation and can also develop a unique policy. We also to consider whether to share parameters between all agents for both the actor network and critic network networks or just one of them. In the study <ref type="bibr" target="#b20">[21]</ref>, the experimental results show that in the case of sharing only the critic networks, it is more likely that all agents have similar policies. This is because critics has a significant guiding effect on actors. Therefore, we only share some layers between the actor networks in the experiment, as shown in the Fig. <ref type="figure" target="#fig_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimal Policy</head><p>During training process, it is impossible for each agent to traverse all state spaces in a limited training episode, which may result in the agent not learning the optimal policy. Similar to the situation in the DQN algorithm, the agent only experiences limited intersection states. The agent may not be able to accurately estimate the Q-value of the unexperienced state. Therefore, we face a trade-off problem exploration and exploitation, that is, use the best policy we have learned before or exploring the new and possibly better policy. We adopt a simple method to balance the exploration and exploitation. When selecting the action according to the current policy, we introduce a random noise N according to a t,i = µ θi i h actor t,i + N t to select action, which can the coverage of learning and explore more potential optimal policy. In this paper, we use ADAptive Moment estimation (Adam) <ref type="bibr" target="#b41">[42]</ref> to optimize the critic networks and actor networks. The Adam algorithm has the advantages of both the AdaGrad and RMSProp algorithms. Adam not only calculates the adaptive parameter learning rate based on the first-order moment mean like the RMSProp algorithm, but also makes full use of the second-order moment mean of the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>In this section, we evaluate our method through simulation experiments and compare it with previous work. The simula-  </p><formula xml:id="formula_33">W 1 W 2 W 3 W 4 W 5 W 6 α -0.25 0.2 -1 1 -4 -0.5 -0.5</formula><p>tion results verify the efficiency of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment and Parameter Setting</head><p>The evaluation is on a simulation platform conducted in SUMO <ref type="bibr" target="#b42">[43]</ref> (Simulation of Urban Mobility). SUMO is a timediscrete, spatially continuous microscopic traffic simulation software that supports dynamic routing based on the right-side driving rules of road intersections. It also provides a visual graphical interface and supports multiple grid format inputs and various road network designs. Moreover, it can control the traffic lights at each intersection according to any given policy.</p><p>The shorter the distance between two intersections is, the stronger the correlation between them will be in the traffic network. Our experiments are carried out in two experimental scenarios, that is, a traffic network with two intersections and six (2×3) intersections with three lanes in each direction of the intersection. Each traffic light contains only four phases and cycles in a fixed phase sequence. Assume that each intersection scenario is a 480m × 480m area. The length of the ordinary vehicle is 4 meters, the length of the bus is 8 meters and the minimum distance between vehicles is 1.5 meters. Thus we set the cell length c to 6 meters, thus the dimensions of the position matrix P i and the velocity matrix V i are both 3×40. The straight-going vehicles arrival rate for each road is 0.2 vehicles per second, and the left-turning and right-turning vehicles arrival rate both are 0.05 vehicles per second. This traffic flow is close with the case in the real world where the flow rate of all through traffic and the turning traffic are not the same. The ratio between bus arrival rate and ordinary vehicle arrival rate is 1:10. And the destination of each vehicle on each lane is randomly set according to the discrete uniform distribution. The pedestrian arrival rate is set to 0.3 persons per second. The weights of various parameters and rewards are shown in Table <ref type="table" target="#tab_4">III</ref> and<ref type="table" target="#tab_6">Table IV</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compared Methods</head><p>To test the effectiveness of our model, we compare our method with the following baseline methods. All baselines use the same states, actions and rewards we defined above. And we assume that all vehicles are equipped with VANET technology.</p><p>1) Fixed-time Traffic Light Control (FT): Traffic lights are cycled in a fixed phase sequence <ref type="bibr" target="#b1">[2]</ref>. The duration of each phase is set first and does not change with traffic flow. It is the one of the most widely used traffic control policy in real world.</p><p>2) Self-Organizing Traffic Light Control (SOTL) <ref type="bibr" target="#b43">[44]</ref>: The states of traffic lights change directly according to the elapsed time and the number of vehicles in queues.</p><p>3) Independent Deep Q-learning (IDQN) <ref type="bibr" target="#b25">[26]</ref>: Each intersection is controlled by an agent, and the DQN algorithm is applied to find the optimal traffic light control policy. Specifically, there is no exchange of information between each agent and the decision is made independently according to the greedy algorithm.</p><p>4) Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b22">[23]</ref>: All intersections are controlled by centralized traffic light controller and the DDPG algorithm is used to find the optimal traffic light control policy.</p><p>In addition, in order to verify the effectiveness of the MARDDPG algorithm that incorporates the LSTM, we also consider a variant of our model without the LSTM, the algorithm uses the same network structure, i.e., Multi-agent Deep Deterministic Policy Gradient (MADDPG) algorithm. Similarly, we also tested the method of removing parameter sharing and optimal policy to verify the effectiveness of parameter sharing and policy. It is worth noting that the SOTL algorithm relies on processed traffic information such as the elapsed time and the number of vehicles in queues, while the DRL algorithms can take raw traffic information as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metric</head><p>We the average reward over time and all intersections as our reward. This reward is the weighted sum of each metric.  A smaller reward means there are worse performance. In addition, for a more comprehensive evaluation of the proposed algorithm, we also use the average vehicle queue length, average travel time, average delay and average number of pedestrians during the last 100 episodes as our evaluation metrics. The average vehicle queue length refers to the average sum of the vehicle queue lengths of all approaching lanes over time and all intersections. The average travel time is the average total time it takes a vehicle to pass through each intersection. The average delay is the average sum of the delays of all approaching lanes over time and all intersections. The average number of pedestrians refers to the average sum of the number of pedestrians waiting to pass at the intersection over time and all intersections. The smaller the values of the three metrics indicate the better the performance of the algorithm. Similarly, the reward of the SOTL algorithm fluctuates around -10, and the standard deviations of the rewards are greater. This is because the FT and SOTL algorithm cannot control the traffic lights in real time through continuous learning from environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results in</head><p>The reward of the IDQN algorithm has an upward trend during the first 1200 episodes, but the fluctuations are large. After 1200 cycles, the reward shows a sharp downward trend, and the reward for each episode is very unstable. That is because the traffic lights at each intersection in the IDQN algorithm are only controlled according to the local traffic conditions, so it is difficult to ensure that all intersections can learn the optimal policy. The curve of the DDPG algorithm rises slowly before 1100 episodes, and the final reward fluctuates within (-4,1). This is because the centralized traffic light controller in the DDPG algorithm considers the traffic conditions of multiple intersections simultaneously to update the joint policy. Thus the algorithm can slowly converge. We can see that the MARDDPG algorithm can learn the fastest among the five algorithms, and the final reward is stable around 2. That means our algorithm reaches the best policy and faster than others. Therefore, the MARDDPG algorithm performs significantly better than the other four algorithms in reducing traffic congestion.</p><p>2) Comparision with variant of our proposed method: The comparison of the results of the MARDDPG and MADDPG algorithms are shown in Fig. <ref type="figure" target="#fig_8">6</ref>. It can be seen from the figure that the general trend of the two curves is the same. When the training episodes over 600 times, the rewards of the two algorithms become more stable than previous episodes. However, the final reward of the MARDDPG algorithm is more stable and slightly higher than the MADDPG algorithm. This indicates that the adding LSTM enables agent to learn a better policy. The superscripts 1 and 2 represent scenario 1 and respectively in Table <ref type="table" target="#tab_8">VII</ref>. It can be seen from Table VII that after removing the parameter sharing, the total convergence time is increased by 500 episodes, and the average reward after convergence is also lower than the method without removing the parameter sharing. This shows that the use of parameter sharing can significantly accelerate the convergence time of our method. After removing the optimal strategy, the average reward is significantly reduced, which indicates that the optimal strategy may not be found. This is because the method after removing the optimal policy loses explorability and it is difficult to find the optimal policy.</p><p>Comprehensive evaluation metrics are shown in Table <ref type="table" target="#tab_6">V</ref>. We can see from the datas in the table that the DDPG, MADDPG and MARDDPG algorithms performed well in the scenario 1 during the testing phase, for they are based on the framework of the actor-critic algorithm. In the MARDDPG algorithm, the average vehicle queue length is reduced by 22.08% and 5.79% of the average vehicle queue length of the DDPG and MADDPG algorithms, respectively. And the average travel time is reduced by 10.81% of the average travel time of DDPG. This proves that under our method, the total time it takes for a vehicle to cross an intersection is significantly reduced. The average pedestrian number is reduced by 1.55% and 1.15% of the average pedestrian number of the DDPG and MADDPG algorithms respectively. Moreover, the standard deviation is also the smallest.</p><p>3) Comparision with varying penetration rates: In the real-world environment, not all vehicles are equipped with VANET technology (wireless transceivers), and some agents may also fail during operation, so the traffic light controllers at each intersection may not be able to detect partial vehicle information. Thus, we evaluate our methods under different penetration rates. Penetration rate refers to the proportion of vehicles that are VANET enabled. The experimental results are shown in Fig. <ref type="figure">8</ref>. Both algorithms performed well under the higher penetration rates in scenario 1, but the MADDPG algorithm performed significantly worse than the MARDDPG algorithm under the lower penetration rates. This is because the information of some vehicles is not available, making the environment become POMDP <ref type="bibr" target="#b44">[45]</ref>. Since the traffic states have time continuity, historical information can be remembered in LSTM. Thus the MARDDPG algorithm has better robustness than the MADDPG algorithm under the low penetration rate in scenario 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparision under peak hours:</head><p>In this part, we evaluate our method by comparing the performance under high pedestrian rate. The peak hours mean that the vehicle arrive rate and pedestrian rate are increased. Therefore, we set the pedestrian arrival rate to 0.5 persons per second. And the straight-going vehicles arrival rate for each road is set to 0.3 vehicles per second, and the left-turning and right-turning vehicles arrival rate both are set to 0.1 vehicles per second. The other experimental parameters remain unchanged. The experimental result is shown in Fig. <ref type="figure" target="#fig_13">11</ref>. It can be seen from the figure that the average reward of our algorithm is significantly higher than the other five algorithms. The average vehicle queue length, average number of pedestrians and average delay are at least 50%, 18% and 48% less than the other four algorithms respectively. That means that our algorithm can still show great results during peak hours in scenario 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results in Scenario 2</head><p>1) Comparision with baseline: In scenario 2, the simulation results are shown in Fig. <ref type="figure" target="#fig_9">7</ref>. The reward of the FT and SOTL algorithms fluctuate around -14 and -13 respectively, and the standard deviation is larger than that in scenario 1. The IDQN algorithm does not learn anything, for the changing environment causes the IDQN algorithm to not converge. The DDPG algorithm may converge after 2000 episodes, but the convergence speed is slow. Considering the limited computational cost, it was unnecessary to let it run indefinitely. Our MARDDPG algorithm gradually stabilizes after several fluctuations, which means that our algorithm finds the optimal policy after continuous trial and error. In addition, the reward of our algorithm is smaller than that in scenario 1 and the speed of convergence is slower, which indicates the optimal policy becomes harder to be learnt than scenario 1. However, our algorithm also performs significantly better than the other four algorithms across all episodes.</p><p>2) Comparision with variant of our algorithm: The comparison of the results of the MARDDPG and MADDPG algorithms is shown in Fig. <ref type="figure" target="#fig_11">9</ref>. Starting from the 900th episode, the advantages of the MARDDPG algorithm are apparent. The reward of the MARDDPG algorithm starts to be higher and more stable than the MADDPG algorithm This is because the MARDDPG algorithm has memory characteristics and the useful historical information can be remembered. Therefore, in the traffic network of six intersections, the addition of the LSTM is beneficial to capture potential information. As shown in Table VII, the convergence time of our method is significantly shorter than the method without the parameter sharing, and the average reward of our method is significantly longer than the method without the parameter sharing or the optimal policy. Therefore, the addition of the parameter sharing and optimal policy can improve the experimental results.</p><p>As can be seen from Table <ref type="table" target="#tab_7">VI</ref>, the average queue length, average delay, average travel time, and average number of pedestrians of the MARDDPG algorithm are lower than the corresponding values of the MADDPG algorithm by 4.68%, 9.57%, 3.46%, and 19.66% respectively. Since the traffic condition is more complicated, the standard deviations of these six algorithms are significantly larger than that in scenario 1, while it is also within the acceptance range. It is worth noting that the average reward for the MARDDPG algorithm is also lower than the average reward in scenario 1. Therefore, we also need to consider the impact of the number of intersections on the simulation results. Since each critic network takes the global state as input during training process, when there are more intersections, the state space will increase dramatically, which may lead to curse of dimensionality. Thus the MARD-DPG algorithm may not be suitable for large-scale traffic network. Fortunately, for the medium-scale traffic network in the experiment, our algorithm can still handle the traffic congestion problem significantly.</p><p>3) Comparision with varying penetration rates: As shown in Fig. <ref type="figure">8</ref>, the trend of fold line in scenario 2 are similar to that in scenario 1. As the penetration rate decreases, the MARDDPG algorithm is significantly more stable than the MADDPG algorithm. This means that the addition of LSTM allows our algorithm to run more stably in uncertain environments.</p><p>4) Comparision under peak hours: We use the same vehicle arrival rate and pedestrian arrival rate as that in scenario 1 under peak hours. The experimental results are shown in Fig. <ref type="figure" target="#fig_13">11</ref>. Although the average reward of our algorithm is slightly lower than that of scenario 1, it still outperforms the other five algorithms. The average vehicle queue length is at least 50% less than the other algorithms. The average delay and average number of pedestrians are at least 23% and 50% less than the other algorithms respectively. This indicates that our algorithm can still stably learn an excellent policy under complex traffic conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Scalability analysis</head><p>The performance of our method at two intersections and six intersections has been analyzed in the two subsections above. It can be seen from the results that our method shows excellent performance in both scenarios, but it is slightly worse in scenario 2. Therefore, we speculate that the effectiveness of our method may be affected by the scale of the traffic    We performed the same tests on 9 intersections, 12 intersections, and 14 intersections, 15 intersections, and 16 intersections using the same environmental configuration as before, that is, the traffic flow is the same as that in scenario 1 and scenario 2 with the penetration rate of 0.8.</p><p>As shown in Fig. <ref type="figure" target="#fig_12">10</ref>, we show the time it takes for our method to converge in different scenarios and the average reward after convergence. The dashed line in the figure indicates the number of episodes required for the method to converge, and the solid line indicates the average reward after convergence. The polyline of the IDQN and DDPG algorithms is not shown in the figure because the IDQN and DDPG algorithms cannot converge in scenarios where the number of intersections is greater than or equal to 6. It is experimentally obtained that our method cannot converge in the scenario of 16 intersections, so we only draw the scenario where the number of intersections is less than 16. Our method can converge in scenarios with less than 16 intersections, and the reward after convergence is still higher than the other two algorithms. This proves that our method has a good effect on the medium-scale (less than 16) traffic network. However, with the increase of traffic scale (more than 15), our method may not be able to handle. This is because as the scale of the traffic network increases, the state space and behavior space of each agent input will be larger, and the learning time will be longer. Moreover, when there are too many intersections, a single agent will consider the state of all other intersections. However, the state of intersections far from the local intersections does not actually have a greater impact on the policy of the local intersections, so too much useless information will lead to lower learning efficiency for each agent. All in all, our algorithm is more suitable for mediumscale (less than 16) traffic networks. In this paper, we propose the MARDDPG algorithm to reduce traffic congestion at multiple intersections. We utilize various road information based on vehicular networks' data collection to change the phase of multiple traffic lights in real time. The traffic light controller at each intersection is not isolated, for it can observe the global state during the training process. Each traffic light controller can estimate the traffic light control policies of other intersections when making decisions, thereby adjusting the local traffic light control policy to make optimal decisions. Moreover, in our TLC system, we also take pedestrian and bus into consideration, which make the whole system more humanized. Scalability analysis shows that our method is more suitable for medium-scale traffic networks. It can be concluded from the simulation results on SUMO that our algorithm can be applied to the TLC at multiple intersections with complicated road conditions, and can achieve good results even in a partially observable environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 V2IFig. 1 :</head><label>41</label><figDesc>Fig. 1: The upper left part shows a four-way intersection in our model. The lower left part shows the corresponding position matrix on this road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The left part shows a deep reinforcement learning model for traffic light control in the left part. The right part shows the phase transition diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: MARDDPG-TLC algorithm of traffic light control for N intersections.</figDesc><graphic coords="6,345.60,200.81,70.70,71.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>update critic by minimizing the loss:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The architecture of actor networks to select actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The reward during all the training episodes in scenario 1.</figDesc><graphic coords="9,59.29,56.07,230.40,165.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The rewards of the MARDDPG and MADDPG algorithms during all the training episodes in scenario 1.</figDesc><graphic coords="9,325.91,56.07,223.20,161.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The reward during all the training episodes in scenario 2.</figDesc><graphic coords="9,325.91,259.71,223.20,157.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Scenario 1 1 )Fig. 8 :</head><label>18</label><figDesc>Fig. 8: The average reward under different penetration rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The rewards of the MARDDPG and MADDPG algorithms during all the training episodes in scenario 2.</figDesc><graphic coords="10,325.91,56.07,223.20,162.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: The performance of our method at different numbers of intersections.</figDesc><graphic coords="11,48.96,56.07,252.00,163.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: The comparison of each evaluation metric under peak hours.</figDesc><graphic coords="12,54.00,339.86,504.00,154.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>0018-9545 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TVT.2020.2997896, IEEE Transactions on Vehicular Technology 13 VII. CONCLUSION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Notations</figDesc><table><row><cell>Notation</cell><cell>Meaning</cell></row><row><cell>Env</cell><cell>Environment</cell></row><row><cell>St</cell><cell>Globe state at time step t</cell></row><row><cell>o t,i</cell><cell>Observation of agent i at time step t</cell></row><row><cell>a t,i</cell><cell>Action of agent i at time step t</cell></row><row><cell>rt</cell><cell>Reward at time step t</cell></row><row><cell>∆t</cell><cell>Time interval between actions</cell></row><row><cell>N</cell><cell>The number of agents</cell></row><row><cell>P i</cell><cell>Position matrix of the i-th road</cell></row><row><cell>V i</cell><cell>Velocity matrix of the i-th road</cell></row><row><cell>M</cell><cell>The number of pedestrains</cell></row><row><cell>L</cell><cell>State of traffic light</cell></row><row><cell>D</cell><cell>Queue length matrix</cell></row><row><cell>ty</cell><cell>Yellow light duration</cell></row><row><cell>tmax</cell><cell>Maximum duration of each phase</cell></row><row><cell>W</cell><cell>Waiting time</cell></row><row><cell>d j</cell><cell>Delay of vehicle j</cell></row><row><cell>Nc</cell><cell>Number of vehicles passed the intersection</cell></row><row><cell>C</cell><cell>Blink condition of traffic light</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>The state of traffic light.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>t+1,i |o t,i ; Env) = P (o t+1,i |o t,i ; o t,1 , . . . , o t,N , a t,1 , . . . , a t,N , π 1 , . . . , π N ) = P (o t+1,i |o t,i ; o t,1 , . . . , o t,N , a t,1 , . . . , a t,N ) = P (o t+1,i |o t,i ; o t,1 , . . . , o t,N , a t,1 , . . . , a t,N , π 1 , . . . , π N ) for any</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Model parameter setting.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>v min</cell><cell>0.2 m/s</cell></row><row><cell>vmax</cell><cell>16.67 m/s</cell></row><row><cell>∆t</cell><cell>5 seconds</cell></row><row><cell>ty</cell><cell>3 seconds</cell></row><row><cell>tmax</cell><cell>90 seconds</cell></row><row><cell>γ</cell><cell>0.99</cell></row><row><cell>Sample size</cell><cell>64</cell></row><row><cell>Memory length</cell><cell>20000</cell></row><row><cell>Pre-training steps tp</cell><cell>2000</cell></row><row><cell>Target network update rate τ</cell><cell>0.001</cell></row><row><cell>Learning rate</cell><cell>0.0001</cell></row><row><cell>Simulation times per episode</cell><cell>300 seconds</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Reward coefficients.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Comprehensive evaluation in scenario 1. .34 ± 5.43 12.23 ± 8.15 3.09 ± 1.16 6.86 ± 5.83 46.12 ± 4.83 SOTL -10.53 ± 8.75 10.27 ± 12.19 4.14 ± 1.19 6.25 ± 6.12 42.49 ± 7.23 IDQN -4.84 ± 6.26 7.98 ± 7.21 3.57 ± 1.01 3.16 ± 4.93 49.63 ± 9.34 DDPG -3.03 ± 4.74 5.36 ± 5.98 3.19 ± 1.01 3.19 ± 4.00 45.88 ± 8.47 MADDPG 1.67 ± 3.29 4.09 ± 5.76 2.47 ± 0.89 1.17 ± 3.93 40.79 ± 6.89 MARDDPG 1.88 ± 3.19 3.04 ± 5.19 2.59 ± 0.89 1.13 ± 3.88 40.92 ± 5.16</figDesc><table><row><cell>Algorithm</cell><cell>Reward</cell><cell>Queue length</cell><cell>Delay</cell><cell>No. of pedestrians</cell><cell>Travel time</cell></row><row><cell>FT</cell><cell>-13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Comprehensive evaluation in scenario 2. .12 ± 6.19 13.23 ± 9.13 3.80 ± 1.35 8.37 ± 6.09 50.35 ± 7.46 SOTL -12.02 ± 8.97 11.76 ± 10.05 4.61 ± 1.21 7.10 ± 7.27 59.72 ± 12.68 IDQN -16.06 ± 9.78 15.26 ± 12.89 4.02 ± 1.33 9.77 ± 9.56 68.56 ± 16.13 DDPG -8.29 ± 7.63 9.89 ± 8.84 3.67 ± 1.38 5.24 ± 6.31 54.04 ± 10.76 MADDPG -2.99 ± 4.84 6.93 ± 6.89 3.62 ± 1.12 2.92 ± 4.81 48.24 ± 12.55 MARDDPG -1.87 ± 3.82 4.32 ± 5.87 3.27 ± 1.20 1.82 ± 4.29 46.57 ± 9.27</figDesc><table><row><cell>Algorithm</cell><cell>Reward</cell><cell>Queue length</cell><cell>Delay</cell><cell>No. of pedestrians</cell><cell>Travel time</cell></row><row><cell>FT</cell><cell>-14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Comparision with variant of our proposed method.</figDesc><table><row><cell>Method</cell><cell>Reward 1</cell><cell>Convergence episode 1</cell><cell>Reward 2</cell><cell>Convergence episode 2</cell></row><row><cell>Ours</cell><cell>1.88</cell><cell>1100</cell><cell>-1.87</cell><cell>1600</cell></row><row><cell>Without parameter sharing</cell><cell>1.14</cell><cell>1600</cell><cell>-3.02</cell><cell>&gt;2000</cell></row><row><cell>Without optimal policy</cell><cell>-4.79</cell><cell>1300</cell><cell>-13.25</cell><cell>1900</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University of Glasgow. Downloaded on June 04,2020 at 08:03:05 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported in part by the National Natural Science Foundation of China under Grant No. 61972448, 61872150, 61872049 and 61902445. It was also supported in part by Guangdong Basic and Applied Basic Research Foundation under Grant No.2020A1515011209. It was also supported by Fundamental Research Funds for the Central Universities of China (19lgpy222), and Natural Science Foundation of Guangdong Province of China (2019A1515011798).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sensitivity analysis of traffic congestion costs in a network under a charging policy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mussone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grant-Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Case Studies on Transport Policy</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Traffic signal settings</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Webster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Internet of things: A survey on enabling technologies, protocols, and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Fuqaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aledhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ayyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE communications surveys &amp; tutorials</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2347" to="2376" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A tutorial survey on vehicular ad hoc networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Laberteaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for traffic light control in vehicular networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed cooperative reinforcement learning-based traffic signal control that integrates v2x networks&apos; dynamic clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="8667" to="8681" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The scoot on-line traffic signal optimisation technique</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bretherton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Royle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Traffic Engineering &amp; Control</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The sydney cooridinated adaptive traffic (scat) systemprinciples, methodology, algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lowrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Road Traffic Signaling</title>
		<meeting>of International Conference on Road Traffic Signaling</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">OPAC: A demand-responsive strategy for traffic signal control</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Gartner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controlled optimization of phases at an intersection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Head</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning in neurofuzzy traffic signal control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="241" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural networks for realtime traffic signal control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Cheu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traffic signal optimization in &quot;la almozara&quot; district in saragossa under congestion conditions, using genetic algorithms, traffic microsimulation, and cluster computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sánchez-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Galán-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rubio-Royo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of intelligence methods in urban traffic signal control</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCSNS International Journal of Computer Science and Network Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: A brief survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">0172395. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cooperation and communication in multiagent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05438</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep deterministic policy gradient for urban traffic light control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Casas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09035</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning for integrated network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto</title>
		<author>
			<persName><forename type="first">S</forename><surname>El-Tantawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelgawad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1140" to="1150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning for traffic signal control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prabuchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2529" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cooperative deep reinforcement learning for traffic signal control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Intelligent traffic light control using distributed multi-agent q learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intellilight: A reinforcement learning approach for intelligent traffic light control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2496" to="2505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reinforcement learning with function approximation for traffic signal control</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="412" to="421" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using a deep reinforcement learning agent for traffic signal control</title>
		<author>
			<persName><forename type="first">W</forename><surname>Genders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Razavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01142</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive traffic signal control with deep recurrent q-learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1215" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coordinated deep reinforcement learners for traffic light control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Learning, Inference and Control of Multi-Agent Systems</title>
		<meeting>Learning, Inference and Control of Multi-Agent Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distributed traffic light control at uncoupled intersections with real-world topology by deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schutera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smolarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reischl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11233</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for intelligent transportation systems</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00979</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning for traffic light control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Seventeenth International Conference (ICML&apos;2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1151" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4031" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 AAAI Fall Symposium Series</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Memory-based control with recurrent neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04455</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sumo (simulation of urban mobility)-an open-source traffic simulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krajzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hertkorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rössel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th middle East Symposium on Simulation and Modelling</title>
		<meeting>the 4th middle East Symposium on Simulation and Modelling</meeting>
		<imprint>
			<date type="published" when="2002">20002. 2002</date>
			<biblScope unit="page" from="183" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-organizing traffic lights: A realistic simulation</title>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Cools</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gershenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>D'hooghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in applied self-organizing systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
