<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Robust Manipulation Strategies with Multimodal State Transition Models and Recovery Heuristics</title>
				<funder ref="#_fEZHyPc">
					<orgName type="full">Amazon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Austin</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Austin S. Wang is with</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Kroemer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Oliver Kroemer is with the Robotics Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon Univer-sity</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Robust Manipulation Strategies with Multimodal State Transition Models and Recovery Heuristics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robots are prone to making mistakes when performing manipulation tasks in unstructured environments. Robust policies are thus needed to not only avoid mistakes but also to recover from them. We propose a framework for increasing the robustness of contact-based manipulations by modeling the task structure and optimizing a policy for selecting skills and recovery skills. A multimodal state transition model is acquired based on the contact dynamics of the task and the observed transitions. A policy is then learned from the model using reinforcement learning. The policy is incrementally improved by expanding the action space by generating recovery skills with a heuristic. Evaluations on three simulated manipulation tasks demonstrate the effectiveness of the framework. The robot was able to complete the tasks despite multiple contact state changes and errors encountered, increasing the success rate averaged across the tasks from 70.0% to 95.3%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robots operating in unstructured environment will inevitably make mistakes. Mistakes are not desirable, but they are only a major problem if the robot cannot detect and recover from them autonomously. Usually the potential errors that a robot may encounter will not be known a priori, and thus the robot will have to learn strategies for recovering from errors on its own. Learning to recover from errors is however a challenging problem, as the robot needs to consider a variety of different recovery options when learning a robust policy. The robot may be able to continue from the current situation and try to reach the original goals via a different path, or it may need to backtrack an unknown number of steps to reattempt a part of the task. Detecting errors and performing recovery actions will allow the robot to perform the overall task more reliably.</p><p>Contacts play an important role in performing manipulation tasks and recovering from errors. Contacts constrain motions and changes in contact states often correspond to subgoals or errors in manipulation tasks, e.g., contacts for grasping or accidental collisions. By monitoring for changes in the contact state, the robot can detect subgoals and errors more reliably. Distinct contact states are modeled as a discrete set of contact modes, which will be leveraged in our framework to learn a more accurate representation of the task dynamics.</p><p>We propose a framwork that increases the robustness of a policy learned from demonstration. The robot optimizes the Fig. <ref type="figure">1</ref>: Illustration of the task structure for an insertion task. The purple circles are distributions of states with similar properties, the orange diamonds are skills, the green blocks are contact modes, and the arrows are transitions and skill selections (darker lines indicate higher probabilities). (Top) A demonstration of inserting the of the blue peg into the hole. (Middle) Executing the demonstrated skills cause unintended collisions, resulting in additional erroneous contact modes ? and ?. (Bottom) Optimizing the policy and generating recovery skills results in additional skills 3 and 6 for transitioning between state distributions to recover from errors.</p><p>policy based on a task representation that was learned from executions of the initial demonstrated skills. As illustrated in Fig. <ref type="figure">1</ref>, the framework is initialized with a demonstration of the desired task. The robot then repeatedly executes the demonstrated skills to detect different types of errors. The robot uses the resulting data, together with additional contact mode information extracted at the end of each skill, to construct a state transition model. The robot subsequently optimizes the policy based on the generated task model. To further increase the robustness of the policy, the robot utilizes the model structure to generate additional recovery skills over time. The resulting policy maximizes the success rate of the tasks by skipping unnecessary skills and autonomously recovering from errors.</p><p>We evaluated the proposed framework on three simulated manipulation tasks with a Sawyer robot. The experiments showed that incorporating the contact modes and recovery skills, instead of simply executing the demonstrated skills as given, lowered the mean task failure rate from 30.0% to 4.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The proposed framework learns a policy for sequencing skills to perform manipulation tasks. Skill chaining was proposed by Konidaris et al. <ref type="bibr" target="#b0">[1]</ref> as a framework that starts by learning skills to reach the goal states and then learns additional skills to reach the initiation sets of the current skills. In this manner, the framework grows skill trees from the goal outward. Robots often learn to perform complex tasks using hierarchical policies or sequences of skills. Some approaches assume that the individual skills are provided to the robot <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, while others focus on segmenting and learning skills <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The high-level policies for selecting skills have been learned using reinforcement learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and imitation learning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Contact modes are often used to model distinct contact states in manipulation tasks. The term modes is adopted from hybrid systems literature <ref type="bibr" target="#b10">[11]</ref>, where they correspond to distinct continuous system dynamics that the system can jump between. The modal structure of a task may be learned <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> or predefined. A number of previous planning approaches exploit the contact mode structure of manipulation tasks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. This structure can also be used as a basis for learning skills for transitioning between contact modes <ref type="bibr" target="#b11">[12]</ref>. Common mode transition skills include grasping and placing. The detection of mode transitions can be used to reduce uncertainty <ref type="bibr" target="#b17">[18]</ref>, or to determine if a subgoal or error has occurred <ref type="bibr" target="#b12">[13]</ref>. In our framework, the robot uses observations of the contact modes as auxiliary information for modeling the dynamics of the task.</p><p>The high dimensional state spaces and highly nonlinear dynamics in manipulation tasks have always posed a challenge for planning manipulation strategies. Koval et al. <ref type="bibr" target="#b18">[19]</ref> used a lattice in configuration space and performed planning by solving regions relevant to the task online. Other methods, similar to our approach, create simplified task representations by exploiting the structure of contact-rich manipulation tasks, e.g. aggregating similar states together <ref type="bibr" target="#b19">[20]</ref>, or creating abstract models based on transitions between sets of states <ref type="bibr" target="#b20">[21]</ref>. Our approach to creating the state transition model was inspired by <ref type="bibr" target="#b21">[22]</ref>, which showed that the state space for sequencing skills depends on the pre-and post-conditions of the skills.</p><p>The improved robustness of the proposed framework is the result of the robot incorporating additional recovery skills over time. The additional skills allow the robot to recover from errors and skip unnecessary skills in the task execution. Niekum et al. <ref type="bibr" target="#b3">[4]</ref> proposed a framework for learning low-level skills and high-level policies for selecting skills from demonstrations. Their framework also allowed the robot to identify errors in the task execution and incorporate additional demonstrations of recovery skills to create a more robust overall policy. Su et al. <ref type="bibr" target="#b12">[13]</ref> proposed a method for detecting when a skill failed to reach its goal state. The robot would then reattempt the same skill to recover from the error. Phillips-Grafflin et al. <ref type="bibr" target="#b19">[20]</ref> recovered from errors by assuming reversibility and repeating skills multiple times until they succeeded. In the proposed framework, the robot automatically generates additional skills for recovering to different states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STATE TRANSITION MODELS</head><p>The goal of the proposed framework is to learn a highlevel policy for chaining together low-level skills. The robot will begin by learning a state transition model based on the data collected while attempting the task using the initial demonstrated skills.</p><p>We assume a quasi-static environment, i.e. the state of the system can be fully defined by the poses of the arm and the manipulated objects after each skill execution. The task is then modeled as a continuous-state Markov Decision Process (MDP). The states are defined as the poses of the end-effector relative to the manipulated object, and are assumed to be fully observable.</p><p>The state transition distributions are only dependent on the current state and the executed actions. However, in the presence of contact mode switches, those distributions are hard to model using the pose information alone, as the effects of skills are strongly affected by the low-level action constraints imposed by the contact mode. A slight shift in pose might be the difference between contact or no contact, thus executing the same skill might have a completely different effect. Hence, contact mode observations will be used to provide additional structure to the transition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Exploratory Data Generation</head><p>To extract the structure of the manipulation task, we begin by providing the robot with a sequence of skills for performing the task. In our evaluations, the skills are defined as straight line Cartesian end-effector movements, and each skill a is parameterized by an end-effector goal pose g relative to the observed object frame. Although one could use more complex skill representations, e.g., DMPs <ref type="bibr" target="#b22">[23]</ref>, straight line movements are often well suited for low-level skills <ref type="bibr" target="#b12">[13]</ref>. Due to stochastic transitions and contact constraints, the endeffector pose after executing a will not be the same as g. The robot will encounter a variety of states as it explores the task, including erroneous states that the demonstrator had not intended as part of the task.</p><p>Each state will correspond to a contact mode which will impose a certain set of local dynamics and constraints. The robot can observe these constraints by applying low-level actions and observing the results. Hence, at the start and end of each skill execution, the robot performs a sequence of local 3D perturbations in the end-effector pose ?x d , which we assume to be reversible, and observes the resulting changes in the end-effector position ?x. Note that ?x d is the controller input in the form of the change in desired end-effector position. The robot then fits a linear model to this data ?x = A?x d . In our experiments, the off-diagonal elements tended to be close to zero, so the diagonal elements of the matrix A are used as contact mode observations o m = diag(A). In the future, we will explore more expressive representations of the local constraints. The auxiliary contact mode observation is then appended to the state to form an augmented state feature z t =&lt; s t , o m t &gt;. The robot attempts the task multiple times using the initial sequence of demonstrated skills. For each execution of a skill a t the robot receives an experience in the form of the tuple e t =&lt; z t , a t , r t , z t &gt;, with the current augmented state z t , the skill executed a t , the reward received r t , and the next augmented state z t . The experiences are stored in a buffer and used to learn state transition models in an offline fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transition Model</head><p>Given the exploratory dataset, the next step is to model the state transition distributions for the skills. Our skills have the property of acting as funnels in the state space: given a state within an input region of the state space, the skill execution will result in a transition to a state within a corresponding output region of the state space. Sequentially selecting skills can thus be seen as chaining together funnels to reach desired states <ref type="bibr" target="#b0">[1]</ref>. A funnel i is associated with an action a i , an input region ? i , and an output region ? i . In our model, input regions are represented as unimodal gaussian distributions ? i (z) = N (z|? i , ? i ). Due to the unpredictable nature of hybrid systems (e.g. a pushed bottle may slide or fall over), output regions are modeled as multimodal gaussian mixtures</p><formula xml:id="formula_0">? i (z ) = j w j N (z |? ij , ? ij ).</formula><p>The state transition model is then formed in a locally weighted fashion:</p><formula xml:id="formula_1">P (z |z, a) = i ? i (z )? i (z)?(a i , a) j ? j (z)?(a j , a)</formula><p>where a i is the skill associated with the ith funnel, and ? is the Kroenecker delta function that only takes a value of one if the two inputs are the same and zero otherwise. The input and output regions can thus be seen as a model of the skill's preconditions and corresponding postconditions <ref type="bibr" target="#b21">[22]</ref>.</p><p>Contact modes are discrete in nature, and thus an ideal model would be to incorporate them as discrete variables. These discrete mode values would be functions of the continuous state. However, the discontinuous mapping between the states and the contact modes is unknown and hard to learn, which is why the continuous contact mode observations are used in our model. The auxiliary mode observations allow the robot to better distinguish between regions of the state space with different contact constraints. Intuitively, we can decompose the input and output distributions as Cluster all x ? X using DBSCAN with D(x i , x j )</p><formula xml:id="formula_2">? i (z t ) = ? s i (s t )? m i (o m t )</formula><p>16:</p><p>return {Y 1 , Y 2 , ..., Y N }, where each Y i is a set containing all x's belonging to the ith cluster transition model are constructed by a clustering of states, similar to the symbolic state generation procedure proposed by Konidaris et al. <ref type="bibr" target="#b21">[22]</ref>. The clustering approach for the individual skills is illustrated in Fig. <ref type="figure">2</ref> and Algorithm 1. The robot begins by clustering the skill's samples according to the next states z . The clustering is performed using DBSCAN <ref type="bibr" target="#b23">[24]</ref>. The algorithm clusters together any samples z i , z j within a distance D z (z i , z j ) = s is j 2 + o m io m j 2 &lt; s , where s is a length scale hyperparameter with a value of 2. This process results in a set of output clusters ?. The samples for each of these output clusters is then clustered again based on the initial states z using DBSCAN. The resulting clusters ? correspond to distinct regions of the state space, and each of these clusters should be associated to one input Gaussian and one output Gaussian. The final step in our clustering approach merges clusters of samples with similar initial state distributions z. In particular, two state distributions ? i , ? j are merged if D ? (? i , ? j ) &lt; ? , where D ? is implemented with the Bhattacharyya distance <ref type="bibr" target="#b24">[25]</ref> and</p><formula xml:id="formula_3">? = 0.05.</formula><p>The resulting clusters of samples are subsequently used to compute parameters of the input distributions ? i (z) using maximum likelihood. To avoid slight mismatches between input distributions ? i across skills, we compare the distance measure D ? between all pairs of input distributions regardless of the associated skill, and redefine the input distributions using their combined samples for any pairs ? i , ? j that satisfies the same merging condition D ? (? i , ? j ) &lt; ? . The multimodal output distributions ? i (z ) could in theory also be approximated using the clustered samples. However, in practice, these distributions do not need to be computed explicitly to learn the policy, as we will explain in the following section.</p><p>An example of the clustering can be seen in Fig. <ref type="figure">3</ref> for a basic maze task. The resulting clusters allow the transition distribution to capture both the high-level skill transitions and the low-level constraints corresponding to the different contact modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. POLICY LEARNING</head><p>Given the state transition model, the next step is to learn a policy for selecting skills given the current states and contact mode observations. To further improve the robustness of the policy, we present a heuristic for incorporating additional recovery skills over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning a Policy for Selecting Skills</head><p>To learn a policy for selecting skills, we associate an expected reward r i with each skill a i and input region ? i pair, which is obtained by taking the weighted average over experiences</p><formula xml:id="formula_4">r i = t r t ? i (z t )?(a i , a t ) t ? i (z t )?(a i , a t )</formula><p>where we use t to iterate over all experiences. We then model the reward function as r(z, a) = i r i ? i (z)?(a i , a) j ? j (z)?(a j , a) where we use i and j to iterate over learned funnels.</p><p>Using this reward function and state transition model, the action value function can be written as:</p><formula xml:id="formula_5">Q(z, a) = r(z, a) + ? Z P (z |z, a) max a Q(z , a )dz = i q i ? i (z)?(a i , a) j ? j (z)?(a j , a)</formula><p>where q i = r i + ? Z ? i (z ) max a Q(z , a )dz . However, the Q-value function parameters q i cannot be computed analytically because integrating over all possible output states is intractable. We therefore treat the next state z t as a sample from ? i (z ), and compute the parameters q i using gradient descent with the update rule:</p><formula xml:id="formula_6">q i ? q i + ?[Q t -Q(z t , a t )] ? ?q i Q(z, a) zt,at</formula><p>where</p><formula xml:id="formula_7">Q t = r t + ? max a Q(z t , a )</formula><p>and</p><formula xml:id="formula_8">? ?q i Q(z, a) zt,at = ? i (z t )?(a i , a t )</formula><p>j ? j (z t )?(a j , a t ) Once the value estimates are computed, a softmax policy ?(a|z) = e Q(z,a) / k e Q(z,a k ) is used to select actions while continuing to explore state-action pairs in a structured manner. After every attempt of the task, the model is updated with the new data and value iteration is run until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generating Recovery Skills</head><p>After obtaining a policy for the initial set of skills, the robot will sometimes perform unneccessary or erroneous skill executions. These executions may result in transitions to states that were not seen in the original demonstrations, and the initial skills are therefore also not well-suited for handling these situations. To increase the robustness and performance of the overall policy, the robot will need to acquire additional recovery skills. These skills should be generated in an incremental manner and exploit the prior structure of the task model.</p><p>We propose a skill generation heuristic for backtracking to previous states and skipping immediate successor states. These skills will allow the robot to effectively undo the effects of erroneous skill executions. At each skill selection step, the robot attempts to generate and execute a new skill using the proposed heuristic with a probability of 0.1 in our evaluations.</p><p>Our skill generation heuristic is based on the funnel inputs ? i (z t ). We define the jth funnel to be a successor of the ith</p><formula xml:id="formula_9">funnel if t ? j (z t )? i (z t ) t k ? k (z t )? i (z t ) &gt; h</formula><p>with a h = 0.1, which corresponds to the probability of transitioning to z t when at z t marginalized over different skills. Conversely, if the jth funnel is a successor of the ith funnel, then the ith funnel is a predecessor of the jth funnel.</p><p>Given the current augmented state z t the current funnel is given by arg max i ? i (z t ). The robot then defines a set of candidate funnels that include the current funnel's predecessors and its successors' successors. The predecessor candidates allow the robot back track one step, while the successors' successors will allow the robot to skip erroneous or redundant actions.</p><p>Candidate funnels are removed from the candidate set if the robot already has a skill for transitioning from the current funnel to the candidate one. Candidates are also removed if the value of the current funnel q i is larger than the discounted value of the candidate q c , i.e., q i &gt; ?q c . The robot then selects the candidate with the highest value ? = arg max c q c and generates a corresponding skill with the goal parameters given by the target funnel's mean g = ?(? ? ).</p><p>As the robot generates new skills, the sets of successors and predecessors will also change and new candidates will be created as a result. For example, if the robot generates a skill for reaching a predecessor, then this funnel will also become part of the successors. The robot will subsequently attempt to create skills for reaching its successor's as part of the successor's successor rule. Similarly, if the robot manages to skip a skill and reach a successor's successor, the robot will then attempt to generate skills for skipping this next skill as well. In this manner, the heuristic allows the robot to incrementally generate skills for improving robustness and performance based on the task structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we explain how we evaluate the proposed approach and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>Three simulated tasks were used to evaluate the performance of the proposed framework using the Mujoco physics engine <ref type="bibr" target="#b25">[26]</ref>. First, the robot has to escape a 2D maze as shown in Fig. <ref type="figure">1</ref> where the rod end-effector remains at a fixed height above the table. The second task involved a key-shaped end-effector and a T-shaped hole. The goal of the task is to insert the key into the vertical part of the hole and then slide it to the end of the horizontal slot. In the final task, the robot is equipped with a gripper. The robot has to grasp a drawer handle, pull the handle down to release a latch, and then pull the drawer open. The task environments For each of the tasks we define a goal region G: the rod's tip is outside of the maze, the key is at the end of the horizontal slot, and the drawer is more than 10cm open. After each skill execution, the robot receives a reward r = -d -[s /</p><p>? G]?t, where ?t is the time duration of the skill, d is the distance travelled during the execution of the skill, and s is the resulting state. A 60-second time limit is also imposed on all tasks, at which point episodes terminate automatically. The arm is controlled using an Cartesian impedance controller. We simulated action noise using a zero-mean Gaussian with a standard deviation of 1cm.</p><p>To evaluate the effects of incorporating recovery skills, the robot was initially provided with a sequence of 6, 4, and 6 skills to perform each of the tasks respectively. The robot then generated an MDP model with state transition estimates of the task as described in Section III. The robot was then given an additional 40, 100, and 100 episodes for each of the Maze, Keyhole, and Drawer tasks respectively. During these episodes, the robot was allowed to generate additional skills using the heuristic described in Section IV. We designate checkpoints for evaluation right after learning the initial MDP model and after every 25 episodes of skill generation. At each checkpoint, we evaluate performance by freezing the policy and attempting the task 100 times. No new skills were generated and no additional training data was acquired during these evaluation attempts. The success rates are shown in the top row of Fig. <ref type="figure">5</ref>. The success rates are averaged over 10 runs of the experiment. Fig. <ref type="figure">6</ref> shows the success rates as a function of time.</p><p>To evaluate the effects of incorporating the contact modes, we reran the experiment without using the contact mode information z m in the state clustering process. Results of running the same experiments are on the right of Fig. <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>As shown in the left column of Fig. <ref type="figure">5</ref>, implementing the additional recovery skills increased the mean success rate of the three tasks from 70.0%, 62.0%, and 78.0% to 96.1%, 94.6%, and 95.1% respectively. Perfect success rates were even achieved in at least one experiment run for all three tasks using the recovery skills. Fig. <ref type="figure">7</ref> illustrates an episode of the execution of the drawer task in which the arm was able to recover from multiple erroneous states by utilizing newly acquired recovery skills. The importance of contact mode information is emphasized in the right column of Fig. <ref type="figure">5</ref>: Success rates for the three simulated manipulation tasks (Left) with contact mode information and (Right) without contact mode information. The success rate is evaluated over 100 test episodes and averaged over 10 training trials. Results from executing the demonstration skills, using the initial MDP policy without recovery skills, and using the MDP policy augmented with recovery skills are colored as red, green, and blue respectively. Fig. <ref type="figure">6</ref>: Cumulative plot of success rate over time. The percentage corresponds to how often the robot solves the task before the given time. The discrete jumps in percentages are due to the fact that skills usually require a constant amount of time to execute, and thus episodes involving the same combination of skills will terminate at approximately the same time. Fig. <ref type="figure">7</ref>: Visualization of an execution of the drawer task. In the fourth image, the handle slipped away during the process of pulling the drawer out, but since the latch is already out of the socket, the manipulator uses learned recovery skills to swoop back to pull the drawer out without grabbing it. Fig. <ref type="figure">5</ref>, where the resulting performance exhibits no obvious improvement over naively executing the demonstration skills.</p><p>More insight into the learned strategies can be gained by looking at how long it takes for the manipulator to complete the tasks in Fig. <ref type="figure">6</ref>. Executing the fixed demonstration skill sequence results in the episodes terminating at the same time regardless of whether the task has been completed. Directly after forming the policy based on the state transition model, the algorithm often finds shortcuts by directly executing the next skill. However, there are times when the shortcut is more risky and leads to a decrease in success rate, as in the Keyhole case. After generating recovery skills, the agent not only discovers more shortcuts and optimizes paths better due to the additional data, but also generates skills to recover from errors, which results in much higher success rates and shorter completion times.</p><p>The set of distributions ? i 's is currently based on exploring the task with the initial demonstration skills. The performance could therefore potentially be further increased by continuously adapting the state distributions over time as the robot acquires more data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We introduced a framework for learning more robust policies for performing contact-based manipulation tasks under uncertainty. An initial set of skills is used to explore the task and subsequently extract a suitable state abstraction based on observed transitions and contact modes. We proposed a heuristic for generating additional recovery skills based on the learned task model. The robot subsequently learned a policy for selecting skills using value iteration. The framework was successfully evaluated on three simulated manipulation tasks and lowered the failure rate averaged over all three tasks from 30.0% to 4.7%.</p><p>In the future, we will explore modeling the tasks as Partially Observable Markov Decision Processes (POMDPs) <ref type="bibr" target="#b26">[27]</ref> to handle observation uncertainties. We will also explore contact mode estimation methods based on sensory signals during skill executions <ref type="bibr" target="#b12">[13]</ref>, or by actively perturbing the end-effector to maximize information gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Conceptual illustration of the clustering process. (A) Raw data. (B) Cluster output states. (C) Cluster initial states corresponding to each output cluster. (D) Merge initial states.</figDesc><graphic url="image-6.png" coords="4,52.25,239.47,238.83,85.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Task Environments. (Left) Maze. (Middle) Keyhole. (Right) Drawer.</figDesc><graphic url="image-7.png" coords="5,304.31,88.27,238.85,70.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) capture the effects of lowlevel actions. Both of these transition types are important for creating a robust transition model.Given the general form of the transition model, the next step is to extract the input and output regions. The robot learns a model of the regions by clustering the exploratory data points for the individual skills. The components of the Algorithm 1 Learning Funnels in State Space Experiences: &lt; z t , a t , r t , z t &gt;, t ? {1, ..., N } Set of skills (action space): A where a t ? A ?t Distance measure between states: D z (z i , z j ) Distance measure between distributions: D ? (? i , ? j ) 1: ? result ? ? 2: for all a ? A do CLUSTER({z i |a i == a}, D z ) CLUSTER({z j |z j ? ?}, D z )</figDesc><table><row><cell>and similarly ? i (z t ) = ? s i (s t )? m i (o m t ), i (s) and ? s where ? s i (s ) capture the effects of high-level skills and ? m i (o m ) and ? m i (o m 3: 4: 5: ? ? 6: ? ? ? // Cluster output states for all ? ? ? do 7: // Cluster respective initial states 8: ? ? 9: ? ? ? ? ? 10: // Merge similar initial state clusters 11:</cell></row></table><note><p><p>P ? CLUSTER({ ?| ? ? ?}, D ? ) 12: ? result ? ? result ? {</p>??? ?|? ? P } 13: return ? result 14: function CLUSTER(X, D) 15:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_0"><p>International Conference on Robotics and Automation (ICRA) Palais des congres de Montreal, Montreal, Canada, May 20-24, 2019 978-1-5386-6027-0/19/$31.00 ?2019 IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>*This work received funding from <rs type="funder">Amazon</rs> through the <rs type="programName">Amazon Research Award program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fEZHyPc">
					<orgName type="program" subtype="full">Amazon Research Award program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skill discovery in continuous reinforcement learning domains using skill chaining</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS), Y. Bengio</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards associative skill memories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Righetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Movement segmentation using a primitive library</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental semantically grounded learning from demonstration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niekum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osentoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental learning of subtasks from unsegmented demonstration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Grollman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="261" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autonomous learning of high-level states and actions in continuous environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development (TAMD)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical relative entropy policy search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">93</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning parameterized motor skills on a humanoid robot</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="page" from="5239" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to sequence movement primitives from demonstrations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Manschitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gienger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4414" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robot learning from demonstration by constructing skill trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuindersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grupen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Introduction to Hybrid Dynamical Systems, ser</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Schaft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schumacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards learning hierarchical skills for multi-phase manipulation tasks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1503" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to switch between sensorimotor primitives using multimodal haptic signals</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sukhatme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 14: 14th International Conference on Simulation of Adaptive Behavior</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9825</biblScope>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Programming by demonstration -constructing task level plans in hybrid dynamic framework</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Mccarragher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA). Symposia Proceedings (Cat. No.00CH37065)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1402" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical planning for multi-contact non-prehensile manipulation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Prez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient hierarchical robot motion planning under uncertainty and hybrid dynamics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niekum</surname></persName>
		</author>
		<idno>abs/1802.04205</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient planning for nearoptimal compliant manipulation leveraging environmental contact</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><forename type="middle">;</forename><surname>Vega-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interleaving motion in contact and in free space for planning under uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sieverling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017-09">Sept 2017</date>
			<biblScope unit="page" from="4011" to="4073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Configuration lattices for planar contact manipulation under uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Koval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<idno>abs/1605.00169</idno>
		<ptr target="http://arxiv.org/abs/1605.00169" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Planning and resilient execution of policies for manipulation in contact with actuation uncertainty</title>
		<author>
			<persName><forename type="first">C</forename><surname>Phillips-Grafflin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<idno>abs/1703.10261</idno>
		<ptr target="http://arxiv.org/abs/1703.10261" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grasping pomdps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2007 IEEE International Conference on Robotics and Automation</title>
		<meeting>2007 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="4685" to="4692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From skills to symbols: Learning symbolic representations for abstract high-level planning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="215" to="289" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning attractor landscapes for learning motor primitives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1547" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Partially Observable Markov Decision Processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T J</forename><surname>Spaan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="387" to="414" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
