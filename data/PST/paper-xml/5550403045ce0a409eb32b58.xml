<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">System identification via sparse multiple kernel-based regularization using sequential convex optimization techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
							<email>tschen@isy.liu.se</email>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
						</author>
						<author>
							<persName><roleName>Life Fellow, IEEE</roleName><forename type="first">Lennart</forename><surname>Ljung</surname></persName>
							<email>ljung@isy.liu.se.m.s.andersen</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Gianluigi</forename><surname>Pillonetto</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Chiuso</surname></persName>
							<email>chiuso@dei.unipd.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<address>
									<addrLine>Linköping Uni-versity</addrLine>
									<postCode>58183</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Ap-plied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">System identification via sparse multiple kernel-based regularization using sequential convex optimization techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67AF8E9586C0CBF966612CD871B9BD57</idno>
					<idno type="DOI">10.1109/TAC.2014.2351851</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014 Limited circulation. For review only Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014 Limited circulation. For review only Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014 4 Limited circulation. For review only Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014 12</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>System identification</term>
					<term>regularization methods</term>
					<term>kernel methods</term>
					<term>convex optimization</term>
					<term>sparsity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model estimation and structure detection with short data records are two issues that receive increasing interests in System Identification. In this paper, a multiple kernel-based regularization method is proposed to handle those issues. Multiple kernels are conic combinations of fixed kernels suitable for impulse response estimation, and equip the kernel-based regularization method with three features. First, multiple kernels can better capture complicated dynamics than single kernels. Second, the estimation of their weights by maximizing the marginal likelihood favors sparse optimal weights, which enables this method to tackle various structure detection problems, e.g., the sparse dynamic network identification and the segmentation of linear systems. Third, the marginal likelihood maximization problem is a difference of convex programming problem. It is thus possible to find a locally optimal solution efficiently by using a majorization minimization algorithm and an interior point method where the cost of a single interior-point iteration grows linearly in the number of fixed kernels. Monte Carlo simulations show that the locally optimal solutions lead to good performance, regardless of the initialization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>system with smallest possible variance. However, available data records are often short in practice due to cost and/or time reasons. As shown by extensive simulations in <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, ML/PEM equipped with classical model structure selection techniques sometimes fails to get model estimates with good accuracy and robustness for short and noisy data records.</p><p>Structure detection. Structural constraints widely exist in engineering systems. In networked and decentralized systems, certain inputs usually influence only certain outputs. In piecewise affine systems, each data point must be associated to the most suitable submodel. They are often tackled, see e.g., <ref type="bibr" target="#b6">[7]</ref>, in a ML or related framework by using ARX model and LASSO <ref type="bibr" target="#b7">[8]</ref>, group LASSO <ref type="bibr" target="#b8">[9]</ref> techniques. For short data records, the high variance of ARX model deteriorates the detection accuracy. Moreover, there are better sparsity techniques, e.g., sparse Bayesian learning (SBL) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which can produce more sparse solutions with also more favorable properties in terms of mean square error (MSE), see e.g., <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>.</p><p>A new approach, which has been shown particularly useful for model estimation with short data records, is the kernelbased regularization method (KRM) introduced in <ref type="bibr" target="#b3">[4]</ref> and further studied in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The performance of KRM depends on both kernel structure design, i.e., parameterization of the kernel by some parameters often called hyper-parameters, and hyper-parameter estimation. There are several ways for the hyper-parameter estimation, e.g. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. So far, the most effective one is to embed the regularization in Bayesian framework and invoke the empirical Bayes method, i.e., the marginal likelihood maximization method. This method embodies an automatic Occam's razor (parsimonious) principle, i.e., tradeoff between data fit and model complexity <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr">[15, p. 110]</ref>, which is an important reason why KRM outperforms ML/PEM in dealing with the bias-variance tradeoff for short and noisy data records. Since <ref type="bibr" target="#b3">[4]</ref>, several kernel structures have been introduced <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, they do not work so well if the true system has complicated dynamics, e.g., with several widely spread time constants. An outstanding question is how to parameterize the kernel with flexible structure so that complicated dynamics can be better captured? Interestingly, KRM in <ref type="bibr" target="#b5">[6]</ref> also has close connection with SBL <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which is a Bayesian method for finding sparse solutions and has proven better than LASSO and Group LASSO in terms of sparsity property <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and in terms of MSE <ref type="bibr" target="#b13">[14]</ref>. In fact, if the kernel in <ref type="bibr" target="#b5">[6]</ref> is diagonal and has all diagonal elements as hyper-parameters, KRM in <ref type="bibr" target="#b5">[6]</ref> becomes SBL for basis selection <ref type="bibr" target="#b11">[12]</ref>. It finds sparse solutions in the hyper-parameter space, which in turn leads to sparse solutions in the parameter (hypothesis) space. Noticing this connection, a natural question is whether it is possible to incorporate SBL's feature of favoring sparsity into KRM and accordingly tackle structure detection problems in System Identification <ref type="bibr" target="#b6">[7]</ref>?</p><p>Both questions aforementioned are related to the kernel structure design. Indeed, we are looking for kernel structures that can both capture complicated dynamics and induce sparse hyper-parameters (and sparse hypotheses in the end), but is there any other concern that should be taken into account? Since the marginal likelihood maximization is non-convex and often has no closed-form solution, in our opinion, one such concern is if the designed kernel structure can bring the marginal likelihood maximization problem certain structures so that a locally optimal solution can be found efficiently.</p><p>In this paper, we aim to address the three questions raised above. Considering the superposition property of linear systems, it is natural to propose using the multiple kernel, which is a conic combination of suitable fixed kernels and has the combination coefficients as hyper-parameters. The fixed kernels can be instances of existing single kernels <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and can also be constructed based on model estimates which can be either data-driven or data-free. Due to their flexible structures, multiple kernels can better capture complicated dynamics than single kernels. What's more, the marginal likelihood maximization problem with multiple kernel favors sparse hyper-parameters. This feature enables this multiple KRM (MKRM) to tackle various structure detection problems. For illustration, the sparse dynamic network identification <ref type="bibr" target="#b18">[19]</ref> and the segmentation problems of linear systems <ref type="bibr" target="#b19">[20]</ref> will be studied here. For both model estimation and structure detection, MKRM reduces to a marginal likelihood maximization problem. The multiple kernel brings the problem a special structure that it is a difference of convex programming (DCP) problem <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Its locally optimal solution can thus be found efficiently using sequential convex optimization techniques. In particular, we use a majorization minimization (MM) algorithm <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and an interior-point method, where the cost of a single interior-point iteration grows linearly in the number of fixed kernels. Monte Carlo simulations show that the locally optimal solutions lead to good performance, regardless of the initialization, which is a practical advantage over ML/PEM and KRM with nonlinearly parameterized kernels where the initialization is critical and tricky.</p><p>The remaining parts of this paper is organized as follows. MKRM is proposed in Section II where it is also shown that the marginal likelihood maximization with multiple kernel is a DCP problem. By exploiting this structure, its locally optimal solution is found in Section III by using an MM algorithm and an interior point method. In Section IV, it is further shown that the marginal likelihood maximization with multiple kernel favors sparse hyper-parameters. This feature is then used to study the sparse dynamic network identification and the segmentation problems of linear systems. To illustrate the effectiveness of the proposed method, three sets of simulations, two of which are Monte Carlo ones, are considered in Section V. We finally conclude this paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MODEL ESTIMATION WITH MULTIPLE KERNEL-BASED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGULARIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem statement</head><p>Consider a single-input-single-output (SISO) linear casual and stable system</p><formula xml:id="formula_0">y(t) = G 0 (q)u(t) + v(t), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where t is the time index (the sampling interval is assumed to be one time unit), q is the shift operator, meaning qu(t) = u(t + 1), y(t), u(t) and v(t) are the output, input and disturbance at time t, respectively. The disturbance v(t) is modeled as a white noise with mean zero and variance σ 2 , independent of u(t). See Remark 2.5 for discussions about the case where v(t) is modeled as a filtered white noise. The transfer function G 0 (q) can be written as G 0 (q) = ∑ ∞ k=1 g 0 k q -k , where the coefficients</p><formula xml:id="formula_2">g 0 k , k = 1, • • • , ∞, form the impulse response of G 0 (q). Given a data record {u(t), y(t)} N</formula><p>t=1 , the goal is to find an estimate of G 0 (q), or equivalently, an estimate of the impulse response of G 0 (q) that is as good as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regularized FIR model estimation</head><p>Consider system <ref type="bibr" target="#b0">(1)</ref>. Since the impulse response of a linear system decays exponentially, it is often enough to truncate the infinite impulse response at a certain order and estimate an FIR (finite impulse response) model</p><formula xml:id="formula_3">G(q, θ ) = n ∑ k=1 g k q -k , θ = g 1 g 2 . . . g n T .</formula><p>(</p><p>The model of system (1) can then be written as</p><formula xml:id="formula_5">y(t) = φ (t) T θ + v(t), t = n + 1, • • • , N,<label>(3)</label></formula><p>with φ (t</p><formula xml:id="formula_6">) T = [u(t -1) • • • u(t -n)],</formula><p>which can be further written in a more compact form</p><formula xml:id="formula_7">Y N = Φ T N θ + V N .<label>(4)</label></formula><p>The</p><formula xml:id="formula_8">ith row of Y N ,V N ∈ R N-n and Φ T N ∈ R (N-n)×n are y(n + i), v(n + i) and φ (n + i) T , respectively. For t = 1, • • • , n, y(t) depends the unknown u(0), • • • , u(t -n),</formula><p>which can be handled in different ways, see <ref type="bibr">[1, p. 320]</ref>. Like <ref type="bibr" target="#b5">[6]</ref>, the non-windowed method is used here, i.e., y(t</p><formula xml:id="formula_9">), t = 1, • • • , n are not used. Then the regularized least squares estimate θ R N of θ is θ R N = arg min θ Y N -Φ T N θ 2 2 + σ 2 θ T P -1 θ (5a) = PΦ N (Φ T N PΦ N + σ 2 I N-n ) -1 Y N ,<label>(5b)</label></formula><p>where I N-n denotes the Nn dimensional identity matrix and P is positive semi-definite (denoted by P 0) and often called kernel (matrix) in Machine Learning <ref type="bibr" target="#b14">[15]</ref> and Bayesian Framework <ref type="bibr" target="#b24">[25]</ref>. If P is positive definite, it is denoted by P ≻ 0 below, where 0 denotes a zero matrix with suitable dimension which can be judged from the context. Remark 2.1: When P is singular, (5a) is not well-defined. In this case, consider the singular value decomposition of P:</p><formula xml:id="formula_10">P = [U 1 U 2 ]diag(Λ P , 0)[U 1 U 2 ]</formula><p>T where Λ P is a diagonal matrix with diagonal elements being strictly positive singular values of P, [U 1 U 2 ] is an orthogonal matrix with U 1 having the same number of columns as Λ P , and diag(Λ P , 0) is a block-diagonal matrix with Λ P and 0 on the main diagonal. Then (5a) should be interpreted as</p><formula xml:id="formula_11">θ R N = arg min θ Y N -Φ T N θ 2 + σ 2 θ T U 1 Λ -1 P U T 1 θ , s.t. U T 2 θ = 0 (6)</formula><p>It is easy to verify that (5b) is still the optimal solution of (6).</p><p>For convenience, we will still use (5a) in the sequel and refer to <ref type="bibr" target="#b5">(6)</ref> for its rigorous meaning when P is singular. Assume g 0 k = 0, k &gt; n so that the true system (1) is described by an FIR model, and denote the true impulse response coefficients by θ 0 = g 0 1 g 0 2 . . . g 0 n T . Then we have the</p><formula xml:id="formula_12">following convergence result for θ R N . Theorem 2.1: Assume that u(t) is deterministic, v(t) is i.i.d. with mean 0 and variance σ 2 , Φ N Φ T N /N → Ω as N → ∞</formula><p>where Ω is positive definite, and Φ N V N /N → 0 with probability one as N → ∞. If θ 0 can be represented as a linear combination of eigenvectors of PΩ, then θ R N → θ 0 with probability one as N → ∞.</p><p>Proof: The proof is straightforward and omitted due to the limitation of space.</p><p>Remark 2.2: The kernel P brings the regularized FIR model estimate θ R N a special structure. Since</p><formula xml:id="formula_13">Φ N (Φ T N PΦ N + σ 2 I N-n ) -1 Y N is a column vector, θ R</formula><p>N is a linear combination of the column vectors of P. It is preferable to have the column space of P include θ 0 . From Theorem 2.1, if Ω is nonsingular and P is nonsingular or P = cθ 0 θ T 0 for c &gt; 0, θ 0 can be represented as a linear combination of the eigenvectors of PΩ and thus θ R N is asymptotically consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiple kernel</head><p>The design of P consists of two parts: kernel structure design, i.e., parameterization of P by some parameters called hyper-parameters, and hyper-parameter estimation for a kernel structure. Many efforts have been spent on designing kernel structures and several kernels have been introduced in [4]- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, e.g., the stable spline (SS), the tuned/correlated (TC) and the diagonal/correlated (DC) kernels:</p><formula xml:id="formula_14">SS cP ss k, j (β ) = c λ 2k 2 (λ j -λ k 3 ), k ≥ j λ 2 j 2 (λ k -λ j 3 ), k &lt; j , β = λ (7a) TC cP tc k, j (β ) = c min(λ k , λ j ), β = λ (7b) DC cP dc k, j (β ) = cλ (k+ j)/2 ρ |k-j| , β = [λ ρ] T (7c)</formula><p>where c, β are hyper-parameters with c ≥ 0, 0 ≤ λ &lt; 1, |ρ| ≤ 1. However, those single kernels cP(β ) in ( <ref type="formula">7</ref>) could be improved to better capture complicated dynamics. For example, consider the case θ 0 = θ 1 + θ 2 where θ 1 , θ 2 ∈ R n are two FIRs that have very different dynamics in terms of e.g., decay rate and magnitude. Instead of cP(β ), better impulse response estimate θ R N can often be obtained using c 1 P(β 1 ) + c 2 P(β 2 ), but the initialization of the hyper-parameters becomes important and tricky for the associated hyper-parameter estimation problem. Interestingly, the domain of β is compact, so if there is no knowledge about the estimate of β i , i = 1, 2, it is natural to introduce a grid of points β1 , . . . , βm over the domain of β and use the kernel ∑ m i=1 c i P( βi ) with c 1 , . . . , c m being the hyperparameters instead. From this observation and the supposition property of linear systems that impulse response of a linear system is the sum of impulse responses of its partial fraction expansion, it is natural to propose using the multiple kernel</p><formula xml:id="formula_15">P(α) = m ∑ i=1 c i P i , α = c 1 , • • • , c m T ,<label>(8)</label></formula><p>where c i ≥ 0, P i 0 and P i = 0, i = 1, • • • , m, are fixed kernels.</p><p>The fixed kernels P i can be constructed in different ways. In what follows, we mainly consider the way to construct P i as instances of single kernels <ref type="bibr" target="#b6">(7)</ref>, but in Section V-A we will briefly discuss another way. Example 2.1: We illustrate the advantage of the multiple kernel (8) by a simple example:</p><formula xml:id="formula_16">G 0 (q) = z 1 q -1 (1 -p 1 q -1 ) -1 + z 2 q -1 (1 -p 2 q -1 ) -1 ,<label>(9)</label></formula><p>where z 1 = 1, z 2 = -50 and p i , i = 1, 2 are generated as p 1 = rand(1)/2+0.5 and p 2 = sign(randn(1)) * rand(1)/2 in MATLAB. Example <ref type="bibr" target="#b8">(9)</ref> contains two distinct modes: the fast one dominates the dynamics in the initial phase and the slow one dominates afterwards. Here, 1000 instances of <ref type="bibr" target="#b8">(9)</ref> and associated data sets are generated. The multiple kernel ( <ref type="formula" target="#formula_15">8</ref>) is constructed with 20 fixed kernels obtained by evaluating (7b) on the grid with c = 1, λ = 0.05 : 0.05 : 0.95, 0.98. This multiple kernel (denoted by TC-M) is compared with the single kernels <ref type="bibr" target="#b6">(7)</ref>. The simulation result in terms of model fit <ref type="bibr" target="#b25">(26)</ref> is shown on the left plot of Fig. <ref type="figure" target="#fig_0">1</ref>. The advantage of using multiple kernel is quite clear. As can be seen from the right plot of Fig. <ref type="figure" target="#fig_0">1</ref>, the single kernels try to capture the fast mode in the initial phase so that they, unlike the multiple kernel, do not have extra flexibility to well capture the slow mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyper-parameter estimation</head><p>Given a multiple kernel P(α), there exist several ways to estimate the hyper-parameter α. Currently, the most effective one is to embed the regularization term θ T P -1 θ in (5a) in Bayesian framework and estimate α by maximizing the marginal likelihood.</p><p>Assume v(t) in ( <ref type="formula" target="#formula_0">1</ref>) is Gaussian distributed, independent of the input, and</p><formula xml:id="formula_17">θ ∼ N (θ ap , P(α)), θ ap = 0,<label>(10)</label></formula><p>where θ ap is the prior mean and P(α) is the prior covariance. Note that θ ap can be nonzero, see <ref type="bibr">[</ref> </p><formula xml:id="formula_18">p(Y N |α) = N (0, Φ T N P(α)Φ N + σ 2 I N-n ).</formula><p>The marginal likelihood maximization method arg max α≥0 p(Y N |α, σ 2 ) to estimate α is equivalent to</p><formula xml:id="formula_19">α = arg min α≥0 Y T N Σ(α, σ 2 ) -1 Y N + log detΣ(α, σ 2 ),<label>(11)</label></formula><p>where Σ(α, σ 2 ) = Φ T N P(α)Φ N + σ 2 I N-n . Although <ref type="bibr" target="#b10">(11)</ref> is non-convex, the multiple kernel P(α) renders (11) a special structure. Note from e.g. <ref type="bibr" target="#b25">[26]</ref> that both Y T N Σ(α, σ 2 ) -1 Y N andlog detΣ(α, σ 2 ) are convex in Σ(α, σ 2 ) ≻ 0 and Σ(α, σ 2 ) is affine in α and σ 2 . Therefore both Y T N Σ(α, σ 2 ) -1 Y N and -log det Σ(α, σ 2 ) are convex in α ≥ 0 and σ 2 &gt; 0, respectively. So the objective function of ( <ref type="formula" target="#formula_19">11</ref>) is a difference of two convex functions with respect to α ≥ 0 and σ 2 &gt; 0, and thus ( <ref type="formula" target="#formula_19">11</ref>) is a difference of convex programming (DCP) problem <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. It will be shown in Section III that a stationary point of ( <ref type="formula" target="#formula_19">11</ref>) can be found efficiently using sequential convex optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.3:</head><p>The noise variance σ 2 is not known and needs to be estimated from the data. As suggested in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b26">[27]</ref>, a simple and effective way is to estimate an ARX model <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or an FIR model <ref type="bibr" target="#b5">[6]</ref> with least squares and use the sample variance as the estimate of σ 2 . An alternative way is to treat σ 2 as an additional "hyper-parameter" and estimate it together with α by solving <ref type="bibr" target="#b10">(11)</ref>, see e.g., <ref type="bibr" target="#b9">[10]</ref>. All arguments below (with minor changes) still hold with σ 2 as an optimization argument in <ref type="bibr" target="#b10">(11)</ref>. At least for the test data bank in Section V-A, the alternative way seems a better choice for MKRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.4:</head><p>If we choose m = n in (8) and the fixed kernels</p><formula xml:id="formula_20">P i = e i e T i , i = 1, • • • ,</formula><p>n with e i being the orthonormal basis of R n , MKRM becomes SBL for basis selection <ref type="bibr" target="#b11">[12]</ref>. It favors sparse α in the hyper-parameter space and in turn leads to sparse θ in the parameter space. This observation prompts us to consider if MKRM has the same feature, as SBL for basis selection, of favoring sparse α in the more general multiple kernel <ref type="bibr" target="#b7">(8)</ref>. The answer to this question is affirmative and will be elaborated in Section IV.</p><p>Remark 2.5: Consider the case where v(t) in ( <ref type="formula" target="#formula_0">1</ref>) is a filtered white noise, i.e., v(t) = H 0 (q)e(t). Here H 0 (q) is unknown, both stable and inversely stable <ref type="bibr" target="#b0">[1]</ref> with unit static gain, and e(t) is a white noise with mean zero and variance σ 2 . Now our goal is to estimate G 0 (q) and H 0 (q) as well as possible. Recall that system (1) with v(t) = H 0 (q)e(t) can be well approximated (see <ref type="bibr" target="#b27">[28]</ref>) by a high order ARX model</p><formula xml:id="formula_21">y(t) = A n (q) -1 B n (q)u(t) + A n (q) -1 e(t) with A n (q) = 1 + a 1 q -1 + . . . + a n q -n , B n (q) = b 1 q -1 + . . . + b n q -n</formula><p>, which can be written as a linear regression</p><formula xml:id="formula_22">y(t) = φ T y (t)θ a + φ T u (t)θ b + e(t) where θ a = [a 1 , . . . , a n ] T , θ b = [b 1 , . . . , b n ] T ,</formula><p>and φ y (t), φ u (t) are defined in an obvious way. Further, note that for large n, θ a , θ b can be interpreted as the two FIRs for the one-step-ahead predictor of system (1) with v(t) = H 0 (q)e(t) from y and u, respectively, see <ref type="bibr" target="#b0">[1]</ref>. So the ARX model estimation problem becomes an FIR model estimation problem with two inputs and the same idea of regularization can be applied. Similar to</p><formula xml:id="formula_23">(5a), θa , θb = arg min θ a ,θ b N ∑ t=n+1 (y(t) -φ T y (t)θ a -φ T u (t)θ b ) 2 + σ 2 θ a θ b Q -1 θ a θ b T .</formula><p>It is intuitive to partition Q as a block-diagonal matrix Q(α a , α b ) = diag(P(α a ), P(α b )). The hyper-parameters α a , α b are still estimated by maximizing the marginal likelihood.</p><p>Remark 2.6: The multiple kernel (8) has been used in machine learning. Multiple kernel learning (MKL) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> is one such method and has also been used recently to handle linear system identification problems in <ref type="bibr" target="#b30">[31]</ref>. It can be shown that, see e.g. <ref type="bibr" target="#b13">[14]</ref> for derivations for group variable selection case, MKL also reduces to an estimation problem of α:</p><formula xml:id="formula_24">α = arg min α≥0 Y T N Σ(α, σ 2 ) -1 Y N + γ1 T α,<label>(12)</label></formula><p>where <ref type="formula" target="#formula_24">12</ref>) is much easier to solve than (11) since ( <ref type="formula" target="#formula_24">12</ref>) is convex. However, there is a price to pay for that. The comparison between <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref> shows that their difference lies in the second term: log det(Φ T N P(α)Φ N + σ 2 I N-n ) is replaced by γ1 T α. This fact shows that MKL actually solves a suboptimal marginal likelihood maximization problem <ref type="bibr" target="#b10">(11)</ref>. Such an approximation often results in less accurate and less robust model estimates for model estimation problems, and in general tends to produce less sparse solutions with also less favorable properties in terms of MSE for sparsity problems, see e.g. <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_25">1 = [1 1 • • • 1] T . Clearly, (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NEGATIVE LOG MARGINAL LIKELIHOOD MINIMIZATION WITH MULTIPLE KERNEL USING SEQUENTIAL CONVEX OPTIMIZATION TECHNIQUES</head><p>The hyper-parameter estimation problem <ref type="bibr" target="#b10">(11)</ref> can be put into the following form</p><formula xml:id="formula_26">minimize x≥0 Y T p ∑ i=1 x i B i B T i + σ 2 I n o -1 Y + log det p ∑ i=1 x i B i B T i + σ 2 I n o , x = x 1 ... x p T ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_27">Y ∈ R n o , B i ∈ R n o ×n i and B i = 0, and n o , n i , i = 1, • • • , p,</formula><p>are positive integers. Since P i 0, it can be factorized as P i = L i L T i , where L i ∈ R n×n i with n i being a positive integer. For example, for P i ≻ 0, L i can be its Cholesky factorization and n i = n. So ( <ref type="formula" target="#formula_19">11</ref>) can be put into the form of <ref type="bibr" target="#b12">(13)</ref> </p><formula xml:id="formula_28">with p = m, n o = N -n, x i = c i , i = 1, • • • , p, Y = Y N and B i = Φ T N L i , i = 1, • • • , p.</formula><p>In what follows, ( <ref type="formula" target="#formula_26">13</ref>) is referred to as the negative log marginal likelihood minimization with multiple kernel. This is because for both model estimation and structure detection, see Section IV, the associated negative log marginal likelihood minimization problems can all be put into the form of <ref type="bibr" target="#b12">(13)</ref>.</p><p>In general, Y and x have the interpretation of measurement output and hyper-parameter, respectively, and B i contains the information of the measurement input and the fixed kernel P i in <ref type="bibr" target="#b7">(8)</ref>. Obviously, ( <ref type="formula" target="#formula_26">13</ref>) is still a DCP problem. Now, we consider how to tackle (13) by exploiting its DC structure and using sequential convex optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sequential convex optimization techniques: majorization minimization (MM) algorithms</head><p>There are a couple of sequential convex optimization techniques that can be used to tackle DCP problems. One of them is the so-called MM algorithm <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and its main idea is to yield an iterative scheme for minimize x∈C f (x) with C ⊆ R p where each iteration consists of minimizing a so-called majorization function f</p><formula xml:id="formula_29">(x, x (k) ) of f (x) at x (k) ∈ C: x (k+1) = arg min x∈C f (x, x (k) ),<label>(14)</label></formula><p>where f : <ref type="formula" target="#formula_29">14</ref>) yields a descent algorithm. Construction of a suitable majorizization function is a key step for MM algorithms. For DCP problems minimize x∈C f (x) where f (x) = g(x)h(x), g, h : C → R are convex and differentiable functions with C being a convex set in R p , there are many ways to construct the majorization function <ref type="bibr" target="#b23">[24]</ref>. The simplest one is the so-called linear majorization or majorization via "supporting hyperplane" <ref type="bibr" target="#b23">[24]</ref>, i.e.,</p><formula xml:id="formula_30">C × C → R satisfies f (x, x) = f (x) for x ∈ C and f (x) ≤ f (x, z) for x, z ∈ C. Clearly, (</formula><formula xml:id="formula_31">f (x, x (k) ) = g(x) -h(x (k) ) -∇h(x (k) ) T (x -x (k) ).<label>(15)</label></formula><p>For this particular choice of majorization function, the MM algorithm ( <ref type="formula" target="#formula_29">14</ref>) is also referred to as "sequential convex optimization" or "the convex concave procedure" (CCCP) <ref type="bibr" target="#b22">[23]</ref>. Remark 3.1: The so-called simplified difference of convex functions algorithm (DCA) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> is another sequential convex optimization technique that can be used to tackle DCP problems. Simplified DCA is a primal-dual method that alternates between majorization minimization updates based on the problem inf x {g(x)h(x)} and its Fenchel-Rockafellar dual. For differentiable f (x), the CCCP algorithm <ref type="bibr" target="#b22">[23]</ref> is equivalent to a primal-only variant of the simplified DCA, which is a special case of MM algorithm (with linear majorization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MM algorithms to the negative log marginal likelihood minimization with multiple kernel</head><p>From now on, we identify f (x) as the objective function of <ref type="bibr" target="#b12">(13)</ref> </p><formula xml:id="formula_32">, C = {x ∈ R p |x ≥ 0} and f (x) = g(x) -h(x) where g(x) = Y T Σ(x) -1 Y, h(x) = -log det Σ(x), Σ(x) = p ∑ i=1 x i B i B T i + σ 2 I n o . (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>Algorithm 3.1: The MM algorithm to the problem ( <ref type="formula" target="#formula_26">13</ref>) can be summarized as follows: Set x (0) , k = 0 and then go to the following iterative steps: 1) Compute the gradient ∇h(x (k) ) according to</p><formula xml:id="formula_34">∇ x i h(x) = -Tr Σ(x) -1 ∂ Σ(x) ∂ x i , i = 1, • • • , p,</formula><p>and then solve the convex optimization problem ( <ref type="formula" target="#formula_29">14</ref>) and ( <ref type="formula" target="#formula_31">15</ref>) to obtain x (k+1) . 2) Check if the optimality condition is satisfied. If satisfied, stop. If otherwise, set k = k + 1 and go to step 1).</p><p>The convergence of MM algorithms to a stationary point (the point satisfies the Karush-Kuhn-Tucker (KKT) conditions, see e.g., <ref type="bibr" target="#b25">[26]</ref>) has been discussed in e.g., <ref type="bibr" target="#b31">[32]</ref>. For the MM Algorithm 3.1, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Thm. 4</ref>] can be employed to show the convergence. In the following, we show the four assumptions of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Thm. 4</ref>] are satisfied for the MM algorithm 3.1. First, both g(x) and h(x) are real-valued differentiable convex functions. Second, ∇h(x) is obviously continuous. Third, for any x ≥ 0, the set H(x) = {z| f (z) ≤ f (x), z ≥ 0} is indeed bounded. This is because for any x ≥ 0 and x = 0, lim t→+∞ Y T Σ(tx) -1 Y + log detΣ(tx) → +∞. Fourth, there is no equality constraint involved and moreover, the inequality constraint x ≥ 0 leads to</p><formula xml:id="formula_35">c i (x) = -x i , i = 1, • • • , p in [32, eq. (<label>1</label></formula><p>)], which are real-valued convex functions. Therefore, the MM Algorithm 3.1 converges to a stationary point of <ref type="bibr" target="#b12">(13)</ref>.</p><p>Remark 3.2: In <ref type="bibr" target="#b32">[33]</ref>, CCCP algorithm was used to solve the marginal likelihood maximization problem for basis selection <ref type="bibr" target="#b11">[12]</ref>. Here, we consider MM algorithm instead of simplified DCA to tackle <ref type="bibr" target="#b12">(13)</ref> primarily because the simplified DCA involves the conjugate function h * (y) = sup x {y T x+logdet Σ(x)}, which has no closed-form solution and thus is expensive to evaluate. There are also two secondary reasons. First, for a given DCP problem, different MM algorithms can be easily derived by employing different majorization functions. Second, noting that MM algorithms are not widely known as its special case expectation maximization (EM) algorithms <ref type="bibr" target="#b23">[24]</ref> in System Identification, <ref type="bibr" target="#b12">(13)</ref> shows that they can be alternative choices for parameter estimation problems in System Identification. In this regard, it is also interesting to note <ref type="bibr" target="#b33">[34]</ref>, which minimizes a convex upper bound of a nonconvex objective function for a nonlinear state-space model identification problem. If the procedure in <ref type="bibr" target="#b33">[34]</ref> is done in a sequential way, it will be inline with the idea of handling DCP problems with sequential convex optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. An efficient and accurate implementation</head><p>It is possible to solve each iteration (14) using a (fast) projected gradient method <ref type="bibr" target="#b34">[35]</ref> or Quasi-Newton methods such as L-BFGS-B <ref type="bibr" target="#b35">[36]</ref>. These methods often require many iterations to obtain a moderately accurate solution and thus may be suitable for an inexact MM scheme where ( <ref type="formula" target="#formula_29">14</ref>) is solved only approximately. However, such an inexact MM scheme typically slows down the rate of convergence of the iteration <ref type="bibr" target="#b13">(14)</ref>.</p><p>It is worth to note that g(x) in ( <ref type="formula" target="#formula_29">14</ref>) is a matrix fractional function, see <ref type="bibr">[26, p. 76</ref>]. So each iteration <ref type="bibr" target="#b13">(14)</ref> in fact involves solving a convex matrix fractional minimization problem <ref type="bibr" target="#b25">[26]</ref>,</p><p>which is well-known to be equivalent to a semidefinite programming problem minimize</p><formula xml:id="formula_36">z,x z -∇h(x (k) ) T x, z ∈ R, x ∈ R p , subject to z Y T Y Σ(x) 0, x ≥ 0.</formula><p>The cost of solving this SDP is at least cubic in the number of hyper-parameters p as well as the number of observations n o , and hence solving this SDP is too costly for all but small problems. Note that modelling packages such as CVX <ref type="bibr" target="#b36">[37]</ref> commonly use such an SDP reformulation of a matrix fractional minimization problem.</p><p>From the definition of Σ(x) in ( <ref type="formula" target="#formula_32">16</ref>) and the constraint x ≥ 0, we see that Σ(x) is a sum of positive semidefinite terms. This implies that the matrix fractional minimization problem <ref type="bibr" target="#b13">(14)</ref> can be cast as a conic quadratic optimization problem (see e.g. <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b38">[39]</ref>). In particular, each iteration ( <ref type="formula" target="#formula_29">14</ref>) amounts to solving the following conic optimization problem with p + 1 rotated quadratic cone constraints (see <ref type="bibr">[38, p. 202]</ref>), i.e., minimize</p><formula xml:id="formula_37">z,x,v,w 2(1 T z) -∇h(x (k) ) T x subject to w i 2 ≤ 2x i z i , i = 1, . . . , p, v 2 ≤ 2σ 2 z p+1 (17) Y = p ∑ i=1 B i w i + σ v, x ≥ 0, z ≥ 0 where x ∈ R p , z = z 1 • • • z p+1 T ∈ R p+1 , v ∈ R n o ,<label>and</label></formula><formula xml:id="formula_38">w i ∈ R n i for i = 1, . . . , p.</formula><p>The problem <ref type="bibr" target="#b16">(17)</ref>, which is equivalent to a second-order cone program, can be solved efficiently and accurately using an interior-point method. The computational cost depends on the implementation. If the rotated quadratic cone constraints are handled carefully, the computational cost of a single interior-point iteration is O(n o 2 max(n o , ∑ p i=1 n i )) and in particular linear in p if all n i s are equal; see e.g. <ref type="bibr" target="#b39">[40]</ref>. We have implemented such a method for solving <ref type="bibr" target="#b16">(17)</ref> in CVXOPT <ref type="bibr" target="#b40">[41]</ref>, a Python extension for convex optimization. Our implementation is based on the cone LP solver in CVX-OPT, and uses a custom solver for the so-called KKT system that defines the search direction at each interior-point iteration, see <ref type="bibr" target="#b41">[42]</ref>. The implementation details cannot be included here due to space limitations. The problem <ref type="bibr" target="#b16">(17)</ref> can also be solved efficiently using, e.g., the commercial solver MOSEK.</p><p>Monte Carlo simulations in Section V show that, the proposed MM Algorithm 3.1 and implementation requires on average 12 iterations of ( <ref type="formula" target="#formula_29">14</ref>) to obtain a high accuracy locally optimal solution of (13). Moreover, the locally optimal solutions lead to good performance, regardless of the initialization, which is a practical advantage over ML/PEM and KRM with nonlinearly parameterized kernels where the initialization is critical and tricky.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. STRUCTURE DETECTION WITH MULTIPLE KERNEL-BASED REGULARIZATION</head><p>Structure detection problems are in essence model structure selection problems in parameter space, e.g. <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. As will be seen in Sections IV-B and IV-C, using MKRM, the structure detection problems in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> are converted to problems of finding a suitable sparse pattern (the number and the location of zeros) of the hyper-parameter x, which can be seen as model structure selection problems in the hyper-parameter space. For convenience, we start the discussion from the problem of finding a suitable sparse pattern of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Finding a suitable sparse pattern of x</head><p>Since x ∈ R p , there are in total 2 p sparse patterns of x, denoted by x [i] ∈ R p , and accordingly 2 p model structures denoted by M i , i = 1, • • • , 2 p . Here x [i] should be understood as follows: some of its elements are locked to zero and the others are free variables. For example, for p = 2, there are four sparse patterns:</p><formula xml:id="formula_39">[0 0] T , [x 1 0] T , [0 x 2 ] T and [x 1 x 2 ] T .</formula><p>In Bayesian framework, model structure selection problems are typically tackled by using the evidence maximization method (EMM), see e.g., <ref type="bibr" target="#b9">[10]</ref>. Since all fixed kernels in <ref type="bibr" target="#b7">(8)</ref> are instances of existing single kernels <ref type="bibr" target="#b6">(7)</ref> that are independent of the data, it is natural to assume that the "subjective priors" 2 , where p(x [i] |M i ) and p(σ 2 |M i ) are independent hyper-priors. EMM selects the model structure or equivalently the sparse pattern of x with the largest evidence as the best one, and moreover, has the ability on the average to identify the true model structure, see <ref type="bibr">[10, p. 441</ref>]. There are however two practical difficulties for EMM. One is that the integral in p(Y |M i ) often has no closed-form solution. The other is that p(Y |M i ) may need to be computed for a large number of times. EMM is thus in general expensive to implement and only applicable for small scale problems in practice.</p><formula xml:id="formula_40">p(M i ), i = 1, • • • , 2 p , are equal. In this case, M i , i = 1, • • • , 2 p , are ranked by evaluating the evidence of M i , defined as p(Y |M i ) = p(Y |M i , x [i] , σ 2 )p(x [i] |M i )p(σ 2 |M i )dx [i] dσ</formula><p>1) An efficient approximation of EMM: An approximation of EMM was suggested in <ref type="bibr">[43, p. 778</ref></p><formula xml:id="formula_41">]. For i = 1, • • • , 2 p , let x[i] , σ 2 i = arg max x [i] ,σ 2 p(Y |M i , x [i] , σ 2 ) = arg min x [i] ,σ 2 f (x [i] , σ 2 )</formula><p>, where f (x [i] , σ 2 ) is the objective function of (13) with x replaced by x [i] . Then</p><formula xml:id="formula_42">logp(Y |M i )/p(Y |M j ) ≈ log p(Y |M i , x[i] , σ 2 i )<label>(18)</label></formula><formula xml:id="formula_43">-log p(Y |M j , x[ j] , σ 2 j ) - 1 2 (d i -d j ) log(n o )</formula><p>where d i and d j are the numbers of nonzero elements of</p><formula xml:id="formula_44">[( x[i] ) T σ 2 i ] T and [( x[ j]</formula><p>) T σ 2 j ] T , respectively. Interestingly, minus twice of ( <ref type="formula" target="#formula_42">18</ref>) is actually the Bayesian information criterion (BIC), see <ref type="bibr" target="#b42">[43]</ref>. While ( <ref type="formula" target="#formula_42">18</ref>) is more convenient to compute, the required computation in handling a model structure selection problem with 2 p model structures is still combinatorial. This difficulty can be overcome by noting the feature of the negative log marginal likelihood minimization problem (13):</p><p>Theorem 4.1: Consider (13). There exists a σ 2 max such that for σ 2 &gt; σ 2 max , the optimal solution of ( <ref type="formula" target="#formula_26">13</ref>) is unique and exactly zero. In particular, for the case where</p><formula xml:id="formula_45">B i B T i , i = 1, • • • , p, are nonsingular, let δ i = inf{s|B T i (sI n o -YY T )B i ≻ 0}, i = 1, • • • , p,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and assume without loss of generality</head><formula xml:id="formula_46">+∞ = δ 0 &gt; δ 1 ≥ δ 2 ≥ • • • ≥ δ p . Then for each i = 0, • • • , p -1, if σ 2 ∈ (δ i+1 , δ i ],</formula><p>every locally optimal solution of (13) contains at least pi zeros.</p><p>Proof: Denote the objective function of (13) by f (x). Then for each</p><formula xml:id="formula_47">i = 1, • • • , p, ∇ x i f (x) = Tr B T i Σ(x) -1 (Σ(x) -YY T )Σ(x) -1 B i . Note that x is a stationary point of (13) if for each i = 1, • • • , p, either ∇ x i f (x) = 0 for x i ≥ 0 or ∇ x i f (x) &gt; 0 for x i = 0. For convenience, define σ 2 max = inf{s|sI n o -YY T ≻ 0}.</formula><p>Then for any σ 2 &gt; σ 2 max , ∇ x i f (x) &gt; 0 for each i = 1, • • • , p and any x ≥ 0, and thus (13) has a unique optimal solution and is exactly zero. What's more, if</p><formula xml:id="formula_48">B i B T i , i = 1, • • • , p, are nonsingular, ∇ x i f (x) = Tr{B T i Σ(x) -1 (B i B T i ) -1 B i B T i ( p ∑ k=1 x k B k B T k + σ 2 I n o -YY T )B i B T i (B i B T i ) -1 Σ(x) -1 B i }.</formula><p>For each i = 0,</p><formula xml:id="formula_49">• • • , p -1, if σ 2 ∈ (δ i+1 , δ i ], ∇ x j f (x) &gt; 0 for j = i + 1,</formula><p>• • • , p and x ≥ 0, which implies all locally optimal solutions have x j = 0, j = i + 1, • • • , p. We thus complete the proof. Remark 4.1: It should be noted that no constraint is imposed on n o and p in Theorem 4.1. If n o &lt; p, it can be shown that for any σ 2 ≥ 0, every locally optimal solution of ( <ref type="formula" target="#formula_26">13</ref>) is achieved at a sparse solution with at most n o nonzeros. The proof of this claim is a straightforward extension of the proof of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Thm. 2]</ref> to the general multiple kernel (8) and thus is omitted. Theorem 4.1 and this claim indicate that the negative log marginal likelihood minimization problem ( <ref type="formula" target="#formula_26">13</ref>) has an inherent mechanism of favoring sparse hyper-parameters.</p><p>From Theorem 4.1 and Remark 4.1, we have the efficient way to find a suitable sparse pattern of x, which is referred to as MKRM-BIC below. First solve</p><formula xml:id="formula_50">x[0] , σ 2 0 = arg min x,σ 2 f (x, σ 2 ).<label>(19)</label></formula><p>Then, set k = 0 and go to the next iterative steps: a) Determine x [k+1] as follows: x [k+1] is similar to x [k] with the only difference that x [k+1] has one more zero that corresponds to the smallest nonzero element of x[k] ; b) Invoking <ref type="bibr" target="#b17">(18)</ref>, if log p(Y |M k )/p(Y |M k+1 ) &gt; 0, stop and select x [k] as the best sparse pattern; if otherwise, set k = k + 1 and go to step a). 2) Two heuristic methods: Noticing the form of (18), we consider two heuristic methods that are also based on the marginal likelihood maximization. The idea of the first one is to solve <ref type="bibr" target="#b18">(19)</ref> and select the sparse pattern x [0] as a reference, and then trim x [0] by removing the small nonzeros that has little influence on the marginal likelihood. This heuristic method is referred to as MKRM-H1 and is detailed as follows. First, solve <ref type="bibr" target="#b18">(19)</ref> and let o 0 = f ( x[0] , σ 2 0 ). Then, set the threshold o h &gt; 0, k = 0 and go to the next iterative steps: a) Determine x [k+1] as follows: x [k+1] is similar to x [k] with the only difference that x [k+1] has one more zero that corresponds to the smallest nonzero element of x</p><formula xml:id="formula_51">[k] ; b) Solve x[k+1] , σ 2 k+1 = arg min x [k+1] ,σ 2 f (x [k+1] , σ 2 ) and let o k+1 = f ( x[k+1] , σ 2 k+1 ). Check if |(o k+1 -o 0 )/o 0 | &gt; o h : if yes,</formula><p>stop and select x [k] as the best sparse pattern; if otherwise, set k = k + 1 and go to step a). Here, the threshold o h is a tuning parameter and can be tuned, e.g., by cross validation.</p><p>Both MKRM-BIC and MKRM-H1 rely on σ 2 0 , the estimate of σ 2 by maximizing the marginal likelihood <ref type="bibr" target="#b18">(19)</ref>, which can however be very inaccurate. It can even happen that σ 2 0 = 0 because there can exist nonzero x such that ∑ p i=1 x i B i B T i has identical contribution as σ 2 I n o on Σ(x) in <ref type="bibr" target="#b15">(16)</ref>, see [44, Section 3.C] for related discussions on basis selection problems. Interestingly, σ 2 0 = 0 happens for the segmentation problem in Section IV-C due to the use of the over-parameterized model <ref type="bibr" target="#b22">(23)</ref>. In this case, the corresponding sparse pattern x [0] in ( <ref type="formula" target="#formula_50">19</ref>) is very inaccurate, and hence MKRM-BIC and MKRM-H1 should not be applied. As suggested in [44, Section 3.C] for basis selection problems, an alternative way is to tune the sparse pattern of x by tuning σ 2 and solving (13) accordingly, which is possible by noting Theorem 4.1 and Remark 4.1. As for which sparse pattern is more suitable, one can use application dependent heuristic <ref type="bibr" target="#b43">[44]</ref>, cross validation <ref type="bibr" target="#b18">[19]</ref>, and EMM <ref type="bibr" target="#b9">[10]</ref> if possible. This heuristic method is referred to as MKRM-H2 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse dynamic network identification [19]</head><p>1) Problem statement and formulation: Consider a multiple-input-single-output (MISO) linear stable system</p><formula xml:id="formula_52">y(t) = r ∑ j=1 G j (q)u j (t) + v(t), (<label>20</label></formula><formula xml:id="formula_53">)</formula><p>where y(t) and q are defined as in ( <ref type="formula" target="#formula_0">1</ref>), u j (t) is the input for the jth subsystem G j (q), and v(t) is a white noise with mean zero and variance σ 2 , independent of u j (t), j = 1, • • • , r. The assumption is that there exists an index set I ⊂ {1, • • • , r} such that the inputs u j (t) with j ∈ I do not influence y(t), i.e., the corresponding G j (q) are zero. The goal is to estimate the index set I with a given data record {u 1 (t), • • • , u r (t), y(t)} N t=1 . To tackle the problem, subsystems G j (q), j = 1, • • • , r, are modeled as FIR models G j (q, θ j ) = n ∑ k=1 g k, j q -k , θ j = g 1, j g 2, j . . . g n, j</p><p>T . ( <ref type="formula">21</ref>)</p><p>Like ( <ref type="formula" target="#formula_7">4</ref>), Y N = ∑ r j=1 Φ T N, j θ j + V N where Y N ,V N are defined in (4), and Φ N, j is defined similarly as Φ N by replacing u with u j . Then the problem is tackled by using the following MKRM:</p><formula xml:id="formula_54">θ1 , • • • , θr = arg min θ 1 ,••• ,θ r Y N - r ∑ j=1 Φ T N, j θ j 2 2 + σ 2 r ∑ j=1 θ T j P(α j ) -1 θ j (<label>22</label></formula><formula xml:id="formula_55">)</formula><p>where</p><formula xml:id="formula_56">P(α j ) = ∑ m i=1 c i, j P i , α j = c 1, j , • • • , c m, j T , P i 0, i = 1, • • • , m, are fixed kernels and c i, j ≥ 0, i = 1, • • • , m, are</formula><p>the hyper-parameters associated with θ j . If α j = 0 for some j = 1, • • • , r, P(α j ) = 0 and thus θ j = 0, which indicates G j (q, θ j ) = 0. In this way, the problem is converted to a problem of finding a suitable sparse pattern of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Finding a suitable sparse pattern of hyper-parameters:</head><p>The regularization term ∑ r j=1 θ T j P(α j ) -1 θ j in ( <ref type="formula" target="#formula_54">22</ref>) is first embedded into Bayesian framework. Assume v(t) in ( <ref type="formula" target="#formula_52">20</ref>) is Gaussian distributed and θ j , j = 1, • • • , r, are independent and θ j ∼ N (0, P(α j )). Then it can be shown that the MAP estimation problem arg max θ j , j=1,•••,r p(θ j , j = 1, • • • , r|Y N ) is equivalent to <ref type="bibr" target="#b21">(22)</ref>. Noting the factorization of</p><formula xml:id="formula_57">P i = L i L T i , i = 1, • • • , m, the marginal likelihood maximization problem maximize α 1 ,••• ,α r p(Y N |α 1 , • • • , α r ) can be put into the form (13) with p = mr, n o = N -n, Y = Y N , x = α T 1 • • • α T r T and</formula><p>Limited circulation. For review only Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST</p><formula xml:id="formula_58">B ( j-1)m+i = Φ T N, j L i , i = 1, • • • , m, j = 1, • • • , r.</formula><p>This problem is handled by using MKRM-BIC and MKRM-H1 in Section IV-A.</p><p>Remark 4.2: In theory, single kernels ( <ref type="formula">7</ref>) can be used to tackle the sparse dynamic network identification problem. In practice, they however cannot be applied due to the difficulty of the solution of the associated marginal likelihood maximization problem. To overcome this difficulty, the problem was handled in <ref type="bibr" target="#b18">[19]</ref> using a variant of KRM in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, where SS kernels (7a) for different subsystems are assumed to have the same β in (7a), i.e., θ j ∼ N (0, c j P ss (β )), j = 1, • • • , r. Moreover, exponential hyper-priors on c j , i.e., p(c j = γ exp(-γc j ) with γ ≥ 0 are imposed to enhance the sparsity</p><formula xml:id="formula_59">of c 1 , • • • , c r , which is achieved by solving the MAP problem maximize c 1 ,••• ,c r ,β p(c 1 , • • • , c r , β |Y N )</formula><p>with a suitable γ, tuned by cross validation. The non-convex MAP problem has no special structure and is handled by using a Quasi-Newton algorithm. An important issue for the numerical algorithm is the availability of a good starting point, which is provided by a Bayesian forward selection algorithm. In contrast, employing the multiple kernel <ref type="bibr" target="#b7">(8)</ref> and accordingly the MM Algorithm 3.1 and implementation greatly simplifies the solution but yields comparable performance as the method in <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b19">[20]</ref> 1) Problem statement and formulation: Consider system (1). The assumption is that G 0 (q) changes its dynamics at certain time instants which are rare. The goal is to detect the changes with a given data record {(y(t), u(t))} N t=1 . Since G 0 (q) may change at any time instant, we associate with each time instant t an FIR model with parameter vector t , that is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation of linear systems</head><formula xml:id="formula_60">y(t) = φ (t) T θ t + v(t), t = n + 1, • • • , N, (<label>23</label></formula><formula xml:id="formula_61">)</formula><p>where φ (t) is defined in (3) and θ t = g 1,t g 2,t • • • g n,t T .</p><p>Define θ n = 0. Then the problem is tackled by using the following MKRM:</p><formula xml:id="formula_62">θn+1 , • • • , θN = arg min θ n+1 ,••• ,θ N N ∑ t=n+1 (y t -φ (t) T θ t ) 2 + σ 2 (θ t -θ t-1 ) T P(α t ) -1 (θ t -θ t-1 ),<label>(24)</label></formula><p>where</p><formula xml:id="formula_63">P(α t ) = ∑ m i=1 c i,t P i , α t = [c 1,t , • • • , c m,t ] T , P i , i = 1, • • • , P m , are fixed kernels in (8) and c i,t ≥ 0, i = 1, • • • , m, are hyper-parameters associated with θ t . If α t = 0 for some t = n + 1, • • • , N, then θ t = θ t-1</formula><p>and the corresponding term σ 2 (θ t -θ t-1 ) T P( αt ) -1 (θ t -θ t-1 ) would disappear from (24). Therefore, α t = 0 for certain t = n + 1, • • • , N, is an indication that the dynamics of system (1) changes at time t. In this way, the segmentation problem is converted to a problem of finding a suitable sparse pattern of hyper-parameters. Also note that ( <ref type="formula" target="#formula_62">24</ref>) is convex with respect to θ t ,t = n + 1, • • • , N, therefore θt ,t = n + 1, • • • , N can be computed efficiently with a convex optimization package, e.g., CVX or CVXOPT.</p><p>2) Finding a suitable sparse pattern of hyper-parameters: The regularization term ∑ N t=n+1 (θ t -θ t-1 ) T P(α t ) -1 (θ t -θ t-1 ) in ( <ref type="formula" target="#formula_62">24</ref>) is first embedded into Bayesian framework. Assume v(t) in ( <ref type="formula" target="#formula_52">20</ref>) is Gaussian distributed and for t = n + 1, • • • , N, θ t is Gaussian distributed as θ t = N (θ t-1 , P(α t )) with θ n = 0. Then it can be shown that the MAP estimation problem arg max</p><formula xml:id="formula_64">θ n+1 ,••• ,θ N p(θ n+1 , • • • , θ N |Y N ) is equivalent to (24). Moreover, the marginal likelihood p(Y N |α n+1 , • • • , α N ) = N (0, K(α n+1 , • • • , α N , σ 2 )), where for t = n + 1, • • • , N -1, K(α t , • • • , α N , σ 2 ) = σ 2 0 0 K(α t+1 , • • • , α N , σ 2 ) +    φ (t) T . . . φ (N) T    P(α t ) φ (t) • • • φ (N) K(α N , σ 2 ) = φ (N) T P(α N )φ (N) + σ 2 .<label>(25)</label></formula><p>Noting <ref type="bibr" target="#b24">(25)</ref> and the factorization of the fixed kernels</p><formula xml:id="formula_65">P i = L i L T i , i = 1, • • • , m, the marginal likelihood maximiza- tion problem maximize α n+1 ,••• ,α N p(Y N |α n+1 , • • • , α N ) can be put into the form of (13) with p = m(N -n), n o = N -n, Y = Y N , x = α T n+1 • • • α T N T and B tm+i = 0 n×t φ (n + t + 1) • • • φ (N) T L i , t = 0, • • • , N -n -1, i = 1, • • • , m.</formula><p>The problem cannot be handled by using either MKRM-BIC or MKRM-H1 in Section IV-A, because solving <ref type="bibr" target="#b18">(19)</ref> often yields σ 2 0 = 0 and the sparse pattern x [0] is very inaccurate. Instead, as suggested in <ref type="bibr" target="#b43">[44]</ref>, the problem is handled by using MKRM-H2 in Section IV-A.</p><p>Remark 4.3: In our notations, the segmentation problem was formulated in <ref type="bibr" target="#b19">[20]</ref> as</p><formula xml:id="formula_66">θn+1 , • • • , θN = arg min θ n+1 ,••• ,θ N N ∑ t=n+1 (y t -φ (t) T θ t ) 2 + γ||θ t -θ t-1 || 2 , γ ≥ 0</formula><p>which can be seen as a variant of Group LASSO. In contrast, MKRM induces sparsity in the hyper-parameter space, which in turn results in sparsity in the parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NUMERICAL ILLUSTRATIONS</head><p>The proposed MKRM, MM Algorithm 3.1 and implementation are tested for the model estimation problem in Section II and the structure detection problems in Sections IV-B and IV-C. Before proceeding to the details, some common settings for all simulations are given.</p><p>Generic systems. Generic systems should be representatives of real-life systems in that the underlying system is not of low order but could allow good low order approximations. The generic system that will be tested is generated in the same way as detailed below. A SISO continuous-time system of 30th order is first generated using the command m=rss <ref type="bibr" target="#b29">(30)</ref> in MATLAB. The continuous-time system m is then sampled at 3 times of its bandwidth to yield the corresponding discrete-time system md using the following commands in MATLAB: bw=bandwidth(m); f = bw * 3 * 2 * pi; md=c2d(m,1/f,'zoh'). If all poles of md are within the circle with center at the origin and radius 0.95, set the feedthrough matrix of md to 0 and save it as one generic system.</p><p>Unknown initial conditions. Using y(t <ref type="formula">0</ref>), which can be handled in different ways. For convenience, y(t),t = 1, • • • , n, will not be used for KRM for model estimation and MKRM for both model estimation and structure detection.</p><formula xml:id="formula_67">),t = 1, • • • , n requires the unknown u(1 -n), • • • , u(</formula><p>Implementation. All marginal likelihood maximization problems are tackled with the MM Algorithm 3.1 and our custom interior-point method in Python using CVXOPT, see Section III-C. For MKRM, all initializations are randomly generated in MATLAB as x (0) = abs(5 * randn(p,1)) and σ 2(0) = abs(5 * randn(1)).</p><p>A. Model estimation 1) Data-bank of test systems and data sets: The data-bank consists of four collections of 1000 generic systems and data sets: D1, D2, D3 and D4. For each generic system in D1, the associated data set contains 210 data points and is generated as follows: simulate the generic system with an input which is white Gaussian noise with unit variance, and an output additive white Gaussian noise whose variance is one tenth of the variance of the noise-free output. D2 is generated similarly as D1 with the only difference that the output additive white Gaussian noise has the same variance as the noise-free output. D3 and D4 are generated similarly as D1 and D2, respectively, with the difference that a band-limited random Gaussian signal is used to simulate the generic system and moreover, each data set contains 500 data points. The band-limited random Gaussian signal is generated using the command idinput in <ref type="bibr" target="#b44">[45]</ref> with band [0, 0.8], where 0 and 0.8 are the lower and upper limits of the pass band, expressed as fractions of the Nyquist frequency.</p><p>2) Simulation setup and results: The order of the FIR model ( <ref type="formula" target="#formula_4">2</ref>) is set to 100 and two multiple kernels <ref type="bibr" target="#b7">(8)</ref> are generated based on the following collections of fixed kernels:</p><p>• 54 DC kernels (7c). They are obtained by evaluating (7c) on the grid with c = 1, λ = 0.1 : 0.1 : 0.9, and ρ = -0.95, -0.65, -0.35, 0.35, 0.65, 0.95.</p><p>• 21 TC kernels (7b) and 8 SS kernel (7a). The 21 TC kernels are obtained by evaluating (7b) on the grid with c = 1 and λ = 0.1 : 0.05 : 0.75, 0.81 : 0.02 : 0.93. The 8 SS kernels are obtained by evaluating (7a) on the grid with c = 1 and λ = 0.8 : 0.02 : 0.94. In fact, P i can also be constructed based on an available model estimate G(q). This idea is motivated by the form of the optimal kernel that minimizes the MSE matrix</p><formula xml:id="formula_68">E [( θ R N - θ 0 )( θ R N -θ 0 ) T ]. According to [6, Thm. 1], E [( θ R N -θ 0 )( θ R N - θ 0 ) T ] is minimized at P opt = θ 0 θ T 0 .</formula><p>So it is natural to construct P i = θ (G(q))( θ (G(q))) T , where θ (G(q)) is the column vector containing the first n impulse response coefficients of G(q). Note that G(q) can be either data-driven or data-free. For example, it can be an output error (OE) model G(q, θ oe N ) with a suitable order estimated based on {u(t), y(t)} N t=1 , see <ref type="bibr" target="#b0">[1]</ref>. For illustration, we consider the third multiple kernel <ref type="bibr" target="#b7">(8)</ref> with the collection of fixed kernels:</p><p>• 6 kernels in the form of θ (G(q, θ oe N ))( θ (G(q, θ oe N ))) T , where G(q, θ oe N ) are OE model estimates of order 2 to 7 using the oe command in <ref type="bibr" target="#b44">[45]</ref>. Note that for this kind of fixed kernels, the factorization L i = θ (G(q, θ oe N )) and n i = 1 in the derivation of <ref type="bibr" target="#b12">(13)</ref>.</p><p>Remark 5.1:</p><formula xml:id="formula_69">Assume P i = θ (G i (q))( θ (G i (q))) T , i = 1, • • • , m,</formula><p>where G i (q), i = 1, . . . , m are some available model estimates. Solving <ref type="bibr" target="#b10">(11)</ref> </p><formula xml:id="formula_70">yields α = ĉ1 , • • • , ĉm T , P( α) = ∑ m i=1 ĉi θ (G i (q))( θ (G i (q))) T , and θ R N = ∑ m i=1 a i ĉi θ (G i (q)), where a i = ( θ (G i (q))) T (Φ N Φ T N P( α) + σ 2 I n ) -1 Φ N Y N , i = 1, • • • , m. Since a i is a scalar, θ R</formula><p>N is a weighted average over the impulse responses of the model estimates G i (q), i = 1, • • • , m. That means for this multiple kernel, MKRM is closely related with the composite modeling in <ref type="bibr" target="#b45">[46]</ref>. It is also interesting to note that only some of G i (q) actually contribute to θ R N since α is often sparse.</p><p>The three multiple kernels are denoted below by "DC-M", "TCSS-M" and "OE(2:7)-M", respectively. The noise variance σ 2 is estimated together with α by maximizing the marginal likelihood and the impulse response estimate θ R N is computed according to (5b). The proposed approach is compared with the KRM <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> with kernels <ref type="bibr" target="#b6">(7)</ref> where the implementation in <ref type="bibr" target="#b46">[47]</ref> is used. It is also compared with the ML/PEM (the oe command in <ref type="bibr" target="#b44">[45]</ref> is used) equipped with both AIC (Akaike's information criterion) and cross validation (CV) to select the best model order testing orders 1 : 1 : 30. To evaluate various approaches, the impulse response estimates ĝk , k = 1, • • • , n, are compared to the true ones by the measure</p><formula xml:id="formula_71">W = 100   1 - ∑ 100 k=1 |g 0 k -ĝk | 2 ∑ 100 k=1 |g 0 k -ḡ0 | 2 1/2   , ḡ0 = 1 100 100 ∑ k=1 g 0 k .</formula><p>(26) where W corresponds to the "fit" in the compare command in <ref type="bibr" target="#b44">[45]</ref>. The results are shown in the following table where "AF" denotes the average fit <ref type="bibr" target="#b25">(26)</ref> and "NO" denotes the number of outliers below zero for the associated data collection: It is interesting to study the distribution of the fits over individual data collections, which are shown by box-plots in Fig. <ref type="figure" target="#fig_1">2</ref>. As can be seen from the table and Fig. <ref type="figure" target="#fig_1">2</ref>, for all four data collections there is always one multiple kernel, for which the MKRM outperforms the other approaches. Moreover, MKRM with TCSS-M gives the overall best performance. Remark 5.2: It should be noted that for PEM-AIC and PEM-CV, the oe command in MATLAB uses all data points {u(t), y(t)} N t=1 . If all data points are used for KRM and MKRM and the unknown u(1n), • • • , u(0) are set to zero, the performance of KRM and MKRM can be further improved. For example, the average fit for MKRM with DC-M and TCSS-M increases from 84.4 to 87.1 and 86.9, respectively. It is also interesting to note that PEM works much worse for D3 and D4 than KRM and MKRM. The reason is that with band limited input, there is not full information in the data about the impulse response, and kernel-based regularization methods benefit from the smoothness assumption implicitly present in the regularization.</p><p>Remark 5.3: When using DC-M and TCSS-M, as the grid density increases from small to large, the performance tends to increase but it will remain virtually the same after certain point even if a more dense grid is used. In fact, overfitting is avoided thanks to the use of the marginal likelihood maximization  for hyper-parameter estimation. What's more, even for model estimation, DC-M and TCSS-M often have sparse hyperparameters. For illustration, the average number of "alive" fixed kernels in TCSS-M for the four data collections are 3.92, 2.95, 5.20, and 3.58, respectively, where the P i in ( <ref type="formula" target="#formula_15">8</ref>) with ĉi &gt;1e-5 is identified as alive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse dynamic network identification 1) Data-bank of test systems and data sets:</head><p>The data-bank consists of two collections of 500 data sets: D5 and D6. Each data set in D5 contains 600 data points and is generated as follows. First, 10 generic systems are generated. The command zInd = unique(sort(randi(10,1,10),'ascend')); Ind = ones(10,1);Ind(zInd) = 0; in MATLAB is then used to generate Ind. Each element of Ind describes if the corresponding input or generic system has influence on the overall output: "1" means true and "0" means otherwise. Those systems which have influence on the overall output are simulated individually with an input which is white Gaussian noise with unit variance. Then the individual simulated outputs are summed and the sum is regarded as the overall noise free output. Further the noise free output is perturbed by an additive white Gaussian noise whose variance is one tenth of the variance of the noise free output. D6 is generated similarly as D5 with the only difference that the output additive white Gaussian noise has the same variance as the noise-free output.</p><p>2) Simulation setup and results: The order of all FIR models ( <ref type="formula">21</ref>) is set to 100 and the multiple kernel ( <ref type="formula" target="#formula_15">8</ref>) is generated based on 6 TC kernels (7b), which are obtained by evaluating (7b) on the grid with c = 1 and λ = 0.82 : 0.02 : 0.92. As shown in Section IV-B, the problem is converted to a problem of finding a suitable sparse pattern of hyper-parameters and handled by using MKRM-BIC and MKRM-H1 in Section IV-A. In particular for MKRM-H1, o h is set to 8e-3 for D5 and 3e-3 for D6. Here, MKRM is compared with the stable spline exponential hyper-prior (SSEH) approach <ref type="bibr" target="#b18">[19]</ref> and the group LAR <ref type="bibr" target="#b8">[9]</ref>, which are implemented as described in <ref type="bibr" target="#b18">[19]</ref>. The percentage of whether the inputs have influence on the output or not is correctly identified is summarized in the following table, which shows that, MKRM works comparably as SSEH and they all behave better than Group LAR. in MATLAB are generated. The system M1 is simulated with the input u and the simulated output is denoted by ynf1. At the 301st time instant, the other system M2 is switched on and is simulated as follows: ynf2=ynf1(301)+sim(M2,u(301:500)) where ynf2 denotes the simulated output. Then set ynf=[ynf1;ynf2] as the noise free output. The measurement output is then collected by disturbing the noise free output with an output additive white Gaussian noise whose variance is one tenth of the variance of the noise free output. In this way we get the test data set which contains 500 data points and has the change occurring at t ⋆ = 301. The impulse response of the two generic systems together with their difference and the profile of the measurement output are shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>2) Simulation setup and results: The order of FIR models (23) at each time instant t is set to 100. The multiple kernel ( <ref type="formula" target="#formula_15">8</ref>) is generated based on 3 TC kernels (7b), which are obtained by evaluating (7b) on the grid with c = 1 and λ = 0.8 : 0.06 : 0.92. As shown in Section IV-C, the problem is converted to a problem of finding a suitable sparse pattern of hyper-parameters and handled by using MKRM-H2 in Section IV-A. Since there is only one change, we can simply tune the sparse pattern of hyper-parameters by tuning σ 2 and solving (13) such that there is only one αt = [ ĉ1,t , ĉ2,t , ĉ3,t ] T = 0 among t = 101, • • • , 500. That means that the system changes its dynamics at that time instant. It turns out that σ 2 = 6 is a suitable choice. From the profile of ∑ 3 i=1 ĉi,t , t = 101, • • • , 500, in Fig. <ref type="figure" target="#fig_2">3</ref>, obtained by solving (13) with σ 2 = 6, we see clearly the system changes its dynamics around t = 301.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>While kernel techniques have been used for quite some time in linear regression model estimation problems in statistics and machine learning, they have only recently been introduced in the system identification literature. This has led to several contributions on how to choose suitable kernels for identification applications. In this paper we have discussed the use of multiple kernels and pointed to three distinct advantages with such a choice.</p><p>Firstly, that they can handle estimation of models with complicated dynamics, e.g., with widely spread time constants, better than well-tuned single kernels.</p><p>Secondly, that estimation of their weights by maximizing the marginal likelihood has an inherent feature of favoring sparse optimal weights. This method thus has an interesting potential for structure detection problems, such as finding the most important links in networked systems, and segmentation of time-varying systems.</p><p>Thirdly, the marginal likelihood maximization problem is a difference of convex programming problem, whose locally optimal solutions can be found efficiently using sequential convex optimization techniques. In particular, each subproblem can be solved efficiently using an interior-point method where the cost of a single interior-point iteration grows linearly in the number of fixed kernels. Monte Carlo simulations show that the locally optimal solutions lead to good performance, regardless of the initialization, which is a practical advantage over the maximum likelihood/prediction error method and the kernel-based regularization method with nonlinearly parameterized kernels where the initialization is critical and tricky.</p><p>A key issue to use multiple kernels is how to design suitable fixed kernels. A simple but effective way is to use the state of art single kernels SS, TC and DC, see <ref type="bibr" target="#b6">(7)</ref>: introduce a grid (could be uniform if there is no other prior knowledge about the unknown system) over the compact domain of β and generate fixed kernels on the points of the grid. As the grid density (the number of fixed kernels) increases from small to large, the performance tends to increase but it will remain virtually the same after certain point even if a more dense grid is used. In contrast with DC kernel, both TC and SS kernels have dim β = 1, which becomes advantageous in the design of fixed kernels in a computational perspective. Moreover, TC and SS kernels enjoy some interesting maximum entropy properties <ref type="bibr" target="#b16">[17]</ref>. In some sense, they represent the least committing Bayesian priors when regularity and stability is the only information about the unknown system. Hence, in the design of fixed kernels for system identification, the combination of TC and SS, adopted by TCSS-M, appears a natural and efficient choice. Instead of SS, TC and DC kernels, different kernels could be however used when more information on the unknown system is available. For instance, when identifying relaxation systems, it could be advantageous to resort to kernels whose sections are completely monotonic (one example is the exponential kernel in [31, Eq. (4.2)]).</p><p>Motivated by the form of the optimal kernel in the sense of the minimizing the MSE, another way to design fixed kernels is to use rank-1 kernels θ (G(q))( θ (G(q))) T , see Section V-A, where G(q) can be either data-driven or data free. Design of data-driven rank-1 kernels is more involved and interested readers are referred to <ref type="bibr" target="#b47">[48]</ref> for guidelines. An interesting research topic is how to construct multiple data-free rank-1 kernels in an efficient way and enrich the multiple SS, TC and DC kernels with the data-free rank-1 kernels. In fact, this topic is closely related with the compressive sensing and basis selection <ref type="bibr" target="#b11">[12]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Box-plots of the 1000 model fits (left) and the estimated impulse responses for one instance (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Box-plots of the 1000 fits: Left plot for D1 (top) and D2 (bottom); Right plot for D3 (top) and D4 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The impulse response of the two generic systems and their difference (left), the measurement output (middle) and the profile of ∑ 3 i=1 ĉi,t , t = 101,••• ,500, obtained by solving (13) with σ 2 = 6 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAC.2014.2351851, IEEE Transactions on Automatic Control IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014 10</figDesc><table /><note><p>0018-9286 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For MKRM-H1, the threshold o h is tuned on a small number of data sets by cross validation. It is reasonable to have smaller o h for D6 because each data set has larger noise and leads to smaller estimate of the hyper-parameters, which in turn has less influence on the marginal likelihood. First, two generic systems M1 and M2 and a white Gaussian noise input u=randn(500,1)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Remark 5.4: C. Segmentation of linear systems</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1) Test data set:</cell></row><row><cell cols="5">Data Group LAR SSEH MKRM-BIC MKRM-H1</cell></row><row><cell>D5</cell><cell>83.5%</cell><cell>98.0%</cell><cell>98.3%</cell><cell>99.0%</cell></row><row><cell>D6</cell><cell>81.7%</cell><cell>94.1%</cell><cell>90.6%</cell><cell>93.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. XX, NO. X, X 2014</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Preprint submitted to IEEE Transactions on Automatic Control. Received: January 10, 2014 04:58:54 PST</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors thank the reviewers and guest editors for their constructive comments. This research has been partially supported by the Linnaeus Center CADICS, funded by the Swedish Research Council, and the ERC advanced grant LEARN, no 287381, funded by the European Research Council as well as by the MIUR FIRB project "Learning meets time" (RBFR12M3AC) and European Community's Seventh Framework Programme [FP7/2007-2013] under agreement n. 257462 HYCON2 Network of excellence.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">System Identification -Theory for the User</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Upper Saddle River, N.J.</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">System Identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Söderström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall Int</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">System Identification: A frequency domain approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pintelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schoukens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Wiley-IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new kernel-based approach for linear system identification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Nicolao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction error identification of linear systems: a nonparametric Gaussian regression approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Nicolao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="305" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the estimation of transfer functions, regularizations and Gaussian processes -Revisited</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1525" to="1535" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Four encounters with system identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hjalmarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Control</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="449" to="471" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the relevance vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning for basis selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2153" to="2164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative reweighted and methods for finding sparse solutions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="329" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Selected Topics in Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convex vs nonconvex estimators for regression and sparse estimation: the mean squared error properties of ARD and GLasso</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel selection in linear system identification. Part I: A Gaussian process perspective</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Nicolao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 50th IEEE Conference on Decision and Control and European Control Conference</title>
		<meeting>50th IEEE Conference on Decision and Control and European Control Conference<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="4318" to="4325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel selection in linear system identification. Part II: A classical perspective</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 50th IEEE Conference on Decision and Control and European Control Conference</title>
		<meeting>50th IEEE Conference on Decision and Control and European Control Conference<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="4326" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Bayesian approach to sparse dynamic network identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1553" to="1565" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of ARX-models using sum-of-norms regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ohlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1107" to="1111" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convex analysis aprroach to D. C. programming: Theory, Algorithms and Applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T H</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACTA Mathematica Vietnamica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="289" to="355" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DC programming: Overview</title>
		<author>
			<persName><forename type="first">R</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Thoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The concave-convex procedure (CCCP)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A tutorial on MM algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bayes and Empirical Bayes methods for data analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantifying the error in estimated transfer functions with application to model order selection</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ninness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automatic Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="913" to="929" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Asymptotic properties of the least-squares method for estimating transfer functions and disturbance spectra</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="412" to="440" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning, ICML &apos;04</title>
		<meeting>the twenty-first international conference on Machine learning, ICML &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with kernel methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="637" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernels for linear time invariant system identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dinuzzo</surname></persName>
		</author>
		<idno>abs/1203.4930</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the convergence of the concave-convex procedure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1759" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new view of automatic relevance determination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convex optimization in identification of stable non-linear state space models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tobenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manchester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Megretski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th IEEE Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="7232" to="7237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interior gradient and proximal methods for convex and conic optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auslender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">697</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Applications of second-order cone programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lebret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="193" to="228" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interior-point polynomial methods in convex programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="1994">1994</date>
			<publisher>SIAM</publisher>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On implementing a primaldual interior-point method for conic quadratic optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Terlaky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="277" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">CVXOPT: A Python package for convex optimization, version 1.1.5</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<ptr target="http://abel.ee.ucla.edu/cvxopt" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interiorpoint methods for large-scale cone programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="55" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayes factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An empirical Bayesian strategy for solving the simultaneous sparse approximation problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3704" to="3716" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">System Identification Toolbox for use with MATLAB</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>The MathWorks, Inc</publisher>
			<biblScope unit="volume">8</biblScope>
			<pubPlace>Natick, MA</pubPlace>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Composite modeling of transfer functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hjalmarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gustafsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="820" to="832" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Implementation of algorithms for tuning parameters in regularized least squares problems in system identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2213" to="2220" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rank-1 kernels for regularized system identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pillonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 52nd Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="5162" to="5167" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
