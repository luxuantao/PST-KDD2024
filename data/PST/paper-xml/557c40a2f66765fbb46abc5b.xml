<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Resource and Environmental Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Resource and Environmental Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Resource and Environmental Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Resource and Environmental Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Resource and Environmental Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">396D83DB65BAF398D4735F2BB9301FE8</idno>
					<idno type="DOI">10.1109/JSTARS.2013.2283236</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Remote sensing image pan-sharpening is an important way of enhancing the spatial resolution of a multispectral (MS) image by fusing it with a registered panchromatic (PAN) image. The traditional pan-sharpening methods often suffer from color distortion and are still far from being able to synthesize a real high-resolution MS image, as could be directly acquired by a better sensor. Inspired by the rapid development of sparse representation theory, we propose a two-step sparse coding method with patch normalization (PN-TSSC) for image pan-sharpening. Traditional one-step sparse coding has difficulty in choosing dictionary atoms when the structural information is weak or lost. By exploiting the local similarity between the MS and PAN images, the proposed sparse coding method deals with the dictionary atoms in two steps, which has been found to be an effective way of overcoming this problem. The experimental results with IKONOS, QuickBird, and WorldView-2 data suggest that the proposed method can effectively improve the spatial resolution of a MS image, with little color distortion. The pan-sharpened high-resolution MS image outperforms those images fused by other traditional and state-of-the-art methods, both quantitatively and perceptually.</p><p>Index Terms-Image fusion, pan-sharpening, remote sensing image, two-step sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H IGH spatial resolution multispectral (MS) images are favored by a lot of remote sensing applications, and a number of high-resolution remote sensing systems have been launched and widely used, such as IKONOS, QuickBird, GeoEye-1, WorldView-1, and WorldView-2. Due to the physical limitations, it is difficult to acquire a high-resolution MS image directly. One commonly used strategy is to use two sensors to separately acquire a high-resolution panchromatic (HRP) image and a low-resolution multispectral (LRM) image, and then utilize a pan-sharpening technique to produce a high-resolution multispectral (HRM) image by fusing the two images.</p><p>For the pan-sharpening task, the key problem is to find and utilize the relationship between the LRM image and the HRP image. A lot of pan-sharpening methods have been developed to date, and they can be categorized into four types: transformation and substitution based, arithmetic based, ARSIS <ref type="bibr" target="#b0">[1]</ref> concept based (from its French name Amélioration de la Résolution Spatiale par Injection de Structures), and restoration based. The transformation and substitution based methods first transform the resampled LRM image to a space where one of its components is strongly correlated with the low-resolution panchromatic (LRP) image degraded from the HRP image; the HRP image is then stretched to replace the component; finally, an inverse transform is performed. The transformation and substitution based methods have been widely used, e.g., intensity-hue-saturation (IHS or LHS) <ref type="bibr" target="#b1">[2]</ref>, principal component analysis (PCA) <ref type="bibr" target="#b2">[3]</ref>, and the Gram-Schmidt (GS) process <ref type="bibr" target="#b3">[4]</ref>. As to the arithmetic based methods, the operations of multiplication, division, addition, and subtraction are combined in different ways to achieve a better fusion performance <ref type="bibr" target="#b4">[5]</ref>. Among them, the Brovey transform <ref type="bibr" target="#b4">[5]</ref> is the most well known. For the ARSIS concept based methods, these approaches assume that the missing spatial information in the LRM image can be obtained from the high frequencies of the HRP image, so the relationships between the high frequencies in the HRP image and the LRM image are searched and exploited <ref type="bibr" target="#b0">[1]</ref>. The typical ARSIS concept based methods are high-pass filtering (HPF) <ref type="bibr" target="#b2">[3]</ref> and the wavelet based methods <ref type="bibr" target="#b5">[6]</ref>. The above three types of methods can be viewed as traditional methods <ref type="bibr" target="#b6">[7]</ref>, which tend to cause color distortions due to the wavelength extension of the new satellite PAN images <ref type="bibr" target="#b6">[7]</ref>. Inspired by the rapid development of the single-image super-resolution technique, restoration based methods have recently become popular. For the restoration based methods, the LRM image and the HRP image are both viewed as observations of the HRM image via the image degradation model, and a prior of the target HRM image should be assumed <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. In addition to the four main types of methods, hybrid methods have also been developed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> and have shown improved performance.</p><p>Recently, the compressed sensing (CS) technique has been applied to pan-sharpening <ref type="bibr" target="#b13">[14]</ref>. Since the fused image patch is adaptively estimated with different sets of dictionary atoms, the CS based method achieves impressive results. However, the applicability is limited as the dictionary construction method needs HRM training images, which may not be available. To deal with this problem, Jiang et al. <ref type="bibr" target="#b14">[15]</ref> constructed a joint dictionary from upsampled LRM and HRP training images, which are easily acquired from the available remote sensing systems. The shortcoming of the method in <ref type="bibr" target="#b14">[15]</ref> is that it still needs a large collection of training images, and the trained dictionary is also relatively large, which takes much computing time for sparse solutions. Following the well-known coupled sparse coding technique for natural image super-resolution <ref type="bibr" target="#b15">[16]</ref>, the latest work <ref type="bibr" target="#b16">[17]</ref> brings the coupled sparse coding technique to image pan-sharpening. The method in <ref type="bibr" target="#b16">[17]</ref> shows good potential by only using the HRP image and its degraded version, which has the same resolution as the LRM image. The coupled image pairs are directly used, without training, to construct two coupled dictionaries, which consist of a high-resolution dictionary and a low-resolution dictionary. Since we assume that the sparse coefficients of the target HRM patch over the high-resolution (HR) dictionary are the same as the sparse coefficients of the LRM patch over the low-resolution dictionary, the sparse coefficients of every LRM patch should be estimated as accurately as possible. However, at the sparse coding stage, the method in <ref type="bibr" target="#b16">[17]</ref> relies strongly on the structural information of the LRM patch, which may be lost or very weak, due to its low resolution, so the coding of the LRM patch may be contaminated by unsuitable low-resolution (LR) dictionary atoms, for which the corresponding HR dictionary atoms cannot effectively represent the ideal HRM patch. From the experimental results in Section IV, it is found that the method in <ref type="bibr" target="#b16">[17]</ref> preserves the spectral properties well, but it shows poor spatial results in the regions with small objects or fine details.</p><p>In view of this, we propose a novel sparse coding based remote sensing image pan-sharpening method. The main contribution of this paper is twofold. First, a two-step sparse coding is used to obtain the sparse coefficients, instead of the traditional one-step sparse coding. For the two-step sparse coding, we fully utilize the strong correlation between the LRM patch and the corresponding LRP patch, and thus overcome the problem of structural information loss in some LRM patches. This greatly improves the spatial details but may introduce a little color distortion in some cases. To handle this issue, the patch normalization strategy, which aims to improve the stability of the sparse coding, is exploited and used to prevent the color distortion that may happen when using the two-step sparse coding on the image patch directly, and further improves the fusion result. Moreover, since the proposed method deals with each patch independently, it can be fast with a multi-thread implementation. For simplicity, the proposed method also processes each MS band separately, as in <ref type="bibr" target="#b16">[17]</ref>, and thus less memory and computational burden are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PAN-SHARPENING WITH SPARSE REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Representation</head><p>Inspired by the research into the receptive fields, sparse representation has been proposed as a powerful statistical image modeling technique <ref type="bibr" target="#b17">[18]</ref>. Since the emergence of CS theory <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, the theory and application of sparse representation, which is strongly correlated with CS, has been widely studied.</p><p>In fact, natural images tend to be sparse in the redundant image domain <ref type="bibr" target="#b20">[21]</ref>. The redundant image domain is often called as the dictionary, where the image can be represented as a linear combination of only a small number of the dictionary atoms. For practical applications, the images are processed patch by patch, and it has been shown that a trained dictionary is better than the fixed redundant bases such as discrete cosine transform (DCT) <ref type="bibr" target="#b21">[22]</ref>. The sparse representation problem is often represented as: <ref type="bibr" target="#b0">(1)</ref> where indicates the -norm, and denotes the -norm. is a column vector representing a signal or a lexicographically ordered image patch. is a matrix representing the dictionary, with each column called as an atom. is the vector of the sparse coefficients, with most of its coefficients being close to or zero.</p><p>is the regularization parameter. Since (1) is well known as an NP-hard problem, greedy algorithms such as orthogonal matching pursuit (OMP) are often used to tackle this problem. An alternative approach is to relax (1) to an -norm convex optimization problem <ref type="bibr" target="#b18">[19]</ref>: <ref type="bibr" target="#b1">(2)</ref> where indicates the -norm. Many -norm optimization methods have been proposed to solve this convex problem <ref type="bibr" target="#b22">[23]</ref>, and the least angle regression stagewise (LARS) algorithm <ref type="bibr" target="#b23">[24]</ref> has been widely used due to its good performance and stability. In this paper, the LARS algorithm package developed by Julien Mairal et al. <ref type="bibr" target="#b24">[25]</ref>, optimized with multi-thread implementation, is adopted for our second sparse coding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse Representation With Coupled Dictionaries</head><p>Sparse representation was first applied to single natural image super-resolution by Yang et al. <ref type="bibr" target="#b15">[16]</ref>, and the main idea of the method is to assume an occurrence relation in that the upsampled LR and HR image patch pairs share the same sparse coefficients with respect to their own dictionaries. Supposing that coupled dictionaries are available (Section II-C describes how to construct the coupled dictionaries for pan-sharpening), then for a certain upsampled LR image patch, the sparse coefficients of the LR image patch over the LR dictionary are first solved by using <ref type="bibr" target="#b1">(2)</ref>. Then, with this assumption, the sparse coefficients of the LR image patch are taken as the sparse coefficients of the HR image patch over the HR dictionary. Finally, the target HR image patch is obtained by multiplying the HR dictionary with the sparse coefficients. Recently, Zhu and Bamler <ref type="bibr" target="#b16">[17]</ref> brought this idea to the application of pan-sharpening, with each band of the LRM image being processed one by one. Due to its effectiveness, this idea is also adopted in our proposed method. Note that in the work of Yang et al. <ref type="bibr" target="#b15">[16]</ref>, the first-and second-order gradients of the upsampled LR image patch are concatenated into one vector to represent the LR image patch, and so are the LR training samples when training the dictionaries. However, in our work, the images are used directly, which is more suitable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Construction of the Coupled Dictionaries for Pan-Sharpening</head><p>For pan-sharpening, a corresponding LRM image and HRP image are available, and the goal is to produce the HRM image by fusing and . Unlike the traditional sparse representation based methods, which need external image collections to train the dictionaries, the coupled dictionaries are constructed directly from the panchromatic (PAN) image and its degraded version <ref type="bibr" target="#b16">[17]</ref>. The HRP image is first blurred and downsampled by a factor to get the LRP image , which has the same resolution and the same image size as the LRM image</p><p>. We extract all the patches from the LRP image with a patch size and a step size , then rearrange every patch to a vector. Finally, we build a matrix with these vectors, which is viewed as the LR dictionary. Following the same method, the HR dictionary is obtained from the HRP image with a patch size and a step size . In this way, the numbers of the dictionary atoms of the LR dictionary and the HR dictionary are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PAN-SHARPENING WITH TWO-STEP SPARSE CODING</head><p>The overall flowchart of the proposed pan-sharpening method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. For the proposed pan-sharpening method, the coupled dictionaries are constructed as described in Section II-C, and the next task is to find the sparse representations of the LR patches over the LR dictionary, and reconstruct the fused patches with the HR dictionary, as mentioned in Section II-B. In the latest work <ref type="bibr" target="#b16">[17]</ref>, at the sparse coding stage, every patch of the LRM image is coded with one step by directly using (2) to obtain the sparse coefficients over the same LR dictionary. In our method, a two-step sparse coding approach is utilized that fully considers the structural similarity between the LRP patch and the LRM patch. In addition, the patch normalization technique is used to improve the stability of the sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Patch Structural Similarity Property</head><p>For the th band of the LRM image , all the patches are extracted with a patch size and a step size . For the th patch (we use upper-case to represent an image and lower-case to denote a patch from it), it is strongly correlated with the corresponding patch from the LRP band, as the patches share the same scene and are often acquired by different sensors with an overlapping spectral domain. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, despite the differences among the spectral responses, each patch from the LRM bands tends to share a similar structural pattern to the patch at the same location on the LRP band. We name this important discovery the "patch structural similarity property." In addition, we call the th LR dictionary atom (also denoted by ), which comes from the column stacked patch , the "adjoint-atom" of the LRM patch .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two-Step Sparse Coding</head><p>For the traditional sparse coding, when the LR image patch to be coded is weak in its structural information, it will be easily contaminated by unsuitable dictionary atoms, which may have a negative effect on the performance of the image pan-sharpening. Therefore, an intuitive way is to use a two-step sparse coding to distinguish the more reliable atoms from the less reliable ones. That is, at the first sparse coding step, the reliable atoms are used to sparsely represent the vector, and at the second sparse coding step, the rest of the atoms are used to sparsely represent the residual produced by the first sparse coding step. How to find the more reliable atoms remains a problem, which may differ between different applications. However, motivated by the patch structural similarity property of the LRP and LRM images, the adjoint-atom of the patch to be coded is sure to be a reliable atom. The proposed two-step sparse coding method is therefore simplified by using only one reliable atom at the first sparse coding step. In this way, the adjoint-atom is treated differently when we find the sparse coefficients of over the LR dictionary . We first use this reliable atom to construct a special one-atom dictionary (also denoted by ), and then use (3) to sparsely represent the LRM patch at the first sparse coding step:</p><p>(3) where is the coefficient and is the regularization parameter. Since the special dictionary consists of only one atom, is a constant, and the sparse term can be omitted. We then get: <ref type="bibr" target="#b3">(4)</ref> It is easy to solve (4) by letting its derivative be zero. Clearly, cannot be well represented by only one atom, so the second step is to represent the first step's residual over the general LR dictionary , which takes all the patches from the corresponding LRP image as atoms: <ref type="bibr" target="#b4">(5)</ref> where is the vector of sparse coefficients by the second sparse coding step, and is the regularization parameter. Equation ( <ref type="formula">5</ref>) is a traditional sparse coding problem. Since (5) cannot be solved exactly with many dictionary atoms, we relax it to <ref type="bibr" target="#b5">(6)</ref> and use the LARS algorithm optimized with multi-thread implementation (which can be downloaded from Julien Mairal's personal page <ref type="bibr" target="#b25">[26]</ref>) to speed up the proposed method. <ref type="bibr" target="#b5">(6)</ref> With the two-step sparse coding, the final sparse vector is acquired by: <ref type="bibr" target="#b6">(7)</ref> where is a vector with the same dimension as , with only the th element being one and the others being zero. Finally, as mentioned in Section II, the fused patch is reconstructed by: (8) By using ( <ref type="formula">4</ref>), ( <ref type="formula">6</ref>), <ref type="bibr" target="#b6">(7)</ref>, and ( <ref type="formula">8</ref>), all the patches from each band of the LRM image are processed independently to acquire the corresponding fused HRM patches, and the overlapping areas of the fused patches are averaged.</p><p>Note that in the first sparse coding step, the one-atom dictionary varies with the current image patch being processed. However, at the second sparse coding step, the dictionary is the same for all the LRM patches. One may wonder why the adjoint-atom is still in the common dictionary at the second sparse coding step, since it has already been coded in the first sparse coding step. The reason for this is that in the first sparse coding stage, the other dictionary atoms are not considered, so there may be too much priority given to the atom , especially for the cases where the structural similarity between the LRM band patch and the LRP band patch is not very strong, e.g., the NIR patch with the LRP patch in the first row in Fig. <ref type="figure" target="#fig_1">2</ref>, and the blue, green, and red band patch with the LRP patch in the fifth row in Fig. <ref type="figure" target="#fig_1">2</ref>. Therefore, at the second sparse coding step, the adjoint-atom is still considered, to adaptively remedy this problem.</p><p>With the first sparse coding step, the proposed method puts more emphasis on the adjoint-atom than a single-step sparse coding based method. Thus, the two-step sparse coding adaptively utilizes the patch structural similarity property in a simple way, rather than only relying on the structural information of the LRM image patch to infer the correct sparse representation. The experiments in Section IV show that this consideration improves the performance greatly over the traditional one-step sparse coding method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch Normalization</head><p>Patch normalization is widely used in regression problems <ref type="bibr" target="#b26">[27]</ref> to subtract a constant before the main procedures. It has also been used in sparse representation based natural image processing to improve the numerical stability of sparse coding, and has resulted in a better visual quality of the results <ref type="bibr" target="#b27">[28]</ref>. We adopt this strategy in the proposed method, and find that it is applicable to our two-step sparse coding based remote sensing image pan-sharpening process.</p><p>Patch normalization consists of two steps in the proposed method. First, for each atom of the common LR dictionary and HR dictionary, the mean intensity value of the vector is subtracted from all its elements. Second, for the LRM patch , the mean intensity value is also subtracted from all its elements before the first sparse coding step in (4), and is added back to all the elements of the fused patch after the patch reconstruction in <ref type="bibr" target="#b7">(8)</ref>. With this simple processing strategy, the spectral quality of the proposed two-step sparse coding is improved greatly, and the resulting fused image looks even smoother.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSIONS</head><p>In Wald's <ref type="bibr" target="#b28">[29]</ref> view, a synthetic image should be as similar as possible to the image that the corresponding sensor would observe at the highest spatial resolution. This idea is adopted here to assess the different pan-sharpening methods. In this section, we use IKONOS, QuickBird, and WorldView-2 data to test the performance of the proposed method. To quantitatively assess the quality of the results, the original data are low-pass filtered and degraded (for convenience, we use MATLAB's "imresize" function, with the "antialiasing" parameter being "true", for degrading images in this paper). The experiments are performed on the degraded data, and the original MS image is used as the reference. All the data used in the following experiments are Geo format images, and the MS image and PAN image are well corresponded with sub-pixel accuracy. All the images are 11-bit without dynamic range adjustment, and the final results are displayed by ENVI 4.7.</p><p>We compare the proposed method with three popular traditional methods: generalized IHS (GIHS) <ref type="bibr" target="#b1">[2]</ref>, GS <ref type="bibr" target="#b3">[4]</ref> (implemented with ENVI 4.7, and we select the average of the LRM as the LRP), AWLP <ref type="bibr" target="#b29">[30]</ref>, and the state-of-the-art adjustable model based method (denoted by AM) <ref type="bibr" target="#b9">[10]</ref>. The method used in the related study <ref type="bibr" target="#b16">[17]</ref>, which uses the traditional sparse coding for pan-sharpening, is also included (denoted by SC). To more comprehensively examine the performance of the proposed approach, we give two sets of results for the proposed method: the results using only two-step sparse coding (denoted by TSSC), and the results using both two-step sparse coding and patch normalization (denoted by PN-TSSC). For a fair comparison, the SC method is implemented on every patch independently, with an averaging operation for the overlapping areas, and the same multi-thread LARS is employed as the -norm optimization tool.</p><p>We evaluate the results both visually and quantitatively. To quantitatively evaluate the experiments, five widely used quality indices are used: the correlation coefficient (CC), the structural similarity metric (SSIM), the spectral angle mapper (SAM), the erreur relative globale adimensionnelle de synthèse (ERGAS), and the Q4 index. Based on Wald's view, these indices call upon distances that measure the discrepancy between a fused image (monomodal or multimodal) and its reference <ref type="bibr" target="#b30">[31]</ref>. The details of these indices can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments With IKONOS Data</head><p>The IKONOS system simultaneously offers a 4 m-resolution MS image with four bands and a single-band 1 m-resolution PAN image. The original MS and PAN images are low-pass filtered and downsampled to acquire the 16 m-resolution and 4 m-resolution test images, respectively, and the original 4 m-resolution MS image is used as the reference. The data set used in this experiment is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. For the AM method, the Huber parameter is set as 2, the adjustable parameter between the enhancement of the spatial information and the preservation of the spectral information is set as 10, and the termination threshold is set as . We choose a patch size and step size of <ref type="bibr">[7 7</ref>] and [3 3], respectively, for the SC, TSSC, and PN-TSSC methods. For the SC method, in (2) is set as . For the TSSC and PN-TSSC methods, in ( <ref type="formula">6</ref>) is set as and , respectively. We set the parameters of the proposed method by taking both performance and computation time into consideration, and the influence of these parameters is analyzed in Appendix B.</p><p>The experimental results of the seven pan-sharpening methods are shown in Fig. <ref type="figure" target="#fig_3">4</ref>(b)-(h), respectively. To facilitate a spatial comparison, detailed regions are illustrated in the top-left corner of the images. From the middle-right vegetation area, it can be clearly observed that the GIHS result suffers from severe color distortion when compared with the original MS image, and many details of the PAN image which do not belong to the original MS image are contained in the fused image. The AWLP result has good color, but some small details are lost, and the result does not look very sharp. For the GS method, the result seems good, perceptually, with high contrast. However, compared with the original MS image, some areas are over-sharpened, and the color in the middle-right vegetation area is brighter. The result of the AM method is similar to the GS result, with better color. The SC result shows a good spectral quality, but contains blurring effects and outlier pixels in some regions with small objects or fine details. The main reason for this is that the SC method only relies on the structural information of the LR patch to infer the structural information of the HR patch. As shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>, the structural information of some areas is almost lost, so it is difficult to infer the correct sparse representation of the LRM image patch via one-step sparse coding. By using the two-step sparse coding, which can adaptively utilize the structural similarity property between the LRM patch and its corresponding LRP patch, the TSSC result overcomes the blurring effects observed in the SC result and shows very good spatial detail information, but suffers from some color distortion when compared with the result of the SC method. By simultaneously incorporating both the two-step sparse coding approach and the patch normalization strategy, the proposed method achieves the best overall performance among all the methods. The color of the PN-TSSC method is very similar to that of the SC method and the AWLP method, and it is much better than the colors of the other methods, while the spatial details of the PN-TSSC method are much better than  those of the SC method, and are comparable with or even better than those of the other methods.</p><p>The quantitative assessment results are shown in Table <ref type="table" target="#tab_1">I</ref>, in which the best results for each quality index are marked in bold, and the second best are underlined. B, G, R, and NIR represent the results of the blue, green, red, and near-infrared bands, respectively, and Avg. is the average result of the B, G, R, and NIR results. In our experience, a difference of 0.01 for CC, SSIM, and Q4, and a difference of 0.1 for SAM and ERGAS, indicate significant differences which can be noticed by visual inspection. Bearing this in mind, it can be seen that the quantitative assessment results are consistent with the visual evaluations. The AWLP method injects the high frequencies of the PAN image into every MS band, in proportion to their original values, in such a way that the spectral angle is not changed before and after fusion for the MS image. Due to the blurring effect, SC fails to surpass the traditional methods. Comparing the results of SC, TSSC, and PN-TSSC, the results improve successively from SC to PN-TSSC, which indicates the effectiveness of the two-step sparse coding approach and the patch normalization strategy for the sparse coding based pan-sharpening methods from a quantitative perspective. As a state-of-the-art method, AM is distinctly better than the traditional methods, and PN-TSSC is comparable with or even better than AM in most of the quality measures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments With QuickBird Data</head><p>To test the proposed method with a different sensor, the fourband 2.8 m-resolution MS image and the 0.7 m-resolution PAN image of the QuickBird data are low-pass filtered and resampled to 11.2 m-resolution and 2.8 m-resolution test images, respectively, and the original 2.8 m-resolution MS image is used as the reference, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. For the AM method, the Huber parameter is set as 3, the adjustable parameter is set as 120, and the termination threshold is set as . The patch size and step size are set as <ref type="bibr">[7 7</ref>] and [3 3], respectively, for SC, TSSC, and PN-TSSC. For the SC method, in (2) is set as . For the TSSC and PN-TSSC methods, in ( <ref type="formula">6</ref>) is set as for both methods.</p><p>The results of the seven different methods are presented in Fig. <ref type="figure" target="#fig_5">6(b)-(h)</ref>, respectively. To facilitate a spatial comparison, magnified regions are displayed in the top-left corner of the images. Compared with the traditional methods, the SC method shows a good color appearance, but blurring effects and some outlier pixels are obvious. With the two-step sparse coding, the TSSC method shows good spatial details, but the result is accompanied by slight color distortion. By simultaneously exploiting the two-step sparse coding method and the patch normalization strategy, the proposed PN-TSSC method succeeds in achieving the best overall quality among all the methods. It should also be noted that, despite the proposed method showing superiority in both spatial and spectral qualities, an exception in the tiny region inside the yellow rectangle in Fig. <ref type="figure" target="#fig_5">6</ref> can be observed. Here, there is still a blurring effect for the small building surrounded by trees. The reason for this may be as follows: on the one hand, the small building covers an area of about 17 4 pixels, which is small compared with the reconstructed 28 28 image patch, and the sparse coding step is only concerned with the overall reconstruction accuracy of the whole patch. On the other hand, the occasion of a small building surrounded by trees is a relatively rare occurrence in the whole image, which also means that dictionary atoms similar to this kind of occasion will not be abundant. Therefore, to preserve the spectral quality, tiny details are sacrificed with the PN-TSSC method, and further research into adopting a multi-resolution dictionary may be able to solve this imperfection.</p><p>The quantitative assessment results are shown in Table <ref type="table" target="#tab_2">II</ref>, in which the best results for each quality index are marked in bold, and the second best are underlined. Again, it can be seen that the quantitative assessment results are consistent with the visual evaluations, and are similar to those in Table <ref type="table" target="#tab_1">I</ref>, which indicates that the proposed PN-TSSC method is stable with different kinds of sensors.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments With WorldView-2 Data</head><p>To further test the proposed method with a different sensor type and more bands, we utilize a 2 m-resolution MS image and the corresponding 0.5 m-resolution PAN image, as acquired by the WorldView-2 earth observation satellite. The spectral ranges of all the bands of the WorldView-2 data are given in Table <ref type="table" target="#tab_3">III</ref>. The original data are degraded to obtain the test images, as shown in Fig. <ref type="figure" target="#fig_6">7</ref>. The results of all the pan-sharpening methods are then obtained by fusing the 8 m-resolution MS image and the 2 m-resolution PAN image, and the original 2 m-resolution MS image is used as the reference. For the AM method, the Huber parameter is set as 0.5, the adjustable parameter is set as 30, and the termination threshold is set as . The patch size and the step size are set as <ref type="bibr">[7 7</ref>] and [3 3], respectively, for SC, TSSC, and PN-TSSC. For the SC method, in (2) is set as . For the TSSC and PN-TSSC methods, in ( <ref type="formula">6</ref>) is set as and , respectively. The results of the different methods are presented in Fig. <ref type="figure" target="#fig_7">8</ref>, and zoomed regions are displayed in the top-left corner of the  images to facilitate a spatial comparison. For the GS method, the blurring effect is obvious. This is because GS selects the average of all the LRM bands as the LRP, and the spectral ranges of the coastal band, the NIR-1 band, and the NIR-2 band are both out (almost out for NIR-1) of the range of the panchromatic band. This also affects the AM method, which assumes that the panchromatic band is a linear combination of all the multispectral bands with the same resolution. Since the PN-TSSC method can process every band and every local patch adaptively by choosing atoms from the dictionary, it is relatively robust to this kind of problem. From the results, it can be easily concluded that the proposed PN-TSSC method ranks first among all the methods, with a much better spatial quality.</p><p>The quantitative assessment results are shown in Table <ref type="table" target="#tab_4">IV</ref>, in which the best results for each quality index are marked in bold, and the second best are underlined. C, B, G, Y, R, R-E, NIR-1, and NIR-2 represent the results of the coastal, blue, green, yellow, red, red edge, near-infrared-1, and near-infrared-2 bands, respectively, and Avg. is the average result of the C, B, G, Y, R, R-E, NIR-1, and NIR-2 results. As with the visual evaluations, the proposed PN-TSSC method performs the best among all the methods, in terms of the quantitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cost of Time</head><p>The GS method is implemented in ENVI 4.7, AM is implemented in Microsoft Visual C++ 6.0, and the other methods are implemented in MATLAB 2012a. The personal computer we use is a Dell T1500. The central processing unit (CPU) is an Intel Core i3 540 @ 3.07 GHz with dual-cores and four threads, the RAM is 3 Gb with 3.06 GHz, and the operating system is 32-bit Windows XP. The time costs for the experiments with the IKONOS, QuickBird, and WorldView-2 data are shown in Table <ref type="table" target="#tab_5">V</ref>. From Table <ref type="table" target="#tab_5">V</ref>, it can be seen that the time cost of the proposed PN-TSSC method is comparable with the AM method; however, it should be noted that the proposed method could be easily implemented with parallel processing. It is also reasonable to believe that with the rapid development in computer hardware and computation techniques, the time cost of the proposed method will soon no longer be an issue. For the proposed method, the whole of the PAN image is used to construct the dictionary. Therefore, when the PAN image is very large, i.e., 1000 1000, or even larger, the dictionary will also be large, which will increase the computation time for each patch. A simple solution is to partition the whole image into several parts with overlaps, and process each part with the proposed method separately. In addition to this, a pre-trained dictionary pair for a certain kind of remote sensing system could be a better choice, and this will be investigated in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper presents a novel sparse coding based pan-sharpening method for remote sensing images. Spatial resolution enhancement and spectral information preservation are the two key issues in the remote sensing image pan-sharpening task. On the one hand, to improve the spatial resolution of the existing one-step sparse coding based method, a two-step sparse coding method is utilized on the basis of the patch structural similarity between the low-resolution panchromatic image patch and the low-resolution multispectral image patch. On the other hand, a patch normalization strategy is used to improve the stability of the sparse coding and preserve the spectral information. The experimental results with IKONOS, QuickBird, and World-View-2 data show that the performance of the proposed method is competitive in both the spatial and spectral qualities, and it outperforms both traditional and state-of-the-art methods when considering the overall performance. Moreover, since a whole image can be processed in patches independently with multi-thread calculation, the proposed method can also be fast. With further work, better results may be obtained by considering all the multispectral bands together. A better dictionary should also be considered to speed up the method when the processed image is very large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>The five indices used in this paper are defined as follows.</p><p>1) The Correlation Coefficient (CC): The CC shows the similarity in small-size structures between the reference image band and the fused image band <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>. As a measure of correlation, it should be as close as possible to 1, and it is defined as: <ref type="bibr" target="#b8">(9)</ref> where and are the th bands of the reference image and the fused image, respectively.</p><p>is the covariance between and . and represent the standard deviation of and , respectively.</p><p>2) The Structural Similarity Metric (SSIM): Based on the fact that the human visual system has evolved to extract structural information from images, the SSIM <ref type="bibr" target="#b31">[32]</ref> was developed to capture the loss of image structure. It has been widely used and is considered to be better than the root mean square error (RMSE) since RMSE only measures the radiometric distortion of the fused image band from the reference image band. The value of the SSIM should be as close as possible to 1, and the SSIM metric between the th band of the reference and that of the fused image is defined as: <ref type="bibr" target="#b9">(10)</ref> where , , and are the luminance, contrast, and structural comparison measures. , , and are parameters used to define the relative importance of the three components, given as follows: <ref type="bibr" target="#b10">(11)</ref> (12) <ref type="bibr" target="#b12">(13)</ref> where and are the mean values of and , respectively. , , and are small constants given as: , , and , respectively. is the dynamic range of the pixel values. In this paper, we set , , , and , as suggested in <ref type="bibr" target="#b31">[32]</ref>.</p><p>3) The Spectral Angle Mapper (SAM): The SAM measures the absolute angle between the spectral vectors of the reference and fused images, which reflects the spectral distortion introduced by the fusion process. The ideal value is 0, and it is defined as: <ref type="bibr" target="#b13">(14)</ref> where and are the th pixels of the th bands of the reference and fused images, respectively. and are the horizontal and vertical sizes of the image.</p><p>4) The Erreur Relative Globale Adimensionnelle De Synthèse (ERGAS): ERGAS is widely used and it gives a global depiction of the radiometric difference between the reference and fused images. ERGAS fulfills three requirements: the independence of units, the independence of the number of spectral bands, and the independence of the ratio of the scales <ref type="bibr" target="#b32">[33]</ref>. The ideal value for ERGAS is 0, and it is defined as: <ref type="bibr" target="#b14">(15)</ref> where and are the spatial resolutions of the PAN image and the MS image, respectively. For the IKONOS, QuickBird, and WorldView-2 data, is four times as large as , and is the number of the bands. The RMSE is defined as:</p><p>5) The Q4 Index: Based on the theory of quaternions, the Q4 index <ref type="bibr" target="#b33">[34]</ref> is an extension of the Q index <ref type="bibr" target="#b34">[35]</ref>, which is a product of CC, mean bias, and contrast variation. The Q4 index is suitable for testing both the spectral and spatial qualities of fused images <ref type="bibr" target="#b13">[14]</ref>. The ideal value for the Q4 index is 1, and it is defined as: <ref type="bibr" target="#b15">(16)</ref> where is defined as:</p><p>where and are the quaternions, defined as: , and . denotes the modulus of the quaternion .</p><p>is the hypercomplex covariance between and . and are the square roots of the variances of and , respectively. and are the expected values of and , respectively. The first term of ( <ref type="formula" target="#formula_0">17</ref>) is the modulus of the hypercomplex CC between and , and the second and third terms measure contrast changes and the mean bias on all bands, respectively.</p><p>The conjugates of and are defined as and , respectively. As quaternions, and have the following rules:</p><p>By using ( <ref type="formula" target="#formula_1">18</ref>)-( <ref type="formula">22</ref>), ( <ref type="formula" target="#formula_0">17</ref>) can be rewritten as:</p><p>where is obtained by averaging the pixel quaternions within a block. We choose in this paper, and   </p><p>Note that and are still hypercomplex numbers with only their number of dimensions changing from 4 to 8, and we can assume that we can still use ( <ref type="formula">16</ref>) and ( <ref type="formula" target="#formula_2">23</ref>) to calculate Q8 with dimensional extension when calculating , , , , , and , e.g., ( <ref type="formula" target="#formula_1">18</ref>) is replaced with ( <ref type="formula" target="#formula_4">26</ref>):</p><p>APPENDIX B</p><p>There are three parameters for the proposed method, i.e., the patch size, the step size, and the regularization parameter . For simplicity, the ERGAS and Q4 indices are used to evaluate the performance here. How these three parameters affect the performance and computation time of the proposed method is discussed as follows. Similar results were also observed for SC and TSSC, but have been omitted to save space.</p><p>We first set the patch size and step size as <ref type="bibr">[7 7</ref>] and [3 3], respectively. The influence of on the IKONOS, QuickBird, and WorldView-2 data is shown in Table VI, Table VII, and Table VIII, respectively. By setting from 16384 to 1/16384, more dictionary atoms are allowed to be used to represent a certain patch; thus, the performance is first improved, then converges because the patch has already been well represented with enough dictionary atoms. For the computation time, it increases continuously with a smaller . Similar results are also observed with other patch sizes and step sizes. In fact, we can simply set , because for different data, the results converge with , which has a very similar performance and consumes only a little more time than the value we chose in this paper.</p><p>To test the influence of the step size, we fix , and test different step sizes with patch size <ref type="bibr">[7 7</ref>] on the data sets used in this paper. The results are shown in Table <ref type="table" target="#tab_9">IX</ref>, Table <ref type="table" target="#tab_10">X</ref>, and Table XI, respectively. It can be seen that a smaller step size achieves an obviously better result; however, the computation time will be greatly increased. Similar results are also observed with the other patch sizes. We chose the step size as <ref type="bibr">[3 3</ref>] in this paper because it is able to show the advantages of the proposed method over the other methods, and the computation time is tolerable with the basic PC used in this study. With much better computation power offered by a workstation or distributed computation technique, the step size could be set as [1 1] to achieve the best performance.</p><p>Similarly, to test the influence of the patch size, we fix and set the step size as <ref type="bibr">[3 3</ref>]. The different results obtained by the different patch sizes with the three data sets are displayed in Table XII, Table XIII, and Table XIV, respectively. It can be seen that a bigger patch size needs more computation time. The best patch size for the IKONOS data is <ref type="bibr">[11 11</ref>], and the best patch size for the WordView-2 data is [ <ref type="bibr">7 7</ref>]. For the QuickBird data, a patch size of <ref type="bibr">[7 7</ref>] can achieve the best Q4 index, while a     patch size of <ref type="bibr">[13 13</ref>] leads to the best ERGAS index. Therefore, unlike the step size, the best patch size varies with different data. Moreover, in our experiments, we also found that the best patch size varied from <ref type="bibr">[7 7</ref>] to <ref type="bibr">[11 11</ref>] with different data acquired by the same sensor. Since the computation time for <ref type="bibr">[7 7</ref>] is much less than for [ <ref type="bibr">9 9</ref>] or <ref type="bibr">[11 11</ref>], we chose <ref type="bibr">[7 7</ref>] for all the data in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flowchart of the proposed method.</figDesc><graphic coords="3,81.00,65.16,432.00,298.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Similar structural patterns among patches at the same location from the LRM and LRP images. Each row of the 7 7 patches is randomly selected at the same spatial location from the IKONOS image. B, G, R, and NIR represent the blue, green, red, and near-infrared bands, respectively. Each image is magnified 10 times.</figDesc><graphic coords="4,43.98,66.12,240.00,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Input 600 600 IKONOS images: (a) upsampled 16 m MS image; (b) 4 m PAN image.</figDesc><graphic coords="6,102.00,63.12,385.98,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pan-sharpening results with the degraded IKONOS image. (a) Original; (b) GIHS; (c) AWLP; (d) GS; (e) AM; (f) SC; (g) TSSC; (h) PN-TSSC.</figDesc><graphic coords="6,43.02,301.14,504.00,262.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Input 600 600 QuickBird images: (a) upsampled 11.2 m MS image; (b) 2.8 m PAN image.</figDesc><graphic coords="7,103.02,246.12,385.98,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Pan-sharpening results with the degraded QuickBird image. (a) Original; (b) GIHS; (c) AWLP; (d) GS; (e) AM; (f) SC; (g) TSSC; (h) PN-TSSC.</figDesc><graphic coords="8,43.02,65.16,504.00,265.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Input 600 600 WorldView-2 images: (a) upsampled 8 m MS image; (b) 2 m PAN image.</figDesc><graphic coords="9,103.98,63.12,385.98,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Pan-sharpening results with the degraded WorldView-2 image. (a) Original; (b) GIHS; (c) AWLP; (d) GS; (e) AM; (f) SC; (g) TSSC; (h) PN-TSSC.</figDesc><graphic coords="9,45.00,298.14,504.00,264.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,109.98,93.12,370.14,238.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Two-Step Sparse Coding for the Pan-Sharpening of Remote Sensing Images Cheng Jiang, Hongyan Zhang, Member, IEEE, Huanfeng Shen, Member, IEEE, and Liangpei Zhang, Senior Member, IEEE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>QUANTITATIVE ASSESSMENT RESULTS OF THE SIMULATED EXPERIMENTS WITH THE IKONOS DATA SHOWN IN FIG. 4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>QUANTITATIVE ASSESSMENT RESULTS OF THE SIMULATED EXPERIMENTS WITH THE QUICKBIRD DATA SHOWN IN FIG. 6</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>SPECTRAL RANGES OF THE BANDS OF WORLDVIEW-2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV THE</head><label>IV</label><figDesc>QUANTITATIVE ASSESSMENT RESULTS OF THE SIMULATED EXPERIMENTS WITH THE WORLDVIEW-2 DATA SHOWN IN FIG. 8</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V TIME</head><label>V</label><figDesc>COSTS FOR THE DIFFERENT METHODS WITH 600 600 DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI INFLUENCE</head><label>VI</label><figDesc>OF ON THE IKONOS DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII INFLUENCE</head><label>VII</label><figDesc>OF ON THE QUICKBIRD DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII INFLUENCE</head><label>VIII</label><figDesc>OF ON THE WORDVIEW-2 DATA extend the Q4 index to Q8 for the 8-band WorldView-2 data by redefining and as:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX INFLUENCE</head><label>IX</label><figDesc>OF STEP SIZE ON THE IKONOS DATA WITH PATCH SIZE [7 7] AND</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X INFLUENCE</head><label>X</label><figDesc>OF STEP SIZE ON THE QUICKBIRD DATA WITH PATCH SIZE [7 7] AND</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI INFLUENCE</head><label>XI</label><figDesc>OF STEP SIZE ON THE WORDVIEW-2 DATA WITH PATCH SIZE [7 7] AND</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XII INFLUENCE</head><label>XII</label><figDesc>OF THE PATCH SIZE ON THE IKONOS DATA WITH STEP SIZE [3 3] AND</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII INFLUENCE</head><label>XIII</label><figDesc>OF THE PATCH SIZE ON THE QUICKBIRD DATA WITH STEP SIZE [3 3] AND</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIV INFLUENCE</head><label>XIV</label><figDesc>OF THE PATCH SIZE ON THE WORLDVIEW-2 DATA WITH STEP SIZE [3 3] AND</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Julien Mairal et al. for sharing the multi-thread implementation of the LARS algorithm. Thanks are also due to Ron Rubinstein and Michael Elad for sharing the fast MEX implementation of the patch operations. The authors would also like to thank DigitalGlobe for providing the WorldView-2 images used in this study, and the IEEE GRSS Data Fusion Technical Committee for organizing the 2006 <ref type="bibr" target="#b35">[36]</ref> and 2012 Data Fusion Contest.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program of China (973 Program) under Grant 2011CB707105, by the 863 program under Grant 2013AA12A301, by the National Natural Science Foundation of China under Grants 61201342 and 40930532, and by Program for Changjiang Scholars and Innovative Research Team in University (IRT1278), the Fundamental Research Funds for the Central Universities. are with the State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R. China</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image fusion-The ARSIS concept and some successful implementation schemes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="18" />
			<date type="published" when="2003-06">Jun. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new look at IHSlike image fusion methods</title>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparison of three different methods to merge multiresolution and multispectral data: Landsat TM and SPOT panchromatic</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Sides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Laben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Brower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2000-04">Jan. 4, 2000</date>
			<publisher>Eastman Kodak Company</publisher>
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Problems in the fusion of commercial high-resolution satellite images as well as Landsat-7 images and initial solutions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Archives Photogramm</title>
		<meeting>Int. Archives Photogramm</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
	<note>Part 4</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wavelet based image fusion techniques-An introduction, review and comparison</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amolins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="263" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="657" to="661" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A model-based approach to multiresolution fusion in remotely sensed images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2549" to="2562" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusion of multispectral and panchromatic images using a restoration-based method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1482" to="1491" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adjustable model-based fusion method for multispectral and panchromatic images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1693" to="1704" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An IHS and wavelet integrated approach to improve pan-sharpening visual quality of natural colour IKONOS and QuickBird images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="234" />
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient pan-sharpening method via a combined adaptive PCA approach and contourlets</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Younan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1323" to="1335" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decision-based fusion for pansharpening of remote sensing images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bienvenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="23" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new pan-sharpening method using a compressed sensing technique</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="738" to="746" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A practical compressed sensing-based pan-sharpening method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="633" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sparse image fusion algorithm with application to pan-sharpening</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2827" to="2836" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the role of sparse and redundant representations in image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="972" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computational methods for sparse solution of linear inverse problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="948" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<ptr target="http://spams-devel.gforge.inria.fr/index.htmlFeb" />
		<title level="m">Sparse Modeling Software</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Julien Mairal&apos;s Homepage</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<ptr target="http://lear.inrialpes.fr/people/mairal/Feb" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fusion of satellite images of different spatial resolutions: Assessing the quality of resulting images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mangolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="691" to="699" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction of sensor spectral response into image fusion methods. Application to wavelet-based methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Audicana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2376" to="2385" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparing distances for quality assessment of fused products</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th EARSeL Annu. Symp. New Develop. Chal-lenges Remote Sens</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Bochenek</surname></persName>
		</editor>
		<meeting>26th EARSeL Annu. Symp. New Develop. Chal-lenges Remote Sens<address><addrLine>Warsaw, Poland; Rotterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Balkema</publisher>
			<date type="published" when="2006">May 29-31, 2006. 2007</date>
			<biblScope unit="page" from="101" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error measurement to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quality of high resolution synthesized images: Is there a simple criterion?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Fusion of Earth Data</title>
		<meeting>Int. Conf. Fusion of Earth Data<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A global quality measurement of pan-sharpened multispectral imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="317" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cheng Jiang received the B.S. degree in remote sensing science and technology from Wuhan University</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3012" to="3021" />
			<date type="published" when="2007-10">Oct. 2007. 2010</date>
			<pubPlace>Wuhan, China</pubPlace>
		</imprint>
	</monogr>
	<note>Comparison of pansharpening algorithms: Outcome of the 2006 GRSS data-fusion contest. He is currently pursuing the Ph.D. degree at the State Key Laboratory of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
