<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Infomax improves GNNs for Molecular Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hannes</forename><surname>Stärk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution 1 EECS</orgName>
								<orgName type="institution">Massachusetts Institute of Tech-nology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Valence Discovery</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Valence Discovery</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Mu-nich</orgName>
								<address>
									<region>DE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><forename type="middle">G</forename><surname>Ünnemann</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Mu-nich</orgName>
								<address>
									<region>DE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Infomax improves GNNs for Molecular Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecular property prediction is one of the fastestgrowing applications of deep learning with critical real-world impacts. However, although the 3D molecular graph structure is necessary for models to achieve strong performance on many tasks, it is infeasible to obtain 3D structures at the scale required by many real-world applications. To tackle this issue, we propose to use existing 3D molecular datasets to pre-train a model to reason about the geometry of molecules given only their 2D molecular graphs. Our method, called 3D Infomax, maximizes the mutual information between learned 3D summary vectors and the representations of a graph neural network (GNN). During fine-tuning on molecules with unknown geometry, the GNN is still able to produce implicit 3D information and uses it for downstream tasks. We show that 3D Infomax provides significant improvements for a wide range of properties, including a 22% average MAE reduction on QM9 quantum mechanical properties. Moreover, the learned representations can be effectively transferred between datasets in different molecular spaces.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The understanding of molecular and quantum chemistry is a rapidly growing area for deep learning, with models having direct real-world impacts in quantum chemistry <ref type="bibr" target="#b12">(Dral, 2020)</ref>, protein structure prediction <ref type="bibr">(Jumper et al., 2021)</ref>, materials science <ref type="bibr" target="#b38">(Schmidt et al., 2019)</ref>, and drug discovery <ref type="bibr" target="#b40">(Stokes et al., 2020)</ref>. In particular, for the task of molecular property prediction, GNNs have had great success <ref type="bibr" target="#b53">(Yang et al., 2019)</ref>.</p><p>GNNs operate on the molecular graph by updating each atom's representation based on the atoms connected to it via covalent bonds. However, these models reason poorly about other important interatomic forces that depend on the atoms' relative positions in space. Previous works showed that using the atoms' 3D positions improves the accuracy of molecular property prediction <ref type="bibr" target="#b39">(Schütt et al., 2017;</ref><ref type="bibr" target="#b27">Klicpera et al., 2020b;</ref><ref type="bibr" target="#b32">Liu et al., 2021;</ref><ref type="bibr" target="#b28">Klicpera et al., 2021)</ref>.</p><p>However, using classical molecular dynamics simulations to explicitly compute a molecule's geometry before predicting its properties is computationally intractable for many real-world applications. Even recent ML methods for conformation generation <ref type="bibr" target="#b51">(Xu et al., 2021a;</ref><ref type="bibr" target="#b40">Shi et al., 2021;</ref><ref type="bibr" target="#b14">Ganea et al., 2021)</ref> are still too slow for large-scale applications. This issue, as it is summarized in Figure <ref type="figure" target="#fig_0">1</ref>, is the motivation for our method.</p><p>Our Solution: 3D Infomax. We propose to pre-train a GNN to encode implicit 3D information in its latent vectors using publicly available molecular structures. In particular, our method, 3D Infomax, pretrains a GNN by maximizing the mutual information (MI) between its embedding of a 2D molecular graph and a learned representation of the 3D graph. This way, the GNN learns to embed latent 3D information using only the information given by the 2D molecular graphs. After pre-training, the weights can be transferred and fine-tuned on molecular tasks where no 3D information is available. For those molecules, the GNN is still able to produce implicit 3D information that can be used to inform property predictions.</p><p>Several other self-supervised learning (SSL) methods that do not use 3D information have been proposed and evaluated to pre-train GNNs and obtain better property predictions after fine-tuning <ref type="bibr" target="#b22">(Hu et al., 2020b;</ref><ref type="bibr">You et al., 2020;</ref><ref type="bibr" target="#b52">Xu et al., 2021b)</ref>. These often rely on augmentations (such as removing atoms) that significantly alter the molecules while assuming that their properties do not change. Meanwhile, 3D Infomax pre-training teaches the model to reason about how atoms interact in space, which is a principled and generalizable form of information.</p><p>We analyze our method's performance by pre-training with multiple 3D datasets before fine-tuning on quantum mechanical properties and evaluating the generalization abilities of the learned representations. 3D Infomax improves property predictions by large margins and the learned representations are highly generalizable: significant improvements are obtained even when the molecular space of the pretraining dataset is vastly different (e.g., in size) from the kinds of molecules in the downstream tasks. Moreover, while conventional pre-training methods sometimes suffer from negative transfer <ref type="bibr" target="#b33">(Pan &amp; Yang, 2010)</ref>, i.e., a decrease in performance, this is not observed for 3D Infomax.</p><p>Our main contributions are:</p><p>• A 3D pre-training method that enables GNNs to reason about the geometry of molecules given only their 2D molecular graphs, which improves predictions.</p><p>• Experiments showing that our learned representations are meaningful for various molecular tasks, without negative transfer.</p><p>• Empirical evidence that the embeddings generalize across different molecular spaces.</p><p>• An approach to leverage information from multiple conformers of the same molecule that further improves downstream property predictions and an evaluation to what extent this is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>2D Molecular Graphs A molecule's 2D information can be represented as a graph G = (V, E) with atoms V as nodes, and the edges E given by covalent bonds. The 2D information of edges could contain the bond type while nodes are attributed with features such as the atomic number, but no 3D coordinates.</p><p>3D Molecular Conformers. Molecules are dynamic 3D structures that can exist in different spatial conformations. For a single 2D molecular graph, there are multiple low energy atom arrangements that are likely to occur in nature. These are called conformers and they can exhibit different chemical properties. For a model to properly capture 3D information, it is important to consider all the most likely conformations.</p><p>When considering a number c of known conformers of a molecule, we represent them as a set of point clouds {R j } j∈{1...c} . Each point cloud R = {r v } v∈V specifies the locations of all atoms V in the molecule.</p><p>Several tools exist to compute conformers ranging from methods based on classical force fields to slower but more accurate molecular dynamics simulations. Methods such as RDKit's ETKDG algorithm <ref type="bibr" target="#b29">(Landrum, 2016)</ref> are fast but less accurate. The popular metadynamics method CREST <ref type="bibr" target="#b19">(Grimme, 2019)</ref> offers a good tradeoff between speed and ac-curacy but still requires about 6 hours per drug-like molecule per CPU-core <ref type="bibr" target="#b0">(Axelrod &amp; Gomez-Bombarelli, 2020)</ref>. This makes it infeasible to explicitly compute precise structures in applications where large numbers of molecules need to be processed. For instance, virtual screening for drug discovery where screening datasets sometimes are comprised of millions or billions of molecules <ref type="bibr" target="#b16">(Gorgulla et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetries of Molecules</head><p>. A molecule's conformation does not change if all the atom coordinates are jointly translated or rotated around a point, i.e., molecules are symmetric with respect to these two types of transformations which is also known as SE( <ref type="formula" target="#formula_3">3</ref>) symmetry. Note that some molecules (called chiral) are not invariant to reflections: their properties depend on their chirality. Deep learning architectures that capture these symmetries are usually more sample efficient, and they generalize to all symmetric inputs the architecture has been designed for <ref type="bibr" target="#b5">(Bronstein et al., 2021)</ref>. In our method, the produced representations of the 3D structure respect these symmetries.</p><p>Graph Neural Networks. We make use of GNNs to predict molecular properties given a molecular graph. Many GNNs can be described in the framework of MPNNs <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>, such as the PNA model, <ref type="bibr" target="#b11">(Corso et al., 2020)</ref> which we employ.</p><p>The aim of MPNNs is to learn a representation of a graph G = (V, E) with nodes V connected by edges E. They do so by iteratively applying message-passing layers and then combining all node representations in a readout function.</p><p>A message-passing layer updates the representation of a node given its neighbors and the edges between them using permutation invariant functions such as mean, max, or sum.</p><p>After the message-passing layers, another permutation invariant function can be used as a readout to obtain a final graph level representation from the node level embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Molecular property prediction. Since <ref type="bibr" target="#b15">Gilmer et al. (2017)</ref> introduced the MPNN framework, GNNs became popular for quantum chemistry <ref type="bibr" target="#b4">(Brockschmidt, 2020;</ref><ref type="bibr" target="#b42">Tang et al., 2020;</ref><ref type="bibr" target="#b49">Withnall et al., 2020)</ref>, drug discovery <ref type="bibr" target="#b30">(Li et al., 2017;</ref><ref type="bibr" target="#b40">Stokes et al., 2020;</ref><ref type="bibr" target="#b44">Torng &amp; Altman, 2019)</ref>, and molecular property prediction in general <ref type="bibr" target="#b10">(Coley et al., 2019;</ref><ref type="bibr" target="#b23">Hy et al., 2018;</ref><ref type="bibr" target="#b45">Unke &amp; Meuwly, 2019)</ref>. The field is well established with easily accessible molecular datasets driving progress <ref type="bibr" target="#b50">(Wu et al., 2017;</ref><ref type="bibr" target="#b21">Hu et al., 2020a)</ref> and rigorous evaluations of MPNNs for property prediction <ref type="bibr" target="#b53">(Yang et al., 2019)</ref> showing the effectiveness of the approach.</p><p>While these GNNs have had great successes by operating on the 2D graph, many tasks on molecules can be improved by additionally using 3D information. A simple approach is to use bond lengths as edge features <ref type="bibr" target="#b7">(Chen et al., 2020a)</ref>, but methods that capture more molecular geometry improve on this such as SchNet <ref type="bibr" target="#b39">(Schütt et al., 2017)</ref>. Similarly, DimeNet <ref type="bibr" target="#b27">(Klicpera et al., 2020b;</ref><ref type="bibr">a)</ref> proposed extracting more 3D information via bond angles, which further improved quantum mechanical property prediction. SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref> included another angular quantity, and GemNet <ref type="bibr" target="#b28">(Klicpera et al., 2021)</ref> developed an approach to also capture torsion angles, such that all relative atom positions are uniquely defined. EGNN <ref type="bibr" target="#b37">(Satorras et al., 2021)</ref> achieved the same by operating on all pairwise atom distances.</p><p>Self-Supervised Learning. (SSL) attempts to find supervision signals in unlabelled data to learn meaningful representations. Contrastive learning <ref type="bibr" target="#b46">(van den Oord et al., 2018;</ref><ref type="bibr" target="#b20">Gutmann &amp; Hyvärinen, 2010;</ref><ref type="bibr" target="#b3">Belghazi et al., 2018;</ref><ref type="bibr" target="#b20">Hjelm et al., 2019)</ref> is a popular class of methods that learn representations by comparing the embeddings of similar and dissimilar inputs and have achieved impressive results in computer vision <ref type="bibr" target="#b8">(Chen et al., 2020b;</ref><ref type="bibr" target="#b6">Caron et al., 2020)</ref>.</p><p>Learning from unlabeled data also is a critical challenge in molecular chemistry since datasets are relatively small due to experimental or computational costs. Several works have explored contrastive learning variants in the context of molecular graphs for non-quantum molecular properties <ref type="bibr" target="#b22">(Hu et al., 2020b;</ref><ref type="bibr" target="#b48">Wang et al., 2021;</ref><ref type="bibr">You et al., 2020;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b52">Xu et al., 2021b;</ref><ref type="bibr" target="#b58">Zhu et al., 2021)</ref>. However, the improvements these methods provide in molecular property prediction are still limited and often fail to generalize.</p><p>Previous methods for SSL on molecules only leveraged the 2D information of molecules. Meanwhile, 3D Infomax and the concurrently developed GraphMVP <ref type="bibr" target="#b31">(Liu et al., 2022)</ref> additionally make use of molecules </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">3D Infomax</head><p>To achieve our goal of having a 2D GNN that is able to reason about 3D geometry from only 2D inputs, we pre-train our model using contrastive learning. Figure <ref type="figure" target="#fig_1">2</ref> visualizes our method which maximizes the mutual information between a 2D GNN using the 2D molecular graph and a 3D GNN using the associated 3D conformers. After pre-training, we transfer the weights and fine-tune them on property prediction tasks. During fine-tuning, the GNN's produced 3D information can be used to improve predictions.</p><p>3D Infomax uses two different models, as visualized in Figure <ref type="figure" target="#fig_1">2</ref>. Firstly, the model that should be pre-trained which we call 2D network f a since its inputs are 2D molecular graphs G = (V, E) with atoms V and bonds E from which it produces a representation f a (G) = z a ∈ R dz . This can be any GNN that one chooses for the downstream task.</p><p>Secondly, the 3D network f b which encodes the atoms' 3D</p><formula xml:id="formula_0">coordinates R = {r v } v∈V in a 3D representation f b (R) = z b ∈ R dz .</formula><p>Our pre-training can also be understood from a contrastive distillation <ref type="bibr" target="#b43">(Tian et al., 2019)</ref> perspective where the student 2D network learns from the teacher 3D network to produce 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contrastive Framework</head><p>To teach the 2D network f a to produce 3D information from the 2D graph inputs, we maximize the mutual information between the latent 2D representations z a and 3D representations z b . Intuitively, we wish to maximize the agreement between z a and z b if they are derived from the same molecule.</p><p>For this purpose, we use contrastive learning (visualized in Figure <ref type="figure" target="#fig_2">3</ref>). We consider a batch of N molecular graphs {G i } i∈{1...N } with their atom coordinates {R i } i∈{1...N } from which the networks produce multiple representations z a i and z b i . The first objective of contrastive learning is to maximize the representations' similarity if they are a positive pair, meaning that they come from the same molecule (same index i). The second objective is to enforce dissimilarity between negative pairs z a i and z b k where i ̸ = k, i.e., the 2D and 3D representations in the batch should be dissimilar if they come from different molecules. These objectives are captured in the popular NTXent loss <ref type="bibr" target="#b8">(Chen et al., 2020b)</ref> and we use a similar loss to jointly optimize our models:</p><formula xml:id="formula_1">L = − 1 N N i=1   log e sim(z a i , z b i )/τ N k=1 k̸ =i e sim(z a i , z b k )/τ  <label>(1)</label></formula><p>where sim(z a , z b ) = z a • z b /(∥z a ∥∥z b ∥) is the cosine similarity and τ is a temperature parameter which can be seen as weight for the most similar negative pair. While different combinations of contrastive losses and SSL are possible to learn a joint embedding space between 2D and 3D representations, we found the above loss to perform best. Other methods (Barlow Twins <ref type="bibr" target="#b57">(Zbontar et al., 2021)</ref>, BYOL <ref type="bibr" target="#b18">(Grill et al., 2020)</ref>, VICReg <ref type="bibr" target="#b2">(Bardes et al., 2021)</ref>) are explored in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Using Multiple Conformers</head><p>For most molecules, there are multiple low-energy stable conformers. Instead of only using the most probable conformer (with the lowest energy), we found that leveraging structural information from multiple conformers provides significant benefits. To achieve this, we now consider the c highest probability conformers {R j i } j∈{1...c} of the i-th molecule. If there are fewer than c conformers for a molecule, the lowest energy conformer is repeated. Our choice for the following approach is justified by its good trade-off between simplicity and performance in the comparisons with other possible methods in Appendix C.3. For every molecule the 3D network now takes all conformers as input and produces their latent 3D representations {z b i,j } j∈{1...c} . The objective is to maximize the similarity between z a i and all conformer representations z b i,j that stem from the same molecule (see Figure <ref type="figure" target="#fig_2">3</ref>). As such, we modify our loss to sum over the similarities of all conformers to obtain the final loss:</p><formula xml:id="formula_2">L multi3D = − 1 N N i=1   log c j=1 e sim(z a i , z b i,j )/τ N k=1 k̸ =i c j=1 e sim(z a i , z b k,j )/τ    .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D Network</head><p>The 3D network takes as input the coordinates of the atoms as a 3D point cloud and has to produce an SE(3) invariant representation vector z b that encodes as much information as possible about the 3D structure. Note that it does not have access to the 2D information such as atom or bond features; otherwise, the empirical estimate of the mutual information could be increased by both networks encoding this information instead of the desired 3D structure.</p><p>Our concrete architecture encodes the 3D information given by the pairwise Euclidean distances of all atoms. This representation uniquely defines all relative atom positions and is invariant to translation and rotation, as desired. However, like the representations used by many other 3D graph processing architectures <ref type="bibr" target="#b27">(Klicpera et al., 2020b;</ref><ref type="bibr" target="#b37">Satorras et al., 2021)</ref>, it is also invariant to reflection, which prevents it from distinguishing chiral molecules. Using all pairwise distances also means that the model's complexity is quadratic in the number of atoms, which is feasible for drug-like molecules.</p><p>The pairwise distances d uv between atoms u and v are first mapped to a higher dimensional space using sine and cosine functions with high frequencies. Recent work provides theoretical <ref type="bibr">(Rahaman et al., 2019)</ref> and empirical <ref type="bibr" target="#b41">(Tancik et al., 2020)</ref> evidence that this enables deep networks to better fit data with high-frequency variations. Such high-frequency variation is present in the 3D data we wish to encode since differences in bond lengths are often small. Further, the bond distances are likely the most important since close-by atoms typically have the most relevant interactions. As such, we use the following mapping γ : R → R 2F +1 with the number of frequencies F set to 4:</p><formula xml:id="formula_3">γ(d uv ) = d uv , sin d uv 2 0 , cos d uv /2 0 , . . . , sin d uv 2 F −1 , cos d uv .<label>(3)</label></formula><p>The further components can be seen as an MPNN <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref> operating on the fully connected graph of a molecule with the encoded distances as edge features and a constant learned vector as node features. The message passing layers iteratively encode the 3D information into the node features, which are pooled to produce the 3D representation z b . The differences to standard MPNNs are detailed in Appendix A. Instead of the presented architecture, a 3D GNN such as SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref> operating on learned node embeddings could also be used. We validate the improvements provided by our architecture and γ encoding with a set of ablation experiments reported in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pre-training Baselines</head><p>Distance Predictor. A simpler method to use available 3D structures to pre-train a GNN instead of 3D Infomax is to learn to directly predict all atom distances in the lowest energy conformer. To predict the distance between node v and u, we concatenate their representations h u , h v ∈ R d h that were produced by the GNN and feed them to a multilayer perceptron (MLP) that produces a single scalar U : R 2d h → R. The distance prediction dist uv is then given by</p><formula xml:id="formula_4">dist uv = softplus(U (h v ∥ h u ) + U (h u ∥ h v ))<label>(4)</label></formula><p>where ∥ denotes concatenation and softplus(x) = log(1 + e x ). The node representations are concatenated in both orders and fed to the MLP to ensure that the function is symmetric. The pre-training loss f is the mean squared error between the predicted and true distances.</p><p>Conformer Generation. GeoMol <ref type="bibr" target="#b14">(Ganea et al., 2021)</ref> is the state-of-the-art deep learning method for generating molecular conformations. It is a generative model that produces a distribution of likely 3D structures for a single molecule, thus capturing the information of multiple conformers. Their architecture employs a GNN whose node representations are used to obtain the final distribution. To use their model as a baseline, we use their training process as a pre-training task and then extract the GNN and fine-tune it on the different downstream tasks.</p><p>GraphCL. We compare against the conventional augmentation-based pre-training method GraphCL <ref type="bibr">(You et al., 2020)</ref> with the settings of JOAO <ref type="bibr" target="#b56">(You et al., 2021)</ref> since it outperformed other SSL approaches for multiple molecular tasks. It uses a common self-supervised objective in which the model has to learn to produce representations that are invariant to augmentations. We use randomly dropping nodes with a ratio of 0.2 on both branches of the SSL setup since JOAO found this combination of augmentations to work particularly well for molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data and Evaluation Protocol</head><p>To evaluate our method, we pre-train using subsets of 3D molecular structure datasets. We then fine-tune the models to predict properties of 2D datasets or of 3D datasets while ignoring their 3D information. It is clear that using a 3D GNN and the explicit 3D information of these datasets that was calculated with expensive quantum simulations would yield the best performance. However, we only use the 2D molecular graphs of the fine-tuning data to simulate the scenario when 3D structures are not available. This allows us to compare the improvements through the implicit 3D information of 3D Infomax pre-trained GNN with the maximum possible improvements represented by using the explicit ground truth 3D conformers with a 3D GNN. We choose PNA <ref type="bibr" target="#b11">(Corso et al., 2020)</ref> as the GNN to pre-train due to its simplicity and state-of-the-art performance for molecular tasks.  Fine-tuning setup. After pre-training, the models are finetuned on 50k molecules from QM9 (in Table <ref type="table" target="#tab_3">1</ref>) or 140k from GEOM-Drugs (in Table <ref type="table" target="#tab_4">2</ref>) that have no overlap with the molecules from the pre-training data. On the same molecules, we also train PNA with random weight initialization (labeled Rand Init) to compare how much the downstream performance is improved by the different pre-training methods.</p><p>Explicit 3D baseline and ground truth comparison setup.</p><p>We additionally train and test the 3D GNN SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref> with 3D coordinates generated by RDKit's ETKDG algorithm, <ref type="bibr" target="#b29">(Landrum, 2016)</ref> which can be done in a fast manner (labeled RDKit SMP). Using conformers generated by the state-of-the-art learned method, GeoMol <ref type="bibr" target="#b14">(Ganea et al., 2021)</ref> always performed worse (Appendix C.7). Lastly, we evaluate SMP using the accurate ground truth 3D conformers of QM9 which were computed with time-consuming simulations that would be infeasible for many real-world applications. These structures are not available to the other methods.</p><p>3D Infomax for QM9 vs. conventional pre-training. Table <ref type="table" target="#tab_3">1</ref> shows that 3D Infomax pre-training leads to large improvements over the randomly initialized baseline and over GraphCL with all three pre-training datasets. After 3D pretraining on one half of QM9, the average decrease in MAE is 22%. Comparing 3D Infomax on GEOM-Drugs with GraphCL shows that even though the latter is pre-trained on two times as many molecules from the same dataset, 3D pre-training is always better by a large margin.</p><p>Generalization from pre-training to fine-tuning. Pretraining with the disjoint half of QM9 performs best since it shares the molecular space of the test set. Nevertheless, the learned representations also generalize well: pre-training on GEOM-Drugs and QMugs leads to improvements of 19% and 18% respectively, even though QM9 contains much smaller molecules with on average 18 atoms compared to the 44.4 atoms for the drug-like molecules of GEOM-Drugs.</p><p>Comparison with Ground Truth Conformers. 3D Infomax' large improvements have to be compared to methods that also do not use explicit ground truth 3D information and only operate on the 2D molecular graph for the molecules of which properties should be predicted. However, for the quantum property experiment datasets we employ, highquality ground truth 3D information was calculated with expensive quantum simulations and we are thus able to compare with 3D GNNs that use this additional input for property prediction.</p><p>The results of SMP in Table <ref type="table" target="#tab_8">5</ref>.3 show that for some properties, the MAE of 3D Infomax with its implicit 3D information is still higher than what is possible when using explicit ground truth conformers (which are time-consuming and expensive to obtain). One likely contributing factor for this is that QM9's properties are conformer-specific. There might be a maximum accuracy that can be achieved if only the molecule is known and not for which conformer the property should be predicted. Nevertheless, this performance gap suggests that there is still room for improvement.</p><p>3D pre-training Baselines. 3D pre-training by directly predicting 3D quantities is simpler than 3D Infomax and would be preferable in case of similar gains. Therefore, we compare with the baselines in Section 5.1 using the same 140k molecules of GEOM-Drugs for all 3D pre-training methods. DisPred refers to predicting all atom distances of the highest probability conformer, and ConfGen means pre-training by predicting up to 10 conformers. Table <ref type="table" target="#tab_8">5</ref>.3 shows that 3D Infomax pre-training is always superior to the 3D pre-training baselines and is the only method to not suffer from negative transfer <ref type="bibr" target="#b33">(Pan &amp; Yang, 2010)</ref>.</p><p>3D Infomax for GEOM-Drugs.    Figure <ref type="figure" target="#fig_4">4</ref> highlights the benefit of using more than a single conformer. However, the marginal gain reduces as higher energy conformers are added and beyond a certain point (around three conformers), the reduced focus on the most likely conformers worsens the downstream performance. This is in line with the observation that, on average, three conformers are enough to cover 70% of the cumulative Boltzmann weight for GEOM-Drugs. Additionally, experi- ments in Appendix C.3 show that using multiple conformers is essential when pre-training with QMugs: the MAE for QM9's homo property is 82.57 with a single conformer while it improves to 70.77 when using three.</p><p>In Figure <ref type="figure" target="#fig_5">5</ref> we can observe the performance improving as the size of the pre-training dataset increases. However, the returns are diminishing, and we cannot claim that even larger pre-training datasets are likely to drastically improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a pre-training strategy, 3D Infomax, that teaches a GNN to produce latent 3D and quantum information from 2D molecular graphs. This can later be used during fine-tuning to improve molecular property predictions while retaining the inference speed of a standard GNN operating on 2D molecular graphs. We found consistently large improvements (∼22%) for quantum properties, overshadowing the gains possible with conventional SSL methods. The embedded 3D knowledge can be transferred across highly different types of molecules (e.g., from molecules with an average of 18 atoms to drug-like molecules with 44.4 atoms) since the representations capture a principled form of information that is known to be useful for several molecular tasks. Lastly, we observed that using multiple molecular conformers during pre-training provides valuable additional information to further improve downstream property predictions.</p><p>We provide open-source access to our method at https: //github.com/HannesStark/3DInfomax with a simple setup to 3D pre-train a GNN and the additional possibility to generate molecular fingerprint embeddings that carry latent 3D information from a list of SMILES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further Explanations</head><p>3D Network Details The l-th layer of the 3D network takes two sets as input. First, n 2 − n edge representations</p><formula xml:id="formula_5">{d l uv ∈ R d d | u, v ∈ V ∧ u ̸ = v} (</formula><p>the edges of a complete graph without self-loops). In the first layer they are given by the encoded distances fed through an initial feed-forward network U init : R 2F +1 → R d d which projects them to the hidden dimension of the edges d 0 uv = U init (γ(d uv )). The second input is a set of n atom representations {h l 1 , . . . h l n } with dimensionality R d h . In the first layer, the atom representations are all set to the same learned vector that is initialized with a standard normal. With ∥ meaning concatenation, every layer updates the edge and atom representations and iteratively encodes 3D information into them as follows:</p><formula xml:id="formula_6">m uv = U edge ([h l u ∥ h l v ∥ d l uv ])<label>(5)</label></formula><formula xml:id="formula_7">d l+1 uv = d l uv + m uv<label>(6)</label></formula><formula xml:id="formula_8">h l+1 u = U h ([h u ∥ n v=1 v̸ =u m uv * σ(U sof tedge (m uv )]). (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>The layer is parameterized by three MLPs where the first one updates the edges U edge : R 2d h +d d → R d d . The second one updates the atom representations U h : R d h +d d → R d h . The third one U sof tedge : R d d → R is followed by the logistic sigmoid function to create a value between 0 and 1 that can be seen as a soft edge weight telling us how probable an edge is for each message m uv as it is done by <ref type="bibr" target="#b37">Satorras et al. (2021)</ref>.</p><p>To produce the final 3D representation z b , all atom representations are aggregated by concatenating their mean, their maximum, and their standard deviation and feeding this through a final feed-forward network U : R 3d h → R dz .  <ref type="bibr" target="#b3">&amp; Murcko, 1996)</ref> which is different from the scaffold to which the two molecules in the bottom row belong. We can easily obtain the scaffold of a molecule using RDKit <ref type="bibr" target="#b29">(Landrum, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Parameter Details</head><p>The hyperparameters for SMP are taken from the official repository <ref type="foot" target="#foot_0">1</ref> where <ref type="bibr" target="#b32">Liu et al. (2021)</ref> provide their code, and we predict gap even though it could be calculated as |homo − lumo|. The parameter search space and final parameters for the PNA architecture are specified in Table <ref type="table">3</ref> and those of the 3D network in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training:</head><p>We use Adam with a start learning rate of 8 × 10 −5 and a batch size of 500. The learning rate schedule during pre-training starts with 700 optimization steps of linear warmup followed by the schedule given by the ReduceLROnPlateau scheduler by PyTorch<ref type="foot" target="#foot_1">2</ref> with reduction parameter 0.6, patience 25, and a cooldown of 20.</p><p>Fine-tuning quantum mechanical properties: We use Adam with a start learning rate of 7 × 10 −5 , weight decay 1 × 10 −11 and a batch size of 128. For the learning rate schedule, we first perform warmup as follows. We consider three different sets of learnable parameters: (1) the batch norm parameters, (2) all newly initialized parameters that were not transferred, and (3) all parameters. For these sets, we increase the learning rate in this order from 0 to the start learning rate with linear interpolation. For parameter group one, we warm up for 700 steps, 700 steps for group 2, and 350 steps for group 3. After that we use the schedule given by the ReduceLROnPlateau with reduction parameter 0.5, patience 25, and a cooldown of 20.</p><p>Fine-tuning non-quantum properties: We use Adam with a start learning rate of 1 × 10 −3 and a batch size of 32. The learning rate schedule is the same as for the quantum mechanical properties.</p><p>The experiment on the non-quantum properties has different hyperparameters for PNA since the smaller datasets are easily overfitted on with the large architecture we use for the quantum mechanical properties. A smaller PNA yields better performance with the random initialization baseline. Therefore the PNA in these experiments has a hidden dimension of 50 and 3 message passing layers as propagation depth. Apart from that, it is the same as the PNA described in Table <ref type="table">3</ref>.</p><p>Table <ref type="table">3</ref>. Search space for the 2D network PNA through which we searched to obtain a strong baseline performance on the homo property of the QM9 dataset. The parameters were tuned in the order in which they are listed in this table from top to bottom. After this was completed for all parameters, we performed a second round of tuning for a subset of them. The final parameters are marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARAMETER SEARCH SPACE</head><p>PROPAGATION DEPTH <ref type="bibr">[4, 5, 6 ,7]</ref> HIDDEN DIMENSION <ref type="bibr">[40, 50, 75, 90, 100, 150, 200 ,300</ref>  <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3]</ref> BATCHNORM MOMENTUM [0.1, 0.9, 0.93] Table <ref type="table">4</ref>. Search space for the 3D network Net3D through which we searched to obtain a strong baseline performance on the homo property of the QM9 dataset and we considered the size of the network where parameters leading to less memory use are preferred. The parameters were tuned in the order in which they are listed in this table from top to bottom. After this was completed for all parameters, we performed a second round of tuning for a subset of them. The final parameters are marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARAMETER SEARCH SPACE</head><p>PROPAGATION DEPTH <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b61">3,</ref><ref type="bibr" target="#b62">4,</ref><ref type="bibr" target="#b63">5]</ref> HIDDEN DIMENSION <ref type="bibr" target="#b68">[10,</ref><ref type="bibr">20,</ref><ref type="bibr">40,</ref><ref type="bibr">60,</ref><ref type="bibr">80</ref>, 100]</p><formula xml:id="formula_10">F USED IN γ : R → R 2F +1</formula><p>[0, <ref type="bibr" target="#b61">3,</ref><ref type="bibr" target="#b62">4,</ref><ref type="bibr" target="#b66">8,</ref><ref type="bibr" target="#b68">10,</ref><ref type="bibr">50</ref>] MESSAGE MLP LAYERS <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3</ref>] UPDATE MLP LAYERS <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3]</ref> READOUT AGGREGATORS  <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3]</ref> BATCHNORM MOMENTUM [0.1, 0.9, 0.93]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Data Details</head><p>We use three datasets containing 3D information for pre-training with diversity in molecule size and the number of molecules, as can be seen in Table <ref type="table" target="#tab_8">5</ref>. The pre-training datasets are:</p><p>1. QM9 3 <ref type="bibr" target="#b36">(Ramakrishnan et al., 2014)</ref> contains 134k stable small organic molecules of 5 elements (CHONF). Every molecule has the 3D coordinates of one low-energy conformer and is annotated with 12 quantum mechanical properties as regression targets. The molecules are considered very small, with at most 9 heavy atoms.</p><p>2. GEOM-Drugs 4 (Axelrod &amp; Gomez-Bombarelli, 2020) consists of 304k realistically-sized biologically and pharmacologically relevant molecules of 16 elements, annotated with multiple 3D conformers, the ensemble Gibbs free energy, and the ensemble energy as regression targets. For the average molecule, 70% of the Boltzmann weight is captured by just three conformers as can be seen in Figure <ref type="figure">B</ref>.2 where we also provide a histogram for the number of molecules that have a certain amount of conformers in Figure B.2. The conformers are generated using CREST <ref type="bibr" target="#b19">(Grimme, 2019)</ref>.</p><p>3. QMugs 5 <ref type="bibr" target="#b24">(Isert et al., 2021)</ref> has 665k drug-like molecules with three diverse conformers each and multiple conformer specific quantum mechanical properties as regression tasks. The conformers are generated using CREST <ref type="bibr" target="#b19">(Grimme, 2019)</ref>.</p><p>For fine-tuning, we use a variety of datasets that cover a wide range of domains and applications. The molecular properties are relevant for quantum mechanics, physical chemistry, biophysics, and physiology such that we can obtain a good estimate of how valuable our 3D pre-training is for each domain. For quantum mechanical properties, which are often specific to a conformer, it is clear that 3D information is important and there has been a lot of evidence that learned methods highly benefit from its use <ref type="bibr" target="#b27">(Klicpera et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b32">Liu et al., 2021;</ref><ref type="bibr" target="#b39">Schütt et al., 2017)</ref>. For these properties, the interest is in how much our method can leverage this information and transfer it to molecules where no 3D geometry is available.</p><p>Meanwhile, for biological or physiological properties such as blood-brain barrier penetration, it is not as clear if improvements from 3D information are to be expected. As such, this question needs to be answered next to how much of the benefits 3D pre-training recovers. For this purpose, we use the following molecular graph datasets, which are mainly taken from MoleculeNet <ref type="bibr" target="#b50">(Wu et al., 2017)</ref> and we use the scaffold splits 6 with an 80/10/10 split ratio provided by <ref type="bibr">OGB Hu et al. (2020a)</ref>.</p><p>The fine-tuning datasets are:  <ref type="bibr" target="#b13">(Fey &amp; Lenssen, 2019)</ref> and Deep Graph Library <ref type="bibr" target="#b47">(Wang et al., 2019)</ref>. The code we use for SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref> is under the GNU General Public License v3.0 and we use their implementation after discussing it with the first author of the paper and under the consideration that their project welcomed our contributions to their library.</p><p>The experiments were conducted on two different machines while the same system was always used in direct comparisons.</p><p>The first machine has an AMD Ryzen 1700 CPU @ 3.70Ghz, 16GB of RAM, and an Nvidia GTX 1060 GPU with 6GB vRAM. The second system contains two Intel Xeon Gold 6248 CPUs @ 2.50GHz each with 20/40 cores, 400GB of RAM, and four Quadro RTX 8000 GPUs with 46GB vRAM of which only a single one was used for each experiment. All mentions and of training time refer to the second system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Units and Meaning of Quantum Properties</head><p>For the GEOM-Drugs dataset, all reported numbers have the unit kcal/mol, Gibbs refers to the ensemble Gibbs free energy, and ⟨E⟩ to the ensemble energy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Confidence Interval Details</head><p>All the specified confidence intervals in our work are standard deviations calculated from different weight initializations using the seeds <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3,</ref><ref type="bibr" target="#b62">4,</ref><ref type="bibr" target="#b63">5,</ref><ref type="bibr" target="#b64">6]</ref> or <ref type="bibr" target="#b59">[1,</ref><ref type="bibr" target="#b60">2,</ref><ref type="bibr" target="#b61">3,</ref><ref type="bibr" target="#b62">4]</ref>. The following tables provide additional confidence intervals for the results in the main text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Non-Quantum Properties</head><p>We found that 3D Infomax yields large improvements for predicting quantum properties. For non-quantum properties, there is less empirical evidence that explicit 3D information improves prediction accuracy with first explorations by <ref type="bibr" target="#b1">Axelrod &amp; Gómez-Bombarelli (2020)</ref> showing only little improvements for their COVID 19 related predictions. Nevertheless, for tasks such as binding prediction in the bace dataset, we would expect it to be helpful, and we compare different methods pre-trained with GEOM-Drugs.</p><p>In Table <ref type="table" target="#tab_13">10</ref>, we find that 3D Infomax improved performance for 4 out of 10 OGB datasets. In contrast to the results for quantum mechanical property predictions (Section 5.3), it is not always superior to GraphCL and ConfGen. However, 3D Infomax never decreases performance which can be valuable in practice and make the method worth employing for non-quantum properties as well.</p><p>When investigating for which tasks 3D Infomax is useful, we see that abstract tasks such as predicting clinical test outcomes (clintox) benefit less. The most significant improvements are rather possible for tasks like predicting solubility and lipophilicity in esol and lipo. These are more directly related to molecular mechanics and a molecule's intrinsic properties (e.g., the dipole moment/polarity is important for predicting lipophilicity). They do not depend on how a molecule will interact with others to result in, e.g., different effects on patients. For such tasks, ConfGen often leads to significant improvements, providing further evidence for the value of 3D pre-training.</p><p>Additionally, for datasets like bace with its binding prediction task where 3D information should be valuable, the improvements are only modest. This could suggest that our method does not capture all of the 3D information that is relevant for predicting protein binding, and there is still room for improvement. Another explanation is that a molecule's geometry is less helpful for bace since the geometry of the protein and the binding pocket the molecule has to fit into are not known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Different 3D Networks and Ablation</head><p>In this section, we justify the design of our 3D network, which we call Net3D in this comparison. In Table <ref type="table" target="#tab_14">11</ref> we compare Net3D with different alternative 3D networks, which are the 3D GNNs SMP and EGNN operating on learned node embeddings similar to Net3D. Additionally, we ablate the use of our γ function that maps the pairwise distances to a higher dimensional space since it would be an unnecessary complication if it provides no benefit. We call it Net3D w/o γ if Net3D directly operates on the pairwise distances. In Table <ref type="table" target="#tab_14">11</ref> we can observe that Net3D yields the best downstream performance and that using our γ function is a valuable component of it. Possibly EGNN would benefit similarly from this encoding. SMP's downstream performance is the worst which could be expected since the 3D input representation which it uses does not uniquely define all the relative positions in a molecule.</p><p>We note that SMP is able to distinguish chiral molecules, unlike the other 3D networks, but this advantage cannot be evaluated with our experiments on quantum mechanical properties. Chirality only becomes relevant when considering the interactions between molecules, and in these situations, SMP might be able to leverage its advantage such that our evaluation could be criticized as not entirely fair. Additionally, SMP has much lower memory requirements since it does not suffer from the quadratic complexity of EGNN and Net3D in the molecule size. Nevertheless, Net3D performs the best, and for drug-like molecules, the quadratic complexity is not problematic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Different Methods for Multiple Conformers</head><p>We test three main approaches and variations of them for incorporating the 3D information of multiple conformers to justify our choice in the main text. The most straightforward one is conformer sampling. We use one of the single conformer setups, but when sampling the batch, we additionally sample j ∈ {1 . . . c i } and use the single conformer R j i . The probability of sampling a conformer is either distributed uniformly (so 1/c i is the probability for each j) or given by the Boltzmann weight of each conformer. multi3D is the approach from the main text where we include multiple conformers as additional positive pairs in contrastive learning. For each molecule (G i , {R j i } j∈{1...ci} ) we choose the c lowest energy conformers to have a fixed number of them. If there are fewer than c conformers for a molecule (c i &lt; c), then the lowest energy conformer is repeated. For every molecule the 3D network now takes all c conformers {R j i } j∈{1...c} as input and produces their latent 3D representations {z b i,j } j∈{1...c} which we can see as additional positive samples. In our contrastive setting, we, therefore, want the similarity between z a i and all conformer representations that come from the same molecule z b i,j to be high. As such, we modify our loss to obtain:</p><formula xml:id="formula_11">L multi3D = − 1 N N i=1   log c j=1 e simcos(z a i , z b i,j )/τ N k=1 k̸ =i c j=1 e simcos(z a i , z b k,j )/τ    .<label>(8)</label></formula><p>One concern with this formulation is the following. Let us consider a single molecule. The objective of high similarity between the many 3D representations and the single 2D representation might be easier to solve through encoding the same 2D information in the 3D representations instead of capturing the 3D information of all conformers in the single 2D representation. The 2D network would therefore not learn to produce 3D information from its 2D inputs because the mutual information could be maximized through encoded 2D information.</p><p>To address this problem, multi3D+2D is our third approach. The 2D network is now modified to produce c many latent 2D representations f a (G i ) = {z a i,j } j∈{1...c} which are compared to all 3D representations of the same molecule in a similarity function sim. We simply use this similarity in the loss instead of the cosine similarity. Intuitively, the 2D network now has to produce an embedding for each 3D conformer.</p><p>One way to define such a similarity between two same-sized sets of vectors is to use the sum of all pairwise cosine similarities (for brevity we drop the subscript and only write {z a i,j } to mean the set of all representations corresponding to the i-th molecule):</p><formula xml:id="formula_12">sim all ({z a i,j }, {z b i,j }) = c j=1 c k=1 sim cos (z a i,j , z b i,k )<label>(9)</label></formula><p>More principled would be to find the optimal transport matching with the highest cosine similarity, such that one 2D representation always corresponds to one 3D representation. However, this approach was not computationally feasible with the batch sizes we use in contrastive learning. We instead opt for an upper bound on the maximum similarity matching. For every 3D representation, we choose the 2D representation that has the highest similarity. This way, one 2D representation could be associated with multiple 3D embeddings, and we no longer have a mass preserving matching:</p><formula xml:id="formula_13">sim max ({z a i,j }, {z b i,j }) = c k=1 max j∈{1...c} sim cos (z a i,j , z b i,k ).<label>(10)</label></formula><p>Beyond these similarity measures, we explore additional ones based on the inverse of different distance functions and asymmetric metrics such as the maximum mean discrepancy <ref type="bibr" target="#b17">(Gretton et al., 2012)</ref> or the KL-and JS-Divergence when interpreting the conformer representations as samples from a normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We evaluate which of the different approaches best leverage the additional conformer's information to justify our choice for multi3D in the main text. Another hypothesis we wish to test is that for smaller molecules such as those in QM9, the ability to make predictions informed by multiple conformers is not as important as for larger drug-like molecules. The reasoning is that a single conformer takes most of the Boltzmann weight for QM9's molecules due to the fewer degrees of freedom.</p><p>We test conformer sampling, multi3D, and multi3D+2D when pre-training on either QMugs or one half of GEOM-Drugs and fine-tuning on QM9 or the other half of GEOM-Drugs. In QMugs we have three diverse conformers available for each molecule which are all used, while for GEOM-Drugs different numbers of conformers are available of which we use the five with the highest Boltzmann weight i.e. lowest energy. If there are fewer than five we duplicate the lowest energy conformer (see Section 4.2 for details). We recall that for the multi3D+2D loss sets with as many elements as conformers are produced by the 2D and 3D networks. Both the discussed sim all and sim max are used as similarity measures between those sets. For the conformer sampling strategies of using a uniform weighting or sampling conformers according to their Boltzmann weight, we do not evaluate the latter on QMugs since we do not have it available with exactly three conformers per molecule.</p><p>In Table <ref type="table" target="#tab_5">12</ref> we can observe that there are large improvements possible when using multiple conformers. After pre-training on QMugs, the MAE, when predicting the homo property, decreases from 82.57 to 70.77 and from .1966 to .1831 for predicting the Gibbs free energy for GEOM-Drugs. Notably, these improvements are much larger than when pre-training with GEOM-Drugs. This is likely because the GEOM-Drugs dataset contains the lowest energy conformers, and we always use the most probable one with the highest Boltzmann weight when pre-training with a single conformer. Meanwhile, the QMugs dataset contains three diverse conformers per molecule and not the ones with the highest Boltzmann weight. Pre-training with the lowest energy conformer from GEOM-Drugs already captures most of the relevant information, and using more is not as beneficial. However, for QMugs, using the information of all three diverse conformers is crucial.</p><p>Similar to the small improvements over the random initialization baseline with GEOM-Drugs, the different methods for using multiple conformers mostly perform the same when pre-training with GEOM-Drugs. When pre-training with QMugs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. SSL Methods</head><p>Here we compare our 3D Infomax pre-training against three additional SSL methods. These are Barlow Twins <ref type="bibr" target="#b57">(Zbontar et al., 2021)</ref>, multi-modal BYOL <ref type="bibr" target="#b18">(Grill et al., 2020)</ref>, and VICReg <ref type="bibr" target="#b2">(Bardes et al., 2021)</ref>. We pre-train these methods on one half of QM9. For a fair comparison, we search through 8 different hyperparameter settings based on the downstream performance on the QM9 homo property. After these method-specific hyperparameters were selected, we tuned every method with a random search over the same search space.</p><p>For 3D Infomax, we vary the temperature of our loss τ . When using BYOL we try different decay rates γ for the exponential moving average weight copying. Here, we include γ = 0 making our setup similar to a multi-modal version of SimSiam <ref type="bibr" target="#b9">(Chen &amp; He, 2020)</ref>. For Barlow Twins, the hyperparameter is λ weighting the redundancy loss. Lastly, for VICReg we vary µ and ν, the parameters for the variance and the covariance regularization:</p><p>1. 3D Infomax with our loss: τ ∈ [0.02, 0.05, 0.  The results in Table <ref type="table" target="#tab_16">14</ref> showcase that 3D Infomax clearly is the superior method in our setting. It decreases the MAE from 82.10 ± 0.33 to 68.96 ± 0.32 while the other methods either lead to no improvement or to the much smaller drop to 79.16 ± 0.58 for BYOL. This is not due to collapse to a constant solution since we can observe a high variance between the representations in a batch for all methods. Furthermore, with the final parameter settings, all methods were able to achieve a low value for their loss during pre-training, both on the training and validation data and there are no optimization issues.</p><p>Intuitively, the results can be explained by 3D Infomax being the only method that optimizes a lower bound on the mutual information, which potentially makes it especially fit for our setting. The other approaches have no direct relation to mutual information and instead rely on maximizing a notion of similarity with tricks to prevent collapse. While this might work for conventional SSL, we see no success in our scenario where the rigorous guarantee on maximizing the mutual information seems valuable.</p><p>Another reason for the poor performance of BYOL and especially Barlow Twins and VICReg might be that they rely on having symmetric networks to generate the compared representations. In our scenario, we have very little similarity between the architectures with our 2D and 3D networks operating on different modalities. This hypothesis would fit in line with the findings of <ref type="bibr" target="#b2">Bardes et al. (2021)</ref> and <ref type="bibr" target="#b57">Zbontar et al. (2021)</ref> where introducing asymmetries between the networks hurt performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Pre-training a 3D GNN</head><p>We try to use our 3D Infomax setup to pre-train a 3D GNN. For this purpose, we employ SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref> as 3D network during pre-training with half of the QM9 dataset. We then transfer it's weights and fine tune them using the accurate 3D conformers of the other half of QM9's molecules to predict the dataset's properties. We compare this with SMP trained on the same molecules with randomly initialized weights. The only architectural difference between the networks is that the pre-trained GNN does not use atom features for the reasons explained in the 3D Network paragraph in Section 4.</p><p>In Table <ref type="table" target="#tab_17">15</ref>, we find that pre-training improves the 3D GNN's performance. This may be due to the covalent bonding structure and other 2D edge information that is available during pre-training and which SMP usually cannot use since it employs a radius graph. This is the case even though the pre-trained SMP does not have access to the atom features. Pre-training 3D GNNs might be an interesting future direction to attempt beating the state-of-the-art methods for predicting quantum properties with accurate 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. Cheap Neural Conformers as 3D GNN input</head><p>In Section 5.3 we used RDKit's ETKDG algorithm <ref type="bibr" target="#b29">(Landrum, 2016)</ref> to generate inaccurate but cheap and fast conformers and employed them as inputs to the 3D GNN SMP <ref type="bibr" target="#b32">(Liu et al., 2021)</ref>. Here, we attempt the same with conformers generated by the state-of-the-art deep learning method for conformation generation which is GeoMol <ref type="bibr" target="#b14">(Ganea et al., 2021)</ref>. For this purpose, we train GeoMol with 50k molecules of QM9 and use it to generate the conformations for the rest of QM9. SMP is then trained on 50k different molecules to predict their properties, either using RDKit's conformers or those of GeoMol. This enables a fair comparison with 3D Infomax, which uses the same molecules for pre-training that were used to train GeoMol. When visually inspecting some of the conformers generated by GeoMol, we found that they were sometimes of poor quality for molecules with rings and contained outliers with conformations that seem particularly unrealistic. Table <ref type="table" target="#tab_18">16</ref> shows that SMP performs poorly with the conformers generated by GeoMol and using those generated by RDKit is always superior. This is the case even though the average accuracy of GeoMol's conformers is comparable to that of RDKit ETKDG's conformers when GeoMol is trained on all of QM9 <ref type="bibr" target="#b14">(Ganea et al., 2021)</ref>. We hypothesize that the high MAE with GeoMol's conformers occur since they contained some particularly unrealistic outlier conformations, and SMP is not able to handle those well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8. Batch Size</head><p>It is well known that contrastive learning with losses such as NTXent heavily rely on a high number of negative samplesthat is large batch sizes. Thus, we evaluate how our loss benefits from large batch sizes in Table <ref type="table" target="#tab_10">17</ref>. We find that our large batch size of 500 is indeed necessary for the good performance of 3D Infomax.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The different approaches to molecular property prediction and the motivation for our 3D Infomax pre-training.</figDesc><graphic url="image-1.png" coords="2,55.44,67.06,485.99,263.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. We first pre-train a 2D network f a by maximizing the mutual information (MI) between its representation z a of a molecular graph G and a 3D representation z b produced from the molecules' conformers R j . In step 2, the weights of f a are transferred and fine-tuned to predict properties.</figDesc><graphic url="image-2.png" coords="4,102.79,67.06,388.82,177.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The single conformer example shows a batch of three molecular graphs as input to the 2D network with the corresponding three conformers as input to the 3D model. During 3D pre-training, the contrastive loss L enforces high similarity between latent representations that come from the same molecule (green arrows) while encouraging dissimilarity otherwise (red arrows). This is depicted for the first molecule, but the same is calculated for the second and third. The final loss is the average. The multiple conformer example on the right shows two conformers per molecule c = 2, and the loss is adjusted to treat all of them as positive pairs if they come from the same molecule and as negative pairs otherwise. Our loss L multi3D achieves this.</figDesc><graphic url="image-3.png" coords="5,93.08,67.06,408.25,210.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tuning on GEOM-Drugs. In this case, the pretraining data only contains the elements C, H, N, O, and F while the target data contains eleven additional elements that are unseen during pre-training.Results interpretation. Such consistent and out-ofdistribution improvements can be explained by the type of information captured with 3D Infomax. Learning to reason about molecular geometry and its impact does not depend on the data's molecular space. Therefore it is not necessary to have a high similarity between the molecules during pre-training and fine-tuning.Another advantage of 3D Infomax is its comparably fast convergence. Pre-training on 620k molecules of QMugs with 3 conformers takes 12 hours, compared to 71 hours for GraphCL on 280k molecules of GEOM-Drugs.5.4. Number of Conformers and pre-training Molecules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The MAE for the QM9's homo and GEOM-Drugs' Gibbs property when varying the number of GEOM-Drugs' conformers used during pre-training.</figDesc><graphic url="image-4.png" coords="8,65.90,422.94,210.59,143.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The MAE when using different numbers of molecules of GEOM-Drugs during pre-training.</figDesc><graphic url="image-5.png" coords="8,317.90,67.06,210.59,144.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Depiction of groups of molecules where all the molecules in the top row have the same Bemis-Murcko scaffold (Bemis<ref type="bibr" target="#b3">&amp; Murcko, 1996)</ref> which is different from the scaffold to which the two molecules in the bottom row belong. We can easily obtain the scaffold of a molecule using RDKit<ref type="bibr" target="#b29">(Landrum, 2016)</ref>.</figDesc><graphic url="image-6.png" coords="13,55.44,379.07,486.00,165.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. a) The average number of conformers necessary to cover a certain amount of Boltzmann weight in GEOM-Drugs. For a given amount of cumulative Boltzmann weight on the horizontal axis, the vertical axis shows the average number of conformers necessary to pass that threshold. b) Histogram of how many molecules there are in GEOM-Drugs with a certain amount of conformers. The histogram is created for 1000 molecules of GEOM-Drugs.</figDesc><graphic url="image-8.png" coords="17,175.70,250.31,243.00,182.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The concrete 3D datasets of which we use subsets for pretraining are: QM9<ref type="bibr" target="#b36">(Ramakrishnan et al., 2014)</ref> which contains 134k small molecules (18 atoms on average) with a single conformer, GEOM-Drugs (Axelrod &amp; Gomez-Bombarelli, 2020) with 304k molecules and QMugs<ref type="bibr" target="#b24">(Isert et al., 2021)</ref> with 665k. GEOM-Drugs and QMugs, both consist of larger drug-like molecules (44.4 and 30.6 atoms on average) with multiple conformers.</figDesc><table /><note>For fine-tuning, we predict ten quantum properties of subsets of QM9 and GEOM-Drugs, for which we remove the 3D information. These subsets never have molecules that are contained in the pre-training data, even if the dataset is used to draw pre-training and fine-tuning data from. Additionally, we report results on non-quantum properties of ten OGB<ref type="bibr" target="#b21">(Hu et al., 2020a)</ref> datasets in Appendix C.1. The atom and bond features we use for each 2D molecular graph are the same as used in OGB, and further details about all used data are in Appendix B.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>MAE for QM9's properties. 3D Infomax is tested with three different pre-training datasets and GraphCL uses a two times larger subset of GEOM-Drugs. True 3D SMP is a 3D GNN using ground truth 3D coordinates (hidden from other methods). Details on confidence intervals are in Appendix B. Colors indicate improvement (lower MAE) or worse performance compared to the randomly initialized (Rand Init) model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">PRE-TRAINING BASELINES</cell><cell></cell><cell cols="3">OUR 3D INFOMAX</cell><cell cols="2">RDKIT True 3D</cell></row><row><cell>TARGET</cell><cell>RAND INIT</cell><cell cols="4">GRAPHCL PROPRED DISPRED CONFGEN</cell><cell>QM9</cell><cell cols="2">DRUGS QMUGS</cell><cell>SMP</cell><cell>SMP</cell></row><row><cell>µ</cell><cell>0.4133±0.003</cell><cell>0.3937</cell><cell>0.3975</cell><cell>0.4626</cell><cell>0.3940</cell><cell cols="5">0.3507 0.3512 0.3668 0.4344 0.0726</cell></row><row><cell>α</cell><cell>0.3972±0.014</cell><cell>0.3295</cell><cell>0.3732</cell><cell>0.3570</cell><cell>0.4219</cell><cell cols="5">0.3268 0.2959 0.2807 0.3020 0.1542</cell></row><row><cell>HOMO</cell><cell>82.10±0.33</cell><cell>79.57</cell><cell>93.11</cell><cell>80.58</cell><cell>79.75</cell><cell>68.96</cell><cell>70.78</cell><cell>70.77</cell><cell>82.51</cell><cell>56.19</cell></row><row><cell>LUMO</cell><cell>85.72±1.62</cell><cell>80.81</cell><cell>99.84</cell><cell>84.93</cell><cell>79.16</cell><cell>69.51</cell><cell>71.38</cell><cell>78.10</cell><cell>80.36</cell><cell>43.58</cell></row><row><cell>GAP</cell><cell>123.08±3.98</cell><cell>120.08</cell><cell>131.99</cell><cell>116.21</cell><cell>110.72</cell><cell cols="4">101.71 102.59 103.85 114.24</cell><cell>85.10</cell></row><row><cell>R2</cell><cell>22.14±0.21</cell><cell>21.84</cell><cell>29.21</cell><cell>29.23</cell><cell>20.86</cell><cell>17.39</cell><cell>18.96</cell><cell>18.00</cell><cell>22.63</cell><cell>1.51</cell></row><row><cell>ZPVE</cell><cell>15.08±2.83</cell><cell>12.39</cell><cell>11.17</cell><cell>25.91</cell><cell>21.10</cell><cell>7.966</cell><cell>9.677</cell><cell>12.06</cell><cell>5.18</cell><cell>2.69</cell></row><row><cell>cv</cell><cell>0.1670±0.004</cell><cell>0.1422</cell><cell>0.1795</cell><cell>0.1587</cell><cell>0.1555</cell><cell cols="5">0.1306 0.1409 0.1208 0.1419 0.0498</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>further confirms that 3D Infomax substantially improves quantum property predictions and generalizes out-of-distribution. Our method outperforms GraphCL, even though GraphCL also sees the fine-tuning molecules during pre-training. Moreover, we observe strong generalization when pre-training with QM9</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>The MAE for predicting GEOM-Drugs' properties. 3D Infomax compared with GraphCL and no pre-training.</figDesc><table><row><cell>METHOD</cell><cell cols="2">GIBBS ⟨E⟩</cell></row><row><cell>RAND INIT</cell><cell>.2035</cell><cell>.1026</cell></row><row><cell>GRAPHCL</cell><cell>.1941</cell><cell>.0995</cell></row><row><cell>3D INFOMAX QM9</cell><cell>.1852</cell><cell>.0968</cell></row><row><cell>3D INFOMAX DRUGS</cell><cell>.1811</cell><cell>.0952</cell></row><row><cell cols="2">3D INFOMAX QMUGS .</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Statistics of the used datasets. In the upper section are datasets with 3D information, which we use for pre-training, and the datasets in the bottom section do not contain additional 3D annotations.Code to 3D pre-train a GNN or to reproduce results is available at https://github.com/HannesStark/ 3DInfomax. All experiments were implemented in PyTorch<ref type="bibr" target="#b34">(Paszke et al., 2017)</ref> using the deep learning libraries for processing graphs Pytorch Geometric</figDesc><table><row><cell>DATASET</cell><cell cols="3">#MOLECULES AVG. #ATOMS AVG. #BONDS</cell><cell>SPLIT</cell></row><row><cell>QM9</cell><cell>130 831</cell><cell>18.0</cell><cell>18.6</cell><cell>RANDOM</cell></row><row><cell>GEOM-DR U G S</cell><cell>304 293</cell><cell>44.4</cell><cell>46.4</cell><cell>RANDOM</cell></row><row><cell>QMU G S</cell><cell>665 911</cell><cell>30.6</cell><cell>33.4</cell><cell>RANDOM</cell></row><row><cell>E S O L</cell><cell>1128</cell><cell>13.3</cell><cell cols="2">13.7 SCAFFOLD</cell></row><row><cell>L I P O</cell><cell>4200</cell><cell>27.0</cell><cell cols="2">29.5 SCAFFOLD</cell></row><row><cell>F R E E S O L V</cell><cell>642</cell><cell>8.7</cell><cell cols="2">8.4 SCAFFOLD</cell></row><row><cell>B A C E</cell><cell>1512</cell><cell>34.1</cell><cell cols="2">36.9 SCAFFOLD</cell></row><row><cell>B B B P</cell><cell>2039</cell><cell>24.1</cell><cell cols="2">26.0 SCAFFOLD</cell></row><row><cell>H I V</cell><cell>41 127</cell><cell>25.5</cell><cell cols="2">27.5 SCAFFOLD</cell></row><row><cell>T O X21</cell><cell>7831</cell><cell>18.6</cell><cell cols="2">19.3 SCAFFOLD</cell></row><row><cell>T O X C A S T</cell><cell>8576</cell><cell>18.8</cell><cell cols="2">19.3 SCAFFOLD</cell></row><row><cell>C L I N T O X</cell><cell>1477</cell><cell>26.2</cell><cell cols="2">27.9 SCAFFOLD</cell></row><row><cell>S I D E R</cell><cell>1427</cell><cell>33.6</cell><cell cols="2">35.4 SCAFFOLD</cell></row><row><cell>B.3. Implementation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Units and description of quantum mechanical properties of the QM9 dataset.</figDesc><table><row><cell cols="2">PROPERTY UNIT</cell><cell>DESCRIPTION</cell></row><row><cell>µ</cell><cell cols="2">DEBYE DIPOLE MOMENT</cell></row><row><cell>α</cell><cell>Bohr 3</cell><cell>ISOTROPIC POLARIZABILITY</cell></row><row><cell>homo</cell><cell>MEV</cell><cell>ENERGY OF HIGHEST OCCUPIED MOLECULAR ORBITAL (HOMO)</cell></row><row><cell>lumo</cell><cell>MEV</cell><cell>ENERGY OF LOWEST OCCUPIED MOLECULAR ORBITAL (LUMO)</cell></row><row><cell>gap</cell><cell>MEV</cell><cell>GAP, DIFFERENCE BETWEEN LUMO AND HOMO</cell></row><row><cell>r2</cell><cell>Bohr 2</cell><cell>ELECTRONIC SPATIAL EXTENT</cell></row><row><cell>ZP V E</cell><cell>MEV</cell><cell>ZERO POINT VIBRATIONAL ENERGY</cell></row><row><cell>cv</cell><cell>cal molK</cell><cell>HEAT CAPACITY AT 298.15 K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Additional confidence intervals of our method in Table1. All standard deviations are calculated from 4 seeds except for the homo property where 6 are used.</figDesc><table><row><cell>OUR 3D INFOMAX</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Additional confidence intervals for Table2.</figDesc><table><row><cell>METHOD</cell><cell>GIBBS</cell><cell>⟨E⟩</cell></row><row><cell>RAND INIT</cell><cell cols="2">.2035± 0.0011 .1026± 0.0017</cell></row><row><cell>GRAPHCL</cell><cell>.1941</cell><cell>.0995</cell></row><row><cell>3D INFOMAX QM9</cell><cell>.1852</cell><cell>.0968</cell></row><row><cell>3D INFOMAX DRUGS</cell><cell>.1811</cell><cell>.0952</cell></row><row><cell cols="2">3D INFOMAX QMUGS .1835</cell><cell>.0965</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Additional confidence intervals for 3D pre-training baselines inTable 5.3    </figDesc><table><row><cell>METHOD</cell><cell>µ</cell><cell>α</cell><cell>HOMO</cell><cell>LUMO</cell><cell>GAP</cell><cell>R2</cell><cell>ZPVE cv</cell></row><row><cell>RAND INIT</cell><cell cols="7">±0.003 ±0.014 ±0.33 ±1.62 ±3.98 ±0.21 ±2.83 ±0.004</cell></row><row><cell cols="8">3D INFOMAX ±0.010 ±0.009 ±0.82 ±0.74 ±3.27 ±0.69 ±1.29 ±0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Comparison of 3D pre-training baselines and GraphCL against 3D Infomax on various OGB datasets. Shown is either the root mean squared error (RMSE) (lower is better) or the area under the ROC-curve (ROC-AUC) (higher is better). Colors indicate improvement, worse performance, or no significant change compared to the randomly initialized (Rand Init) model.</figDesc><table><row><cell>DATASET</cell><cell>METRIC</cell><cell>RAND INIT</cell><cell>DISPRED</cell><cell>CONFGEN</cell><cell>GRAPHCL</cell><cell>3D INFOMAX</cell></row><row><cell>ESOL</cell><cell>RMSE ↓</cell><cell cols="4">0.947±0.038 0.986±0.025 0.867±0.045 0.959±0.047</cell><cell>0.894±0.028</cell></row><row><cell>LIPO</cell><cell>RMSE ↓</cell><cell cols="4">0.739±0.009 0.718±0.021 0.757±0.035 0.714±0.011</cell><cell>0.695±0.012</cell></row><row><cell>FREESOLV</cell><cell>RMSE ↓</cell><cell cols="4">2.233±0.261 2.486±0.222 2.428±0.155 3.744±0.292</cell><cell>2.337±0.227</cell></row><row><cell>BACE</cell><cell>ROC-AUC ↑</cell><cell>78.13±1.30</cell><cell>76.51±1.95</cell><cell>80.02±1.58</cell><cell>77.18±4.01</cell><cell>79.42±1.94</cell></row><row><cell>BBBP</cell><cell>ROC-AUC ↑</cell><cell>68.27±1.98</cell><cell>66.06± 1.84</cell><cell>66.16±2.24</cell><cell>71.06±2.00</cell><cell>69.10±1.07</cell></row><row><cell>TOX21</cell><cell>ROC-AUC ↑</cell><cell>73.88±0.64</cell><cell>73.87±0.43</cell><cell>75.24±1.00</cell><cell>78.92±0.61</cell><cell>74.46±0.74</cell></row><row><cell>TOXCAST</cell><cell>ROC-AUC ↑</cell><cell>63.62±0.48</cell><cell>61.58±0.58</cell><cell>64.74±1.20</cell><cell>64.95±0.31</cell><cell>64.41±0.88</cell></row><row><cell>CLINTOX</cell><cell>ROC-AUC ↑</cell><cell>58.98±5.40</cell><cell>55.77±5.86</cell><cell>64.27±5.22</cell><cell>51.07±5.52</cell><cell>59.43±3.21</cell></row><row><cell>SIDER</cell><cell>ROC-AUC ↑</cell><cell>55.95±3.27</cell><cell>57.13±1.89</cell><cell>56.34±4.20</cell><cell>57.32±5.00</cell><cell>53.37±3.34</cell></row><row><cell>HIV</cell><cell>ROC-AUC ↑</cell><cell>77.06±3.16</cell><cell>75.66±1.26</cell><cell>76.57±1.39</cell><cell>76.06±1.06</cell><cell>76.08±1.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 .</head><label>11</label><figDesc>Comparison of 3D networks. The MAE of the homo property pre-training and fine-tuning on different halves of QM9. Net3D w/o γ refers to dropping the distance encoding of Net3D. Net3D achieves the best MAE.</figDesc><table><row><cell>METHOD</cell><cell>QM9 MAE</cell></row><row><cell>RAND INIT</cell><cell>82.10±0.33</cell></row><row><cell>SMP</cell><cell>72.37</cell></row><row><cell>EGNN</cell><cell>70.46</cell></row><row><cell cols="2">Net3D W/O γ 70.34</cell></row><row><cell>Net3D</cell><cell>68.96±0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 .</head><label>14</label><figDesc>Comparison of latent space SSL methods. The numbers show the MAE when predicting QM9's homo property after pre-training on one half of QM9 with the given method and fine-tuning on the other half of QM9. The Rand Init column shows the MAE without pre-training and with random weight initialization. 3D Infomax is our best latent space SSL method.</figDesc><table><row><cell cols="3">RANDOM INIT 3D INFOMAX BYOL</cell><cell cols="2">BARLOW TWINS VICREG</cell></row><row><cell>QM9 MAE 82.10±0.33</cell><cell>68.96±0.32</cell><cell cols="2">79.16±0.58 82.38±0.48</cell><cell>85.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 .</head><label>15</label><figDesc>MAE for predicting QM9's molecular properties. SMP is tested with random weight initialization and with the weights obtained from using it as 3D network in our 3D Infomax pre-training setup.</figDesc><table><row><cell cols="3">TARGET SMP RAND INIT SMP PRE-TRAINED</cell></row><row><cell>µ</cell><cell>0.0726</cell><cell>0.0801</cell></row><row><cell>α</cell><cell>0.1542</cell><cell>0.1276</cell></row><row><cell>HOMO</cell><cell>56.19</cell><cell>44.50</cell></row><row><cell>LUMO</cell><cell>43.58</cell><cell>37.48</cell></row><row><cell>GAP</cell><cell>85.10</cell><cell>70.45</cell></row><row><cell>R2</cell><cell>1.51</cell><cell>1.12</cell></row><row><cell>ZPVE</cell><cell>2.69</cell><cell>2.43</cell></row><row><cell>cv</cell><cell>0.0498</cell><cell>0.0421</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 .</head><label>16</label><figDesc>MAE for QM9's properties. 3D Infomax is tested with three pre-training datasets and compared with the 3D GNN SMP using explicit 3D coordinates. The conformers are generated using the classical method RDKit ETKDG or the learned method GeoMol. Colors indicate improvement (lower MAE) or worse performance compared to the randomly initialized (Rand Init) model.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">OUR 3D INFOMAX</cell><cell cols="2">RDKIT GEOMOL</cell></row><row><cell>TARGET</cell><cell>RAND INIT</cell><cell>QM9</cell><cell cols="2">DRUGS QMUGS</cell><cell>SMP</cell><cell>SMP</cell></row><row><cell>µ</cell><cell cols="4">0.4133±0.003 0.3507 0.3512 0.3668</cell><cell>0.4344</cell><cell>0.6046</cell></row><row><cell>α</cell><cell cols="4">0.3972±0.014 0.3268 0.2959 0.2807</cell><cell>0.3020</cell><cell>0.8435</cell></row><row><cell>HOMO</cell><cell>82.10±0.33</cell><cell>68.96</cell><cell>70.78</cell><cell>70.77</cell><cell>82.51</cell><cell>195.0</cell></row><row><cell>LUMO</cell><cell>85.72±1.62</cell><cell>69.51</cell><cell>71.38</cell><cell>78.10</cell><cell>80.36</cell><cell>201.4</cell></row><row><cell>GAP</cell><cell>123.08±3.98</cell><cell cols="3">101.71 102.59 103.85</cell><cell>114.24</cell><cell>284.1</cell></row><row><cell>R2</cell><cell>22.14±0.21</cell><cell>17.39</cell><cell>18.96</cell><cell>18.00</cell><cell>22.63</cell><cell>65.84</cell></row><row><cell>ZPVE</cell><cell>15.08±2.83</cell><cell>7.966</cell><cell>9.677</cell><cell>12.06</cell><cell>5.18</cell><cell>17.40</cell></row><row><cell>cv</cell><cell cols="4">0.1670±0.004 0.1306 0.1409 0.1208</cell><cell>0.1419</cell><cell>0.5467</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/divelab/DIG</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau. html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>The authors express their gratitude to Simon Axelrod (GEOM Dataset and molecular physics advice), Limei Wang and Meng Liu (Spherical Message Passing), Adrien Bardes and Samuel Lavoie (SSL, VICReg, and BYOL), Octavian Ganea and Lagnajit Pattanaik (molecular geometry, Geo-Mol), Christopher Scarveils (part of the project in its beginnings), Bertrand Charpentier and Aleksandar Bojchevski (general help) as well as Zekarias Kefato, Grégoire Mialon, Kenneth Atz, Johannes Klicpera, Minghao Xu, Mozhi Zhang, Shantanu Thakoor, Minkai Xu, Shitong Luo, Yuning You, and Prannay Khosla for insightful discussions.</p><p>CD acknowledges support by the Bundesministerium für Bildung und Forschung (BMBF), project number 01IS17049.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">12</ref>. Comparison of strategies for using multiple conformers. The middle double-column shows the results for pre-training on one half of GEOM-Drugs and the right double-column corresponds to pre-training on QMugs, and the second row indicates what dataset was used for fine-tuning. The Random Init row shows the performance when training from scratch without any pre-training. For QM9, the reported number is the MAE of the homo property, and for GEOM-Drugs it is the MAE when predicting the ensemble Gibbs free energy. There are large improvements from using multiple conformers, but the differences between the methods are small. instead, the MAE are overall slightly worse, and we find multi3D to perform the best. Note that this is with the slight caveat that the epoch at which pre-training is stopped for all methods was chosen based on where multi3D had the lowest MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LOSS/ESTIMATOR</head><p>Due to these results, we consider multi3D, and conformer sampling with uniform weighting as our best methods since multi3D performs slightly better with pre-training on QMugs but conformer sampling is simpler and especially uses much less memory. For multi3D, all the conformers need to be processed in parallel, and training with more than 5 conformers and a batch size of 500 would not be possible on a 48GB vRAM GPU.</p><p>The hypothesis that the downstream performance on the smaller molecules of QM9 would benefit less from using multiple conformers than the molecules of GEOM-Drugs clearly does not hold. Surprisingly, the improvements on the small molecules of QM9 are larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Different Losses</head><p>We compare the different losses to estimate and maximize the mutual information. For this purpose, we pre-train PNA on 50 000 molecules from QM9 and another instance on 140 000 molecules of GEOM-Drugs, both with a single conformer. We do so with the Donsker-Varadhan <ref type="bibr" target="#b20">(Hjelm et al., 2019)</ref> estimator, the Jensen-Shannon estimator <ref type="bibr" target="#b20">(Hjelm et al., 2019)</ref>, InfoNCE, and our loss. For our loss, we search over seven temperature parameters τ ∈ [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7] and choose τ = 0.1. In Table <ref type="table">13</ref> we see that 3D pre-training on GEOM-Drugs or QM9 can yield significant improvements for predicting quantum mechanical properties, especially when using InfoNCE and our loss as objectives. These two objectives perform better than the Donsker-Varadhan, and Jensen-Shannon estimator in every case and the Jensen-Shannon objective is superior to the Donsker-Varadhan estimator, which seems to yield no significant improvements over random initialization. The superiority of the Jensen-Shannon loss over the Donsker-Varadhan alternative is in line with the findings of <ref type="bibr" target="#b20">Hjelm et al. (2019)</ref> in their different setting on images. While our loss seems to perform better than InfoNCE in three settings, this might be due to the additional investment in searching through temperature parameters for our loss. Here we simply use GraphCL's node drop augmentation for the 2D graph and the 3D information (removing all pairwise distances for a removed atom) with a drop ratio of 0.2 during our 3D pre-training process. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Energyannotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><surname>Geom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Molecular machine learning with conformer ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<idno>CoRR, abs/2012.08452</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vicreg: Varianceinvariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/2105.04906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The properties of known drugs. 1. molecular frameworks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Murcko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">2018. July 10-15, 2018. 2018. 1996</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2887" to="2893" />
		</imprint>
	</monogr>
	<note>Mutual information neural estimation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gnn-film: Graph neural networks with feature-wise linear modulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1144" to="1152" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<idno>CoRR, abs/2104.13478</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Utilizing edge features in graph neural networks via variational information maximization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020b</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011.10566, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A graphconvolutional neural network model for the prediction of chemical reactivity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="370" to="377" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantum chemistry in the age of machine learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2336" to="2347" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Geomol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Torsional geometric generation of molecular 3d conformer ensembles</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An open-source drug discovery platform enables ultralarge virtual screens</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gorgulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boeszoermenyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Coote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Padmanabha Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Malets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Radchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Moroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fackeldey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Iavniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arthanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">580</biblScope>
			<biblScope unit="issue">7805</biblScope>
			<biblScope unit="page" from="663" to="668" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to selfsupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Á</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploration of chemical compound, conformer, and reaction space with meta-dynamics simulations based on tight-binding quantum chemical calculations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grimme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2847" to="2862" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
				<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05-06">May 13-15, 2010. 2010. May 6-9, 2019. OpenReview.net, 2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>7th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. OpenReview.net, 2020b</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting molecular properties with covariant compositional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241745</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Qmugs: Quantum mechanical properties of drug-like molecules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Isert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Atz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiménez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berghammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<editor>NeurIPS-W</editor>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><surname>Gemnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics software</title>
		<author>
			<persName><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR, abs/1709.03741</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pre-training molecular graph representation with 3d geometry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICLR 2022</idno>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Spherical message passing for 3d graph networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno>CoRR, abs/2102.05013</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">E(n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/2102.09844</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recent advances and applications of machine learning in solid-state materials science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Botti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Computational Materials</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Schnet: A continuousfilter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<editor>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Macnair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bloom-Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiappino-Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Badran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Chory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">18-24 July 2021. 2021. 2020</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="688" to="702" />
		</imprint>
	</monogr>
	<note>A deep learning approach to antibiotic discovery</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Contrastive representation distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10699</idno>
		<ptr target="http://arxiv.org/abs/1910.10699" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for predicting drug-target interactions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4131" to="4149" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Physnet: A neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Molclr: Molecular contrastive learning of representations via graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Farimani</surname></persName>
		</author>
		<idno>CoRR, abs/2102.10056</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Building attention and edge message passing neural networks for bioactivity and physical-chemical property prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Withnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindelöf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><surname>Moleculenet</surname></persName>
		</author>
		<idno>CoRR, abs/1703.00564</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning neural generative dynamics for molecular conformation generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Selfsupervised graph-level representation learning with local and global structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021b</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11548" to="11558" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Settels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dual-view molecule pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2106.10234</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On these 3D datasets we also fine-tune and evaluate the quantum mechanical properties of one half of the datasets with a random split. This is done after either pre-training on another 3D dataset (generalization), or after pre-training on the other half of the same dataset</title>
		<author>
			<persName><forename type="first">Geom-Drugs</forename><surname>Qm9</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in distribution</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">ESOL: 1128 common organic small molecules with water solubility data (log solubility in mols per liter)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Experimental data for the octanol/water distribution coefficient of 4200 molecules</title>
		<author>
			<persName><surname>Lipo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">FreeSolv: The hydration free energy of 642 molecules in water</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m">HIV: 41k molecules with binary labels for HIV virus replication inhibition</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">BACE: Binary labels of binding results for inhibitors of human β-secretase 1 for 1512 molecules</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">BBBP: 2039 molecules with binary labels for blood-brain barrier penetration</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Tox21: 7831 molecules with binary labels of their toxic for 12 different targets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">ToxCast: 8576 molecules with binary labels of toxicity experiment outcomes with 617 targets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">SIDER: 1427 approved drugs with 27 different side effect groups and the task is to predict whether the drug is in the side effect group</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">ClinTox: 1477 drugs with two binary annotations where the first is to predict toxicity in clinical trials and the second is the FDA approval status. The reason why muv and pcba are the only datasets from the OGB benchmark suite which we omit is their larger size</title>
		<ptr target="https://github.com/klicperajo/dimenet/blob/master/data/qm9_eV.npz4https://github.com/learningmatter-mit/geom5https://www.research-collection.ethz.ch/handle/20.500.11850/4821296https://ogb.stanford.edu/docs/graphprop" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
