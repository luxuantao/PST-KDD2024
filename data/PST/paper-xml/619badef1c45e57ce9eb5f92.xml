<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the cross-lingual transferability of multilingual prototypical models across NLU tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-19">19 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oralie</forename><surname>Cattan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">QWANT</orgName>
								<address>
									<addrLine>61 rue de Villiers</addrLine>
									<postCode>92200</postCode>
									<settlement>Neuilly-sur-Seine</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LISN</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Servan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">QWANT</orgName>
								<address>
									<addrLine>61 rue de Villiers</addrLine>
									<postCode>92200</postCode>
									<settlement>Neuilly-sur-Seine</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophie</forename><surname>Rosset</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit? Paris-Saclay</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LISN</orgName>
								<address>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the cross-lingual transferability of multilingual prototypical models across NLU tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-19">19 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.09157v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised deep learning-based approaches have been applied to task-oriented dialog and have proven to be effective for limited domain and language applications when a sufficient number of training examples are available. In practice, these approaches suffer from the drawbacks of domain-driven design and under-resourced languages. Domain and language models are supposed to grow and change as the problem space evolves. On one hand, research on transfer learning has demonstrated the cross-lingual ability of multilingual Transformers-based models to learn semantically rich representations. On the other, in addition to the above approaches, meta-learning have enabled the development of task and language learning algorithms capable of far generalization. Through this context, this article proposes to investigate the cross-lingual transferability of using synergistically few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments in natural language understanding tasks on MultiATIS++ corpus shows that our approach substantially improves the observed transfer learning performances between the low and the high resource languages. More generally our approach confirms that the meaningful latent space learned in a given language can be can be generalized to unseen and underresourced ones using meta-learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditionally, Natural Language Understanding (NLU) is an intermediate module between the user interface and the dialogue management module in a dialogue system. It aims to extract semantic information from a user's query or utterance to fill slots in a domain specific semantic frame. Domain classification, intent detection and slot filling are three core components belonging to the NLU. They are in charge of determining the domain or service of a users query, its underlying goal or in-tent and associating utterance segments with conceptual labels, called slots, similar to named entity recognition.</p><p>NLU is usually defined as a supervised learning problem, involving conventional machine learning models on massive amount of annotated training data, which are language dependent. This prerequisite has prevented its widespread adoption for poorly endowed languages and for small technology companies that do not benefit from millions of users to gather data. Besides the requirement of a large amount of annotated data being available, domains, intents and slots are language dependent. Consequently, in practice, the resulting systems are hardly adaptable to expand to new languages.</p><p>As a solution to this problem, cross-lingual transfer approaches were developed to leverage the knowledge from well-resourced languages, with task specific data available to underresourced languages with little or no data. Recent efforts focused on training Transformer models multilingually such as the multilingual version of BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. While earlier work demonstrated the effectiveness of multilingual models to learn representations which are transferable across languages, they show limitations when applied to low-resource languages <ref type="bibr" target="#b18">(Pires et al., 2019;</ref><ref type="bibr" target="#b2">Conneau et al., 2020)</ref>. From another perspective low-shot learning such as fewshot and zero-shot, aims to transfer knowledge learned from one language to another when the training data is limited or is missing some task labels.</p><p>As a core contribution, we explore the potential for cross-lingual transferability of multilingual Transformer-based model <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> (mBERT) combined with a few-shot learning algorithm based on prototypical representations. We also introduce a zero-shot scenario, where models are trained on multiple languages and evalu-ated on another. Our proposed approach relies on appending a mBERT encoder module to the prototypical neural network, which is a proven fewshot model, originally designed for image classification. Our experimental results show that the generated model trained with a limited number of annotated training examples outperforms the transfer learning based approach on MultiATIS++ dataset <ref type="bibr" target="#b31">(Xu et al., 2020;</ref><ref type="bibr" target="#b25">Upadhyay et al., 2018)</ref> and can be applied to unseen languages directly with decent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The availability of large datasets has enabled deep learning methods to achieve great success in a variety of fields. However, most of these successes are based on supervised learning approaches, which require lots of labeled data to train. Most datasets are only available in English. Only a few other languages are supported, and most of them are considered as under-resourced languages.</p><p>Recently, meta-learning approaches have enabled the development of task-agnostic learning algorithms capable of far generalizations (crossdomain or cross-lingual) in the context of having a low-data regime. Because literature on low-shot learning is vast and diverse, only the most relevant approaches to this work are presented and we refer the reader to <ref type="bibr" target="#b26">Vanschoren (2019)</ref> and <ref type="bibr" target="#b29">Wang et al. (2019)</ref> for a surveys of earlier work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Low-shot learning</head><p>Humans manifest a capacity of learning new concepts from few stimuli quickly and efficiently by utilizing prior knowledge and experience. Inspired by this ability, there has been a resurgence of interest in designing specialized models to perform low-shot learning. An example of this form of learning is metric-based approaches founded on the simple idea of learning a discriminative metric space in which similar samples are mapped close to each other and dissimilar ones distant. Siamese <ref type="bibr" target="#b14">(Koch, 2015)</ref>, Matching <ref type="bibr" target="#b28">(Vinyals et al., 2016)</ref> or Prototypical <ref type="bibr" target="#b23">(Snell et al., 2017)</ref> networks belong to this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Supervised generalization</head><p>In recent years, several approaches have been introduced and refined to overcome the issue of data-limited regime. As an example, the Prototypical Neural Networks (PNNs), developed by <ref type="bibr" target="#b23">Snell et al. (2017)</ref> originally for image classification, were used to extract representative characteristics of the data by mapping data points into an embedding space where each sample will cluster around their respective prototype representation. <ref type="bibr" target="#b6">Fort (2017)</ref> proposed to extend their work by adding a confidence region around prototypes with the help of Gaussian covariance models.With the aim of improving the generalization capacity of metric-based methods, <ref type="bibr" target="#b30">Wang et al. (2018)</ref> proposed to enforce a large margin between the class prototypes by modifying the standard softmax loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Semi-supervised generalization</head><p>Other approaches, closely related to the aforementioned ones, proposed to take advantage of labeled and unlabeled data. Among them, <ref type="bibr" target="#b2">Boney and Ilin (2017)</ref> extended PNNs to address semi-supervised image classification problems. They applied a hard clustering to assign the class for the unlabeled examples within the latent space learned by the PNNs. A close method was developed by <ref type="bibr" target="#b21">Ren et al. (2018)</ref> to refine the prototype generation process with clustering. The authors introduced distractor classes with the aim of handling unlabeled samples not belonging to any of the known classes.</p><p>Most of these approaches have mainly been explored in the field of computer vision, and a few of them were applied to NLP fields, such like Natural Language Understanding (NLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NLU using low-shot learning</head><p>A number of different deep learning approaches have been applied to the problem of language understanding in recent years. For a thorough overview of deep learning methods in conversational language understanding, we refer the readers to <ref type="bibr" target="#b8">Gao et al. (2018)</ref>. In the context of relying on limited training resources, fewshot learning has been used for NLU tasks. <ref type="bibr" target="#b32">Yazdani and Henderson (2015)</ref> proposes a method to leverage unlabeled data in order to find the separating hyperplanes that divide the utterances with the same label from those with different labels. <ref type="bibr" target="#b24">Sun et al. (2019)</ref> extended PNNs for intent classification using hierarchical attention mechanisms when generating the prototype representations.</p><p>Slot filling using few-shot models has also been explored. <ref type="bibr" target="#b5">Ferreira et al. (2015)</ref> presented a zeroshot approach based on a knowledge base and on word representations learned from unlabeled data. <ref type="bibr" target="#b7">Fritzler et al. (2019)</ref> applied PNNs to few-shot named entity recognition by training a separate model for each entity type and <ref type="bibr" target="#b13">Hou et al. (2019)</ref> proposed a conditional random forest-based approach enhanced with transfer mechanisms that implicitly incorporate label dependencies and similarities. More recently, <ref type="bibr" target="#b4">Dou et al. (2019)</ref>, <ref type="bibr">Bansal et al. (2020a)</ref> and <ref type="bibr">Bansal et al. (2020b)</ref> applied various meta-learned models to few-shot NLU across domains and tasks.</p><p>Finally, besides the approaches of <ref type="bibr" target="#b9">Gu et al. (2018)</ref> and <ref type="bibr" target="#b33">Zhang et al. (2020)</ref> that focus on handling new and low-resource languages for machine translation, to the best of our knowledge, there are no approaches that combine cross-lingual transfer and meta-learning methods for NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present the design of a Prototypical Neural Network and its episodic training procedure before introducing our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prototypical Neural Networks</head><p>Prototypical Neural Networks <ref type="bibr" target="#b23">(Snell et al., 2017)</ref> or PNNs are based on the computation of distance measures between seen-class prototypes to unseen ones. More specifically, a D-dimensional embedding is generated for each example x ?R D using a neural network based function f (?) parameterized by ?. This function enhances the encoding process with better separability properties through a non-linear mapping f ? :R D ?R M . The Mdimensional prototype of each class is formed as the centroid c i of their embedded support points as seen in Equation (1):</p><formula xml:id="formula_0">c i = 1 |S i | (x j ,y j )?S i f ? (x j ),<label>(1)</label></formula><p>where S i represents the set of examples labeled with class i and y j the corresponding label of x j . Equation (2) shows how, given a query (that is, a new and an unlabeled sample) q i , the probability distribution over the prototypes is computed from d(?, ?), an arbitrary similarity measures function such as the squared euclidean distance or cosine similarity.</p><formula xml:id="formula_1">p ? (y i |q i ) = exp(-d(f ? (q i ), c i )) i ? exp(-d(f ? (q i ), c i ? ))<label>(2)</label></formula><p>Finally, the class with the highest probability is chosen by a softmax over the distances and at optimization time, the negative log-probability J(?) = -log p ? (y i |q i ) of the true class of each query point is minimized by stochastic gradient descent during an episodic learning process described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Episodic learning</head><p>With the aim of generalizing unseen classes from zero to few training examples per class, PNNs is trained from a collection of N -way, k-shot classification tasks through an episodic training procedure <ref type="bibr" target="#b28">(Vinyals et al., 2016)</ref>. Specifically, each episode is one mini-batch consisting of k examples from each of the N classes (both randomly sampled), used to form a labeled (support S) and an unlabeled set of examples (query Q). The parameter k often takes a very small value, meaning we have zero-to-k labeled samples. During training, the model is fed with S to construct the class prototypes using Equation (1). Its parameters are learned in order to minimize the prototypical loss of its predictions for the examples in the given Q according to Equation (3) of Section 3.1. The evaluation is done by averaging the classification performances on query sets of many testing episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transformer-based PNNs</head><p>Studies have demonstrated that contextualized representations produced by language models such as ELMo <ref type="bibr" target="#b17">(Peters et al., 2018)</ref> or BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> gave neural networks a better training initializations. Rather than training the initialized encoder of PNNs with feature extractors such as convolution or recurrent networks we propose to induce robustness of the pre-trained multilingual BERT (mBERT) to test the distinctiveness of the representation of each class accross languages. The embedding layer is initialized with the pretrained mBERT embeddings and fine-tuned together with a dense linear layer that defines the embedding space where the prototype-based classifier operates. This latent space is used to learn prototypes of each class by estimating their mean and the chosen class is derived from the output layer of the network based on a softmax over distance to the class prototypes. The motivation behind fine-tuning the encoder with prototypical loss is to induce better generalization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The cross-lingual way</head><p>As introduced earlier, even though recent works demonstrate strong cross-lingual transfer capability of multilingual pretrained BERT, they exhibit limitations when applied to low-resource languages <ref type="bibr" target="#b18">(Pires et al., 2019;</ref><ref type="bibr" target="#b2">Conneau et al., 2020)</ref>.</p><p>To enable cross-lingual transfer according to our few-shot scenario, we construct mutiple episodic batches E. From the available data, we draw the task sets by sampling a subset of labels to form a support set from data in the high-resources languages and a query set from data in the lowresource languages to be evaluated. NLU data consists of utterances composed of sentence-level intent labels and sequences of slot labels annotated in BIO format <ref type="bibr" target="#b20">(Ramshaw and Marcus, 1995)</ref> to define the boundary of slots. The N -way k-shot NLU task is then defined as follows: given an input query utterance in a new language q i and a k-shot support set S as references, find the most appropriate intent label or slot label sequence y:</p><formula xml:id="formula_2">argmax E ? (q i ,y i )?Q log p ? (y i |q i , S).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our NLU experiments in cross-lingual and fewshot learning for under-resources languages are conducted on MultiATIS++ <ref type="bibr" target="#b31">(Xu et al., 2020;</ref><ref type="bibr" target="#b25">Upadhyay et al., 2018)</ref> corpus, whose description follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The MultiATIS++ corpus</head><p>MultiATIS++ <ref type="bibr" target="#b25">(Upadhyay et al., 2018;</ref><ref type="bibr" target="#b31">Xu et al., 2020)</ref> is the multilingual extension of the ATIS corpus <ref type="bibr" target="#b11">(Hemphill et al., 1990)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>Pour tous les mod?les de base construits, nous utilisons les mod?les mBERT disponibles au public pr?-entra?n?s sur plus d'une centaine de langages diff?rents <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. We use the fine-tuning procedure <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> of the original mBERT model as our baseline. In sequence-level and token-level classification tasks, it takes the final hidden states (the last layer output of the multi-head Transformer) of the first [CLS] sequence token or each individual token representation as input of the prediction layer to compute classification scores. Since we plan to use transfer learning in the context of PNNs, we fine-tune the pre-trained mBERT model together with a dense linear layer that defines the embedding space (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training configurations</head><p>We perform three sets of experiments: target only, multilingual and multilingual zero-shot.</p><p>? target only: this configuration consists of us- ing only the target language data.</p><p>We also considered two cross-lingual classification tasks with a varying quantity of data between source and target languages to investigate the behaviour of different types of knowledge transfer.</p><p>? multilingual: where the training strategy aims to train a network on the concatenation of all of the nine languages and testing the model for each target language.</p><p>? multilingual zero-shot: where the training relies on the concatenation of all training datasets from all languages except the one we want to test.</p><p>This works only for the baseline approach (mBERT), but with our PNNs approach (mBERT+PNN), we performs few-shot learning. This means we use only a few training data in the considered language (target only and multilingual configurations).</p><p>For instance, when we evaluate our approach in the English task, we consider only a fraction of the English training dataset to train our mBERT+PNN model in the target only. In the multilingual configuration, our few-shot approach (mBERT+PNN) is trained using only a fraction of all the examples provided for each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training details</head><p>For all the baseline models built, we use the publicly available mBERT models pre-trained on over a hundred different languages <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. We trained it using 20 epochs like <ref type="bibr" target="#b31">Xu et al. (2020)</ref>.</p><p>PNNs training was done using a number of 1000 episodes using Euclidean distance as suggested by the original authors <ref type="bibr" target="#b23">(Snell et al., 2017)</ref>. We consider a configuration parameter and tried a 5-way k-shot intent classification with k ? [1, 10] (5w1s and 5w10s) and 5-way 10-shots slot filling.</p><p>For all approaches we use AdamW optimizer <ref type="bibr" target="#b16">(Loshchilov and Hutter, 2017</ref>) using a learning rate of 5e-5 to apply gradients with respect to the loss and weight decay.</p><p>All results are reported using the average performances of over 30 runs for intent classification and over 5 runs for slot filling (fewer amount of runs because of higher training time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Our experimental findings are summarized in Tables 2 and 3 for the intent classification and the slot-filling tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Intent classification results</head><p>Using the target only configuration, the baseline obtains optimal scores when applied to high resource languages, e.g. English (en), French (fr) or German (de) reaching nearly identical high scores. We obtain the highest baseline scores with an accuracy of 98.8 on the French model, followed by the English model with an accuracy of 98.5. Unlike other mainstream languages, the baseline is less accurate on under-resourced languages, with a loss of 7 to 15 points for intent classification on Hindi (hi) and Turkish (tr) respectively.</p><p>In multilingual configuration, baseline models perform reasonably well over all the high-resource languages with a significant performance boost due to the availability of additional data. The mBERT + PNN (5w10s) models outperformed the baseline for all languages, except for the Turkish (tr) language.</p><p>When transferring from all languages to an unseen one (multilingual zero-shot configuration) we observe the best results for the mBERT model, except Portuguese (pt) and English (en) languages, in which the mBERT + PNN (5w10s) is 0.5 points better.</p><p>Finally, within the framework of the intent classification task, the mBERT + PNN (5w10s) model achieves better overall performances in the multilingual configuration, especially in the case of under-resourced languages with a gain up to 9 points of accuracy, compared to the target-only configuration and an average of one point compared to the best model in the multilingual zeroshot configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Slot-filling results</head><p>Slot-filling result trends in the target only configuration are about one point better of F1 score for the mBERT + PNN (5w10s) model compared to the baseline model (mBERT). The mBERT + PNN (5w10s) model even outperformed the baseline by more than 4 points of F1 in the Turkish task (tr).</p><p>We can observe the same trend in the multilingual configuration: our approach outperformed the baseline in all languages.</p><p>On the contrary, the mBERT + PNN (5w10s) fails in most of language tasks in the multilingual zero-shot configuration, except for the Hindi (hi) and the Turkish (tr) languages.</p><p>Finally, like the intent classification task, the mBERT + PNN (5w10s) model achieves better overall performance in the multilingual configuration for all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Result analysis</head><p>First, our baseline results are on par with those obtained by <ref type="bibr" target="#b19">Qin et al. (2019)</ref> and <ref type="bibr" target="#b31">Xu et al. (2020)</ref> when they trained BERT-based models using only English training data (en) with intent accuracy scores of 97.5% and 96.08% while we obtain 98.5%. This is the same in our slot-filling experiment in which they report 94.7 F1 points while we obtain 95.6. This difference comes from our results averaging between 30 and 5 runs for intent classification and slot filling, while previous works only performed 5 runs. We also observe that, just like <ref type="bibr" target="#b31">Xu et al. (2020)</ref>, slot filling on Spanish (es) leads to lower results, similar to those obtained in our few-shot setting.</p><p>When transferring from all languages to an unseen one (multilingual zero-shot configuration in both tables 2 and 3) we obtained lower scores than the multilingual configurations. This means the multilingual representation captured in mBERT is efficient enough when data is available in several languages and none are available in the target considered language. But, in both cases, the combina-tion of mBERT+PNN performs better when fewer data is available using the few-shot approach (the multilingual configuration). This means that our approach quickly adapts to the considered target language with only a few examples available and enhances the mBERT multilingual transfer learning capabilities. This is especially true in the case of slot filling with gains in terms of F1-scores ranging from 2 to 5 points.</p><p>Finally, using the mBERT baseline model, transfer learning to French or German has performance scores similar to English while using the Turkish (tr) or Hindi (hi) yielded significant loss. This leads us to the same conclusion as <ref type="bibr" target="#b31">Xu et al. (2020)</ref>: exploiting language interrelationships learnt with transfer learning improve the model performances. This may come from the fact that French, English and German are similar and share some vocabulary while Turkish or Hindi are dissimilar to European languages <ref type="bibr" target="#b12">(Hock and Joseph, 2019)</ref>.</p><p>A detailed inspection of the PNNs results shows that in the target only and in the multilingual configurations, there is an overall and important reduction in recall values, which is balanced by an improvement of the precision values. If we analyze deeper the mislabeled examples we can observe that applying PNNs help to prevent overlapping and annotation mismatch cases that occur in the data.</p><p>We observed that MultiAtis++ corpus seems to be a highly unbalanced labeled dataset with the number of training examples per class varying from 1 to 3300. This impacts the model performance, and it could explain why we observe a lower recall and an improvement in precision using our approach, since it is based on the reduction of the amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we demonstrate the opportunities in leveraging mBERT-based modeling using fewshot learning for both intent classification and slot filling tasks on under-resource languages. We found that our approach model is a highly effective technique for training models for low-resource languages. This illustrates the performance gains that can be achieved by exploiting language interrelationships learnt with transfer learning, a conclusion further emphasised by the fact that multilingual results outperformed other configuration models (target only and specifically multilingual  <ref type="bibr">25 86.99 93.57 91.82 92.38 91.19 90.39 87.49 86.83</ref> Table <ref type="table">3</ref>: Averaged slot F1s obtained with PNNs on 5-way 10-shot and baseline results (highest scores are marked in bold). zero-shot) regardless of the approach. Overall, PNNs models outperform mBERT-based transfer learning approach, enabling us to train competitive NLU systems for under-resources languages with only a fraction of training examples.</p><p>From this work a new challenge naturally comes up and a possible direction is to adapt a few-shot setting to a joint approach of intent detection and slot filling, like in <ref type="bibr" target="#b35">Zhang and Wang (2016)</ref>, <ref type="bibr" target="#b15">Liu and Lane (2016)</ref> and <ref type="bibr" target="#b34">Zhang et al. (2019)</ref>, which demonstrates that performing these two tasks jointly improves the performance of both of them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>properties at testtime to new class labels not seen during training given only a few examples. Details of the MultiATIS++ corpus.</figDesc><table><row><cell>Language</cell><cell cols="2"># utterances train dev test</cell><cell>train</cell><cell># tokens dev</cell><cell>test</cell><cell cols="2"># intents # slot types</cell></row><row><cell>English</cell><cell></cell><cell></cell><cell>50755</cell><cell>5445</cell><cell>9164</cell><cell></cell><cell></cell></row><row><cell>Spanish</cell><cell></cell><cell></cell><cell>55197</cell><cell cols="2">5927 10338</cell><cell></cell><cell></cell></row><row><cell>Portuguese</cell><cell></cell><cell></cell><cell>55052</cell><cell cols="2">5909 10228</cell><cell></cell><cell></cell></row><row><cell>German</cell><cell cols="2">4488 490 893</cell><cell>51111</cell><cell>5517</cell><cell>9383</cell><cell>18</cell><cell>84</cell></row><row><cell>French</cell><cell></cell><cell></cell><cell>55909</cell><cell cols="2">5769 10511</cell><cell></cell><cell></cell></row><row><cell>Chinese</cell><cell></cell><cell></cell><cell>88194</cell><cell cols="2">9652 16710</cell><cell></cell><cell></cell></row><row><cell>Japanese</cell><cell></cell><cell></cell><cell cols="3">133890 14416 25939</cell><cell></cell><cell></cell></row><row><cell>Hindi</cell><cell cols="2">1440 160 893</cell><cell>16422</cell><cell>1753</cell><cell>9755</cell><cell>17</cell><cell>75</cell></row><row><cell>Turkish</cell><cell>578</cell><cell>60 715</cell><cell>6132</cell><cell>686</cell><cell>7683</cell><cell>17</cell><cell>71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Averaged intent accuracies obtained with PNNs on 5-way k-shot classification k ? [1, 10] (best scores are marked in bold) and baseline results.</figDesc><table><row><cell>config.</cell><cell>encoder</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>zh</cell><cell>ja</cell><cell>pt</cell><cell>fr</cell><cell>hi</cell><cell>tr</cell></row><row><cell>target only</cell><cell>mBERT</cell><cell cols="9">98.54 97.31 98.43 97.09 97.20 97.54 98.88 90.93 83.36</cell></row><row><cell></cell><cell>mBERT + PNN (5w1s)</cell><cell cols="9">97.46 95.14 97.18 96.35 95.53 96.80 97.11 84.95 85.17</cell></row><row><cell></cell><cell cols="10">mBERT + PNN (5w10s) 98.77 96.97 98.54 97.0 96.64 97.42 97.98 91.33 89.33</cell></row><row><cell>multilingual</cell><cell>mBERT</cell><cell cols="9">98.42 97.98 98.59 97.65 97.45 98.3 98.46 95.33 93.93</cell></row><row><cell></cell><cell>mBERT + PNN (5w1s)</cell><cell cols="9">95.33 93.71 95.93 95.89 94.42 94.00 94.78 91.4 90.91</cell></row><row><cell></cell><cell cols="10">mBERT + PNN (5w10s) 99.87 98.54 98.60 98.67 98.54 98.32 98.66 95.49 92.61</cell></row><row><cell cols="2">multilingual (zero shot) mBERT</cell><cell cols="9">96.42 97.98 97.54 96.71 97.45 97.42 97.87 94.37 91.61</cell></row><row><cell></cell><cell>mBERT + PNN (5w1s)</cell><cell cols="9">93.73 92.02 93.27 95.62 91.73 93.51 93.28 90.51 89.92</cell></row><row><cell></cell><cell cols="10">mBERT + PNN (5w10s) 96.47 97.87 96.86 97.65 96.64 98.10 97.45 93.17 90.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.52 94.88 92.93 93.13 91.71 92.78 85.12 78.22  mBERT + PNN (5w10s) 95.76 87.40 95.63 93.45 93.93 92.22 93.13 85.70  82.67 multilingual mBERT 96.02 88.03 95.03 93.63 93.01 92.31 91.18 87.39 86.83 mBERT + PNN (5w10s) 98.40 92.09 97.12 95.50 97.24 95.81 96.80 89.59 88.39 multilingual (zero shot) mBERT 94.10 87.14 94.23 92.17 92.61 91.59 90.79 86.14 85.86 mBERT + PNN (5w10s) 93.</figDesc><table><row><cell>config.</cell><cell>encoder</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>zh</cell><cell>ja</cell><cell>pt</cell><cell>fr</cell><cell>hi</cell><cell>tr</cell></row><row><cell>target only</cell><cell>mBERT</cell><cell cols="2">95.64 85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to few-shot learn across diverse natural language classification tasks</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishikesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5108" to="5123" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised meta-learning for few-shot natural language classification tasks</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishikesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297280.3297378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="522" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semisupervised few-shot learning with prototypical networks</title>
		<author>
			<persName><forename type="first">Rinu</forename><surname>Boney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2017. 2017. 2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>Workshop on Meta-Learning</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language un</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating meta-learning algorithms for low-resource natural langu</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1192" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online adaptative zero-shot learning spoken language understanding using wordembedding</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and SP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian prototypical networks for few-shot learning on omniglot</title>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kretov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297280.3297378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC &apos;19</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing, SAC &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural approaches to conversational ai</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210183</idno>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1371" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta-learning for low-resource neural machine translation</title>
		<idno type="DOI">10.18653/v1/D18-1398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The atis spoken language systems pilot corpus</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Charles T Hemphill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Hidden Valley, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06-24">1990. June 24-27, 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language history, language change, and language relationship: An introduction to historical and comparative linguistics</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hock</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Walter de Gruyter GmbH &amp; Co KG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Few-shot sequence labeling with label dependency transfer</title>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08711</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical attention prototypical networks for few-shot text classific</title>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">(almost) zero-shot cross-lingual spoken language understanding</title>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakkani-T?r</forename><surname>Dilek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6034" to="6038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-05318-5_2</idno>
		<title level="m">Meta-Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3293318</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large margin few-shot learning</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1807.02872</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end slot alignment and recognition for cross-lingual NLU</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batool</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5052" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A model of zero-shot learning of spoken language understanding</title>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint slot filling and intent detection via capsule neural networks</title>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5259" to="5267" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A joint model of intent determination and slot filling for spoken language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
