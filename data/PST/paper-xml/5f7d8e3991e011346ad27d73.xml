<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-05">5 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
							<email>james.lichao.sun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Indicates Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Indicates Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wyin@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Tingting</forename><surname>Liang</surname></persName>
							<email>liangtt@hdu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-05">5 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.02394v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup <ref type="bibr" target="#b18">(Zhang et al., 2017)</ref> is a latest data augmentation technique that linearly interpolates input examples and the corresponding labels. It has shown strong effectiveness in image classification by interpolating images at the pixel level. Inspired by this line of research, in this paper, we explore: i) how to apply mixup to natural language processing tasks since text data can hardly be mixed in the raw format; ii) if mixup is still effective in transformer-based learning models, e.g., BERT. To achieve the goal, we incorporate mixup to transformer-based pre-trained architecture, named "mixup-transformer", for a wide range of NLP tasks while keeping the whole end-to-end training system. We evaluate the proposed framework by running extensive experiments on the GLUE benchmark. Furthermore, we also examine the performance of mixup-transformer in low-resource scenarios by reducing the training data with a certain ratio. Our studies show that mixup is a domain-independent data augmentation technique to pre-trained language models, resulting in significant performance improvement for transformer-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has shown outstanding performance in the field of natural language processing (NLP). Recently, transformer-based methods <ref type="bibr" target="#b2">(Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Yang et al., 2019)</ref> have achieved state-ofthe-art performance across a wide variety of NLP tasks 1 . However, these models highly rely on the availability of large amounts of annotated data, which is expensive and labor-intensive. To solve the data scarcity problem, data augmentation is commonly used in NLP tasks. For example, <ref type="bibr" target="#b13">Wei and Zou (2019)</ref> investigated language transformations like insertion, deletion and swap. <ref type="bibr" target="#b8">Malandrakis et al. (2019)</ref> and <ref type="bibr" target="#b16">Yoo et al. (2019)</ref> utilized variational autoencoders (VAEs) <ref type="bibr" target="#b6">(Kingma and Welling, 2013)</ref> to generate more raw inputs. Nevertheless, these methods often rely on some extra knowledge to guarantee the quality of new inputs, and they have to be working in a pipeline. <ref type="bibr" target="#b18">Zhang et al. (2017)</ref> proposed mixup, a domain-independent data augmentation technique that linearly interpolates image inputs on the pixel-based feature space. <ref type="bibr" target="#b4">Guo et al. (2019)</ref> tried mixup in CNN <ref type="bibr" target="#b7">(LeCun et al., 1998)</ref> and LSTM <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) for text applications. Despite effectiveness, they conducted mixup only on the fixed word embedding level like <ref type="bibr" target="#b18">Zhang et al. (2017)</ref> did in image classification. Two questions arise, therefore: (i) how to apply mixup to NLP tasks if text data cannot be mixed in the raw format? Apart from the embedding feature space, what other representation spaces can be constituted and used? ii) whether or not mixup can boost the state-of-the-art further in transformer-based learning models, such as BERT <ref type="bibr" target="#b2">(Devlin et al., 2018</ref>).</p><p>To answer these questions, we stack a mixup layer over the final hidden layer of the pre-trained transformer-based model. The resulting system can be applied to a broad of NLP tasks; in particular, it is still end-to-end trainable. We evaluate our proposed mixup-transformer on the GLUE benchmark, which shows that mixup can consistently improve the performance of each task. Our contributions are summarized as follows:</p><p>• We propose the mixup-transformer that applies mixup into transformer-based pre-trained models. To our best knowledge, this is the first work that explores the effectiveness of mixup in Transformer.</p><p>• In experiments, we demonstrate that mixup-transformer can consistently promote the performance across a wide range of NLP benchmarks, and it is particularly helpful in low-resource scenarios where we reduce the training data from 10% to 90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mixup-Transformer</head><p>In this section, we first introduce the mixup used in previous works. Then, we show how to incorporate the mixup into transformer-based methods and how to do the fine-turning on different text classification tasks. Last, we will discuss the difference between the previous works and our new approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mixup</head><p>Mixup is first proposed for image classification <ref type="bibr" target="#b18">(Zhang et al., 2017)</ref>, which incorporates the prior knowledge that linear interpolations of feature representations should lead to the same interpolations of the associated targets. In mixup, virtual training examples are constructed by two examples drawn at random from the training data:</p><p>x = λx i + (1 − λ)x j , where x i , x j are raw inputs; ŷ = λy i + (1 − λ)y j , where y i , y j are corresponding labels, where λ could be either fixed value in [0, 1] or λ ∼ Beta(α, α), for α ∈ (0, ∞). In previous works, mixup is a static data augmentation approach that improves and robusts the performance in image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixup for Text Classification</head><p>Text classification is the most fundamental problem in the NLP field. Unlike image data, text input consists of discrete units (words) without an inherent ordering or algebraic operations -it could be one sentence, two sentences, a paragraph or a whole document.</p><p>The first step of text classification is to use the word embedding to convert each word of the text into a vector representation. In the traditional approaches, the word embedding method can be bag-of-words, or a fixed word to vector mapping dictionary built by CNN or LSTM. In our approach, instead of using the traditional encoding methods, we use transformer-based pre-trained language models to learn the representations for text data. For downstream tasks, we fine-tune transformer-based models with the mixup data augmentation method. Formally, mixup-transformer constructs virtual hidden representations dynamically durning the training process as follows:</p><formula xml:id="formula_0">x = λ • T (x i ) + (1 − λ) • T (x j</formula><p>), where T (x i ), T (x j ) are output features from transformers; ŷ = λy i + (1 − λ)y j , where y i , y j are corresponding labels, where T (•) represents outputs of the transformer layers as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Note that, the mixup process is trained together with the fine-tuning process in an end-to-end fashion, and the hidden mixup representations are dynamic during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion</head><p>In this section, we highlight two main differences between our approach and previous methods using mixup techniques <ref type="bibr" target="#b18">(Zhang et al., 2017;</ref><ref type="bibr" target="#b4">Guo et al., 2019)</ref> for comparison.</p><p>• Dynamic mixup representation. For each input pair, x i and x j , the previous approaches produce a fixed mixup representation given a fixed λ. However, the mixup hidden representations in our approach are dynamic since they are trained together with the fine-tuning process.</p><p>• Dynamic mixup activation. Since a pre-trained network needs to be fine-tuned for a specific task, we can dynamically activate the mixup during the training. For example, if the training epoch is 3, we can choose to use mixup in any epoch or all epochs. In our experiments, we fine-tune the model without mixup in the first half of epochs for good representations and add mixup in the last half of the epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To show the effectiveness of our proposed mixup-transformer, we conduct extensive experiments by adding the mixup strategy to transformer-based models on seven NLP tasks contained in the GLUE benchmark. Furthermore, we reduce the training data with different ratios (from 10% to 90%) to see how the mixup strategy works with insufficient training data. We report the performance on development sets for all the tasks because the test time is limited by the online GLUE benchmark.</p><p>Datasets. The General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b11">(Wang et al., 2018</ref>) is a collection of diverse natural language understanding tasks. Experiments are conducted on eight tasks in GLUE: CoLA <ref type="bibr" target="#b12">(Warstadt et al., 2019)</ref>, SST-2 <ref type="bibr" target="#b10">(Socher et al., 2013)</ref>, MRPC (Dolan and Brockett, 2005), STS-B <ref type="bibr" target="#b1">(Cer et al., 2017)</ref>, QQP (Z. Chen and Zhao, 2018), MNLI <ref type="bibr" target="#b14">(Williams et al., 2017)</ref>, QNLI <ref type="bibr" target="#b9">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b0">(Bentivogli et al., 2009)</ref>.</p><p>Baselines. Two baselines are conducted in the experiments, including BERT-base and BERT-large <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>. We evaluate the performance of our methods by adding the mixup strategy to these two baselines.</p><p>Implementation details. When fine-tuning BERT with or without the mixup strategy for these NLP tasks, we fix the hyper-parameters as follows: the batch size is 8, the learning rate is 2e-5, the max sequence length is 128, and the number of the training epochs is 3. We test different values of λ, (from 0.1 to 0.9) on the default dataset (CoLA) and find mixup-transformer is insensitive to this hyper-parameter, so we set a fixed value of λ = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on full data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>CoLA SST- Experimental results for eight different NLP tasks are illustrated in Table <ref type="table" target="#tab_0">1</ref>. By adding the proposed mixup technique to BERT-base and BERT-large, the mixup-transformer improves the performance consistently on most of these tasks. The average improvement is around 1% for all the settings. The highest performance gain comes from the RTE task by adding the proposed mixup technique on BERT-base. In this experiment, the accuracy improves from 68.23% to 71.84%, which is an increase of 3.61%. The Matthew's correlation for CoLA increases from 59.71% to 62.39% (improved 2.68%) with mixup on BERT-large. Some experiments also get performance decrease with mixup. For example, adding mixup to BERT-base on STS-B decreases the Spearman correlation from 89.41% to 88.66%. Overall, most of the tasks improved (14 out of 16, while 2 got slightly worse) were found by applying mixup-transformer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on limited data</head><p>As mixup is a technique for augmenting the feature space, it is interesting to see how it works when the training data is insufficient. Therefore, we reduce the training data with a certain ratio (from 10% to 90% with a step 10%) and test the effectiveness of mixup-transformer in low-resource scenarios. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we propose the mixup-transformer that incorporates a data augmentation technique called mixup into transformer-based models for NLP tasks. Unlike using the static mixup in previous works, our approach can dynamically construct new inputs for text classification. Extensive experimental results show that mixup-transformer can be dynamically used with a pre-trained model to achieve better performance on GLUE benchmark. Two future directions are worth considering on text data. First, how to use mixup on other challenging NLP problems, such as zero-shot, few-shot or meta-learning tasks. Second, how to do mixup on document-level text data like paragraphs. Instead of using mixup directly, we may need to extract appropriate information from the data in the training process. Selecting the right information to mix up for text classification would be an exciting and challenging area to be in.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of mixup-transformer. xi, xj are two separate sentences, fed to the same Transformer T . T (xi) is the representation of the input xi, generated by T . x and ŷ are the interpolated representation and label, respectively.</figDesc><graphic url="image-1.png" coords="2,140.03,219.81,317.48,80.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: mixup-transformer with BERT-large runs with reduced training data for four tasks: STS-B, MRPC, RTE and CoLA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>mixup-transformer results on eight NLP tasks. Matthew's Correlations are reported for CoLA, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.</figDesc><table><row><cell></cell><cell></cell><cell cols="7">2 MRPC STS-B QQP MNLI-mm QNLI RTE</cell></row><row><cell></cell><cell cols="5">(Corr) (Acc) (Acc) (Corr) (Acc)</cell><cell>(Acc)</cell><cell cols="2">(Acc) (Acc)</cell></row><row><cell>BERT-base</cell><cell cols="3">57.86 92.20 86.76</cell><cell cols="2">89.41 90.79</cell><cell>84.47</cell><cell cols="2">91.61 68.23</cell></row><row><cell cols="4">BERT-base + mixup 59.58 92.78 88.48</cell><cell cols="2">88.66 90.98</cell><cell>85.12</cell><cell cols="2">91.84 71.84</cell></row><row><cell>Improved</cell><cell>1.72</cell><cell>0.58</cell><cell>1.72</cell><cell>-0.75</cell><cell>0.21</cell><cell>0.65</cell><cell>0.23</cell><cell>3.61</cell></row><row><cell>BERT-large</cell><cell cols="3">59.71 91.97 86.27</cell><cell cols="2">89.21 90.38</cell><cell>85.92</cell><cell cols="2">91.96 69.31</cell></row><row><cell cols="4">BERT-large + mixup 62.39 92.78 87.99</cell><cell cols="2">90.10 90.80</cell><cell>85.91</cell><cell cols="2">92.09 69.68</cell></row><row><cell>Improved</cell><cell>2.68</cell><cell>0.81</cell><cell>1.72</cell><cell>0.89</cell><cell>0.42</cell><cell>-0.01</cell><cell>0.13</cell><cell>0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>mixup-transformer with reduced training data on MRPC. Accuracy scores are used to evaluate the performance.</figDesc><table><row><cell>Training Data</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell cols="2">90% 100%</cell></row><row><cell>BERT-large</cell><cell cols="10">74.51 77.45 77.69 82.11 83.58 84.07 84.07 84.56 86.76 85.53</cell></row><row><cell cols="11">BERT-large + mixup 77.21 79.17 81.13 87.01 85.78 86.27 86.79 86.76 87.25 87.99</cell></row><row><cell>Improved</cell><cell>2.70</cell><cell>1.72</cell><cell>3.44</cell><cell>4.90</cell><cell>2.21</cell><cell>2.21</cell><cell>2.72</cell><cell>2.21</cell><cell>0.49</cell><cell>2.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,</head><label>2</label><figDesc>BERT-large + mixup consistently outperforms BERT-large when we reduce the training data for MRPC, where the highest improvement (4.90%) is achieved when only using 40% of the training data. Using the full training data (100%) gets an increase of 2.46%, which indicates that mixup-transformer works even better with reduced annotations. We also report the experiments of reducing training data on other tasks, including STS-B, RTE and CoLA. As shown in Figure2, the mixup-transformer again consistently improves the performance for all the experiments. The performance gains with less training data (like 10% for STS-B, CoLA, and RTE) are higher than using full training data since data augmentation is more effective when the annotations are insufficient. Therefore, the mixup strategy is highly helpful in most low-resource scenarios.</figDesc><table><row><cell>Performance</cell><cell>0.82 0.84 0.86 0.88 0.90 0.92</cell><cell>STS-B-no-mixup</cell><cell>STS-B-mixup</cell><cell>Performance</cell><cell>0.75 0.80 0.85 0.90</cell><cell>MRPC-no-mixup</cell><cell>MRPC-mixup</cell></row><row><cell></cell><cell>0.80</cell><cell cols="2">10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Training Data</cell><cell></cell><cell>0.70</cell><cell cols="2">Training Data 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) STS-B</cell><cell></cell><cell></cell><cell cols="2">(b) MRPC</cell></row><row><cell></cell><cell>0.75</cell><cell>RTE-no-mixup</cell><cell>RTE-mixup</cell><cell></cell><cell>0.65</cell><cell>CoLA-no-mixup</cell><cell>CoLA-mixup</cell></row><row><cell>Performance</cell><cell>0.55 0.60 0.65 0.70</cell><cell></cell><cell></cell><cell>Performance</cell><cell>0.50 0.55 0.60</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell cols="2">10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Training Data</cell><cell></cell><cell>0.45</cell><cell cols="2">Training Data 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell></row><row><cell></cell><cell></cell><cell cols="2">(c) RTE</cell><cell></cell><cell></cell><cell cols="2">(d) CoLA</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Controlled text generation for data augmentation in intelligent artificial agents</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03487</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data augmentation for spoken language understanding via joint variational generation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youhyun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7402" to="7409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Quora question pairs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
