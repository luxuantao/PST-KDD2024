<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning: A Critical Appraisal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-15">September 15, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Departments of Psychology and Neural Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>gary</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning: A Critical Appraisal</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-15">September 15, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic 2012 (Krizhevsky, Sutskever, &amp; Hinton, 2012)deep net model of Imagenet.</p><p>What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks in the deep learning literature typically consist of a set of input units that stand for things like pixels or words, multiple hidden layers (the more such layers, the deeper a network is said to be) containing hidden units (also known as nodes or neurons), and a set output units, with connections running between those nodes. In a typical application such a network might be trained on a large sets of handwritten digits (these are the inputs, represented as images) and labels (these are the outputs) that identify the categories to which those inputs belong (this image is a 2, that one is a 3, and so forth).</p><p>Over time, an algorithm called back-propagation allows a process called gradient descent to adjust the connections between units using a process, such that any given input tends to produce the corresponding output.</p><p>Collectively, one can think of the relation between inputs and outputs that a neural network learns as a mapping. Neural networks, particularly those with multiple hidden layers (hence the term deep) are remarkably good at learning input-output mappings, Such systems are commonly described as neural networks because the input nodes, hidden nodes, and output nodes can be thought of as loosely analogous to biological neurons, albeit greatly simplified, and the connections between nodes can be thought of as in some way reflecting connections between neurons. A longstanding question, outside the scope of the current paper, concerns the degree to which artificial neural networks are biologically plausible.</p><p>Most deep learning networks make heavy use of a technique called convolution <ref type="bibr" target="#b33">(LeCun, 1989)</ref>, which constrains the neural connections in the network such that they innately capture a property known as translational invariance. This is essentially the idea that an object can slide around an image while maintaining its identity; a circle in the top left can be presumed, even absent direct experience) to be the same as a circle in the bottom right. Deep learning is also known for its ability to self-generate intermediate representations, such as internal units that may respond to things like horizontal lines, or more complex elements of pictorial structure.</p><p>In principle, given infinite data, deep learning systems are powerful enough to represent any finite deterministic "mapping" between any given set of inputs and a set of corresponding outputs, though in practice whether they can learn such a mapping depends on many factors. One common concern is getting caught in local minima, in which a systems gets stuck on a suboptimal solution, with no better solution nearby in the space of solutions being searched. (Experts use a variety of techniques to avoid such problems, to reasonably good effect). In practice, results with large data sets are often quite good, on a wide range of potential mappings.</p><p>In speech recognition, for example, a neural network learns a mapping between a set of speech sounds, and set of labels (such as words or phonemes). In object recognition, a neural network learns a mapping between a set of images and a set of labels (such that, for example, pictures of cars are labeled as cars). In DeepMind's Atari game system <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref>, neural networks learned mappings between pixels and joystick positions.</p><p>Deep learning systems are most often used as classification system in the sense that the mission of a typical network is to decide which of a set of categories (defined by the output units on the neural network) a given input belongs to. With enough imagination, the power of classification is immense; outputs can represent words, places on a Go board, or virtually anything else.</p><p>In a world with infinite data, and infinite computational resources, there might be little need for any other technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Limits on the scope of deep learning</head><p>Deep learning's limitations begin with the contrapositive: we live in a world in which data are never infinite. Instead, systems that rely on deep learning frequently have to generalize beyond the specific data that they have seen, whether to a new pronunciation of a word or to an image that differs from one that the system has seen before, and where data are less than infinite, the ability of formal proofs to guarantee high-quality performance is more limited.</p><p>As discussed later in this article, generalization can be thought of as coming in two flavors, interpolation between known examples, and extrapolation, which requires going beyond a space of known training examples <ref type="bibr" target="#b43">(Marcus, 1998a)</ref>.</p><p>For neural networks to generalize well, there generally must be a large amount of data, and the test data must be similar to the training data, allowing new answers to be interpolated in between old ones. In Krizhevsky et al's paper <ref type="bibr" target="#b27">(Krizhevsky, Sutskever, &amp; Hinton, 2012)</ref>, a nine layer convolutional neural network with 60 million parameters and 650,000 nodes was trained on roughly a million distinct examples drawn from approximately one thousand categories. 6</p><p>This sort of brute force approach worked well in the very finite world of ImageNet, into which all stimuli can be classified into a comparatively small set of categories. It also works well in stable domains like speech recognition in which exemplars are mapped in constant way onto a limited set of speech sound categories, but for many reasons deep learning cannot be considered (as it sometimes is in the popular press) as a general solution to artificial intelligence.</p><p>Here are ten challenges faced by current deep learning systems:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep learning thus far is data hungry</head><p>Human beings can learn abstract relationships in a few trials. If I told you that a schmister was a sister over the age of 10 but under the age of 21, perhaps giving you a single example, you could immediately infer whether you had any schmisters, whether your best friend had a schmister, whether your children or parents had any schmisters, and so forth.</p><p>(Odds are, your parents no longer do, if they ever did, and you could rapidly draw that inference, too.)</p><p>In learning what a schmister is, in this case through explicit definition, you rely not on hundreds or thousands or millions of training examples, but on a capacity to represent abstract relationships between algebra-like variables.</p><p>Humans can learn such abstractions, both through explicit definition and more implicit means <ref type="bibr" target="#b42">(Marcus, 2001)</ref>. Indeed even 7-month old infants can do so, acquiring learned abstract language-like rules from a small number of unlabeled examples, in just two</p><p>Using a common technique known as data augmentation, each example was actually presented along with its label 6 in a many different locations, both in its original form and in mirror reversed form. A second type of data augmentation varied the brightness of the images, yielding still more examples for training, in order to train the network to recognize images with different intensities. Part of the art of machine learning involves knowing what forms of data augmentation will and won't help within a given system.</p><p>minutes <ref type="bibr" target="#b46">(Marcus, Vijayan, Bandi Rao, &amp; Vishton, 1999)</ref> In problems where data are limited, deep learning often is not an ideal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.Deep learning thus far is shallow and has limited capacity for transfer</head><p>Although deep learning is capable of some amazing things, it is important to realize that the word "deep" in deep learning refers to a technical, architectural property (the large number of hidden layers used in a modern neural networks, where there predecessors used only one) rather than a conceptual one (the representations acquired by such networks don't, for example, naturally apply to abstract concepts like "justice", "democracy" or "meddling").</p><p>Even more down-to-earth concepts like "ball" or "opponent" can lie out of reach.</p><p>Consider for example DeepMind's Atari game work <ref type="bibr" target="#b48">(Mnih et al., 2015)</ref> on deep reinforcement learning, which combines deep learning with reinforcement learning (in which a learner tries to maximize reward). Ostensibly, the results are fantastic: the system meets or beats human experts on a large sample of games using a single set of "hyperparameters" that govern properties such as the rate at which a network alters its weights, and no advance knowledge about specific games, or even their rules. But it is easy to wildly overinterpret what the results show. To take one example, according to a widely-circulated video of the system learning to play the brick-breaking Atari game Breakout, "after 240 minutes of training, [the system] realizes that digging a tunnel thought the wall is the most effective technique to beat the game".</p><p>But  <ref type="bibr" target="#b22">(Huang, Papernot, Goodfellow, Duan, &amp; Abbeel, 2017)</ref>.</p><p>Recent experiments by Robin Jia and Percy Liang (2017) make a similar point, in a different domain: language. Various neural networks were trained on a question answering task known as SQuAD (derived from the Stanford Question Answering Database), in which the goal is to highlight the words in a particular passage that correspond to a given question. In one sample, for instance, a trained system correctly, and impressively, identified the quarterback on the winning of Super Bowl XXXIII as John Elway, based on a short paragraph. But Jia and Liang showed the mere insertion of distractor sentences (such as a fictional one about the alleged victory of Google's Jeff</p><p>In the same paper, Vicarious proposed an alternative to deep learning called schema networks <ref type="bibr" target="#b25">(Kansky et al., 2017)</ref> 7 that can handle a number of variations in the Atari game Breakout, albeit apparently without the multi-game generality of DeepMind's Atari system.</p><p>Page ! of ! Dean in another Bowl game ) caused performance to drop precipitously. Across sixteen 8 models, accuracy dropped from a mean of 75% to a mean of 36%.</p><p>As is so often the case, the patterns extracted by deep learning are more superficial than they initially appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.Deep learning thus far has no natural way to deal with hierarchical structure</head><p>To a linguist like Noam Chomsky, the troubles Jia and Liang documented would be unsurprising. Fundamentally, most current deep-learning based language models represent sentences as mere sequences of words, whereas Chomsky has long argued that language has a hierarchical structure, in which larger structures are recursively constructed out of smaller components. (For example, in the sentence the teenager who previously crossed the Atlantic set a record for flying around the world, the main clause is the teenager set a record for flying around the world, while the embedded clause who previously crossed the Atlantic is an embedded clause that specifies which teenager.)</p><p>In the 80's <ref type="bibr" target="#b13">Fodor and Pylyshyn (1988)</ref>expressed similar concerns, with respect to an earlier breed of neural networks. Likewise, in <ref type="bibr" target="#b42">(Marcus, 2001)</ref>, I conjectured that single recurrent neural networks (SRNs; a forerunner to today's more sophisticated deep learning based recurrent neural networks, known as RNNs; <ref type="bibr" target="#b11">Elman, 1990)</ref> would have trouble systematically representing and extending recursive structure to various kinds of unfamiliar sentences (see the cited articles for more specific claims about which types).</p><p>Earlier this year, Brenden Lake and Marco Baroni (2017) tested whether such pessimistic conjectures continued to hold true. As they put it in their title, contemporary neural nets were "Still not systematic after all these years". RNNs could "generalize well when the differences between training and test ... are small [but] when generalization requires systematic compositional skills, RNNs fail spectacularly".</p><p>Similar issues are likely to emerge in other domains, such as planning and motor control, in which complex hierarchical structure is needed, particular when a system is likely to encounter novel situations. One can see indirect evidence for this in the struggles with transfer in Atari games mentioned above, and more generally in the field of robotics, in which systems generally fail to generalize abstract plans well in novel environments.</p><p>Here's the full Super Bowl passage; Jia and Liang's distractor sentence that confused the model is at the end. The core problem, at least at present, is that deep learning learns correlations between sets of features that are themselves "flat" or nonhierachical, as if in a simple, unstructured list, with every feature on equal footing. Hierarchical structure (e.g., syntactic trees that distinguish between main clauses and embedded clauses in a sentence) are not inherently or directly represented in such systems, and as a result deep learning systems are forced to use a variety of proxies that are ultimately inadequate, such as the sequential position of a word presented in a sequences.</p><p>Systems like Word2Vec <ref type="bibr" target="#b47">(Mikolov, Chen, Corrado, &amp; Dean, 2013</ref>) that represent individuals words as vectors have been modestly successful; a number of systems that have used clever tricks try to represent complete sentences in deep-learning compatible vector spaces <ref type="bibr" target="#b64">(Socher, Huval, Manning, &amp; Ng, 2012)</ref>. But, as Lake and Baroni's experiments make clear. recurrent networks continue limited in their capacity to represent and generalize rich structure in a faithful manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.Deep learning thus far has struggled with open-ended inference</head><p>If you can't represent nuance like the difference between "John promised Mary to leave" and "John promised to leave Mary", you can't draw inferences about who is leaving whom, or what is likely to happen next. Current machine reading systems have achieved some degree of success in tasks like SQuAD, in which the answer to a given question is explicitly contained within a text, but far less success in tasks in which inference goes beyond what is explicit in a text, either by combining multiple sentences (so called multi-hop inference) or by combining explicit sentences with background knowledge that is not stated in a specific text selection. Humans, as they read texts, frequently derive wide-ranging inferences that are both novel and only implicitly licensed, as when they, for example, infer the intentions of a character based only on indirect dialog.</p><p>Altough Bowman and colleagues <ref type="bibr" target="#b4">(Bowman, Angeli, Potts, &amp; Manning, 2015;</ref><ref type="bibr" target="#b70">Williams, Nangia, &amp; Bowman, 2017)</ref> have taken some important steps in this direction, there is, at present, no deep learning system that can draw open-ended inferences based on realworld knowledge with anything like human-level accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5.Deep learning thus far is not sufficiently transparent</head><p>The relative opacity of "black box" neural networks has been a major focus of discussion in the last few years <ref type="bibr" target="#b59">(Samek, Wiegand, &amp; Müller, 2017;</ref><ref type="bibr" target="#b58">Ribeiro, Singh, &amp; Guestrin, 2016)</ref>. In their current incarnation, deep learning systems have millions or even billions of parameters, identifiable to their developers not in terms of the sort of human interpretable labels that canonical programmers use ("last_character_typed") but only in terms of their geography within a complex network (e.g., the activity value of the i th node in layer j in network module k). Although some strides have been in visualizing the contributions of individuals nodes in complex networks (Nguyen, Clune, Bengio, Dosovitskiy, &amp; Yosinski, 2016), most observers would acknowledge that neural networks as a whole remain something of a black box.</p><p>How much that matters in the long run remains unclear <ref type="bibr" target="#b36">(Lipton, 2016)</ref>. If systems are robust and self-contained enough it might not matter; if it is important to use them in the context of larger systems, it could be crucial for debuggability.</p><p>The transparency issue, as yet unsolved, is a potential liability when using deep learning for problem domains like financial trades or medical diagnosis, in which human users might like to understand how a given system made a given decision. As Catherine O'Neill (2016) has pointed out, such opacity can also lead to serious issues of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.6.Deep learning thus far has not been well integrated with prior knowledge</head><p>The dominant approach in deep learning is hermeneutic, in the sense of being selfcontained and isolated from other, potentially usefully knowledge. Work in deep learning typically consists of finding a training database, sets of inputs associated with respective outputs, and learn all that is required for the problem by learning the relations between those inputs and outputs, using whatever clever architectural variants one might devise, along with techniques for cleaning and augmenting the data set. With just a handful of exceptions, such as LeCun's convolutional constraint on how neural networks are wired <ref type="bibr" target="#b33">(LeCun, 1989)</ref>, prior knowledge is often deliberately minimized.</p><p>Thus, for example, in a system like <ref type="bibr" target="#b34">Lerer et al's (2016)</ref> efforts to learn about the physics of falling towers, there is no prior knowledge of physics (beyond what is implied in convolution). Newton's laws, for example, are not explicitly encoded; the system instead (to some limited degree) approximates them by learning contingencies from raw, pixel level data. As I note in a forthcoming paper in innate <ref type="bibr">(Marcus, in prep)</ref> researchers in deep learning appear to have a very strong bias against including prior knowledge even when (as in the case of physics) that prior knowledge is well known.</p><p>It also not straightforward in general how to integrate prior knowledge into a deep learning system:, in part because the knowledge represented in deep learning systems pertains mainly to (largely opaque) correlations between features, rather than to abstractions like quantified statements (e.g. all men are mortal), see discussion of universally-quantified one-to-one-mappings in <ref type="bibr" target="#b42">Marcus (2001)</ref>, or generics (violable statements like dogs have four legs or mosquitos carry West Nile virus <ref type="bibr" target="#b15">(Gelman, Leslie, Was, &amp; Koch, 2015)</ref>).</p><p>A related problem stems from a culture in machine learning that emphasizes competition on problems that are inherently self-contained, without little need for broad general knowledge. This tendency is well exemplified by the machine learning contest platform known as Kaggle, in which contestants vie for the best results on a given data set.</p><p>Everything they need for a given problem is neatly packaged, with all the relevant input and outputs files. Great progress has been made in this way; speech recognition and some aspects of image recognition can be largely solved in the Kaggle paradigm. As far as I know, nobody has even tried to tackle this sort of thing with deep learning.</p><p>Such apparently simple problems require humans to integrate knowledge across vastly disparate sources, and as such are a long way from the sweet spot of deep learning-style perceptual classification. Instead, they are perhaps best thought of as a sign that entirely different sorts of tools are needed, along with deep learning, if we are to reach human-level cognitive flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.7.Deep learning thus far cannot inherently distinguish causation from correlation</head><p>If it is a truism that causation does not equal correlation, the distinction between the two is also a serious concern for deep learning. Roughly speaking, deep learning learns complex correlations between input and output features, but with no inherent representation of causality. A deep learning system can easily learn that height and vocabulary are, across the population as a whole, correlated, but less easily represent the way in which that correlation derives from growth and development (kids get bigger as they learn more words, but that doesn't mean that growing tall causes them to learn more words, nor that learning new words causes them to grow). Causality has been central strand in some other approaches to AI <ref type="bibr" target="#b55">(Pearl, 2000)</ref> but, perhaps because deep learning is not geared towards such challenges, relatively little work within the deep learning tradition has tried to address it. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.8.Deep learning presumes a largely stable world, in ways that may be problematic</head><p>The logic of deep learning is such that it is likely to work best in highly stable worlds, like the board game Go, which has unvarying rules, and less well in systems such as politics and economics that are constantly changing. To the extent that deep learning is applied in tasks such as stock prediction, there is a good chance that it will eventually face the fate of Google Flu Trends, which initially did a great job of predicting epidemological data on search trends, only to complete miss things like the peak of the 2013 flu season <ref type="bibr" target="#b31">(Lazer, Kennedy, King, &amp; Vespignani, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Deep learning thus far works well as an approximation, but its answers often cannot be fully trusted</head><p>In part as a consequence of the other issues raised in this section, deep learning systems are quite good at some large fraction of a given domain, yet easily fooled.</p><p>An ever-growing array of papers has shown this vulnerability, from the linguistic examples of Jia and Liang mentioned above to a wide range of demonstrations in the domain of vision, where deep learning systems have mistaken yellow-and-black patterns of stripes for school buses (Nguyen, Yosinski, &amp; Clune, 2014) and sticker-clad parking signs for well-stocked refrigerators <ref type="bibr" target="#b68">(Vinyals, Toshev, Bengio, &amp; Erhan, 2014)</ref> in the context of a captioning system that otherwise seems impressive.</p><p>More recently, there have been real-world stop signs, lightly defaced, that have been mistaken for speed limit signs <ref type="bibr" target="#b12">(Evtimov et al., 2017)</ref> and 3d-printed turtles that have been mistake for rifles <ref type="bibr" target="#b0">(Athalye, Engstrom, Ilyas, &amp; Kwok, 2017)</ref>. A recent news story One example of interesting recent work is (Lopez-Paz, Nishihara, Chintala, Schölkopf, &amp; Bottou, 2017), albeit 9 focused specifically on an rather unusual sense of the term causation as it relates to the presence or absence of objects (e.g., "the presence of cars cause the presence of wheel <ref type="bibr">[s]</ref>). This strikes me as quite different from the sort of causation one finds in the relation between a disease and the symptoms it causes.</p><p>recounts the trouble a British police system has had in distinguishing nudes from sand dunes. 10</p><p>The "spoofability" of deep learning systems was perhaps first noted by <ref type="bibr" target="#b67">Szegedy et al(2013)</ref>. Four years later, despite much active research, no robust solution has been found. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.">Deep learning thus far is difficult to engineer with</head><p>Another fact that follows from all the issues raised above is that is simply hard to do robust engineering with deep learning. As a team of authors at Google put it in 2014, in the title of an important, and as yet unanswered essay (Sculley, Phillips, Ebner, Chaudhary, &amp; Young, 2014), machine learning is "the high-interest credit card of technical debt", meaning that is comparatively easy to make systems that work in some limited set of circumstances (short term gain), but quite difficult to guarantee that they will work in alternative circumstances with novel data that may not resemble previous training data (long term debt, particularly if one system is used as an element in another larger system).</p><p>In an important talk at ICML, Leon <ref type="bibr" target="#b3">Bottou (2015)</ref> compared machine learning to the development of an airplane engine, and noted that while the airplane design relies on building complex systems out of simpler systems for which it was possible to create sound guarantees about performance, machine learning lacks the capacity to produce comparable guarantees. As Google's Peter <ref type="bibr" target="#b51">Norvig (2016)</ref> has noted, machine learning as yet lacks the incrementality, transparency and debuggability of classical programming, trading off a kind of simplicity for deep challenges in achieving robustness.</p><p>Henderson and colleagues have recently extended these points, with a focus on deep reinforcement learning, noting some serious issues in the field related to robustness and replicability <ref type="bibr" target="#b21">(Henderson et al., 2017)</ref>.</p><p>Although there has been some progress in automating the process of developing machine learning systems <ref type="bibr" target="#b72">(Zoph, Vasudevan, Shlens, &amp; Le, 2017)</ref>, there is a long way to go.</p><p>https://gizmodo.com/british-cops-want-to-use-ai-to-spot-porn-but-it-keeps-m-1821384511/amp 10 Deep learning's predecessors were vulnerable to similar problems, as <ref type="bibr" target="#b57">Pinker and Prince (1988)</ref>pointed out, in a 11 discussion of neural networks that produced bizarre past tense forms for a subset of its inputs. The verb to mail, for example, was inflected in the past tense as membled, the verb tour as toureder. Children rarely if ever make mistakes like these.</p><p>Page ! of !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11.">Discussion</head><p>Of course, deep learning, is by itself, just mathematics; none of the problems identified above are because the underlying mathematics of deep learning are somehow flawed. In general, deep learning is a perfectly fine way of optimizing a complex system for representing a mapping between inputs and outputs, given a sufficiently large data set.</p><p>The real problem lies in misunderstanding what deep learning is, and is not, good for. The technique excels at solving closed-end classification problems, in which a wide range of potential signals must be mapped onto a limited number of categories, given that there is enough data available and the test set closely resembles the training set.</p><p>But deviations from these assumptions can cause problems; deep learning is just a statistical technique, and all statistical techniques suffer from deviation from their assumptions.</p><p>Deep learning systems work less well when there are limited amounts of training data available, or when the test set differs importantly from the training set, or when the space of examples is broad and filled with novelty. And some problems cannot, given realworld limitations, be thought of as classification problems at all. Open-ended natural language understanding, for example, should not be thought of as a classifier mapping between a large finite set of sentences and large, finite set of sentences, but rather a mapping between a potentially infinite range of input sentences and an equally vast array of meanings, many never previously encountered. In a problem like that, deep learning becomes a square peg slammed into a round hole, a crude approximation when there must be a solution elsewhere.</p><p>One clear way to get an intuitive sense of why something is amiss to consider a set of experiments I did long ago, in 1997, when I tested some simplified aspects of language development on a class of neural networks that were then popular in cognitive science. The 1997-vintage networks were, to be sure, simpler than current models -they used no more than three layers (inputs nodes connected to hidden nodes connected to outputs node), and lacked Lecun's powerful convolution technique. But they were driven by backpropagation just as today's systems are, and just as beholden to their training data.</p><p>In language, the name of the game is generalization -once I hear a sentence like John pilked a football to Mary, I can infer that is also grammatical to say John pilked Mary the football, and Eliza pilked the ball to Alec; equally if I can infer what the word pilk means, I can infer what the latter sentences would mean, even if I had not hear them before.</p><p>Distilling the broad-ranging problems of language down to a simple example that I believe still has resonance now, I ran a series of experiments in which I trained threelayer perceptrons (fully connected in today's technical parlance, with no convolution) on the identity function, f(x) = x, e.g, f( <ref type="formula">12</ref></p><formula xml:id="formula_0">)=12.</formula><p>Training examples were represented by a set of input nodes (and corresponding output nodes) that represented numbers in terms of binary digits. The number 7 for example, would be represented by turning on the input (and output) nodes representing 4, 2, and 1. As a test of generalization, I trained the network on various sets of even numbers, and tested it all possible inputs, both odd and even.</p><p>Every time I ran the experiment, using a wide variety of parameters, the results were the same: the network would (unless it got stuck in local minimum) correctly apply the identity function to the even numbers that it had seen before (say 2, 4, 8 and 12), and to some other even numbers (say 6 and 14) but fail on all the odds numbers, yielding, for example f(15) = 14.</p><p>In Odd numbers were outside the training space, and the networks could not generalize identity outside that space. Adding more hidden units didn't help, and nor did adding 12 more hidden layers. Simple multilayer perceptrons simply couldn't generalize outside their training space <ref type="bibr" target="#b43">(Marcus, 1998a;</ref><ref type="bibr" target="#b44">Marcus, 1998b;</ref><ref type="bibr" target="#b42">Marcus, 2001)</ref>. (Chollet makes quite similar points in the closing chapters of his his <ref type="bibr" target="#b5">(Chollet, 2017)</ref> text.)</p><p>What we have seen in this paper is that challenges in generalizing beyond a space of training examples persist in current deep learning networks, nearly two decades later.</p><p>Many of the problems reviewed in this paper -the data hungriness, the vulnerability to fooling, the problems in dealing with open-ended inference and transfer -can be seen as extension of this fundamental problem. Contemporary neural networks do well on challenges that remain close to their core training data, but start to break down on cases further out in the periphery.</p><p>Of course, the network had never seen an odd number before, but pretraining the network on odd numbers in a 12 different context didn't help. And of course people, in contrast, readily generalize to novel words immediately upon hearing them. Likewise, the experiments I did with seven-month-olds consisted entirely of novel words.</p><p>The widely-adopted addition of convolution guarantees that one particular class of problems that are akin to my identity problem can be solved: so-called translational invariances, in which an object retains its identity when it is shifted to a location. But the solution is not general, as for example Lake's recent demonstrations show. (Data augmentation offers another way of dealing with deep learning's challenges in extrapolation, by trying to broaden the space of training examples itself, but such techniques are more useful in 2d vision than in language).</p><p>As yet there is no general solution within deep learning to the problem of generalizing outside the training space. And it is for that reason, more than any other, that we need to look to different kinds of solutions if we want to reach artificial general intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Potential risks of excessive hype</head><p>One of the biggest risks in the current overhyping of AI is another AI winter, such as the one that devastated the field in the 1970's, after the Lighthill report <ref type="bibr" target="#b35">(Lighthill, 1973)</ref>, suggested that AI was too brittle, too narrow and too superficial to be used in practice.</p><p>Although there are vastly more practical applications of AI now than there were in the 1970s, hype is still a major concern. When a high-profile figure like Andrew Ng writes in the Harvard Business Review promising a degree of imminent automation that is out of step with reality, there is fresh risk for seriously dashed expectations. Machines cannot in fact do many things that ordinary humans can do in a second, ranging from reliably comprehending the world to understanding sentences. No healthy human being would ever mistake a turtle for a rifle or parking sign for a refrigerator.</p><p>Executives investing massively in AI may turn out to be disappointed, especially given the poor state of the art in natural language understanding. Already, some major projects have been largely abandoned, like Facebook's M project, which was launched in August 2015 with much publicity as a general purpose personal assistant, and then later 13 downgraded to a significantly smaller role, helping users with a vastly small range of well-defined tasks such as calendar entry.</p><p>It is probably fair to say that chatbots in general have not lived up to the hype they received a couple years ago. If, for example, driverless car should also, disappoint, relative to their early hype, by proving unsafe when rolled out at scale, or simply not achieving full autonomy after many promises, the whole field of AI could be in for a sharp downturn, both in popularity and funding. We already may be seeing hints of this, https://www.wired.com/2015/08/how-facebook-m-works/ 13 Page ! of ! as in a just published Wired article that was entitled "After peak hype, self-driving cars 14 enter the trough of disillusionment."</p><p>There are other serious fears, too, and not just of the apocalyptic variety (which for now to still seem to be stuff of science fiction). My own largest fear is that the field of AI could get trapped in a local minimum, dwelling too heavily in the wrong part of intellectual space, focusing too much on the detailed exploration of a particular class of accessible but limited models that are geared around capturing low-hanging fruitpotentially neglecting riskier excursions that might ultimately lead to a more robust path.</p><p>I am reminded of Peter Thiel's famous (if now slightly outdated) damning of an often too-narrowly focused tech industry: "We wanted flying cars, instead we got 140 characters". I still dream of Rosie the Robost, a full-service domestic robot that take of my home; but for now, six decades into the history of AI, our bots do little more than play music, sweep floors, and bid on advertisements.</p><p>If didn't make more progress, it would be a shame. AI comes with risk, but also great potential rewards. AI's greatest contributions to society, I believe, could and should ultimately come in domains like automated scientific discovery, leading among other things towards vastly more sophisticated versions of medicine than are currently possible. But to get there we need to make sure that the field as whole doesn't first get stuck in a local minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">What would be better?</head><p>Despite all of the problems I have sketched, I don't think that we need to abandon deep learning.</p><p>Rather, we need to reconceptualize it: not as a universal solvent, but simply as one tool among many, a power screwdriver in a world in which we also need hammers, wrenches, and pliers, not to mentions chisels and drills, voltmeters, logic probes, and oscilloscopes.</p><p>In perceptual classification, where vast amounts of data are available, deep learning is a valuable tool; in other, richer cognitive domains, it is often far less satisfactory.</p><p>The question is, where else should we look? Here are four possibilities.</p><p>https://www.wired.com/story/self-driving-cars-challenges/ 14 Page ! of !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.Unsupervised learning</head><p>In interviews, deep learning pioneers Geoff Hinton and Yann LeCun have both recently pointed to unsupervised learning as one key way in which to go beyond supervised, datahungry versions of deep learning.</p><p>To be clear, deep learning and unsupervised learning are not in logical opposition. Deep learning has mostly been used in a supervised context with labeled data, but there are ways of using deep learning in an unsupervised fashion. But there is certainly reasons in many domains to move away from the massive demands on data that supervised deep learning typically requires.</p><p>Unsupervised learning, as the term is commonly used, tends to refer to several kinds of systems. One common type of system "clusters" together inputs that share properties, even without having them explicitly labeled. Google's cat detector model <ref type="bibr" target="#b32">(Le et al., 2012)</ref> is perhaps the most publicly prominent example of this sort of approach.</p><p>Another approach, advocated researchers such as Yann LeCun <ref type="bibr" target="#b37">(Luc, Neverova, Couprie, Verbeek, &amp; LeCun, 2017)</ref>, and not mutually exclusive with the first, is to replace labeled data sets with things like movies that change over time. The intuition is that systems trained on videos can use each pair of successive frames as a kind of ersatz teaching signal, in which the goal is to predict the next frame; frame t becomes a predictor for frame t1, without the need for any human labeling.</p><p>My view is that both of these approaches are useful (and so are some others not discussed here), but that neither inherently solve the sorts of problems outlined in section 3. One is still left with data hungry systems that lack explicit variables, and I see no advance there towards open-ended inference, interpretability or debuggability.</p><p>That said, there is a different notion of unsupervised learning, less discussed, which I find deeply interesting: the kind of unsupervised learning that human children do. Children often y set themselves a novel task, like creating a tower of Lego bricks or climbing through a small aperture, as my daughter recently did in climbing through a chair, in the space between the seat and the chair back . Often, this sort of exploratory problem solving involves (or at least appears to involve) a good deal of autonomous goal setting (what should I do?) and high level problem solving (how do I get my arm through the chair, now that the rest of my body has passed through?), as well the integration of abstract knowledge (how bodies work, what sorts of apertures and affordances various objects have, and so forth). If we could build systems that could set their own goals and do reasoning and problem-solving at this more abstract level, major progress might quickly follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.Symbol-manipulation, and the need for hybrid models</head><p>Another place that we should look is towards classic, "symbolic" AI, sometimes referred to as GOFAI (Good Old-Fashioned AI). Symbolic AI takes its name from the idea, central to mathematics, logic, and computer science, that abstractions can be represented by symbols. Equations like f = ma allow us to calculate outputs for a wide range of inputs, irrespective of whether we have seen any particular values before; lines in computer programs do the same thing (if the value of variable x is greater than the value of variable y, perform action a).</p><p>By themselves, symbolic systems have often proven to be brittle, but they were largely developed in era with vastly less data and computational power than we have now. The right move today may be to integrate deep learning, which excels at perceptual classification, with symbolic systems, which excel at inference and abstraction. One might think such a potential merger on analogy to the brain; perceptual input systems, like primary sensory cortex, seem to do something like what deep learning does, but there are other areas, like Broca's area and prefrontal cortex, that seem to operate at much higher level of abstraction. The power and flexibility of the brain comes in part from its capacity to dynamically integrate many different computations in real-time. The process of scene perception, for instance, seamlessly integrates direct sensory information with complex abstractions about objects and their properties, lighting sources, and so forth.</p><p>Some tentative steps towards integration already exist, including neurosymbolic modeling <ref type="bibr" target="#b1">(Besold et al., 2017)</ref> and recent trend towards systems such as differentiable neural computers <ref type="bibr" target="#b20">(Graves et al., 2016)</ref>, programming with differentiable interpreters <ref type="bibr" target="#b2">(Bošnjak, Rocktäschel, Naradowsky, &amp; Riedel, 2016)</ref>, and neural programming with discrete operations <ref type="bibr" target="#b49">(Neelakantan, Le, Abadi, McCallum, &amp; Amodei, 2016)</ref>. While none of this work has yet fully scaled towards anything like full-service artificial general intelligence, I have long argued <ref type="bibr" target="#b42">(Marcus, 2001</ref>) that more on integrating microprocessorlike operations into neural networks could be extremely valuable.</p><p>To the extent that the brain might be seen as consisting of "a broad array of reusable computational primitives-elementary units of processing akin to sets of basic instructions in a microprocessor-perhaps wired together in parallel, as in the reconfigurable integrated circuit type known as the field-programmable gate array", as I have argued elsewhere <ref type="bibr" target="#b39">(Marcus, Marblestone, &amp; Dean, 2014)</ref>, steps towards enriching the instruction set out of which our computational systems are built can only be a good thing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.More insight from cognitive and developmental psychology</head><p>Another potential valuable place to look is human cognition <ref type="bibr" target="#b7">(Davis &amp; Marcus, 2015;</ref><ref type="bibr" target="#b29">Lake et al., 2016;</ref><ref type="bibr" target="#b42">Marcus, 2001;</ref><ref type="bibr" target="#b57">Pinker &amp; Prince, 1988)</ref>. There is no need for machines to literally replicate the human mind, which is, after all, deeply error prone, and far from perfect. But there remain many areas, from natural language understanding to commonsense reasoning, in which humans still retain a clear advantage; learning the mechanisms underlying those human strengths could lead to advances in AI, even the goal is not, and should not be, an exact replica of human brain.</p><p>For many people, learning from humans means neuroscience; in my view, that may be premature. We don't yet know enough about neuroscience to literally reverse engineer the brain, per se, and may not for several decades, possibly until AI itself gets better. AI can help us to decipher the brain, rather than the other way around.</p><p>Either way, in the meantime, it should certainly be possible to use techniques and insights drawn from cognitive and developmental and psychology, now, in order to build more robust and comprehensive artificial intelligence, building models that are motivated not just by mathematics but also by clues from the strengths of human psychology.</p><p>A good starting point might be to first to try understand the innate machinery in humans minds, as a source of hypotheses into mechanisms that might be valuable in developing artificial intelligences; in companion article to this one (Marcus, in prep) I summarize a number of possibilities, some drawn from my own earlier work <ref type="bibr" target="#b42">(Marcus, 2001)</ref> and others from Elizabeth Spelke's <ref type="bibr" target="#b65">(Spelke &amp; Kinzler, 2007)</ref>. Those drawn from my own work focus on how information might be represented and manipulated, such as by symbolic mechanisms for representing variables and distinctions between kinds and individuals from a class; those drawn from Spelke focus on how infants might represent notions such as space, time, and object.</p><p>A second focal point might be on common sense knowledge, both in how it develops (some might be part of our innate endowment, much of it is learned), how it is represented, and how it is integrated on line in the process of our interactions with the real world <ref type="bibr" target="#b7">(Davis &amp; Marcus, 2015)</ref>. Recent work by <ref type="bibr" target="#b34">Lerer et al (2016)</ref>, <ref type="bibr">Watters and colleagues (2017)</ref>, Tenenbaum and colleagues <ref type="bibr" target="#b71">(Wu, Lu, Kohli, Freeman, &amp; Tenenbaum, 2017)</ref> and Davis and myself <ref type="bibr" target="#b9">(Davis, Marcus, &amp; Frazier-Logue, 2017)</ref> suggest some competing approaches to how to think about this, within the domain of everyday physical reasoning.</p><p>A third focus might be on human understanding of narrative, a notion long ago suggested by Roger <ref type="bibr" target="#b60">Schank and Abelson (1977)</ref> and due for a refresh <ref type="bibr" target="#b40">(Marcus, 2014;</ref><ref type="bibr" target="#b26">Kočiský et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.Bolder challenges</head><p>Whether deep learning persists in current form, morphs into something new, or gets replaced altogether, one might consider a variety of challenge problems that push systems to move beyond what can be learned in supervised learning paradigms with large datasets. Drawing in part of from a recent special issue of AI Magazine devoted to moving beyond the Turing Test that I edited with Francesca Rossi, Manuelo Veloso <ref type="bibr" target="#b38">(Marcus, Rossi, Veloso -AI Magazine, &amp; 2016</ref><ref type="bibr">, 2016)</ref>, here are a few suggestions:</p><p>• A comprehension challenge <ref type="bibr" target="#b54">(Paritosh &amp; Marcus, 2016;</ref><ref type="bibr" target="#b26">Kočiský et al., 2017)</ref>] which would require a system to watch an arbitrary video (or read a text, or listen to a podcast) and answer open-ended questions about what is contained therein. (Who is the protagonist? What is their motivation? What will happen if the antagonist succeeds in her mission?) No specific supervised training set can cover all the possible contingencies; infererence and real-world knowledge integration are necessities. • Scientific reasoning and understanding, as in the Allen AI institute's 8th grade science challenge <ref type="bibr" target="#b62">(Schoenick, Clark, Tafjord, P, &amp; Etzioni, 2017;</ref><ref type="bibr" target="#b8">Davis, 2016)</ref>. While the answers to many basic science questions can simply be retrieved from web searches, others require inference beyond what is explicitly stated, and the integration of general knowledge. • General game playing <ref type="bibr" target="#b16">(Genesereth, Love, &amp; Pell, 2005)</ref>, with transfer between games <ref type="bibr" target="#b25">(Kansky et al., 2017)</ref>, such that, for example, learning about one first-person shooter enhances performance on another with entirely different images, equipment and so forth. (A system that can learn many games, separately, without transfer between them, such as DeepMind's Atari game system, would not qualify; the point is to acquire cumulative, transferrable knowledge). • A physically embodied test an AI-driven robot that could build things <ref type="bibr" target="#b53">(Ortiz Jr, 2016)</ref>, ranging from tents to IKEA shelves, based on instructions and real-world physical interactions with the objects parts, rather than vast amounts trial-and-error.</p><p>No one challenge is likely to be sufficient. Natural intelligence is multi-dimensional <ref type="bibr" target="#b14">(Gardner, 2011)</ref>, and given the complexity of the world, generalized artificial intelligence will necessarily be multi-dimensional as well.</p><p>By pushing beyond perceptual classification and into a broader integration of inference and knowledge, artificial intelligence will advance, greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>As a measure of progress, it is worth considering a somewhat pessimistic piece I wrote for The New Yorker five years ago , conjecturing that "deep learning is only part of the 15 larger challenge of building intelligent machines" because "such techniques lack ways of representing causal relationships (such as between diseases and their symptoms), and are likely to face challenges in acquiring abstract ideas like "sibling" or "identical to." They have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used."</p><p>As we have seen, many of these concerns remain valid, despite major advances in specific domains like speech recognition, machine translation, and board games, and despite equally impressive advances in infrastructure and the amount of data and compute available.</p><p>Intriguingly, in the last year, a growing array of other scholars, coming from an impressive range of perspectives, have begun to emphasize similar limits. A partial list includes Brenden Lake and Marco Baroni (2017), François <ref type="bibr" target="#b5">Chollet (2017)</ref>, Robin Jia and Percy Liang (2017), Dileep George and others at Vicarious <ref type="bibr" target="#b25">(Kansky et al., 2017)</ref> and Pieter Abbeel and colleagues at Berkeley <ref type="bibr" target="#b66">(Stoica et al., 2017)</ref>.</p><p>Perhaps most notably of all, Geoff Hinton has been courageous enough to reconsider has own beliefs, revealing in an August interview with the news site Axios that he is 16 "deeply suspicious" of back-propagation, a key enabler of deep learning that he helped pioneer, because of his concern about its dependence on labeled data sets.</p><p>Instead, he suggested (in Axios' paraphrase) that "entirely new methods will probably have to be invented." </p><formula xml:id="formula_1">I</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>general, the neural nets I tested could learn their training examples, and interpolate to a set of test examples that were in a cloud of points around those examples in ndimensional space (which I dubbed the training space), but they could not extrapolate beyond that training space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Subsequent work byGervain  and colleagues (2012)  suggests that newborns are capable of similar computations.Deep learning currently lacks a mechanism for learning abstractions through explicit, verbal definition, and works best when there are thousands, millions or even billions of training examples, as in DeepMind's work on board games and Atari. As Brenden Lake and his colleagues have recently emphasized in a series of papers, humans are far more efficient in learning complex rules than deep learning systems are<ref type="bibr" target="#b28">(Lake, Salakhutdinov, &amp; Tenenbaum, 2015;</ref><ref type="bibr" target="#b29">Lake, Ullman, Tenenbaum, &amp; Gershman, 2016)</ref>. (See also related work by<ref type="bibr" target="#b17">George et al (2017)</ref>, and my own work with Steven Pinker on children's overregularization errors in comparison to neural networks<ref type="bibr" target="#b45">(Marcus et al., 1992)</ref>.)</figDesc><table /><note>Geoff Hinton has also worried about deep learning's reliance on large numbers of labeled examples, and expressed this concern in his recent work on capsule networks with his coauthors<ref type="bibr" target="#b59">(Sabour et al., 2017)</ref> noting that convolutional neural networks (the most common deep learning architecture) may face "exponential inefficiencies that may lead to their demise. A good candidate is the difficulty that convolutional nets have in generalizing to novel viewpoints [ie perspectives on object in visual recognition tasks]. The ability to deal with translation[al invariance] is built in, but for the other ... [common type of] transformation we have to chose between replicating feature detectors on a grid that grows exponentially ... or increasing the size of the labelled training set in a similarly exponential way."</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The trouble, however, is that life is not a Kaggle competition; children don't get all the data they need neatly packaged in a single directory. Real-world learning offers data much more sporadically, and problems aren't so neatly encapsulated. Deep learning works great on problems like speech recognition in which there are lots of labeled examples, but scarcely any even knows how to apply it to more open-ended problems.</figDesc><table /><note>What's the best way to fix a bicycle that has a rope caught in its spokes? Should I major in math or neuroscience? No training set will tell us that.Problems that have less to do with categorization and more to do with commonsense reasoning essentially lie outside the scope of what deep learning is appropriate for, and so far as I can tell, deep learning has little to offer such problems. In a recent review of commonsense reasoning, ErnieDavis and I (2015)  began with a set of easily-drawn inferences that people can readily answer without anything like direct training, such as Who is taller, Prince William or his baby son Prince George? Can you make a salad out of a polyester shirt? If you stick a pin into a carrot, does it make a hole in the carrot or in the pin?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>share Hinton's excitement in seeing what comes next.</figDesc><table /><note>https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence 15 https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html 16</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Page ! of !</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>marcus at nyu.edu. I thank Christina 1 Chen, François Chollet, Ernie Davis, Zack Lipton, Stefano Pacifico, Suchi Saria, and Athena Vouloumanos for sharp-eyed comments, all generously supplied on short notice during the holidays at the close of 2017.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<idno>arXiv, cs.CV</idno>
		<title level="m">Synthesizing Robust Adversarial Examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<idno>arXiv, cs.AI</idno>
		<title level="m">Neural-Symbolic Learning and Reasoning: A Survey and Interpretation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Programming with a Differentiable Forth Interpreter</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two big challenges in machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from 32nd International Conference on Machine Learning</title>
				<meeting>from 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>arXiv, cs.CL</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Learning with Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Manning Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Neural networks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Commonsense reasoning and commonsense knowledge in artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="92" to="103" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How to Write Science Questions that Are Easy for People and Hard for Computers. AI magazine</title>
		<author>
			<persName><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Commonsense reasoning about containers using radically incomplete information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frazier-Logue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">248</biblScope>
			<biblScope unit="page" from="46" to="84" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database. Proceedings from Computer Vision and Pattern Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li -Computer</forename><surname>Vision</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<idno>arXiv, cs.CR</idno>
		<title level="m">Robust Physical-World Attacks on Deep Learning Models</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: a critical analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Frames of mind: The theory of multiple intelligences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Basic books</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Children&apos;s interpretations of general quantifiers, specific quantifiers, and generics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Was</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="461" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pell</surname></persName>
		</author>
		<title level="m">General game playing: Overview of the AAAI competition. AI magazine</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lehrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6368</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Binding at birth: the newborn brain detects identity relations and sequential position in speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gervain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="564" to="574" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno>arXiv, cs.LG</idno>
		<title level="m">Deep Reinforcement Learning that Matters</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>arXiv, cs.LG</idno>
		<title level="m">Adversarial Attacks on Neural Network Policies</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow (1st pbk</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Farrar, Straus and Giroux</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eldawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lou</surname></persName>
		</author>
		<idno>arXIv, cs.AI</idno>
		<title level="m">Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<idno>arXiv, cs.CL</idno>
		<title level="m">The NarrativeQA Reading Comprehension Challenge</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>In (pp.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building Machines That Learn and Think Like People</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Brain Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="101" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Big data. The parable of Google Flu: traps in big data analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="issue">6176</biblScope>
			<biblScope unit="page" from="1203" to="1205" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from International Conference on Machine Learning</title>
				<meeting>from International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generalization and network design strategies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CRG-TR-89-4</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning Physical Intuition of Block Towers by Example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>arXiv, cs.AI</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: a paper symposium</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lighthill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
	<note>Artificial Intelligence: A General Survey</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C ;</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Lg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>from of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
	<note>The Mythos of Model Interpretability. arXiv, cs</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Predicting Deeper into the Future of Semantic Segmentation. International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Page ! of</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Beyond the Turing Test</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso -Ai Magazine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
	<note>Whole issue</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The atoms of neural computation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marblestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">6209</biblScope>
			<biblScope unit="page" from="551" to="552" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<title level="m">Innateness, AlphaZero, and Artificial Intelligence</title>
				<meeting><address><addrLine>Marcus, G.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>What Comes After the Turing Test? The New Yorker</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Is &quot;Deep Learning&quot; a Revolution in Artificial Intelligence? The New Yorker</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2012. 2008</date>
			<publisher>Houghton</publisher>
			<pubPlace>Boston; Mifflin</pubPlace>
		</imprint>
	</monogr>
	<note>Kluge : the haphazard construction of the human mind</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Algebraic Mind: Integrating Connectionism and cognitive science</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F G F</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking eliminative connectionism</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Psychol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="282" />
			<date type="published" when="1998">1998a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Can connectionism save constructivism?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="182" />
			<date type="published" when="1998">1998b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Overregularization in language acquisition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hollander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monogr Soc Res Child Dev</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="182" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rule learning by sevenmonth-old infants</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandi Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Vishton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="issue">5398</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Learning a Natural Language Interface with Neural Programmer</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What Artificial Intelligence Can and Can&apos;t Do Right Now</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>arXiv, cs.CV</idno>
	</analytic>
	<monogr>
		<title level="m">Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space. arXiv, cs</title>
				<editor>
			<persName><surname>Cv</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2016. 2016. 2014</date>
		</imprint>
	</monogr>
	<note>Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">State-of-the-Art AI: Building Tomorrow&apos;s Intelligent Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from EmTech Digital</title>
				<meeting>from EmTech Digital<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Weapons of math destruction : how big data increases inequality and threatens democracy</title>
		<author>
			<persName><forename type="first">C</forename><surname>O'neil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Why we need a physically embodied Turing test and what it might look like. AI magazine</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Ortiz</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Toward a comprehension challenge, using crowdsourcing as a tool</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Causality : models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Cambridge</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On language and connectionism: analysis of a parallel distributed processing model of language acquisition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="193" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Why Should I Trust You?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>&quot;: Explaining the Predictions of Any Classifier. arXiv, cs.LG</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dynamic Routing Between Capsules. arXiv, cs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E ;</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Cv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno>arXiv, cs.AI</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Abelson</surname></persName>
		</author>
		<title level="m">Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures</title>
				<editor>
			<persName><forename type="first">L</forename><surname>Erlbaum</surname></persName>
		</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Moving beyond the Turing Test with the Allen AI Science Challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="60" to="64" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Machine learning: The high-interest credit card of technical debt</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from SE4ML: Software Engineering for Machine Learning</title>
				<meeting>from SE4ML: Software Engineering for Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>Workshop</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
				<meeting>from of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Spelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Kinzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Core knowledge</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<idno>arXiv, cs.AI</idno>
		<title level="m">A Berkeley View of Systems Challenges for AI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>arXiv, cs.CV</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno>arXiv, cs.CV</idno>
		<title level="m">Show and Tell: A Neural Image Caption Generator</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Visual Interaction Networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>arXiv, cs.CL</idno>
		<title level="m">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning to See Physics via Visual De-animation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings from Advances in Neural Information Processing Systems</title>
				<meeting>from Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Learning Transferable Architectures for Scalable Image Recognition. arXiv, cs.CV. Page ! of !</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
