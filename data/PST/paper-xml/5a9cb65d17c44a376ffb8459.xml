<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shangxing</forename><surname>Wang</surname></persName>
							<email>shangxiw@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanpeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Gomes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bhaskar</forename><surname>Krishnamachari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6DC5AE53E9BEF20B54EDA9BE7FB4AFDC</idno>
					<idno type="DOI">10.1109/TCCN.2018.2809722</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multichannel Access</term>
					<term>Cognitive Sensing</term>
					<term>POMDP</term>
					<term>DQN</term>
					<term>Reinforcement Learning</term>
					<term>Online Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a dynamic multichannel access problem, where multiple correlated channels follow an unknown joint Markov model and users select the channel to transmit data. The objective is to find a policy that maximizes the expected long-term number of successful transmissions. The problem is formulated as a partially observable Markov decision process (POMDP) with unknown system dynamics. To overcome the challenges of unknown dynamics and prohibitive computation, we apply the concept of reinforcement learning and implement a Deep Q-Network (DQN). We first study the optimal policy for fixed-pattern channel switching with known system dynamics and show through simulations that DQN can achieve the same optimal performance without knowing the system statistics. We then compare the performance of DQN with a Myopic policy and a Whittle Index-based heuristic through both more general simulations as well as real data trace and show that DQN achieves nearoptimal performance in more complex situations. Finally, we propose an adaptive DQN approach with the capability to adapt its learning in time-varying scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P RIOR work [2], [3] has shown that dynamic spectrum access is one of the keys to improving the spectrum utilization in wireless networks and meeting the need for more capacity. In the context of cognitive radio research, a standard assumption has been that secondary users may search and use idle channels that are not being used by their primary users (PU). While prior work has generally assumed a simple independent-channel (or PU activity) model, in practice external interference can cause the channels in wireless networks to be highly correlated, and the design of new algorithms and schemes in dynamic multichannel access is required to tackle this challenge.</p><p>We consider in this work a multichannel access problem with N correlated channels. Each channel has two possible states: good or bad, and their joint distribution follow a 2 N -states Markovian model. There is a single user (wireless node) that selects one channel at each time slot to transmit a packet. If the selected channel is in the good state, the transmission is successful; otherwise, there is a transmission failure. The goal is to obtain as many successful transmissions as possible over time. As the user is only able to sense his selected channel at each time slot, there is no full observation of the system available. In general, the problem can be formulated as a partially observable Markov decision process (POMDP), which is PSPACE-hard and the best known solution for finding the exact solution requires an exponential computation complexity <ref type="bibr" target="#b3">[4]</ref>. Even worse, the parameters of the joint Markovian model might not be known a-priori.</p><p>We investigate the use of Deep Reinforcement Learning, in particular, Deep Q learning, from the field of machine learning as a way to enable learning in an unknown environment as well as overcome the prohibitive computational requirements. By integrating deep learning with Q learning, Deep Q learning or Deep Q Network (DQN) <ref type="bibr" target="#b4">[5]</ref> can use a deep neural network with states as input and estimated Q values as output to efficiently learn policies for high-dimensional, large state-space problems. We implement a DQN that can find a channel access policy through online learning. This DQN approach is able to deal with large systems, and find a good or even optimal policy directly from historical observations without any requirement to 2332-7731 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 2</p><p>know the system dynamics a-priori.</p><p>The rest of the paper is organized as follows. Section II shows the related work. Section III formulates the dynamic multichannel access problem when channels are potentially correlated. Section IV presents a Myopic and a Whittle Index-based heuristic to solve this problem. Section V presents the DQN framework. Section VI presents an optimal policy study on known fixed-pattern channel switching, and Section VII shows through simulations that DQN can achieve optimal performance. The evaluation results considering both synthetic and testbed-based datasets are shown in section VIII. Section IX introduces an adaptive DQN approach and, finally, Section X concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The dynamic multichannel access problem has been widely studied. But unlike many decision making problems, such as vertical handoff <ref type="bibr" target="#b5">[6]</ref> and power allocation <ref type="bibr" target="#b6">[7]</ref>, that can be modeled as MDP, the dynamic multichannel problem is modeled as a POMDP, as channels are generally (two-state) Markov chains and a user has only partial observations. Finding an optimal channel access policy requires exponential time and space complexities. When channels are independent and identically distributed (i.i.d.), a Myopic policy has been shown to be optimal under certain conditions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. But the Myopic policy does not have any performance guarantee when channels are correlated or follow different distributions.</p><p>When channels are independent but may follow different Markov chains, the dynamic multichannel access problem can be modeled as a Restless Multiarmed bandit problem (RMAB). A Whittle Index policy is introduced in <ref type="bibr" target="#b9">[10]</ref> and shares the same simple semi-universal structure and optimality result as the Myopic policy. Numerical results are also provided showing that the Whittle Index policy can achieve near-optimal performance when channels are nonidentical. But the Whittle Index approach is not applicable when channels are correlated, which is the focus of our work.</p><p>In recent years, some works began to focus on the more practical and complex problem where both the system statistics is unknown and the channels are correlated. Q-learning is widely used as it is a model-free method that can learn the policy directly via online learning. The authors in <ref type="bibr" target="#b10">[11]</ref> apply Qlearning to design channel sensing sequences, while in <ref type="bibr" target="#b11">[12]</ref> it is shown that Q-learning can also take care of imperfect sensing. All these works assume the system state is fully observable and formulate the problem as an MDP, which significantly reduces the state space. On the contrary, our problem falls into the framework of POMDP because of the limit of the partial observation, and its large state space makes it impossible to maintain a simple look-up Q table to update Q values. New methods able to find approximations of Q-values are required to solve the large space challenge.</p><p>Reinforcement learning, including Q learning, has been integrated with advanced machine learning techniques to tackle difficult high-dimensional problems <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In 2013, Google DeepMind used a deep neural network, called DQN, to approximate Q values in Q learning that overcomes the limitation of the traditional look-up table approach, and provide an end-to-end approach to allow an agent to learn a policy from its observations. To the best of our knowledge, ours is the first study and implementation of DQN in the field of dynamic multi-channel access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>Consider a dynamic multichannel access problem where there is a single user dynamically choosing one out of N channels. At the beginning of each time slot, a user selects one channel to sense and transmit a packet. If the channel quality is good, the transmission succeeds and the user receives a positive reward (+1), else the user transmission fails and there is a negative reward (-1). The objective is to design a policy that maximizes the expected long-term reward.</p><p>To model correlation across channels, the whole system is described as a 2 N -state Markov chain. Formally, let the state space of the Markov chain be S = {s 1 , ..., s 2 N }. Each state s i (i ∈ {1, ..., 2 N }) is a length-N vector [s i1 , ..., s iN ], where s ik is the binary representation of the state of channel k: good (1) or bad (0). The transition of the Markov chain is denoted as P. Since the user can only sense one channel at the beginning of each time slot, the full state of all channels is not observable. However, the user can infer a distribution over the system state according to his sensing decisions and observations. Thus, the dynamic multichannel access problem falls into the general framework of POMDP. Let Ω(t) = [ω s 1 (t), ..., ω s 2 N (t)] represent the belief vector maintained by the user, where ω s i (t) is the conditional probability that the system is in state s i given all previous decisions and observations. Given the sensing action a(t) ∈ {1, ..., N } representing which channel to sense at time slot t, the user can observe the state of channel a(t), denoted as o(t) ∈ {0, 1}. Then, based on this observation, the user can update the belief vector at time slot t, denoted as Ω(t) = [ ωs 1 (t), ..., ωs 2 N (t)]. The belief of each possible state ωs i (t) is updated as follows:</p><formula xml:id="formula_0">ωs i (t) =          ω s i (t)1(s ik (t)=1) 2 N i=1 ω s i (t)1(s ik (t)=1) a(t) = k, o(t) = 1 ω s i (t)1(s ik (t)=0) 2 N i=1 ω s i (t)1(s ik (t)=0) a(t) = k, o(t) = 0</formula><p>where 1(.) is the indicator function.</p><p>Combining the newly updated belief vector Ω(t) for time slot t with the system transition matrix P, the belief vector for time slot t + 1 can be updated as Ω(t + 1) = Ω(t)P.</p><p>A sensing policy π : Ω(t) → a(t) is a function that maps the belief vector Ω(t) to a sensing action a(t) at each time slot t. Given a policy π, the longterm reward considered in this paper is the expected accumulated discounted reward over infinite time horizon, defined as E π [ ∞ t=1 γ t-1 R π(Ω(t)) (t)|Ω(1)], where 0 ≤ γ &lt; 1 is a discounted factor, π(Ω(t)) is the channel sensing action at time t given belief vector Ω(t), and R π(Ω(t)) (t) is the corresponding reward. Our objective is to find a sensing policy π * that maximizes the expected accumulated discounted reward over infinite time</p><formula xml:id="formula_1">π * = arg max π E π [ ∞ t=1 γ t-1 R π(Ω(t)) (t)|Ω(1)]</formula><p>As the dynamic multichannel access problem is a POMDP, the optimal sensing policy π * can be found by considering its belief space and solving an augmented MDP instead, for example, via value iteration, however the dimension of the belief vector is exponentially large in the number of channels. Even worse, the infinite size of the continuous belief space and the impact of the current action on the future reward makes POMDP PSPACE-hard, which is even less likely to be solved in polynomial time than NP-hard problems <ref type="bibr" target="#b3">[4]</ref>. To exemplify the time complexity of solving such POMDP problem, we simulate the multichannel access problem with known system dynamics and use a POMDP solver called SolvePOMDP [16] to find its optimal solution. In Figure <ref type="figure" target="#fig_0">1</ref>, we show the run-time as we increase the number of channels in the system. When the number of channels is higher than 5, we find that the POMDP solver can not converge after a long interval, and it gets terminated when the run-time exceeds the time limit. All these factors make it impossible to find the optimal solution to the problem in general, and many existing works <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[17]</ref>- <ref type="bibr" target="#b19">[21]</ref> attempt to address this challenge of prohibitive computation by considering either simpler models or approximation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MYOPIC POLICY AND WHITTLE INDEX</head><p>In the domain of dynamic multichannel access, there are many existing works on finding the optimal/near-optimal policy with low computation cost when the channels are independent and system statistics (P) is known. The Myopic policy and the Whittle Index policy are two effective and easy-toimplement approaches for this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Myopic Policy</head><p>A Myopic policy only focuses on the immediate reward obtained from an action and ignores its effects in the future. Thus the user always tries to select a channel which gives the maximized expected immediate reward.</p><p>The Myopic policy is not optimal in general. Researchers in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have studied its optimality when N channels are independent and statistically identical Gilbert-Elliot channels that follow the same 2-state Markov chain with the transition matrix as optimal for any number of channels when the channel state transitions are positively correlated, i.e., p 11 ≥ p 01 . The same optimal result still holds for two or three channels when channel state transitions are negatively correlated, i.e., p 11 &lt; p 01 . In addition, the Myopic policy has a simple robust structure that follows a round-robin channel selection procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Whittle Index Based Heuristic Policy</head><p>When channels are independent, the dynamic multichannel access problem can also be considered as a restless multi-armed bandit problem (RMAB) if each channel is treated as an arm. An index policy assigns a value to each arm based on its current state and chooses the arm with the highest index at each time slot. The index policy is not guaranteed to be optimal in general.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, the Whittle Index is obtained in closedform for the case when P is known and all channels are independent but may follow different 2-state Markov chain models. In the special case when all channels are identical, it is shown to coincide with the above-described Myopic policy.</p><p>When channels are correlated, the Whittle Index cannot be defined and thus the Whittle Index policy cannot be directly applied to our problem. Nevertheless, as a baseline in our evaluations, to leverage its simplicity, we propose an heuristic that ignores the correlations among channels and uses the joint transition matrix P and Bayes' Rule to compute the 2-state Markov chain for each individual channel. Once each channel model is found, we can apply the Whittle Index policy accordingly.</p><p>The Myopic policy and the Whittle Index policy are easy to implement in practice and have polynomial run-time, however they achieve optimality only under certain conditions when channels are independent. Moreover, both policies require prior knowledge of the system dynamics, which is hard to obtain beforehand. However, to the best of our knowledge, there is no easy-to-implement policy applicable to the general case where channels are correlated and the system dynamics are unknown -thus a new approach is needed.</p><p>V. DEEP REINFORCEMENT LEARNING FRAMEWORK When channels are correlated and system dynamics are unknown, there are two main approaches to tackle the dynamic multichannel access problem: (i) Model-based approach: first estimate the system model from observations and then apply dynamic programming or a computationally efficient heuristic policy such as Myopic/Whittle Index policies; (ii) Model-free approach: learn the policy directly through interactions with the system without estimating the system model. The model-based approach is less favored since the user's limited observation capability may result in a bad system model estimation. Even worse, even if the system dynamics is well estimated, solving a POMDP in a large state space is always a bottleneck as the dynamic programming method has exponential time complexity (as explained in Section III) and the heuristic approaches do not have any performance guarantee. All these challenges motivate us to follow the model-free approach, which, by incorporating the idea of Reinforcement Learning, can learn directly from observations without the necessity of finding an estimated system model and can be easily extended to very large and complicated systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Q-Learning</head><p>We focus on the reinforcement Learning paradigm, Q-learning <ref type="bibr" target="#b20">[22]</ref> specifically, to incorporate learning for the dynamic multichannel access problem. The goal of Q-learning is to find an optimal policy, i.e., a sequence of actions that maximizes the long-term expected accumulated discounted reward. Q-learning is an empirical value iteration approach and the essence is to find the Q-value of each state and action pairs, where the state x is a function of observations (and rewards) and the action a is some action that a user can take given the state x. The Q-value of a state-action pair (x, a) from policy π, denoted as Q π (x, a), is defined as the sum of the discounted reward received when taking action a in the initial state x and then following the policy π thereafter. Q π * (x, a) is the Q-value with initial state x and initial action a, and then following the optimal policy π * . Thus, the optimal policy π * can be derived as π * (x) = arg max a Q π * (x, a), ∀x.</p><p>One can use online learning method to find Q π * (x, a) without any knowledge of the system dynamics. Assume at the beginning of each time slot, the agent takes an action a t ∈ {1, ..., N } that maximizes its Q-value of state-action pair (x t , a t ) given the state is x t , and gains a reward r t+1 . Then the online update rule of Q-values with learning rate 0 &lt; α &lt; 1 is given as follows:</p><formula xml:id="formula_2">Q(x t , a t ) ← Q(x t , a t ) + α[r t+1 + γ max a t +1 Q(x t+1 , a t+1 ) -Q(x t , a t )]</formula><p>In the context of the dynamic multichannel access, the problem can be converted to an MDP when considering the belief space, and Q-learning can be applied consequently. However, this approach is impractical since the belief update is maintained by knowing the system transition matrix P a-priori, which is hardly available in practice. Instead, we apply Q-learning by directly considering the history of observations and actions. We define the state for the Q-learning at time slot t as a combination of historical selected channels as well as their observed channel conditions over previous M time slots, i.e.,</p><formula xml:id="formula_3">x t = [a t-1 , o t-1 , ..., a t-M , o t-M ].</formula><p>And intuitively, the more historical information we consider (i.e., the larger M is), the better Q-learning can learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Q-Network</head><p>Q-learning works well when the problem's state space is small, as a look-up table can be used to update Q values. But this is impossible when the state space becomes large. The state space size in this work grows exponentially as O(N M ), as we use a combination of M vectors of length N to represent historical observations and actions for a system with N 2-state channels over past M time slots. M is required to be large so that Q-learning can capture enough information for learning. Even worse, since many states are rarely visited, their corresponding Q-values are seldom updated. This causes Q learning to take a very long time to converge.</p><p>Motivated by its success in other domains <ref type="bibr" target="#b4">[5]</ref>, we adopt the deep Q-Network approach to address the very large state space. DQN takes the state-action pair as input and outputs the corresponding Q-value. Q-network updates its weights θ at each iteration i to minimize the loss function</p><formula xml:id="formula_4">L i (θ i ) = E[(y i -Q(x, a; θ i )) 2 ], where y i = E[r + γ max a Q(x , a ; θ i-1 )</formula><p>] is derived from the same Qnetwork with old weights θ i-1 and new state x after taking action a from state x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. OPTIMAL POLICY FOR KNOWN FIXED-PATTERN CHANNEL SWITCHING</head><p>To study the performance of DQN, we first consider a correlated channel model that we refer to as fixed-pattern channel switching, in which all the N channels in the system can be divided into several independent subsets and these subsets take turns to be activated following a fixed pattern. Specifically, we assume all channels in one currently activated subset are good and all channels in inactivated subsets are bad. At each time slot, with a known probability p (0 ≤ p ≤ 1) the next following subset is activated, and with probability 1p the current subset remains activated. We assume the activation order of the subsets is known, fixed, and will not change over time. In this special case, the optimal policy can be found analytically and is summarized in Theorem 1, providing a baseline to evaluate the performance of DQN implementation in the next section.</p><p>Theorem 1: When the system follows a fixedpattern channel switching model, the optimal channel access policy follows Algorithm 1 depending on the value of p. Choose a channel in the next activated subset according to the subset activation order Proof: Please see proof in <ref type="bibr" target="#b21">[23]</ref>. It turns out that the optimal policy for the fixedpattern channel switching shares a structure that is simple and robust similar to the Myopic policy in <ref type="bibr" target="#b7">[8]</ref>: the optimal policy has a round-robin structure (in terms of the channel subset activation order) and does not require to know the exact value of p except whether it is above/below 0.5. This semiuniversal property makes the optimal policy easy to implement in practice and robust to mismatches of system dynamics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENT AND EVALUATION OF LEARNING FOR UNKNOWN FIXED-PATTERN</head><p>CHANNEL SWITCHING We present details of our DQN implementation and then evaluate its performance on the fixedpattern switching pattern model, comparing it to the optimal policy, through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DQN Architecture</head><p>We design a DQN by following the Deep Qlearning with Experience Replay Algorithm <ref type="bibr" target="#b4">[5]</ref> and implement it in TensorFlow <ref type="bibr" target="#b22">[24]</ref>. The structure of our DQN is finalized as a fully connected neural network with each of the two hidden layers containing 200 neurons 1 . The activation function of each neuron is Rectified Linear Unit (ReLU), which computes the function f (x) = max(x, 0). The input of the DQN is defined as the combination of previous actions and observations over previous M time slots, selecting M = N. The output of the DQN is a vector of length N, where the ith item represents the Q value of a given state if channel is selected. We apply the -greedy policy with the random action exploration probability fixed as 0.1. A technique called Experience Replay introduced in <ref type="bibr" target="#b4">[5]</ref> is used to store previous observation data and break correlations among data samples that makes training stable and convergent. When updating the weights θ of the DQN, a minibatch of 32 samples are randomly selected from the replay memory to compute the loss function, and a recently proposed Adam algorithm <ref type="bibr" target="#b23">[25]</ref> is used to update the weights (details on the hyperparameters are listed in Table <ref type="table" target="#tab_0">I</ref>). In the following experiment settings, we consider a system of 16 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment and Evaluation</head><p>In our experiments, we considered different situations: single good channel or multiple good channels, sequential switching or arbitrary switching, 1 Please refer to <ref type="bibr" target="#b21">[23]</ref> for more explanations on this structure. Fig. <ref type="figure">3</ref>: Average discounted reward as we increase the number of good channels in the multiple good channels situation and observe that DQN can achieve the optimal performance in all situations. Due to the space constraint, we only present results on the multiple good channels situation, and refer the reader to <ref type="bibr" target="#b21">[23]</ref> for more results on other situations. In this section, we investigate the fixed-pattern channel switching model with 16 channels are evenly divided into several subsets that take turns to become available with a switching probability fixed at p = 0.9. In Fig. <ref type="figure" target="#fig_4">2</ref>, we provide a pixel illustration to visualize how the states of channels change in a multiple good channels situation over 50 time slots, where a white cell indicates the corresponding channel is good at the corresponding time.</p><p>We compare the DQN with two other policies: the Whittle Index heuristic policy and the optimal policy with known system dynamics from section VI. The optimal policy has full knowledge of the system dynamics and serves as a performance upper bound. In the Whittle Index heuristic, the user assumes all channels are independent and observes each channel individually for 10, 000 time slots to estimate its 2state Markov chain transition matrix.</p><p>We vary the number of channels in a subset as 1, 2, 4 and 8 in the experiment, and present the experimental results in Fig. <ref type="figure">3</ref>. The 16 channels in the system are in order and the subsets are activated in a sequential round-robin order in the upper graph in Fig. <ref type="figure">3</ref>, while the channels are arranged arbitrarily and the activation order of subsets is also arbitrary in the bottom graph in Fig. <ref type="figure">3</ref>. As can be seen, DQN remains robust and achieves the same optimal performance in all cases as the optimal policy and performs significantly better than the Whittle Index heuristic. This shows that DQN can implicitly learn the system dynamics including the correlation among channels, and finds the optimal policy accordingly. On the contrary, the Whittle Index heuristic simply assumes the channels are independent and is not able to find or make use of the correlation among channels. Moreover, the training time decreases as the number of good channels increases. This is because there is more chance to find a good channel when more good channels are available at a time slot, and the learning process becomes easier so that the DQN agent can take less time exploring and is able to find the optimal policy more quickly. This also explains why Whittle Index heuristic performs better when there are more good channels available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. EXPERIMENT AND EVALUATION OF DQN</head><p>FOR MORE COMPLEX SITUATIONS We now consider whether DQN can achieve a good or even optimal performance in more complex and realistic situations. For this set of experiments, we re-tuned our neural network structure to become a fully connected neural network with each hidden layer containing 50 neurons (and the learning rate is set as 10 -5 ) 2 , and considered more complex simulated situations as well as real data traces described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Perfectly correlated scenario</head><p>We consider a highly correlated scenario. In a 16channel system, we assume only two or three channels are independent, and other channels are exactly identical or opposite to one of these independent channels. In addition to the Whittle Index heuristic, we also compare DQN with a Random Policy in which the user randomly selects one channel with equal probability at each time slot. Since the optimal policy is computationally prohibitive, we implement the Myopic policy instead as a genie (knowing the system statistics a-priori) since it is simple, robust and can achieve an optimal performance in certain situations. 2 Please refer to <ref type="bibr" target="#b21">[23]</ref> for the explanations on the DQN structure. During the simulation, we arbitrarily set the independent channels to follow the same 2-state Markov chain with p 11 ≥ p 01 . When the correlation coefficient ρ = 1, the user can ignore those channels that are perfectly correlated with independent channels and only select a channel from the independent channels. In this case, the multichannel access problem becomes selecting one channel from several i.i.d. channels that are positively correlated, i.e., p 11 ≥ p 01 . Then as it is shown in the previous work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, the Myopic policy with known P is optimal and has a simple round-robin structure alternating among independent channels. In the case when ρ = -1, the Myopic policy with known P also has a simple structure that alternates between two negatively perfectly correlated channels. Though more analysis needs to be done in future to show whether the Myopic policy is optimal/near-optimal when ρ = -1, it can still serve as a performance benchmark as the Myopic policy is obtained with full knowledge of the system dynamics.</p><p>In Fig. <ref type="figure" target="#fig_5">4</ref> we present the performance of all four policies: (i) DQN, (ii) Random, (iii) Whittle Index heuristic, and (iv) Myopic policy with known P. In the first three cases (x-axis 0, 1 and 2), the correlation coefficient ρ is fixed as 1 and in the last three cases (x-axis 3, 4 and 5), ρ is fixed as -1. We also vary the set of correlated channels to make cases different. The Myopic policy in the first three cases is optimal, and in the last three cases is conjectured to be near-optimal. As shown in Fig. <ref type="figure" target="#fig_5">4</ref>, the Myopic policy, which is implemented based on the full knowledge of the system, is the best among all six cases and serves as an upper bound. DQN provides a performance very close to the Myopic policy without any knowledge of the system dynamics. The Whittle Index policy performs worse than DQN in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real data trace</head><p>We use real data trace collected from our indoor testbed Tutornet 3 to train and evaluate the performance of DQN on real systems. The testbed is composed of TelosB nodes with IEEE 802.15.4 radio. We programmed a pair of motes distanced approximately 20 meters to be transmitter/receiver. The transmitter continually transmits one packet rapidly to each one of the 16 available channels within one time slot and the synchronized receiver records the successful and failed attempts, with the only interference coming from surrounding WiFi networks that show high dynamic variability.</p><p>The data are collected for about 17 hours. In order to create a challenging environment to test the learning capability of DQN, we ignore 8 good channels and use only the data trace from the remaining 8 channels that show significant WiFi interference.</p><p>We use the same data trace to train the DQN and to compute the MLE of the transition matrices of each channel for the Whittle index based heuristic policy. We compare the performance of the DQN policy, the Whittle index based heuristic policy and the Random policy. The Myopic Policy is not considered as finding the transmission matrix of the entire system is computationally expensive. The average accumulated discounted reward from each policy is listed in descending order: 0.947 (DQN), 0.767 (Whittle Index) and -2.170 (Random Policy). It can be seen that DQN performs best in this complicated real scenario. We also present the channel utilization of each policy in Fig. <ref type="figure" target="#fig_6">5</ref> to illustrate the difference among them. It shows DQN benefits from using other channels when the two best channels (used by the Whittle Index heuristic all the time) may not be in good states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Practical Issues</head><p>We now consider the practical issue of synchronization between the sender and receiver of the system. One approach is to run the same structured DQNs at the sender and the receiver separately. The two DQNs start with the same default channel and 3 More information about the testbed on http://anrg.usc.edu/www/tutornet/ are trained concurrently. By allowing the receiver to send back an ACK/NAK to the sender indicating whether it receives a message or not every time slot, the sender and receiver are guaranteed to have the same channel observations and thus training samples. By using the same random seed on both sides to initialize the pseudorandom number generator, we can avoid any difference occurred during exploration and back propagation during training. Thus, the two DQNs always have the same parameters and select the same channels, and the final learned policy is guaranteed to be the same.</p><p>The channel mismatch problem can still happen when an ACK/NAK is lost, which not only causes loss of communication, but also results in different learned DQN models at the sender and receiver that give different channel selection policies. One possible approach is to find a way to let the sender and the receiver be aware of the time when a channel mismatch happens, and try to recover in time. The sender can detect the mismatch event if no ACK/NAK is received. Once the mismatch happens, the sender stops updating its DQN model as well as training dataset and transmits data in the future using one single good channel -or a small set of channels known so far to have better channel conditions <ref type="bibr" target="#b24">[26]</ref>. Along with the data messages, the sender also sends the timestamp when the channel mismatch was perceived. The sender keeps sending this channel mismatch time information until an ACK being received, which indicates the receiver is on the same channel again and receives the channel mismatch information. The receiver can then set its DQN model as well as its training dataset back to the state right before the channel mismatch happened, which guarantees that the sender and the receiver have the same DQN models as well At the beginning of period n 4:</p><p>Evaluate the accumulated reward of the current policy 5:</p><p>if The reward is reduced by a given threshold 4 then 6:</p><p>Re-train the DQN to find a new good policy 7:</p><p>else 8:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keep using the current policy</head><p>To enhance DQN and make it more applicable in realistic, dynamic situations, we have designed an adaptive extension in Algorithm 2 to make DQN able to be aware of the system change and re-learn if needed. The main idea is to let DQN periodically evaluate the performance (i.e., the accumulated reward) of its current policy, and if the performance degrades by a certain amount, the DQN can infer that the environment has changed and start relearning.</p><p>To evaluate this enhancement, we make the system initially follow one of the fixed-pattern channel switching cases from Section VII, and after some time it changes to another case. We consider both single good channel and multiple good channels situations. We let DQN automatically operate according to Alg. 2, while we manually re-train Whittle Index heuristic (as it is not able to detect any change) when there is a change in the environment. Fig. <ref type="figure">6</ref> compares the reward of both the old and new policies learned for DQN and the Whittle Index heuristic in the new environment, as we vary the pattern changes. As can be seen, DQN is able to find an optimal policy for the new environment as the genie optimal policy does, while Whittle Index heuristic, even manually tuned, does not. We also provide the real-time accumulated reward during the learning process of DQN and the Whittle Index heuristic in one of the above pattern changing situations in Fig. <ref type="figure">7</ref>. The system initially starts with an environment that has 8 channels being good at each time slot for the first 10 iterations. As can be seen, both DQN and the Whittle Index heuristic are able to quickly find a good channel access policy, but DQN achieves the optimal performance. At iteration 11, the environment changes to having only 1 channel being good at each time slot. As there is a significant drop in the reward, DQN can detect the change and starts re-learning. And at iteration 70, DQN finds the optimal policy and our system keeps following the optimal policy thereafter. On the other hand, even though we manually enable the Whittle Index heuristic to detect the change and reestimate the system model and re-find a new policy, its performance is still unsatisfying as it cannot make use of the correlation among channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have considered the dynamic multichannel access problem in a more general and practical scenario when channels are correlated and system statistics is unknown. The problem is an unknown POMDP without any tractable solution, and we have applied an end-to-end DQN approach that directly utilizes historical observations and actions to find the access policy via online learning.</p><p>In the fixed-pattern channel switching, we have analytically found the optimal access policy with known system statistics and full observation ability.</p><p>Through simulations, we have shown DQN is able to achieve the same optimal performance even without any prior knowledge. We have also shown from both simulations and real data trace that DQN can achieve near-optimal performance in more complex scenarios. In addition, we have designed an adaptive DQN and shown through numerical simulations that it can detect system changes and re-learn in nonstationary dynamic environments.</p><p>There are a number of open directions suggested by the present work. First, we plan to apply the DQN framework to consider more realistic and complicated scenarios such as multi-user, multihop and simultaneous transmissions in WSNs. The framework of DQN can be directly extended to consider these practical factors in a simple way. For example, in the situation of multiple users, to avoid interference and collisions among users, we can adopt a centralized approach: assuming there is a centralized controller that can select a subset of non-interfering channels at any time slot, and assign one to each user to avoid a collision. By redefining the action as selecting a subset of non-interfering channels, the DQN framework can be directly used for this multi-user scenario. As the action space becomes large when selecting multiple channels, the current DQN structure requires careful re-design and may require very long training interval before finding a reasonable solution. Instead, we use the same DQN structure as that in Section VII and consider the multiple-users situation in a smaller system that contains 8 channels where at any time slot 6 channels become good and channel conditions change in a round-robin pattern. The number of users varies from 2 to 4. As is shown in Fig. <ref type="figure">8</ref>, DQN can still achieve a good performance in the multiple-user case. Other deep reinforcement learning approaches, such as Deep Deterministic Policy Gradient (DDPG) [27], will be studied in future to tackle the large action space challenge.</p><p>Second, when the number of users in the network Whittle index heuristic DQN Optimal policy with known system dynamics Fig. <ref type="figure">8</ref>: Average discounted reward as we vary the number of users in the multiple-user situation becomes large, the above proposed centralized approach becomes too computationally expensive. In future, we plan to study a more practical distributed approach where each user can learn a channel selection policy independently. One intuitive idea is to implement a DQN at each user independently.</p><p>Then users can learn their channel selection policies parallelly, and avoid interference and conflicts by making proper channel-selection decisions based on the information gained from observations and rewards. However, whether a good or optimal policy can be learned, and whether an equilibrium exists are unknown and need further investigation. Moreover, as DQN is not easy to tune and may get stuck in local optima easily, we plan to work on improving our DQN implementation as well as considering other Deep Reinforcement Learning approaches to see if they have the ability to reach the optimal performance in general situations and study the tradeoff between implementation complexity and performance guarantee. Also as a way to test the full potential of DQN (or Adaptive DQN) as well as other deep reinforcement learning technologies in the problem of multichannel access, we encourage the networking community to work together to create an open source dataset that contains different practical channel access scenarios so that researchers can benchmark the performance of different approaches. We have published all the channel access environments and real data trace considered in this work <ref type="foot" target="#foot_0">5</ref> . This might serve as an useful benchmark dataset for researchers to use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Running time (seconds) in log scale of the POMDP solver as we vary the number of channels in the system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>p 00 p 01 p 10 p 11 .</head><label>11</label><figDesc>It is shown that the Myopic policy is 2332-7731 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2332-7731 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 policy 1 : 3 : 4 : 6 :</head><label>11346</label><figDesc>Optimal At the beginning of time slot 0, choose a channel in the initial activated subset C 1 2: for n = 1, 2, . . . do At the beginning of time slot n, if 0.5 ≤ p ≤ 1 then 5:if The previous chosen channel is good then Choose a channel in the next activated subset according to the subset activation order 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: A capture of a multiple good channels situation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Average discounted reward for 6 different cases. Each case considers a different set of correlated channels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Channel utilization of 8 channels in the testbed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Average discounted reward as we vary the channel switching pattern situations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>List of DQN Hyperparameters</figDesc><table><row><cell>Hyperparameters</cell><cell>Values</cell></row><row><cell></cell><cell>0.1</cell></row><row><cell>Minibatch size</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Activation Function</cell><cell>ReLU</cell></row><row><cell>Learning rate</cell><cell>10 -4</cell></row><row><cell>Experience replay size</cell><cell>1, 000, 000</cell></row><row><cell>γ</cell><cell>0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2332-7731 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCCN.2018.2809722, IEEE Transactions on Cognitive Communications and Networking 9 as training datasets. They can resume operating and training thereafter. It can be shown that the expected number of time slots needed for re-syncing on a good channel after a channel mismatch is N p good (1-p ack ) , where N is the number of channels, p good the probability this channel being good, p ack</figDesc><table><row><cell>the probability an ACK/NAK being lost, and the</cell></row><row><cell>exploration probability. As long as the sender and</cell></row><row><cell>the receiver can re-synchronize again after a channel</cell></row><row><cell>mismatch, the effectiveness and performance of the</cell></row><row><cell>DQN approach is guaranteed.</cell></row><row><cell>IX. ADAPTIVE DQN FOR UNKNOWN,</cell></row><row><cell>TIME-VARYING ENVIRONMENTS</cell></row><row><cell>Algorithm 2 Adaptive DQN</cell></row><row><cell>1: First train DQN to find a good policy to operate with</cell></row><row><cell>2: for n = 1, 2, . . . do</cell></row><row><cell>3:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>https://github.com/ANRGUSC/MultichannelDQN-channelModel</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This material is based upon work supported by Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0053. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. This work is also supported in part by NSF through grant number 1423624.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dynamic multichannel access in This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCCN.2018.2809722</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>wireless networks,&quot; in ICNC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information capacity and power control in single-cell multiuser communications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Humblet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICC</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Next generation/dynamic spectrum access/cognitive radio wireless networks: A survey</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Akyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Vuran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohanty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer networks</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2127" to="2159" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The complexity of markov decision processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An mdp-based vertical handoff decision algorithm for heterogeneous wireless networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stevens-Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1243" to="1254" />
			<date type="published" when="2008-03">March 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online learning of power allocation policies in energy harvesting communications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sakulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPCOM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On myopic sensing for multi-channel opportunistic access: structure, optimality, and performance</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5431" to="5440" />
			<date type="published" when="2008-12">dec 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimality of myopic sensing in multichannel opportunistic access</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4040" to="4050" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5547" to="5567" />
			<date type="published" when="2010-11">nov 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Opportunistic bandwidth sharing through reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamdaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3148" to="3153" />
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model free dynamic sensing order selection for imperfect sensing multichannel cognitive radio networks: A q-learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">End-toend training of deep visuomotor policies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00702</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data-efficient learning of feedback policies from image pixels using deep dynamical models</title>
		<author>
			<persName><forename type="first">J.-A</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wahlström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02173</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Logarithmic weak regret of nonbayesian restless multi-armed bandit</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online learning in opportunistic spectrum access: A restless bandit approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient online learning for opportunistic spectrum access</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online learning for multi-channel opportunistic access over unknown markovian channels</title>
	</analytic>
	<monogr>
		<title level="m">IEEE SECON</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regret bounds for restless markov bandits</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A D</forename><surname>Ryabko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ALT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for dynamic multichannel access in wireless networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06958</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSFI</title>
		<imprint>
			<biblScope unit="page" from="265" to="283" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A fast joining scheme on channel quality for IEEE802</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-V</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07">jul 2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>4e TSCH in severe interference environment</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">His primary research interest is in the design, theoretical analysis, and experimental evaluation of algorithms and protocols for next-generation wireless networks, including low power wireless sensor networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has co-authored over 200 publications on this topic, including best paper awards at Mobicom in 2010, IPSN in 2004 and 2010, and MSWiM</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Viterbi School of Engineering, University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note>1998 and the M.S. and Ph.D. degrees from Cornell University in 1999 and 2002, respectively. that have been collectively over 20000 times per Google Scholar</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
