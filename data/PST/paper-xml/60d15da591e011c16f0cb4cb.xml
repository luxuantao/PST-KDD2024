<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-18">18 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<email>akolesnikov@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
							<email>rwightman@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<email>lbeyer@google.com</email>
						</author>
						<title level="a" type="main">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-18">18 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.10270v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation ("AugReg" for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. 1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.</p><p>Preprint. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Vision Transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> has recently emerged as a competitive alternative to convolutional neural networks (CNNs) that are ubiquitous across the field of computer vision. Without the translational equivariance of CNNs, ViT models are generally found to perform best in settings with large amounts of training data <ref type="bibr" target="#b9">[10]</ref> or to require strong AugReg schemes to avoid overfitting <ref type="bibr" target="#b33">[34]</ref>. To our knowledge, however, so far there was no comprehensive study of the trade-offs between model regularization, data augmentation, training data size and compute budget in Vision Transformers.</p><p>In this work, we fill this knowledge gap by conducting a thorough empirical study. We pre-train a large collection of ViT models (different sizes and hybrids with ResNets <ref type="bibr" target="#b13">[14]</ref>) on datasets of different sizes, while at the same time performing carefully designed comparisons across different amounts of regularization and data augmentation. We proceed by conducting extensive transfer learning experiments with the resulting models. We focus mainly on the perspective of a practitioner with limited compute and data annotation budgets.</p><p>The homogeneity of the performed study constitutes one of the key contributions of this paper. For the vast majority of works involving Vision Transformers it is not practical to retrain all baselines and proposed methods on equal footing, in particular those trained on larger amounts of data. Furthermore, there are numerous subtle and implicit design choices that cannot be controlled for effectively, such as the precise implementation of complex augmentation schemes, hyper-parameters (e.g. learning Figure <ref type="figure">1</ref>: Left: Adding the right amount of regularization and image augmentation can lead to similar gains as increasing the dataset size by an order of magnitude. Right: For small and mid-sized datasets it is very hard to achieve a test error that can trivially be attained by fine-tuning a model pre-trained on a large dataset like ImageNet-21k -see also Figure <ref type="figure">2</ref>. With our recommended models (Section 4.5), one can find a good solution with very few trials (bordered green dots). Note that AugReg is not helpful when transferring pre-trained models (borderless green dots).</p><p>rate schedule, weight decay), test-time preprocessing, dataset splits and so forth. Such inconsistencies can result in significant amounts of noise added to the results, quite possibly affecting the ability to draw any conclusions. Hence, all models on which this work reports have been trained and evaluated in a consistent setup.</p><p>The insights we draw from our study constitute another important contribution of this paper. In particular, we demonstrate that carefully selected regularization and augmentations roughly correspond (from the perspective of model accuracy) to a 10x increase in training data size. However, regardless of whether the models are trained with more data or better AugRegs, one has to spend roughly the same amount of compute to get models attaining similar performance. We further evaluate if there is a difference between adding data or better AugReg when fine-tuning the resulting models on datasets of various categories.</p><p>In addition, we aim to shed light on other aspects of using Vision Transformers in practice such as comparing transfer learning and training from scratch for mid-sized datasets. Finally, we evaluate various compute versus performance trade-offs. We discuss all of the aforementioned insights and more in detail in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scope of the study</head><p>With the ubiquity of modern deep learning <ref type="bibr" target="#b20">[21]</ref> in computer vision it has quickly become common practice to pre-train models on large datasets once and re-use their parameters as initialization or a part of the model as feature extractors in models trained on a broad variety of other tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this setup, there are multiple ways to characterize computational and sample efficiency. When simply considering the overall costs of pre-training and subsequent training or fine-tuning procedures together, the cost of pre-training usually dominates, often by orders of magnitude. From the vantage point of a researcher aiming to improve model architectures or pre-training schemes, the pre-training costs might therefore be most relevant. Most practitioners, however, rarely, if ever perform pretraining on today's largest datasets but instead use some of the many publicly available parameter sets.</p><p>For them the costs of fine-tuning, adaptation or training a task-specific model from scratch would be of most interest.</p><p>Yet another valid perspective is that all training costs are effectively negligible since they are amortized over the course of the deployment of a model in applications requiring a very large number of invocations of inference.</p><p>In this setup there are different viewpoints on computational and data efficiency aspects. One approach is to look at the overall computational and sample cost of both pre-training and fine-tuning. Normally, "pre-training cost" will dominate overall costs. This interpretation is valid in specific scenarios, especially when pre-training needs to be done repeatedly or reproduced for academic/industrial purposes. However, in the majority of cases the pre-trained model can be downloaded or, in the worst case, trained once in a while. Contrary, in these cases, the budget required for adapting this model may become the main bottleneck.</p><p>Thus, we pay extra attention to the scenario, where the cost of obtaining a pre-trained model is free or effectively amortized by future adaptation runs. Instead, we concentrate on time and compute spent on finding a good adaptation strategy (or on tuning from scratch training setup), which we call "practitioner's cost".</p><p>A more extreme viewpoint is that the training cost is not crucial, and all what matters is eventual inference cost of the trained model, "deployment cost", which will amortize all other costs. This is especially true for large scale deployments, where a visual models is executed to be used massive number of times. Overall, there are three major viewpoints on what is considered to be the central cost of training a vision model. In this study we touch on all three of them, but mostly concentrate on "practioner's" and "deployment" costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup</head><p>In this section we describe our unified experimental setup, which is used throughout the paper. We use a single JAX/Flax <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3]</ref> codebase for pre-training and transfer learning using TPUs. Inference speed measurements, however, were obtained on V100 GPUs (16G) using the timm PyTorch library <ref type="bibr" target="#b36">[37]</ref>. All datasets are accessed through the TensorFlow Datasets library, which helps to ensure consistency and reproducibility. More details of our setup are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and metrics</head><p>For pre-training we use two large-scale image datasets: ILSVRC-2012 (ImageNet-1k) and ImageNet-21k. ImageNet-21k dataset contains approximately 14 million images with about 21'000 distinct object categories <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. ImageNet-1k is a subset of ImageNet-21k consisting of about 1.3 million training images and 1000 object categories. We make sure to de-duplicate images in ImageNet-21k with respect to the test sets of the downstream tasks as described in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Additionally, we used ImageNetV2 <ref type="bibr" target="#b24">[25]</ref> for evaluation purposes.</p><p>For transfer learning evaluation we use 4 popular computer vision datasets from the VTAB benchmark <ref type="bibr" target="#b39">[40]</ref>: CIFAR-100 <ref type="bibr" target="#b19">[20]</ref>, Oxford IIIT Pets <ref type="bibr" target="#b23">[24]</ref> (or Pets37 for short), Resisc45 <ref type="bibr" target="#b4">[5]</ref> and Kittidistance <ref type="bibr" target="#b10">[11]</ref>. We selected these datasets to cover the standard setting of natural image classification (CIFAR-100 and Pets37), as well as classification of images captured by specialized equipment (Resisc45) and geometric tasks (Kitti-distance). In some cases we also use the full VTAB benchmark (19 datasets) to additionally ensure robustness of our findings.</p><p>For all datasets we report top-1 classification accuracy as our main metric. Hyper-parameters for fine-tuning are selected by the result from the validation split, and final numbers are reported from the test split. Note that for ImageNet-1k we follow common practice of reporting our main results on the validation set. Thus, we set aside 1% of the training data into a minival split that we use for model selection. Similarly, we use a minival split for CIFAR-100 (2% of training split) and Oxford IIT Pets (10% of training split). For Resisc45, we use only 60% of the training split for training, and another 20% for validation, and 20% for computing test metrics. Kitti-distance finally comes with an official validation and test split that we use for the intended purpose. See <ref type="bibr" target="#b39">[40]</ref> for details about the VTAB dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>This study focuses mainly on the Vision Transformer (ViT) <ref type="bibr" target="#b9">[10]</ref>. We use 4 different configurations from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>: ViT-Ti, ViT-S, ViT-B and ViT-L, which span a wide range of different capacities. The details of each configuration are provided in Table <ref type="table" target="#tab_1">1</ref>. We use patch-size 16 for all models, and additionally patch-size 32 for the ViT-S and ViT-B variants. The only difference to the original ViT model <ref type="bibr" target="#b9">[10]</ref> in our paper is that we drop the hidden layer in the head, as empirically it does not lead to more accurate models and often results in optimization instabilities.  In addition, we train hybrid models that first process images with a ResNet <ref type="bibr" target="#b13">[14]</ref> backbone and then feed the spatial output to a ViT as the initial patch embeddings. We use a ResNet stem block (7 × 7 convolution + batch normalization + ReLU + max pooling) followed by a variable number of bottleneck blocks <ref type="bibr" target="#b13">[14]</ref>. We use the notation Rn+{Ti,S,L}/p where n counts the number of convolutions, and p denotes the patch-size in the input image -for example R+Ti/16 reduces image dimensions by a factor of two in the ResNet stem and then forms patches of size 8 as an input to the ViT, which results in an effective patch-size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularization and data augmentations</head><p>To regularize our models we use robust regularization techniques widely adopted in the computer vision community. We apply dropout to intermediate activations of ViT as in <ref type="bibr" target="#b9">[10]</ref>. Moreover, we use the stochastic depth regularization technique <ref type="bibr" target="#b15">[16]</ref> with linearly increasing probability of dropping layers.</p><p>For data augmentation, we rely on the combination of two recent techniques, namely Mixup <ref type="bibr" target="#b40">[41]</ref> and RandAugment <ref type="bibr" target="#b5">[6]</ref>. For Mixup, we vary its parameter α, where 0 corresponds to no Mixup. For RandAugment, we vary the magnitude parameter m, and the number of augmentation layers l.</p><p>We also try two values for weight decay <ref type="bibr" target="#b22">[23]</ref> which we found to work well, since increasing AugReg may need a decrease in weight decay <ref type="bibr" target="#b1">[2]</ref>.</p><p>Overall, our sweep contains 28 configurations, which is a cross-product of the following hyperparameter choices:</p><p>• Either use no dropout and no stochastic depth (e.g. no regularization) or use dropout with probability 0.1 and stochastic depth with maximal layer dropping probability of 0.1, thus 2 configuration in total.</p><p>• 7 data augmentation setups for (l, m, α): none (0, 0, 0), light1 (2, 0, 0), light2 (2, 10, 0.2), medium1 (2, 15, 0.2), medium2 (2, 15, 0.5), strong1 (2, 20, 0.5), strong2 (2, 20, 0.8).</p><p>• Weight decay: 0.1 or 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-training</head><p>We pre-trained the models with Adam <ref type="bibr" target="#b16">[17]</ref>, using β 1 = 0.9 and β 2 = 0.999, with a batch size of 4096, and a cosine learning rate schedule with a linear warmup (10k steps). To stabilize training, gradients were clipped at global norm 1. The images are pre-processed by Inception-style cropping <ref type="bibr" target="#b30">[31]</ref> and random horizontal flipping. On the smaller ImageNet-1k dataset we trained for 300 epochs, and for 30 and 300 epochs on the ImageNet-21k dataset. Since ImageNet-21k is about 10x larger than ImageNet-1k, this allows us to examine the effects of the increased dataset size also with a roughly constant total compute used for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-tuning</head><p>We fine-tune with SGD with a momentum of 0.9 (storing internal state as bfloat16), sweeping over 2-3 learning rates and 1-2 training durations per dataset as detailed in Table <ref type="table">4</ref> in the appendix. We used a fixed batch size of 512, gradient clipping at global norm 1 and a cosine decay learning rate schedule with linear warmup. Fine-tuning was done both at the original resolution (224), as well as at a higher resolution (384) as described in <ref type="bibr" target="#b34">[35]</ref>. 4 Findings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scaling datasets with AugReg and compute</head><p>One major finding of our study, which is depicted in Figure <ref type="figure">1</ref> (left), is that by judicious use of image augmentations and model regularization, one can (pre-)train a model to similar accuracy as by increasing the dataset size by about an order of magnitude. More precisely, our best models trained on AugReg ImageNet-1k <ref type="bibr" target="#b26">[27]</ref> perform about equal to the same models pre-trained on the 10x larger plain ImageNet-21k <ref type="bibr" target="#b26">[27]</ref> dataset. Similarly, our best models trained on AugReg ImageNet-21k, when compute is also increased (e.g. training run longer), match or outperform those from <ref type="bibr" target="#b9">[10]</ref> which were trained on the plain JFT-300M <ref type="bibr" target="#b29">[30]</ref> dataset with 25x more images. Thus, it is possible to match these private results with a publicly available dataset, and it is imaginable that training longer and with AugReg on JFT-300M might further increase performance.</p><p>Of course, these results cannot hold for arbitrarily small datasets. For instance, according to Table <ref type="table" target="#tab_7">5</ref> of <ref type="bibr" target="#b38">[39]</ref>, training a ResNet50 on only 10% of ImageNet-1k with heavy data augmentation improves results, but does not recover training on the full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer is the better option</head><p>Here, we investigate whether, for reasonably-sized datasets a practitioner might encounter, it is advisable to try training from scratch with AugReg, or whether time and money is better spent transferring pre-trained models that are freely available. The result is that, for most practical purposes, transferring a pre-trained model is both more cost-efficient and leads to better results.</p><p>We perform a thorough search for a good training recipe<ref type="foot" target="#foot_1">2</ref> for both the small ViT-Ti/16 and the larger ViT-B/16 models on two datasets of practical size: Pet37 contains only about 3000 training images and is relatively similar to the ImageNet-1k dataset. Resisc45 contains about 30'000 training images and consists of a very different modality of satellite images, which is not well covered by either ImageNet-1k or ImageNet-21k. Figure <ref type="figure">1</ref> (right) and Figure <ref type="figure">2</ref> show the result of this extensive search.</p><p>The most striking finding is that, no matter how much training time is spent, for the tiny Pet37 dataset, it does not seem possible to train ViT models from scratch to reach accuracy anywhere near that of transferred models. Furthermore, since pre-trained models are freely available for download, the pre-training cost for a practitioner is effectively zero, only the compute spent on transfer matters, and thus transferring a pre-trained model is simultaneously significantly cheaper.</p><p>For the larger Resisc45 dataset, this result still holds, although spending two orders of magnitude more compute and performing a heavy search may come close (but not reach) to the accuracy of pre-trained models.  Notably, this does not account for the "exploration cost", which is difficult to quantify. For the pre-trained models, we highlight those which performed best on the pre-training validation set and could be called recommended models (see Section 4.5). We can see that using a recommended model has a high likelihood of leading to good results in just a few attempts, while this is not the case for training from-scratch, as evidenced by the wide vertical spread of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">More data yields more generic models</head><p>We investigate the impact of pre-training dataset size by transferring pre-trained models to unseen downstream tasks. We evaluate the pre-trained models on VTAB, including 19 diverse tasks <ref type="bibr" target="#b39">[40]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">3</ref> shows the results on three VTAB categories: natural, specialized and structured. The models are sorted by the inference time per step, thus the larger model the slower inference speed. We first compare two models using the same compute budget, with the only difference being the dataset size of ImageNet-1k (1.3M images) and ImageNet-21k (13M images). We pre-train for 300 epochs on ImageNet-1k, and 30 epochs on ImageNet-21k. Interestingly, the model pre-trained on ImageNet-21k is significantly better than the ImageNet-1k one, across all the three VTAB categories.</p><p>As the compute budget keeps growing, we observe consistent improvements on ImageNet-21k dataset with 10x longer schedule. On a few almost solved tasks, e.g. flowers, the gain is small in absolute numbers. For the rest of the tasks, the improvements are significant compared to the model pre-trained for a short schedule. All the detailed results on VTAB could be found from supplementary section C.</p><p>Overall, we conclude that more data yields more generic models, the trend holds across very diverse tasks. We recommend the design choice of using more data with a fixed compute budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prefer augmentation to regularization</head><p>It is not clear a priori what the trade-offs are between data augmentation such as RandAugment and Mixup, and model regularization such as Dropout and StochasticDepth. In this section, we aim to discover general patterns for these that can be used as rules of thumb when applying Vision Transformers to a new task. In Figure <ref type="figure" target="#fig_1">4</ref>, we show the upstream validation score obtained for each individual setting, i.e. numbers are not comparable when changing dataset. The colour of a cell encodes its improvement or deterioration in score when compared to the unregularized, unaugmented setting, i.e. the leftmost column. Augmentation strength increases from left to right, and model "capacity" increases from top to bottom.</p><p>The first observation that becomes visible, is that for the mid-sized ImageNet-1k dataset, any kind of AugReg helps. However, when using the 10x larger ImageNet-21k dataset and keeping compute fixed, i.e. running for 30 epochs, any kind of AugReg hurts performance for all but the largest models. It is only when also increasing the computation budget to 300 epochs that AugReg helps more models, although even then, it continues hurting the smaller ones. Generally speaking, there are significantly more cases where adding augmentation helps, than where adding regularization helps. More specifically, the thin columns right of each map in Figure <ref type="figure" target="#fig_1">4</ref> shows, for any given model, its best regularized score minus its best unregularized score. This view, which is expanded in Figure <ref type="figure">7</ref> in the Appendix, tells us that when using ImageNet-21k, regularization hurts almost across the board.  The improvement or deterioration in validation accuracy when using various amounts of augmentation and regularization. For relatively small amount of data, almost everything helps. However, when switching to ImageNet-21k while keeping the training budget fixed, almost everything hurts; only when also increasing compute, does AugReg help again. The single column right of each plot show the difference between the best setting with regularization and the best setting without, highlighting that regularization typically hurts on ImageNet-21k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Choosing which pre-trained model to transfer</head><p>As we show above, when pre-training ViT models, various regularization and data augmentation settings result in models with drastically different performance. Then, from the practitioner's point of view, a natural question emerges: how to select a model for further adaption for an end application? One way is to run downstream adaptation for all available pre-trained models and then select the best performing model, based on the validation score on the downstream task of interest. This could be quite expensive in practice. Alternatively, one can select a single pre-trained model based on the upstream validation accuracy and then only use this model for adaptation, which is much cheaper.</p><p>In this section we analyze the trade-off between these two strategies. We compare them for a large collection of our pre-trained models on 5 different datasets. Specifically, in Figure <ref type="figure" target="#fig_2">5</ref> (left) we highlight the performance difference between the cheaper strategy of adapting only the best pre-trained model and the more expensive strategy of adapting all pre-trained models (and then selecting the best).</p><p>The results are mixed, but generally reflect that the cheaper strategy works equally well as the more expensive strategy in the majority of scenarios. Nevertheless, there are a few notable outliers, when it Ever since <ref type="bibr" target="#b17">[18]</ref> first showed good results when pre-training BiT on ImageNet-21k, more architecture works have mentioned using it for select few experiments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>, with <ref type="bibr" target="#b25">[26]</ref> arguing more directly for the use of ImageNet-21k. However, none of these works thoroughly investigates the combined use of AugReg and ImageNet-21k and provides conclusions, as we do here.</p><p>An orthogonal line of work can introduces cleverly designed inductive biases in ViT variants or retain some of the general architectural parameters of successful convolutional architectures while adding self-attention to them. <ref type="bibr" target="#b28">[29]</ref> carefully combines a standard convolutional backbone with bottleneck blocks based on self-attention instead of convolutions. In <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> the authors propose hierarchical versions of ViT. <ref type="bibr" target="#b7">[8]</ref> suggests a very elegant idea of initializing Vision Transformer, such that it behaves similarly to convolutional neural network in the beginning of training.</p><p>Yet another way to address overfitting and improve transfer performance is to rely on self-supervised learning objectives. <ref type="bibr" target="#b0">[1]</ref> pre-trains ViT to reconstruct perturbed image patches. Alternatively, <ref type="bibr" target="#b3">[4]</ref> devises a self-supervised training procedure based on the idea from <ref type="bibr" target="#b11">[12]</ref>, achieving impressive results. We leave the systematic comparison of self-supervised and supervised pre-training to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We conduct the first systematic, large scale study of the interplay between regularization, data augmentation, model size, and training data size when pre-training Vision Transformers, including their respective effects on the compute budget needed to achieve a certain level of performance. We also evaluate pre-trained models through the lens of transfer learning. As a result, we characterize a quite complex landscape of training settings for pre-training Vision Transformers across different model sizes. Our experiments yield a number of surprising insights around the impact of various techniques and the situations when augmentation and regularization are beneficial and when not.</p><p>We also perform an in-depth analysis of the transfer learning setting for Vision Transformers. We conclude that across a wide range of datasets, even if the downstream data of interest appears to only be weakly related to the data used for pre-training, transfer learning remains the best available option. Our analysis also suggests that among similarly performing pre-trained models, for transfer learning a model with more training data should likely be preferred over one with more data augmentation.</p><p>We hope that our study will help guide future research on Vision Transformers and will be a useful source of effective training settings for practitioners seeking to optimize their final model performance in the light of a given computational budget.</p><p>Table <ref type="table">3</ref>: ImageNet-1k transfer. Tabular representation of the data in Figure <ref type="figure">6</ref>. The JFT-300M numbers are taken from <ref type="bibr" target="#b9">[10]</ref> (bold numbers indicate our results that are on par or surpass the published JFT-300M results without AugReg for the same models). Inference speed measurements were computed on an NVIDIA V100 GPU using timm <ref type="bibr" target="#b36">[37]</ref>, sweeping the batch size for best throughput. v2 Model selected using ImageNetV2 validation set. 30 / 300 denotes the number of epochs trained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C VTAB results</head><p>In Table <ref type="table" target="#tab_7">5</ref>, we show all the results in percentage for all the models on the full VTAB. We report VTAB score only for the best pre-trained models, selected by their upstream validation accuracy. For VTAB tasks, we sweep over 8 hyper parameters, include four learning rates {0.001, 0.003, 0.01, 0.03} and two schedules {500, 2500} steps. The best run was selected on VTAB validation split. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Pretraining on more data yields more transferable models on average, as tested on the VTAB suite<ref type="bibr" target="#b39">[40]</ref> of 19 tasks grouped into 3 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The improvement or deterioration in validation accuracy when using various amounts of augmentation and regularization. For relatively small amount of data, almost everything helps. However, when switching to ImageNet-21k while keeping the training budget fixed, almost everything hurts; only when also increasing compute, does AugReg help again. The single column right of each plot show the difference between the best setting with regularization and the best setting without, highlighting that regularization typically hurts on ImageNet-21k.</figDesc><graphic url="image-1.png" coords="7,126.22,88.56,112.25,72.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Choosing best models. Left: Difference of fine-tuning test scores between models chosen by best validation score on pre-training data vs. validation score on fine-tuning data (negative values mean that selecting models by pre-training validation deteriorates fine-tuning test metrics). Right: Correlation between "minival" validation score vs. ImageNetV2 validation score and official ImageNet-1k validation score (that serves as a test score in this study). The red circles correspond to best models by validation score -see Section 4.5 for an explanation.</figDesc><graphic url="image-7.png" coords="7,125.74,512.10,70.01,125.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Configurations of ViT models.</figDesc><table><row><cell>Model</cell><cell cols="2">Layers Width</cell><cell>MLP</cell><cell>Heads</cell><cell>Params</cell></row><row><cell>ViT-Ti [34]</cell><cell>12</cell><cell>192</cell><cell>768</cell><cell>3</cell><cell>5.8M</cell></row><row><cell>ViT-S [34]</cell><cell>12</cell><cell>384</cell><cell>1536</cell><cell>6</cell><cell>22.2M</cell></row><row><cell>ViT-B [10]</cell><cell>12</cell><cell>768</cell><cell>3072</cell><cell>12</cell><cell>86M</cell></row><row><cell>ViT-L [10]</cell><cell>24</cell><cell>1024</cell><cell>4096</cell><cell>16</cell><cell>307M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ResNet+ViT hybrid models.</figDesc><table><row><cell>Model</cell><cell cols="2">Resblocks Patch-size</cell><cell>Params</cell></row><row><cell>R+Ti/16</cell><cell>[]</cell><cell>8</cell><cell>6.4M</cell></row><row><cell cols="2">R26+S/32 [2, 2, 2, 2]</cell><cell>1</cell><cell>36.6M</cell></row><row><cell cols="2">R50+L/32 [3, 4, 6, 3]</cell><cell>1</cell><cell>330.0M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>32 31 30 29 33 31 28 27 26 25 24 38 37 35 34 33 32 31 34 32 29 27 26 25 25 40 39 38 37 35 35 34 37 34 33 32 31 30 29 44 43 43 42 41 40 39 42 40 39 38 37 35 35 43 42 43 42 41 40 40 42 40 40 38 38 36 36 45 45 43 43 42 41 41 44 44 42 41 40 40 39</figDesc><table><row><cell>RTi Ti/16 S/32 S/16 B/32 R26S B/16 R50L L/16</cell><cell>69 76 77 78 78 76 76 74 78 78 78 79 77 77 70 75 76 77 77 76 76 75 78 78 78 79 77 77 69 73 73 72 70 69 68 71 70 67 65 63 62 61 72 76 75 75 74 72 71 71 72 68 65 63 63 62 64 71 76 76 76 74 74 70 72 72 71 71 69 68 71 77 79 81 82 80 80 76 79 80 79 79 77 77 63 70 73 75 76 75 76 69 74 77 77 78 77 77 72 76 78 79 80 80 80 75 78 81 82 82 81 81 70 76 79 79 81 80 80 76 79 81 82 83 82 82 No regularization Regularization 0.1 ImageNet-1k, 300ep</cell><cell>0 2 -2 -4 -4 -2 2 2 2</cell><cell>45 48 49 49 49 48 47 48 48 48 47 46 45 45 45 47 48 48 48 47 47 48 48 48 47 47 45 46 36 35 33 46 47 47 46 46 45 44 46 45 45 44 43 42 41 No regularization Regularization 0.1 ImageNet-21k, 30ep</cell><cell>-1 0 -4 -3 -2 -1 -1 -1 -3</cell><cell>43 46 49 49 51 51 51 46 48 51 51 51 51 51 42 46 47 49 51 50 51 46 48 51 51 51 51 51 41 41 40 39 37 37 36 38 36 34 33 32 31 29 43 43 43 42 42 41 40 41 40 39 38 36 36 35 46 47 47 47 46 45 45 45 45 43 43 42 41 40 42 46 48 47 47 47 46 45 46 46 45 44 44 43 46 48 48 47 47 46 46 48 48 46 46 45 44 43 43 48 50 51 50 50 50 47 49 49 49 48 48 48 39 38 37 36 35 34 33 36 34 32 31 30 29 28 No regularization Regularization 0.1 ImageNet-21k, 300ep</cell></row><row><cell></cell><cell>none light1 light2 med1 med2 heavy1 heavy2 none light1 light2 med1 med2 heavy1 heavy2</cell><cell>reg -noreg</cell><cell>none light1 light2 med1 med2 heavy1 heavy2 none light1 light2 med1 med2 heavy1 heavy2</cell><cell>reg -noreg</cell><cell>none light1 light2 med1 med2 heavy1 heavy2 none light1 light2 med1 med2 heavy1 heavy2</cell><cell>reg -noreg</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Detailed VTAB results, including the "Mean" accuracy shown in Figure3. We show datasets under NATURAL, SPECIALIZED, STRUCTURED groups, following<ref type="bibr" target="#b39">[40]</ref>. .0 68.9 93.8 92.5 72.0 96.1 85.7 83.7 98.7 95.6 81.6 89.9 98.0 91.9 68.5 99.<ref type="bibr" target="#b6">7</ref> 83.2 82.0 26.5 65.9 77.0 R26+S/32 90.2 86.2 74.0 95.5 94.3 74.5 95.6 87.2 84.5 98.6 96.0 83.4 90.6 99.7 91.6 73.3 100 84.8 84.5 28.2 51.3 76.7 S/16 93.1 86.9 72.8 95.7 93.8 74.3 96.2 87.5 84.1 98.7 95.9 82.7 90.3 98.7 91.5 69.8 100 84.3 79.6 27.3 58.0 76.1 R50+L/32 90.7 88.1 73.7 95.4 93.5 75.6 95.9 87.6 85.8 98.4 95.4 83.1 90.7 99.8 90.4 71.1 100 87.5 82.4 23.5 53.0 76.0 B/16 93.0 87.8 72.4 96.0 94.5 75.3 96.1 87.9 85.1 98.9 95.7 82.5 90.5 98.1 91.8 69.5 99.9 84.5 84.0 25.9 53.9 76.0 L/16 91.0 86.2 69.5 91.4 93.0 75.3 94.9 85.9 81.0 98.7 93.8 81.6 88.8 94.3 88.3 63.9 98.5 85.1 81.3 25.3 51.2 73.5 ImageNet-21k (30ep) R+Ti/16 92.4 82.7 69.5 98.7 88.0 72.4 95.1 85.6 83.6 98.8 94.9 80.7 89.5 95.7 90.2 66.6 99.9 87.0 80.3 24.4 47.0 73.9 S/32 92.7 88.5 72.4 98.9 90.5 75.4 95.4 87.7 83.5 98.7 95.0 79.5 89.2 94.5 89.8 64.4 99.8 87.9 81.2 24.9 57.7 75.0 B/32 93.6 90.5 74.5 99.1 91.9 77.8 95.7 89.0 83.5 98.8 95.1 78.8 89.1 93.6 90.1 62.9 99.8 89.0 78.3 24.1 55.9 74.2 Ti/16 93.3 85.5 72.6 99.0 90.0 74.3 95.1 87.1 85.5 98.8 95.5 81.6 90.4 97.7 91.7 67.4 99.9 83.8 81.2 26.3 55.1 75.4 R26+S/32 94.7 89.9 76.5 99.5 93.0 79.1 95.9 89.8 86.3 98.6 96.1 83.1 91.0 99.7 92.0 73.4 100 88.7 84.8 26.2 53.3 77.3 S/16 94.3 89.4 76.2 99.3 92.3 78.1 95.7 89.3 84.5 98.8 96.3 81.7 90.3 98.4 91.5 68.3 100 86.5 82.8 25.9 52.7 75.8 R50+L/32 95.4 92.0 79.1 99.6 94.3 81.7 96.0 91.1 85.9 98.7 95.9 82.9 90.9 99.9 90.9 72.9 100 86.3 82.6 25.4 57.4 76.9 B/16 95.1 91.6 77.9 99.6 94.2 80.9 96.3 90.8 84.8 99.0 96.1 82.4 90.6 98.9 90.9 72.1 100 88.3 83.5 26.6 69.6 78.7 L/16 95.7 93.4 79.5 99.6 94.6 82.3 96.7 91.7 88.4 98.9 96.5 81.8 91.4 99.3 91.8 72.1 100 88.5 83.7 25.0 62.9 77.9 ImageNet-21k (300ep) R+Ti/16 93.2 85.3 71.5 99.0 90.3 74.7 95.2 87.0 85.2 98.3 95.3 81.3 90.0 95.5 90.5 67.4 99.9 87.4 78.2 24.5 45.2 73.6 S/32 93.2 89.7 75.3 99.2 92.0 78.1 96.1 89.1 84.0 98.5 95.4 80.6 89.6 96.9 88.7 68.1 100 91.0 79.6 26.2 55.0 75.7 B/32 95.2 92.3 77.2 99.5 92.8 81.2 96.6 90.7 87.0 98.8 96.0 81.3 90.8 97.7 89.8 70.5 100 92.3 82.7 25.9 83.1 80.2 Ti/16 93.7 87.2 73.1 99.2 91.0 77.3 95.7 88.2 86.0 98.5 95.8 81.9 90.6 98.3 89.7 70.8 100 86.0 82.6 26.8 49.9 75.5 R26+S/32 94.8 90.9 78.9 99.5 94.1 81.3 96.7 90.9 87.5 98.7 96.4 84.2 91.7 99.9 92.4 77.0 100 87.1 83.4 28.6 56.0 78.1 S/16 95.2 90.8 77.8 99.6 93.2 80.6 96.6 90.5 86.7 98.8 96.4 82.9 91.2 99.1 89.8 73.9 100 87.6 85.1 26.8 61.1 77.9 R50+L/32 95.7 93.9 81.6 99.5 94.9 83.6 97.1 92.3 85.8 98.7 96.7 84.2 91.3 100 92.0 76.8 100 87.2 85.2 26.8 61.8 78.7 B/16 96.0 93.2 79.1 99.6 94.7 83.0 97.0 91.8 87.4 98.7 96.8 83.5 91.6 99.7 89.0 76.0 100 86.7 85.7 28.3 68.2 79.2 L/16 95.5 94.1 80.3 99.6 95.0 83.4 97.4 92.2 86.4 99.0 96.6 83.3 91.3 99.8 91.7 75.6 100 90.4 84.7 27.5 76.5 80.8</figDesc><table><row><cell></cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>DTD</cell><cell>Flowers102</cell><cell>Pets</cell><cell>Sun397</cell><cell>SVHN</cell><cell>Mean</cell><cell>Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Retinopathy</cell><cell>Mean</cell><cell>Clevr-Count</cell><cell>Clevr-Dist</cell><cell>DMLab</cell><cell>dSpr-Loc</cell><cell>dSpr-Ori</cell><cell>KITTI-Dist</cell><cell>sNORB-Azim</cell><cell>sNORB-Elev</cell><cell>Mean</cell></row><row><cell>ImageNet-1k (300ep)</cell><cell cols="22">R+Ti/16 91.6 81.9 68.0 94.0 91.9 70.6 95.6 84.8 85.2 98.4 94.8 80.4 89.7 96.1 89.8 67.4 99.9 86.9 81.9 25.1 46.3 74.2 S/32 92.7 86.4 70.7 93.6 91.2 72.9 95.8 86.2 83.6 98.6 95.5 79.6 89.3 94.2 88.4 65.8 99.9 86.1 80.7 24.9 68.2 76.0 B/32 92.6 87.6 72.7 94.4 92.2 73.8 95.8 87.0 82.7 98.6 94.9 79.8 89.0 94.0 89.6 66.1 99.8 84.7 80.3 24.7 62.4 75.2 Ti/16 92.7 84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We release more than 50'000 ViT models trained under diverse settings on various datasets. We believe this to be a treasure trove for model analysis. Available at https://github.com/google-research/vision_ transformer and https://github.com/rwightman/pytorch-image-models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Not only do we further increase available AugReg settings, but we also sweep over other generally important training hyperparameters: learning-rate, weight-decay, and training duration, as described in Section A in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">As opposed to 3.3 where we specify weight decay values as typically defined in common frameworks, here the values are "decoupled" following<ref type="bibr" target="#b22">[23]</ref> that is better suited for sweeps; multiplying weight decay by the base learning-rate recovers the "coupled" value as used elsewhere.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Alexey Dosovitskiy, Neil Houlsby, and Ting Chen for many insightful feedbacks; the Google Brain team at large for providing a supportive research environment.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageNet top-1 accuracy [%]</p><p>Figure <ref type="figure">6</ref>: Imagenet transfer. Left: For every architecture and upstream dataset, we selected the best model by upstream validation accuracy. Main ViT-S,B,L models are connected with a solid line to highlight the trend, with the exception of ViT-L models pre-trained on i1k, where the trend breaks down. The same data is also shown in Table <ref type="table">3</ref>. Right: Focusing on small models, it is evident that using a larger patch-size (/32) significantly outperforms making the model thinner (Ti).</p><p>is beneficial to adapt all models. Thus, we conclude that selecting a single pre-trained model based on the upstream score is a cost-effective practical strategy and also use it throughout our paper. However, we also stress that if extra compute resources are available, then in certain cases one can further improve adaptation performance by fine-tuning additional pre-trained models.</p><p>A note on validation data for the ImageNet-1k dataset. While performing the above analysis, we observed a subtle, but severe issue with models pre-trained on ImageNet-21k and transferred to ImageNet-1k dataset. The validation score for these models (especially for large models) is not well correlated with observed test performance, see Figure <ref type="figure">5</ref> (left). This is due to the fact that ImageNet-21k data contains ImageNet-1k training data and we use a "minival" split from the training data for evaluation (see Section 3.1). As a result, large models on long training schedules memorize the data from the training set, which biases the evaluation metric computed in the "minival" evaluation set. To address this issue and enable fair hyper-parameter selection, we instead use the independently collected ImageNetV2 data <ref type="bibr" target="#b24">[25]</ref> as the validation split for transferring to ImageNet-1k. As shown in Figure <ref type="figure">5</ref> (right), this resolves the issue. We did not observe similar issues for the other datasets. We recommend that researchers transferring ImageNet-21k models to ImageNet-1k follow this strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Prefer increasing patch-size to shrinking model-size</head><p>One unexpected outcome of our study is that we trained several models that are roughly equal in terms of inference throughput, but vary widely in terms of their quality. Specifically, Figure <ref type="figure">6</ref> (right) shows that models containing the "Tiny" variants perform significantly worse than the similarly fast larger models with "/32" patch-size. For a given resolution, the patch-size influences the amount of tokens on which self-attention is performed and, thus, is a contributor to model capacity which is not reflected by parameter count. Parameter count is reflective neither of speed, nor of capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>The scope of this paper is limited to studying pre-training and transfer learning of Vision Transformer models and there already are a number of studies considering similar questions for convolutional neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. Here we hence focus on related work involving ViT models.</p><p>As first proposed in <ref type="bibr" target="#b9">[10]</ref>, ViT achieved competitive performance only when trained on comparatively large amounts of training data, with state-of-the-art transfer results using the ImageNet-21k and JFT-300M datasets, with roughly 13M and 300M images, respectively. In stark contrast, <ref type="bibr" target="#b33">[34]</ref> focused on tackling overfitting of ViT when training from scratch on ImageNet-1k by designing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A From-scratch training details</head><p>We present from-scratch training details for Ti/16 and B/16 models, on both Resisc45 and Pets37 datasets. We perform a grid search over the following parameters:</p><p>• B/32 on Pets37 -Epochs: {1k, 3k, 10k, 30k, 300k} -Learning rates: {1e−4, 3e−4, 1e−3, 3e−3} -Weight decays 3 : {1e−5, 3e−5, 1e−4, 3e−4} All these from-scratch runs sweep over dropout rate and stochastic depth in range {(0.0, 0.0), (0.1, 0.1), (0.2, 0.2)}, and data augmentation (l, m, α) in range { (0, 0,</p><p>}. The definition of (l, m, α) could be found from Section 3.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Finetune details</head><p>In Table <ref type="table">4</ref>, we show the hyperparameter sweep range for finetune jobs. We use the same finetune sweep for all the pre-trained models in this paper.  ImageNet-21k, 300ep</p><p>Figure <ref type="figure">7</ref>: The improvement or deterioration in validation accuracy when using or not using regularization (e.g. dropout and stochastic depth).</p><p>In Figure <ref type="figure">7</ref>, we show the gain (green, positive numbers) or loss (red, negative numbers) in accuracy when adding regularization to the model by means of dropout and stochastic depth. We did verify in earlier experiments that combining both with (peak) drop probability 0.1 is indeed the best setting.</p><p>What this shows, is that model regularization mainly helps larger models, and only when trained for long Specifically, for ImageNet-21 pre-training, it hurts all but the largest of models across the board.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sit: Self-supervised vision transformer</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Stéphane D'ascoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2005">2020. 1, 3, 4, 5</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<title level="m">Transformer in transformer</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flax: A neural network library and ecosystem for JAX</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Rondepierre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Van Zee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge. IIJCV</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2002">June 2014. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Smaller models and faster training</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnetv2</surname></persName>
		</author>
		<idno>CoRR, abs/2104.00298</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>CoRR, abs/2105.01601</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2008">2020. 1, 3, 4, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pytorch image models (timm): Vit training details</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models/issues/252#issuecomment-713838112" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2, 3, 6, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
