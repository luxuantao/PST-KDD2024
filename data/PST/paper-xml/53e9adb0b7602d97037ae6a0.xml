<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Frequent Graph Patterns with Differential Privacy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Entong</forename><surname>Shen</surname></persName>
							<email>eshen@ncsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">North Carolina State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">North Carolina State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">KDD&apos; 13</orgName>
								<address>
									<addrLine>August 11-14</addrLine>
									<postCode>2013</postCode>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Frequent Graph Patterns with Differential Privacy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F3154B956241B9DA376C3A56C798385</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Security and Protection Differential privacy; graph pattern mining</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phonecall graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. Differential privacy has recently emerged as the de facto standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns.</p><p>We first show that previous techniques on differentially private discovery of frequent itemsets cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Frequent graph pattern mining (FPM) is an important topic in data mining research. It has been increasingly applied in a variety of application domains such as bioinformatics, cheminformatics and social network analysis. Given a graph dataset D = {D1, D2, . . . , Dn}, where each Di is a graph, let gid(G) be the set of IDs of graphs in D which contain G as a subgraph. G is a frequent pattern if its count |gid(G)| (also called support) is no less than a user-specified support threshold f . Frequent subgraphs can help the discovery of common substructures, and are the building blocks of further analysis, including graph classification, clustering and indexing. For instance, discovering frequent patterns in social interaction graphs can be vital to understand functioning of the society or dissemination of diseases.</p><p>Meanwhile, publishing frequent graph patterns may impose potential threat to privacy, if the graph dataset contains private information of individuals. In many applications, each graph (rather than a node) is associated with an individual and may be sensitive. For example, the click stream during a browser session of a user is typically a sparse subgraph of the underlying web graph; in location-based services, a database may consist of a set of trajectories, each of which corresponds to the locations of an individual in a given period of time. Other scenarios of frequent pattern mining with sensitive graphs may include mobile phone call graphs <ref type="bibr" target="#b23">[23]</ref> and XML representation of profiles of individuals. Therefore, extra care is needed when mining and releasing frequent patterns in these graphs to prevent leakage of private information of individuals.</p><p>Recently, the model of differential privacy <ref type="bibr">[9]</ref> was proposed to restrict the inference of private information even in the presence of a strong adversary. It requires that the output of a differentially private algorithm is nearly identical (in a probabilistic sense), whether or not a participant contributes her data to the dataset. For the problem of frequent graph mining, it means that even an adversary who is able to actively influence the input graphs cannot infer whether a specific pattern exists in a target graph. Although tremendous progress has been made in processing flat data (e.g. relational and transactional data) in a differentially private manner, there has been very little work (discussed in Section 7) on differentially private analysis of graph data, due to the inherent complexity in handling the structural information in graphs.</p><p>In this paper we propose the first algorithm for privacy-preserving mining of frequent graph patterns that guarantees differential privacy. Recently several techniques <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17]</ref> have been proposed to publish frequent itemsets in a transactional database in a differentially private manner. It would seem attractive to adapt those techniques to address the problem of frequent subgraph<ref type="foot" target="#foot_0">1</ref> mining. Unfortunately, compared with private frequent itemset mining, the private FPM problem imposes much more challenges. First, graph datasets do not have a set of well-defined dimensions (i.e.,items), which is required by the techniques in <ref type="bibr" target="#b17">[17]</ref>. Second, counting graph patterns is much more difficult than counting itemsets (due to graph isomorphism), which makes the size of the output space not immediately available in our problem. This prevents us from applying the techniques in <ref type="bibr" target="#b3">[3]</ref>. We will explain the distinctions between <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17]</ref> and our work with more details in Section 2.3.</p><p>Contributions. The major contributions of this paper are summarized as follows:</p><p>1. For the first time, we introduce a differentially private algorithm for mining frequent patterns in a graph database. Our algorithm, called Diff-FPM, makes novel use of a Markov Chain Monte Carlo (MCMC) random walk method to bypass the roadblock of an output space with unknown size. This enables us to apply the exponential mechanism, which is an essential approach to achieving differential privacy.</p><p>2. Our approach provides provable privacy and utility guarantee on the output of our algorithm. We first show that our algorithm gives (ε, δ)-differential privacy, which is a relaxed version of ε-differential privacy. We then show that when the random walk has reached its steady state, Diff-FPM gives εdifferential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>In order to propose a neighboring pattern more efficiently in MCMC sampling, we develop optimization techniques that significantly reduce the number of invocations to the subgraph isomorphism test subroutine. Experiment shows that our techniques can reduce the time to propose a neighboring pattern by an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We conduct an extensive experimental study on the effectiveness and efficiency of our algorithm. With moderate amount of privacy budget, Diff-FPM is shown to output private frequent graph patterns with at least 80% precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frequent Graph Pattern Mining</head><p>Frequent graph pattern mining (FPM) aims at discovering the subgraphs that frequently appear in a graph dataset. Formally, let D = {D1, D2, . . . , Dn} be a sensitive graph database which contains a multiset of graphs. Each graph Di ∈ D has a unique identifier that corresponds to an individual. Let G = (V, E) be a (sub)graph pattern, the graph identifier set gid(G) = {i : G ⊆ Di ∈ D} includes all IDs of graphs in D that contain a subgraph isomorphic to G. We call |gid(G)| the support of G in D. The FPM algorithm can be defined either as returning all subgraph patterns whose supports are no less than a user-specified threshold f , or as returning the top k frequent patterns given an integer k as input. One can easily convert one version to the other. All graphs we consider in this paper are undirected, connected and labeled. Note that each node has a label and multiple nodes can have the same label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differential Privacy</head><p>Differential privacy <ref type="bibr">[9]</ref> is a recent privacy model which provides strong privacy guarantee. Informally, a data mining or publishing procedure is differentially private if the outcome is insensitive to any particular record in the dataset. In the context of graph pattern mining, let D, D be two neighboring datasets, i.e., D and D differ in only one graph (by adding or removing an individual), written as ||D -D || = 1. Let D n be the space of graph datasets containing n graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 1 (ε-DIFFERENTIAL PRIVACY).</head><p>A randomized algorithm A is ε-differentially private if for all neighboring datasets D,D ∈ D n , and any set of possible output O ⊂ Range(A):</p><formula xml:id="formula_0">Pr[A(D) ∈ O] ≤ e ε Pr[A(D ) ∈ O].</formula><p>The parameter ε &gt; 0 allows us to control the level of privacy. A smaller ε suggests more limit posed on the influence of a single graph. Typically, the value of ε should be small (ε &lt; 1). ε is usually specified by the data owner and referred as the privacy budget. In section 5.1 our discussion is related to a weaker notion called (ε, δ)-differential privacy <ref type="bibr" target="#b8">[8]</ref>, which allows a small additive error factor of δ.</p><p>DEFINITION 2 ((ε, δ)-DIFFERENTIAL PRIVACY). A randomized algorithm A is (ε, δ)-differential private if for all neighboring datasets D,D ∈ D n , and any set of possible output O ⊂ Range(A):</p><formula xml:id="formula_1">Pr[A(D) ∈ O] ≤ e ε Pr[A(D ) ∈ O] + δ.</formula><p>A popular technique in applying differential privacy is the Laplace mechanism <ref type="bibr">[9]</ref>, which adds noise following Laplace distribution to the numeric output of a function. Applying the Laplace mechanism in our problem means adding noise to the support of all possible patterns and selecting the patterns with the highest noisy supports. However, this would be infeasible since it is computationally prohibitive to enumerate all possible patterns in any non-trivial sized graph mining problem. Exponential Mechanism. A general technique of applying differential privacy is the exponential mechanism <ref type="bibr" target="#b20">[20]</ref>. It not only supports non-numeric output but also captures the full class of differential privacy mechanisms. The exponential mechanism considers the whole output space and assumes that each possible output is associated with a real-valued utility score. By sampling from a distribution where the probability of the desired outputs are exponentially amplified, the exponential mechanism (approximately) finds the desired outputs while ensuring differential privacy.</p><p>Formally, given input space D n and output space X , a score function u : D n × X → R assigns each possible output x ∈ X a score u(D, x) based on the input D ∈ D n . The mechanism then draws a sample from the distribution on X which assigns each x a probability mass proportional to exp(εu(D, x)/2∆u), where ∆u = max ∀x,D,D |u(D, x) -u(D , x)| is the sensitivity of the score function. Intuitively, the output with a higher score is exponentially more likely to be chosen. It is shown that this mechanism satisfies ε-differential privacy <ref type="bibr" target="#b20">[20]</ref>. gives ε-differential privacy.</p><p>The exponential mechanism has been shown to be a powerful technique in finding private medians <ref type="bibr" target="#b6">[6]</ref>, mining private frequent itemset <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17]</ref> and more generally adapting a deterministic algorithm to be differentially private <ref type="bibr" target="#b22">[22]</ref>. Our Diff-FPM algorithm works by carefully applying the exponential mechanism. In this process we must overcome several critical challenges, which are identified next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges and Strategies</head><p>There has been work <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17]</ref> on mining frequent itemsets in a transaction dataset under differential privacy. However, the shift from transactions to graphs poses significant new challenges. In <ref type="bibr" target="#b17">[17]</ref>, transaction datasets are viewed as high-dimensional tabular data, and the proposed approach projects the input database onto lower dimensions. However, graph datasets do not have a well defined set of items, i.e., dimensions, which renders the approach in <ref type="bibr" target="#b17">[17]</ref> inapplicable in our FPM problem. In <ref type="bibr" target="#b3">[3]</ref>, two methods are proposed which make use of a notion of truncated frequency. However, those methods cannot be used in our problem due to the following fundamental challenges: Support Counting. Obtaining the support of a graph pattern is much more difficult than counting itemsets. An itemset pattern can be represented by an ordered list or a bitmap of item IDs Checking the existence of an itemset in a transaction only takes O(1) time, while checking whether a subgraph pattern exists in a graph is NPcomplete due to subgraph isomorphism.</p><p>Unknown Output Space. The output space X in our problem contains a finite number of graph patterns which may or may not exist in the input dataset. Under differential privacy, any pattern in the output space should have non-zero probability to be in the final output. The probability of sampling a pattern x from the output space is</p><formula xml:id="formula_2">π(x) = exp(εu(x)/2∆u) C ,<label>(1)</label></formula><p>where C = x∈X exp(εu(x)/2∆u) is the normalizing constant according to Theorem 1. The most straightforward way to compute C requires enumerating all the patterns in the output space. In <ref type="bibr" target="#b3">[3]</ref>, a technique is proposed to apply the exponential mechanism without enumerating if the size of the output space is known. However, unlike <ref type="bibr" target="#b3">[3]</ref>, in which the output space size can be obtained by simple combinatorics (i.e., m l patterns of size l given an alphabet of size m), the size of the output space X in our problem is not immediately available (due to graph isomorphism<ref type="foot" target="#foot_1">2</ref> ), which prohibits us from applying exponential mechanism directly. Therefore we cannot apply the same techniques as in <ref type="bibr" target="#b3">[3]</ref>.</p><p>Given the analysis above, we need to develop new ways to overcome the issue of an unknown |X |. Note that although the global information on the output space is not accessible, we do have the local information on any specific pattern -given any pattern x, we can immediately calculate its utility score u(x). In addition, the unknown normalizing constant C is common to all patterns. That is, given any pair of patterns x1, x2, the ratio of probability mass π(x1)/π(x2) is available without knowing the exact probabilities, according to Eq.(1). Such scenarios, where one needs to draw samples from a probability distribution known up to a constant factor, also arise in statistical physics when analyzing dynamic systems, where Markov Chain Monte Carlo (MCMC) methods are often used. Inspired by that, our idea is to perform a random walk based on locally computed probabilities. By carefully choosing the neighbor and the probability of moving in each step using the Metropolis-Hastings (MH) method <ref type="bibr" target="#b24">[24]</ref>, the random walk will converge to the target distribution, from which we can output samples. Next we discuss the details of our Diff-FPM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIVATE FPM ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The key challenge of handling graph datasets is the unknown output space when applying the exponential mechanism. The Diff-FPM algorithm meets the challenge by unifying frequent pattern mining and applying differential privacy into an MCMC sampling framework. The main idea of Diff-FPM is to simulate a Markov chain by performing an MCMC random walk in the output space. Our goal is that when the random walk reaches its steady state, the stationary distribution of the Markov chain matches the target distribution π in Eq.( <ref type="formula" target="#formula_2">1</ref>). In Section 3.2.1 we will explain in detail how to apply the Metropolis-Hastings (MH) method in our problem to achieve this goal. Before that, we need to define the state space in which we perform the random walk.</p><p>Partial Order Full Graph. To facilitate the MH-based random walk in the output space, we define the Partial Order Full Graph (POFG) as the state space of the Markov chain on which the sampling algorithm run the simulation. Each node in POFG corresponds to a unique graph pattern and each edge in POFG represents a possible 'extension' (add or remove one edge) to a neighboring pattern. Naturally, each node in the POFG has three types of neighbors: sub-neighbor (by removing an edge), super-backward neighbor (by connecting two existing nodes) and super-forward neighbor (by adding and connecting to a new node). EXAMPLE 1. Figure <ref type="figure" target="#fig_0">1</ref> shows a simple graph dataset containing 3 graphs and its POFG. The dashed patterns have support smaller than 2 in the dataset. Pattern A -A -C has two subneighbors, one super-backward neighbor and several super-forward neighbors (only one shown in Figure <ref type="figure" target="#fig_3">1(b)</ref>). Self-loops and multiedges are not considered in this example and thus are excluded from the output space.</p><p>At a higher level, the random walk starts with an arbitrary pattern and proceeds to an adjacent pattern with certain probability in each step. Since the transition decision is made solely based on local information (related to the neighborhood of the current pattern), there is no need to construct the global POFG explicitly. When the random walk has reached its steady state, the probability of being in state x follows exactly the target distribution π(x) in Eq.( <ref type="formula" target="#formula_2">1</ref>). Then the current state is drawn as a sampled pattern. Since the frequent patterns have larger probabilities in the target distribution, they are more likely to appear in the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detailed Descriptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Diff-FPM Algorithm</head><p>The core of the Diff-FPM algorithm is a careful application of the MH method. The MH method is a Markov Chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a target probability distribution for which direct sampling is difficult. It only requires that a function proportional to the probability mass be calculable.</p><p>Suppose we want to generate a random variable X taking values in X = {x1, . . . , x |X | }, according to a target distribution π, with</p><formula xml:id="formula_3">π(xi) = b(xi) C , xi ∈ X</formula><p>where all b(xi) are strictly positive, |X | is large, and the normal-</p><formula xml:id="formula_4">izing constant C = |X | i=1 b(xi) is difficult to calculate.</formula><p>The MH method first constructs an |X |-state Markov chain {Xt, t = 0, 1, . . . } on X whose evolution relies on an arbitrary proposal transition matrix Q = q(x, y) in the following way:  where αxy = min π(y)q(y,x) π(x)q(x,y) , 1 = min b(y)q(y,x) b(x)q(x,y) , 1 . It means that given a current state x, the next state is proposed according to the proposal distribution Q. q(x, y) is the probability mass of state y among all possible states given the current state is x. With probability αxy, the proposal is accepted and the chain moves to the new state y. Otherwise it remains at state x. It follows that {Xt, t = 0, 1, . . . } has a one-step transition probability matrix P :</p><formula xml:id="formula_5">1. When Xt = x, generate a random variable Y satisfying P (Y = y) = q(x, y), y ∈ X 2. Given Y = y, let<label>Xt+1</label></formula><formula xml:id="formula_6">A D A C A C A A D A C D A A A C A D A B B C C A A A C D A A A A D A C … … … … … … … … ∅ (b) Part of POFG of Figure 1(a)</formula><formula xml:id="formula_7">P (x, y) = q(x, y)αxy, if x = y 1 -z =x q(x, z)αxz, if x = y</formula><p>It can be shown that for the above P , the Markov chain is reversible and has a stationary distribution π, equal to the target distribution. Therefore, once the chain has reached the steady state, the sequence of samples we get from the MH method should follow the target distribution. EXAMPLE 2. Consider a random walk on the POFG illustrated in Figure <ref type="figure" target="#fig_3">1(b)</ref>. Suppose the current state of the walk is 'A-A-D' (pattern x). Following the MH method, one of pattern x's neighbors needs to be proposed according to a proposal distribution q(x, y). For simplicity, in this example each neighbor has an equal probability to be proposed, i.e., q(x, y)</p><formula xml:id="formula_8">= 1/|N (x)|, where N (x) is the neighbor set of x. Assuming 'A-D' (pattern y) is pro- posed and |N (x)| = 5, |N (y)| = 10, b(•) = exp(|gid(•)|/2),</formula><p>the probability of accepting the proposal is calculated as αxy = min{ exp(3/2)•(1/10) exp(2/2)•(1/5) , 1} = 0.82. We can then draw a random number between 0 and 1 to decide whether walking to pattern y or staying at x.</p><p>The description of the Diff-FPM algorithm above can be summarized in Algorithm 1. The input consists of the raw graph dataset D, a support threshold f and the privacy budget ε = ε1 + ε2. If the top-k frequent patterns are desired, we first run non-private FPM algorithms such as gSpan <ref type="bibr" target="#b29">[29]</ref> to get the support threshold f , i.e., the support of the kth frequent pattern. If one only needs k patterns whose supports are no less than a threshold, f can be directly provided to the algorithm. At a higher level, Algorithm 1 consists  Initially, we select an arbitrary pattern in the output space to start the walk (Line 2). At each step, we propose a neighboring pattern y of the current pattern x according to a proposal distribution (Line 4). The proposal distribution does not affect the correctness of the MH method, so we defer the details to Section 3.2.3. The proposed pattern is then accepted with probability αxy as in the MH-algorithm (Line 5), where u(•) is the score function with ∆u being the sensitivity of u(•). We explore the design space of the score function in the next paragraph. When the Markov chain has converged (see Section 3.3 for convergence diagnostic), we output the current pattern and remove it from the output space (Line 6 to 8). We then start a new walk until k patterns have been sampled. Finally, if one wants to include the support of each output pattern as well, the count of each pattern is perturbed by adding Lap(k/ε2) noise (Line 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Score Function Design</head><p>Choosing the utility score function is vital in our approach as it directly affects the target distribution. A general guideline is that the patterns with higher supports should have higher utility scores in order to have larger probabilities to be chosen according to exponential mechanism. Under this guideline, given an input database D, the most straightforward choice is to let u(x, D) = |gid(x)| for any pattern x. In this case, the sensitivity ∆u is exactly 1 since the support of any subgraph pattern may vary by at most 1 with the addition or removal of a graph in the dataset. This is also the score function we use in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Proposal Distribution</head><p>Although in theory the proposal distribution can be arbitrary, it can significantly impact the efficiency of the MH method by affecting the mixing time (time to reach steady state). A good proposal distribution can improve the convergence speed by increasing the accept rate αxy in the MH method. On the contrary, if the proposed pattern is often rejected, the chain can hardly move forward. It has been suggested that one should choose a proposal distribution close to the target distribution <ref type="bibr" target="#b11">[11]</ref>. In our problem setting, it is preferable to make a distinction between the patterns having support no less than f (referred as frequent patterns) and those whose supports are lower (referred as infrequent patterns). Given a current state x, we denote the set of frequent neighbors of x as N1(x) and the set of infrequent neighbors as N2(x). Since |N2(x)| is usually larger than |N1(x)|, we will balance the probability mass assigned to N1(x) and N2(x) by introducing a tunable parameter η (0 &lt; η &lt; 1). Our heuristic based proposal distribution is formally described below:</p><formula xml:id="formula_9">Q(x, y) = η × 1 |N 1 (x)| , if y ∈ N1(x) (1 -η) × 1 |N 2 (x)| , if y ∈ N2(x)<label>(2)</label></formula><p>In the experiment we use η &gt; 0.5 such that a frequent pattern has a higher probability to be proposed than an infrequent pattern. If any of N1(x) or N2(x) is empty, its probability mass will be redistributed (by setting η = 0 or η = 1 respectively). Note that the choice of the proposal distribution does not impact the privacy and utility guarantee of Diff-FPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Pattern Removal</head><p>In line 6 to 8 of Algorithm 1, after the convergence conditions are met and a sample pattern g is outputted, we need to exclude g from the output space by connecting g's neighbors and removing g in the POFG. In our implementation this is done by replacing g by all the neighbors of g whenever g appears in some pattern's neighborhood. Note that we do not output multiple patterns when the chain has converged. This is because once a pattern is sampled, it should be excluded from the output space and thus have zero probability to be chosen. Therefore adjustment to the output space is necessary after each sample. For the same reason we do not run multiple chains at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convergence Diagnostics</head><p>The theory of MCMC sampling requires that samples are drawn when the Markov chain has converged to the stationary distribution, which is also our target distribution π. The most straightforward way to diagnose convergence is to monitor the distance between the target distribution π and the distribution of samples π. In practice, however, π is often known only up to a constant factor. To deal with this problem, several online diagnostic tests have been developed in the MCMC literature <ref type="bibr" target="#b11">[11]</ref> and used in random walk based sampling of graphs <ref type="bibr" target="#b12">[12]</ref>.</p><p>Online diagnostics rely on detecting whether the chain has lost its dependence on the starting point. We adopt a standard convergence test called the diagnostic <ref type="bibr" target="#b10">[10]</ref>. The Geweke diagnostic takes two non-overlapping parts (usually the first 0.1 and last 0.5 proportions) of the Markov chain and see if they are from the same distribution. Specifically, let X be a sequence of samples of our metric of interest and X1, X2 be the two non-overlapping subsequences. Geweke computes the Z-score:</p><formula xml:id="formula_10">Z = E(X 1 )-E(X 2 ) √ V ar(X 1 )+V ar(X 2 )</formula><p>.</p><p>With increasing number of iterations, X1 and X2 should move further apart and become less and less correlated. When the chain has converged, X1 and X2 should be identically distributed with Z ∼ N (0, 1) by law of large numbers. We can declare convergence when Z has continuously fallen in the [-1, 1] range. Since the samples in our problem are graph patterns rather than a scalar, we may need to monitor multiple scalar metrics related to different properties of the sampled pattern and declare convergence when all these metrics have converged.</p><p>We need to acknowledge that these convergence diagnostic tools from the MCMC literature are heuristic per se. Verifying the convergence remains an open problem if the distribution of samples is not directly observable. Even so, Diff-FPM still achieves (ε, δ)differential privacy if there exists a small distance between the target and simulation distributions, as we will show in Lemma 2 in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EFFICIENT EXPLORATION OF NEIGH-BORS (EEN)</head><p>We have discussed so far the core of the Diff-FPM algorithm and seemingly it could be run straightforwardly. However, without certain optimization, the computation cost might render the algorithm impractical to run. The most costly operation in the Diff-FPM algorithm is proposing a neighbor of the current pattern x. According to the proposal distribution in Eq.2, this requires knowledge on the support of each pattern in x's neighbors N (x). Due to the fact that subgraph isomorphism test is NP-complete, obtaining the support of each neighbor might become a computation bottleneck.</p><p>To overcome this problem, we have developed an efficient algorithm (called EEN) to explore the neighborhood of a pattern by observing the connection between neighboring patterns and their isomorphic mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The EEN Algorithm</head><p>The task of neighbors exploration can be described as: given a pattern x, find the set of frequent neighbors N1(x) and infrequent neighbors N2(x), as in the proposal distribution (Eq.2). A naive way to populate N1(x) and N2(x) is to test each neighbor of x against the graph dataset D. However, this is extremely inefficient since |N (x)| • |D| isomorphism tests are required, where |D| is the number of graphs in D. A basic optimization would be using the monotonic property of frequent patterns: if x is a frequent pattern, any subgraph of x should be frequent too; likewise, an infrequent pattern's super-graph must be infrequent. However, explicit isomorphism testing is still required for exploring the super-neighbors of x if x is frequent or x's sub-neighbors if x is infrequent.</p><p>The EEN algorithm is able to further reduce the number of isomorphism tests. Observing that x and y only differ in one edge for all y ∈ N (x), the main idea of is to re-use the isomorphic mappings between x and Di ∈ D and examine whether any of the isomorphic mappings can be retained after extending an edge. The EEN algorithm is formally presented in Algorithm 2 and is described in the following.</p><p>Algorithm 2 takes pattern x, graph dataset D and support threshold f as input and returns N1(x) and N2(x). First, pattern x is tested against each graph in D and the result is stored in Bx = {i|x ⊆ Di, Di ∈ D}, which is the set of IDs of graphs containing pattern x (line 2). The subgraph isomorphism algorithm we use is the VF2 algorithm <ref type="bibr" target="#b5">[5]</ref>. Next we populate three types of neighbors of x: sub-neighbors N b , super-back neighbors N p back and superforward neighbors N p f wd (line 3), and handle them differently.</p><p>Explore sub-neighbors (line 4 to 7). For N b , if x is frequent, the entire set N b should be frequent. If x is infrequent, each pattern in N b is examined by the boolean sub-procedure SUB_IS_FREQ. SUB_IS_FREQ takes a sub-neighbor x of x and Bx as input and returns whether x is frequent. First we find BE = e∈x Be, the intersection of ID sets of all edges in pattern x . Then subgraph isomorphism test is only needed for the graphs Di ∈ BE\Bx.</p><p>The set of IDs of graphs that succeed the test together with Bx comprise B x . Finally the procedure returns the frequentness of x by comparing f and the size of B x .</p><p>Explore super-back neighbors (line 8 to 22). For N p back , if x is infrequent, the entire N p back must be infrequent. Otherwise, we test whether x ∈ N p back is a subgraph of Di for each Di. In this part, the EEN algorithm does not require any additional subgraph isomorphism test at all. This is achieved by re-using the isomorphism mappings between the base pattern x and Di and reasoning upon that. In line 12 we find all the subgraph isomorphism mappings M : V n x → V n D i , which can be obtained at the same time when computing Bx in line 2 as part of the VF2 algorithm. Note that the subgraph isomorphism package we use is complete, i.e., it can return all the mappings. Suppose x is extended to x by connecting node u and v (line 15). If any of the isomorphism mappings m ∈ M is preserved with the edge extension (i.e., m(u) and m(v) are adjacent in Di), x must be a subgraph of Di. Otherwise if none of the mappings can be preserved, x is not a subgraph of Di.</p><p>In the above process, we use a dictionary H to keep track of the number of graphs in D so far that contains x as a subgraph, i.e., H[x ] maintains |{Di|x ⊆ Di}| for the Di tested so far. Line 14 ensures that the isomorphism extension test is only performed when H[x ] has not reached f . Explore super-forward neighbors. For N p f wd , the algorithm is similar to the procedures of exploring super-back neighbors, except that the extension test is now on a forward edge instead of a back edge. The details are available in <ref type="bibr" target="#b26">[26]</ref> due to space limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PRIVACY AND UTILITY ANALYSIS</head><p>The proof of the lemmas and theorems in this section can be found in <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Privacy Analysis</head><p>In this part we establish the privacy guarantee of Diff-FPM. We show both the sampling and perturbation phases preserve privacy, and then we use the composition property of differential privacy to show the privacy guarantee of the overall algorithm.</p><p>In the sampling phase, our target probability distribution π(D, •) equals exp(ε 1 u(D,•)/2k∆u) C for a given dataset D. If samples were drawn directly from this distribution, it would achieve strict ε 1 kdifferential privacy due to the exponential mechanism. Since we use MCMC based sampling, the distribution of the samples π(D, •) will approximate π(D, •), i.e. the two distributions are asymptotically identical. In real simulation, there may be a small distance between the two distributions. To quantify the impact on privacy when a small error is present, we use the total variation distance <ref type="bibr" target="#b24">[24]</ref> to measure the distance of the two distributions at a given time:</p><formula xml:id="formula_11">||π(•) -π(•)||T V ≡ max T ⊂X |π(T ) -π(T )|<label>(3)</label></formula><p>which is the largest possible difference between the probabilities that π(•) and π(•) can assign to the same event.</p><p>Let A(D) denote the process of sampling one pattern according to Algorithm 1 (Line 4 to 8). The privacy guarantee that A(D) offers is described by the following lemma: LEMMA 2. Let π(•) and π(•) denote the target distribution and the distribution of samples from A(D) respectively. Suppose ||π(•)-</p><formula xml:id="formula_12">π(•)||T V ≤ θ, procedure A(D) gives ( ε 1 k , δ)-differential privacy, where δ = θ(1 + e ε 1 /k ).</formula><p>Note that θ is a function of simulation time t. The following lemma describes the asymptotic behavior and the speed of convergence of the chain : LEMMA 3. <ref type="bibr" target="#b24">[24]</ref> If a Markov chain on a finite state space is irreducible and aperiodic, and has a transition kernel P and stationary distribution π(•), then for x ∈ X ,</p><formula xml:id="formula_13">||P t (x, •) -π(•)||T V ≤ M ρ t , t = 1, 2, 3, . . .<label>(4)</label></formula><p>for some ρ &lt; 1 and M &lt; ∞. And</p><formula xml:id="formula_14">lim t→∞ ||P t (x, •) -π(•)||T V = 0<label>(5)</label></formula><p>It means θ is decreasing at least at a geometric speed and approximates to zero when the simulation is running long enough.</p><p>Since the sampling process in Algorithm 1 consists of k successive applications of exponential mechanism based on random walk, we need the following well-known composition lemma to provide privacy guarantee for the entire sampling phase. LEMMA 4. <ref type="bibr" target="#b19">[19]</ref> Let A1, . . . , At be t algorithms such that Ai satisfies εi-differential privacy, 1 ≤ i ≤ t. Then their sequential composition A1, . . . , At satisfies ε-differential privacy, for ε = t i=1 εi. Equipped with the results in previous lemmas, we are able to provide the privacy guarantee for Algorithm 1. THEOREM 5. Algorithm 1 satisfies ε-differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Utility Analysis</head><p>Because neighboring inputs must have similar output under differential privacy, a private algorithm usually does not return the exact answers. In the scenario of mining top-k frequent patterns, the Diff-FPM algorithm returns a noisy list of patterns which is close to the real top-k patterns. To quantify the quality of the output of Diff-FPM, we first define two utility parameters, following <ref type="bibr" target="#b3">[3]</ref>. Recall that f is the support of the kth frequent pattern, and let β be an additive error to f . Given 0 &lt; γ &lt; 1, we require that with probability at least 1 -γ, (1) no pattern in the output has true support less than f -β and (2) all patterns having support greater than f + β exist in the output. The following theorems provide the utility guarantee of Diff-FPM. A score function u(x) = |gid(x)| is assumed. THEOREM 6. At the end of the sampling phase in Algorithm 1, for all 0 &lt; γ &lt; 1, with probability at least 1 -γ, all patterns in set S have support greater than f -β, where β = 2k ε 1 (ln(k/γ)+ln M ) and M is an upper bound on the size of output space.</p><p>The following theorem provides the upper bound of noise added to the true support of each output pattern. THEOREM 7. For all 0 &lt; γ &lt; 1, with probability of at least 1 -γ, the noisy support of a pattern differs by at most β, where β = k ε 2 ln(1/γ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL STUDY</head><p>In this section, we evaluate the performance of Diff-FPM through extensive experiments on various datasets. Since this is the first work on differentially private mining of frequent graph patterns, the quality of the output is compared with the result from a nonprivate FPM algorithm and the accuracy is reported. In this section we consider the scenario of mining the top-k frequent patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setup</head><p>Datasets. The following three datasets are used in our experiment: DTP is a real dataset containing DTP AIDS antiviral screening dataset <ref type="foot" target="#foot_2">3</ref> , which is frequently used in frequent graph pattern mining study. It contains 1084 graphs, with an average graph size of 45 edges and 43 vertices. There are 14 unique node labels and all edges are considered having the same label.</p><p>The click dataset consists of 20K small tree graphs (4 nodes and 3 edges on average) obtained by a graph generator developed by Zaki <ref type="bibr" target="#b30">[30]</ref>. To a certain extent, this synthetic dataset simulates user click graphs from web server logs <ref type="bibr" target="#b30">[30]</ref>, which is a suitable type of data requiring privacy-preserving mining. All the tree graphs in this dataset are sampled from a master tree.</p><p>The above two datasets contain graphs that are relatively sparse. To test our algorithm on dense graphs, we also use a dataset containing 5K graphs, in which the average node degree is 7. Each graph contains 10 vertices and 35 edges on average. The graph generator <ref type="bibr" target="#b4">[4]</ref> we use is specially designed for generating graph datasets for evaluation of frequent subgraph mining algorithms. The size of this graph dataset is comparable to the largest datasets used in previous works <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b15">15]</ref>. Utility metrics. We evaluate the quality of the output of Diff-FPM by employing the following three utility metrics: Precision, Support Accuracy and nDCG<ref type="foot" target="#foot_3">4</ref> . Precision is defined as the fraction of identified top-k graph patterns that are in the true top-k, i.e., P recision = |True Positives|/k. This is the complementary measure of the false negative rate used in <ref type="bibr" target="#b3">[3]</ref>. The true top-k patterns are obtained by a non-private graph mining algorithm (gSpan <ref type="bibr" target="#b29">[29]</ref> in our experiment). The measure of precision reflects the percentage of desired/undesired patterns in the output, yet it cannot indicate how good or bad the output patterns are in terms of their supports. For example, if f = 1000, it is much more undesirable if a pattern with support 10 appears in the output compared to a pattern with support 980, even though the precision may be the same in these two cases. We first define the relative support error (RSE) as RSE = (Strue -Sout)/kf , where Strue and Sout are the sum of the supports of the real top-k patterns and sum of the supports of the sampled patterns respectively. This measure reflects the average deviation of an output pattern's support with respect to the support threshold f . In the plots, the support accuracy is reported, which equals 1 -RSE. nDCG is a commonly used metric to compare two ranked lists. This metric is accumulated from the top of the result list to the bottom with the weight of each result discounted at lower ranks. In our problem setting, the top-k patterns are un-ordered. Still, nDCG is able to reveal whether any important pattern is missing in the output.</p><p>All experiments were conducted on a PC with 3.40GHz CPU with 8GB RAM. The random walk in the Diff-FPM algorithm has a small memory footage due to its Markovian nature. We implemented our algorithm in Python 2.7 with the JIT compiler PyPy<ref type="foot" target="#foot_4">5</ref> to speed up. The default parameters of ε = 0.5, η = 0.8 and k = 15 were used unless specified otherwise. In the experiment we do not release the noisy supports of the patterns in the output (line 9 in Algorithm 1), so all the privacy budget is used in the sampling phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment Results</head><p>Comparison of neighbor exploration methods. In Section 4.1 we proposed the EEN algorithm to efficiently explore the neighborhood of a pattern. We now compare it with two other methods: a naive approach which finds the support of each neighbor of the current pattern x and a basic approach which uses the monotonic property of frequent patterns (see Section 4.1). Figure <ref type="figure">2(a)</ref> shows the average iteration time in logarithm of the three methods over three datasets. In each iteration, a neighboring pattern is proposed and then accepted or rejected according to the MH algorithm. Clearly, EEN takes significantly less time in each iteration than the other methods in both datasets, reducing the iteration time by at least an order of magnitude compared to the naive approach. Thus all subsequent results are presented with EEN enabled.</p><p>Run time and scalability. Figure <ref type="figure">2(d)</ref> illustrates the average time taken to output one frequent pattern as the size of the dataset increases. For the full datasets, click takes 20 seconds, DTP takes about 1 minute and dense sits in the middle, although the click dataset contains 20K graphs compared to only 1K in the DTP. It indicates that the size of each individual graph and the size of the neighborhood have a larger impact on the run time than the total number of graphs in the dataset (note that DTP has 14 labels and thus a larger neighborhood of a pattern compared to dense). For scalability, all datasets are observed to have linear scale-up in time as the size of graph dataset increases.</p><p>Utility result. To test the quality of the output by Diff-FPM, we examine the utility metrics introduced above under various parameter settings.</p><p>First, Figure <ref type="figure">2</ref>(b) and Figure <ref type="figure">2</ref>(c) show the precision and SA when we increase the size of the graph dataset from 10% to 100% <ref type="foot" target="#foot_5">6</ref> . An increasing trend of the output quality can be clearly observed here. This is in line with our expectation because achieving differential privacy is more demanding in a small dataset -the larger the number of records in the database, the easier it is to hide an individual record's impact on the output. For all three full datasets, Diff-FPM is able to achieve at least 80% on both precision and SA.  are from the real top-k patterns for DTP and dense. This is inevitable due to the privacy-utility tradeoff. As more privacy budget is given, the precision of Diff-FPM increases fast. At ε = 0.5, the precisions from all datasets have reached 80%. Further increase in privacy budget does not provide significant benefit on the precision. We observed a similar trend in the support accuracy plot (Figure <ref type="figure">3(b)</ref>), with less dramatic changes for ε from 0.1 to 0.5.</p><p>Figures <ref type="figure">3(c</ref>) and 3(d) illustrate the impact of the number of patterns in the output. Recall that in each round of sampling, a budget of ε/k is consumed (cf. proof of Theorem 5). Given a certain privacy budget, the more patterns to output, the less privacy budget each sample can use. Thus we expect the average quality of the output to drop as k increases, which is confirmed in the result. Meanwhile, the support accuracy of the output holds well with the increasing number of output, which can be seen in Figure <ref type="figure">3(d)</ref>.</p><p>We also report the nDCG of the output with respect to different privacy levels in Figure <ref type="figure" target="#fig_6">4</ref>. It can be seen that given moderate amount of privacy budget, the nDCG of the output remains larger than 0.8, suggesting close resemblance (especially on the several most frequent patterns) between the true top-k and the top-k we found.</p><p>Convergence analysis. A decision we have to make is when to stop the random walk and output a sample. In Section 3.3 we in- troduced Z-score based Geweke diagnostic, which compares the distribution at the beginning and end of the chain. Since MCMC is typically used to estimate a function of the underlying random variable instead of structural data like graphs, we need to choose some properties of the patterns which we will monitor using the Geweke test. The three metrics we use in the experiment are the number of neighbors N (x), the number of frequent neighbors N1(x) and the number of nodes in the pattern |x|. Figure <ref type="figure" target="#fig_7">5</ref> shows the convergence traces of a sample run with K = 20 and ε = 0.5 on the DTP dataset. Each curve corresponds to the Z-score of a chain over the number of iterations. It can be seen that the Markov chain we design has pretty fast convergence rate thanks to the tuning of the proposal distribution. For each chain, convergence is declared when the Z-scores of all three metrics have fallen within the [-1, 1] range for 20 iterations continuously. In Figure <ref type="figure" target="#fig_7">5</ref>, this happens around 150 iterations for most chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Data Mining with Differential Privacy. There exist two approaches to differentially private data mining. In the first approach, the data owner releases an anonymized version of the dataset under differential privacy. And the user has the freedom of conducting any data mining task on the anonymized dataset. We call this the 'publishing model'. Examples include releasing anonymized version of contingency tables <ref type="bibr" target="#b28">[28]</ref>, data cubes <ref type="bibr" target="#b7">[7]</ref> and spatial data <ref type="bibr" target="#b6">[6]</ref>. The general idea in these work is to release tables of noisy counts (histograms) and study how to ensure they are sufficiently accurate for different query workloads. In the other approach, differential privacy is applied to a specific data mining task, such as social recommendations <ref type="bibr" target="#b18">[18]</ref> and frequent itemset mining <ref type="bibr" target="#b3">[3]</ref>. The problem addressed in this paper falls into this category.</p><p>Privacy-Protection of Graphs. The aforementioned works on differentially private data mining all deal with structured data. For graph data, there is plenty of research effort <ref type="bibr" target="#b1">[1]</ref> to anonymize a social network graph to prevent node and edge re-identification. But most of them focus on modifying the graph structure to satisfy kanonymity, which has been proved to be insufficient <ref type="bibr" target="#b1">[1]</ref>. Recently, several works <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b21">21]</ref> emerge to provide private analysis of graph data. Two types of differential privacy have been introduced to handle graph data: node differential privacy and edge differential privacy. It is still open whether any nontrivial graph statistics can be released under node differential privacy due to its inherent large sensitivity (e.g., removing a node in a star graph may result in an empty graph). Hay et al. <ref type="bibr" target="#b13">[13]</ref> consider the problem of releasing the degree distribution of a graph under a variant of edge differential privacy. More recently, Karwa et al. <ref type="bibr" target="#b16">[16]</ref> propose algorithms to output approximate answers to subgraph counting queries, i.e., given a query graph H (e.g. a triangle, a k-star), returning the number of edge-induced isomorphic copies of H in the input graph. Unfortunately, their work does not support the case when H is an arbitrary subgraph yet.</p><p>In contrast, we have a different problem setting from <ref type="bibr" target="#b16">[16]</ref>. First, like <ref type="bibr" target="#b3">[3]</ref>, our privacy-preserving algorithm is associated with a specific and more complicated data mining task. Second, we consider a graph database containing a collection of graphs related to individuals.</p><p>Graph Pattern Mining. Finally, we briefly discuss relevant works on traditional non-private graph pattern mining. Earlier works which aim at finding all the frequent patterns in a graph database usually explore the search space in a certain manner. Representative approaches include a priori-based (e.g. <ref type="bibr" target="#b15">[15]</ref>) and pattern growth based (e.g. gSpan <ref type="bibr" target="#b29">[29]</ref>). Recent works aim at mining significant or representative patterns with scalability. One way of achieving this is through random walk <ref type="bibr" target="#b2">[2]</ref>, which also motivates our use of MCMC sampling for privacy preserving purpose. Another remotely related work is <ref type="bibr" target="#b27">[27]</ref>, which connects probabilistic inference and differential privacy. It differs from this work by focusing on inferencing on the output of a differentially private algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUDING REMARKS</head><p>In this paper we have presented a novel technique for differentially private mining of frequent graph patterns. The proposed solution integrates the process of graph mining and privacy protection into an MCMC sampling framework. Moreover, we have established the theoretical privacy and utility guarantee of our algorithm. Experiments on both synthetic and real datasets show good precision and support accuracy with moderate amount of privacy budget. We also notice the drop in utility with the increase of the number of outputs or the decrease in dataset size, which is inevitable under the requirement of differential privacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>THEOREM 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b20">[20]</ref> Given a utility score function u : D n ×X → R for a dataset D, the mechanism A, A(D, x) return x with probability ∝ exp( εu(D, x) 2∆u )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>= y with probability αxy, x with probability 1 -αxy,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example graph database and POFG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Diff-FPM algorithm input : Graph dataset D, threshold f , privacy budget ε1, ε2 output: A set S of k private frequent patterns 1 for i = 1 to k do 2 Choose any pattern in the output space as seed pattern; 3 while True do 4 Propose a neighboring pattern y of current pattern x according to the proposal distribution (Eq. 2); 5 Accept the proposed pattern with probability αxy = min exp(ε 1 u(y)/2k∆u)qyx exp(ε 1 u(x)/2k∆u)qxy , 1 ; 6 if convergence conditions are met then 7 Add current pattern to S and remove it from the output space;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9 (</head><label>9</label><figDesc>Optional) for each pattern in S, perturb its true support by Laplace mechanism with privacy budget ε2/k; of two phases: sampling and perturbation. The sampling phase includes k applications of the exponential mechanism via MH-based random walk in the output space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 (Figure 2 :Figure 3 :</head><label>323</label><figDesc>Figure 2: Effectiveness of EEN and impact of graph dataset size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: nDCG versus ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence trace of 20 chains</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2 :</head><label>2</label><figDesc>The EEN algorithm input : Pattern x, graph dataset D, support threshold f output: N1(x), N2(x) 1 Initialize N1,N2 ← ∅ (x omitted for brevity); 2 Find membership bitmap Bx using VF2 isomorphism test; 3 Populate sub-neighbors N b , super-back neighbors N p</figDesc><table><row><cell cols="2">super-forward neighbors N p f wd ;</cell><cell>back ,</cell></row><row><cell cols="2">/ * Explore sub-neighbors N b</cell><cell>* /</cell></row><row><cell cols="2">4 if sum(Bx) ≥ f then N1 ← N1 ∪ N b ;</cell></row><row><cell cols="2">5 else for x ∈ N b do</cell></row><row><cell>6</cell><cell cols="2">if SUB_IS_FREQ (x , Bx) then N1 ← N1 ∪ {x };</cell></row><row><cell>7</cell><cell>else N2 ← N2 ∪ {x };</cell></row><row><cell cols="2">/ * Explore super-back neighbors N p back 8 if sum(Bx) &lt; f then N2 ← N2 ∪ N p back ;</cell><cell>* /</cell></row><row><cell>9 else</cell><cell></cell></row><row><cell>10</cell><cell>∀x ∈ N p back , initialize dictionary H[x ] = 0;</cell></row><row><cell cols="3">23 Explore super-forward neighbors N p f wd similarly as N p back ,</cell></row><row><cell cols="2">details in [26];</cell></row><row><cell cols="2">24 return N1, N2;</cell></row></table><note><p>11 for i ← 1 to |D| do 12 Find set M of all mappings between Di and x; 13 for x ∈ N p back do 14 if H[x ] &lt; f and |D| -i + H[x ] ≥ f then 15 Let (u, v) be the back edge, i.e., x = x (u, v); 16 for m ∈ M do 17 if m(u), m(v) are adjacent in Di then 18 H[x ] ← H[x ] + 1; 19 break; 20 for x ∈ N p back do 21 if H[x ] ≥ f then N1 ← N1 ∪ {x }; 22 else N2 ← N2 ∪ {x };</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use 'graph pattern' and 'subgraph' interchangeably.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A detailed analysis on the size of the output space can be found in the full version<ref type="bibr" target="#b26">[26]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://dtp.nci.nih.gov/docs/aids/aids_data.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://en.wikipedia.org/wiki/Discounted_cumulative_gain</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://pypy.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The data point for dense at 10% is absent since the smallest dataset size can be generated is 1K.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research is partially supported by the National Science Foundation under the award CNS-0747247.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Privacy-preserving data mining: models and algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Output space sampling for graph patterns</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="730" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering frequent patterns in sensitive data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graphgen: A graph synthetic generator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.cse.ust.hk/graphgen" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A (sub) graph isomorphism algorithm for matching large graphs. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differentially Private Spatial Decompositions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Differentially private data cubes: optimizing noise sources and consistency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Our data, ourselves: Privacy via distributed noise generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Calibrating noise to sensitivity in private data analysis. Theory of Cryptography</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geweke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Markov chain Monte Carlo in practice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Walking in facebook: A case study of unbiased sampling of osns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gjoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Butts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate Estimation of the Degree Distribution of Private Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Differential privacy for location pattern mining</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGSPATIAL Workshop on Security and Privacy in GIS and LBS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An apriori-based algorithm for mining frequent substructures from graph data. Principles of Data Mining and Knowledge Discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Private Analysis of Graph Structure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB</title>
		<meeting>the VLDB</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Privbasis: frequent itemset mining with differential privacy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Personalized social recommendations-accurate or private?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differentially Private Recommender Systems: Building Privacy into the Netflix Prize Contenders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<title level="m">Mechanism Design via Differential Privacy. FOCS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Privacy-preserving subgraph discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mehmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data and Applications Security and Privacy XXVI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="161" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Differentially Private Data Release for Data Mining</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C M</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the structural properties of massive telecom call graphs: findings and implications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nanavati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simulation and the Monte Carlo method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sharing graphs using differentially private graph models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference</title>
		<meeting>the 2011 ACM SIGCOMM conference on Internet measurement conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="81" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mining frequent graph patterns with differential privacy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.7015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic inference and differential privacy</title>
		<author>
			<persName><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Differential privacy via wavelet transforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1200" to="1214" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">gspan: Graph-based substructure pattern mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficiently mining frequent trees in a forest: Algorithms and applications. Knowledge and Data Engineering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1021" to="1035" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
