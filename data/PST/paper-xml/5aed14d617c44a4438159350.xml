<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Auxiliary Learning for Visual Localization and Odometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noha</forename><surname>Radwan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Auxiliary Learning for Visual Localization and Odometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFED485AFE5CB206EEAC9860E775B203</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localization is an indispensable component of a robot's autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-theart SIFT-based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual localization is a fundamental transdisciplinary problem and a crucial enabler for numerous robotics as well as computer vision applications, including autonomous navigation, Simultaneous Localization and Mapping (SLAM), Structure-from-Motion (SfM) and Augmented Reality (AR). More importantly, it plays a vital role when robots lose track of their location, or what is commonly known as the kidnapped robot problem. In order for robots to be safely deployed in the wild, their localization system should be robust to frequent changes in the environment; whether environmental changes such as illumination and seasonal appearance, dynamic changes such as moving vehicles and pedestrians, or structural changes such as constructions.</p><p>Visual localization techniques can be broadly classified into two categories; topological and metric methods. Topological localization provides coarse estimates of the position, usually by dividing the map into a discretized set of locations and employing image retrieval techniques <ref type="bibr" target="#b4">[2]</ref>, <ref type="bibr" target="#b8">[6]</ref>, <ref type="bibr" target="#b26">[24]</ref>. While this approach is well suited for large environments, the resulting location accuracy is bounded by the granularity of the discrete set. Metric localization approaches on the other hand, provide a 6-DoF metric estimate of the pose within the environment. Thus far, local feature-based approaches that utilize SfM information achieve state-of-the- VLocNet: Multitask deep convolutional neural network for 6-DoF visual localization and odometry. Our network takes two consecutive monocular images as input and regresses the 6-DoF global pose and 6-DoF odometry simultaneously. The global pose and odometry subnetworks incorporate hard parameter sharing and utilize our proposed Geometric Consistency Loss function that is robust to environmental aliasing. Online demo: http://deeploc.cs.uni-freiburg.de/ art performance <ref type="bibr" target="#b24">[22]</ref>, <ref type="bibr" target="#b27">[25]</ref>. However, a critical drawback of these approaches is the decrease in speed and increase in complexity of finding feature correspondences as the size of the environment grows. Moreover, most approaches require a minimum number of matches to be able to produce a pose estimate. This in turn causes pose estimation failures when there is large viewpoint changes, motion blur, occlusions or textureless environments.</p><p>Inspired by the outstanding performance of Convolutional Neural Networks (CNNs) in a variety of tasks in various domains and with the goal of eliminating manual engineering of algorithms for feature selection, CNN architectures that directly regress the 6-DoF metric pose have recently been explored <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b28">[26]</ref>, <ref type="bibr" target="#b6">[4]</ref>. However, despite their ability to handle challenging perceptual conditions and effectively manage large environments, they are still unable to match the performance of state-of-the-art feature-based localization methods. This is partly due to their inability to internally model the 3D structural constraints of the environment while learning from a single monocular image.</p><p>As CNN-based approaches become the de-facto standard for more robotics tasks, the need for multitask models becomes increasingly crucial. Moreover, from a robot's learning perspective, it is unlucrative and unscalable to have multiple specialized single-task models as they inhibit both intertask and auxiliary learning. This has lead to a recent surge in research targeted towards frameworks for learning unified models for a range of tasks across different domains <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b22">[20]</ref>, <ref type="bibr" target="#b5">[3]</ref>. The goal of these multitask learning methods is to leverage similarities within task-specific features and exploit complementary features learned across different tasks, with the aim of mutual benefit. An evident advantage is the resulting compact model size in comparison to having multiple task-specific models. Auxiliary learning approaches on the other hand, aim at maximizing the prediction of a primary task by supervising the model to additionally learn a secondary task. For instance, in the context of localization, humans often describe their location to each other with respect to some reference landmark in the scene and giving their position relative to it. Here, the primary task is to localize and the auxiliary task is to be able to identify landmarks. Similarly, we can leverage the complementary relative motion information from odometry to constrict the search space while training the global localization model. However, this problem is non-trivial as we need to first determine how to structure the architecture to ensure the learning of this inter-task correlation and secondly, how to jointly optimize the unified model since different task-specific networks have different attributes and different convergence rates.</p><p>In this work, we address the problem of global pose regression by simultaneously learning to estimate visual odometry as an auxiliary task. We propose the VLocNet architecture consisting of a global pose regression sub-network and a Siamese-type relative pose estimation sub-network. Our network based on the residual learning framework, takes two consecutive monocular images as input and jointly regresses the 6-DoF global pose as well as the 6-DoF relative pose between the images. We incorporate a hard parameter sharing scheme to learn inter-task correlations within the network and present a multitask alternating optimization strategy for learning shared features across the network. Furthermore, we devise a new loss function for global pose regression that incorporates the relative motion information during training and enforces the predicted poses to be geometrically consistent with respect to the true motion model. We present extensive experimental evaluations on both indoor and outdoor datasets comparing the proposed method to state-ofthe-art approaches for global pose regression and visual odometry estimation. We empirically show that our proposed VLocNet architecture achieves state-of-the-art performance compared to existing CNN-based techniques. To the best of our knowledge, our presented approach is the first deep learning-based localization method to perform on par with local feature-based techniques. Moreover, our work is the first attempt to show that a joint multitask model can precisely and efficiently outperform its task-specific counterparts for global pose regression and visual odometry estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are numerous approaches that have been proposed for localization in the literature. In this section, we review some of the techniques developed thus far for addressing this problem, followed by a brief discussion on approaches for visual odometry estimation.</p><p>Sparse feature-based localization approaches learn a set of feature descriptors from training images and build a codebook of 3D descriptors against which a query image is matched <ref type="bibr" target="#b24">[22]</ref>, <ref type="bibr" target="#b10">[8]</ref>. To efficiently find feature correspondences within the codebook, Shotton et al. <ref type="bibr" target="#b25">[23]</ref> and Valentin et al. <ref type="bibr" target="#b27">[25]</ref> train regression forests on 3D scene data and use RANSAC to infer the final location of the query image. Donoser et al. propose a discriminative classification approach using random ferns and demonstrate improved pose accuracy with faster run times <ref type="bibr" target="#b9">[7]</ref>. Despite the accurate pose estimates provided by these methods, the overall run-time depends on the size of the 3D model and the number of feature correspondences found. The approach presented in this paper does not suffer from these scalability issues as the learned model is independent of the size of the environment. Moreover, since it does not involve any expensive matching algorithm, it has a time complexity of O(1).</p><p>Deep learning-based localization: PoseNet <ref type="bibr" target="#b14">[12]</ref> was the first approach to utilize DCNNs to address the metric localization problem. The authors further extended this work by using a Bayesian CNN implementation to estimate the uncertainty of the predicted pose <ref type="bibr" target="#b12">[10]</ref>. Concurrently, Walch et al. <ref type="bibr" target="#b28">[26]</ref> and Clark et al. <ref type="bibr" target="#b6">[4]</ref> propose DCNNs with Long-Short Term Memory (LSTM) units to avoid overfitting while still selecting the most useful feature correlations. Contrary to these approaches and inspired by semantic segmentation architectures, Melekhov et al. introduce the Hour-glassPose <ref type="bibr" target="#b18">[16]</ref> network that utilizes a symmetric encoderdecoder architecture followed by a regressor to estimate the camera pose. In order to provide a more robust approach to balance both the translational and rotational components in the loss term, the commonly employed fixed weight regularizer was replaced with learnable parameters in <ref type="bibr" target="#b13">[11]</ref>. The authors also introduced a loss function based on the geometric reprojection error that does not require balancing of the pose components, but it often has difficulty in converging. More recently, Laskar et al. proposed a learning procedure that decouples feature learning and pose estimation, closely resembling feature-based localization approaches <ref type="bibr" target="#b16">[14]</ref>. Unlike most of the aforementioned approaches that utilize the Euclidean loss for pose regression, we propose a novel loss function that incorporates motion information while training to learn poses that are consistent with the previous prediction.</p><p>Visual Odometry: Another closely related problem in robotics is estimating the incremental motion of the robot using only sequential camera images. In one of the earlier approaches, Konda et al. <ref type="bibr" target="#b15">[13]</ref> adopt a classification approach to the problem, where a CNN with a softmax layer is used to infer the relative transformation between two images using a prior set of discretized velocities and directions. Another approach is proposed by Nicholai et al. <ref type="bibr" target="#b21">[19]</ref>, in which they combine both image and LiDAR information to estimate the relative motion between two frames. They project the point cloud on the 2D image and feed this information to a neural network which estimates the visual odometry. Mohanty et al. <ref type="bibr" target="#b19">[17]</ref> propose a Siamese AlexNetbased architecture called DeepVO, in which the translational and rotational components are regressed through an L2-loss layer with equal weight values. In similar work, Melekhov et al. <ref type="bibr" target="#b17">[15]</ref> add a weighting term to balance both the translational and rotational components of the loss, which yields an improvement in the predicted pose. Additionally, they use a spatial pyramid pooling layer in their architecture which renders their approach robust to varying input image resolutions. Inspired by the success of residual networks in various visual recognition tasks, we propose a Siamese-type two stream architecture built upon the ResNet-50 <ref type="bibr" target="#b11">[9]</ref> model for visual odometry estimation.</p><p>Contrary to the task-specific approaches presented above where individual models are trained for global pose regression and visual odometry estimation, we propose a joint endto-end trainable architecture that simultaneously regresses the 6-DoF global pose and relative motion as an auxiliary output. By jointly learning both tasks, our approach is robust to environmental aliasing by utilizing previous pose and relative motion information, thereby combining the advantages of both local feature and deep learning-based localization methods. Moreover, by sharing features across different scales, our proposed model significantly outperforms the state-of-the-art in CNN-based localization while achieving competitive performance for visual odometry estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP POSE REGRESSION</head><p>The primary goal of our architecture is to precisely estimate the global pose by minimizing our proposed Geometric Consistency Loss function, which in turn constricts the search space using the relative motion between two consecutive frames. We formulate this problem in the context of auxiliary learning with the secondary goal of estimating the relative motion. The features learned for relative motion estimation are then leveraged by the global pose regression network to learn a more distinct representation of the scene. More specifically, our architecture consists of a three-stream neural network; a global pose regression stream and a Siamese-type double-stream for odometry estimation. An overview of our proposed VLocNet architecture is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Given a pair of consecutive monocular images (I t , I t-1 ), our network predicts both the global pose p t = [x t , q t ] and the relative pose p t,t-1 = [x t,t-1 , q t,t-1 ] between the input frames, where x ∈ R 3 denotes the translation and q ∈ R 4 denotes the rotation in quaternion representation. For ease of notation, we assume that the quaternion outputs of the network have been normalized a priori. The input to the Siamese streams are the images I t , I t-1 , while the input to the global pose stream is I t . In the remainder of this section, we present the constituting parts of our VLocNet architecture along with how the joint optimization is carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global Pose Regression</head><p>In this section, we describe the architecture of our global pose sub-network, which given an input image I t and a previous predicted pose pt-1 , predicts the 7-dimensional pose pt . Similar to previous works <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b28">[26]</ref>, p is defined relative to an arbitrary global reference frame.</p><p>1) Network Architecture: To estimate the global pose, we build upon the ResNet-50 <ref type="bibr" target="#b11">[9]</ref> architecture with the following modifications. The structure of our network is similar to ResNet-50 truncated before the last average pooling layer. The architecture is comprised of five residual blocks with multiple residual units, where each unit has a bottleneck architecture consisting of three convolutional layers in the following order: 1 × 1 convolution, 3 × 3 convolution, 1 × 1 convolution. Each of the convolutions is followed by batch normalization, scale and Rectified Linear Unit (ReLU). We modify the standard residual block structure by replacing ReLUs with Exponential Linear Units (ELUs) <ref type="bibr" target="#b7">[5]</ref>. ELUs help in reducing the bias shift in the neurons, in addition to avoiding the vanishing gradient and yield faster convergence. We replace the last average pooling layer with global average pooling and subsequently add three inner-product layers, namely fc1, fc2 and fc3. The first inner-product layer fc1 is of dimension 1024 and the following two inner-product layers are of dimensions 3 and 4, for regressing the translation x and rotation q respectively. Our proposed Geometric Consistency Loss, detailed in Sec. III-A.2, ensures that the predicted pose is consistent with that obtained by accumulating the relative motion to the previous pose. Therefore, we feed the previous pose (groundtruth pose during training and predicted pose during evaluation) to the network so that it can better learn about spatial relations of the environment. We do not incorporate recurrent units into our network as our aim in this work is to localize only using consecutive monocular images and not rely on long-term temporal features. We first feed the previous pose to an inner-product layer fc4 of dimension D and reshape its output to H × W × C, which corresponds in shape to the output of the last residual unit before the downsampling stage. Both tensors are then concatenated and fed to the subsequent residual unit. In total, there are four downsampling stages in our network and we experiment with fusing at each of these stages in Sec. IV-E.</p><p>2) Geometric Consistency Loss: Learning both translational and rotational pose components with the same loss function is inherently challenging due to the difference in scale and units between both the quantities. Eq. ( <ref type="formula">1</ref>) and Eq. ( <ref type="formula" target="#formula_0">2</ref>) describe the loss function for regressing the translational and rotational components in the Euclidean space.</p><formula xml:id="formula_0">L x (I t ) := x t -xt γ (1) L q (I t ) := q t -qt γ ,<label>(2)</label></formula><p>where x t and q t denote the ground-truth translation and rotation components, xt and qt denote their predicted counterparts and γ refers to the L γ -norm. In this work, we use the L 2 Euclidean norm. Previous work has shown that the performance of a model trained to jointly regress the position and orientation, outperforms two separate models trained for each task <ref type="bibr" target="#b13">[11]</ref>. Therefore, as the loss function is required to learn both the position and orientation, a weight regularizer β is used to balance each of the loss terms. We can represent this loss function as:</p><formula xml:id="formula_1">L β (I t ) := L x (I t ) + β L q (I t ).<label>(3)</label></formula><p>Although initial work <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b28">[26]</ref>, <ref type="bibr" target="#b29">[27]</ref>, <ref type="bibr" target="#b20">[18]</ref> has shown that by minimizing this function, the network is able to learn a valid pose regression model, it suffers from the drawback of having to manually tune the hyperparameter β for each new scene in order to achieve reasonable results. To counteract this problem, recently <ref type="bibr" target="#b13">[11]</ref> learnable parameters were introduced to replace β . The resulting loss function is:</p><formula xml:id="formula_2">L s (I t ) := L x (I t ) exp(-ŝx ) + ŝx + L q (I t ) exp(-ŝq ) + ŝq , (4)</formula><p>where ŝx and ŝq are the two learnable variables. Each variable acts as a weighting for the respective component in the loss function. Although this formulation overcomes the problem of having to manually select a β value for each scene, it does not ensure that the estimated poses are consistent with the previous motion.</p><p>As a solution to this problem, we propose a novel loss function that incorporates previous motion information, thereby producing consistent pose estimates. We introduce an additional constraint which bootstraps the loss function by penalizing pose predictions that contradict the relative motion. More precisely, in addition to the loss term shown in Eq. ( <ref type="formula">4</ref>), we enforce that the difference between pt and pt-1 be as close to the groundtruth relative motion p t,t-1 as possible. We use R x (I t ) and R q (I t ) to denote the relative motion between the current image I t and the previous predicted pose pt-1 as:</p><formula xml:id="formula_3">R x (I t ) := xt -xt-1 (5) R q (I t ) := q-1 t-1 qt . (<label>6</label></formula><formula xml:id="formula_4">)</formula><p>The components from Eq. ( <ref type="formula">5</ref>) and Eq. ( <ref type="formula" target="#formula_3">6</ref>) compute the relative motion in terms of the network's predictions. We integrate these components into an odometry loss term to minimize the variance between the predicted poses. The corresponding odometry loss can be formulated as:</p><formula xml:id="formula_5">L x odom (I t ) := x t,t-1 -R x (I t ) γ (<label>7</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">L q odom (I t ) := q t,t-1 -R q (I t ) γ , (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where L x odom computes the difference between the groundtruth relative translational motion and its predicted counterpart, while L q odom computes a similar difference for the rotational component. We combine both the odometry loss terms with the loss function from Eq. ( <ref type="formula">4</ref>), thereby minimizing:</p><formula xml:id="formula_9">L Geo (I t ) := L x (I t ) + L x odom (I t ) exp(-ŝx ) + ŝx + L q (I t ) + L q odom (I t ) exp(-ŝq ) + ŝq . (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>We hypothesize that, by utilizing this relative motion in the loss function, the resulting trained model is more robust to perceptual aliasing within the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Odometry</head><p>In order to integrate motion specific features in our global pose regression network, we train an auxiliary network to regress the 6-DoF relative pose from the images (I t , I t-1 ). We do so by constructing a two stream Siamese-type network also based on the ResNet-50 architecture. We concatenate features from the two individual streams of ResNet-50 truncated before the last downsampling stage (Res5). We then pass these concatenated feature maps to the last three residual units, followed by three inner-product layers, similar to our global pose regression network. We minimize the following loss function:</p><formula xml:id="formula_11">L vo (I t , I t-1 ) := L x (I t , I t-1 ) exp(-ŝx ) + ŝx + L q (I t , I t-1 ) exp(-ŝq ) + ŝq . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>With a slight abuse of notation, we use L x (I t , I t-1 ) to refer to the L 2 Euclidean loss in the translational component of the visual odometry and L q (I t , I t-1 ) for the rotational component. Similar to our approach used for the global pose regression, we additionally learn two weighting parameters to balance the loss between both components. We detail the training procedure in Sec. IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Auxiliary Learning</head><p>The idea of jointly learning both the global pose and visual odometry stems from the inherent similarities across both tasks in the feature space. More importantly, sharing features across both networks can enable a competitive and collaborative action as each network updates its own weights during backpropagation in an attempt to minimize the distance to the groundtruth pose. This symbiotic action introduces additional regularization while training, thereby avoiding overfitting. Contrary to the approaches that use a two stream shared Siamese network for visual odometry estimation, we do not share weights between the two temporal streams, rather we share weights between the stream that takes the image I t from the current timestep as input and the global pose regression stream. By learning separate discriminative features in each timestep before learning the correlation between them, the visual odometry network is able to effectively generalize to challenging corner cases containing motion blur and perceptual aliasing. The global pose regression network also benefits from this feature sharing, as the shared weights are pulled more towards areas of the image from which the relative motion can be easily estimated.</p><p>While sharing features across multiple networks can be inferred as a form of regularization, it is not clear a priori for how many layers should we maintain a shared stream. Sharing only a few initial layers does not have any additive benefit to either network, as early layers learn very generic feature representations. On the other hand, maintaining a shared stream too deep into the network can negatively impact the performance of both tasks, since the features learned at the stages towards the end are more task specific. In this work, we studied the impact of sharing features across both sub-networks and experimented with varying the amount of feature sharing. We elaborate on these experiments in Sec. IV-F. Another critical aspect of auxiliary learning is how the optimization is carried out. We detail our optimization procedure in Sec. IV-B. Finally, during inference, the joint model can be deployed as a whole or each subnetwork individually, since the relative pose estimates are only used in the loss function and there is no inter-network dependency in terms of concatenating or adding features from either sub-networks. This gives additional flexibility at the time of deployment compared to architectures that have cross-connections or cross-network fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, we present results using our proposed VLocNet architecture in comparison to the state-of-the-art on both indoor and outdoor datasets, followed by detailed analysis on the architectural decisions and finally, we demonstrate the efficacy of learning visual localization models along with visual odometry as an auxiliary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Datasets</head><p>We evaluate VLocNet on two publicly available datasets; Microsoft 7-Scenes <ref type="bibr" target="#b25">[23]</ref> and Cambridge Landmarks <ref type="bibr" target="#b14">[12]</ref>. We use the original train and test splits provided by all the datasets to facilitate comparison and benchmarking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene</head><p>PoseNet <ref type="bibr" target="#b14">[12]</ref> Bayesian PoseNet <ref type="bibr" target="#b12">[10]</ref> LSTM-Pose <ref type="bibr" target="#b28">[26]</ref> VidLoc <ref type="bibr" target="#b6">[4]</ref> Hourglass-Pose <ref type="bibr" target="#b18">[16]</ref> BranchNet <ref type="bibr" target="#b29">[27]</ref> PoseNet2 <ref type="bibr" target="#b13">[11]</ref> NNnet <ref type="bibr">[</ref> Microsoft 7-Scenes: is a dataset comprised of RGB-D images collected from seven different scenes in an indoor office environment <ref type="bibr" target="#b25">[23]</ref>. The images were collected with a handheld Kinect RGB-D camera and the groundtruth poses were extracted using KinectFusion <ref type="bibr" target="#b25">[23]</ref>. The images were captured at resolution of 640 × 480 pixels and each scene contains multiple sequences recorded in a room. Each sequence was recorded with different camera motions in the presence of motion blur, perceptual aliasing and textureless features in the room, thereby making it a popular dataset for relocalization and tracking.</p><p>Cambridge Landmarks: provides images collected from five different outdoor scenes around the Cambridge University <ref type="bibr" target="#b14">[12]</ref>. The images were captured using a smartphone at a resolution of 1920 × 1080 pixels while walking in different trajectories and pose labels were computed using an SfM method. The dataset exhibits substantial clutter caused by pedestrians, cyclists and moving vehicles, making it challenging for urban relocalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Training</head><p>In order to train our network on different datasets, we rescale the images maintaining the aspect ratio such that the shorter side is of length 256 pixels. We calculate the pixel-wise mean for each of the scenes in the datasets and subtract them with the input images. We experimented with augmenting the images using pose synthesis <ref type="bibr" target="#b29">[27]</ref> and synthetic view synthesis <ref type="bibr" target="#b20">[18]</ref>, however they did not yield any performance gains, rather in some cases they negatively affected the pose accuracy. We found that using random crops of 224 × 224 pixels acts as a better regularizer helping the network generalize better in comparison to synthetic augmentation techniques while saving preprocessing time. For evaluations, we use the center crop of the images.</p><p>We use the Adam solver for optimization with β 1 = 0.9, β 2 = 0.999 and ε = 10 -10 . We train the network with an initial learning rate of λ 0 = 10 -4 with a mini-batch size of 32 and a dropout probability of 0.2. Details regarding the specific ŝx and ŝq values used for our Geometric Consistency Loss function are covered in Sec. IV-E. In order to learn a unified model and to facilitate auxiliary learning, we employ different optimization strategies that allow for efficient learning of shared features as well as task-specific features, namely alternate training and joint training. In alternate training we use a separate optimizer for each task and alternatively execute each task optimizer on the taskspecific loss function, thereby allowing synchronized transfer of information from one task to the other. This instills a form of hierarchy into the tasks, as the odometry sub-network improves the estimate of its relative poses, the global pose network in turn uses this estimate to improve its prediction. It is often theorized that this enforces commonality between the tasks. The disadvantage of this approach is that a bias in the parameters is introduced by the task that is optimized second. In joint training on the other hand, we add each of the task-specific loss functions and use a single optimizer to train the sub-networks at the same time. The advantage of this approach is that the tasks are trained in a way that they maintain the individuality of their functions, but as each of our tasks is of different units and scale, the task with the larger scale often dominates the training.</p><p>We experiment with bootstrapping the training of VLocNet with different weight initializations for each of the aforementioned optimization schemes. Results from this experiment are discussed in Sec. IV-F. Using the principle of transfer learning, we trained the individual models by initializing all the layers up to the global pooling layer with the weights of ResNet-50 pretrained on ImageNet <ref type="bibr" target="#b23">[21]</ref> and we used Gaussian initialization for the remaining layers. We use the TensorFlow <ref type="bibr" target="#b3">[1]</ref> deep learning library and all the models were trained on a NVIDIA Titan X GPU for a maximum of 120,000 iterations, which approximately took 15 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-art</head><p>We compare the performance of our VLocNet architecture with current state-of-the-art deep learning-based localization methods namely PoseNet <ref type="bibr" target="#b14">[12]</ref>, Bayesian PoseNet <ref type="bibr" target="#b12">[10]</ref>, LSTM-Pose <ref type="bibr" target="#b28">[26]</ref>, VidLoc <ref type="bibr" target="#b6">[4]</ref>, Hourglass-Pose <ref type="bibr" target="#b18">[16]</ref>, Branch-Net <ref type="bibr" target="#b29">[27]</ref>, PoseNet2 <ref type="bibr" target="#b13">[11]</ref>, SVS-Pose <ref type="bibr" target="#b20">[18]</ref>, and NNnet <ref type="bibr" target="#b16">[14]</ref>. We report the performance in terms of the median translation and orientation errors for each scene in the datasets. On the 7-Scenes dataset, we initialized the ŝx and ŝq for our loss function with values between -3 to 0 and -4.8 to -3 respectively. Tab. I reports the comparative results on this dataset. Our VLocNet architecture consistently outperforms the state-of-the-art methods for all the scenes by 77.14% in translation and 59.14% in rotation. On the Cambridge Landmarks dataset, we report the results using ŝx = -3 and ŝq = -6.5 for all the scenes. Using our VLocNet architecture with the proposed Geometric Consistency Loss, we improve upon the current state-of-the-art results by 51.6% in translation and 1.5% in orientation. Note that we did not perform any hyperparameter optimization, we expect further improvements to the results presented here by tuning the parameters. The results demonstrate that our network substantially improves upon the state-of-the-art on both indoor as well as outdoor datasets.</p><p>In order to evaluate the performance of VLocNet on visual odometry estimation, we show quantitative comparison against three state-of-the-art CNN approaches, namely DeepVO <ref type="bibr" target="#b19">[17]</ref>, cnnBspp <ref type="bibr" target="#b17">[15]</ref> and LBO <ref type="bibr" target="#b21">[19]</ref>. Tab. III shows comprehensive results from this experiment on the 7-Scenes dataset. For each scene, we report the average translational and rotational error as a function of sequence length. As illustrated in Tab. III, our network achieves an improvement of 27.0% in translation and 16.67% in orientation outperforming the aforementioned approaches and thus reinforcing its suitability for visual odometry estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Benchmarking</head><p>As mentioned in previous works <ref type="bibr" target="#b28">[26]</ref>, no deep learningbased localization method thus far has been able to match the performance of state-of-the-art local feature-based approaches. In order to gain insights on the performance of VLocNet, we present results on the 7-Scenes dataset, in comparison with Active Search (without prioritization) <ref type="bibr" target="#b24">[22]</ref>, which is a state-of-the-art SIFT-based localization method. Moreover, as a proof of validation that our trained network is able to regress poses beyond those shown in the training images, we also compare with Nearest Neighbor localization <ref type="bibr" target="#b14">[12]</ref>. Tab. IV shows the comparative results of VLocNet against the aforementioned methods.</p><p>Local feature-based approaches often fail to localize in textureless scenes due to the inadequate number of correspondences found. In Tab. IV, we denote the number of images for which the localization fails in parenthesis </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architectural Analysis</head><p>In this section, we quantitatively analyze the effect of the various architectural decisions made while designing VLocNet. Specifically, we show the performance improvements for the following: and previous pose fusion using L Geo loss with ŝx , ŝq Tab. V shows the median error in pose estimation as an average of all the scenes in the 7-Scenes dataset. We observe that incorporating residual units in our architecture yields an improvement of 54.09% and 14.68% for the translation and orientation components respectively in comparison to PoseNet. However, the most notable improvement is achieved by fusing the previous pose information using our Geometric Consistency Loss, which can be seen in the improvement of the translational error between VLocNet-M2 and VLocNet-M3. This clearly shows that constricting the search space with the relative pose information while training substantially increases the performance. Furthermore, by utilizing learnable parameters for weighting the translational and rotational loss terms, our network yields a further improvement in performance compared to manually tuning the weighting. In Fig. <ref type="figure">2</ref> we show the cumulative histogram error of the aforementioned models trained on the RedKitchen scene. It can be seen that even our base VLocNet model (VLocNet-M1) shows a significant improvement over the baseline method for the translational error. Moreover our final architecture (VLocNet-M4) achieves a rotational error below 10 • for 100% of the poses. We additionally performed experiments to determine the downsampling stage to fuse the previous pose while using our Geometric Consistency Loss function. Fig. <ref type="figure" target="#fig_2">3</ref> shows the median error while fusing the pose at Res3, Res4 and Res5 in our architecture. It can be seen that fusing at Res5 where the feature maps are of size 7 × 7, yields the lowest localization error, while fusing at earlier stages produces varying results for different scenes; either lower translational error at the cost of the orientation or vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation of Deep Auxiliary Learning</head><p>In this section, we evaluate the performance of our jointly trained model using auxiliary learning, along with different optimization strategies that we employed. We explored using both joint and alternating optimization to minimize the loss. We found that the average localization error using an alternate optimization strategy was 28.99% and 18.47% lower in translation and rotation respectively, when compared to a joint optimization. This can be attributed to the difference in scales of the loss values for each task, resulting in the optimizer becoming more biased towards minimizing the global pose regression error at the cost of having suboptimal relative pose estimates. This inadvertently results in worse accuracy for both tasks.</p><p>When both global pose regression and visual odometry networks are trained independently, each of them alter the weights of their convolution layers in different ways. Therefore, we evaluated strategies that enable efficient sharing of features between both networks to facilitate the learning of inter-task correlations. Using the model trained on our singletask global pose sub-network (ST) as a baseline, we evaluate the effect of different initializations of the joint model on the localization accuracy. More precisely, we compare the effect of initializing our VLocNet model using weights from: the pretrained task-specific global pose network (MT-GLoc), the pretrained task-specific visual odometry network (MT-VO), and the combined weights from both networks (MT-Dual). Fig. <ref type="figure" target="#fig_3">4</ref> shows the results from this experiment. It can be seen that our joint models that regress relative poses as an auxiliary task, outperform each of the task-specific models, demonstrating the efficacy of our approach. The improvement is most apparent in the Stairs scene which is the most challenging scene in the 7-Scenes dataset as it contains repetitive structures and textureless surfaces. Furthermore, on closer examination, we find that dual initialization of both sub-networks with weights from their task-specific models achieves the best performance, contrary to initializing only one of the sub-networks and learning the other from scratch. Another interesting observation worth noting is that initializing only the global localization stream in the joint network with pretrained weights yields the lowest improvement in pose accuracy compared to the single-task model. This is to be expected as the visual odometry stream does not provide reasonable estimates when the training begins, therefore the localization stream cannot benefit from the motion specific features from the odometry stream.</p><p>We summarize the localization performance achieved by our multitask VLocNet incorporating the Geometric Consistency Loss while simultaneously training the auxiliary odometry network in Tab. VI, where we vary the number of shared layers between the global localization and the visual odometry streams. The table shows the median pose error as an average over all the 7-scenes. We experimented with maintaining a shared stream up to the end of Res2, end of Res3 and end of Res4 in our architecture. The results indicate that the lowest average error is achieved by sharing the streams up to Res3, which shows that features learned after Res3 are highly task-specific and features learned before Res2 are too generic. In comparison to the singletask VLocNet model, the multitask variant achieves an improvement of 12.5% in translational and 18.49% in rotational components of the pose. We believe that these results demonstrate the utility of learning joint multitask models for visual localization and odometry. A live online demo can be viewed at http://deeploc.cs.uni-freiburg.de/.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a novel end-to-end trainable multitask DCNN architecture for 6-DoF visual localization and odometry estimation from subsequent monocular images. We present a framework for learning inter-task correlations in our network using an efficient sharing scheme and a joint optimization strategy. We show that our jointly trained localization model outperforms task-specific networks, demonstrating the efficacy of learning visual odometry as an auxiliary task. Furthermore, we introduced the Geometric Consistency Loss function for regressing 6-DoF poses consistent with the true motion model.</p><p>Using extensive evaluations on standard indoor and outdoor benchmark datasets, we show that both our single-task and multitask models achieve state-of-the-art performance compared to existing CNN-based approaches, which accounts for an improvement of 80% and 66.69% in translation and rotation respectively. More importantly, our approach is the first to close the performance gap between local feature-based and CNN-based localization methods, even outperforming them in some cases. Overall, our findings are an encouraging sign that utilizing multitask DCNNs for localization and odometry is a promising research direction. As future work, we plan to investigate joint training with more auxiliary tasks such as semantic segmentation and image similarity learning that can further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. VLocNet: Multitask deep convolutional neural network for 6-DoF visual localization and odometry. Our network takes two consecutive monocular images as input and regresses the 6-DoF global pose and 6-DoF odometry simultaneously. The global pose and odometry subnetworks incorporate hard parameter sharing and utilize our proposed Geometric Consistency Loss function that is robust to environmental aliasing. Online demo: http://deeploc.cs.uni-freiburg.de/</figDesc><graphic coords="1,312.95,139.49,175.61,123.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>VLocNet-M1: ResNet-50 base architecture with ReLUs, L 2 Euclidean loss for translation and rotation with β = 1 • VLocNet-M2: ResNet-50 base architecture with ELUs, L 2 Euclidean loss for translation and rotation with β = 1 • VLocNet-M3: ResNet-50 base architecture with ELUs and previous pose fusion using L Geo loss with β = 1 • VLocNet-M4: ResNet-50 base architecture with ELUs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of median localization error from fusing previous pose information at various stages in our VLocNet architecture with our proposed Geometric Consistency Loss. The results consistently show that the highest localization accuracy is achieved by fusing the previous predicted pose at Res5.</figDesc><graphic coords="8,76.55,188.09,131.11,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of our single-task model in comparison to the multitask VLocNet with different weight initializations, on the 7-Scenes dataset. (x) and (q) denote the translation and orientation components.</figDesc><graphic coords="8,207.11,188.09,65.81,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF MEDIAN LOCALIZATION ERROR OF VLOCNET WITH EXISTING CNN MODELS ON THE 7-SCENES DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.12 • 0.37m, 7.24 • 0.24m, 5.77 • 0.18m, N/A 0.15m, 6.53 • 0.18m, 5.17 • 0.13m, 4.48 • 0.13m, 6.46 • 0.036m, 1.71 • Fire 0.47m, 14.4 • 0.43m, 13.7 • 0.34m, 11.9 • 0.26m, N/A 0.27m, 10.84 • 0.34m, 8.99 • 0.27m, 11.3 • 0.26m, 12.72 • 0.039m, 5.34 • Heads 0.29m, 12.0 • 0.31m, 12.0 • 0.21m, 13.7 • 0.14m, N/A 0.19m, 11.63 • 0.20m, 14.15 • 0.17m, 13.0 • 0.14m, 12.34 • 0.046m, 6.64 •</figDesc><table><row><cell></cell><cell>14]</cell><cell>VLocNet</cell></row><row><cell></cell><cell></cell><cell>(Ours)</cell></row><row><cell cols="3">Chess 0.32m, 8Office 0.48m, 7.68 • 0.48m, 8.04 • 0.30m, 8.08 • 0.26m, N/A 0.21m, 8.48 • 0.30m, 7.05 • 0.19m, 5.55 • 0.21m, 7.35 • Pumpkin 0.47m, 8.42 • 0.61m, 7.08 • 0.33m, 7.00 • 0.36m, N/A 0.25m, 7.01 • 0.27m, 5.10 • 0.26m, 4.75 • 0.24m, 6.35 • RedKitchen 0.59m, 8.64 • 0.58m, 7.54 • 0.37m, 8.83 • 0.31m, N/A 0.27m, 10.15 • 0.33m, 7.40 • 0.23m, 5.35 • 0.24m, 8.03 • Stairs 0.47m, 13.8 • 0.48m, 13.1 • 0.40m, 13.7 • 0.26m, N/A 0.29m, 12.46 • 0.38m, 10.26 • 0.35m, 12.4 • 0.27m, 11.82 • 0.097m, 6.48 • 0.039m, 1.95 • 0.037m, 2.28 • 0.039m, 2.20 •</cell></row><row><cell>Average</cell><cell>0.44m, 10.4 • 0.47m, 9.81 • 0.31m, 9.85 • 0.25m, N/A 0.23m, 9.53 • 0.29m, 8.30 • 0.23m, 8.12 • 0.21m, 9.30 •</cell><cell>0.048m, 3.80 •</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V COMPARATIVE</head><label>V</label><figDesc>ANALYSIS OF VLOCNET ON THE 7-SCENES DATASET.Fig. 2. Qualitative analysis of the localization performance for our proposed VLocNet architecture against PoseNet presented as a cumulative histogram of normalized errors for the Red Kitchen scene.</figDesc><table><row><cell>Model</cell><cell>Position</cell><cell>Orientation</cell></row><row><cell>PoseNet [12] VLocNet-M1 VLocNet-M2 VLocNet-M3 VLocNet-M4</cell><cell>0.44m 0.202m 0.197m 0.081m 0.048m</cell><cell>10.4 • 8.873 • 8.209 • 7.860 • 3.801 •</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI SUMMARY</head><label>VI</label><figDesc>OF THE LOCALIZATION PERFORMANCE ACHIEVED BY VLOCNET WITH VARYING AMOUNTS OF SHARING.</figDesc><table><row><cell></cell><cell>Res2</cell><cell>Res3</cell><cell>Res4</cell></row><row><cell>7-Scenes Avg.</cell><cell cols="3">0.055m, 2.989 • 0.042m, 3.098 • 0.053m, 3.174 •</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-5386-3081-5/18/$31.00 ©2018 IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work has partially been supported by the European Commission under the grant number H2020-ICT-644227-FLOURISH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2018">IEEE International Conference on Robotics and Automation (ICRA)</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TABLE II COMPARISON OF MEDIAN LOCALIZATION ERROR OF VLOCNET WITH EXISTING CNN MODELS ON THE CAMBRIDGE LANDMARKS DATASET</title>
		<idno>12] Bayesian PoseNet [10</idno>
	</analytic>
	<monogr>
		<title level="j">Scene PoseNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">m, 1.04 • 0.836m, 1.419 • Old Hospital</title>
		<idno>SVS- Pose [18] LSTM- Pose [26] PoseNet2 [11] VLocNet (Ours) King&apos;s College 1.92m, 5.40 • 1.74m, 4.06 • 1.06m, 2.81 • 0.99m, 3.65 • 0.88</idno>
	</analytic>
	<monogr>
		<title level="m">Shop Facade 1.46m, 8.08 • 1</title>
		<imprint>
			<date type="published" when="0411-02-75">2.31m, 5.38 • 2.57m, 5.14 • 1.50m, 4.03 • 1.51m, 4.29 • 3.20m, 3.29 • 1.075m, 2.411</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>25m, 7.54 • 0.63m, 5.73 • 1.18m, 7.44 • 0.88m, 3.78 • 0.593m, 3.529 • St Mary&apos;s Church 2.65m, 8.46 • 2.11m, 8.38 • 2.11m, 8.11 • 1.52m, 6.68 • 1.57m, 3.32 • 0.631m, 3.906 • Average 2.08m, 6.83 • 1.92m, 6.28 • 1.33m, 5.17 • 1.30m, 5.52 • 1.62m, 2.86 • 0.784m, 2.817 • TABLE III COMPARISON OF 6DOF VISUAL ODOMETRY ON THE 7-SCENES DATASET. Scene LBO [19] DeepVO [17] cnnBspp [15] VLocNet (Ours) Chess 1.69, 1.13 2.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Office 3</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The missing link between faces, text, planktons, and cat breeds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vidloc: 6-dof video-clip relocalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06521</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fab-map: Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative feature-to-point matching in image-based localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d visual phrases for landmark recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning visual odometry with a convolutional network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VISAPP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Camera relocalization by computing pairwise relative poses</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Laskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relative camera pose estimation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Melekhov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01381</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Image-based localization using hourglass networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ylioinas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07971</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepvo: A deep learning approach for monocular visual odometry</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mohanty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06069</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep regression for monocular camera-based 6-dof global localization in outdoor environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Naseer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning for laser based odometry estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSSws Limits and Potentials of Deep Learning in Robotics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Vision-based multi-task manipulation for inexpensive robots using end-to-end learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rahmatizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02920</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient effective prioritized matching for large-scale image-based localization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1744" to="1756" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the performance of convnet features for place recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting uncertainty in regression forests for accurate camera relocalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-based localization using lstms for structured feature correlation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Walch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hilsenbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional neural networks for camera relocalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for image understanding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCPaR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
