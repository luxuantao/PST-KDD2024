<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Interfaces: A Survey of Principles, Models and Frameworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bruno</forename><surname>Dumas</surname></persName>
							<email>bruno.dumas@unifr.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DIVA Group</orgName>
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<addrLine>Bd de Pérolles 90</addrLine>
									<postCode>1700</postCode>
									<settlement>Fribourg</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denis</forename><surname>Lalanne</surname></persName>
							<email>denis.lalanne@unifr.ch</email>
							<affiliation key="aff0">
								<orgName type="department">DIVA Group</orgName>
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<addrLine>Bd de Pérolles 90</addrLine>
									<postCode>1700</postCode>
									<settlement>Fribourg</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharon</forename><surname>Oviatt</surname></persName>
							<email>oviatt@incaadesigns.org</email>
							<affiliation key="aff1">
								<orgName type="department">Incaa Designs</orgName>
								<address>
									<addrLine>821 Second Ave., Ste. 1100</addrLine>
									<postCode>98104</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Interfaces: A Survey of Principles, Models and Frameworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31FCD5E68C8228C6E2B63198E65C731F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The grand challenge of multimodal interface creation is to build reliable processing systems able to analyze and understand multiple communication means in real-time. This opens a number of associated issues covered by this chapter, such as heterogeneous data types fusion, architectures for real-time processing, dialog management, machine learning for multimodal interaction, modeling languages, frameworks, etc. This chapter does not intend to cover exhaustively all the issues related to multimodal interfaces creation and some hot topics, such as error handling, have been left aside. The chapter starts with the features and advantages associated with multimodal interaction, with a focus on particular findings and guidelines, as well as cognitive foundations underlying multimodal interaction. The chapter then focuses on the driving theoretical principles, time-sensitive software architectures and multimodal fusion and fission issues. Modeling of multimodal interaction as well as tools allowing rapid creation of multimodal interfaces are then presented. The article concludes with an outline of the current state of multimodal interaction research in Switzerland, and also summarizes the major future challenges in the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Of the numerous ways explored by researchers to enhance human-computer communication, multimodal interaction has shown much development in the past decade. On one hand, multimodal interfaces target a more "human" way of interacting with computers, by means of speech, gestures or other modalities, as well as being preferred over unimodal interfaces by users <ref type="bibr" target="#b48">[49]</ref>; on the other hand, multimodal interfaces have been demonstrated to offer better flexibility and reliability than other human/machine interaction means <ref type="bibr" target="#b50">[51]</ref>.</p><p>As a research subject, multimodal interaction encompasses a broad spectrum of research domains, from cognitive psychology to software engineering, including human-computer interaction, which is already cross-disciplinary. While cognitive psychologists study how the human brain processes information and interacts through various modalities, interaction practitioners are interested by how humans use multimodal interfaces, and finally software engineers are interested in building tools and systems supporting the development of such multimodal interfaces, thus studying software architectures and multimodal processing techniques.</p><p>Cognitive psychologists have extensively studied how humans perceive, process, and express multimodal information; their conclusions are of interest for developers and HCI practitioners. The creation of a typical multimodal application requires a number of different components and careful implementation work. Hence, "good practices" and algorithms regarding the general architecture of a multimodal application, its fusion and fission engines or dialogue management components emerged during the past 20 years <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b61">62]</ref>. In a more theoretical way, modeling of multimodal interaction and, generally speaking, of the underlying human-machine dialog has seen extensive work. This theoretical work leads to the definition of a number of languages dedicated to multimodal data description, multimodal human-machine dialog modeling or multimodal applications scripting. Together with these different languages, different tools targeted at expediting the creation of multimodal interfaces have appeared.</p><p>This chapter runs the spectrum from cognitive foundations to development tools, with a particular emphasis on the multimodal processing aspects. The article is not an exhaustive summary of the findings and issues in this broad and multidisciplinary field, but rather presents the major issues and findings, with an emphasis on the driving principles for the creation of multimodal interfaces, their models, and programming frameworks. The chapter begins with a global view on multimodal interaction, with a presentation of its aims and advantages, its features, and cognitive foundations underlying multimodal systems; seminal works, findings and guidelines particular to multimodal interaction conclude this second section. The third section gives a detailed look at theoretical and practical principles of multimodal systems, architectures and key components of such systems; among those key components, fusion engines, fission engines and dialog management all have a dedicated subsection. The third section ends with a view of potential uses of machine learning for multimodal interaction. The fourth section focuses on modeling and creation of multimodal interfaces, with subsections detailing models, modeling languages and programming frameworks for multimodal interaction. The fifth section is devoted to multimodal applications in Switzerland, and the sixth and last section concludes this chapter with future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Foundations, Aims and Features of Multimodal Interaction</head><p>This section will present the aims underlying multimodal interaction research, as well as the distinctive features of multimodal interfaces compared to other types of interfaces. The first part will present a general view of multimodal systems, and more specifically their aims and advantages. The section continues with a part focused on particular features of multimodal interfaces, compared to standard GUI interfaces. The third part introduces cognitive theories linked to multimodal interaction design. Finally, the fourth part presents seminal works, findings and guidelines in the field of multimodal interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aims and Advantages of Multimodal Systems</head><p>Multimodal systems are computer systems endowed with multimodal capabilities for human/machine interaction and able to interpret information from various sensory and communication channels. Literally, multimodal interaction offers a set of "modalities" to users to allow them to interact with the machine. According to Oviatt <ref type="bibr" target="#b48">[49]</ref>, « Multimodal interfaces process two or more combined user input modes (such as speech, pen, touch, manual gesture, gaze, and head and body movements) in a coordinated manner with multimedia system output. They are a new class of interfaces that aim to recognize naturally occurring forms of human language and behavior, and which incorporate one or more recognition-based technologies (e.g. speech, pen, vision) ». Two unique features of multimodal architectures and processing are: (1) the fusion of different types of data; and (2) real-time processing and temporal constraints imposed on information processing <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Thus, multimodal systems represent a new class of user-machine interfaces, different from standard WIMP interfaces. They tend to emphasize the use of richer and more natural ways of communication, such as speech or gestures, and more generally all the five senses. Hence, the objective of multimodal interfaces is twofold: (1) to support and accommodate users' perceptual and communicative capabilities; and (2) to integrate computational skills of computers in the real world, by offering more natural ways of interaction to humans.</p><p>Multimodal interfaces were first seen as more efficient than unimodal interfaces; however, evaluations showed that multimodal interfaces only speed up task completion by 10% <ref type="bibr" target="#b49">[50]</ref>. Hence, efficiency should not be considered the main advantage of multimodal interfaces. On the other hand, multimodal interfaces have been shown to improve error handling &amp; reliability: users made 36% fewer errors with a multimodal interface than with a unimodal interface <ref type="bibr" target="#b49">[50]</ref>. Multimodal interfaces also add greater expressive power, and greater potential precision in visual-spatial tasks. Finally, they provide improved support for users' preferred interaction style, since 95%-100% of users prefer multimodal interaction over unimodal interaction <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>Compared to other types of human/computer interaction, multimodal interaction seeks to offer users a more natural and transparent interaction, using speech, gestures, gaze direction, etc. Multimodal interfaces are hence expected to offer easier, more expressively powerful and more intuitive ways to use computers. Multimodal systems have the potential to enhance human/computer interaction in a number of ways:</p><p>• Enhanced robustness due to combining different partial information sources;</p><p>• Flexible personalization based on user and context; • New functionality involving multi-user and mobile interaction.</p><p>When comparing multimodal user interfaces (MUI) with standard graphical user interfaces (GUI), it is possible to draw the following differences <ref type="bibr" target="#b53">[54]</ref>: In standard WIMP interaction style (Window, Icon, Menu, Pointing device), a singular physical input device is used to control the position of a cursor and present information organized in windows and represented with icons. In contrast, in multimodal interfaces, various modalities can be used as input streams (voice, gestures, facial expressions, etc.). Further, input from graphical user interfaces is generally deterministic, with either mouse position or characters typed on a keyboard used to control the computer. In multimodal interfaces, input streams have to be first interpreted by probabilistic recognizers (HMM, GMM, SOM, etc.) and thus their results are weighted by a degree of uncertainty. Further, events are not always clearly temporally delimited and thus require a continuous interpretation. Due to the multiple recognizers necessary to interpret multimodal input and the continuous property of input streams, multimodal systems depend on time synchronized parallel processing. Further, as we will see in the following section, the time sensitivity of multimodal systems is crucial to determining the order of processing multimodal commands in parallel or in sequence. Finally, multimodal systems often implement a distributed architecture, to deal out the computation and insure synchronization. Multimodal systems can be very resource demanding in some cases (e.g., speech/gesture recognition, machine-learning augmented integration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cognitive Foundations</head><p>The advantages of multimodal interface design are elucidated in the theory of cognitive psychology, as well as human-computer interaction studies, most specifically in cognitive load theory, gestalt theory, and Baddeley's model of working memory <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref>. Findings in cognitive psychology reveal:</p><p>• humans are able to process modalities partially independently and, thus, presenting information with multiple modalities increases human working memory; • humans tend to reproduce interpersonal interaction patterns during multimodal interaction with a system; • human performance is improved when interacting multimodally due to the way human perception, communication, and memory function.</p><p>For example, when processing both auditory and visual information during speech, a listener is able to extract a higher rate of lexical intelligibility (Grant &amp; Greenberg <ref type="bibr" target="#b23">[24]</ref>). This section thus presents works from cognitive science related to multimodal interaction, following cognitive load theory, gestalt theory and Baddeley's model of working memory; the section ends with the description of a framework aimed at human performance prediction.</p><p>Mousavi et al <ref type="bibr" target="#b43">[44]</ref> experimented with presenting students content using partly auditory and partly visual modes. The split-attention effect (Sweller et al. <ref type="bibr" target="#b65">[66]</ref>) that resulted "suggested that working memory has partially independent processors for handling visual and auditory material." The authors argued that if working memory is a primary limitation in learning, then increasing effective working memory by presenting information in a dual-mode form rather than a purely visual one, could expand processing capabilities. The results of Mousavi et al. were confirmed by Tindall-Ford et al. <ref type="bibr" target="#b66">[67]</ref>, who used more general types of tasks than pure mathematical ones, and by Mayer &amp; Moreno <ref type="bibr" target="#b38">[39]</ref> who studied the same effect with multimedia learning material. All this work is in line with the cognitive load theory, which assumes a limited working memory in which all conscious learning and thinking occurs, and an effectively unlimited long-term memory that holds a large number of automated schemas that can be brought into working memory for processing. Oviatt <ref type="bibr" target="#b52">[53]</ref> applied these findings to educational interface design in testing a number of different user-centered design principles and strategies, showing that user-interface design that minimizes cognitive load can free up mental resources and improve student performance. One strategy for accomplishing this is designing a multimodal interface for students.</p><p>In the design of map-based pen/voice interfaces, Oviatt et al. <ref type="bibr" target="#b54">[55]</ref> demonstrated that Gestalt theoretic principles successfully predicted a number of human behaviors, such as: users consistently followed a specific multimodal integration pattern (i.e. sequential versus simultaneous), and entrenched further in their pattern during error handling when you might expect them to switch their behavior. Gestalt theory also correctly predicted in this study a dominant number of subjects applying simultaneous integration over sequential integration.</p><p>The original short-term memory model of Baddeley &amp; Hitch <ref type="bibr" target="#b5">[6]</ref>, refined later by Baddeley <ref type="bibr" target="#b4">[5]</ref>, described short-term or working memory as being composed of three main components: the central executive (which acts as supervisory system and controls the flow of information), the phonological loop, and the visuo-spatial sketchpad, with the latter two dedicated to auditory-verbal and visuo-spatial information processing, respectively. Although these two slave processors are coordinated by a central executive, they function largely independently in terms of lower-level modality processing. This model was derived from experimental findings with dual-task paradigms. Performance of two simultaneous tasks requiring the use of two perceptual domains (i.e. a visual and a verbal task) were observed to be nearly as efficient as performance of individual tasks. In contrast, when a person tries to carry out two tasks simultaneously that use the same perceptual domain, performance is less efficient than when performing the tasks individually. As such, human performance is improved when interacting with two modalities that can be co-processed in separate stores.</p><p>Wickens <ref type="bibr" target="#b70">[72]</ref>[73] also developed a framework, the "multiple resource model", aimed at performance prediction involving coordination between user input and system output modes for different types of tasks. This model suggests that four different dimensions are to be taken into account when predicting coordination versus interference during human task processing involving different modes. The four dimensions considered are stages (perceptual/cognitive vs. response), sensory modalities (auditory vs. visual), codes (visual vs. spatial) and channels of visual information (focal vs. ambient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Seminal Works, Findings and Guidelines</head><p>Multimodal interfaces emerged approximately 30 years ago within the field of human/computer interaction with Richard Bolt's "Put-That-There" application <ref type="bibr" target="#b8">[9]</ref>, which was created in 1980. First multimodal systems sought ways to go beyond the standard interaction mode at this time, which was graphical interfaces with keyboards and mice. Bolt's "Put-that-there" processed spoken commands linked to a pointing gesture using an armrest-mounted touchpad to move and change shapes displayed on a screen in front of the user. Since this seminal work, multimodal interaction practitioners have strived to integrate more modalities, to refine hardware and software components, and to explore limits and capabilities of multimodal interfaces. Historically, the main trend has focused on pointing and speech combined using speech/mouse, speech/pen <ref type="bibr" target="#b16">[17]</ref>, speech/gesture <ref type="bibr" target="#b44">[45]</ref>, or speech/gaze tracking <ref type="bibr" target="#b30">[31]</ref>. Later multimodal interfaces evolved beyond pointing into richer interaction, allowing users to produce symbolic gestures such as arrows and encircling.</p><p>Another direction in multimodal research has been speech/lip movement integration <ref type="bibr">[57][12]</ref>, driven by cognitive science research in intersensory audio-visual perception. This kind of work has included classification of human lip movement (visemes) and the viseme-phoneme mappings that occur during articulated speech. Such work has contributed improving robustness of speech recognition in noisy environments. For more details about these systems, see <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. 10 myths of multimodal interaction (We acknowledge ACM for allowing the reprint of this table)</head><p>Myth #1: If you build a multimodal system, users will interact multimodally. Myth #2: Speech and pointing is the dominant multimodal integration pattern. Myth #3: Multimodal input involves simultaneous signals. Myth #4: Speech is the primary input mode in any multimodal system that includes it. Myth #5: Multimodal language does not differ linguistically from unimodal language. Myth #6: Multimodal integration involves redundancy of content between modes. Myth #7: Individual error-prone recognition technologies combine multimodally to produce even greater unreliability. Myth #8: All users' multimodal commands are integrated in a uniform way. Myth #9: Different input modes are capable of transmitting comparable content. Myth #10: Enhanced efficiency is the main advantage of multimodal systems.</p><p>In the course of the last decade, researchers have highlighted particular empirical findings that have guided the design of multimodal interfaces compared to other sorts of human-computer interfaces. Key findings are illustrated in the following "10 myths" shown in Table <ref type="table">2</ref>, which exposed common engineering myths regarding how people interact multimodally <ref type="bibr" target="#b51">[52]</ref>. Based on empirical findings, Oviatt distilled implications for how more effective multimodal interfaces could be designed.</p><p>In more recent years, research has also focused on mainstreaming multimodal interfaces. In this trend, Reeves et al. defined the following "guidelines for multimodal user interface design" <ref type="bibr" target="#b58">[59]</ref>:</p><p>• Multimodal systems should be designed for the broadest range of users and contexts of use, since the availability of multiple modalities supports flexibility. For example, the same user may benefit from speech input in a car, but pen input in a noisy environment.</p><p>• Designers should take care to address privacy and security issues when creating multimodal systems: speech, for example, should not be used as a modality to convey private or personal information in public contexts.</p><p>• Modalities should be integrated in a manner compatible with user preferences and capabilities, for example, combining complementary audio and visual modes that users can co-process more easily. • Multimodal systems should be designed to adapt easily to different contexts, user profiles and application needs.</p><p>• Error prevention and handling is a major advantage of multimodal interface design, for both user-and system-centered reasons. Specific guidelines include integrating complementary modalities to improve system robustness, and giving users better control over modality selection so they can avoid errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Principles of User-Computer Multimodal Interaction</head><p>The driving principles of multimodal interaction are well described in numerous surveys <ref type="bibr" target="#b7">[8]</ref>[26] <ref type="bibr" target="#b50">[51]</ref>[54] <ref type="bibr" target="#b61">[62]</ref>. The following concepts are popularly accepted: fusion (also called multimodal signal integration), fission (also called response planning), dialog management, context management and time-sensitive architectures. In the following subsections, we introduce these concepts, at a high level first to illustrate how they are organized around a common conceptual architecture, and later at a lower level to probe key principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical Principles</head><p>Inspired by Norman's action cycle <ref type="bibr" target="#b46">[47]</ref>, and based on well accepted findings and taxonomies, the following model of multimodal man-machine communication can be drawn, together with the major concepts that should be considered when building a multimodal system (Figure <ref type="figure">1</ref>): the fusion of multimodal inputs, and the multimodal fission to generate an adequate message to the user, according to the context of use, preferences and profile. When a human interacts with a machine, his communication can be divided in four different states. The first state is a decision state, in which the communication message content is prepared consciously for an intention, or unconsciously for attentional content or emotions. The second state is the action state, where the communication means to transmit the message are selected, such as speech, gestures or facial expressions. The machine, in turn, will make use of a number of different modules to grasp the most information possible from a user, and will have similarly four main states Fig. <ref type="figure">1</ref>. A representation of multimodal man machine interaction loop (Figure <ref type="figure">1</ref>). At first, the messages are interpreted in the perception state, where the multimodal system receives information from one or multiple sensors, at one or multiple levels of expression. In the interpretation state, the multimodal system will try to give some meaning to the different information it collected in the perception state. This is typically the place where fusion of multimodal messages takes place. Further, in the computational state, action is taken following the business logic and dialogue manager rules defined by the developer. Depending on the meaning extracted in the interpretation state, an answer is generated and transmitted in the action state, in which a fission engine will determine the most relevant modalities to return the message, depending on the context of use (e.g. in the car, office, etc.) and the profile of the user (blind user, elderly, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational Architecture and Key Components</head><p>The previous section illustrated multimodal man-machine interaction underlying features. In this section, we describe multimodal interaction from the machine side, and the major software components that a multimodal system should contain. The generic components for handling of multimodal integration are: a fusion engine, a fission module, a dialog manager and a context manager, which all together form what is called the "integration committee". Figure <ref type="figure">2</ref> illustrates the processing flow between these components, the input and output modalities, as well as the potential client applications. As illustrated in the figure, input modalities are first perceived though various recognizers, which output their results to the fusion engine, in charge of giving a common interpretation of the inputs. The various levels at which recognizers' results can be fused are described in the next section, together with the various Fig. <ref type="figure">2</ref>. The architecture of a multimodal system, with the central integration committee and its major software components fusion mechanisms. When the fusion engine comes to an interpretation, it communicates it to the dialog manager, in charge of identifying the dialog state, the transition to perform, the action to communicate to a given application, and/or the message to return through the fission component. The fission engine is finally in charge of returning a message to the user through the most adequate modality or combination of modalities, depending on the user profile and context of use. For this reason, the context manager, in charge of tracking the location, context and user profile, closely communicates any changes in the environment to the three other components, so that they can adapt their interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusion of Input Modalities</head><p>Fusion of input modalities is one of the features that distinguish multimodal interfaces from unimodal interfaces. The goal of fusion is to extract meaning from a set of input modalities and pass it to a human-machine dialog manager. Fusion of different modalities is a delicate task, which can be executed at three levels: at data level, at feature level and at decision level. Three different types of architectures can in turn Sharma et al. <ref type="bibr" target="#b61">[62]</ref> consider these three levels for fusion of incoming data. Each fusion scheme functions at a different level of analysis of the same modality channel. As a classic illustration, consider the speech channel: data from this channel can be processed at the audio signal level, at the phoneme (feature) level, or at the semantic (decision) level (Figure <ref type="figure" target="#fig_0">3</ref>).</p><p>• Data-level fusion is used when dealing with multiple signals coming from a very similar modality source (e.g., two webcams recording the same scene from different viewpoints). With this fusion scheme, no loss of information occurs, as the signal is directly processed. This benefit is also the main shortcoming of data-level fusion. Due to the absence of pre-processing, it is highly susceptible to noise and failure.</p><p>• Feature-level fusion is a common type of fusion when tightly-coupled or time synchronized modalities are to be fused. The standard example is the fusion of speech and lip movements. Feature-level fusion is susceptible to low-level information loss, although it handles noise better. The most classic architectures used for this type of fusion are adaptive systems like artificial neural networks, Gaussian mixture models, or hidden Markov models. The use of these types of adaptive architecture also means that feature-level fusion systems need numerous data training sets before they can achieve satisfactory performance.</p><p>• Decision-level fusion is the most common type of fusion in multimodal applications. The main reason is its ability to manage loosely-coupled modalities like, for example, pen and speech interaction. Failure and noise sensitivity is low with decision-level feature, since the data has been preprocessed. On one hand, this means that decision-level fusion has to rely on the quality of previous processing. On the other hand, unification-based decision-level fusion has the major benefit of improving reliability and accuracy of semantic interpretation, by combining partial semantic information coming from each input mode which can yield "mutual disambiguation" <ref type="bibr" target="#b48">[49]</ref>.</p><p>Table 3 below summarizes the three fusion levels, their characteristics, sensitivity to noise, and usage contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Characteristics of fusion levels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-level fusion Features-level fusion Decision-level fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input type Raw data of same type Closely coupled modalities</head><p>Loosely coupled modalities</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level of information</head><p>Highest • Frame-based fusion <ref type="bibr" target="#b69">[70]</ref> uses data structures called frames or features for meaning representation of data coming from various sources or modalities. These structures represent objects as attribute-value pairs. • Unification-based fusion <ref type="bibr" target="#b26">[27]</ref> is based on recursively merging attribute-value structures to obtain a logical whole meaning representation.</p><p>• Symbolic/statistical fusion <ref type="bibr" target="#b72">[74]</ref> is an evolution of standard symbolic unificationbased approaches, which adds statistical processing techniques to the fusion techniques described above. These kinds of "hybrid" fusion techniques have been demonstrated to achieve robust and reliable results. An example of a symbolicstatistical hybrid fusion technique is the Member-Team-Committee (MTC) architecture used in Quickset <ref type="bibr" target="#b73">[75]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fission of Output Modalities</head><p>When multiple output modalities such as text-to-speech synthesis, audio cues, visual cues, haptic feedback or animated agents are available, output selection becomes a delicate task to adapt to a context of use (e.g. car, home, work), type of task (e.g., information search, entertainment) or type of user (e.g. visually impaired, elderly).</p><p>Fission techniques <ref type="bibr" target="#b22">[23]</ref> allow a multimodal application to generate a given message in an adequate form according to the context and user profiles. Technically speaking, fission consists of three tasks: • Message construction, where the information to be transmitted to the user is created; approaches for content selection and structuring revolve mainly around either schema-based approaches or plan-based approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>• Output channel selection, where interfaces are selected according to context and user profile in order to convey all data effectively in a given situation. Characteristics such as available output modalities, information to be presented, communicative goals of the presenter, user characteristics and task to be performed are forms of knowledge that can be used for output channel selection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. • of a coherent and synchronized result: when multiple output channels are used, layout and temporal coordination are to be taken into account. Moreover, some systems will produce multimodal and cross-modal referring expressions, which will also have to be coordinated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dialogue Management and Time-Sensitive Architectures</head><p>The time constraint is highly important in multimodal systems and all the modalities should be properly time-stamped and synchronized. Time-sensitive architectures need to establish temporal thresholds for time-stamping start and end of each input signal piece, so that two commands sequences can be identified. Indeed, when two commands are performed in parallel, in a synergistic way, it is important to know in which order the commands have been entered because the interpretation will vary accordingly. For instance, in the following application, in which voice and gestures are used simultaneously to control a music player, depending on the order in which modalities are presented the interpretation varies:</p><p>• &lt;pointing&gt; "Play next track": will result in playing the track following the one selected with a gesture; • "Play" &lt;pointing&gt; "next track": will result in first playing the manually selected track and then passing to the following at the time "next is pronounced"; • "Play next track" &lt;pointing&gt;: In this case, the system should interpret the commands as being redundant. The dialog management system and synchronization mechanism should consider multiple potential causes of lag:</p><p>• delay due to technology (e.g. speech recognition); • delay due to multimodal system architecture; • user differences in habitual multimodal integration pattern <ref type="bibr" target="#b50">[51]</ref> <ref type="bibr" target="#b54">[55]</ref>.</p><p>For this reason, multi-agent architectures (or similar architectures such as components-based systems) are advantageous for distributing processing and for coordinating many system components (e.g., speech recognition, pen recognition, natural language processing, graphic display, TTS output, application database).</p><p>Bui <ref type="bibr" target="#b12">[13]</ref> considers four different approaches to dialog management: • Finite-state and frame-based approaches: in this kind of dialog management approach, the dialog structure is represented in the form of a state machine. Framebased models are an extension of finite-state models, using a slot-filling strategy in which a number of predefined information sources are to be gathered <ref type="bibr" target="#b15">[16]</ref>.</p><p>• Information state-based and probabilistic approaches: these approaches try to describe human-machine dialog following information states, consisting of five main components: informational components, formal representations of those components, a set of dialog moves, a set of update rules and an update strategy <ref type="bibr" target="#b67">[68]</ref>.</p><p>• Plan-based approaches: the plan-based approaches are based on the plan-based theories of communicative action and dialog <ref type="bibr" target="#b15">[16]</ref>. These theories claim that the speaker's speech act is part of a plan and that it is the listener's job to identify and respond appropriately to this plan <ref type="bibr" target="#b14">[15]</ref>.</p><p>• Collaborative agents-based approaches: these approaches view dialog as a collaborative process between intelligent agents. The agents work together to obtain a mutual understanding of the dialog. This induces discourse phenomena such as clarifications and confirmations <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Machine Learning for Multimodal Interaction</head><p>Machine learning techniques play an important role in multimodal interfaces <ref type="bibr" target="#b25">[26]</ref>, and most certainly will continue to extend this role. Indeed, many parts of multimodal systems are likely to receive support from machine learning. Modality recognizers already make extensive use of machine learning: speech recognition, face detection, face recognition, facial expression analysis, gesture recognition or eye tracking are examples of different domains of interest both for multimodal interaction and machine learning.</p><p>Aside from modality handling, machine learning has been applied for fusion of input recognizers' data, mainly at the feature level. Fewer works have been achieved on decision level fusion with assistance from machine learning. An example of such work is Pan et al. <ref type="bibr" target="#b54">[55]</ref>, who proposed context-dependent versions of Bayesian inference method for multisensory data fusion. Nonetheless, Jaimes &amp; Sebe <ref type="bibr" target="#b25">[26]</ref> reckon that "further research is still required to investigate fusion models able to efficiently use the complementary cues provided by multiple modalities". User, task and context modeling also can benefit from machine learning techniques. Novel research fields related to machine learning, such as social signal processing <ref type="bibr" target="#b63">[64]</ref>, will help building a refined representation of the user in her collaborative context. Adaptability can then be addressed with the help of machine learning, by watching the users' behavior in the sensed context <ref type="bibr" target="#b20">[21]</ref>.</p><p>As Jaimes &amp; Sebe <ref type="bibr" target="#b25">[26]</ref> highlight, currently "most researchers process each channel (visual, audio) independently, and multimodal fusion is still in its infancy". Thus, multimodal interaction researchers have work to achieve in order to attain efficient multimodal fusion, with careful consideration of the different available modalities and the way modalities interlock. Machine learning will be of interest in order to attain such a goal. Besides multimodal fusion, machine learning will help multimodal applications take into account the affective aspect of communication -emotions based on their physiological manifestations <ref type="bibr" target="#b40">[41]</ref>, such as facial expressions, gestures, postures, tone of voice, respiration, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Languages and Frameworks</head><p>There have been several attempts to model and formalize multimodal interaction. This section presents several different levels of modeling. The first part introduces two abstract models designed to help developers evaluate the different types of multimodal interaction, viewed first from the machine side, then from the user side. The second part lists a number of languages used for multimodal recognizer output and multimodal synthesizer input representations, and modeling languages used to configure multimodal systems. The final part displays different programming frameworks for rapid creation of multimodal interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Interaction Modeling</head><p>Modeling multimodal interaction is no simple task, due to the multiple input and output channels and modes, and the combination of possibilities between data coming from different sources, not to mention output modality selection based on context and user profile.</p><p>The shape taken by formal modeling of multimodal interaction depends on the level of abstraction considered. At lower levels of abstraction, formal modeling would focus on tools used for modality recognition and synthesis. At higher levels of abstraction, multimodal interaction modeling would focus more on modality combination and synchronization.</p><p>Formal modeling can also focus on the "pure" technical part as well as on the usermachine interaction. Two formal models exist for modality combination description:</p><p>• The CASE model <ref type="bibr" target="#b45">[46]</ref>, focusing on modality combination possibilities at the fusion engine level;</p><p>• the CARE model <ref type="bibr" target="#b17">[18]</ref>, giving attention to modality combination possibilities at the user level.</p><p>The CASE model introduces four properties: Concurrent -Alternate -Synergistic -Exclusive (figure <ref type="figure" target="#fig_1">4</ref>). Each of those four properties describes a different way to combine modalities at the integration engine level, depending on two factors: combined or independent fusion of modalities, and sequential or synergistic use of modalities on the other hand. "Fusion of modalities" considers if different modalities are combined or managed independently, whereas "Use of modalities" observes the way modalities are activated: either one at a time, or in a synergistic manner.</p><p>The CARE model is more focused on the user-machine interaction level. This model also introduces four properties, which are Complementarity -Assignment -Redundancy -Equivalence. Complementarity is to be used when multiple complementary modalities are necessary to grasp the desired meaning (e.g. "put that there" <ref type="bibr" target="#b8">[9]</ref> would need both pointing gestures and voice in order to be resolved). Assignment indicates that only one modality can lead to the desired meaning (e.g. the steering wheel of a car is the only way to direct the car). Redundancy implies multiple modalities which, even if used simultaneously, can be used individually to lead to the desired meaning (e.g. user utters a "play" speech command and pushes a button labeled "play", but only one "play" command would be taken into account). Finally, Equivalence entails multiple modalities that can all lead to the desired meaning, but only one would be used at a time (e.g. speech or keyboard can be used to write a text). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multimodal Interaction Modeling Languages</head><p>Interesting attempts at creating a full-fledged language for description of usermachine multimodal interaction have arisen in the past few years. Most of the approaches presented below revolve around the concept of a "multimodal web", enforced by the World Wide Web Consortium (W3C) Multimodal Interaction Activity and its proposed multimodal architecture [71]. This theoretical framework describes major components involved in multimodal interaction, as well as potential or existent markup languages used to relate those different components. Many elements described in this framework are of practical interest for multimodal HCI practitioners, such as the W3C EMMA markup language, or modality-focused languages such as VoiceXML or InkML. The work of the W3C inspired Katsurada et al. for their work on the XISL XML language <ref type="bibr" target="#b27">[28]</ref>. XISL focuses on synchronization of multimodal input and output, as well as dialog flow and transition. Another approach of the problem is the one of Araki et al. <ref type="bibr" target="#b3">[4]</ref>, who propose MIML (Multimodal Interaction Markup Language). One of the key characteristics of this language is its three-layered description of interaction, focusing on interaction, tasks and platform. Finally, Stanciulescu et al. <ref type="bibr" target="#b63">[64]</ref> followed a transformational approach for developing multimodal web user interfaces based on UsiXML, also in the steps of the W3C. Four steps are achieved to go from a generic model to the final user interface. Thus, one of the main features of their work is a strong independence to the actual input and output available channels.</p><p>Sire and Chatty describe in <ref type="bibr" target="#b62">[63]</ref> what one should expect from a multimodal user interfaces programming language. From their proposal, the following requirements for a multimodal description language have been derived.</p><p>• Such a language should be modality agnostic, as research in input and output modalities continues to evolve today.</p><p>• A binding mechanism to link the definition of the user interface composition with its runtime realization should be provided.</p><p>• Explicit control structures should be present, such as conditional clauses and loops.</p><p>• Extensible event definition mechanisms are also needed for communication between user interface objects and the interaction model.</p><p>• Data Modeling should be carefully planned, as application data tends to be distributed in multiple places.</p><p>• Finally, a major requirement for a multimodal integration description language is the definition of reusable components.</p><p>"Modality agnostic" is the most debatable of those requirements, as one could argue that such a requirement will never be achievable, as every modality has its own particularities. Our interpretation of this requirement is the following: "modality agnostic" means that the language should not be specific for each individual modality, as modalities are all different; the language should be flexible enough (or canonic enough) to be adapted to a new and different modality. Hence, if a scripting or programming language can be in principle modality agnostic, such cannot be said of the fusion engine that needs to take into account the specificities of each modality to fuse data or features correctly.</p><p>A last point that stems from these six guidelines is readability: a language for description of multimodal interaction should be readable, as much in regard to the machine as to humans.</p><p>Formal languages for description of multimodal description can be approached from two different directions: either from expressiveness, or from usability. Expressiveness covers technical features such as extensibility, completeness, reusability, or temporal aspects considerations; usability covers more human features such as programmability or readability. Any formal language will have to find its place between those two general requirements; some languages will tend more toward expressiveness or usability. An interesting approach is to seek balance between usability and expressiveness: that is, a language able to configure a multimodal system, with high level modeling, and readable enough to be used as a learning tool, or even a communication tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Programming Frameworks</head><p>Further to multimodal interface creation, a number of tools have become available in recent years. Krahnstoever et al. <ref type="bibr" target="#b31">[32]</ref> proposed a framework using speech and gestures to create a natural interface. The output of their framework was to be used on large screen displays enabling multi-user interaction. Fusion was done using a unification-based method. Cohen et al. <ref type="bibr" target="#b16">[17]</ref> worked on Quickset, a speech/pen multimodal interface, based on Open Agent Architecture, which served as a test bed for unification-based and hybrid fusion methods. Bourguet <ref type="bibr" target="#b10">[11]</ref> endeavored in the creation of a multimodal toolkit in which multimodal scenarios could be modelled using finite state machines. This multimodal toolkit is composed of two components, a graphical user interface named IMBuilder which interfaces the multimodal framework itself, named MEngine. Multimodal interaction models created with IMBuilder are saved as a XML file. Flippo et al. <ref type="bibr" target="#b21">[22]</ref> also worked on the design of a multimodal framework, geared toward direct integration into a multimodal application. One of the most interesting aspects of their work is the use of a parallel application-independent fusion technique. The general framework architecture is based on agents, while the fusion technique itself uses frames. Configuration of the fusion is done via an XML file, specifying for each frame a number of slots to be filled and direct link to actual resolver implementations. Lastly, Bouchet et al. <ref type="bibr" target="#b9">[10]</ref> proposed a component-based approach called Table 4. Characteristics of different tools for creation of multimodal interfaces ICARE -OI <ref type="bibr" target="#b9">[10]</ref> OpenInterface <ref type="bibr" target="#b60">[61]</ref> IMBuilder/ MEngine <ref type="bibr" target="#b10">[11]</ref> Flippo et al. <ref type="bibr" target="#b21">[22]</ref> Krahnstoever <ref type="bibr" target="#b31">[32]</ref> Quickset <ref type="bibr" target="#b16">[17]</ref> Phidgets <ref type="bibr" target="#b24">[25]</ref> Papier-Mâché <ref type="bibr" target="#b29">[30]</ref> Architecture ICARE thoroughly based on the CARE <ref type="bibr" target="#b17">[18]</ref> design space. These components cover elementary tasks, modality-dependent tasks or generic tasks like fusion. Finally, communication between components is based on events. The components-based approach of ICARE has provided inspiration for a comprehensive open-source toolkit called OpenInterface <ref type="bibr" target="#b60">[61]</ref>. OpenInterface components are configured via CIDL XML files, and a graphical editor. Table <ref type="table">4</ref> summarizes the different characteristics of the systems described above: extensible systems (i.e. toolkits) have the potential ability to add other input modalities in a practical way. Pluggability refers to the ability of a toolkit to insert itself into an architecture without having to rewrite everything. The other characteristics are self-explanatory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multimodal Interfaces in Switzerland</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multimodal Interfaces in IM2</head><p>The Swiss National Center of Competence in Research (NCCR) on Interactive Multimodal Information Management (IM2) is one of the 20 Swiss National Centers of Competence in Research (NCCR). IM2 aims at developing natural multimodal interfaces for human-computer interaction and to foster collaboration, focusing on new multimodal technologies to support human interaction, in the context of smart meeting rooms and remote meeting assistants.</p><p>The Individual Project on "Human Machine Interaction" is part of the NCCR IM2. While other activities in IM2 develop multimodal analysis and recognition technologies, the primary objective of IM2.HMI is to build cutting-edge technologies to develop interactive multimodal meeting browsers. The main goal of IM2.HMI is to design, develop and evaluate, with human subjects, novel interactive multimodal meeting browsers/assistants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Multimodal processing chain in IM2 meeting application</head><p>In order to support the development of so-called meeting browsers (4), and facilitate access to multimodal data and annotations (2), the JFerret framework has been designed and implemented. Using the JFerret framework, and taking benefits of most of the multimodal analysis, multimodal input recognizers and multimodal indexing and retrieval strategies made available in IM2, various meeting browsers have been implemented <ref type="bibr" target="#b32">[33]</ref>. Those meeting browsers take benefit of most of the annotations made available by the other IM2 IPs: speech browsers (accelerated and overlapped), document-centric meeting browsers (JFriDoc, FaericWorld) <ref type="bibr" target="#b59">[60]</ref>, Dialog-centric browsers (TQB) <ref type="bibr" target="#b57">[58]</ref>, multimodal enabled browsers (Archivus, HephaisTK), multilingual (M3C) and recently personalized browsers (WotanEye) <ref type="bibr" target="#b33">[34]</ref>. Most of these meeting browsers are in fact complete and transversal systems that access the multimodal meeting data, analyse them, process high level indexes and provide interactive user interfaces so that the user can browse the meeting corpora through multimodal queries. In the last couple of years, IM2.HMI has gently shifted towards online, a.k.a real-time, meeting assistance leveraging on past works. This includes new research on personalized meeting browsing, mobile and remote access to meetings <ref type="bibr" target="#b37">[38]</ref>, and meeting assistance before, during and after meetings.</p><p>IM2.HMI has tackled multimodality both at the content and at the interaction levels. While projects handling multimodality at the content level try to use the best of multimodal data indexing in order to create useful and usable meeting browsers, research projects handling multimodality at the interaction level study and build novel multimodal interaction paradigms, benefiting from various input modes.</p><p>Archivus, developed in the framework of IM2, is a good example of a research project handling multimodality both at the content and interaction levels. Archivus is a multimodal (pen, voice, mouse and keyboard) language-enabled dialogue-based interface for browsing and retrieving multimodal meeting data <ref type="bibr" target="#b0">[1]</ref>. It allows users to access a multimedia database of recorded and annotated meetings, containing the original video and audio streams, electronic copies of all documents used or referred to as well as handwritten notes made by participants during the meeting, and a text transcript of the meeting itself <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>. Multimodal man-machine interaction in this context has been carefully studied. Large-scale Wizard of Oz experiments with the system (involving 91 users) were carried out and it resulted in 180 hours of video data and 70MB of text log files. The data was analyzed along several different lines including the modalities most often used, contexts of use, relationships between modalities, usage change over time, training impact, etc. <ref type="bibr" target="#b35">[36]</ref>. To summarize the major findings: exposure and training can have a strong impact on the way people use multimodality, and speech is a preferred modality both at the content and interaction levels, i.e. as a cue for querying the multimodal database and as an interaction channel.</p><p>HephaisTK, developed both in the framework of the NCCR IM2 and of the MeModules project presented in chapter 5, handles multimodality at the interaction level and aims at providing a tool allowing developers to easily prototype multimodal interfaces <ref type="bibr" target="#b19">[20]</ref>. The HephaisTK toolkit has been designed to plug itself in a client application that wishes to receive notifications of multimodal events received from a set of modality recognizers. It is based on a software agents architecture, in which agents, collaborating through a blackboard, are dispatched to manage individual modality recognizers, handle fusion and dialog management. HephaisTK can be configured with the SMUIML language (Synchronized Multimodal User Interfaces Markup Language) <ref type="bibr" target="#b18">[19]</ref>, allowing a clear description of the human-machine multimodal dialog and control over the way multiple input modalities have to be fused. More details about this tool can be found in chapter 5 of this book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multimodal Interfaces in the MMI Program</head><p>The IM-HOST project, described in detail in chapter 4 of this book, is representative of one class of multimodal applications, although it focuses on a single modality: speech, which has been historically the leading modality in multimodal interaction. The IM-HOST project targets voice-enabled man-machine interaction in noisy environments. However, still, current performances of voice applications are reasonably good in quiet environments but the surrounding noise in many practical situations drastically deteriorates the quality of the speech signal and, as a consequence, significantly decreases the recognition rate. The major scenario considered in this project is a person using voice command in an outdoor environment: a racing boat. For this reason, the project explores new interaction paradigms enabling voice recognition in a hostile environment.</p><p>The MeModules project, fully detailed in chapter 5 of this book, has the objective of developing, experimenting and evaluating the concept of tangible shortcuts to multimedia digital information. Moreover, it investigates the opportunity of a more complex, multi-sensorial combination of physical objects with multimedia information by associating tangible interaction with multiple other interaction modalities such as voice, gesture, etc. One of the expected research outcomes of the project is to assess which modalities are best combined with tangible interaction depending on the context and application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Directions and Conclusions</head><p>Although many issues have been addressed well in the multimodal interaction research and systems literature, such as fusion of heterogeneous data types, architectures for real-time processing, dialog management, map-based multimodal interaction, and so forth, nonetheless the field is still young and needs further research to build reliable multimodal systems and usable applications. Machine learning methods have begun to be applied to a number of different aspects of multimodal interfaces, including individual modality recognition, early or late modality fusion, user-machine dialog management, and identification of users' multimodal integration patterns. But future work clearly is needed to work toward the design of usable adaptive multimodal interfaces. Multimodal dialog processing also will gain in the future from the recent and promising subfield of social signal processing, which can assist dialog modeling by providing a dialog manager with real-time information about a given user's state and her current social and collaborative context.</p><p>Other important future directions for multimodal research include human/machine interaction using new tangible interfaces such as digital paper and pen, and multitouch tables, surfaces and screens. Further modeling of multimodal interaction still is needed too, in areas such as multimodal educational exchanges, collaborative multimodal interaction, multimodal interaction involving diverse and underserved user groups, and mobile multimodal interaction with emerging cell phone applications. Finally, further work is needed to improve tools for the creation of multimodal applications and interfaces so they can become more mainstream, especially since multimodal interfaces are viewed as the most promising avenue for achieving universal access in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The various levels of multimodal fusion</figDesc><graphic coords="10,42.18,55.40,340.44,69.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The CASE model</figDesc><graphic coords="15,128.46,55.40,173.28,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,44.94,55.40,340.44,210.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,42.18,55.55,345.88,297.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Differences between GUIs and MUIs</figDesc><table><row><cell>GUI</cell><cell>MUI</cell></row><row><cell>Single input stream</cell><cell>Multiple input streams</cell></row><row><cell>Atomic, deterministic</cell><cell>Continuous, probabilistic</cell></row><row><cell>Sequential processing</cell><cell>Parallel processing</cell></row><row><cell>Centralized architectures</cell><cell>Distributed &amp; time-sensitive architectures</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ailomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Melichar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajmanm</surname></persName>
		</author>
		<title level="m">Archivus: A Multimodal System for Multimedia Meeting Browsing and Retrieval</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">July 17th-21st (2006</date>
		</imprint>
	</monogr>
	<note>Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing Intentions in Dialogues</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Perault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="178" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The generation of multimedia documents</title>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Dale</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Moisl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Somers</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="305" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal Dialog Description Language for Rapid System Development</title>
		<author>
			<persName><forename type="first">M</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the 7th SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Working Memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="page" from="556" to="559" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Working Memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent advances in learning and motivation</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Bower</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the knowledge underlying multimedia presentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vossers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Interfaces</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting><address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1993">1993. 1998</date>
			<biblScope unit="page" from="157" to="172" />
		</imprint>
	</monogr>
	<note>Reprinted in Maybury and Wahlster</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audio-visual and multimodal speech-based systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Suhm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Multimodal and Spoken Dialogue Systems: Resources, Terminology and Product Evaluation</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Mertins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="102" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Put-that-there: voice and gesture at the graphics interface</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICARE Software Components for Rapidly Developing Multimodal Interfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nigay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ganille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings of ICMI 2004, State College, Pennsylvania</title>
		<meeting><address><addrLine>USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Toolkit for Creating and Testing Multimodal Interface Designs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bourguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion proceedings of UIST 2002</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing speech: Investigations into the synthesis and recognition of visible speech movements using automatic image processing and computer graphics</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech Input and Output: Techniques and Applications</title>
		<meeting>the International Conference on Speech Input and Output: Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multimodal Dialogue Management -State of the Art</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<idno>series No. 06-01</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>University of Twente</publisher>
			<pubPlace>UT), Enschede, The Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Psychology of Human-Computer Interaction. Lawrence Erlbaum Associates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Churcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Atwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Souter</surname></persName>
		</author>
		<title level="m">Dialogue management systems: a survey and overview</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Survey of the State of the Art in Human Language Technology</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Varile</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zaenen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zampolli</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
	<note>Dialogue Modeling</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">QuickSet: multimodal interaction for distributed applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM international Conference on Multimedia</title>
		<meeting>the Fifth ACM international Conference on Multimedia<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Four Easy Pieces for Assessing the Usability of Multimodal Interaction: The CARE properties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Coutaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nigay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blandford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERACT 1995</title>
		<meeting>INTERACT 1995<address><addrLine>Lillehammer, Norway; Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall Publ</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototyping Multimodal Interfaces with SMUIML Modeling Language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI 2008 Workshop on User Interface Description Languages for Next Generation User Interfaces, CHI 2008</title>
		<meeting><address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="63" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strengths and Weaknesses of Software Architectures for the Rapid Creation of Tangible and Multimodal Interfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guinard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd international conference on Tangible and Embedded Interaction</title>
		<meeting>2nd international conference on Tangible and Embedded Interaction<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-02-19">2008. February 19 -21. 2008</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integrating perceptual and cognitive modeling for adaptive and intelligent human-computer interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Duric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the IEEE</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1272" to="1289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Framework for Rapid Development of Multimodal Interfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Flippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI 2003</title>
		<meeting>ICMI 2003<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07">November 5-7. 2003</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">State of the art review: Multimodal fission</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMIC project Deliverable</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002-09">September 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech intelligibility derived from asynchronous processing of auditory-visual information</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Audio-Visual Speech Processing</title>
		<meeting><address><addrLine>Scheelsminde, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phidgets: easy development of physical interfaces through physical widgets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fitchett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual ACM Symposium on User interface Software and Technology</title>
		<meeting>the 14th Annual ACM Symposium on User interface Software and Technology<address><addrLine>Orlando, Florida; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal human-computer interaction: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="116" to="134" />
			<date type="published" when="2007">2007</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unificationbased multimodal integration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on European Chapter</title>
		<meeting>the Eighth Conference on European Chapter<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>the Association For Computational Linguistics</publisher>
			<date type="published" when="1997-12">July 07-12. 1997</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">XISL: a language for describing multimodal interaction scenarios</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katsurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI 2003</title>
		<meeting>ICMI 2003<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An overview of the EPIC architecture for cognition and performance with application to human-computer interaction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kieras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="391" to="438" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<title level="m">Papier-Mâché: Toolkit Support for Tangible Input</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
	<note>Proceedings of CHI 2004</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating simultaneous input from speech, gaze, and hand gestures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sparrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thorisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Interfaces</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="257" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A real-time framework for natural multimodal interaction with large screen displays</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kettebekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<date type="published" when="2002-10">2002. October 2002</date>
			<pubPlace>Pittsburgh, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Janvier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Melichar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moenne-Loccoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu-Belis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Von Rotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wellner</surname></persName>
		</author>
		<title level="m">The IM2 Multimodal Meeting Browser Family</title>
		<meeting><address><addrLine>Fribourg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An ego-centric and tangible approach to meeting indexing and browsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Evequoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMI 2007</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Popescu-Belis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4892</biblScope>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<title level="m">Multimodal Interface Design for Multimedia Meeting Content Retrieval</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Minimizing Modality Bias When Exploring Input Preference for Multimodal Systems in New Domains: the Archivus Case Study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betrancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI 2007</title>
		<meeting>CHI 2007<address><addrLine>San José, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1805" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multimodal Interface Design for the Multimodal Meeting Domain: Preliminary Indications from a Query Analysis Study. IM2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<idno>IM2.MDM-11</idno>
		<imprint>
			<date type="published" when="2003-11">November 2003</date>
		</imprint>
	</monogr>
	<note type="report_type">MDM Internal Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graphical representation of meetings on mobile devices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobileHCI 2008 (10th International Conference on Human-Computer Interaction with Mobile Devices and Services)</title>
		<meeting>MobileHCI 2008 (10th International Conference on Human-Computer Interaction with Mobile Devices and Services)<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="503" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A split-attention effect in multimedia learning: evidence for dual processing systems in working memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hand and Mind: What Gestures Reveal About Thought</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Univ. of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From vocal to multimodal dialogue management</title>
		<author>
			<persName><forename type="first">M</forename><surname>Melichar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cenek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Multimodal Interfaces (ICMI 2006)</title>
		<meeting>the Eighth International Conference on Multimodal Interfaces (ICMI 2006)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04">November 2-4. 2006</date>
			<biblScope unit="page" from="59" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Participating in Explanatory Dialogues: Interpreting and Responding to Questions in Context</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reducing cognitive load by mixing auditory and visual presentation modes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="334" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Intelligent multimedia interface technology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent User Interfaces</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Tyler</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="11" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Design space for multimodal systems: concurrent processing and data fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nigay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Coutaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the INTERACT 1993 and CHI 1993 Conference on Human Factors in Computing Systems</title>
		<meeting>the INTERACT 1993 and CHI 1993 Conference on Human Factors in Computing Systems<address><addrLine>Amsterdam, The Netherlands; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">April 24 -29. 1993</date>
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Design of Everyday Things</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Basic Book</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mutual Beliefs of Multiple Conversants: A computational model of collaboration in Air Trafic Control</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Novick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 1993</title>
		<meeting>AAAI 1993</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Advances in Robust Multimodal Interface Design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal interactive maps: Designing for human performance</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="93" to="129" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<editor>Jacko, J., Sears, A.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="286" to="304" />
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
	<note>Multimodal interfaces</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ten myths of multimodal interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human-centered design meets cognitive load theory: designing interfaces that help people think</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual ACM international Conference on Multimedia</title>
		<meeting>the 14th Annual ACM international Conference on Multimedia<address><addrLine>Santa Barbara, CA, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">October 23-27. 2006</date>
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Designing the user interface for multimodal speech and gesture applications: State-of-the-art systems and research directions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vergo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Suhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Computer Interaction in the New Millennium</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Carroll</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Press</publisher>
			<date type="published" when="2000">2000. 2001</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="421" to="456" />
		</imprint>
	</monogr>
	<note>Reprinted</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Toward a theory of organized multimodal integration patterns during human-computer interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coulston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tomko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lunsford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI 2003</title>
		<meeting>ICMI 2003<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting the dependencies in information fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Anastasio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="407" to="412" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Automatic Lipreading to Enhance Speech Recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">TQB: Accessing Multimedia Data Using a Transcriptbased Query and Browsing Interface</title>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu-Belis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgescul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2006 (5th International Conference on Language Resources and Evaluation)</title>
		<meeting>LREC 2006 (5th International Conference on Language Resources and Evaluation)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1560" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Buisine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Collings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kraal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mctear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Stanney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guidelines for multimodal user interface design</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="57" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">FaericWorld: Browsing Multimedia Events Through Static Documents And Links</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERACT 2007</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Baranauskas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Palanque</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Abascal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">D J</forename><surname>Barbosa</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4663</biblScope>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The OpenInterface framework: a tool for multimodal interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nigay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><forename type="middle">L</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct Proceedings of CHI 2008</title>
		<meeting><address><addrLine>Florence, Italy; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008-10">April 5-10. 2008</date>
			<biblScope unit="page" from="3501" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Toward multimodal human-computer interface</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special issue on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The Markup Way to Multimodal Toolkits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chatty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>W3C Multimodal Interaction Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
		<ptr target="http://www.sspnet.eu" />
	</analytic>
	<monogr>
		<title level="j">SSPNet: Social Signal Processing Network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A transformational approach for multimodal web user interfaces based on UsiXML</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Limbourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderdonckt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Montero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI 2005</title>
		<meeting>ICMI 2005<address><addrLine>Torento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">October 04-06. 2005</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cognitive Load as a Factor in the Structuring of Technical Material</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="176" to="192" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">When two sensory modes are better than one</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tindall-Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The Information State Approach to Dialogue Management</title>
		<author>
			<persName><forename type="first">D</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and New Directions in Discourse and Dialogue</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C J</forename><surname>Van Kuppevelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="325" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Perceptual user interfaces (Introduction)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="70" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Building an application framework for speech and pen input integration in multimodal learning interfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics Speech and Signal Processing<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3545" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multiple resources and performance prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wickens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomic Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="177" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Compatibility and resource competition between modalities of input, central processing, and output</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wickens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sandry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vidulich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="248" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">From members to teams to committee -a robust approach to gestural and multimodal recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="972" to="982" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multimodal integration -A statistical view</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="341" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Manual and gaze input cascaded (MAGIC) pointing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ihde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems (CHI 1999)</title>
		<meeting>the Conference on Human Factors in Computing Systems (CHI 1999)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
