<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiased Contrastive Learning of Unsupervised Sentence Representations</title>
				<funder ref="#_kud2EMF">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_9MqWTyB">
					<orgName type="full">Outstanding Innovative Talents Cultivation Funded Programs 2021 and Public Computing Cloud, Renmin University of China</orgName>
				</funder>
				<funder ref="#_phVyjUM">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing Academy of Artificial Intelligence</orgName>
					<orgName type="abbreviated">BAAI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-02">2 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country>of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
							<email>zhangbeichen724@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country>of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country>of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country>of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Debiased Contrastive Learning of Unsupervised Sentence Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-02">2 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.00656v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt inbatch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (e.g., false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space. To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives. In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space. Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: https: //github.com/RUCAIBox/DCLR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a fundamental task in the natural language processing (NLP) field, unsupervised sentence representation learning <ref type="bibr" target="#b25">(Kiros et al., 2015;</ref><ref type="bibr" target="#b19">Hill et al., 2016)</ref> aims to derive high-quality sentence representations that can benefit various downstream tasks, especially for low-resourced domains or computationally expensive tasks, e.g., zero-shot text semantic matching <ref type="bibr" target="#b36">(Qiao et al., 2016)</ref>, large-scale semantic similarity comparison <ref type="bibr" target="#b0">(Agirre et al., 2015)</ref>, and document retrieval (Le and <ref type="bibr" target="#b27">Mikolov, 2014)</ref>.</p><p>Recently, pre-trained language models (PLMs) <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> have become a widely-used se- ? ? Corresponding author 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9  <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>. Almost half of the negatives have high similarities with the input. mantic representation approach, achieving remarkable performance on various NLP tasks. However, several studies have found that the native sentence representations derived by PLMs are not uniformly distributed with respect to directions, but instead occupy a narrow cone in the vector space <ref type="bibr" target="#b14">(Ethayarajh, 2019)</ref>, which largely limits their expressiveness. To address this issue, contrastive learning <ref type="bibr" target="#b48">(Chen et al., 2020)</ref> has been adopted to refine PLM-derived sentence representations. It pulls semantically-close neighbors together to improve the alignment, while pushes apart non-neighbors for the uniformity of the whole representation space. In the learning process, both positive and negative examples are involved in contrast with the original sentence. For positive examples, previous works apply data augmentation strategies <ref type="bibr" target="#b46">(Yan et al., 2021)</ref> on the original sentence to generate highly similar variations. While, negative examples are commonly sampled from the batch or training data (e.g., in-batch negatives <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>) at random, due to the lack of ground-truth annotations for negatives.</p><p>Although such a negative sampling way is simple and convenient, it may cause sampling bias and affect the sentence representation learning. First, the sampled negatives are likely to be false negatives that are indeed semantically close to the original sentence. As shown in Figure <ref type="figure">1</ref>, given an input sentence, about half of in-batch negatives have a cosine similarity above 0.7 with the original sentence based on the SimCSE model <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>. It is likely to hurt the semantics of the sentence representations by simply pushing apart these sampled negatives. Second, due to the anisotropy problem <ref type="bibr" target="#b14">(Ethayarajh, 2019)</ref>, the representations of sampled negatives are from the narrow representation cone spanned by PLMs, which cannot fully reflect the overall semantics of the representation space. Hence, it is sub-optimal to only rely on these representations for learning the uniformity objective of sentence representations.</p><p>To address the above issues, we aim to develop a better contrastive learning approach with debiased negative sampling strategies.The core idea is to improve the random negative sampling strategy for alleviating the sampling bias problem. First, in our framework, we design an instance weighting method to punish the sampled false negatives during training. We incorporate a complementary model to evaluate the similarity between each negative and the original sentence, then assign lower weights for negatives with higher similarity scores. In this way, we can detect semanticallyclose false negatives and further reduce their influence. Second, we randomly initialize new negatives based on random Gaussian noises to simulate sampling within the whole semantic space, and devise a gradient-based algorithm to optimize the noise-based negatives towards the most nonuniform points. By learning to contrast with the nonuniform noise-based negatives, we can extend the occupied space of sentence representations and improve the uniformity of the representation space.</p><p>To this end, we propose DCLR, a general framework towards Debiased Contrastive Learning of unsupervised sentence Representations. In our approach, we first initialize the noise-based negatives from a Gaussian distribution, and leverage a gradient-based algorithm to update the new negatives by considering the uniformity of the representation space. Then, we adopt the complementary model to produce the weights for these noise-based negatives and randomly sampled negatives, where the false negatives will be punished. Finally, we augment the positive examples via dropout <ref type="bibr" target="#b16">(Gao et al., 2021)</ref> and combine them with the above weighted negatives for contrastive learning. We demonstrate that our DCLR outperforms a number of competitive baselines on seven semantic textual similarity (STS) tasks using BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b29">(Liu et al., 2019)</ref>.</p><p>Our contributions are summarized as follows:</p><p>(1) To our knowledge, our approach is the first attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.</p><p>(2) We propose DCLR, a debiased contrastive learning framework that incorporates an instance weighting method to punish false negatives and generates noise-based negatives to guarantee the uniformity of the representation space.</p><p>(3) Experimental results on seven semantic textual similarity tasks show the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the related work from the following three aspects.</p><p>Sentence Representation Learning. Learning universal sentence representations <ref type="bibr" target="#b25">(Kiros et al., 2015;</ref><ref type="bibr" target="#b19">Hill et al., 2016)</ref> is the key to the success of various downstream tasks. Previous works can be roughly categorized into supervised <ref type="bibr" target="#b12">(Conneau et al., 2017;</ref><ref type="bibr" target="#b8">Cer et al., 2018)</ref> and unsupervised approaches <ref type="bibr" target="#b19">(Hill et al., 2016;</ref><ref type="bibr" target="#b28">Li et al., 2020)</ref>. Supervised approaches rely on annotated datasets (e.g., <ref type="bibr">NLI (Bowman et al., 2015;</ref><ref type="bibr" target="#b42">Williams et al., 2018)</ref>) to train the sentence encoder <ref type="bibr" target="#b8">(Cer et al., 2018;</ref><ref type="bibr" target="#b38">Reimers and Gurevych, 2019)</ref>. Unsupervised approaches consider deriving sentence representations without labeled datasets, e.g., pooling word2vec embeddings <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref>. Recently, to leverage the strong potential of PLMs <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>, several works propose to alleviate the anisotropy problem <ref type="bibr" target="#b14">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b28">Li et al., 2020)</ref> of PLMs via special strategies, e.g., flow-based approach <ref type="bibr" target="#b28">(Li et al., 2020)</ref> and whitening method <ref type="bibr" target="#b21">(Huang et al., 2021)</ref>. Besides, contrastive learning <ref type="bibr" target="#b45">(Wu et al., 2020;</ref><ref type="bibr" target="#b16">Gao et al., 2021)</ref> has been used to refine the representations of PLMs.</p><p>Contrastive Learning. Contrastive learning has been originated in the computer vision <ref type="bibr" target="#b17">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b18">He et al., 2020)</ref> and information retrieval <ref type="bibr" target="#b6">(Bian et al., 2021;</ref><ref type="bibr" target="#b47">Zhou et al., 2022)</ref> field with significant performance improvement. Usually, it relies on data augmentation strategies such as random cropping and image rotation <ref type="bibr" target="#b48">(Chen et al., 2020;</ref><ref type="bibr" target="#b46">Yan et al., 2021)</ref> to produce a set of se-mantically related positive examples for learning, and randomly samples negatives from the batch or whole dataset. For sentence representation learning, contrastive learning can achieve a better balance between alignment and uniformity in semantic representation space. Several works further adopt back translation <ref type="bibr" target="#b15">(Fang and Xie, 2020)</ref>, token shuffling <ref type="bibr" target="#b46">(Yan et al., 2021)</ref> and dropout <ref type="bibr" target="#b16">(Gao et al., 2021)</ref> to augment positive examples for sentence representation learning. However, the quality of the randomly sampled negatives is seldom studied.</p><p>Virtual Adversarial Training. Virtual adversarial training (VAT) <ref type="bibr" target="#b34">(Miyato et al., 2019;</ref><ref type="bibr" target="#b26">Kurakin et al., 2017)</ref> perturbs a given input with learnable noises to maximize the divergence of the model's prediction with the original label, then utilizes the perturbed examples to improve the generalization <ref type="bibr" target="#b33">(Miyato et al., 2017;</ref><ref type="bibr" target="#b30">Madry et al., 2018)</ref>. A class of VAT methods can be formulated into solving a min-max problem, which can be achieved by multiple projected gradient ascent steps <ref type="bibr" target="#b37">(Qin et al., 2019)</ref>. In the NLP field, several studies incorporate adversarial perturbations in the embedding layer, and show its effectiveness on text classification <ref type="bibr" target="#b33">(Miyato et al., 2017)</ref>, machine translation <ref type="bibr" target="#b41">(Sun et al., 2020)</ref>, and natural language understanding <ref type="bibr" target="#b22">(Jiang et al., 2020)</ref> tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>This work aims to make use of unlabeled corpus for learning effective sentence representations that can be directly utilized for downstream tasks, e.g., semantic textual similarity task <ref type="bibr" target="#b0">(Agirre et al., 2015)</ref>. Given a set of input sentences X = {x 1 , x 2 , . . . , x n }, our goal is to learn a representation h i ? R d for each sentence x i in an unsupervised manner. For simplicity, we denote this process with a parameterized function</p><formula xml:id="formula_0">h i = f (x i ).</formula><p>In this work, we mainly focus on using BERTbased PLMs <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr" target="#b29">Liu et al., 2019)</ref> to generate sentence representations. Following existing works <ref type="bibr" target="#b28">(Li et al., 2020;</ref><ref type="bibr" target="#b46">Yan et al., 2021)</ref>, we fine-tune PLMs on the unlabeled corpus via our proposed unsupervised learning approach. After that, for each sentence x i , we encode it by the fine-tuned PLMs and take the representation of the [CLS] token from the last layer as its sentence representation h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>Our proposed framework DCLR focuses on reducing the influence of sampling bias in the contrastive learning of sentence representations. In this framework, we devise a noise-based negatives generation strategy to reduce the bias caused by the anisotropy PLM-derived representations, and an instance weighting method to reduce the bias caused by false negatives. Concretely, we initialize the noise-based negatives based on a Gaussian distribution and iteratively update these negatives towards non-uniformity maximization. Then, we utilize a complementary model to produce weights for all negatives (i.e., randomly sampled and the noisebased ones). Finally, we combine the weighted negatives and augmented positive examples for contrastive learning. The overview of our DCLR is presented in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generating Noise-based Negatives</head><p>We aim to generate new negatives beyond the sentence representation space of PLMs during the training process, to alleviate the sampling bias derived from the anisotropy problem of PLMs (Ethayarajh, 2019). For each input sentence x i , we first initialize k noise vectors from a Gaussian distribution as the negative representations<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_1">{ ?1 , ?2 , ? ? ? , ?k } ? N (0, ? 2 ), (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where ? is the standard variance. Since these vectors are randomly initialized from such a Gaussian distribution, they are uniformly distributed within the whole semantic space. By learning to contrast with these new negatives, it is beneficial for the uniformity of sentence representations.</p><p>To further improve the quality of the new negatives, we consider iteratively updating the negatives to capture the non-uniformity points within the whole semantic space. Inspired by VAT <ref type="bibr" target="#b33">(Miyato et al., 2017;</ref><ref type="bibr" target="#b48">Zhu et al., 2020)</ref>, we design a non-uniformity loss maximization objective to produce gradients for improving these negatives. The non-uniformity loss is denoted as the contrastive loss between the noise-based negatives { ?j } and the positive representations of the original sentence We show the case that a false negative is punished by assigning the weight 0.</p><p>(h i , h + i ) as:</p><formula xml:id="formula_3">LU (hi, h + i , { ?}) = -log e sim(h i ,h + i )/?u ?j ?{ ?j } e sim(h i , ?i )/?u ,<label>(2)</label></formula><p>where ? u is a temperature hyper-parameter and sim(h i , h + i ) is the cosine similarity</p><formula xml:id="formula_4">h i h + i ||h i ||?||h + i || .</formula><p>Based on it, for each negative ?j ? { ?}, we optimize it by t steps gradient ascent as</p><formula xml:id="formula_5">?j = ?j + ?g( ?j )/||g( ?j )|| 2 ,<label>(3)</label></formula><formula xml:id="formula_6">g( ?j ) = ?j L U (h i , h + i , { ?}),<label>(4)</label></formula><p>where ? is the learning rate, || ? || 2 is the L 2 -norm. g( ?j ) denotes the gradient of ?j by maximizing the non-uniformity loss between the positive representations and the noise-based negatives. In this way, the noise-based negatives will be optimized towards the non-uniform points of the sentence representation space. By learning to contrast with these negatives, the uniformity of the representation space can be further improved, which is essential for effective sentence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Contrastive Learning with Instance Weighting</head><p>In addition to the above noise-based negatives, we also follow existing works <ref type="bibr" target="#b46">(Yan et al., 2021;</ref><ref type="bibr" target="#b16">Gao et al., 2021)</ref> that adopt other in-batch representations as negatives { h-}. However, as discussed before, the sampled negatives may contain examples that have similar semantics with the positive example (i.e., false negatives).</p><p>To alleviate this problem, we propose an instance weighting method to punish the false negatives. Since we cannot obtain the true labels or semantic similarities, we utilize a complementary model to produce the weights for each negative. In this paper, we adopt the state-of-the-art SimCSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref> as the complementary model.<ref type="foot" target="#foot_2">2</ref> Given a negative representation h -from { h-} or { ?} and the representation of the original sentence h i , we utilize the complementary model to produce the weight as</p><formula xml:id="formula_7">? h -= 0, sim C (h i , h -) ? ? 1, sim C (h i , h -) &lt; ? (5)</formula><p>where ? is a hyper-parameter of the instance weighting threshold, and sim C (h i , h -) is the cosine similarity score evaluated by the complementary model. In this way, the negative that has a higher semantic similarity with the representation of the original sentence will be regarded as a false negative and will be punished by assigning the weight 0. Based on the weights, we optimize the sentence representations with a debiased crossentropy contrastive learning loss function as</p><formula xml:id="formula_8">L = -log e sim(h i ,h + i )/? h -?{ ?}?{ h-} ? h -? e sim(h i ,h -)/? ,<label>(6)</label></formula><p>where ? is a temperature hyper-parameter. In our framework, we follow SimCSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref> that utilizes dropout to augment positive examples h + i . Actually, we can utilize various positive augmentation strategies, and will investigate it in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overview and Discussion</head><p>In this part, we present the overview and discussion of our DCLR approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Overview of DCLR</head><p>Our framework DCLR contains three major steps. In the first step, we generate noise-based negatives to extend in-batch negatives. Concretely, we first initialize a set of new negatives via random Gaussian noises using Eq. 1. Then, we incorporate a gradient-based algorithm to adjust the noise-based negatives by maximizing the non-uniform objective using Eq. 3. After several iterations, we can obtain the noise-based negatives that correspond to the nonuniform points within the whole semantic space, and we mix up them with in-batch negatives to compose the negative set. In the second step, we adopt a complementary model (i.e., SimCSE) to compute the semantic similarity between the original sentence and each example from the negative set, and produce the weights using Eq. 5. Finally, we augment the positive examples via dropout and utilize the negatives with corresponding weights for contrastive learning using Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Discussion</head><p>As mentioned above, our approach aims to reduce the influence of the sampling bias about the negatives, and is agnostic to various positive data augmentation methods (e.g., token cutoff and dropout). Compared with traditional contrastive learning methods <ref type="bibr" target="#b46">(Yan et al., 2021;</ref><ref type="bibr" target="#b16">Gao et al., 2021)</ref>, our proposed DCLR expands the negative set by introducing noise-based negatives { ?}, and adds a weight term ? h -to punish false negatives. Since the noise-based negatives are initialized from a Gaussian distribution and do not correspond to real sentences, they are highly confident negatives to broaden the representation space. By learning to contrast with them, the learning of the contrastive objective will not be limited by the anisotropy representations derived from PLMs. As a result, the sentence representations can span a broader semantic space, and the uniformity of the representation semantic space can be improved.</p><p>Besides, our instance weighting method also alleviates the false negative problem caused by the randomly sampling strategy. With the help of a complementary model, the false negatives with similar semantics as the original sentence will be detected and punished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment -Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Following previous works <ref type="bibr" target="#b23">(Kim et al., 2021;</ref><ref type="bibr" target="#b16">Gao et al., 2021)</ref>, we conduct experiments on seven standard STS tasks. For all these tasks, we use the SentEval toolkit <ref type="bibr" target="#b11">(Conneau and Kiela, 2018)</ref> for evaluation.</p><p>Semantic Textual Similarity Task. We evaluate our approach on 7 STS tasks: STS 2012-2016 <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016))</ref>, STS Benchmark <ref type="bibr" target="#b9">(Cer et al., 2017)</ref> and SICK-Relatedness <ref type="bibr" target="#b31">(Marelli et al., 2014)</ref>. These datasets contain pairs of two sentences, whose similarity scores are labeled from 0 to 5. The relevance between gold annotations and the scores predicted by sentence representations is measured by the Spearman correlation. Following the suggestions from previous works <ref type="bibr" target="#b16">(Gao et al., 2021;</ref><ref type="bibr" target="#b38">Reimers and Gurevych, 2019)</ref>, we directly compute the cosine similarity between sentence embeddings for all STS tasks. Baseline Methods. We compare DCLR with competitive unsupervised sentence representation learning methods, consisting of non-BERT and BERTbased methods:</p><p>(1) GloVe <ref type="bibr" target="#b35">(Pennington et al., 2014)</ref> averages GloVe embeddings of words as the sentence representation.</p><p>(2) USE <ref type="bibr" target="#b8">(Cer et al., 2018</ref>) utilizes a Transformer model that learns the objective of reconstructing the surrounding sentences within a passage.</p><p>(3) CLS, Mean and First-Last AVG <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> adopt the [CLS] embedding, mean pooling of token representations, average representations of the first and last layers as sentence representations, respectively.</p><p>(4) Flow <ref type="bibr" target="#b28">(Li et al., 2020)</ref> applies mean pooling on the layer representations and maps the outputs to the Gaussian space as sentence representations.</p><p>(5) Whitening <ref type="bibr" target="#b40">(Su et al., 2021)</ref> uses the whitening operation to refine representations and reduce dimensionality.</p><p>(6) Contrastive (BT) (Fang and Xie, 2020) uses contrastive learning with back-translation for data augmentation to enhance sentence representations.</p><p>(7) ConSERT <ref type="bibr" target="#b46">(Yan et al., 2021)</ref> explores various text augmentation strategies for contrastive learning of sentence representations.</p><p>(8) SG-OPT <ref type="bibr" target="#b23">(Kim et al., 2021)</ref> proposes a contrastive learning method with a self-guidance mech- anism for improving the sentence embeddings of PLMs. ( <ref type="formula">9</ref>) SimCSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref> proposes a simple contrastive learning framework that utilizes dropout for data augmentation.</p><p>Implementation Details. We implement our model based on Huggingface's transformers <ref type="bibr" target="#b43">(Wolf et al., 2020)</ref>. For BERT-base and RoBERTa-base, we start from the pre-trained checkpoints of their original papers. For BERT-large and RoBERTalarge, we utilize the checkpoints of SimCSE for stabilizing the convergence process. Following Sim-CSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>, we use 1,000,000 sentences randomly sampled from Wikipedia as the training corpus. During training, we train our models for 3 epoch with temperature ? = 0.05 using an Adam optimizer <ref type="bibr" target="#b24">(Kingma and Ba, 2015)</ref>. For BERT-base and RoBERTa-base, the batch size is 128, the learning rate is 3e-5. For BERT-large and RoBERTalarge, the batch size is 256, the learning rate is 3e-5 and 1e-5, respectively. For the four backbone models, we set the instance weighting threshold ? as 0.9, 0.85, 0.9 and 0.85, respectively. For each batch, we generate k ? batch_size noise-based negatives as the shared negatives of all instance within it, and k is 1, 2.5, 4 and 5 for BERT-base, RoBERTa-base, BERT-large and RoBERTa-large, respectively. The standard variance of the noise-based negatives is 1, and we update the noise-based negatives four times with the learning rate of 1e-3. We evaluate the model every 150 steps on the development set of STS-B and SICK-R and keep the best checkpoint for evaluation on test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>To verify the effectiveness of our framework on PLMs, we select BERT-base and RoBERTa-base as the base model. Table <ref type="table" target="#tab_0">1</ref> shows the results of different methods on seven STS tasks.</p><p>Based on the results, we can find that the non-BERT methods (i.e., GloVe and USE) mostly outperform native PLM representation based baselines (i.e., CLS, Mean and First-Last AVG). The reason is that directly utilizing the PLM native representations is prone to be influenced by the anisotropy issue. Among non-BERT methods, USE outperforms Glove. A potential reason is that USE encodes the sentence using the Transformer model, which is more effective than simply averaging GloVe embeddings.</p><p>For other PLM-based approaches, first, we can see that flow and whitening achieve similar results and outperform the native representations based methods by a margin. These two methods adopt specific improvement strategies to refine the representations of PLMs. Second, approaches based on contrastive learning outperform the other baselines in most cases. Contrastive learning can enhance both the alignment between semantically related positive pairs and the uniformity of the representation space using negative samples, resulting in better sentence representations. Furthermore, Sim-CSE performs the best among all the baselines. It indicates that dropout is a more effective positive augmentation method than others since it rarely hurts the semantics of the sentence.</p><p>Finally, DCLR performs better than all the baselines in most settings, including the approaches based on contrastive learning. Since these methods mostly utilize randomly sampled negatives (e.g., in-batch negatives) to learn the uniformity of all sentence representations, it may lead to sampling bias, such as false negatives and anisotropy representations. Different from these methods, our framework adopts an instance weighting method to punish false negatives and a gradient-based algorithm to generate noise-based negatives towards the nonuniform points. In this way, the sampling bias problem can be alleviated, and our model can better learn the uniformity to improve the quality of the sentence representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment -Analysis and Extension</head><p>In this section, we continue to study the effectiveness of our proposed DCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Debiased Contrastive Learning on Other Methods</head><p>Since our proposed DCLR is a general framework that mainly focuses on negative sampling for contrastive learning of unsupervised sentence representations, it can be applied to other methods that rely on different positive data augmentation strategies. Thus, in this part, we conduct experiments to examine whether our framework can bring improvements with the following positive data augmentation strategies: (1) Token Shuffling that randomly shuffles the order of the tokens in the input sequences;</p><p>(2) Feature/Token/Span Cutoff <ref type="bibr" target="#b46">(Yan et al., 2021)</ref> that randomly erases features/tokens/token spans in the input; (3) Dropout that is similar to SimCSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>. Note that we only revise the negative sampling strategies to implement these variants of our DCLR. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, our DCLR can boost the performance of all these augmentation strategies, it demonstrates the effectiveness of our framework with various augmentation strategies. Furthermore, the Dropout strategy leads to the best performance among all the variants. It indicates that dropout is a more effective approach to augment high-quality positives, and is also more appropriate for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>Our proposed DCLR incorporates an instance weighting method to punish false negatives and also utilizes noise-based negatives to improve the uniformity of the whole sentence representation space. To verify their effectiveness, we conduct an ablation study for each of the two components on seven STS tasks and report the average value of the Spearman's correlation metric. As shown in Table <ref type="table" target="#tab_1">2</ref>, removing each component would lead to the performance degradation. It indicates that the instance weighting method and the noise-based negatives are both important in our framework. Besides, removing the instance weighting method results in a larger performance drop. The reason may be that the false negatives have a larger effect on sentence representation learning. Besides, we prepare three variants for further comparison: (1) Random Noise directly generates noise-based negatives without the gradient-based optimization; (2) Knowledge Distillation <ref type="bibr" target="#b20">(Hinton et al., 2015)</ref> utilizes SimCSE as the teacher model to distill knowledge into the student model during training; (3) Self Instance Weighting adopts the model itself as the complementary model to generate the weights. From Table <ref type="table" target="#tab_1">2</ref>, we can see that these variations don't perform as well as the original DCLR. These results indicate the proposed designs in Section 4 are more suitable for our DCLR framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Uniformity Analysis</head><p>Uniformity is a desirable characteristic for sentence representations, describing how well the represen- tations are uniformly distributed. To validate the improvement of the uniformity of our framework, we compare the uniformity loss curves of DCLR and SimCSE using BERT-base during training.</p><p>Following SimCSE <ref type="bibr" target="#b16">(Gao et al., 2021)</ref>, we utilize the following function to evaluate the uniformity:</p><formula xml:id="formula_9">unif orm log E x i ,x j i.i.d. ? p data e -2 f (x i )-f (x j ) 2 ,</formula><p>where p data is the distribution of all sentence representations, and a smaller value of this loss indicates a better uniformity. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, the uniformity loss of DCLR is much lower than that of SimCSE in almost the whole training process. Furthermore, we can see that the uniformity loss of DCLR decreases faster as training goes, while the one of SimCSE shows no significant decreasing trend. It might be because our DCLR samples noise-based negatives beyond the representation space, which can better improve the uniformity of sentence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Performance under Few-shot Settings</head><p>To validate the reliability and the robustness of DCLR under the data scarcity scenarios, we conduct few-shot experiments using BERT-base as the backbone model. We train our model via different amounts of available training data from 100% to the extremely small size (i.e., 0.3%). We report the results evaluated on STS-B and SICK-R tasks. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, our approach achieves stable results under different proportions of the training data. Under the most extreme setting with 0.3% data proportion, the performance of our model drops by only 9 and 4 percent on STS-B and SICK-R, respectively. The results reveal the robustness and effectiveness of our approach under the data scarcity scenarios. Such characteristics are important in real-world application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Hyper-parameters Analysis</head><p>For hyper-parameters analysis, we study the impact of instance weighting threshold ? and the proportion of noise-based negatives k. The ? is the threshold to punish false negatives, and k is the ratio of the noise-based negatives to the batch size. Both hyper-parameters are important in our framework.</p><p>Concretely, we evaluate our model with varying values of ? and k on the STS-B and SICK-R tasks using the BERT-base model.</p><p>Weighting threshold. Figure <ref type="figure" target="#fig_5">6</ref>(a) shows the influence of the instance weighting threshold ?. For the STS-B tasks, ? has a significant effect on the model performance. Too large or too small ? may lead to a performance drop. The reason is that a larger threshold cannot achieve effective punishment and a smaller one may cause misjudgment of true negatives. In contrast, the SICK-R is insensitive to the changes of ?. The reason may be that the problem of false negatives is not serious in this task.</p><p>Negative proportion. As shown in Figure <ref type="figure" target="#fig_5">6</ref>(b), our DCLR performs better when the number of noise-based negatives is close to the batch size. Under these circumstances, the noise-based negatives are more capable to enhance the uniformity of the whole semantic space without hurting the alignment, which is key why DCLR works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed DCLR, a debiased contrastive learning framework for unsupervised sentence representation learning. Our core idea is to alleviate the sampling bias caused by the random negative sampling strategy. To achieve it, in our framework, we incorporated an instance weighting method to punish false negatives during training and generated noise-based negatives to alleviate the influence of anisotropy PLM-derived representation. Experimental results on seven STS tasks have shown that our approach outperforms several competitive baselines.</p><p>In the future, we will explore other approaches to reducing the bias in contrastive learning of sentence representations (e.g., debiased pre-training). Besides, we will also consider to apply our method for multilingual or multimodal representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Consideration</head><p>In this section, we discuss the ethical considerations of this work from the following two aspects. First, for intellectual property protection, the code, data and pre-trained models adopted from previous works are granted for research-purpose usage. Second, since PLMs have been shown to capture certain biases from the data they have been pretrained on <ref type="bibr" target="#b5">(Bender et al., 2021)</ref>, there is a potential problem about biases that are from the use of PLMs in our approach. There are increasing efforts to address this problem in the community <ref type="bibr" target="#b39">(Ross et al., 2020)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The distribution of cosine similarity between an input sentence and 255 in-batch negatives from the commonly-used Wikipedia Corpus. It is evaluated by the SimCSE model<ref type="bibr" target="#b16">(Gao et al., 2021)</ref>. Almost half of the negatives have high similarities with the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"Figure 2 :</head><label>2</label><figDesc>Figure 2: The overview of our DCLR framework with noise-based negatives and the instance weighting strategy.We show the case that a false negative is punished by assigning the weight 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFigure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison using different positive augmentation strategies on the test set of seven STS tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The uniformity loss of DCLR and SimCSE using BERT-base on the validation set of STS-B during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance tuning of our DCLR w.r.t. different amounts of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance tuning w.r.t. ? and k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Models</cell><cell cols="7">STS12 STS13 STS14 STS15 STS16 STS-B SICK-R</cell><cell>Avg.</cell></row><row><cell>Non-BERT</cell><cell>GloVe (avg.)  ? USE  ?</cell><cell>55.14 64.49</cell><cell>70.66 67.80</cell><cell>59.73 64.61</cell><cell>68.25 76.83</cell><cell>63.66 73.18</cell><cell>58.02 74.92</cell><cell>53.76 76.69</cell><cell>61.32 71.22</cell></row><row><cell></cell><cell>CLS  ?</cell><cell>21.54</cell><cell>32.11</cell><cell>21.28</cell><cell>37.89</cell><cell>44.24</cell><cell>20.30</cell><cell>42.42</cell><cell>31.40</cell></row><row><cell></cell><cell>Mean  ?</cell><cell>30.87</cell><cell>59.89</cell><cell>47.73</cell><cell>60.29</cell><cell>63.73</cell><cell>47.29</cell><cell>58.22</cell><cell>52.57</cell></row><row><cell>BERT-base</cell><cell>First-Last AVG  ? . +flow  ?</cell><cell>39.70 58.40</cell><cell>59.38 67.10</cell><cell>49.67 60.85</cell><cell>66.03 75.16</cell><cell>66.19 71.22</cell><cell>53.87 68.66</cell><cell>62.06 64.47</cell><cell>56.70 66.55</cell></row><row><cell></cell><cell>+whitening  ?</cell><cell>57.83</cell><cell>66.90</cell><cell>60.90</cell><cell>75.08</cell><cell>71.31</cell><cell>68.24</cell><cell>63.73</cell><cell>66.28</cell></row><row><cell></cell><cell>+Contrastive (BT)  ?</cell><cell>54.26</cell><cell>64.03</cell><cell>54.28</cell><cell>68.19</cell><cell>67.50</cell><cell>63.27</cell><cell>66.91</cell><cell>62.63</cell></row><row><cell></cell><cell>+ConSERT</cell><cell>64.64</cell><cell>78.49</cell><cell>69.07</cell><cell>79.72</cell><cell>75.95</cell><cell>73.97</cell><cell>67.31</cell><cell>72.74</cell></row><row><cell></cell><cell>+SG-OPT  ?</cell><cell>66.84</cell><cell>80.13</cell><cell>71.23</cell><cell>81.56</cell><cell>77.17</cell><cell>77.23</cell><cell>68.16</cell><cell>74.62</cell></row><row><cell></cell><cell>+SimCSE</cell><cell>68.40</cell><cell>82.41</cell><cell>74.38</cell><cell>80.91</cell><cell>78.56</cell><cell>76.85</cell><cell>72.23</cell><cell>76.25</cell></row><row><cell></cell><cell>+DCLR (Ours)</cell><cell>70.81</cell><cell>83.73</cell><cell>75.11</cell><cell>82.56</cell><cell>78.44</cell><cell>78.31</cell><cell>71.59</cell><cell>77.22</cell></row><row><cell></cell><cell>CLS  ?</cell><cell>27.44</cell><cell>30.76</cell><cell>22.59</cell><cell>29.98</cell><cell>42.74</cell><cell>26.75</cell><cell>43.44</cell><cell>31.96</cell></row><row><cell></cell><cell>Mean  ?</cell><cell>27.67</cell><cell>55.79</cell><cell>44.49</cell><cell>51.67</cell><cell>61.88</cell><cell>47.00</cell><cell>53.85</cell><cell>48.91</cell></row><row><cell></cell><cell>First-Last AVG</cell><cell>57.73</cell><cell>61.17</cell><cell>61.18</cell><cell>68.07</cell><cell>70.25</cell><cell>59.59</cell><cell>60.34</cell><cell>62.62</cell></row><row><cell>BERT-large</cell><cell>+flow  ?</cell><cell>62.82</cell><cell>71.24</cell><cell>65.39</cell><cell>78.98</cell><cell>73.23</cell><cell>72.72</cell><cell>63.77</cell><cell>70.07</cell></row><row><cell></cell><cell>+whitening</cell><cell>64.34</cell><cell>74.60</cell><cell>69.64</cell><cell>74.68</cell><cell>75.90</cell><cell>72.48</cell><cell>60.80</cell><cell>70.35</cell></row><row><cell></cell><cell>+Contrastive (BT)  ?</cell><cell>52.04</cell><cell>62.59</cell><cell>54.25</cell><cell>71.07</cell><cell>66.71</cell><cell>63.84</cell><cell>66.53</cell><cell>62.43</cell></row><row><cell></cell><cell>+ConSERT</cell><cell>70.69</cell><cell>82.96</cell><cell>74.13</cell><cell>82.78</cell><cell>76.66</cell><cell>77.53</cell><cell>70.37</cell><cell>76.45</cell></row><row><cell></cell><cell>+SG-OPT  ?</cell><cell>67.02</cell><cell>79.42</cell><cell>70.38</cell><cell>81.72</cell><cell>76.35</cell><cell>76.16</cell><cell>70.20</cell><cell>74.46</cell></row><row><cell></cell><cell>+SimCSE</cell><cell>70.88</cell><cell>84.16</cell><cell>76.43</cell><cell>84.50</cell><cell>79.76</cell><cell>79.26</cell><cell>73.88</cell><cell>78.41</cell></row><row><cell></cell><cell>+DCLR (Ours)</cell><cell>71.87</cell><cell>84.83</cell><cell>77.37</cell><cell>84.70</cell><cell>79.81</cell><cell>79.55</cell><cell>74.19</cell><cell>78.90</cell></row><row><cell></cell><cell>CLS  ?</cell><cell>16.67</cell><cell>45.57</cell><cell>30.36</cell><cell>55.08</cell><cell>56.98</cell><cell>45.41</cell><cell>61.89</cell><cell>44.57</cell></row><row><cell></cell><cell>Mean  ?</cell><cell>32.11</cell><cell>56.33</cell><cell>45.22</cell><cell>61.34</cell><cell>61.98</cell><cell>54.53</cell><cell>62.03</cell><cell>53.36</cell></row><row><cell>RoBERTa-base</cell><cell>First-Last AVG  ? +whitening  ?</cell><cell>40.88 46.99</cell><cell>58.74 63.24</cell><cell>49.07 57.23</cell><cell>65.63 71.36</cell><cell>61.48 68.99</cell><cell>58.55 61.36</cell><cell>61.63 62.91</cell><cell>56.57 61.73</cell></row><row><cell></cell><cell>+Contrastive (BT)  ?</cell><cell>62.34</cell><cell>78.60</cell><cell>68.65</cell><cell>79.31</cell><cell>77.49</cell><cell>79.93</cell><cell>71.97</cell><cell>74.04</cell></row><row><cell></cell><cell>+SG-OPT  ?</cell><cell>62.57</cell><cell>78.96</cell><cell>69.24</cell><cell>79.99</cell><cell>77.17</cell><cell>77.60</cell><cell>68.42</cell><cell>73.42</cell></row><row><cell></cell><cell>+SimCSE</cell><cell>70.16</cell><cell>81.77</cell><cell>73.24</cell><cell>81.36</cell><cell>80.65</cell><cell>80.22</cell><cell>68.56</cell><cell>76.57</cell></row><row><cell></cell><cell>+DCLR (Ours)</cell><cell>70.01</cell><cell>83.08</cell><cell>75.09</cell><cell>83.66</cell><cell>81.06</cell><cell>81.86</cell><cell>70.33</cell><cell>77.87</cell></row><row><cell></cell><cell>CLS  ?</cell><cell>19.25</cell><cell>22.97</cell><cell>14.93</cell><cell>33.41</cell><cell>38.01</cell><cell>12.52</cell><cell>40.63</cell><cell>25.96</cell></row><row><cell></cell><cell>Mean  ?</cell><cell>33.63</cell><cell>57.22</cell><cell>45.67</cell><cell>63.00</cell><cell>61.18</cell><cell>47.07</cell><cell>58.38</cell><cell>52.31</cell></row><row><cell></cell><cell>First-Last AVG</cell><cell>58.91</cell><cell>58.62</cell><cell>61.44</cell><cell>69.05</cell><cell>65.23</cell><cell>59.38</cell><cell>58.84</cell><cell>61.64</cell></row><row><cell>RoBERTa-large</cell><cell>+whitening</cell><cell>64.17</cell><cell>73.92</cell><cell>71.06</cell><cell>76.40</cell><cell>74.87</cell><cell>71.68</cell><cell>58.49</cell><cell>70.08</cell></row><row><cell></cell><cell>+Contrastive (BT)  ?</cell><cell>57.60</cell><cell>72.14</cell><cell>62.25</cell><cell>71.49</cell><cell>71.75</cell><cell>77.05</cell><cell>67.83</cell><cell>68.59</cell></row><row><cell></cell><cell>+SG-OPT  ?</cell><cell>64.29</cell><cell>76.36</cell><cell>68.48</cell><cell>80.10</cell><cell>76.60</cell><cell>78.14</cell><cell>67.97</cell><cell>73.13</cell></row><row><cell></cell><cell>+SimCSE</cell><cell>72.86</cell><cell>83.99</cell><cell>75.62</cell><cell>84.77</cell><cell>81.80</cell><cell>81.98</cell><cell>71.26</cell><cell>78.90</cell></row><row><cell></cell><cell>+DCLR (Ours)</cell><cell>73.09</cell><cell>84.57</cell><cell>76.13</cell><cell>85.15</cell><cell>81.99</cell><cell>82.35</cell><cell>71.80</cell><cell>79.30</cell></row></table><note><p><p><p><p><p>Sentence embedding performance on STS tasks (Spearman's correlation). The best performance and the second-best performance methods are denoted in bold and underlined fonts respectively. ?: results from</p><ref type="bibr" target="#b23">Kim et al. (2021)</ref></p>; ?: results from</p><ref type="bibr" target="#b16">Gao et al. (2021)</ref></p>; all other results are reproduced or reevaluated by ourselves.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation and variation studies of our approach on the test set of seven STS tasks.</figDesc><table><row><cell>Model</cell><cell>STS-Avg.</cell></row><row><cell>BERT-base+Ours</cell><cell>77.22</cell></row><row><cell>w/o Noise-based Negatives</cell><cell>76.17</cell></row><row><cell>w/o Instance Weighting</cell><cell>76.31</cell></row><row><cell>BERT-base+Random Noise</cell><cell>75.22</cell></row><row><cell>BERT-base+Knowledge Distillation</cell><cell>75.05</cell></row><row><cell>BERT-base+Self Instance Weighting</cell><cell>73.93</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A contemporaneous work(Wu et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2021) has also utilized Gaussian distribution initialized noise vectors as smoothed representations for contrastive learning of sentence embeddings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For convenience, we utilize SimCSE on BERT-base or RoBERTa-base model as the complementary model.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially supported by <rs type="funder">Beijing Natural Science Foundation</rs> under Grant No. <rs type="grantNumber">4222027</rs>, and <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">61872369</rs>, <rs type="programName">Beijing Outstanding Young Scientist Program</rs> under Grant No. <rs type="grantNumber">BJJWZYJH012019100020098</rs>, the <rs type="funder">Outstanding Innovative Talents Cultivation Funded Programs 2021 and Public Computing Cloud, Renmin University of China</rs>. This work is also supported by <rs type="funder">Beijing Academy of Artificial Intelligence (BAAI)</rs>. <rs type="person">Xin Zhao</rs> is the corresponding author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kud2EMF">
					<idno type="grant-number">4222027</idno>
				</org>
				<org type="funding" xml:id="_phVyjUM">
					<idno type="grant-number">61872369</idno>
					<orgName type="program" subtype="full">Beijing Outstanding Young Scientist Program</orgName>
				</org>
				<org type="funding" xml:id="_9MqWTyB">
					<idno type="grant-number">BJJWZYJH012019100020098</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s15-2045</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/s14-2010</idno>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s16-1081</idno>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">*sem 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<editor>*SEM</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive curriculum learning for sequential user behavior modeling via data augmentation</title>
		<author>
			<persName><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3481905</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
		<meeting><address><addrLine>Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-11-01">2021. November 1 -5, 2021</date>
			<biblScope unit="page" from="3737" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for english</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2029</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of bert, elmo, and GPT-2 embeddings</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CERT: contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno>CoRR, abs/2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Whiteningbert: An easy unsupervised sentence embedding approach</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno>CoRR, abs/2104.01767</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMART: robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.197</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-guided contrastive learning for BERT sentence representations</title>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.197</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2528" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, Antonio Torralba, and Sanja Fidler</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
	<note>Skip-thought vectors</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom?s</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Less is more: zero-shot learning from online textual documents with noise suppression</title>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial robustness through local linearization</title>
		<author>
			<persName><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13824" to="13833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Measuring social biases in grounded vision and language embeddings</title>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08911</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Whitening sentence representations for better semantics and faster retrieval</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno>CoRR, abs/2103.15316</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust unsupervised neural machine translation with adversarial denoising training</title>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xugang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.374</idno>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4239" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Bowman</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP -Demos</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Smoothed contrastive learning for unsupervised sentence embedding</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/2109.04321</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">CLEAR: contrastive learning for sentence representation</title>
		<author>
			<persName><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2012.15466</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Consert: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.393</idno>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Peng Jiang, and He Hu. 2022. C 2 -crs: Coarse-to-fine contrastive learning for conversational recommender system</title>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3488560.3498514</idno>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;22: The Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Tempe, AZ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">February 21 -25, 2022</date>
			<biblScope unit="page" from="1488" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
