<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active deep learning method for semi-supervised sentiment classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shusen</forename><surname>Zhou</surname></persName>
							<email>zhoushusen@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electrical Engineering</orgName>
								<orgName type="institution">Ludong University</orgName>
								<address>
									<settlement>Yantai</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
							<email>qingcai.chen@hitsz.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>wangxl@insun.hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active deep learning method for semi-supervised sentiment classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA38FC4F54499608F35540DE451F5890</idno>
					<idno type="DOI">10.1016/j.neucom.2013.04.017</idno>
					<note type="submission">Received 9 May 2012 Received in revised form 9 April 2013 Accepted 15 April 2013 Communicated by M. Wang</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural networks Deep learning Active learning Sentiment classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In natural language processing community, sentiment classification based on insufficient labeled data is a well-known challenging problem. In this paper, a novel semi-supervised learning algorithm called active deep network (ADN) is proposed to address this problem. First, we propose the semi-supervised learning framework of ADN. ADN is constructed by restricted Boltzmann machines (RBM) with unsupervised learning based on labeled reviews and abundant of unlabeled reviews. Then the constructed structure is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Second, in the semi-supervised learning framework, we apply active learning to identify reviews that should be labeled as training data, then using the selected labeled reviews and all unlabeled reviews to train ADN architecture. Moreover, we combine the information density with ADN, and propose information ADN (IADN) method, which can apply the information density of all unlabeled reviews in choosing the manual labeled reviews. Experiments on five sentiment classification datasets show that ADN and IADN outperform classical semi-supervised learning algorithms, and deep learning techniques applied for sentiment classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, sentiment analysis has received considerable attention in natural language processing (NLP) community <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, such as 'positive' or 'negative', 'thumbs up' or 'thumbs down', 'favorable' or 'unfavorable', etc. <ref type="bibr" target="#b3">[4]</ref>, rather than the subject or topic. Labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success <ref type="bibr" target="#b4">[5]</ref> and is helpful in business intelligence applications, recommender systems, and message filtering tasks <ref type="bibr" target="#b0">[1]</ref>.</p><p>While topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task <ref type="bibr" target="#b0">[1]</ref>. First, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-specific contextual cues <ref type="bibr" target="#b5">[6]</ref>. For example, although the sentence "the thief tries to protect his excellent reputation" contains the word "excellent", it tells us nothing about the author's opinion and in fact could be well embedded in a negative review. Second, sentiment classification systems are typically domain-specific, which makes the expensive process of annotating a large amount of data for each domain and is a bottleneck in building high-quality systems <ref type="bibr" target="#b2">[3]</ref>. This motivates the task of learning robust sentiment models from minimal supervision <ref type="bibr" target="#b5">[6]</ref>.</p><p>Recently, semi-supervised learning, which uses a large amount of unlabeled data together with labeled data to build better learners <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, has attracted more and more attention in sentiment classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. <ref type="bibr">Wang et al.</ref> propose a novel semi-supervised learning algorithm named semi-supervised kernel density estimation, which is developed based on kernel density estimation approach, both labeled and unlabeled data are leveraged to estimate class conditional probability densities based on an extended form of kernel density estimation <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr">Wang et al.</ref> propose a method named optimized multigraph-based semi-supervised learning which aims to simultaneously tackle insufficiency of training data and the curse of dimensionality problems in a unified scheme <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr">Zha et al.</ref> propose a novel graph-based learning framework in the setting of semi-supervised learning with multiple labels <ref type="bibr" target="#b10">[11]</ref>. As argued by several research works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, deep architecture that is composed of multiple levels of non-linear operations <ref type="bibr" target="#b13">[14]</ref>, is expected to perform well in semi-supervised learning, because of its capability of modeling hard artificial intelligent tasks. Deep belief network (DBN) is a representative deep learning algorithm that has achieved notable success for semi-supervised learning in NLP community <ref type="bibr" target="#b13">[14]</ref>. Ranzato and Szummer <ref type="bibr" target="#b14">[15]</ref> introduce an algorithm to learn text Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/neucom document representations, which is based on semi-supervised auto-encoders that are combined to form a deep network.</p><p>Active learning is another way that can minimize the number of required labeled data while getting a competitive result. Rather than choosing the training set randomly, active learning chooses the training data actively, which reduces the needs of labeled data <ref type="bibr" target="#b15">[16]</ref>. It has been widely explored in multimedia research community for its capability of reducing human annotation effort <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Zha et al.</ref> propose a novel active learning approach based on the optimum experimental design criteria in statistics for interactive video indexing <ref type="bibr" target="#b17">[18]</ref>. Active learning is well-suited to many NLP problems, where unlabeled data may be abundant but annotation is slow and expensive <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr">Druck et al.</ref> propose an active learning approach in which the machine solicits labels on features rather than instances <ref type="bibr" target="#b19">[20]</ref>. Zhu et al. combine active and semi-supervised learning under a Gaussian random field model; the active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection <ref type="bibr" target="#b20">[21]</ref>. Recently, active learning has been applied in sentiment classification <ref type="bibr" target="#b2">[3]</ref>.</p><p>Inspired by the study of semi-supervised learning, active learning and deep learning, this paper proposes a semisupervised sentiment classification method called active deep network (ADN). It is based on a representative deep learning method DBN <ref type="bibr" target="#b13">[14]</ref> and active learning method <ref type="bibr" target="#b15">[16]</ref>. First, we introduce the semi-supervised learning procedure of ADN method, which constructs the deep architecture with all unlabeled and labeled reviews, and fine tunes the deep architecture with few labeled reviews. To maximize the separability of the classifier, an exponential loss function is suggested. Second, we introduce the active learning procedure of ADN method. It first identifies a small number of unlabeled reviews for manual labeling by an active learner, and then trains the deep architecture with the labeled reviews and all other unlabeled reviews. Moreover, we propose information ADN (IADN) method, to combine the information density with ADN, which puts the information density of all unlabeled reviews into consideration while choosing the unlabeled reviews for further labeling.</p><p>The main contributions of this paper include: First, this paper introduces a new deep architecture that integrates the abstraction ability of deep belief networks and the classification ability of backpropagation strategy. It improves the generalization capability by using the abundant number of unlabeled reviews, and directly optimizes the classification results in training dataset via the back propagation strategy, which makes it possible to achieve attractive classification performance with few labeled reviews. Second, this paper proposes two effective active learning methods that integrate the review selection ability of active learning and classification ability of deep architecture. Both the labeled review selector and classifier are based on the same architecture, which provides a unified framework for the semi-supervised classification task. Third, this paper applies semi-supervised learning and active learning to sentiment classification successfully and gets competitive performance. Our experimental results on five sentiment classification datasets show that both ADN and IADN outperform previous sentiment classification methods and deep learning methods.</p><p>This paper is an expanded version of Zhou et al. <ref type="bibr" target="#b21">[22]</ref>. Many new contents are incorporated here: First, the related works of sentiment classification have been extended; more detail introduction about sentiment classification methods has been made. Second, an active learning method called IADN is proposed, which combines information density with ADN and achieves competitive performance for sentiment classification. Third, more experiments have been conducted to evaluate the performance of deep architecture, information density incorporation, and various loss functions. Moreover, we evaluate the proposed active learning methods with a different number of labeled and unlabeled reviews.</p><p>The rest of the paper is organized as follows. Section 2 gives an overview of sentiment classification. The proposed semi-supervised learning method ADN is described in Section 3. Section 4 combines ADN and information density into IADN method. Section 5 evaluates ADN and IADN by comparing their classification performance with existing sentiment classification methods and deep learning methods on sentiment datasets. The paper is closed with a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sentiment classification</head><p>Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based <ref type="bibr" target="#b22">[23]</ref> and corpus-based classification methods <ref type="bibr" target="#b23">[24]</ref>. The detail survey about techniques and approaches of sentiment classification can be seen in the book <ref type="bibr" target="#b24">[25]</ref>. In this paper, we focus on corpus-based classification methods.</p><p>Corpus-based methods use a labeled corpus to train a sentiment classifier <ref type="bibr" target="#b23">[24]</ref>. Pang et al. <ref type="bibr" target="#b0">[1]</ref> are the first who apply machine learning approach to corpus-based sentiment classification. They found that standard machine learning techniques outperform human-produced baselines. They also carried out important experiments on selecting the best features and concluded that unigrams performed better than bigrams or unigrams and bigrams together. Dave et al. <ref type="bibr" target="#b25">[26]</ref> draw attention on information retrieval techniques for feature extraction and scoring in the sentiment classification task. Pang and Lee <ref type="bibr" target="#b26">[27]</ref> apply text-categorization techniques to the subjective portions of the sentiment documents. These portions are extracted by efficient techniques for finding minimum cuts in graphs. Gamon <ref type="bibr" target="#b4">[5]</ref> demonstrates that high accuracy can be achieved by using large feature vectors in combination with feature reduction in the very noisy domain of customer feedback data. Mullen and Collier <ref type="bibr" target="#b27">[28]</ref> use support vector machines to bring together diverse sources of potentially pertinent information for sentiment classification, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Ng et al. <ref type="bibr" target="#b28">[29]</ref> demonstrate that sentiment classification can be performed with high accuracy using only unigrams as features. Mcdonald et al. <ref type="bibr" target="#b29">[30]</ref> investigate a structured model for jointly classifying the sentiment of text at various levels of granularity, which is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. Xia et al. <ref type="bibr" target="#b30">[31]</ref> introduce the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light-hearted and heavy-hearted. Li et al. <ref type="bibr" target="#b31">[32]</ref> propose a machine learning approach to incorporate polarity shifting information into a document-level sentiment classification system. Liu et al. <ref type="bibr" target="#b32">[33]</ref> present an adaptive sentiment analysis model that aims to capture the hidden sentiment factors in reviews through the capability of being incrementally updated as more data becoming available. Wei et al. <ref type="bibr" target="#b33">[34]</ref> propose a novel approach to label attributes of a product and their associated sentiments in product reviews by a hierarchical learning process with a defined sentiment ontology tree.</p><p>Supervised sentiment classification systems are domainspecific and annotating a large-scale corpus for each domain is very expensive <ref type="bibr" target="#b2">[3]</ref>. There exists several solutions for this issue.</p><p>The first solution is cross-domain sentiment classification. Aue and Gamon <ref type="bibr" target="#b34">[35]</ref> survey four different approaches to customize a sentiment classification system for a new target domain in the absence of large amounts of labeled data. Blitzer et al. <ref type="bibr" target="#b1">[2]</ref> investigate domain adaptation for sentiment classifiers, which reducing the relative error due to adaptation between domains by an average of 46% over a supervised baseline, and identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. Tan et al. <ref type="bibr" target="#b35">[36]</ref> combine old-domain labeled examples with newdomain unlabeled ones, and retrain the base classifier over all these examples. Li and Zong <ref type="bibr" target="#b36">[37]</ref> study multi-domain sentiment classification which aims to improve performance through fusing training data from multiple domains. Pan et al. <ref type="bibr" target="#b37">[38]</ref> propose a cross-domain sentiment classification method that aligns domainspecific words extracted from different domains into unified clusters, with the help of domain-independent words as a bridge. Bollegala et al. <ref type="bibr" target="#b38">[39]</ref> automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. He et al. <ref type="bibr" target="#b39">[40]</ref> modify the joint sentiment-topic model by incorporating word polarity priors through modifying the topic-word Dirichlet priors, study the polarity-bearing topics extracted by joint sentimenttopic model and show that by augmenting the original feature space with polarity-bearing topics, achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset.</p><p>The second solution is semi-supervised sentiment classification. Goldberg and Zhu <ref type="bibr" target="#b40">[41]</ref> present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference, inferring numerical ratings based on the perceived sentiment. Sindhwani and Melville <ref type="bibr" target="#b41">[42]</ref> propose a semisupervised sentiment classification algorithm that utilizes lexical prior knowledge in conjunction with unlabeled data. Dasgupta and Ng <ref type="bibr" target="#b2">[3]</ref> first mine the unambiguous reviews using spectral techniques, and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning. Li et al. <ref type="bibr" target="#b3">[4]</ref> adopt two views, personal and impersonal views, and employ them in both supervised and semisupervised sentiment classification.</p><p>The third solution is unsupervised sentiment classification. Zagibalov and Carroll <ref type="bibr" target="#b42">[43]</ref> describe an automatic seed word selection method for unsupervised sentiment classification of product reviews in Chinese.</p><p>There are also several other methods to solve this issue. Read <ref type="bibr" target="#b43">[44]</ref> demonstrates that training data automatically labeled with encountered there emoticons has the potential of being independent of domain, topic and time. Wan <ref type="bibr" target="#b23">[24]</ref> studies on cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, English features and Chinese features are considered as two independent views of the classification problem. Lu et al. <ref type="bibr" target="#b44">[45]</ref> present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data.</p><p>However, unsupervised learning of sentiment is difficult, partially because of the prevalence of sentimentally ambiguous reviews <ref type="bibr" target="#b2">[3]</ref>. Using multi-domain sentiment corpus to sentiment classification is also hard to apply. It is because that each domain has a very limited amount of training data, due to annotating a large corpus is difficult and time-consuming <ref type="bibr" target="#b36">[37]</ref>. Cross-domain, semi-supervised learning and unsupervised learning methods are used based on the background that training data is not enough. When there are not enough training data for each domain, we can use cross-domain methods. When there are not enough labeled data, we can use semisupervised learning methods. When there is no labeled data, we can use unsupervised learning methods. In this paper, we just focus on semi-supervised sentiment classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Active deep networks</head><p>In this part, we propose a semi-supervised learning algorithm, active deep network (ADN), to address the sentiment classification problem with active learning. Section 3.1 formulates the ADN problem. Section 3.2 proposes the semi-supervised learning method of ADN. Section 3.3 proposes active learning method of ADN. Section 3.4 gives the ADN procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>The dataset is compound by a substantial amount of product reviews. We preprocess the reviews to be classified, the experimental setting is same with <ref type="bibr" target="#b2">[3]</ref>. Each review is represented as a vector of unigrams, using binary weight equal to 1 for terms present in a vector. Moreover, the punctuation, numbers, and words of length one are removed from the vector. Finally, we sort the vocabulary by document frequency and remove the top 1.5%. It is because that many of these high document frequency words are stopwords or domain specific general-purpose words (e.g., "book" in the book domain), these noise words would not be helpful for sentiment classification. These words typically comprise 1-2% of a vocabulary, the decision of exactly how many terms to remove is subjective: a large corpus typically requires more removals than a small corpus. To be consistent, we simply remove the top 1.5% high frequency words.</p><p>After preprocessing, each review is represented by a vector. Then the dataset is represented as a matrix</p><formula xml:id="formula_0">X ¼ ½x 1 ; x 2 ; …; x RþT ¼ x 1 1 ; x 2 1 ; …; x RþT 1 x 1 2 ; x 2 2 ; …; x RþT 2 ⋮; ⋮; …; ⋮ x 1 D x 2 D ; …; x RþT D 2 6 6 6 6 4 3 7 7 7 7 5<label>ð1Þ</label></formula><p>where R is the number of training reviews, T is the number of test reviews, D is the number of feature words in the dataset. Each column of X corresponds to a sample x of review. A sample that has all features is viewed as a vector in R D , where the jth coordinate corresponds to the jth feature.</p><p>The L labeled reviews are chosen randomly from R training reviews, or chosen actively by active learning, which can be seen as</p><formula xml:id="formula_1">X L ¼ X R ðSÞ; S ¼ ½s 1 ; …; s L 1 ≤s i ≤R<label>ð2Þ</label></formula><p>where S is the index set of selected training reviews to be labeled manually.</p><p>Let Y be a set of labels correspond to L labeled training reviews and is denoted as</p><formula xml:id="formula_2">Y L ¼ ½y 1 ; y 2 ; …; y L ¼ y 1 1 ; y 2 1 ; …; y L 1 y 1 2 ; y 2 2 ; …; y L 2 ⋮; ⋮; …; ⋮ y 1 C ; y 2 C ; …; y L C 2 6 6 6 6 4 3 7 7 7 7 5<label>ð3Þ</label></formula><p>where C is the number of classes. Each column of Y is a vector in R C , where the jth coordinate corresponds to the jth class</p><formula xml:id="formula_3">y j ¼ 1 if x∈jth class -1 if x∉jth class (<label>ð4Þ</label></formula><p>For example, if a review x i is positive, y i ¼ ½1; -1′; otherwise,</p><formula xml:id="formula_4">y i ¼ ½-1; 1′.</formula><p>We intend to seek the mapping function X L -Y L using the L labeled reviews and all unlabeled reviews in order to determine y when a new review x comes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semi-supervised learning</head><p>To address the problem formulated in Section 3.1, we propose a deep architecture for ADN method, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The deep architecture is a fully interconnected directed belief nets with one input layer h 0 , N hidden layers h 1 ; h 2 ; …; h N , and one labeled layer at the top. The input layer h 0 has D units, equal to the number of features of sample review x. The label layer has C units, equal to the number of classes of label vector y. The numbers of units for hidden layers, currently, are pre-defined according to the experience or intuition. The seeking of the mapping function, here, is transformed to the problem of finding the parameter space W ¼ ffw 1 ; w 2 ; …; w N gg for the deep architecture. The semi-supervised learning method based on ADN architecture can be divided into two stages: First, ADN architecture is constructed by greedy layer-wise unsupervised learning using RBMs as building blocks. All the unlabeled reviews together with L labeled reviews are utilized to find the parameter space W with N layers. Second, ADN architecture is trained according to the exponential loss function using gradient descent method. The parameter space W is retrained by an exponential loss function using L labeled data. As it is difficult to optimize a deep architecture using supervised learning directly, the unsupervised learning stage can abstract the reviews effectively, and prevent overfitting of the supervised training.</p><p>For unsupervised learning, we define the energy of the joint configuration ðh</p><formula xml:id="formula_5">k-1 ; h k Þ as [14] Eðh k-1 ; h k ; θÞ ¼ -∑ D k-1 s ¼ 1 ∑ D k t ¼ 1 w k st h k-1 s h k t -∑ D k-1 s ¼ 1 b k-1 s h k-1 s -∑ D k t ¼ 1 c k t h k t<label>ð5Þ</label></formula><p>where θ ¼ ðw; b; cÞ are the model parameters: w k st is the symmetric interaction term between unit s in the layer h k-1 and unit t in the</p><formula xml:id="formula_6">layer h k , k ¼ 1; …; N-1. b k-1 s</formula><p>is the sth bias of layer h k-1 and c t k is the tth bias of layer h k . D k is the number of units in the kth layer.</p><p>The network assigns a probability to every possible data via this energy function. The probability of a training data can be raised by adjusting the weights and biases to lower the energy of that data and to raise the energy of similar, confabulated data that h k would prefer to the real data. When we input the value of h k , the network can learn the content of h k-1 by minimizing this energy function.</p><p>The probability that the model assigns to a h k-1 is</p><formula xml:id="formula_7">Pðh k-1 ; θÞ ¼ 1 ZðθÞ ∑ h k expð-Eðh k-1 ; h k ; θÞÞ<label>ð6Þ</label></formula><formula xml:id="formula_8">ZðθÞ ¼ ∑ h k-1 ∑ h k expð-Eðh k-1 ; h k ; θÞÞ<label>ð7Þ</label></formula><p>where ZðθÞ denotes the normalizing constant.</p><p>The conditional distributions over h k and h k-1 are given as</p><formula xml:id="formula_9">pðh k jh k-1 Þ ¼ ∏ t pðh k t jh k-1 Þ ð 8Þ pðh k-1 jh k Þ ¼ ∏ s pðh k-1 s jh k Þ ð<label>9Þ</label></formula><p>the probability of turning unit t is a logistic function of the states of h k-1 and w k st</p><formula xml:id="formula_10">pðh k t ¼ 1jh k-1 Þ ¼ sigm c k t þ ∑ s w k st h k-1 s<label>ð10Þ</label></formula><p>the probability of turning unit s is a logistic function of the states of h k and w k st pðh k-1 s</p><formula xml:id="formula_11">¼ 1jh k Þ ¼ sigm b k-1 s þ ∑ t w k st h k t<label>ð11Þ</label></formula><p>where the logistic function is</p><formula xml:id="formula_12">sigm ðηÞ ¼ 1=ð1 þ e -η Þ ð<label>12Þ</label></formula><p>The derivative of the log-likelihood with respect to the model parameter w k can be obtained from Eq. ( <ref type="formula" target="#formula_7">6</ref>)</p><formula xml:id="formula_13">∂ log pðh k-1 Þ ∂ w k st ¼ 〈h k-1 s h k t 〉 P 0 -〈h k-1 s h k t 〉 P Model<label>ð13Þ</label></formula><p>where 〈 Á 〉 P 0 denotes an expectation with respect to the data distribution and 〈 Á 〉 P Model denotes an expectation with respect to the distribution defined by the model <ref type="bibr" target="#b45">[46]</ref>. The expectation 〈 Á 〉 P Model cannot be computed analytically. In practice, 〈 Á 〉 P Model is replaced by Ái P 1 , which denotes a distribution of samples when the feature detectors are being driven by reconstructed h k-1 . This is an approximation to the gradient of a different objective function, called the contrastive divergence (CD)</p><p>[47]</p><formula xml:id="formula_14">Δw k st ¼ ηð〈h k-1 s h k t 〉 P 0 -〈h k-1 s h k t 〉 P 1 Þ ð<label>14Þ</label></formula><p>where η is the learning rate.</p><p>Then the parameter w k can be adjusted through</p><formula xml:id="formula_15">w k st ¼ ϑw k st þ Δw k st<label>ð15Þ</label></formula><p>where ϑ is the momentum. The above discussion is based on the training of the parameters between two hidden layers with one sample review x. For unsupervised learning, we construct the deep architecture using all labeled reviews with unlabeled reviews by inputting them one by one from layer h 0 , train the parameters between h 0 and h 1 .</p><p>Then h 1 is constructed, the value of h 1 is calculated by h 0 and the trained parameters between h 0 and h 1 . We can use it to construct the next layer h 2 . The deep architecture is constructed layer by layer from bottom to top. In each time, the parameter space w k is trained by the calculated data in the ðk-1Þth layer.</p><p>According to the w k calculated above, the layer h k is obtained as below for a sample x fed from layer h</p><formula xml:id="formula_16">0 h k t ðxÞ ¼ sigm c k t þ ∑ D k-1 s ¼ 1 w k st h k-1 s ðxÞ ! t ¼ 1; …; D k ; k ¼ 1; …; N-1<label>ð16Þ</label></formula><p>The parameter space w N is initialized randomly, just as backpropagation algorithm. Then ADN architecture is constructed. The top hidden layer is formulated as</p><formula xml:id="formula_17">h N t ðxÞ ¼ c N t þ ∑ D N-1 s ¼ 1 w N st h N-1 s ðxÞ t ¼ 1; …; D N<label>ð17Þ</label></formula><p>For supervised learning, the ADN architecture is trained by L labeled data. The optimization problem is formulized as where</p><formula xml:id="formula_18">argmin h N f ðh N ðX L Þ; Y L Þ ð<label>18Þ</label></formula><formula xml:id="formula_19">fðh N ðX L Þ; Y L Þ ¼ ∑ L i ¼ 1 ∑ C j ¼ 1 Tðh N j ðx i Þy i j Þ ð<label>19Þ</label></formula><p>and the loss function is defined as</p><formula xml:id="formula_20">TðrÞ ¼ exp ð-rÞ ð<label>20Þ</label></formula><p>In the supervised learning stage, the stochastic activities are replaced by deterministic, real valued probabilities. The greedy layer-wise unsupervised learning is just used to initialize the parameter of deep architecture, the parameters of the deep architecture are updated based on Eq. ( <ref type="formula" target="#formula_15">15</ref>). After initialization, real values are used in all the nodes of the deep architecture. We use gradient-descent through the whole deep architecture to retrain the weights for optimal classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Active learning</head><p>Semi-supervised learning allows us to classify reviews with few labeled data. However, annotating the reviews manually is expensive, so we expect to get higher performance with fewer labeled data. Active learning can help to choose those reviews that should be labeled manually in order to achieving higher classification performance with the same number of labeled data. For such purpose, we incorporate pool-based active learning with the ADN method, which accesses to a pool of unlabeled instances and requests the labels for some number of them <ref type="bibr" target="#b15">[16]</ref>.</p><p>Given an unlabeled pool X R and an initial labeled dataset X L (one positive, one negative), the ADN architecture h N will decide which instance in X R to query next. Then the parameters of h N are adjusted after new reviews are labeled and inserted into the labeled dataset. The main issue for an active learner is the choosing of next unlabeled instance to query. In this paper, we choose the reviews of which the labels are most uncertain for the classifier. Drawing on previous work on active learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>, we define the uncertainty of a review as the reciprocal of its distance from the separating hyperplane. In other words, reviews that are near the separating hyperplane are more uncertain than the reviews that are farther away.</p><p>The deep architecture of ADN is trained by all unlabeled data and initial labeled training set with DBN based semi-supervised learning first, which has been introduced in Section 3.2. After semi-supervised learning, the parameters of ADN are adjusted. Given an unlabeled pool X R , the next unlabeled instance to be queried is chosen according to the location of h N ðX R Þ. The distance between a point h N ðx i Þ and the classes separation line</p><formula xml:id="formula_21">h N 1 ¼ h N 2 is d i ¼ jh N 1 ðx i Þ-h N 2 ðx i Þj= ffiffiffi 2 p<label>ð21Þ</label></formula><p>The selected training review to be labeled manually is given by</p><formula xml:id="formula_22">s ¼ ffj : d j ¼ min ðdÞgg<label>ð22Þ</label></formula><p>We can select a group of the most uncertain reviews to label at each time.</p><p>The experimental setting is similar with Dasgupta and Ng <ref type="bibr" target="#b2">[3]</ref>. We perform active learning for five iterations and select 20 of the most uncertainty reviews to be queried each time. Then the ADN is retrained on all labeled and unlabeled reviews so far with semisupervised learning. At last, the label of a review x is determined according to the output h N ðxÞ of the ADN architecture as below</p><formula xml:id="formula_23">y j ¼ 1 if h N j ðxÞ ¼ max ðh N<label>ðxÞÞ</label></formula><formula xml:id="formula_24">-1 if h N j ðxÞ≠max ðh N<label>ðxÞÞ</label></formula><formula xml:id="formula_25">8 &lt; :<label>ð23Þ</label></formula><p>As shown by Tong and Koller <ref type="bibr" target="#b15">[16]</ref>, the balance random method, which randomly sample an equal number of positive and negative instances from the pool, has much better performance than the regular random method. So we incorporate this "Balance" idea with ADN method. However, it is not possible to choose an equal number of positive and negative instances without labeling the entire pool of instances in advance. So we present a simple way to approximate the balance of positive and negative reviews. For each iteration, we count, first, the number of positive and negative labeled reviews respectively. Second, we classify the unlabeled reviews in the pool with the deep architecture trained by the previous iteration. Third, we choose the appropriate number of positive and negative reviews labeled in the second step and add them into the labeled dataset, to let the number of labeled and unlabeled review equally. Fourth, we relabel all these new added reviews manually to ensure the correctness of all the review's label in the labeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ADN procedure</head><p>The procedure of ADN is shown in Algorithm 1. For the training of ADN architecture, the parameters are random initialized with normal distribution. All the training data and test data are used to train the ADN with unsupervised learning, which can be seen as transductive learning <ref type="bibr" target="#b47">[48]</ref>. The training set X R can be seen as an unlabeled pool. We randomly select one positive and one negative review in the pool to input as the initial labeled training set that are used for supervised learning. The number of units in hidden layer D 1 ; …; D N and the number of epochs Q are set manually based on the dimension of the input data and the size of training dataset. The iteration times I and the number of active choosing reviews for each iteration G can be set based on the number of labeled reviews in the experiment.</p><p>For each iteration, the ADN architecture is re-trained by all the unlabeled and labeled reviews with unsupervised learning and supervised learning first, the parameters of deep architecture are initialized with the training results of previous iteration. Then we choose G reviews from the unlabeled pool based on the distance of these data from the separating line, label these reviews manually, and add them into the labeled dataset. For the next iteration, the unsupervised learning is initialized with the parameters trained in the supervised stage of previous iteration, then the supervised learning is applied based on the new labeled dataset again. The unsupervised and supervised learning stages in turn can adjust the parameters with each other, and improve the abstraction and classification ability of the deep architecture. At last, ADN architecture is retrained by all the unlabeled reviews and existing labeled reviews. After training, the ADN architecture is tested based on Eq. <ref type="bibr" target="#b22">(23)</ref>.</p><p>Since the proposed ADN method can active choose the labeled dataset and classify the reviews with the same architecture, which avoids the barrier between choosing and training procedures with different architectures. More importantly, the parameters of ADN are trained iteratively on the labeled data selection process, which further improves the performance of ADN. Algorithm 1. Active deep networks procedure.</p><p>Input: data X, ðX L ; Y L Þ (one positive and one negative reviews); number of layers N; number of epochs Q; number of training data R; number of test data T; normal distribution based random initialize parameter space W; number of iterations I; number of active choosing reviews for each iteration G; Output: deep architecture with parameter space W for i ¼ 1; i ≤I do</p><p>Step 1. Greedy layer-wise unsupervised learning</p><formula xml:id="formula_26">for n ¼ 1; n ≤N-1 do for q ¼ 1; q ≤ Q do for k ¼ 1; k ≤R þ T do</formula><p>Calculate the non-linear positive and negative:</p><formula xml:id="formula_27">pðh k t ¼ 1jh k-1 Þ ¼ sigm ðc k t þ ∑ s w k st h k-1 s Þ pðh k-1 s ¼ 1jh k Þ ¼ sigm ðb k-1 s þ ∑ t w k st h k t Þ</formula><p>Update the weights and biases:</p><formula xml:id="formula_28">Δw k st ¼ ηð〈h k-1 s h k t 〉 P 0 -〈h k-1 s h k t 〉 P 1 Þ end end end</formula><p>Step 2. Supervised learning with gradient descent Minimize f ðh N ðXÞ; YÞ on labeled dataset X L , update the parameter space W according to:</p><formula xml:id="formula_29">argmin h N f ðh N ðX L Þ; Y L Þ</formula><p>Step 3. Choose instances for labeled dataset Choose G instances which near the separating line by:</p><formula xml:id="formula_30">s ¼ ffj : d j ¼ minðdÞgg</formula><p>Add G instances into the labeled dataset X L end Train ADN with Steps 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Information ADN</head><p>In this part, we combine information density idea <ref type="bibr" target="#b18">[19]</ref> with ADN, propose a novel information ADN (IADN) method for semisupervised sentiment classification.</p><p>The proposed ADN method can actively choose the reviews that are near the separating hyperplane as the training data to be labeled manually. However, ADN does not consider the information density of these review candidates. For example, in Fig. <ref type="figure" target="#fig_1">2</ref> is likely to contain more information about the dataset. The IADN method is proposed to put this observation into consideration.</p><p>When the deep architecture is trained by L labeled data and all unlabeled data, the parameters are adapted, h N ðxÞ is used to represent the sample x. Given an unlabeled pool X R , the next unlabeled instance to be queried is chosen according to the location of h N ðX R Þ. The informativeness of h N ðxÞ is weighted by its average similarity to other samples which in the same side of the separation line with h N ðxÞ. It is formalized as</p><formula xml:id="formula_31">ID i ¼ d i Â 1 U-1 ∑ U j ¼ 1;j≠i disðh N ðx i Þ; h N ðx j ÞÞ ! β<label>ð24Þ</label></formula><p>where</p><formula xml:id="formula_32">X U ¼ ffj : x j ∈X R ∩ðh N 1 ðxÞ-h N 2 ðxÞÞ Â ðh N 1 ðx j Þ-h N 2 ðx j ÞÞ 4 0gg<label>ð25Þ</label></formula><p>indicates the unlabeled instances that belong to the same class of x based on the classification result of current trained classifier</p><formula xml:id="formula_33">disðh N ðx i Þ; h N ðx j ÞÞ ¼ jh N 1 ðx i Þ-h N 1 ðx j Þj þ jh N 2 ðx i Þ-h N 2 ðx j Þj<label>ð26Þ</label></formula><formula xml:id="formula_34">denotes the distance of h N ðx i Þ and h N ðx j Þ. d i denotes the distance</formula><p>between a point h N ðx i Þ and the separation line, and is defined by Eq. ( <ref type="formula" target="#formula_21">21</ref>). β controls the relative importance of the density term.</p><p>The training reviews that should be labeled manually are given by</p><formula xml:id="formula_35">s ¼ ffx i : ID i ¼ minðIDÞgg<label>ð27Þ</label></formula><p>The balance selection procedure in ADN is not consider the cases when there are not enough positive (or negative) reviews for section, it will select the reviews randomly in this case. For IADN method, we select all remaining positive (or negative) reviews, and fill the gap with remaining negative (or positive) reviews in this case. The density calculation relies on the prediction of the labels by the classifier is necessary, although it might be mislead in. Because we do not know the label of the reviews in the pool. The classifier can recognize most of the reviews rightly. Even though some reviews which near the separation line are recognized wrongly, there are little effect for the density calculation. Because these wrong recognized reviews are close to these two classes of reviews at the same time.</p><p>The ADN and IADN architecture have different number of hidden units for each hidden layer. The width of deep architecture used in different datasets is setting based on the scale of the dataset and the dimension of the input data. If the scale of the dataset is increasing, and the dimension of the input data is increasing, the number of hidden units for each hidden layer is increasing too. Because more parameters in the large deep architecture need more high dimension training data to train the architecture effectively. Moreover, the number of units in the last hidden layer is more than other hidden layers, because the last hidden layer is linear, and the other hidden layers are non-linear, more units are needed to be used in linear layer to represent a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct several experiments to compare the performance of ADN and IADN with which of existing methods. We have the following questions in mind while designing and conducting the experiments:</p><p>1. How do ADN and IADN perform when compared with other state-of-the-art semi-supervised learning methods for sentiment classification? 2. How do ADN and IADN perform when compared with semisupervised learning method based on our proposed deep architecture? 3. How does information density performs when there are few labeled data? 4. How does deep architecture performs for different loss functions? 5. How does varying the number of labeled reviews affect the performance of ADN and IADN? 6. How does varying the number of unlabeled reviews affect the performance of ADN and IADN?</p><p>These questions are answered in the following subsections: question 1 in Section 5.2, question 2 in Section 5.3, question 3 in Section 5.4, question 4 in Section 5.5, question 5 in Section 5.6, and question 6 in Section 5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>The performance of proposed ADN and IADN methods is evaluated by five sentiment classification datasets. The first dataset is MOV <ref type="bibr" target="#b0">[1]</ref>, which is a widely-used movie review dataset. The other four datasets contain reviews of four types of products, which include books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT) respectively <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Each dataset contains 1000 positive and 1000 negative reviews.</p><p>For MOV dataset, the ADN and IADN structures used in this experiment are 100-100-200-2, which represents the number of units in output layer is 2, and in 3 hidden layers are 100, 100, and 200 respectively. For the other four datasets, the ADN and IADN structures used in this experiment are 50-50-200-2. The number of units in input layer is the same as the dimensions of each dataset. The number of units in the third hidden layer is more than previous two hidden layers, because the unit of third hidden layer is linear, more units can improve the representation ability of third hidden layer. As the size of vocabulary in MOV dataset is larger than in other four datasets, the number of units in previous two hidden layers for MOV dataset is more than for other four datasets. The architecture of ADN and IADN is similar with DBN, but with a different loss function introduced for supervised learning stage. The parameters of the deep architecture are fixed as the default parameter settings of Hinton DBN package <ref type="bibr" target="#b13">[14]</ref>. For greedy layer-wise unsupervised learning, we train the weights of each layer independently with the 30 epochs and the learning rate is set to 0.1. The initial momentum is 0.5 and after five epochs, the momentum is set to 0.9. For supervised learning, we run 10 epochs, three times of linear searches are performed in each epoch.</p><p>We compare the classification performance of ADN and IADN with six representative classifiers, i.e., semi-supervised spectral learning (Spectral) <ref type="bibr" target="#b48">[49]</ref>, transductive SVM (TSVM), active learning (Active) <ref type="bibr" target="#b15">[16]</ref>, mine the easy classify the hard (MECH) <ref type="bibr" target="#b2">[3]</ref>, deep belief networks (DBN) <ref type="bibr" target="#b13">[14]</ref>, and recursive autoencoders (RAE) <ref type="bibr" target="#b22">[23]</ref>. Spectral learning, TSVM, and active learning method are three baseline methods for sentiment classification. Spectral learning incorporates labeled data into the clustering framework in the form of must-link and cannot-link constraints. TSVM is the semisupervised learning version of SVM. Active learning is implemented based on SVM, which training an inductive SVM on one labeled review from each class, iteratively labeling the most uncertain unlabeled reviews and re-training the SVM until 100 reviews are labeled. MECH is a new semi-supervised method for sentiment classification <ref type="bibr" target="#b2">[3]</ref>, which first mines the unambiguous reviews using spectral techniques, and then exploits them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning. The implementation details of Spectral, TSVM, Active, and MECH methods are introduced by Dasgupta and Ng <ref type="bibr" target="#b2">[3]</ref>. DBN is a classical deep learning method proposed recently <ref type="bibr" target="#b13">[14]</ref>. The parameters of the DBN in our experiment are fixed as the default parameter settings of Hinton DBN package <ref type="bibr" target="#b13">[14]</ref>, the minimum mean squared error loss function is used in DBN architecture. RAE learns vector space representations for multi-word phrases based on recursive autoencoders. The implementation details of RAE are introduced by Socher et al. <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ADN performance</head><p>To compare the performance of ADN and IADN with previous works, similar to Dasgupta and Ng <ref type="bibr" target="#b2">[3]</ref>, we randomly divide the 2000 reviews into 10 folds and test all the algorithms using crossvalidation. All reviews are used as unlabeled data, where 1000 used for training and the other 1000 for test. In each fold, 100 reviews are random selected as training data and the remaining 100 reviews are used for test. For the randomness involved in the choice of labeled data, all the results of Spectral, TSVM, and DBN methods are acquired by repeating 10 times for each fold and then taking average over results. For Active, MECH, ADN, and IADN methods, one positive and one negative reviews are selected for the initialization of active leaning, 100 labeled reviews are chosen from the training dataset by active learning and are used for training the classifier. For Active, MECH, ADN, and IADN methods, the active learning is performed for five iterations. In each iteration, 20 of the most uncertain points are selected and labeled, and then the classifier is retrained on all of the unlabeled reviews and labeled reviews annotated so far. After five iterations, 100 labeled reviews are used for training. For these active learning methods, the initial two labeled reviews are selected randomly, so we repeat 30 times for each method, and the results are averaged. For Spectral, TSVM, DBN, and RAE methods, 100 labeled reviews are selected randomly. For Active, MECH, ADN, and IADN methods, 100 labeled reviews are selected by active learning, just the first two labeled reviews are selected randomly.</p><p>The classification accuracies on test data in cross validation for five datasets and eight methods are shown in Table <ref type="table" target="#tab_1">1</ref>. The results of previous four methods are reported by Dasgupta and Ng <ref type="bibr" target="#b2">[3]</ref>. The structure and parameter used for DBN are the same as ADN and IADN in this experiment. The experiment of RAE is done based on the default parameters of the source code proposed by Socher et al. <ref type="bibr" target="#b22">[23]</ref>. Through Table <ref type="table" target="#tab_1">1</ref>, we can see that the performance of DBN is competitive with MECH. Since MECH is the combination of spectral clustering, TSVM and active learning, DBN is just a classification method based on deep neural network, this result shows the good learning ability of deep architecture. ADN is a combination of semi-supervised learning and active learning based on deep architecture, the performance of ADN is better than previous six methods on all the five datasets. This could be contributed by: First, ADN uses a deep architecture to guide the output vector of samples belonged to different regions of new Euclidean space, which can abstract the useful information that is not accessible to other learners; Second, ADN uses an exponential loss function to maximize the separability of labeled reviews in global refinement for better discriminability; Third, ADN fully exploits the embedding information from the large amount of unlabeled reviews to improve the robustness of the classifier; Fourth, ADN chooses the useful training reviews actively, which also improves the classification performance. The performance of IADN is better than previous seven methods. It is because that the semi-supervised learning method used in IADN is same as in ADN, and the active learning method used in IADN improves the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of active learning</head><p>To evaluate the contribution of the active learning in the proposed methods, we conduct following experiments. The architectures used in this section are the same as Section 5.2.</p><p>Passive learning: We are randomly select 100 reviews from the training fold and use them as labeled data. Then the semisupervised learning method used in ADN is applied, to train and test the performance, which is called ADN with passive learning method (or Passive learning for short) and is denoted as "Pas." in Fig. <ref type="figure">3</ref>. The experiment is run 10 times for each fold, and the average is taken over all results. The test accuracies of DBN, ADN with passive learning, and ADN on five datasets are shown in Fig. <ref type="figure">3</ref>. Compared with DBN, the mean accuracy on five datasets for ADN with passive learning has been improved from 69.7% to 71.2%, which is contributed by the exponential loss function used in ADN architecture. Compared with ADN method, the mean accuracy on five datasets for ADN with passive learning is reduced from 74.2% to 71.2%, which proves the effectiveness of the active learning.</p><p>Fully supervised learning: We train a fully supervised classifier using all 1000 training reviews based on the ADN architecture, which is called ADN with supervised learning and is denoted by "Sup." in Fig. <ref type="figure">3</ref>. The test accuracies of ADN, and ADN with supervised learning on five datasets are also shown in Fig. <ref type="figure">3</ref>. Compared with the ADN method, we can see that employing only 100 active learning points enables us to reach nearly the same performance as the fully-supervised method on three datasets. Compared with ADN method, the mean accuracy on five datasets for ADN with supervised learning is just improved from 74.2% to 75.4%. However, the number of required labeled data has been increased from 100 to 1000.</p><p>Performance curve: We use KIT dataset to test the performance curve of ADN and IADN with iterations of active learning, the results are shown in Fig. <ref type="figure">4</ref>. Through the figure, we can see that the performance of IADN is much better than ADN, especially in previous iterations. This proves the effect of information density method. Moreover, with iterations of active leaning, ADN and IADN methods curve quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of information density</head><p>To evaluate the contribution of the information density idea in the proposed IADN methods, we conduct following experiments.</p><p>The architectures used in this section are the same as Section 5.2. Different from previous experiments, for active learning of ADN and IADN, two of the most uncertain points are selected and labeled in each iteration, after five iterations, 10 labeled reviews are used for training.</p><p>The test accuracies of ADN and IADN with 10 labeled reviews on five datasets are shown in Fig. <ref type="figure" target="#fig_3">5</ref>. We can see that the performance of IADN is better than ADN in all five datasets. Because for every iteration, just two most uncertain points are selected and labeled. The wrong select of any points can let the performance of the classifier worse. So this experimental setting can emphasize the effect of information density idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effect of loss function</head><p>In ADN and IADN architectures, we use exponential loss function to replace the squared error loss function of classical DBN architecture. Another type of popular loss function is hinge loss function used in SVM. The detail analysis about these loss functions can be seen in <ref type="bibr" target="#b49">[50]</ref>. In this part, we just experimentally evaluate the performance of these loss functions for sentiment classification.  The test accuracies of deep architectures with different loss functions on five datasets are shown in Fig. <ref type="figure">6</ref>. The results show that exponential loss function reaches the best performance on all sentiment datasets, and the differences against the second best methods are statistically significant (p o 0:05) with the paired ttest for all five datasets. The performance of squared error loss function is competitive with hinge loss function. This proves the effectiveness of exponential loss function used in ADN and IADN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Semi-supervised learning with variance of labeled data</head><p>To verify the performance of ADN and IADN with different number of labeled data, we conduct another series of experiments on five datasets and show the results in Fig. <ref type="figure">7</ref>. The architectures for ADN and IADN used in this experiment are the same as Section 5.2. For both ADN and IADN methods, we repeat 30 times for each experimental setting, and the results are averaged. Fig. <ref type="figure">7</ref> shows that ADN and IADN can reach a relative high accuracy by using just 20 labeled reviews for training. For most of the five sentiment datasets, the test accuracies are increasing slowly while the number of labeled review is growing. It also shows that the performance of ADN is competitive with IADN. In MOV and ELE datasets, the performance of ADN is even better than IADN. Because there are few abnormal reviews in these two datasets, the information density restriction does not take effect in these two experiments. In the other three datasets, the performance of IADN is better than ADN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Semi-supervised learning with variance of unlabeled data</head><p>To verify the contribution of unlabeled reviews for ADN and IADN methods, we conduct several experiments with different number of unlabeled reviews and 100 labeled reviews. In these experiments, 1000 reviews are used as training data, ADN and IADN can select 100 reviews actively and labeled these reviews for supervised learning. Comparing with the experiments in Section 5.2, we just reduce the number of unlabeled data in unsupervised learning stage. The architectures for ADN and IADN used here are also the same as Section 5.2. For both ADN and IADN methods, we repeat 30 times for each experimental setting, and the results are averaged.</p><p>The test accuracies of ADN and IADN with different number of unlabeled reviews and 100 labeled reviews on five datasets are shown in Fig. <ref type="figure" target="#fig_5">8</ref>. We can see that ADN and IADN perform well when  just using 800 unlabeled reviews. When the number of unlabeled reviews is reduced from 2000 to 800, the performance of ADN and IADN is not worse. For DVD dataset, the performance of ADN and IADN which use 800 unlabeled reviews is better than them using 2000 unlabeled reviews. When the number of unlabeled reviews is reduced from 800 to 100, the performance of ADN and IADN get worse quickly. This proves that ADN and IADN can get competitive performance with just few labeled reviews and appropriate number of unlabeled reviews. Inclusion of unlabeled data does always improve the performance, however, if there are enough unlabeled data, add more unlabeled data just add much time needed for training, the performance will not improve significantly. Considering the much time needed for training with more unlabeled reviews and less accuracy improved for ADN and IADN method, we suggest using appropriate number of unlabeled reviews in real application. In this experiment, the performance of ADN is competitive with IADN too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper proposes a novel semi-supervised learning algorithm ADN to address the sentiment classification problem with a small number of labeled reviews. ADN can choose the proper training reviews to be labeled manually, and fully exploit the embedding information from the large amount of unlabeled reviews to improve the robustness of the classifier. We propose a new architecture to guide the output vector of samples locating into different regions of new Euclidean space, and use an exponential loss function to maximize the separability of labeled reviews in global refinement for better discriminability. Moreover, we also propose IADN method, which puts the information density of different reviews into consideration when choosing reviews to be labeled manually.</p><p>The performance of ADN and IADN is compared with existing semi-supervised learning methods and deep learning technique. Experiment results show that both ADN and IADN reach better performance than compared methods. We also conduct experiments to verify the effectiveness of ADN and IADN with different number of labeled reviews and unlabeled reviews separately, and demonstrate that ADN and IADN can get competitive classification performance just by using few labeled reviews and appropriate number of unlabeled reviews.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of active deep networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of information density idea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Test accuracy of DBN and ADN with different experiment setting on five datasets.</figDesc><graphic coords="8,33.46,58.64,249.60,163.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Test accuracy of ADN and IADN with 10 labeled reviews on five datasets.</figDesc><graphic coords="8,302.45,299.23,249.60,191.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Test accuracy of deep architecture with different loss function on five datasets.</figDesc><graphic coords="9,43.27,163.37,249.60,173.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Test accuracy of ADN and IADN with different number of unlabeled reviews on five datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Test accuracy with 100 labeled reviews for five datasets and eight methods.</figDesc><table><row><cell>Type</cell><cell>MOV</cell><cell>KIT</cell><cell>ELE</cell><cell>BOO</cell><cell>DVD</cell></row><row><cell>Spectral</cell><cell>67.3</cell><cell>63.7</cell><cell>57.7</cell><cell>55.8</cell><cell>56.2</cell></row><row><cell>TSVM</cell><cell>68.7</cell><cell>65.5</cell><cell>62.9</cell><cell>58.7</cell><cell>57.3</cell></row><row><cell>Active</cell><cell>68.9</cell><cell>68.1</cell><cell>63.3</cell><cell>58.6</cell><cell>58.0</cell></row><row><cell>MECH</cell><cell>76.2</cell><cell>74.1</cell><cell>70.6</cell><cell>62.1</cell><cell>62.7</cell></row><row><cell>DBN</cell><cell>71.3</cell><cell>72.6</cell><cell>73.6</cell><cell>64.3</cell><cell>66.7</cell></row><row><cell>RAE</cell><cell>66.3</cell><cell>69.4</cell><cell>68.2</cell><cell>61.3</cell><cell>63.1</cell></row><row><cell>ADN</cell><cell>76.3</cell><cell>77.5</cell><cell>76.8</cell><cell>69.0</cell><cell>71.6</cell></row><row><cell>IADN</cell><cell>76.4</cell><cell>78.2</cell><cell>77.9</cell><cell>69.7</cell><cell>72.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: S. Zhou, et al., Active deep learning method for semi-supervised sentiment classification, Neurocomputing (2013), http://dx.doi.org/10.1016/j.neucom.2013.04.017i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the Scientific Research Fund of Ludong University (LY2013004) and National Natural Science Foundation of China (Nos. 61173075 and 60973076).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: domain adaptation for sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="701" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Employing personal/impersonal views in supervised and semi-supervised sentiment classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="414" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="841" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<meeting><address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised Learning Literature Survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Madison, WI, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised kernel density estimation for video annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="384" to="396" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified video annotation via multigraph learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised learning with multiple labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Representation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning Deep Architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>IRO, Universite de Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="419" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of compact document representations with deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active learning in multimedia annotation and retrieval: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tat-Seng, Interactive video indexing with statistical active learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan-Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Richang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="17" to="27" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active learning by labeling features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining active learning and semisupervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2003 Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining</title>
		<meeting><address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active deep networks for semi-supervised sentiment classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics, Coling 2010 Organizing Committee</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1515" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentiment analysis using support vector machines with diverse information sources</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M N</forename><surname>Arifin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured models for fine-to-coarse sentiment analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reynar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lyric-based song sentiment classification with sentiment vector space model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentiment classification and polarity shifting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="635" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PLASA+: adaptive sentiment analysis with application to sales performance prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="873" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentiment learning on product reviews via sentiment ontology tree</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Customizing sentiment classifiers to new domains: a case study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Recent Advances in Natural Language Processing, RANLP 2005 Organising Committee</title>
		<meeting><address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel scheme for domain-transfer problem in the context of sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="979" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-domain sentiment classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatically polarity-bearing topics for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seeing stars when there aren&apos;t many stars: graph-based semi-supervised learning for sentiment categorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs: The First Workshop on Graph Based Methods for Natural Language Processing</title>
		<meeting>TextGraphs: The First Workshop on Graph Based Methods for Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Document-word co-regularization for semisupervised sentiment analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1025" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic seed word selection for unsupervised sentiment classification of Chinese text</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zagibalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using emoticons to reduce dependency in machine learning techniques for sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Association of Computational Linguistics Student Research Workshop, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint bilingual sentiment classification with unlabeled parallel corpora</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spectral learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<meeting><address><addrLine>Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="561" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discriminative deep belief networks for visual data classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2287" to="2296" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
