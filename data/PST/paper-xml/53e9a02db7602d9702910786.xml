<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A novel system for robust lane detection and tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-08-04">4 August 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
							<email>yifei.wang@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Merchant Venturers School of Engineering</orgName>
								<orgName type="laboratory">Visual Information Lab</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Merchant Venturers Building, Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naim</forename><surname>Dahnoun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Merchant Venturers School of Engineering</orgName>
								<orgName type="laboratory">Visual Information Lab</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Merchant Venturers Building, Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alin</forename><surname>Achim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Merchant Venturers School of Engineering</orgName>
								<orgName type="laboratory">Visual Information Lab</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Merchant Venturers Building, Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A novel system for robust lane detection and tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-08-04">4 August 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">E185AF6514C9CE7280A952E8DF6F0FF1</idno>
					<idno type="DOI">10.1016/j.sigpro.2011.07.019</idno>
					<note type="submission">Received 20 December 2010 Received in revised form 15 April 2011 Accepted 25 July 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Lane feature extraction Lane detection Lane tracking Particle filtering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a lane detection and tracking system based on a novel lane feature extraction approach and the Gaussian Sum Particle filter (GSPF). The proposed feature extraction approach is based on the fact that by zooming into the vanishing point of the lanes, the lane markings/boundaries will only move on the same straight lines they are on. Objects other than the lanes in the frame do not share this property and can be ignored during the model parameter estimation. This algorithm is able to iteratively refine various traditional feature maps and to operate with curved roads. The tracking part of the system is initialised by a deformable template matching algorithm. Three types of tracking algorithms are compared in our study: the original Sequential Importance Resampling (SIR) particle filter, the Gaussian Particle Filter (GPF) and the Gaussian Sum Particles Filter (GSPF). The GSPF achieves the best performance by integrating a novel likelihood function and an intuitive parameter selection process. Both the GSPF and GPF provide improved tracking performance and require less computational power than the SIR. It has also been found that the detection and tracking performance is enhanced significantly by incorporating the refined feature map.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Throughout the last two decades, a significant amount of research has been carried out in the area of road/lane analysis. This topic can be separated into two essential building blocks: lane detection and tracking. For the lane detection algorithms, most of the existing solutions can be divided into two categories: feature-based and modelbased methods. The feature-based methods locate the road areas using segmentation methods whereas the model-based methods represent the lane boundaries by mathematical models. The uses of the two different methods are mainly determined by the road condition. Feature-based methods extract and analyse local lane features in order to separate the lane from the background pixel by pixel. Many feature-based algorithms are able to detect unmarked or unstructured road. Unlike the feature-based methods, model-based methods normally incorporate various constraints (such as a flat road or parallel road boundaries) during the detection stage to minimise the error and provide a simple description of the lane with mathematical models such as parabolas <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> or splines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Existing lane detection algorithms include, among others, the Hough Transform <ref type="bibr" target="#b2">[3]</ref>, active contour <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, Maximum a Posterior estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and segmentation-based approaches <ref type="bibr" target="#b6">[7]</ref>. However, all these algorithms operate based on the lane feature map extracted earlier. The performance of the detection algorithms is closely related to the quality of the lane features.</p><p>One of the most popular lane features is the edges <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> since the lane boundaries are normally assumed to be clearly marked with white or yellow paint in high contrast. However, if the edge threshold is either too small or too high, a large number of irrelevant features will be included or correct lane features can be omitted. As a result, if the driving environment is noisy or varying rapidly, the edges can mislead the detection algorithms. There are also systems that use the intensity-bump (dark-bright-dark pattern) filter to extract the lane markings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. This feature extractor introduces less noise on the feature map but it only works with marked roads and the determination of the threshold is still a difficult problem. Colour information has not been extensively used as a primary type of feature until recently due to the fact that the lane boundary colour is not unique and it can be easily influenced by illumination changes. In order to counter these difficulties, a lane detection system based on the colour sampled from the road surface is introduced <ref type="bibr" target="#b12">[13]</ref>. The algorithm first locates the rough positions of the lane boundaries using the algorithm proposed in <ref type="bibr" target="#b13">[14]</ref>. Pixel values in an area within the lane boundaries are used as samples to construct the Gaussian distributed colour profile of the road. By doing so, the road colour profile is updated frame by frame to adapt to different lighting conditions. However, if large areas between the lanes are occupied by other objects, the Gaussian colour model may be inaccurate and cause the inclusion of unwanted features. In <ref type="bibr" target="#b14">[15]</ref>, the authors addressed this problem not only by using the colour information but also by evaluating the shapes and movements of the highlighted areas to exclude the unwanted features. Another important feature can be extracted since the lane is a line structure which contains strong orientation information. For the gradient-based methods, since the edge magnitudes are measured with intensity changes, they cannot be used to represent the strength of the directivity strength. In <ref type="bibr" target="#b15">[16]</ref>, the DCT coefficients are used to locate the diagonally orientated high-frequency locations. However, this type of feature is not general enough to accurately represent the lane boundaries in all situations. In <ref type="bibr" target="#b6">[7]</ref>, the authors used texture anisotropy fields as features, based on the covariance matrix of the grey value changes in an image, to find the image areas with distinct orientations. Another well-known algorithm using the orientation information is introduced in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> where the authors propose a feature extraction algorithm based on the steerable filter. The steerable filter uses the second derivative of Gaussian as the basis function and is able to find the orientations corresponding to the maximum and minimum filter responses. This difference in the filter response reveals the strength of the orientation at the current pixel position. There are also algorithms that combine various information for more accurate feature extraction results. In <ref type="bibr" target="#b18">[19]</ref>, the authors combine the line intensity, position and the gradient direction as lane features.</p><p>Most of the feature extraction algorithms represent the detected features on the image space apart from colourbased methods. Since the main focus of the proposed feature extraction approach is to improve the feature maps represented on the image space, colour-based methods have been excluded from this paper.</p><p>One main problem with most existing feature extraction algorithms is that the global shape information of the lane geometry is not included <ref type="bibr" target="#b19">[20]</ref>. In complicated situations where the road is cluttered with heavy shadows or other objects, image content in various locations share similar properties with the lane boundaries. If the feature extraction algorithm is only based on a local image area, unwanted feature points will be detected. These features will then be included during the parameter estimation and tracking stages that cause inaccuracies and errors. Hence, these feature points are not desired and should be excluded before the parameter estimation process.</p><p>This paper proposes a novel lane feature extraction approach which readily solves the above problem. It significantly improves the feature extraction performance based on the traditional methods and subsequently improves lane detection and tracking performance. This approach utilises the global lane shape information and is able to extract feature points that are most likely to belong to the lanes. Another important factor is that the proposed algorithm is designed to iteratively refine the feature map obtained by most of the traditional feature extraction methods such as edge detection, intensity-bump filter and steerable filter. The only requirement for applying the proposed algorithm is that the lane features are able to be represented on the image space. The computational complexity of the algorithm can vary according to the chosen feature extraction algorithm. If the chosen algorithm is likely to include a large amount of noise such as edge detection, more iterations are demanded.</p><p>After feature extraction, a classic model-based parameter estimation algorithm, proposed in <ref type="bibr" target="#b0">[1]</ref>, is applied to optimise the model parameters. In our work, similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, the left and right lane boundaries are modelled as two parallel parabolas on the ground plane. The proposed feature extraction approach can be used with different lane models and in conjunction with most of existing parameter estimation algorithms such as the ones proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. In addition, our method does not rely on a flat road assumption.</p><p>In general, after the lane boundaries are detected, lane tracking is the final building block of any complete system. This step is often omitted in many existing lane detection systems and only few tracking algorithms have been tested in this application <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21]</ref>. However, the tracking step is very important since it gathers valuable information from previous results and utilises this information to find the lane parameters accurately and quickly.</p><p>Generally, the most popular tracking algorithms employ various types of Kalman filters and particle filters. These algorithms normally have lower computational complexity compared with the detection algorithms. The particle filter algorithm and its derivatives normally demand much higher computing power compared with Kalman filters. However, due to the rapidly growing parallel computing systems, the particle filters can be implemented very efficiently. The particle filters are capable of operating under severe non-linear and non-Gaussian environments. They also have a higher ability to keep tracking correctly when encountering discontinuities. The authors in <ref type="bibr" target="#b21">[22]</ref> incorporate multiple cues and use a particle filter algorithm for both lane detection and tracking. However, their system only detects and tracks straight lanes. In <ref type="bibr" target="#b9">[10]</ref> a novel algorithm for vanishing points and lane detection is presented and the parameters of the lane model are tracked by applying the condensation algorithm. The paper also claimed that the tracking performance of the condensation algorithm was much more robust than that of the extended Kalman filter. In <ref type="bibr" target="#b22">[23]</ref> the authors implemented a particle filter on the Inverse Perspective Mapping (IPM) transformed image in order to select edge points corresponding to the lane boundaries or markings based on the gradient magnitude and colour.</p><p>Although the traditional SIR particle filter has been applied for lane tracking purposes, the performance of derivatives of particle filters has not yet been exploited. Possibly, the later developments of particle filters are more suitable for lane tracking. Also, one of the problems of applying the SIR particle filter lies in the parameter selection stage. Even though the posterior probability density of the parameters is available, it is still difficult to pick a single set of parameters to represent the actual lanes. In complicated environments, if the mean is chosen, the final results could be influenced by non-zero mean noise and become inaccurate. If the values corresponding to the highest probability are chosen, the results could be unstable or drift away completely. Furthermore, the resampling step of the SIR particle filter cannot be implemented efficiently in parallel, which limits the speed of the tracking step significantly in a hardware implementation.</p><p>Motivated by the above reasons, three types of particle filters are tested and compared in our system: SIR <ref type="bibr" target="#b23">[24]</ref>, GPF <ref type="bibr" target="#b24">[25]</ref>, and GSPF <ref type="bibr" target="#b25">[26]</ref>. While implementing the GSPF, we also apply a new likelihood function and an intuitive way of selecting the final parameter values. Furthermore, while comparing with the SIR particle filter, the GPF and the GSPF do not require resampling, which means they are amenable for parallel implementations with less processing power.</p><p>Better tracking results are not only achieved by applying a suitable tracking algorithm, but also by incorporating our proposed feature map. A good feature map allows a higher detection rate and more accurate detection results. Superior detection results then lead to higher tracking quality. On the contrary, even with identical parameter initialisations, the particle filter performance can be significantly enhanced based on the proposed feature map.</p><p>The contributions of the paper are twofold. One relates to the lane detection stage and the other to the tracking stage. During the lane detection stage, a novel lane feature extraction approach is proposed. This approach is able to successfully improve most types of lane feature maps and consequently improve the lane model parameter estimation and the tracking results. In the tracking stage, the performance of three different particle filters are compared including the GPF and GSPF that have not been used in any lane tracking systems. The lane tracking process is based on a novel likelihood function and parameter selection process in order to achieve high quality tracking results in various situations.</p><p>The rest of this paper is organised as follows. Section 2 presents our proposed lane feature extraction algorithm and lane parameter estimation based on the proposed feature map. Section 3 describes the implementation of the three different particle filters for lane tracking. Section 4 illustrates the lane detection and tracking results and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Lane detection</head><p>The complete lane detection process is separated into three steps: lane modelling, feature extraction and parameter estimation. Each of the steps will be discussed in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lane modelling</head><p>Various lane models have already been proposed, ranging from straight line segments <ref type="bibr" target="#b5">[6]</ref> to flexible splines <ref type="bibr" target="#b4">[5]</ref>. Simple models cannot represent the lane shapes accurately, especially in the far-field. On the other hand, sophisticated lane models result in much heavier computational costs and may increase the amount of detection error.</p><p>In our system, the left and right lane boundaries are represented by two parabolas with constant curvature on the ground plane (the constant curvature assumption is relaxed during the lane tracking stage). These two parabolas are transformed onto the image plane based on perspective mapping for processing <ref type="bibr" target="#b26">[27]</ref>. This classic model has been used most widely in the existing lane detection systems <ref type="bibr" target="#b17">[18]</ref>. The model incorporates parallel line and planar ground surface constraints which are suitable for most of the highway applications. Also, this model requires a small number of parameters which lead to more accurate and faster parameter estimation. The drawback is the lack of flexibility in representing complicated lane shapes. This model is chosen in our system based on the above properties and, also, as the algorithm could be compared with more existing systems. The detailed derivation of this lane model can be found in <ref type="bibr" target="#b26">[27]</ref>.</p><p>The input image has M rows and N columns and the intensity of the pixel ðm,nÞ is represented as Iðm,nÞ. The transformed parabolas are shown in Eqs. (1) and (2):</p><formula xml:id="formula_0">LM 1 ðx,mÞ ¼ a 1 mÀvpm þ b 1 ðmÀvpmÞþc 1<label>ð1Þ</label></formula><formula xml:id="formula_1">LM 2 ðx,mÞ ¼ a 2 mÀvpm þ b 2 ðmÀvpmÞþc 2<label>ð2Þ</label></formula><formula xml:id="formula_2">where x ¼ ½a 1 ,a 2 ,b 1 ,b 2 ,c 1 ,c<label>2</label></formula><p>is the array storing all the unknown parameters. LM 1 ðx,mÞ and LM 2 ðx,mÞ are defined as the models for the left and right lane boundaries. These also represent the n-axis positions of the sample points on image row m for the left and right lane model respectively. vpm denotes the vanishing line position on the image. Assuming a flat road, this parameter can be calculated from the fixed camera parameters and is treated as known in this paper.</p><p>A restriction is included in the lane model in order to minimise the number of parameters for faster and more accurate detection and tracking as suggested in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10]</ref>. Specifically, the left and right lanes are assumed to be parallel. By doing so, the parameters a 1 and a 2 controlling the curvature of the two lane boundaries are set to be identical. Also, as an effect of perspective projection, the two lines intersect the vanishing line at the same point. The parameters c 1 and c 2 , representing the vanishing point positions, are set to be identical as well. The simplified lane model is shown in the following equations:</p><formula xml:id="formula_3">LM 1 ðx,mÞ ¼ a mÀvpm þb 1 ðmÀvpmÞþc<label>ð3Þ</label></formula><formula xml:id="formula_4">LM 2 ðx,mÞ ¼ a mÀvpm þb 2 ðmÀvpmÞþc<label>ð4Þ</label></formula><p>As a result, the six parameters in the model are reduced to four. This allows for more prior information to be included in the model while significantly simplifying the computation. However, the downside of this restriction is obviously that the model lacks the ability to represent non-parallel lane boundaries accurately. Nevertheless, this is rarely the case. The dynamic range of the parameter a which controls the lane curvature normally varies from À 1500 to 1500. b 1 and b 2 correspond to the horizontal positions (this controls the positions of the intersections between the curves and the lowest row of the image) of the left and right curves. Their values normally vary between À2 and 2. The vanishing point position parameter c represents the intersection between the vanishing line and the two tangent lines to the curves at the lowest row of the image. Its dynamic range is typically from 0 to the image width. The suggested dynamic ranges of the parameters are based on the most common situations, in some extreme cases the values of some parameters can exceed the above ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Proposed lane feature extraction algorithm</head><p>In the existing lane detection systems, the most commonly used feature is the image gradient or edge <ref type="bibr" target="#b17">[18]</ref>. It requires small computational power and extracts sharp transitions in the image intensity. Well-painted lane markings produce strong edges, which benefits the detection of lanes. However, as the environment changes, the lane edges may not be as strong and may be heavily affected by shadows or adverse weather conditions. The choice of the edge threshold has always been a difficult task and some existing systems choose a very small value or use the image gradient directly without thresholding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. This means that many unwanted features, such as edges corresponding to trees, cars, buildings, shadows and so on, are included. The detection and tracking of lanes are thus more difficult and time consuming since a large number of outliers are involved. Other lane feature extractors, such as the steerable filter, have been proved to be useful as well <ref type="bibr" target="#b17">[18]</ref>. The steerable filter is able to find feature points with high directivity. It requires more computational power than the gradient and it still considers only local information. As a result, distractive features could also be included.</p><p>The proposed feature extraction algorithm considers the characteristics of the lanes and the global shape information. This method can be used with most types of lane features to extract only those feature points corresponding to lane boundaries. In this paper, only the Sobel gradient and the steerable filter results are considered. The idea is to gradually zoom towards the vanishing point of the lanes on a single frame in order to overlap the lane boundaries. The feature points found on the zoomed images are compared with the original image feature points and the previously zoomed feature maps. Most of the irrelevant features can be removed after this process. The algorithm block diagram is shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Theoretical preliminaries</head><p>The purpose of the proposed algorithm is to find in the image those features corresponding solely to the lanes. Suppose a vehicle is driven on a clear straight road with continuous lane marking and maintaining a constant lateral offset from the lanes. It is easy to notice that the positions of lane markings, from the driver's point of view, do not change over short periods of time. Of course, from frame to frame, the lane markings are actually moving backwards as the vehicle moves forward, but since the colour and the width of the markings are similar, the driver does not feel this effect and perceives the lane markings as static objects. This algorithm takes advantage of the above phenomenon and tries to find the non-moving features from the scene. Additionally, the proposed algorithm does not require multiple consecutive frames. It operates based on a single image.</p><p>Similar to the driver's view, if digital zooming is applied towards the vanishing point of the lanes, the above property of the lane boundaries still holds. By carefully selecting a region of the image according to the vanishing point position and interpolating this region back to the original image size, most of the objects except the lane boundaries will be misaligned.</p><p>The first task is to select an appropriate area on the image. As the goal is to zoom towards the vanishing point, the position of the vanishing point must be determined. Also, after the interpolation, the vanishing point should stay at the same position on the image space. As illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, defining the position of the vanishing point, vp, as (vpm, vpn), the total number of rows as M, and the number of columns as N, the width and height of the selected area could be calculated as</p><formula xml:id="formula_5">saN ¼ z Â N<label>ð5Þ</label></formula><formula xml:id="formula_6">saM ¼ z Â M<label>ð6Þ</label></formula><p>where z is the zooming ratio and z 2 is the ratio between the area of the selected region and the original image area. The selection of the zooming area must follow the following rule:</p><formula xml:id="formula_7">vpn NÀvpn ¼ vpnÀsaL saL þsaNÀvpn<label>ð7Þ</label></formula><formula xml:id="formula_8">vpm MÀvpm ¼ vpmÀsaU saU þ saMÀvpm<label>ð8Þ</label></formula><p>where saL and saU are the position of the left and upper border of the selected area respectively. Subsequently, the selected area is interpolated back to the original size. This operation moves all points except the vanishing point to new positions, which are calculated as</p><formula xml:id="formula_9">n I ðt þ 1Þ ¼ vpnþðn I ðtÞÀvpnÞ Á 1 z<label>ð9Þ</label></formula><formula xml:id="formula_10">m I ðt þ1Þ ¼ vpm þðm I ðtÞÀvpmÞ Á 1 z<label>ð10Þ</label></formula><p>where n I (t), m I (t) and n I ðt þ1Þ, m I ðt þ 1Þ represent the column and row coordinates of point I before and after the interpolation respectively. A point on a straight lane boundary before interpolation needs to stay on the same line after interpolation. To prove this, we assume a straight line</p><formula xml:id="formula_11">m I ¼ an I þb<label>ð11Þ</label></formula><p>which passes through the vanishing point and consider I a point on the line. Substituting Eq. ( <ref type="formula" target="#formula_11">11</ref>) into Eq. ( <ref type="formula" target="#formula_10">10</ref>):</p><formula xml:id="formula_12">m I ðt þ1Þ ¼ a Á vpn þ b þða Á n I ðtÞþbÀa Á vpnÀbÞ Á 1 z<label>ð12Þ</label></formula><p>which could be rearranged to give</p><formula xml:id="formula_13">m I ðt þ1Þ ¼ a Á vpn þðn I ðtÞÀvpnÞ Á 1 z þb<label>ð13Þ</label></formula><p>Substitute Eq. ( <ref type="formula" target="#formula_9">9</ref>) into Eq. ( <ref type="formula" target="#formula_13">13</ref>), we get</p><formula xml:id="formula_14">m I ðt þ1Þ ¼ a Á n I ðt þ 1Þþb<label>ð14Þ</label></formula><p>Thus, Eq. ( <ref type="formula" target="#formula_14">14</ref>) proves that the points on the lane will stay on the same line after interpolation.</p><p>So far we have assumed straight lanes and continuous lane markings. However, a multiple vanishing points detection algorithm, along with the iterative zooming process readily solves the problem for the cases of curved and discontinuous lanes. This will be discussed in details in Sections 2.2.2 and 2.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Vanishing point detection</head><p>This constitutes the first step of the algorithm. Its location is vitally important for the rest of the task. Although a few pixels variation in the vanishing point position does not significantly influence the system performance, the detected vanishing point has to correspond to the lanes. Most of the vanishing point detection algorithms are based on the Hough transform <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. However, these methods require the choice of hard thresholds for both edge detection and the Hough space accumulator. It is very difficult to find a suitable set of thresholds for various tasks and environments. In this paper, we assume the road is flat and, therefore, the vanishing line position is calculated using the camera parameters. Hence, the detection of the vanishing point is reduced to a one-dimensional search. This is a suitable assumption since the lane model includes the flat road information as well. Even if the vanishing line is determined automatically, the pitch angle change of the road still cannot be represented.</p><p>First, the gradient map is generated by means of a Sobel edge detector. A very small threshold is applied to reduce the computation. The threshold in our case is between 20 and 40 from the non-normalised gradient which is small enough to locate lane features under various conditions. Assuming an edge point belongs to a lane, it is likely that the orientation of this edge is perpendicular to the gradient direction of the lanes. In this case, a line passing through this edge with the direction normal to its gradient orientation is generated to estimate the lanes. The intersection of this line and the vanishing line contributes to the estimation of the vanishing point.</p><p>A one-dimensional accumulator with a length equal to 2N (À 0.5N-1.5N) is created to account for the possibility of the vanishing point being outside the image. Each edge produces a line and each time the line intersects the vanishing line, the corresponding element in the accumulator increments by ðgm þ1Þ where gm is the normalised gradient magnitude. The accumulator is then smoothed with a Gaussian low pass filter to compensate for the inaccuracy of the edge orientation. The element with the most votes corresponds to the vanishing point position. The problem is that if the lane is a curve, the vanishing point position of the lane changes gradually with distance. If the zooming process is applied based on a static vanishing point, the lane boundaries could be misaligned in some areas of the image. In order to solve this problem, each frame is partitioned into a few horizontal sections as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The lane boundaries within each image section are treated as piecewise straight lines. This allows us to find the correct vanishing point positions for different image regions so that the zooming process for different image sections can also be separated. The area of each image section decreases linearly from the lowest image section to the highest section to accommodate the perspective effect.</p><p>The vanishing points for different image sections are detected only using the edge points in the current section. In the far-field, the number of lane edges is lower than that of the near field. This indicates the lane boundaries' edge points contribute less in the accumulation process. In this case, the search region of the upper section is based on the vanishing point position in the lower section and the vanishing point position difference between the two lower image sections. Finally, during lane tracking, the search region of the lowest section is constrained by the corresponding vanishing point position in the previous frame to reduce error and computation. An example of the multiple vanishing point detection results is given in Fig. <ref type="figure" target="#fig_3">3</ref>. The vanishing point corresponding to each image band is labelled respectively. In most of our experiments the image contents below the vanishing line are separated into three sections to avoid the existences of very narrow image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Feature extraction</head><p>The task is now to extract the lane features. In this paper, we illustrate the effect of the proposed algorithm operating on two different feature maps: the Sobel gradient map and the steerable filtered feature map proposed in <ref type="bibr" target="#b17">[18]</ref>. The steerable filter is a well-known method for lane feature detection. It is able to extract features with strong orientation. For the implementation of the steerable filter, only the line structure detector is built for comparison. The drawback of the steerable filter is it only extracts lane markings which is not suitable for unmarked roads.</p><p>Since the points on the lane will still stay on the same line after zooming and interpolation, the simplest idea is to apply the logical 'and' operator to the original image feature map and to the interpolated image feature map pixel by pixel. This means that if the interpolated features overlap with the original image features, these features are likely to belong to the lanes.</p><p>For the gradient map, the orientation of the overlapping features should be similar. The allowed direction difference is set to be between 0-p/2 rad in order to tolerate curves, edge detection errors and the orientation change caused by severe shadows. For the steerable filter feature map, the orientation information is already included during the filtering and thresholding. The comparison between the feature orientation is not included while applying our algorithm.</p><p>However, unwanted features still have the potential to overlay and have similar orientations due to the large amount of unwanted feature points that will be detected if a low threshold is used. In this case, an iterative zooming process is suggested. Based on experiments, 10 iterations of a gradually zooming process are normally sufficient to remove most of the noise even under very adverse conditions for the gradient map. The steerable filtered feature map contains less noise and thus fewer iterations are required. Our experiments show that three iterations of zooming are sufficient in most cases.</p><p>For the Sobel gradient map, since the edge orientation is needed at each iteration, we need to either interpolate the x and y directed edge maps separately or interpolate the whole image and apply edge detection at every iteration. The edge map is then compared with the original image edge map. Only the positions occupied by similarly orientated edges throughout the whole process are preserved. Specifically, if the orientation of Iðm,nÞ original ,Iðm,nÞ  Another possibility is to accumulate the overlapping edges and set a threshold to ensure the final edge positions are occupied most of the time.</p><p>Since no orientation comparison is needed for the steerable filtered results, the original feature map is interpolated at each iteration and then compared. The steerable filter requires a little more computational power compared with the edge detection. However, during the zooming stage the computation is saved. This also shows the adaptability of our algorithm. If the feature map contains a large amount of noise, more iterations of the zooming process can successfully remove most of the noise. If the feature map contains less noise, less iterations are needed.</p><p>During the interpolation stage, bilinear interpolation is chosen for its low complexity and satisfactory performance. The one-dimensional bilinear interpolation between two points ðm 0 ,n 0 Þ and ðm 1 ,n 1 Þ is given by</p><formula xml:id="formula_15">m ¼ m 0 þ ðnÀn 0 Þðm 1 Àm 0 Þ n 1 Àn 0<label>ð16Þ</label></formula><p>In the 2D cases, interpolation is first applied in the n-direction then in the m-direction.</p><p>The degree of zooming or the zooming ratio z is also an important parameter. It is unreasonable to select a very small zooming area. This introduces large image distortion and, also, causes the system to remove a large number of lane features if the lanes are discontinued. This will not be a problem if an adequately large zooming area is selected. It has been found by experiment that the minimum zooming ratio z should be above 90%. Consequently, a large zooming ratio is applied at each iteration (decremented linearly from 100% to 90%) and only a very small portion of each discontinuous lane marking will be erased during the process which will not introduce any problem during the parameter estimation and the tracking stage. This allows the algorithm to deal with segmented or dashed lines. Fig. <ref type="figure" target="#fig_4">4</ref> shows the intermediate stage results of the iterative zooming process. In order to illustrate the significant difference of applying or not applying the proposed algorithm, the feature map is plotted as a binary image. All feature points above the low edge threshold are shown as white.</p><p>To extend this algorithm to curved lanes, a lane is separated into piecewise straight lines as described in Section 2.2.2. The zooming process is applied to each section of the image according to the corresponding vanishing point. By doing so, the lane boundaries' edges will move in the correct direction in different image areas.</p><p>Finally, most of the unwanted features are removed and the remaining features are marked as '1's. In order to give each pixel a different weighting, the '1's are replaced by the corresponding magnitudes. Furthermore, a weighted sum of this result with the original feature map produces a new feature map with magnified lane features.</p><p>The proposed algorithm can be extended easily to work with non-flat roads if different vanishing line positions are allowed for each image section. However, this is not included in this paper since vanishing point voting with varying vanishing line provides less reliable results.</p><p>The experimental results of the proposed feature extraction approach and the comparison between the gradient based and the steerable filter based feature maps can be found in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Lane model parameter estimation</head><p>After feature extraction, the lane model parameters need to be optimised to produce an accurate representation of the lane boundaries. The lane detection results are then characterised by a few model parameters.</p><p>In our system, the parameter optimisation stage applies the Maximum A Posterior (MAP) estimation implemented in the LOIS system. This algorithm is implemented based on the feature map extracted using our proposed approach and the Sobel operator. The steerable filter feature map is not included during the parameter optimisation and tracking stage as the feature orientations extracted using the steerable filter do not accurately represent the orientations of the lane boundaries.</p><p>In order to construct a likelihood function that utilises all the properties of the gradient map, the characteristics of the gradient map need to be analysed. Three types of information can be extracted from the gradient map: pixel positions, gradient magnitudes and the gradient orientations. The parameters to be estimated are denoted by an array x where x ¼ ½a,b 1 ,b 2 ,c. In order to simplify the likelihood function, we use LM l ¼ 1 ðx,mÞ and LM l ¼ 2 ðx,mÞ to represent the left and right lane curves respectively. The likelihood function is defined as</p><formula xml:id="formula_16">LðxÞp X 2 l ¼ 1 X m,n</formula><p>Fðm,nÞf ða prox ,nÀLM l ðx,mÞÞ</p><formula xml:id="formula_17">f ða orient ,cosð F ðm,nÞÀ L M l ðx,mÞÞÞ<label>ð17Þ</label></formula><p>where Fðm,nÞ represents the gradient magnitude at pixel (m,n) on the feature map. LM l ðx,mÞ denotes the column number of the sample point on the estimated curve on image row m. F and L M denote the orientations of the image gradient and the lane curve gradient. f is a function, defined as</p><formula xml:id="formula_18">f ða,xÞ ¼ 1 1 þ a Á x 2<label>ð18Þ</label></formula><p>a prox and a orient are the two parameters controlling the degrading speed of function f. In our experiments, a prox ¼ 0:01 and a orient ¼ 1:13. Although Eq. ( <ref type="formula" target="#formula_18">18</ref>) does not degrade as fast as exponential functions, it creates less distinct local maxima. Eq. ( <ref type="formula" target="#formula_17">17</ref>) incorporates all three types of gradient map information by multiplying them together. In order to maximise the likelihood function, the estimated curves need to cross or stay close to a large number of edge points with high magnitudes. Also, the orientations of the close pixels' gradients need to be approximately perpendicular to the orientations of the estimated curve gradients on the same image row.</p><p>p prior ðxÞ, shown in Eq. ( <ref type="formula" target="#formula_19">19</ref>), is the prior probability density function which adds tunable constraints to the estimation. With this prior constraint, the width of the lane cannot be too narrow and LM 1 must be on the left side of LM 2 :</p><formula xml:id="formula_19">p prior ðxÞp 2 p Â arctanða width Â ðb r Àb l ÞÞ<label>ð19Þ</label></formula><p>The final posterior probability density p posterior is found by combining the normalised Eq. ( <ref type="formula" target="#formula_17">17</ref>) and Eq. ( <ref type="formula" target="#formula_19">19</ref>):</p><formula xml:id="formula_20">p posterior ¼ p prior ðxÞ Â LðxÞ ð<label>20Þ</label></formula><p>This posterior density function is maximised using the Metropolis algorithm <ref type="bibr" target="#b30">[31]</ref>. This is an iterative optimisation algorithm with the certain ability of escaping from the local maxima. However, since the global maximum is not guaranteed, the estimation could be inaccurate or even completely fail. The algorithms that find the global maximum such as simulated annealing require unacceptably high computational power which would introduce difficulties for real-time implementation.</p><p>The reason why the Metropolis algorithm is preferred in this system is that our proposed feature extraction algorithm removes most unwanted features. This directly reduces the number of local maxima in the posterior probability distribution and consequently the Metropolis algorithm is more likely to find the global maximum. The complete process normally requires above 400 iterations to converge depending on the image and the initialisation.</p><p>The proposed feature map includes less feature points and also boosts the speed of the optimisation process significantly. Some of the parameter estimation results based on different feature maps and the analysis of the results can be found in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Lane tracking</head><p>After the lanes are successfully detected, the lane model is modified to include another parameter to model the spatial gradient of the curvature. The lane model is defined as LM l ðx,m f Þ where x ¼ ½a,b 1 ,b 2 ,c,d:</p><formula xml:id="formula_21">LM 1 ðx,mÞ ¼ a mÀvpm þ d ðmÀvpmÞ 2 þ b 1 ðmÀvpmÞþc LM 2 ðx,mÞ ¼ a mÀvpm þ d ðmÀvpmÞ 2 þ b 2 ðmÀvpmÞþc<label>ð21Þ</label></formula><p>where parameters a, b 1 , b 2 and c are identical to the lane model parameters in the detection stage. The parameter d is included to represent the spatial derivative of the lane curvature. Without this parameter, the description would be highly inaccurate when the vehicle is entering or getting out of an accentuated curve. This may severely influence the tracking performance or even result in losing track of the lane. By applying the tracking algorithms, the unknown parameters and their probability densities can be estimated according to the previous parameters and the measurements (the measurements in our case are the image gradient magnitude and orientation). The likelihood function is constructed based on the feature map obtained by the proposed feature extraction algorithm.</p><p>The problem of non-linear filtering can be stated as follows: defining x kÀ1 as the vector containing the values of the lane model parameters at time kÀ1, we want to estimate x k at time k. The change of parameter values in time is given as</p><formula xml:id="formula_22">x k ¼ gðx kÀ1 ,w kÀ1 Þ ð<label>22Þ</label></formula><p>The (possibly known) non-linear time update function g is applied to update the parameter array from time k to k þ 1. w k is the process noise which may be non-Gaussian. The measurement function h, shown in Eq. ( <ref type="formula" target="#formula_23">23</ref>), defines the relationship between the parameter vector and the measurements:</p><formula xml:id="formula_23">z k ¼ hðx k ,v k Þ ð<label>23Þ</label></formula><p>where z k denotes the measurement matrix at time k and v k is the measurement noise with possibly non-Gaussian distribution.</p><p>Our tracking algorithm is based on particle filters. Due to the existence of various forms of particle filters, the performance of three different types of particle filters were evaluated: the sampling importance resampling (SIR) algorithm, the Gaussian particle filter (GPF) and the Gaussian sum particle filter (GSPF).</p><p>The GPF is based on a Gaussian pdf assumption. The mean and covariance of the weighted particles are calculated at each time instance to describe the Gaussian posterior density. By doing so, the time consuming resampling step is avoided since the particles can be easily drawn from a Gaussian density. The entire GPF process can thus be completed with highly paralleled implementations.</p><p>In the lane tracking application, although the posterior density of parameters is normally non-Gaussian, it does not mean using a Gaussian density as an approximation is inappropriate. If the feature map contains too many unwanted feature points, the Gaussian density is normally not a suitable assumption <ref type="bibr" target="#b31">[32]</ref>. However, after applying our proposed feature extraction algorithm, most of the unwanted features are already removed. The effect of outliers is minimised and the GPF can be a suitable choice. The complete GPF algorithm and mathematical derivation of GPF can be found in <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, there is still another possibility to avoid resampling without the Gaussian assumption. The GSPF models the noise and the posterior density with a mixture of Gaussian densities. One of the most important properties of the Gaussian mixture model is that the posterior density is aggregated with a group of individual Gaussian densities. This provides more flexibility during the parameter selection process. By utilising this property to fit the requirement of this particular application, the accuracy of the final estimations of the parameter values surpasses the ones provided by the traditional SIR and GPF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Time update</head><p>In a typical computer vision-based lane tracking system, the actual time update equation is unknown. We incorporate a random Gaussian noise to follow the parameter variations. During the time update stage, particles are drawn from the previous posterior density and propagated through the time update. In our experiments, same time update equations are used for all three algorithms shown in Eq. ( <ref type="formula" target="#formula_24">24</ref>):</p><formula xml:id="formula_24">x i k ¼ x i kÀ1 þ w i kÀ1<label>ð24Þ</label></formula><p>where</p><formula xml:id="formula_25">w i kÀ1 $ Nðm,Q kÀ1 Þ Q kÀ1 ¼ s 2 a 0 0 0 0 0 s 2 b1 0 0 0 0 0 s 2 b 2 0 0 0 0 0 s 2 c 0 0 0 0 0 s 2 d 0 B B B B B B B @ 1 C C C C C C C A</formula><p>where w i kÀ1 $ Nðm,Q kÀ1 Þ is the process noise added in the time update equation to follow changes of the lanes and prevent sample impoverishment. For the SIR and PF, m ¼ 0. For the GSPF, each Gaussian component represents a change in lane shape or position. Five Gaussian components are used, corresponding to a small change of lane shape (m ¼ 0), fast positive or negative change of lane curvature (m a ¼ 7 100) and the spatial change of curvature (m d ¼ 7 100) respectively. Q kÀ1 denotes the covariance matrix of the Gaussian noise. In order to cope with rapid changes in lane parameters and avoid sample impoverishment, we have chosen s a ¼ 100, s b 1 ¼ 0:17, s b 2 ¼ 0:17, s c ¼ 4 and s d ¼ 100 for both the PF and GPF and s a ¼ 60, s b 1 ¼ 0:17, s b 2 ¼ 0:17, s c ¼ 4 and s d ¼ 60 for the GSPF. The variance of a and d for the GSPF is smaller due to the fact that the additive noise has non-zero mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Measurement update</head><p>The measurement update stage evaluates the weightings of the importance samples and finally produces an estimation of the posterior probability density of the lane model parameters x ¼ ½a,b 1 ,b 2 ,c,d. In this paper, the likelihood function chosen for tracking is different from the one for detection but is similar to the one used in <ref type="bibr" target="#b9">[10]</ref> and is also improved specifically for the lane tracking application.</p><p>As mentioned in Section 2.3, the gradient map contains three most important types of information: pixel positions, gradient magnitudes and gradient orientations. The likelihood function needs to utilise this information and produces a high likelihood when the curve is fitted to the correct feature points. During the parameter estimation stage, all pixels are included in the likelihood calculation since little prior information is available and the initialisation of the algorithm could be inaccurate. During the tracking stage, previous detection or tracking results can be utilised so that most of the feature points that are far away from the estimated curve can be excluded from the likelihood calculation. Furthermore, a fast degrading function is needed so that the parameters which correspond to the mean value of the posterior density function can be used as the final estimation. For these reasons, the likelihood function is constructed as an exponential function, and for each lane boundary, only one feature point from each image row is included in the calculation. The selection of this point is based on evaluating the distance and the orientation differences between the curve sample point and the feature points along with the gradient magnitudes. The feature point which produces the minimum distance on an image row is selected.</p><p>Similar to the detection stage, the normalised gradient magnitudes of the edge points are represented as Fðm,nÞ. R defines the width of the search region centred at LM l ðx,mÞ. The n-axis coordinate of any feature point found inside the search region is denoted as nðiÞ where i 2 ð1,T m Þ, and T m denotes the total number of feature points on the image line m. The likelihood function is shown in Eq. ( <ref type="formula" target="#formula_26">25</ref>):</p><formula xml:id="formula_26">p image ðz9xÞp exp À X 2 l ¼ 1 X M m ¼ 1 min Tm i ¼ 1 " LM l ðx,mÞÀnðiÞ R þ a9cosð L M l ðx,mÞÀ F ðm,nðiÞÞÞ9 þ bð1ÀFðm,nðiÞÞÞ # =M Á S !<label>ð25Þ</label></formula><p>where M denotes the number of image rows under the vanishing line. S is a coefficient controlling the weight difference of particles. In all our experiments, S was set to 10. L Mðx,mÞ and F ðm,nðiÞÞ denote the orientation of the lane curve gradient and the orientation of the feature point Fðm,nðiÞÞ, respectively. The cos function is used here since the image gradient is perpendicular to the edge lines. b is set to 0.25 which indicates the gradient magnitude contributes much less to the overall likelihood.</p><p>If there is no feature point on an image line m, the contribution of that image line to the likelihood is suppressed to the minimum value by setting:</p><formula xml:id="formula_27">min Tm i ¼ 1 " LM l ðx,mÞÀnðiÞ R þ a9cosð L M l ðx,mÞÀ F ðm,nðiÞÞÞ9 þbð1ÀFðm,nðiÞÞÞ ¼ R R þ aþb ¼ 1 þa þ b;<label>ð26Þ</label></formula><p>The result of Eq. ( <ref type="formula" target="#formula_26">25</ref>) is then combined with the prior probability distribution, shown in Eq. ( <ref type="formula" target="#formula_19">19</ref>), to constrain the lane width and results in the posterior probability distribution as shown in Eq. ( <ref type="formula" target="#formula_28">27</ref>):</p><formula xml:id="formula_28">pðz9xÞpp image ðz9xÞ Â p prior<label>ð27Þ</label></formula><p>where p prior is defined in Eq. ( <ref type="formula" target="#formula_19">19</ref>) and pðz9xÞ is normalised so that it integrates to 1.</p><p>The main difference of this likelihood function to the one presented in <ref type="bibr" target="#b9">[10]</ref> is summarised as follows. The likelihood function in <ref type="bibr" target="#b9">[10]</ref> only considers the distance between the feature points to the model sample point when selecting the matched feature points. In our system, the feature point chosen for each image line is selected based on the combination of edges to lane model sample points distances, edge and curve gradient orientation differences and also the gradient magnitudes. By including more information during the edge point selection process, the possibility of ignoring useful feature points is minimised. Furthermore, the exponential factor is averaged with the number of image lines and then multiplied by a scaling factor S instead of summation. The summation results in a very large exponential factor which, in turn, makes the weighting difference unacceptably large. If only very few particles have dominating weights, the 'spiky' posterior pdf lacks the ability to accurately represent the probability of each particle. Finally, the likelihood value should be independent of the image size. Therefore, choosing the average is much more appropriate.</p><p>The main reason for choosing a local likelihood function is to minimise the effect of the outliers and reduce the computational complexity. By including only those feature points around the sampled lines, the effect of noise is minimised.</p><p>In implementing the SIR, the effective sample size is used to decide if resampling of the particles is required. For the GSPF, after the posterior probability density becomes available for the first frame, the Expectation Maximisation (EM) <ref type="bibr" target="#b32">[33]</ref> algorithm is used to initialise the Gaussian components for the GSPF. For the following frames, the Gaussian components weights are updated using the sum of weights of the samples drawn from the corresponding components to multiply with the previous Gaussian weights. The minimum weight of a Gaussian component is limited to avoid the situations where the weights of few components are considerably higher than the rest.</p><p>It is important to note that the output of the particle filters is a probability density of the parameters represented by samples, not the values of the final estimation.</p><p>In order to generate the final estimation, this probability density needs to be carefully analysed according to the characteristics of different applications. However, since it is almost impossible in the lane tracking case to obtain a reasonable characterisation of the surrounding noise, the final multivariate probability density is complex to analyse. That is why most of the existing lane tracking systems choose the mean as the final parameter estimation, which is not always suitable especially in complicated environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter selection</head><p>In this section, we propose an intuitive parameter selection process based on the GSPF. The mean values of the Gaussian component with the highest weight is considered to be the final estimation result. Based on experiments, the variance of each Gaussian component is much lower than the variance of the posterior density obtained with the SIR or the GPF. This indicates the GSPF divides the influence of the feature points into different groups. Each group of feature points contributes to the likelihood values of some closely related particles. For example, if a group of feature points corresponds to the lane boundaries, they will contribute to the parameter values described with a single Gaussian component. Similarly, if a group of feature points correspond to some outliers, they will contribute to the parameter values described with another Gaussian component. As a result, if the Gaussian with the highest weight corresponds to the lane boundaries, then the mean of this Gaussian constitutes an accurate estimator.</p><p>The final output of the PF and GPF is chosen to be the mean of the posterior probability density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Lane feature extraction results</head><p>The feature extraction performance of our proposed algorithm is compared with the original Sobel gradient map and the steerable filter approach <ref type="bibr" target="#b17">[18]</ref>. The experiments show that the proposed algorithm is able to remove most of the unwanted features based on various traditional feature maps if the features can be represented in the image space. There are other types of feature extraction algorithms and not all methods are presented here.</p><p>While choosing the minimum zooming ratio z in our experiments, it has been found experimentally that its value should be above 90%. Consequently, a large zooming ratio is applied at each iteration (decremented linearly from 100% to 90%). By doing so, only a very small portion of each discontinuous lane marking will be erased during the process and this will not introduce any problem during the parameter estimation and the tracking stage. In all our experiments, z is set to be 0.92. Modifying this value from 0.9 to 0.93 does not induce a large difference in the feature extraction results. Value of 0.92 is chosen based on subjective tests. Values above 0.93 will influence the results since the noise removal ability of the process is reduced and in complicated environments, high noise removal ability is required. Some example feature extraction results are shown in Fig. <ref type="figure" target="#fig_6">5</ref>. As the figure illustrates, the proposed feature extraction approach improves feature extraction results significantly. In complicated situations (as shown in the first column in Fig. <ref type="figure" target="#fig_6">5</ref>) the traditional feature maps include numerous unwanted feature points (as shown in the second and fourth column in Fig. <ref type="figure" target="#fig_6">5</ref>. Column three shows the results achieved using the proposed feature extraction approach based on the gradient map. Numerous unwanted feature points are removed after 15 iterations. With the improved feature map, the performance of the parameter estimation algorithm and the tracking algorithm will be subsequently improved as well. Only 15 iterations are used because very little difference is observed with a higher number of iterations.</p><p>Column four shows the lane feature extraction results achieved by applying the steerable filters. Comparing these results with the gradient based feature map, significant improvement can be observed since less unwanted features are included and the magnitudes of some of the unwanted feature points are generally smaller. However, the steerable filter results can still be improved by removing the outliers. After the results are refined with the proposed approach, the feature map is much cleaner after only 7 iterations (chosen based on experiments) and the final feature points can be used to represent the road boundaries more adequately (as shown in column 5 in Fig. <ref type="figure" target="#fig_6">5</ref>). As mentioned in Section 2.2.3, less number of iterations of refinement are needed to achieve satisfactory results for better original feature maps. In practical situations, the number of iterations can be minimised according to the chosen original feature extraction algorithm and the driving environment.</p><p>The chosen system parameters are applied to various road images and video sequences. In our experiments, the proposed lane feature extraction approach achieved high performance in our subjective tests.</p><p>Finally, it is interesting to see that the refined feature maps based on both edge detection and steerable filter become similar. Originally, the extensive performance differences can be observed between the two algorithms (as shown in columns 2 and 4 in Fig. <ref type="figure" target="#fig_6">5</ref>). After refinement, the results based on the steerable filter are only marginally better than that based on edge detection. This demonstrates the capability of the proposed approach to improve various feature maps and produce high quality output.</p><p>The proposed feature extraction approach based on gradient maps was implemented on the TMS320DM6437 DSP platform from Texas Instruments to achieve real-time performance. The system is able to achieve above 23 frames per second (fps) with a 240 Â 360 video input. The frame rate can be further increased by code optimisation <ref type="bibr" target="#b33">[34]</ref>. More details about the implementation of the algorithm can be found in <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Lane model parameter estimation results</head><p>This section presents the experimental results of the model parameter estimation process. The main purpose of this experiment is to show that after applying the proposed feature extraction approach, the curve fitting results can be improved. It is worth noting that only the image gradient map is chosen here for comparison since the original likelihood function of the parameter estimation algorithm proposed in <ref type="bibr" target="#b0">[1]</ref> is based on the gradient map. Also, the feature orientations extracted with the steerable filter are not accurate and cannot be used as data source for the curve fitting.</p><p>The test images included in this section are chosen from the most difficult scenes in several video sequences. In Fig. <ref type="figure" target="#fig_8">6</ref>(a) and (d), both scenes are affected heavily by shadows and a diverging lane scene is included in Fig. <ref type="figure" target="#fig_8">6</ref>(g). The corresponding gradient maps are shown in Fig. <ref type="figure" target="#fig_8">6</ref>(b), (e) and (h). All of these gradient maps contain a large number of unwanted feature points. Fig. <ref type="figure" target="#fig_8">6</ref>(c), (f) and (i) shows the feature map obtained using the proposed algorithm. Most of the unwanted features are removed. Compared with gradient maps, the proposed feature maps are less noisy and the lane features are well preserved.</p><p>The detection of the lanes is based on the Metropolis algorithm, which does not guarantee to find the global maximum since the update of the parameters is based on a random selection process. In this case, the detection result varies even if the same feature map is processed for multiple times. Therefore, the feature map of the input images shown in Fig. <ref type="figure" target="#fig_8">6</ref>  </p><p>The pre-set variances of all the parameters are shown in Section 2.3. Table <ref type="table">1</ref> shows the ER value corresponding to different parameters calculated from Fig. <ref type="figure" target="#fig_8">6</ref>(a), (d) and (g) as well as the detection time ratio T P =T G .</p><p>It is important to note that large curve fitting errors can be observed by subjective testing because of errors in only few parameters values. If the far-field of the scene is noisy, then parameter estimation results may only contain large errors in a, since the change of a (curvature) has little influence on the near-field curve shape. Similarly, if only the estimations of b 1 and b 2 are not accurate, the largest difference between the estimated curve and the true road boundaries should lie in the near-field on the image plane. The accuracy of parameter c influences the accuracy of all other parameters. As illustrated in the second and the third column in Table <ref type="table">1</ref>, remarkable curve fitting performance improvement is achieved by applying the proposed feature extraction approach. However, for Fig. <ref type="figure" target="#fig_8">6</ref>(g) (fourth column in Table <ref type="table">1</ref>), improvements is mainly in the estimations of b 1 and b 2 where estimation errors of b 1 and b 2 are only 41% and 19%    of the ones based on the traditional gradient map. This improvement is very significant in terms of near-field lane position estimation. The values of ER(a) and ER(c) for Fig. <ref type="figure" target="#fig_8">6</ref>(g) are very close to 1. This indicates that the estimations of a and c based on both feature maps are quite accurate and there is little room for improvement. In terms of performance, Table <ref type="table">1</ref> shows that the detection results based on the proposed feature map have been significantly improved over the results based on the traditional gradient map. Finally, the parameter estimation time based on the proposed feature map is also massively reduced because fewer feature points are included during curve fitting. This also compensates for the extra computational complexity that the proposed feature extraction approach introduces. Since the feature extraction processes normally need only a fraction of the computational power used by the detection algorithms, on a system level, the overall processing time is reduced by applying the proposed feature extraction approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Lane tracking results</head><p>In our experiments, the tracking results were obtained based on several lane video sequences. These videos contain different road scenes, including severe shadows, heavy curvatures, sudden curvature changes and the presence of other vehicles. These videos are mainly recorded on motorways and include some degree of camera shaking. All the experimental results included in this section are based on two selected video sequences, both containing discontinuous lane markings. Video a (selected frames are shown in Fig. <ref type="figure" target="#fig_12">8</ref>) includes scenes with heavy shadows, other vehicles and offroad objects. Video b (selected frames are shown in Fig. <ref type="figure" target="#fig_13">9</ref>) is not as noisy as video a but shows rapid curvature changes when the vehicle exits a section of a heavily curved road.</p><p>The Tracking Quality (TQ) and the variance of the unknown parameters are measured in order to compare and evaluate the performance of the three types of particle filters, which are the PF, GPF and GSPF. TQ is defined as the inverse of the Mean-Squared Error (MSE) between the sample points' positions on the estimated curves and the manually chosen ground truth (as shown in Eq. ( <ref type="formula" target="#formula_30">29</ref>)). The ground truth is generated by marking a few key points (ðK n ,K m Þ where K n and K m are arrays storing the x-and y-coordinates of these points respectively) on the left and right lane boundaries. Furthermore, the comparisons between the tracking performance based on the traditional gradient map and our proposed feature map are included.</p><p>Throughout the experiments, 500 particles are used since no significant performance enhancement was observed once the particle size reaches 500. A correct detection result is used for initialising the particle filters and the covariance of the added noise is the same for all particle filters. Furthermore, reinitialisation is switched off during our TQ measurement so that large errors are allowed to ensure a thorough evaluation of the algorithms.</p><p>The TQ for each frame is defined as</p><formula xml:id="formula_30">TQ ¼ 1=E½ðLMðx,K m ÞÀK n Þ 2 ð<label>29Þ</label></formula><p>Eq. <ref type="bibr" target="#b28">(29)</ref> shows that the TQ is closely related to the distance between the samples on the predicted lane and the ground truth. The TQ measurement of video a is smoothed and illustrated in Fig. <ref type="figure" target="#fig_11">7</ref>. As Fig. <ref type="figure" target="#fig_11">7</ref>(a) shows, all three types of filters are able to correctly track the lanes in complicated environments, based on the proposed feature extraction algorithm. For the complete video sequence, the averaged TQ is calculated and shown in Table <ref type="table" target="#tab_2">2</ref>. On average, the GSPF achieved the highest performance which means the final model parameters produce lines closest to the ground truth. This significant observation proves that the GSPF, combined with the proposed parameter selection approach, is the most suitable algorithm for lane tracking applications. Not only does it achieve better tracking results, but it can also be easily implemented in parallel.</p><p>The variances of the lane model parameters are also recorded after each frame as another indicator of the tracking performance. In order to illustrate the variance difference of different parameters, the parameter variances are averaged over all frames as shown in Table <ref type="table" target="#tab_3">3</ref>. Since the final estimation of the GSPF is the mean of the highest weighted Gaussian, the listed averaged variances are the variances of the chosen Gaussian components.   For the PF and GPF, the variances of the parameter posterior probability densities are given. In the P row (tracking based on the proposed feature map) for both videos, the GSPF achieved the minimum variance for most of the parameters among the three. The variance of the GPF estimation is also smaller than that of the tradition PF. This observation is very important since it proves that estimating the posterior probability density with a Gaussian or a Gaussian mixture is accurate. This also means that if our proposed feature map is used for measurements, the GSPF and the GPF are two viable alternatives to the SIR particle filter in lane tracking applications and offer better performance. The variance of the parameter d when using GSPF is surprisingly high, based on both the proposed feature map and the Sobel feature map. This is caused mainly by our time update strategy for the GSPF which adds non-zero mean noise to accommodate sudden curvature changes (detailed in Section 3.1). Next, the tracking results based on the proposed feature map are compared with those based on the traditional edge map. The edge threshold set for the traditional edge map and the proposed feature map are identical. As illustrated in Fig. <ref type="figure" target="#fig_11">7</ref>(a) and (b), the tracking performance improvements for all three types of tracking algorithms are significant, due to most of the outliers being removed. The traditional SIR lost track of the right lane boundary from approximately frame 1000-1600 which results in very low TQ values. Note that the SIR is able to successfully track the lanes based on our proposed feature map. The tracking performance improvement achieved using the proposed feature map is more significant than that achieved using a better tracking algorithm. The TQ achieved using the GSPF based on the proposed algorithm is 2.7 times the TQ achieved using SIR based on the traditional feature map. Another important point is that the TQ of the GPF is the highest among the three, although only marginally higher than the TQ of GSPF. This, again, proves that GPF and GSPF are more suitable than the SIR in lane tracking applications.</p><p>The averaged variance of each parameter is compared based on different feature maps. The results are shown in Table <ref type="table" target="#tab_3">3</ref>. From this table, it can be seen that the variances of the estimated parameters based on the proposed feature maps are significantly lower than those based on the Sobel gradient map in video a. The fact that the effect is more significant in video a is due to a large amount of noise that was successfully removed by using the proposed feature extraction algorithm. In video b, the noise level is much lower than in video a so the variance differences of the parameters are not as distinct. In our subjective tests, when tracking is based on the proposed feature map, particle filters are more sensitive to curvature changes. Also, due to less noisy edge pixels being included, the effect caused by outliers is minimised. This leads to more accurate tracking results. Finally, if the previous estimation is inaccurate, the particle filters recover much faster based on the proposed feature map.</p><p>In our experiments, a thorough comparison between three types of particles filters is carried out by evaluating the tracking quality (TQ) and the variance of the posterior density. Furthermore, the tracking performance based on the feature maps generated using the proposed feature extraction approach and the original feature maps are compared. During the experiments, all the system parameters are chosen so as to achieve the optimum tracking performance.</p><p>To summarise the analysis of the tracking results, three main points can be made. First of all, our experiments proved that the GSPF with the proposed parameter selection process is the best tracking scheme for this application. Secondly, the GPF is a viable alternative to the SIR particle filter which minimises the computational cost and achieves higher performance. Finally, the proposed feature extraction approach generates feature maps with minimum noise which allows the performance of all three tracking algorithms to be improved dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, a complete lane detection and tracking system is presented. One of the significant contribution lies in the proposed lane feature extraction algorithm. It is able to improve most of the existing feature maps by removing the irrelevant feature points produced by unwanted objects in the scene. The detection rate, accuracy, and processing speed are significantly improved after excluding the unwanted feature points. The feature extraction algorithm is also implemented on a DSP platform for real-time application. When investigating the original PF, the GPF, and the GPSF for lane tracking based on our proposed likelihood function, the GPF and the GSPF are found to be more efficient alternatives to the SIR particle filter. Both GSPF and the GPF achieve more accurate tracking results and do not require the resampling stage, which cannot be implemented in parallel. The GSPF achieves the best performance with a new parameter selection process. Finally, the tracking performance is also significantly improved by incorporating our proposed feature map instead of the traditional gradient map. More derivatives of particle filters will be exploited in our future work including the recently proposed alphastable particle filter <ref type="bibr" target="#b35">[36]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/sigpro Signal Processing 0165-1684/$ -see front matter &amp; 2011 Elsevier B.V. All rights reserved. doi:10.1016/j.sigpro.2011.07.019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the proposed system.</figDesc><graphic coords="4,324.52,56.57,144.00,354.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Selected zooming area of an image.</figDesc><graphic coords="5,75.45,56.57,144.00,104.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Vanishing point detection result. The image is horizontally partitioned and each partition is labelled. The white horizontal line is the horizon or the vanishing line. The vanishing point of each region is labelled respectively.</figDesc><graphic coords="6,117.95,56.57,312.12,182.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Iterative results for noise removal with the proposed algorithm. (a) Edge results, (b) 1st iteration, (c) 3rd iteration, (d) 5th iteration, (e) 7th iteration and (f) 10th iteration.</figDesc><graphic coords="7,102.04,532.51,336.24,135.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a and b are weighting factors controlling the importance of distance, orientation difference, and the gradient magnitude to the overall likelihood. In our experiments, a is set to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Lane feature extraction results. The first column shows the original images. The second column shows the image gradient maps extracted using the Sobel operator. The third column shows the proposed algorithm results based on the gradient maps. The fourth column shows the feature maps extracted using the steerable filter. The fifth column shows the refined results based on feature maps obtained using the steerable filter.</figDesc><graphic coords="11,41.96,56.57,456.12,267.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>has been processed repeatedly (200 times in our experiments), with 600 iterations each time. The parameter values are then averaged for comparison purposes. The parameter settings during the detection stage are identical for both feature maps. The averaged parameters: a, b 1 , b 2 and c from Eqs. (3) and (4) are compared with the manually selected true parameters. The average absolute error for each of the parameters is calculated. As the required accuracies and dynamic ranges of a, b 1 , b 2 and c are different (suggested dynamic ranges of the parameters can be found in Section 2.1), a comparison between the detection results based on different feature maps would be more appropriate. Defining the parameter estimation error based on the proposed feature map as EP(s) and the parameter estimation error based on the gradient map as EG(s), the relationship between EP(s) and EG(s) could be represented as ERðsÞ ¼ EPðsÞ EGðsÞ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Lane model parameter estimation results. (a), (d) and (g): Input images excluding the parts above the vanishing lines. (b), (e) and (h): Detection results of (a), (d) and (g) respectively based on image gradient. (c), (f) and (i): Detection results of (a), (d) and (g) respectively based on feature maps generated by the proposed algorithm.</figDesc><graphic coords="12,106.02,56.57,336.24,183.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Table 1 ER values corresponding to a, b 1 , b 2 and c and the time ratio T P =T G calculated from Fig. 6(a), (d), and (g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6(a) Fig. 6(d) Fig. 6(g)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Smoothed tracking quality (TQ) measurements for video a based on different feature maps. (a) TQ measurements based on the proposed feature map. (b) TQ measurements based on the Sobel gradient map.</figDesc><graphic coords="13,113.96,211.16,312.12,456.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Lane tracking results based on video a. Three frames are chosen for illustration. The first row shows the tracking results based on the proposed feature maps. The second row shows the results based on traditional gradient maps.</figDesc><graphic coords="14,106.07,56.57,335.88,84.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Lane tracking results based on video b. Three frames are chosen for illustration. The first row shows the tracking results based on the proposed feature maps. The second row shows the results based on traditional gradient maps.</figDesc><graphic coords="14,105.07,182.10,338.04,115.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Comparison of average TQ based on the Sobel gradient map (S) and the proposed feature map (P).</figDesc><table><row><cell></cell><cell>TQ</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SIR</cell><cell>GPF</cell><cell>GSPF</cell></row><row><cell>P</cell><cell>0.0256</cell><cell>0.0351</cell><cell>0.0392</cell></row><row><cell>S</cell><cell>0.0141</cell><cell>0.0264</cell><cell>0.0258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Comparison of lane model parameters averaged variances between the three particle filters and based on the different feature maps. P represents the proposed feature map and S denotes the Sobel gradient map.</figDesc><table><row><cell></cell><cell>PF</cell><cell>GPF</cell><cell>GSPF</cell></row><row><cell>Video a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a</cell><cell>19,649</cell><cell>11,334</cell><cell>8973</cell></row><row><cell>b 1</cell><cell>0.004</cell><cell>0.003</cell><cell>0.001</cell></row><row><cell>b 2</cell><cell>0.010</cell><cell>0.008</cell><cell>0.002</cell></row><row><cell>c</cell><cell>31.18</cell><cell>20.81</cell><cell>13.39</cell></row><row><cell>d</cell><cell>72,385</cell><cell>48,194</cell><cell>62,937</cell></row><row><cell>S</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a</cell><cell>27,331</cell><cell>17,709</cell><cell>25,470</cell></row><row><cell>b 1</cell><cell>0.005</cell><cell>0.004</cell><cell>0.002</cell></row><row><cell>b 2</cell><cell>0.015</cell><cell>0.008</cell><cell>0.003</cell></row><row><cell>c</cell><cell>36.34</cell><cell>27.71</cell><cell>24.20</cell></row><row><cell>d</cell><cell>81,681</cell><cell>98,187</cell><cell>297,862</cell></row><row><cell>Video b</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a</cell><cell>20,960</cell><cell>14,640</cell><cell>12,727</cell></row><row><cell>b 1</cell><cell>0.0058</cell><cell>0.0044</cell><cell>0.004</cell></row><row><cell>b 2</cell><cell>0.0052</cell><cell>0.01</cell><cell>0.004</cell></row><row><cell>c</cell><cell>33.47</cell><cell>22.49</cell><cell>29.14</cell></row><row><cell>d</cell><cell>201,820</cell><cell>116,060</cell><cell>98,094</cell></row><row><cell>S</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a</cell><cell>22,700</cell><cell>15,870</cell><cell>31,374</cell></row><row><cell>b 1</cell><cell>0.0056</cell><cell>0.0039</cell><cell>0.0097</cell></row><row><cell>b 2</cell><cell>0.0051</cell><cell>0.0035</cell><cell>0.0133</cell></row><row><cell>c</cell><cell>35.37</cell><cell>20.77</cell><cell>45.93</cell></row><row><cell>d</cell><cell>174,420</cell><cell>109,820</cell><cell>127,100</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Y.Wang  et al. / Signal Processing 92 (2012) 319-334</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b0">1</ref> <p>A. Achim was supported in part by the European Science Foundation through the COST Action TU0702.</p><p>Signal Processing 92 (2012) 319-334</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Supplementary materials</head><p>Supplementary data related to this article can be found online at doi:10.1016/j.sigpro.2011.07.019.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A deformable-template approach to lane detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kluge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles &apos;95 Symposium</title>
		<meeting>the Intelligent Vehicles &apos;95 Symposium</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A robust lane detection and tracking method based on computer vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1088/0957-0233/17/4/020</idno>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="736" to="745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Springrobot: a prototype autonomous vehicle and its algorithms for lane detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="300" to="308" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lane detection using spline model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-8655(00)00021-0</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lane detection and tracking using B-snake</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding and tracking road lanes using linesnakes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Intelligent Vehicle</title>
		<meeting>Conference on Intelligent Vehicle</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture-based segmentation of road images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles &apos;94 Symposium</title>
		<meeting>the Intelligent Vehicles &apos;94 Symposium</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="260" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A lane-curve detection based on an lcf</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Jhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2301" to="2313" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate road following and reconstruction by computer vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chapuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aufrere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chausse</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2002.804751</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust road modeling and tracking using condensation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="570" to="579" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the design of a single lanemarkings detectors regardless the on-board camera&apos;s position</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="564" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color-based road detection in urban traffic scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RALPH: rapidly adapting lateral position handler</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Intelligent Vehicles</title>
		<meeting>the IEEE Symposium on Intelligent Vehicles</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="506" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lane detection with moving vehicles in the traffic scenes, Intelligent Transportation Systems</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2006.883940</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="582" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LANA: a lane extraction algorithm that uses frequency domain features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kreucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="343" to="350" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An integrated</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intelligent Vehicles Symposium</title>
		<meeting>IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="533" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video-based lane estimation and tracking for driver assistance: Survey, system, and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three-feature based automatic lane detection algorithm (TFALDA) for autonomous driving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems, Proceedings</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="929" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of road marking feature extraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International IEEE Conference on Intelligent Transportation Systemsdd ITSC</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lane detection and Kalman-based linear-parabolic lane tracking, in: Intelligent Human-Machine Systems and Cybernetics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chin</surname></persName>
		</author>
		<idno type="DOI">10.1109/IHMSC.2009.211</idno>
	</analytic>
	<monogr>
		<title level="m">IHMSC &apos;09. International Conference on</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="351" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust vision based lane tracking using multiple cues and particle filtering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Apostoloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2003.1212973</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="558" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient lane detection and tracking in urban environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sehestedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alempijevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Mobile Robots</title>
		<meeting>the European Conference on Mobile Robots</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<title level="m">Sequential Monte Carlo Methods in Practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gaussian particle filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kotecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2592" to="2601" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaussian sum particle filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kotecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2602" to="2612" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parametric model of the perspective projection of a road with applications to lane keeping and 3d road reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guiducci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="414" to="427" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Road lane segmentation using dynamic programming for active safety vehicles</title>
		<author>
			<persName><forename type="first">D.-J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3177" to="3185" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance evaluation and analysis of vanishing point detection techniques, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shufelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="282" to="288" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lane detection using spline model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Metropolis algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beichl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="65" to="69" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Particle filtering with alpha-stable distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brasnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2005 IEEE Workshop on Statistical Signal Processing (SSP&apos;05)</title>
		<meeting>2005 IEEE Workshop on Statistical Signal Processing (SSP&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="381" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Dahnoun</surname></persName>
		</author>
		<title level="m">Digital Signal Processing Implementation: Using the TMS320C6000 Processors</title>
		<imprint>
			<publisher>Prentice-Hall PTR</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel lane feature extraction algorithm implemented on the TMS320DM6437 DSP platform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dahnoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSP2009: Proceedings of the 16th International Conference on Digital Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="733" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Particle filtering with alpha-stable distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brasnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/SP 13th Workshop on Statistical signal Processing (SSP)</title>
		<meeting><address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">2005. July 2005, 2005</date>
			<biblScope unit="page">346</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
