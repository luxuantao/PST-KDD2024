<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Root Mean Square Layer Normalization</title>
				<funder ref="#_3CqprTg">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Baidu Scholarship</orgName>
				</funder>
				<funder ref="#_P9GgdV8">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<email>b.zhang@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<email>sennrich@cl.uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Root Mean Square Layer Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%?64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How to train deep neural networks efficiently is a long-standing challenge. To accelerate model convergence, Ba et al. <ref type="bibr" target="#b2">[3]</ref> propose the layer normalization (LayerNorm) which stabilizes the training of deep neural networks by regularizing neuron dynamics within one layer via mean and variance statistics. Due to its simplicity and requiring no dependencies among training cases, LayerNorm has been widely applied to different neural architectures, which enables remarkable success on various tasks ranging from computer vision <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>, speech recognition <ref type="bibr" target="#b36">[37]</ref> to natural language processing <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>. In some cases, LayerNorm was found to be essential for successfully training a model <ref type="bibr" target="#b5">[6]</ref>. Besides, the decoupling from batch-based samples endows LayerNorm with the superiority over batch normalization (BatchNorm) <ref type="bibr" target="#b11">[12]</ref> in handling variable-length sequences using RNNs.</p><p>Unfortunately, the incorporation of LayerNorm raises computational overhead. Although this is negligible to small and shallow neural models with few normalization layers, this problem becomes severe when underlying networks grow larger and deeper. As a result, the efficiency gain from faster and more stable training (in terms of number of training steps) is counter-balanced by an increased computational cost per training step, which diminishes the net efficiency, as show in Figure <ref type="figure" target="#fig_2">1</ref>. One major feature of LayerNorm that is widely regarded as contributions to the stabilization is its re-centering invariance property: the summed inputs after LayerNorm remain intact when the inputs or weight matrix is shifted by some amount of noise. We argue that this mean normalization does not reduce the variance of hidden states or model gradients, and hypothesize that it has little impact on the success of LayerNorm.</p><p>In this paper, we propose root mean square layer normalization (RMSNorm), which regularizes the summed inputs to a neuron in one layer with the root mean square (RMS) statistic alone.   RMSNorm reduces the amount of computation and increases efficiency over LayerNorm. Despite the simpler formulation, the RMS normalizer helps stabilize the magnitude of layer activations, ensuring invariance to the re-scaling of both weights and datasets. We also show the possibility of estimating RMS on a subset of the summed inputs, maintaining this invariance property. Assuming that the summed inputs have an independent identically distributed structure, we propose partial RMSNorm, where only the first p% summed inputs are utilized for RMS estimation.</p><p>We thoroughly examine our model on various tasks, including machine translation, image classification, image-caption retrieval and question answering. Experimental results show that across different models, RMSNorm yields comparable performance against LayerNorm but shows superiority in terms of running speed with a speed-up of 7%?64%. When estimating the RMS with partial (6.25%) summed inputs, pRMSNorm achieves competitive performance compared to RMSNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One bottleneck deep neural networks have been hypothesized to suffer from is the internal covariate shift issue <ref type="bibr" target="#b26">[27]</ref>, where a layer's input distribution changes as previous layers are updated, which significantly slows the training. <ref type="foot" target="#foot_0">1</ref> One promising direction to solve this problem is normalization. Ioffe and Szegedy <ref type="bibr" target="#b11">[12]</ref> introduce batch normalization (BatchNorm) to stabilize activations based on mean and variance statistics estimated from each training mini-batch. Unfortunately, the reliance across training cases deprives BatchNorm of the capability in handling variable-length sequences, though several researchers develop different strategies to enable it in RNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>. Instead, Salimans and Kingma <ref type="bibr" target="#b21">[22]</ref> propose weight normalization (WeightNorm) to reparameterize weight matrix so as to decouple the length of weight vectors from their directions. Ba et al. <ref type="bibr" target="#b2">[3]</ref> propose layer normalization which differs from BatchNorm in that statistics are directly estimated from the same layer without accessing other training cases. Due to its simplicity and effectiveness, LayerNorm has been successfully applied to various deep neural models, and achieves state-of-the-art performance on different tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>These studies pioneer the research direction that integrates normalization as a part of the model architecture. This paradigm ensures encouraging performance by shortening model convergence but at the cost of consuming more time for each running step. To improve efficiency, Arpit et al. <ref type="bibr" target="#b1">[2]</ref> employ a data-independent method to approximately estimate mean and variance statistics, thus avoiding calculating batch statistics. Ioffe <ref type="bibr" target="#b10">[11]</ref> propose batch renormalization so as to reduce the dependence of mini-batches in BatchNorm. Ulyanov et al. <ref type="bibr" target="#b29">[30]</ref> replace batch normalization with instance normalization for image generation. Hoffer et al. <ref type="bibr" target="#b9">[10]</ref> and Wu et al. <ref type="bibr" target="#b32">[33]</ref> observe that l1-norm can act as an alternative of variance in BatchNorm with the benefit of fewer nonlinear operations and higher computational efficiency. Nevertheless, all these work still follow the original normalization structure and utilize mean statistic estimated from the whole summed inputs to handle re-centering invariance.</p><p>Different from these related work, the proposed RMSNorm modifies the normalization structure by removing the re-centering operation and regularizing the summed inputs with RMS alone. Our model only maintains the re-scaling invariance property which we find can be inherited when the RMS is estimated from only subset of the summed inputs, partially inspired by the group normalization <ref type="bibr" target="#b33">[34]</ref>.</p><p>As a side effect, our model reduces the computational overhead and increases efficiency. Recently, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> show that with careful initialization, residual networks can be trained as stable as those with normalization. However, the approach mainly aims at improving residual networks and can not be freely switched without modifying all initialization layers. Besides, it is not trivial to be adapted to other general neural networks, such as RNNs where model depth expands along the variable sequence length. By contrast, our model is simple, effective and can be used as a drop-in replacement of LayerNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>We briefly review LayerNorm in this section based on a standard feed-forward neural network. Given an input vector x ? R m , a feed-forward network projects it into an output vector y ? R n through a linear transformation followed by a non-linear activation as follows:</p><formula xml:id="formula_0">a i = m j=1 w ij x j , y i = f (a i + b i ) ,<label>(1)</label></formula><p>where w i is weight vector to the i-th output neuron, b i is bias scalar which is usually initialized by 0, and f (?) is an element-wise non-linear function. a ? R n denotes the weight-summed inputs to neurons, which is also the target of normalization.</p><p>This vanilla network might suffer from internal covariate shift issue <ref type="bibr" target="#b11">[12]</ref>, where a layer's input distribution changes as previous layers are updated. This could negatively affect the stability of parameters' gradients, delaying model convergence. To reduce this shift, LayerNorm normalizes the summed inputs so as to fix their mean and variance as follows:</p><formula xml:id="formula_1">?i = a i -? ? g i , y i = f (? i + b i ) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ?i is the i-th value of vector ? ? R n , which acts as the normalized alternative of a i for layer activation. g ? R n is the gain parameter used to re-scale the standardized summed inputs, and is set to 1 at the beginning. ? and ? 2 are the mean and variance statistic respectively estimated from raw summed inputs a:</p><formula xml:id="formula_3">? = 1 n n i=1 a i , ? = 1 n n i=1 (a i -?) 2 .<label>(3)</label></formula><p>Thus, LayerNorm forces the norm of neurons to be decoupled from the inputs and weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RMSNorm</head><p>A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance property. The former enables the model to be insensitive to shift noises on both inputs and weights, and the latter keeps the output representations intact when both inputs and weights are randomly scaled. In this paper, we hypothesize that the re-scaling invariance is the reason for success of LayerNorm, rather than re-centering invariance.</p><p>We propose RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic:</p><formula xml:id="formula_4">?i = a i RMS(a) g i , where RMS(a) = 1 n n i=1 a 2 i .<label>(4)</label></formula><p>Intuitively, RMSNorm simplifies LayerNorm by totally removing the mean statistic in Eq. ( <ref type="formula" target="#formula_3">3</ref>) at the cost of sacrificing the invariance that mean normalization affords. When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNorm. Although RMSNorm does not re-center the summed inputs as in LayerNorm, we demonstrate through experiments that this property is not fundamental to the success of LayerNorm, and that RMSNorm is similarly or more effective.</p><p>RMS measures the quadratic mean of inputs, which in RMSNorm forces the summed inputs into a ? n-scaled unit sphere. By doing so, the output distribution remains regardless of the scaling of input and weight distributions, benefiting the stability of layer activations. Although Euclidean norm which only differs from RMS by a factor of ? n has been successfully explored <ref type="bibr" target="#b21">[22]</ref>, we empirically find that it does not work for layer normalization. We hypothesize that scaling the sphere with the size of the input vector is important because it makes the normalization more robust across vectors of different size. As far as we know, the idea of employing RMS for neural network normalization has not been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Invariance Analysis</head><p>Invariance measures whether model output after normalization changes highly in accordance with its input and weight matrix. Ba et al. <ref type="bibr" target="#b2">[3]</ref> show that different normalization methods reveal different invariance properties, which contributes considerably to the model's robustness. In this section, we theoretically examine the invariance properties of RMSNorm.</p><p>We consider the following general form of RMSNorm:</p><formula xml:id="formula_5">y = f Wx RMS(a) g + b ,<label>(5)</label></formula><p>where denotes element-wise multiplication. Our main results are summarized in Table <ref type="table" target="#tab_0">1</ref>. RMS-Norm is invariant to both weight matrix and input re-scaling, because of the following linearity property of RMS:</p><formula xml:id="formula_6">RMS(?x) = ?RMS(x),<label>(6)</label></formula><p>where ? is a scale value. Suppose the weight matrix is scaled by a factor of ?, i.e. W = ?W, then this change does not affect the final layer output:</p><formula xml:id="formula_7">y = f W x RMS(a ) g + b = f ?Wx ?RMS(a) g + b = y.<label>(7)</label></formula><p>By contrast, if the scaling is only performed on individual weight vectors, this property does not hold anymore as different scaling factors break the linearity property of RMS. Similarly, if we enforce a scale on the input with a factor of ?, i.e. x = ?x, the output of RMSNorm remains through an analysis analogous to that in Eq. 7. We can easily extend the equality to batch-based inputs as well as the whole dataset. Therefore, RMSNorm is invariant to the scaling of its inputs.</p><p>The main difference to LayerNorm is that RMSNorm is not re-centered and thus does not show similar linearity property for variable shifting. It is not invariant to all re-centering operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gradient Analysis</head><p>The above analysis only considers the effect of scaling inputs and the weight matrix on the layer output. In a general setting, however, a RMSNorm-enhanced neural network is trained via standard stochastic gradient descent approach, where the robustness of model gradient is very crucial to parameters' update and model convergence (see also Santurkar et al. <ref type="bibr" target="#b22">[23]</ref> who argue that the success of normalization methods does not come from the added stability to layer inputs, but due to increased smoothness of the optimization landscape). In this section, we investigate the properties of model gradients in RMSNorm.</p><p>Given a loss function L, we perform back-propagation through Eq. ( <ref type="formula" target="#formula_4">4</ref>) to obtain the gradient with respect to parameters g, b as follows:</p><formula xml:id="formula_8">?L ?b = ?L ?v , ?L ?g = ?L ?v Wx RMS(a) ,<label>(8)</label></formula><p>where v is short for the whole expression inside f (?) in Eq. ( <ref type="formula" target="#formula_4">4</ref>), and ?L /?v is the gradient backpropagated from L to v. Both gradients ?L /?b and ?L /?g are invariant to the scaling of inputs x and the weight matrix W (in the case of ?L /?g because of the linearity property in Eq. ( <ref type="formula" target="#formula_6">6</ref>)). Besides, the gradient of g is proportional to the normalized summed inputs, rather than raw inputs. This powers the stability of the magnitude of g.</p><p>Unlike these vector parameters, the gradient of the weight matrix W is more complicated due to the quadratic computation in RMS. Formally,</p><formula xml:id="formula_9">?L ?W = n i=1 x T ? diag g ?L ?v ? R i , where R = 1 RMS(a) I - (Wx) (Wx) T nRMS(a) 2 ,<label>(9)</label></formula><p>diag(?) denotes the diagonal matrix of input, ? denotes the Kronecker product, and "I" indicates identity matrix. For clarity, we explicitly use "?" to represent matrix multiplication. The matrix term R associates the gradient of W with both inputs x and weight matrix W. With a thorough analysis, we can demonstrate that this term is negatively correlated with both input and weight matrix scaling.</p><p>After assigning a scale of ? to either input x (x = ?x) or weight matrix (W = ?W), we have</p><formula xml:id="formula_10">R = 1 ?RMS(a) I - (?Wx) (?Wx) T n? 2 RMS(a) 2 = 1 ? R. (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>If we put the scaled term R back into Eq. ( <ref type="formula" target="#formula_9">9</ref>), we can easily prove that the gradient ?L /?W is invariant to input scaling, but keeps the negative correlation with weight matrix scaling. Reducing the sensitivity of gradient ?L /?W to the scaling of inputs ensures its smoothness and improves the stability of learning. On the other hand, the negative correlation acts as an implicit learning rate adaptor and dynamically controls the norm of gradients which avoids large-norm weight matrix and improves model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">pRMSNorm</head><p>The re-scaling invariance property of RMSNorm ascribes to the linearity property of RMS. Considering that neurons in one layer often have independent identically distributed structure, we argue that the RMS can be estimated on a subset of these neurons rather than all of them. We propose partial RMSNorm (pRMSNorm). Given the unnormalized input a, pRMSNorm infers the RMS statistic from first-p% elements of a:</p><formula xml:id="formula_12">RMS(a) = 1 k k i=1 a 2 i ,</formula><p>where k = n ? p denotes the number of elements used for RMS estimation. The linearity property still holds for RMS as in Eq. ( <ref type="formula" target="#formula_6">6</ref>), which indicates pRMSNorm shares the same invariance properties as RMSNorm as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>RMS is a biased estimation of the RMS which is often inaccurate. Though theoretically pRMSNorm approximates to RMSNorm, we observe gradient instability where the gradient tends to explode with small m. In practice, however, models with pRMSNorm can succeed in satisfactory convergence with a partial ratio of 6.25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>To test the efficiency of layer normalization across different implementations, we perform experiments with Tensorflow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b19">[20]</ref> and Theano <ref type="bibr" target="#b28">[29]</ref>. We add RMSNorm to different models, comparing against an unnormalized baseline and LayerNorm. These models are based on diverse architectures, covering different RNN variants, convolutional and self-attentional models, and various activations (such as sigmoid, tanh, and softmax), with initialization ranging from uniform, normal, orthogonal with different initialization ranges or variances. Unless otherwise noted, all speed-related statistics are measured on one TITAN X (Pascal). Reported time is averaged over 3 runs. We also list the standard deviation of these three runs.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Machine Translation</head><p>Machine translation aims at transforming a sentence from one (source) language to another (target) language. We focus on neural machine translation based on an attention-enhanced encoder-decoder framework. We train two different models, a GRU-based RNNSearch <ref type="bibr" target="#b3">[4]</ref> and a self-attention based neural Transformer <ref type="bibr" target="#b30">[31]</ref> on WMT14 English-German translation task. More details about the experimental settings as well as comparison with WeightNorm are listed in Appendix A.1</p><p>We first experiment with RNNSearch. Normalization is added to the recurrent connections and feedforward layers. Apart from RNNSearch without any normalization (Baseline) and with Layer-Norm, we also compare against the same model equipped with L2-Norm (i.e. replacing RMS with L2-Norm), which has been observed to improve lexical selection <ref type="bibr" target="#b17">[18]</ref>.</p><p>Figure <ref type="figure" target="#fig_4">2</ref> illustrates the evolution of BLEU score on our development set after every 30k training steps, and Table <ref type="table">2</ref> summarizes the test results. In short, both LayerNorm and RMSNorm outperform the Baseline by accelerating model convergence: they reduce the number of training steps until convergence by about 50%, and improve test accuracy, with RMSNorm being comparable to LayerNorm. This supports our hypothesis that re-scaling invariance is the core property of LayerNorm, and that RMSNorm is an effective substitute. Our results with L2-Norm show that it fails to improve the model. <ref type="foot" target="#foot_1">2</ref> Results in Table <ref type="table">2</ref> highlight the challenge that RNN with LayerNorm in Tensorflow suffers from serious computational inefficiency, where LayerNorm is slower than the Baseline by about 67%. In this respect, RMSNorm performs significantly better, improving upon LayerNorm by ?25%.</p><p>Table <ref type="table">3</ref> further lists translation results of different models implemented in Theano and Pytorch. Overall, RMSNorm yields comparable translation quality compared with LayerNorm but incurs less computational overhead, outperforming LayerNorm with speedups ranging from 11%?34%. In addition, we observe that though in theory the amount of computation in pRMSNorm is less than that in RMSNorm, pRMSNorm (p = 6.25%) sometimes tends to be slower. We ascribe this to the non-optimal implementation of tensor slicing operation in these computational frameworks, which can be improved with specific low-level coding.</p><p>In pRMSNorm, the partial ratio p directly controls the accuracy of estimated RMS, thereby affecting the stability of model training. Figure <ref type="figure" target="#fig_6">3</ref> shows the effect of p on model performance. Surprisingly, we find that the scale of p has little influence on the final translation quality in RNNSearch: using a small ratio does not significantly degenerate BLEU score. We set p to 6.25% for all following experiments.</p><p>We also experiment with Transformer, which is based on self-attention, avoiding recurrent connections and allowing a higher degree of parallelization. Still, layer normalization is an important part of the architecture. We use an in-house Tensorflow implementation of the Transformer, and employ the base setting as in <ref type="bibr" target="#b30">[31]</ref> with all models trained for 300K steps. We treat Transformer with no normalization as our Baseline, and compare RMSNorm-enhanced Transformer with LayerNorm-equipped Transformer.      normalization is lower because there are significantly fewer sequential normalization operations in Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Normalization on Mean and Standard Deviation</head><p>Table <ref type="table" target="#tab_5">5</ref> shows the distribution of mean and standard deviation of hidden representations across token positions for an RNNSearch model. Mean and standard deviation are unstable in the baseline, as observed by Ba et al. <ref type="bibr" target="#b2">[3]</ref>. Due to their normalization properties, both RMSNorm and LayerNorm stabilize standard deviation. Although the mean in RMSNorm is not normalized, in practice it is more stable than the mean of the baseline. This supports our hypothesis that RMSNorm stabilizes recurrent activations without the need to explicitly normalize the mean. On the Robustness of RMSNorm One remaining question is whether the re-centering operation in LayerNorm (which RMSNorm abandons) makes models more robust towards arbitrary weight/bias initializations. We perform an experiment on RNNSearch with Nematus in Tensorflow, and change the center of weight initialization to 0.2. Results in Figure <ref type="figure">4</ref> show that LayerNorm becomes very unstable with abnormal initialization, but RMSNorm is more robust (both underperform the original initialization).</p><p>Our empirical evidence so far suggests that RMSNorm is similarly robust as LayerNorm, or more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CNN/Daily Mail Reading Comprehension</head><p>This reading comprehension task is a cloze-style question answering task, where models are required to answer a question regarding to a passage, and the answer is an anonymized entity from the passage <ref type="bibr" target="#b8">[9]</ref>. We train a bidirectional attentive reader model proposed by Hermann et al. <ref type="bibr" target="#b8">[9]</ref> on the CNN corpus. More details about the experimental settings are given in Appendix A.2. We compare RMSNorm with both LayerNorm and BatchNorm.</p><p>Figure <ref type="figure">5</ref> and (c) Recall@10 Figure <ref type="figure">6</ref>: Recall@K values on validation set for the order-embedding models.</p><p>worse than RMSNorm. Although in Figure <ref type="figure">5</ref> the performance of RMSNorm and LayerNorm is comparable, RMSNorm is around 15% faster than LayerNorm as shown in Table <ref type="table" target="#tab_6">6</ref>. <ref type="foot" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Image-Caption Retrieval</head><p>Image-caption retrieval is a cross-modal task aiming at learning a joint embedding space of images and sentences, which consists of two sub-tasks: image retrieval and caption retrieval. The former ranks a set of images according to a query caption, and the latter ranks a set of captions based on a query image. We train an order-embedding model (OE) proposed by Vendrov et al. <ref type="bibr" target="#b31">[32]</ref> on the Microsoft COCO dataset <ref type="bibr" target="#b16">[17]</ref> using their public source code in Theano. Model details about experimental settings are provides in Appendix A.3. We compare RMSNorm with two models: one without any normalization (Baseline) and one with LayerNorm. Figure <ref type="figure">6</ref> shows the R@K curve on validation set after every 300 training steps, and Table <ref type="table" target="#tab_9">7</ref> lists the final test results. Across all these metrics, RMSNorm and LayerNorm consistently outperform the Baseline in terms of model convergence as shown in Figure <ref type="figure">6</ref>. We observe that on the validation set, RMSNorm slightly exceeds LayerNorm with respect to recall value. For the final test results as shown in Table <ref type="table" target="#tab_9">7</ref>, both RMSNorm and LayerNorm improve the model performance, reaching higher recall values (except LayerNorm on R@5) and lower mean rank, though RMSNorm reveals better generalization than LayerNorm. Besides, results in Table <ref type="table" target="#tab_7">8</ref> show that RMSNorm accelerates training speed by 40%?64% compared with LayerNorm, highlighting better efficiency of pRMSNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">CIFAR-10 Classification</head><p>CIFAR-10 is a supervised image classification task, with 10 different classes. We train a modified version of the ConvPool-CNN-C architecture <ref type="bibr" target="#b14">[15]</ref>, and follow the same experimental protocol as Salimans and Kingma <ref type="bibr" target="#b21">[22]</ref>. BatchNorm, LayerNorm, and WeightNorm are included for comparison.</p><p>Training details are given in Appendix A.4.</p><p>Figure <ref type="figure">9</ref> and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper presents RMSNorm, a novel normalization approach that normalizes the summed inputs according to the RMS. RMSNorm preserves the re-scaling invariance property of LayerNorm but eschews the re-centering invariance property which contributes less to the model training. Compared with LayerNorm, models with RMSNorm suffers from less computational overhead. RMSNorm can be easily applied to different model architectures as a drop-in replacement of LayerNorm. Experiments on several NLP tasks show that RMSNorm is comparable to LayerNorm in quality, but accelerates the running speed. Actual speed improvements depend on the framework, hardware, neural network architecture and relative computational cost of other components, and we empirically observed speedups of 7%?64% across different models and implementations. Our efficiency improvement come from simplifying the computation, and we thus expect them to be orthogonal to other means of increasing training speed, such as low-precision arithmetic and GPU kernel fusion. We also experimented with pRMSNorm which estimates the RMS on a subset of the summed inputs. While theoretically faster, we did not consistently observe empirical speed improvements for pRMSNorm.</p><p>We leave it to future work to investigate if the performance can be improved via code optimization.</p><p>In the future, we would like to take more analysis about the success behind RMSNorm. Inspired by recent success of l1-norm for BatchNorm, we will explore different norms for RMSNorm, and simplify other normalization techniques such as BatchNorm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>33rd</head><label></label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training loss vs. training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training procedure of a GRU-based RNNSearch [4] for the first 10k training steps. Baseline means the original model without any normalization. When the Baseline training loss arrives at 7.0, the loss of LayerNorm reaches 5.4 after the same number of training steps 1(a), but only 5.9 after the same training time 1(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SacreBLEU score on newstest2013 for the RNNSearch. Models are implemented according to Nematus [25] in Tensorflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SacreBLEU score on new-stest2013 (devset) for the RNNSearch with pRMSNorm. We use Tensorflow-version Nematus, and change p by a step size of 10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Figure SacreBLEU score curve of Layer-Norm and RMSNorm on newstest2013 (devset) when the initialization center is 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Invariance properties of different normalization methods. "" indicates invariant, while "" denotes the opposite.</figDesc><table><row><cell>Weight matrix</cell><cell>Weight matrix</cell><cell>Weight vector</cell><cell>Dataset</cell><cell>Dataset</cell><cell>Single training case</cell></row><row><cell>re-scaling</cell><cell>re-centering</cell><cell>re-scaling</cell><cell>re-scaling</cell><cell>re-centering</cell><cell>re-scaling</cell></row><row><cell>BatchNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WeightNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LayerNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMSNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pRMSNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 4 shows the results, from which we observe the importance of normalization for Transformer, without which training fails. RMSNorm achieves BLEU scores comparable to LayerNorm, and yields a speedup of 7%?9%. Compared with RNNSearch, the relative cost of</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>ALL</cell></row><row><cell></cell><cell>Baseline</cell><cell>M S</cell><cell>-2.60 7.35</cell><cell>-1.19 2.33</cell><cell>-1.43 2.61</cell><cell>-1.53 2.73</cell><cell>-1.60 3.04</cell></row><row><cell></cell><cell>LayerNorm</cell><cell>M S</cell><cell>-0.43 1.19</cell><cell>-0.48 1.51</cell><cell>-0.50 1.51</cell><cell>-0.50 1.51</cell><cell>-0.51 1.51</cell></row><row><cell>: SacreBLEU score on newstest2014</cell><cell>RMSNorm</cell><cell>M S</cell><cell>-0.40 1.27</cell><cell>-0.60 1.51</cell><cell>-0.69 1.50</cell><cell>-0.74 1.49</cell><cell>-0.73 1.50</cell></row><row><cell>(Test14) and newstest2017 (Test17) for the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer. "Time": the time in second per</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1k training steps, which is measured using Tesla</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V100. "-" indicates that we fail to train this</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model and BLEU score is 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean (M) and standard deviation (S) statistics estimated on the hidden-to-hidden mapping of decoder-part GRU cell in RNNSearch model. We use the newstest2013 dataset. ALL: the statistics averaged across all token positions. Numbers 1,2,3,4 indicate the statistic estimated for specific token positions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Table 6 show the results. After normalizing RNN by BatchNorm with separate statistics for each time step in a sequence, both BatchNorm-LSTM and BatchNorm-Everywhere help speed up the convergence of training process. By contrast, LayerNorm and RMSNorm not only converge faster than BatchNorm, but also reach lower validation error rate, though pRMSNorm performs slightly Time in seconds per 0.1k training steps for the attentive reader model.</figDesc><table><row><cell></cell><cell cols="2">1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Valid error rate</cell><cell cols="2">0.5 0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell cols="4">BatchNorm-Everywhere BatchNorm-LSTM LayerNorm RMSNorm pRMSNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Model BatchNorm-Everywhere 348?10.5s Time Baseline 315?6.30s BatchNorm-LSTM 345?11.2s LayerNorm 392?5.70s RMSNorm 333?5.20s (15.1%)</cell></row><row><cell></cell><cell cols="2">0.4</cell><cell>0</cell><cell cols="7">50 100 150 200 250 300</cell><cell></cell><cell></cell><cell></cell><cell cols="3">pRMSNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>330?5.50s (15.8%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Training steps (x1k)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Figure 5: Error rate on validation set for the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">attentive reader model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell></row><row><cell>Mean Recall@1</cell><cell>36 38 40 42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Baseline LayerNorm RMSNorm pRMSNorm</cell><cell>Mean Recall@5</cell><cell>72 73 74 75 76 77</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Baseline LayerNorm RMSNorm pRMSNorm</cell><cell>Mean Recall@10</cell><cell>85 86 87 88 89</cell><cell></cell><cell></cell><cell>Baseline LayerNorm RMSNorm pRMSNorm</cell></row><row><cell></cell><cell>34</cell><cell>0</cell><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell></cell><cell>71</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell></cell><cell>84</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Training steps (x0.3k)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Training steps (x0.3k)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Training steps (x0.3k)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Recall@1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Recall@5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Time in seconds per 0.1k training steps for the order-embedding model.</figDesc><table><row><cell>Model</cell><cell>Time</cell></row><row><cell>Baseline</cell><cell>2.11?0.047s</cell></row><row><cell>LayerNorm</cell><cell>12.02?0.191s</cell></row><row><cell>RMSNorm</cell><cell>7.12?0.207s (40.8%)</cell></row><row><cell>pRMSNorm</cell><cell>4.34?0.168s (63.9%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell>Model</cell><cell>R@1</cell><cell cols="2">Caption Retrieval R@5 R@10</cell><cell>Mean r</cell><cell>R@1</cell><cell cols="2">Image Retrieval R@5 R@10</cell><cell>Mean r</cell></row><row><cell></cell><cell>Sym [32]</cell><cell>45.4</cell><cell></cell><cell>88.7</cell><cell>5.8</cell><cell>36.3</cell><cell></cell><cell>85.8</cell><cell>9.0</cell></row><row><cell>Existing</cell><cell>OE + Baseline [32]  ?</cell><cell>46.7</cell><cell></cell><cell>88.9</cell><cell>5.7</cell><cell>37.9</cell><cell></cell><cell>85.9</cell><cell>8.1</cell></row><row><cell>Work</cell><cell>OE + Baseline [3]  ?</cell><cell>46.6</cell><cell>79.3</cell><cell>89.1</cell><cell>5.2</cell><cell>37.8</cell><cell>73.6</cell><cell>85.7</cell><cell>7.9</cell></row><row><cell></cell><cell>OE + LayerNorm [3]</cell><cell>48.5</cell><cell>80.6</cell><cell>89.8</cell><cell>5.1</cell><cell>38.9</cell><cell>74.3</cell><cell>86.3</cell><cell>7.6</cell></row><row><cell></cell><cell>OE + Baseline</cell><cell>45.8</cell><cell>79.7</cell><cell>88.8</cell><cell>5.4</cell><cell>37.6</cell><cell>73.6</cell><cell>85.8</cell><cell>7.7</cell></row><row><cell>This</cell><cell>OE + LayerNorm</cell><cell>47.9</cell><cell>79.5</cell><cell>89.2</cell><cell>5.3</cell><cell>38.4</cell><cell>74.6</cell><cell>86.7</cell><cell>7.5</cell></row><row><cell>Work</cell><cell>OE + RMSNorm</cell><cell>48.7</cell><cell>79.7</cell><cell>89.5</cell><cell>5.3</cell><cell>39.0</cell><cell>74.8</cell><cell>86.3</cell><cell>7.5</cell></row><row><cell></cell><cell>OE + pRMSNorm</cell><cell>46.8</cell><cell>79.8</cell><cell>90.3</cell><cell>5.2</cell><cell>39.0</cell><cell>74.5</cell><cell>86.3</cell><cell>7.4</cell></row></table><note><p><p><p>show the results. Models enhanced with a normalization technique converge faster than Baseline, among which BatchNorm performs the best. Similar to previous observation</p><ref type="bibr" target="#b2">[3]</ref></p>,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Average R@K values across 5 test sets from Microsoft COCO. R@K: Recall @ K, higher is better. Mean r: mean rank, lower is better. The number in bold highlights the best result. ? denotes the reproduced results of ? .</figDesc><table><row><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Baseline</cell></row><row><cell>Error Rate</cell><cell>0.02 0.04 0.06</cell><cell></cell><cell></cell><cell></cell><cell>BatchNorm LayerNorm WeightNorm RMSNorm pRMSNorm</cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training epochs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Training error rate for the ConvPool-CNN-C model.</figDesc><table><row><cell>Model</cell><cell cols="2">Test Error Time</cell></row><row><cell>Baseline</cell><cell>8.96%</cell><cell>21?0.0s</cell></row><row><cell>BatchNorm</cell><cell>8.25%</cell><cell>38?0.0s</cell></row><row><cell>WeightNorm</cell><cell>8.28%</cell><cell>23?0.0s</cell></row><row><cell>LayerNorm</cell><cell>10.49%</cell><cell>39?0.4s</cell></row><row><cell>RMSNorm</cell><cell>8.83%</cell><cell>31?0.5s (20.5%)</cell></row><row><cell>pRMSNorm</cell><cell>10.37%</cell><cell>30?0.4s (23.1%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Test error rate and time in seconds per training epoch for the ConvPool-CNN-C model. Time is measured with GeForce RTX 2080 Ti.we also find that layer normalization works worse than BatchNorm and WeightNorm for image processing. Though LayerNorm outperforms Baseline by shorting model convergence, it fails to generalize to the test set, degenerating the test error by 1.53%. In contrast, RMSNorm shows better generalization, surpassing the Baseline by 0.013% and saving about 20.5% training time compared to LayerNorm. pRMSNorm gains further speedup of 2.6%, albeit at the cost of sacrificing test accuracy of 1.54%.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the internal covariate shift is given as motivation by<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>. Recent studies have proposed alternative explanations for the success of normalization, such as the uncontrollable growth of layer activations in unnormalized deep networks<ref type="bibr" target="#b4">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We note that Nguyen and Chiang<ref type="bibr" target="#b17">[18]</ref> only applied L2-Norm to the last layer, and treat the scaling factor as a hyperparameter. While not a replication of their experiment, we still found it worth testing L2-Norm as an alternative to LayerNorm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Notice that the implementation of BatchNorm is cuDNN-based, so time cost of BatchNorm in Table6can not be directly compared with others.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the reviewers for their insightful comments, and <rs type="person">Antonio Valerio Miceli Barone</rs> for his support with weight normalization for MT. This project has received funding from the grant <rs type="grantNumber">H2020-ICT-2018-2-825460</rs> (ELITR) by the <rs type="funder">European Union</rs>. <rs type="person">Biao Zhang</rs> also acknowledges the support of the <rs type="funder">Baidu Scholarship</rs>. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the <rs type="institution">University of Cambridge Research Computing Service</rs> (http://www.hpc.cam.ac.uk) funded by <rs type="funder">EPSRC</rs> Tier-2 capital grant <rs type="grantNumber">EP/P020259/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3CqprTg">
					<idno type="grant-number">H2020-ICT-2018-2-825460</idno>
				</org>
				<org type="funding" xml:id="_P9GgdV8">
					<idno type="grant-number">EP/P020259/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargava</forename><forename type="middle">U</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venu</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01431</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding batch normalization</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7996-understanding-batch-normalization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7694" to="7705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?sar</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?aglar</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<title level="m">Recurrent batch normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01814</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batchnormalized models</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1945" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>CoRR, abs/1411.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName><forename type="first">C?sar</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil?mon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2657" to="2661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving lexical choice in neural machine translation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01329</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting bleu scores</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2488" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nematus: a Toolkit for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>L?ubli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361</idno>
		<title level="m">Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">L1-norm batch normalization for efficient training of deep neural networks</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A lightweight recurrent network for sequence modeling</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13324</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual learning without normalization via better initialization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10752</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
