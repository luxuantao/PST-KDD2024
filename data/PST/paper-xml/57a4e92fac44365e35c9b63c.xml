<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning of Transferable Representation for Scalable Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning of Transferable Representation for Scalable Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CC4990690DE4DFC0A1F50C3D283C0F28</idno>
					<idno type="DOI">10.1109/TKDE.2016.2554549</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2016.2554549, IEEE Transactions on Knowledge and Data Engineering This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2016.2554549, IEEE Transactions on Knowledge and Data Engineering</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain adaptation</term>
					<term>deep learning</term>
					<term>denoising autoencoder</term>
					<term>neural network</term>
					<term>two-sample test</term>
					<term>multiple kernel learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The generalization performance of supervised learning with insufficient training data will be unsatisfactory for practical applications, while manual labeling of sufficient training data for all application domains may be prohibitive. Domain adaptation has been established to be effective in reducing human labeling efforts by leveraging rich labeled data from other relevant domains <ref type="bibr" target="#b0">[1]</ref>. The challenge is that different domains usually follow substantially different distributions, which has posed a major bottleneck for adapting predictive models across domains. For example, a sentiment classifier <ref type="bibr" target="#b1">[2]</ref> built for one product reviews may not predict well the polarity of reviews to another product, since different words can be used to express sentiment in different domains, e.g. words like "blur", "fast", "sharp" are used to comment the electronics products, while they do not carry sensible opinion in the books products <ref type="bibr" target="#b2">[3]</ref>. For another example, an object recognition model trained on manually annotated images may not generalize well on testing images under substantial variations in pose, occlusion, or illumination. In such cases, effective domain adaptation methods are highly desirable to reduce domain discrepancy and labeling consumption.</p><p>The domain adaptation problems involve two types of datasets, one from a source domain and the other from a target domain. The source domain contains sufficient amount of labeled data such that a classifier can be reliably built.</p><p>• M. Long, J. Wang, Y. Cao, and J. Sun are with the School of Software, Tsinghua TNList Lab for Info. Sci. and Tech., Tsinghua University, Beijing, China. E-mail: mingsheng@tsinghua.edu.cn, jimwang@tsinghua.edu.cn, yue-cao14@mails.tsinghua.edu.cn, sunjg@tsinghua.edu.cn. Corresponding author: J. Wang. • P. S. Yu is with the Institute for Data Science, Tsinghua University, and with the University of Illinois at Chicago, IL, USA. E-mail: psyu@uic.edu.</p><p>The target domain contains large amount of unlabeled data that follows a substantially different but related distribution.</p><p>The goal is to correct the distribution mismatch such that a supervised classifier can be effectively transferred across different domains. This poses two key challenges to enable domain adaptation: <ref type="bibr" target="#b0">(1)</ref> how to reduce the domain discrepancy, and (2) how to learn transferrable feature representation.</p><p>A fruitful stream of previous works have been devoted to address the first challenge, typically by minimizing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b3">[4]</ref>, a nonparametric statistic that measures the distribution discrepancy in terms of the distance between the kernel mean embeddings of the source and target data. The main objective is to identify a feature representation or instance weighting through which the distribution discrepancy (MMD) is formally reduced <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, MMD is a kernel method that may be restricted by several disadvantages. <ref type="bibr" target="#b0">(1)</ref> The kernel functions only characterize local generalization, which is ineffective for capturing global nonlinearity of data to embody the distribution discrepancy <ref type="bibr" target="#b10">[11]</ref>. (2) A predefined kernel is not optimal for maximizing the two-sample matching power of MMD <ref type="bibr" target="#b11">[12]</ref>, while how to learn the optimal kernel is nontrivial. (3) Kernel methods often scale quadratically to the number of samples, which prohibits their applications to big data problems. These open issues are jointly important for robust domain adaptation from large-scale dataset, while they have not been well addressed in previous works.</p><p>With the recent revolution of deep learning <ref type="bibr" target="#b10">[11]</ref>, it has been successfully applied to extract abstract representation for domain adaptation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Deep learning is able to disentangle hidden factors that explain variations underlying the data samples, and hierarchically group features in accordance with their relatedness to the invariant factors <ref type="bibr" target="#b12">[13]</ref>. This establishes transfer across domains as the deep features contain abstract concepts that are invariant to domain-specific distributions. For example, in the electronics domain, domain-specific words like "blur", "fast", "sharp" should reconstruct, and be reconstructed by, co-occurring domain-shared features, typically of similar sentiment (e.g. "good" or "love"). Hence, the source-trained classifier can assign weights to features that even never occur under the original feature representation <ref type="bibr" target="#b14">[15]</ref>. However, disentangling the hidden factors of variations may unexpectedly enlarge the cross-domain distribution discrepancy, as the domains represented by new deep features will become more "compact" and more mutually distinguishable. While the invariant latent factors exploited by deep learning can suppress domain-specific variations and improve generalization, the increased cross-domain discrepancy may reversely deteriorate domain adaptation performance, leading to statistically unbounded target error <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. This problem has not been addressed for general-purpose deep learning methods.</p><p>Inspired by the literature's latest understanding on deep neural networks for learning compact and invariant feature representations, in this paper, we propose a unified deep adaptation framework to jointly learn transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two interdependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders <ref type="bibr" target="#b19">[20]</ref>, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks <ref type="bibr" target="#b20">[21]</ref>. For the unsupervised pre-training paradigm, we propose a Transfer Denoising Autoencoder (TDA) model, where the learned representations of multiple autoencoders are embedded to reproducing kernel Hilbert spaces (RKHSs) and the mean embeddings of different domain distributions are formally matched. For the supervised fine-tuning paradigm, we propose a Transfer Deep Network (TDN) model, which is constructed by stacking the encoders parts unfolded from the pre-trained TDAs, and is fine-tuned using a supervised classifier at the output layer. As the effectiveness of twosample matching based on MMD <ref type="bibr" target="#b3">[4]</ref> is restricted by the local generalization issue <ref type="bibr" target="#b10">[11]</ref> and suboptimal kernel issue <ref type="bibr" target="#b11">[12]</ref>, we propose a multiple kernel method that learns optimal kernel for two-sample matching. To enable scalable adaptation to large-scale applications, we develop a linear-time algorithm using B-test, an unbiased estimate of MMD <ref type="bibr" target="#b21">[22]</ref> that scales linearly to large-scale samples. Extensive empirical evidence shows that the proposed models significantly outperform state of art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition. The contributions of this paper are summarized as follows.</p><p>• A deep adaptation framework is proposed for robust domain adaptation, taking advantages from both unsupervised pre-training and supervised fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A multi-kernel learning method is devised for twosample matching of different domain distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A linear-time learning algorithm is devised to enable scalable deep domain adaptation based on MMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A rigorous theoretical analysis of generalization error bound is provided to establish statistical guarantees.</p><p>The remainder of this paper is organized as follows. We begin by reviewing the related works in Section 2. We present the proposed deep adaptation models in Section 3, and derive the learning algorithms and theoretical analysis in Section 4. Empirical evaluations are reported in Section 5, while conclusion and future work are enclosed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The performance of supervised learning machines depend on training data, while it is usually time-consuming to collect sufficient training data. Domain adaptation is a general learning paradigm that allows classification algorithms to leverage rich labeled data from relevant domains. It has been widely deployed to save the manual-labeling efforts in many areas, e.g. machine learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, data mining <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, natural language processing <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and computer vision <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b8">[9]</ref>, etc. Prior domain adaptation methods can be generally put into two categories <ref type="bibr" target="#b0">[1]</ref>: (1) instance weighting <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which selects the source instances that are the most relevant to the target domain; <ref type="bibr" target="#b1">(2)</ref> feature extraction <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, which learns a shallow feature representation that remains invariant across domains. However, without learning deep features that can suppress domain-specific exploratory factors, such feature invariance may be limited by domain-specific structures.</p><p>Deep learning extracts representation that disentangles and hides more or less the explanatory factors of variation underlying data samples <ref type="bibr" target="#b10">[11]</ref>. Such representation manifests invariant factors underlying different populations and can successfully establish domain adaptation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Deep learning has also been extended to multimodal and multi-source problems <ref type="bibr" target="#b13">[14]</ref>. However, these methods rely on the assumption that deep learning can successfully learn the desired transferable representations for domain adaptation. As we will clarify by both theoretical and empirical results, the domain discrepancy poses a general bottleneck to the generalization performance of machine learning algorithms that cannot be tackled solely by deep learning methods.</p><p>The domain discrepancy should be reduced to achieve lower transfer errors <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Several parallel works <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> add an adaptation layer to the deep convolutional neural network (CNN), but may be restricted by two defects:</p><p>(1) they only adapt a single layer of the network, which may be ineffective since there are multiple layers where hidden features are not directly transferable <ref type="bibr" target="#b15">[16]</ref>; (2) they either use suboptimal kernel for MMD-based two-sample matching, or inefficient adversarial training for the source-target discrimination, which further degrades the adaptation effectiveness <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b32">[33]</ref>. While the deep adaptation network (DAN) model proposed by Long et al. <ref type="bibr" target="#b32">[33]</ref> gives state of the art results, it is restricted to visual domains and is inapplicable to textual domains. Finally, the proposed framework contrasts clearly from a concurrent work on supervised representation learning with deep autoencoders (TLDA) <ref type="bibr" target="#b33">[34]</ref>: (1) TDN is deep neural network while TLDA is limited to only one hidden layer and one classifier layer; (2) TDA and TDN learn optimal kernel to maximize two-sample matching power while TLDA is limited to mean matching with KL-divergence; (3) TDA and TDN have rigorous generalization error bounds while it is still unclear whether TLDA has such guarantees. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCALABLE DOMAIN ADAPTATION NETWORKS</head><p>This section presents two novel deep learning architectures for scalable domain adaptation. We begin by introducing the multi-kernel maximum mean discrepancy (MK-MMD), a nonparametric test statistic for optimal two-sample distribution comparison. Then we propose two interplay architectures: the transfer denoising autoencoder (TDA) model that learns transferable representation through pre-training on unlabeled data, and the transfer deep network (TDN) model that learns both transferable representation and classifier through fine-tuning on labeled and unlabeled data. Both pre-training and fine-tuning are vital for effective learning of deep models: pre-training mitigates the trap of local minima and fine-tuning exploits the supervised information.</p><p>In unsupervised domain adaptation, we have a source domain D s = {(x s i , y s i )} ns i=1 with n s labeled examples, and a target domain D t = {x t j } nt j=1 with n t unlabeled examples. The source domain and target domain follow different probability distributions p and q, respectively. We do not assume that all domains should share identical features and we will pad all input vectors with zeros to make both domains be of equal dimensionality d. We are targeting a deep architecture which is able to learn transferable representation to bridge the domain discrepancy, and construct a classifier y = h(x) which minimizes target risk R Dt (h) = Pr (x,y)∼q [h (x) = y] using source supervision. When multiple source domains {D s } S s=1 are available, we can extend the deep architectures for multiple-source domain adaptation. Furthermore, when there is a small amount of labeled examples available in the target domain, the transferable representation learned by our method can be fed to existing semi-supervised methods to enable semi-supervised domain adaptation. The frequent notations and their descriptions are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Kernel Maximum Mean Discrepancy</head><p>The challenge of domain adaptation is that target domain has no labeled information. To approach this problem, many existing methods bound the target error by the source error plus a discrepancy metric between the source and the target <ref type="bibr" target="#b17">[18]</ref>. Two classes of statistics have been explored for the twosample testing, which makes acceptance or rejection decision to the null hypothesis p = q, given two samples generated respectively from p and q: Energy Distance (ED) and Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b34">[35]</ref>. In this paper, we will focus on the multiple-kernel variant of MMD (MK-MMD) proposed by <ref type="bibr" target="#b11">[12]</ref>, which is formalized to jointly maximize the two-sample test power and minimize the Type II error, i.e. failure of rejecting the false null hypothesis p = q.</p><p>Let H k be the reproducing kernel Hilbert space (RKHS) induced by a characteristic kernel k. The kernel mean embed-</p><formula xml:id="formula_0">ding of distribution p in H k is a unique function µ k (p) which satisfies E x∼p f (x) = f (x) , µ k (p) H k for all f ∈ H k .</formula><p>The kernel k is characteristic if the kernel mean embedding µ k (p) is injective, and thus each distribution can be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding µ k (p) so that we can learn through µ k (p) instead of p, which removes the necessity of density estimation of p. This is advantageous as the kernel mean embedding has dimension-independent rates of convergence, while rates of convergence for many density estimation procedures are dependent on the input dimension <ref type="bibr" target="#b3">[4]</ref>. The multi-kernel variant of maximum mean discrepancy (MK-MMD) <ref type="bibr" target="#b11">[12]</ref> between distributions p and q is defined as the RKHS-distance between µ k (p) and µ k (q),</p><formula xml:id="formula_1">d 2 k (p, q) E p [φ (x s )] -E q φ x t 2 H k ,<label>(1)</label></formula><p>where φ(•) is the nonlinear feature mapping that induces</p><formula xml:id="formula_2">H k . The most important property is that p = q iff d 2 k (p, q) = 0 [4]. The multi-kernel k (x, x ) = φ (x) , φ (x ) is defined as the convex combination of m characteristic kernels {k u }, K k = m u=1 β u k u : m u=1 β u = 1, β u 0, ∀u ,<label>(2)</label></formula><p>where the constraints on coefficients {β u } are imposed to guarantee that the composed multi-kernel k is characteristic. As theoretically studied in <ref type="bibr" target="#b11">[12]</ref>, the kernel adopted for mean embeddings of p and q is critical in ensuring high test power and low test error, i.e. one should minimize the chance of degenerated two-sample test cases d 2 k (p, q) → 0 when p = q. The multi-kernel k can leverage multiple kernels to match the moments statistics of distributions at different scales. By minimizing the test error, we can establish an optimal kernel selection method for powerful two-sample matching.</p><p>A successful strategy to control the domain discrepancy is to find an invariant feature representation through which the source domain and target domain become similar <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Note that MMD has been extensively explored in this line of works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, to date there has been no attempt learning both transferable representation and classification model through MK-MMD in deep networks. Hence, prior shallow learning methods may be restricted by representation weakness as they cannot disentangle the exploratory factors of variation underlying data samples <ref type="bibr" target="#b10">[11]</ref>, while prior deep learning methods may be restricted by adaptation weakness as they cannot correct the distribution mismatch using two-sample matching <ref type="bibr" target="#b11">[12]</ref>. These problems motivate powerful deep architectures that seamlessly integrate optimal two-sample matching module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transfer Denoising Autoencoder</head><p>In this subsection, we present Transfer Denoising Autoencoder (TDA), an unsupervised model for learning from large-scale unlabeled data based on denoising autoencoders <ref type="bibr" target="#b19">[20]</ref>, which explores the idea of optimal two-sample matching to learn transferable representation for scalable domain adaptation. TDA enables domain adaptation by unsupervised layerwise pre-training, which serves both as an effective initialization and adaptive regularization for training deep neural networks <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Unsupervised pre-training is important for domain adaptation since labeled data in this scenario is </p><formula xml:id="formula_3">MK- MMD x s f θ (  x s )  x s xs x t  x t f θ (  x t )  z s </formula><formula xml:id="formula_4">y s ŷt z s z t f θ (2) (z s ) f θ (2) (z t ) z s(2) z t (2)</formula><p>f θ sup (z s (2) ) often insufficient for supervised training of deep networks. Figure <ref type="figure" target="#fig_0">1</ref>(a) shows the architecture of the TDA model.</p><formula xml:id="formula_5">f θ (x t ) f θ sup (z t (2) )<label>(</label></formula><p>Deep learning <ref type="bibr" target="#b10">[11]</ref> is armed with the capability of learning distributed, compositional, and abstract representations for complicated sensory data such as text, image, and video. In this work, we adapt the Denoising Autoencoder (DA) <ref type="bibr" target="#b19">[20]</ref> for transfer learning, which has many successful cases especially for text mining and natural language processing. DA is a fully connected neural network comprising one input layer, one output layer, and one or multiple hidden layers, which targets at denoising artificially corrupted inputs, i.e. learning to reconstruct clean inputs from corrupted versions. It has been shown that training with such a denoising criterion is related to data-dependent adaptive regularization in generalized linear models <ref type="bibr" target="#b38">[39]</ref> and marginalized denoising autoencoders <ref type="bibr" target="#b39">[40]</ref>, which captures the manifold structure of the input distribution to undo the effect of corruption. The reconstruction from DA is a nearby but higher-density point than the corrupted input point, hence DA may learn more robust representation than ordinary autoencoders <ref type="bibr" target="#b19">[20]</ref>.</p><p>Specifically, let x i , xi , xi be the original data sample, the corrupted version of x i generated by pre-defined corruption probability c(x|x), and the reconstructed version of x i that is decoded from DA, respectively. Then DA denoises input corruptions by minimizing the reconstruction error of data:</p><formula xml:id="formula_6">min θ,θ n i=1 E c(x|x) J (x i , xi ) zi = f θ (x i ) and xi = g θ (z i ) ,<label>(3)</label></formula><p>where f θ (•) and g θ (•) are the encoder and decoder respectively, with θ and θ being the autoencoder parameters, zi is the hidden representation of the corrupted input sample xi , and J(•, •) is the loss function, which can be taken as squared loss J (x i , xi )</p><p>x i -xi 2 or cross-entropy loss</p><formula xml:id="formula_7">J (x i , xi ) -j [x ij log xij + (1 -x ij ) log (1 -xij )]</formula><p>. The choices of loss functions will depend on the types of encoder and decoder, which are parametrized respectively as follows</p><formula xml:id="formula_8">f θ (x i ) = a (Wx i + b) g θ (z i ) = a (W zi + b ) ,<label>(4)</label></formula><p>where θ {W, b} and θ {W , b } are the weight and bias terms of the encoder and decoder respectively, a and a are activation functions, which can be set as linear function a</p><formula xml:id="formula_9">(x) = x, rectifier linear unit (ReLU) a(x) = max(0, x), sig- moid function a (x) = 1 1+e -x , or hyperbolic tangent (tanh)</formula><p>function a (x) = e x -e -x e x +e -x . When the decoder activation a is sigmoid and the input x is binary, the cross-entropy loss is preferred; otherwise the squared loss should be used <ref type="bibr" target="#b19">[20]</ref>. We adopt sigmoid activation for the encoder and decoder, and cross-entropy loss for the objective, except for the first layer where we adopt linear activation for the decoder, and squared loss for the objective since our data is real-valued. A featured ingredient of DA is the expectation E c(x|x) (•) in Equation ( <ref type="formula" target="#formula_6">3</ref>) that averages over corrupted samples xi drawn from corruption process c (x i |x i ). Computing this expectation exactly is intractable due to the nonlinear encoding and decoding functions f θ and g θ . In practice, Equation( <ref type="formula" target="#formula_6">3</ref>) is optimized by mini-batch stochastic gradient descent (SGD), where the gradient is estimated by drawing a few corrupted versions of x i at each iteration. In this paper, we focus on the masking corruption c (x i |x i ) which corrupts each x i by random feature removal, i.e. each feature is independently set to 0 with probability c ∈ [0, 1]. This masking corruption proves effective for general-purpose problems <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>It has been shown that the hidden representation zi = f θ (x i ) learned by DA can disentangle the exploratory factors of variations underlying the sample distributions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. However, the latest literature findings also reveal that the deep representation can reduce, but not remove, the distribution discrepancy across the source and target domains <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In this paper, we pre-train DA by requiring the distributions of the source and target become similar under the hidden representation. This is realized by substituting the hidden representation to the MK-MMD formulation (1)</p><formula xml:id="formula_10">min θ,θ max k∈K d 2 k (D z s , D z t ) = E p φ (z s ) -E q φ zt 2 H k ,<label>(5)</label></formula><p>where x s , zs ∼ p and x t , zt ∼ q are the source sample and target sample generated from distributions p and q, respectively, while D z s {z s i } ns i=1 is the hidden representation of source data, and D z t {z t i } nt i=1 is the hidden representation of target data. To enable generalization of DA across source and target D s and D t , we integrate MK-MMD (5) as an adaptation regularizer to the DA empirical risk (3), leading to the proposed Transfer Denoising Autoencoder (TDA) model:</p><formula xml:id="formula_11">min θ,θ max k∈K n i=1 E c(x|x) J (x i , xi ) + λ E p φ (z s ) -E q φ zt 2 H k ,<label>(6)</label></formula><p>where λ &gt; 0 is the penalty parameter of adaptation regularization. By minimizing MK-MMD for learning the nonlinear feature representation, TDA is capable to learn distributed, compositional, and abstract representation which manifests invariant structures across domains. An illustration of TDA is shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>. After the first layer TDA is trained, we can construct a deep architecture by stacking multiple TDAs on its top in a layerwise way <ref type="bibr" target="#b19">[20]</ref>, where the hidden representation of the lower-layer TDA, i.e. f θ (x i ), is used as the input of the upper-layer TDA. It is noteworthy that input corruption is only used for denoising-training of DA so that it can learn useful features. Once the encoder f θ has been learned, it will be applied on uncorrupted inputs to produce hidden representation f θ (x i ). We train the Stacked TDA in a greedy layerwise manner using mini-batch stochastic gradient descent (SGD) <ref type="bibr" target="#b19">[20]</ref>. Through stacking, the hidden representations are made more abstract to disentangle the exploratory factors of variations underlying data samples, and more invariant to reduce the distribution discrepancy across domains to enable effective domain adaptation.</p><p>Marginalized TDA It has been shown that DA <ref type="bibr" target="#b19">[20]</ref> has high computational cost, because the nonlinear activation functions a(•) and a (•) make its optimization problem nonconvex while its random feature corruption is made on each small-batch of input samples. Inspired by the neat idea of marginalizing linear denoising autoencoders (mSDA) <ref type="bibr" target="#b14">[15]</ref>, where the random feature corruption is marginalized out by taking the expectation of random corruptions, we speed up TDA by applying the same marginalization manipulation. Conceptually, the marginalization is equivalent to training the autoencoders with an infinite number of the corrupted input samples. The marginalization is applicable only after simplifying the TDA model ( <ref type="formula" target="#formula_11">6</ref>) using the linear activation function a(x) = x and a (x) = x. This marginalized version of TDA is named as mTDA, whose attractive advantage is the capability of learning from infinite corruptions and performing much faster than TDA. As the nonlinearity and the deep architecture are arguably the two key contributors to the breakthrough of deep learning <ref type="bibr" target="#b10">[11]</ref>, to still benefit from nonlinearity, we inject the nonlinearity using a(•) = tanh(•) after the linear hidden representation f θ (x i ) is computed. To perform the layer-wise training, several mTDA layers are stacked by feeding the output of the ( -1) th mTDA (after the activation function) as the input to the th layer mTDA. Empirical evidence shows that marginalized autoencoders are mainly effective for text mining applications <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b40">[41]</ref>. A distinction is that mSDA only uses decoder g θ while mTDA uses both encoder f θ and decoder g θ , hence mSDA can only extract representations with the same dimension as input, while mTDA can learn dimension-reduced representations using a bottleneck encoder with fewer network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transfer Deep Network</head><p>The composition of multiple levels of nonlinearity in neural networks is key to efficiently model complex relationships across exploratory factors and to achieve better generalization performance on challenging perception tasks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This philosophy has motivated the latest breakthroughs in deep neural networks, such as deep convolutional neural network (CNN) <ref type="bibr" target="#b41">[42]</ref> for computer vision and deep recursive neural network (RNN) <ref type="bibr" target="#b42">[43]</ref> for natural language processing. Recent process shows that unsupervised pre-training as done in TDA, and supervised fine-tuning as done in CNN, are both important for the effective learning of deep neural networks, while supervised fine-tuning is very important by adjusting deep networks to better fit the perception task, which serves as the key to recent breakthroughs of both CNN and RNN. Therefore, one major limitation of TDA is that it cannot be effectively adjusted to the supervised learning task due to its unsupervised pre-training paradigm. Another limitation of TDA is that it constructs the deep architecture through "stacking", which is not truly "deep" since its upper-layers cannot influence its lower-layers by back-propagation.</p><p>To benefit from the worlds of unsupervised pre-training and supervised fine-tuning, we further propose the Transfer Deep Network (TDN) model, a supervised model for learning from both labeled source and unlabeled target data based on multilayer perceptrons <ref type="bibr" target="#b20">[21]</ref>. We again explore the idea of optimal two-sample matching to learn both transferable representation and classifier for scalable domain adaptation. Supervised fine-tuning is critical for domain adaptation as labeled data can be fully exploited to make the transferable representation more discriminative. TDA serves as an indispensable pre-training phase for TDN, otherwise TDN cannot be trained effectively due to gradient vanishing <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Figure <ref type="figure" target="#fig_0">1(b)</ref> shows the architecture of the TDN model.</p><p>Stacking multiple TDAs to initialize the deep network works in much the same way as stacking multiple denoising autoencoders <ref type="bibr" target="#b19">[20]</ref> or ordinary autoencoders <ref type="bibr" target="#b43">[44]</ref>. It is noteworthy to specify that input corruption is only used for the denoising-training of each individual TDA so that it may learn useful feature extractors. Once the encoder f θ has been learned, it will henceforth be applied on uncorrupted inputs to produce the representation that will serve as clean input for training the next TDA. After a stack of TDA encoders has thus been built, a softmax regression classifier can be added on top of the TDA encoders, yielding a deep neural network amenable to supervised learning, which can be fine-tuned to exploit source labeled data and match cross-domain data. We name the resulting architecture as Transfer Deep Network (TDN), which is a multilayer perceptron pre-trained by TDA and regularized by the MK-MMDs on all the hidden layers:</p><formula xml:id="formula_12">min {θ } l 1 max k∈K ns i=1 J (h (x s i ) , y s i )+λ l-1 =1 E p φ(z s, ) -E q φ(z t, ) 2 H k ,<label>(7)</label></formula><p>where λ &gt; 0 is the penalty parameter of adaptation regularization, l is the number of layers, z s, and z t, are the -th layer hidden representation of source sample x s and target sample x t respectively, h(x s i ) is the classifier produced by the deep neural network, and J(•, •) is the cross-entropy loss function for the softmax regression that is defined as follows</p><formula xml:id="formula_13">J (h (x s i ) , y s i ) = - c j=1 1 {y s i = j} log h j (x s i ), (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where c is the number of categories in source domain, and h j (x s i ) = e z s,l ij / j e z s,l ij is the softmax function computing the probability of predicting sample x s i to the j-th category. By minimizing MK-MMD in multiple hidden layers 1 ≤ ≤ l-1 during supervised fine-tuning from labeled source data, TDN is capable of learning both transferable representation and classifier to enable effective deep domain adaptation.</p><p>The TDA and TDN models take advantages from both unsupervised pre-training for effective training of deep networks, and supervised fine-tuning for exploiting supervised information. The optimal multi-kernel two-sample matching based on the MK-MMD is performed for both worlds of methods, leading to an optimal deep learning framework for effective domain adaptation. The formulations of TDA <ref type="bibr" target="#b5">(6)</ref> and TDN <ref type="bibr" target="#b6">(7)</ref> are minimax problems that learn abstract deep representation by minimizing the MK-MMD with respect to network parameters θ, θ , and jointly learn optimal multikernel by maximizing the MK-MMD with respect to the kernel parameters β. We will further show in the theoretical analysis that the TDA and TDN models can achieve a tighter bound for domain adaptation. Another notable thing is that MK-MMD is imposed on multiple layers of TDA and TDN, which matches the hierarchical representations at different abstraction levels for a deep consolidation of transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALGORITHM AND ANALYSIS</head><p>We present linear-time learning algorithms for the TDA and TDN models, and provide theoretical analysis on the learning bound for scalable domain adaptation. The scalable learning algorithms are based on the low-variance unbiased estimate of MK-MMD <ref type="bibr" target="#b21">[22]</ref>. Note that most previous domain adaptation methods requires O(n 2 ) cost to compute MMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Network Parameters</head><p>Using the kernel trick k (x, x ) = φ (x) , φ (x ) , MK-MMD in Equation ( <ref type="formula" target="#formula_1">1</ref>) can be computed as the expectation of kernel functions</p><formula xml:id="formula_15">d 2 k (p, q) = E x s x s k(x s , x s ) + E x t x t k(x t , x t ) - 2E x s x t k(x s , x t</formula><p>), where x s , x s iid ∼ p and x t , x t iid ∼ q, k ∈ K. However, this computation incurs a complexity of O(n 2 ), which is rather undesirable for deep learning, as the power of deep networks largely derives from learning large-scale datasets. Moreover, the summation over pairwise similarity between all data points makes mini-batch stochastic gradient descent (SGD) more difficult, whereas mini-batch SGD is important to the effective training of deep neural networks. While prior works based on MMD <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b30">[31]</ref> rarely address this problem, we believe it is more critical for deep learning.</p><p>In this paper, we adopt B-test <ref type="bibr" target="#b21">[22]</ref>, a low-variance and unbiased estimate of MK-MMD, which is defined as follows</p><formula xml:id="formula_16">d2 k (D s , D t ) ns/B-1 b=0 (b+1)B i=bB+1 (b+1)B j=bB+1,j =i κ x s i , x t i , x s j , x t j n s (B -1) ns/B-1 b=0 d2 k D b s , D b t ,<label>(9)</label></formula><p>where κ(x s i ,</p><formula xml:id="formula_17">x t i , x s j , x t j ) k(x s i , x s j ) + k(x t i , x t j ) -k(x s i , x t j ) - k(x s j , x t i ), B</formula><p>is the size of mini-batch used in the mini-batch SGD, and thus n s /B is the number of mini-batches in each training epoch, and b is the index of mini-batch. B-test can be computed with linear-time cost, i.e. O(Bn), given that in mini-batch SGD the size B is often small, e.g. B = 100. Theoretical result <ref type="bibr" target="#b21">[22]</ref> shows that B-test in Equation ( <ref type="formula" target="#formula_16">9</ref>) is a lowvariance unbiased estimate of MK-MMD in Equation ( <ref type="formula" target="#formula_1">1</ref>):</p><formula xml:id="formula_18">d2 k (D s , D t ) D -→ N d 2 k (p, q) , σ 2 u n -1 s</formula><p>, which shows that Btest converges in distribution to a Gaussian with MK-MMD as the mean and σ 2 u n -1 s as the much lower variance. MK-MMD can be formulated as the sum of n s /B mini-batches, with each mini-batch denoted by d2 k D b s , D b t indexed by b, which is well fitted to the mini-batch SGD algorithm.</p><p>When training deep networks including TDA and TDN by mini-batch SGD, we only need to consider the gradient of TDA objective <ref type="bibr" target="#b5">(6)</ref> or TDN objective <ref type="bibr" target="#b6">(7)</ref>  t } with respect to the network parameters {θ } l 1 . Correspondingly, we can compute the gradient of TDA error</p><formula xml:id="formula_19">x i ∈D ,b s ∪D ,b t ∂J(x i ,x i ) ∂θ or TDN error x s i ∈D b s ∂J(h(x s i ),y s i ) ∂θ</formula><p>for each mini-batch b in the -th layer of the Stacked TDA or TDN. Hence, to perform a mini-batch update, we compute the gradient of the TDN objective <ref type="bibr" target="#b6">(7)</ref> with respect to θ as</p><formula xml:id="formula_20">∇ b θ = x s i ∈D b s ∂J (h (x s i ) , y s i ) ∂θ + λ ∂ d2 k D ,b s , D ,b t ∂θ .<label>(10)</label></formula><p>We omit the update rule for TDA, which is straightforward. Such a mini-batch SGD can be easily implemented based on the Pylearn2 library <ref type="bibr" target="#b44">[45]</ref>. Given kernel k as the linear combination of m Gaussian kernels {k u (x i , x j ) = e -xi-xj 2 /γu }, the gradient</p><formula xml:id="formula_21">∂ d2 k (D ,b s ,D ,b t )</formula><p>∂θ can be computed for each data pair based on the chain rule of derivatives. For example,</p><formula xml:id="formula_22">∂k(z s, i , z t, j ) ∂W = - m u=1 2β u γ u k u (z s, i , z t, j ) × (z s, i -z t, j ) × ȧ(z s, i ) z s,( -1) i -ȧ(z t, j ) z t,( -1) j T , (<label>11</label></formula><formula xml:id="formula_23">)</formula><p>where ȧ is the gradient of activation a. In TDA, the -th layer input is the ( -1)-th layer hidden output, i.e. x i = z -1 i . We speed up the marginalized TDA (mTDA) by exploring the interesting marginalization trick used in <ref type="bibr" target="#b14">[15]</ref>, that is, the expectation over the corruption probability c(x|x) can be computed analytically. To see this, we formally derive the gradient of mTDA objective, i.e. Equation ( <ref type="formula" target="#formula_11">6</ref>) using linear activation functions with respect to each</p><formula xml:id="formula_24">x i ∈ D ,b s ∪ D ,b t as ∂J x i , x i ∂W = -2W T E x i x T i + W WE x i x T i , (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>where the bias terms are absorbed into the weight terms for notation brevity. We compute the above expectations by a marginalization trick <ref type="bibr" target="#b14">[15]</ref>. An off-diagonal entry in x i x T i is uncorrupted if both the two features j and j "survived" the corruption, which happens with probability (1 -c) 2 ; for diagonal entries, this holds with probability (1 -c). Denote by c = [(1 -c), . . . , (1 -c), 1]</p><p>T ∈ R d+1 the "survival" vector, where cd+1 = 1 is the constant feature never corrupted. The expectations in Equation ( <ref type="formula" target="#formula_24">12</ref>) for denoising corruptions are  Update β of the th TDA by QP <ref type="bibr" target="#b14">(15)</ref>.</p><formula xml:id="formula_26">E c(x|x) [x i x T i ] = (x i x T i ) jj cj , E c(x|x) [x i x T i ] =    x i x T i jj cj cj , j = j x i x T i jj cj , j = j<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Compute hidden representation Z = f θ (X ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>Set the input of the ( + 1) th TDA as X +1 ← Z .</p><p>/ * Supervised fine-tuning with TDN * / 9 Initialize TDN parameters {θ } l 1 by TDA weights. Update θ in th layer of TDN by SGD <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>Update β in th layer of TDN by QP <ref type="bibr" target="#b14">(15)</ref>.</p><p>15 Return deep representation R ← Z l-1 and classifier h.</p><p>With the closed-form expectations, we can compute mTDA without explicit corruption and achieve a faster algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Kernel Parameters</head><p>Theoretically, the optimal kernel parameter β for MK-MMD can be learned by jointly maximizing the two-sample testing power and minimizing the Type II error of degenerated twosample test cases d 2 k (p, q) → 0 when p = q <ref type="bibr" target="#b11">[12]</ref>. However, such an optimization problem is not intuitive. We opt to compute an approximation of the Type II error by choosing an optimal multi-kernel k that maximizes the MK-MMD <ref type="bibr" target="#b45">[46]</ref> max</p><formula xml:id="formula_27">1 T β =1,β ≥0 E p φ(z s, ) -E q φ(z t, ) 2 H k -ε β 2 2 ,<label>(14)</label></formula><p>where ε = 10 -3 is a small penalty to bound the magnitude of β . Denote by d = (d 1 , d 2 , . . . , d m ) T the MMDs, where each d u is the MMD computed using a base Gaussian kernel k u by the B-test introduced in Equation ( <ref type="formula" target="#formula_16">9</ref>). Problem <ref type="bibr" target="#b13">(14)</ref> can be reduced to a constrained Quadratic Program (QP) as</p><formula xml:id="formula_28">min 1 T β =1,β ≥0 εβ T β -d T β ,<label>(15)</label></formula><p>which can be solved efficiently using standard QP packages. The complete procedures for learning both TDA and TDN models are summarized in Algorithm 1. All steps in our algorithm scale linearly to both feature dimension and sample size, hence the overall computational complexity is O(nd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalization Error Analysis</head><p>We analyze the expected target-domain risk of TDN, making use of the theory of domain adaptation <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and kernel embedding of probability distributions <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Theorem 1. Let h ∈ H be a hypothesis, s (h) and t (h) be the expected risks of the source and target respectively, then</p><formula xml:id="formula_29">t (h) s (h) + 2d k (p, q) + C, (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>where C is a constant for the Rademacher complexity of hypothesis space and the risk of an ideal hypothesis for both domains.</p><p>Proof sketch: The theoretical result in Ben-David et al. <ref type="bibr" target="#b46">[47]</ref> shows that t (h) s (h) + d H (p, q) + C 0 , where d H (p, q) is the H-divergence that characterizes the discrepancy between distributions p and q by a rich hypothesis space H,</p><formula xml:id="formula_31">d H (p, q) 2 sup η∈H Pr x s ∼p [η(x s ) = 1] -Pr x t ∼q η(x t ) = 1 . (17)</formula><p>The H-divergence relies on the capacity of the hypothesis space H to distinguish distributions p from q, and η ∈ H can be viewed as a two-sample classifier. By choosing η as a Parzen window classifier <ref type="bibr" target="#b47">[48]</ref>, d H (p, q) can be bounded by the risk of the Parzen window classifier (Equation ( <ref type="formula" target="#formula_32">18</ref>), Line 2), which is equivalent to the MK-MMD as revealed by <ref type="bibr" target="#b47">[48]</ref>: </p><formula xml:id="formula_32">d H (p, q) dH (D s , D t ) + C 1 2 1 -inf η∈H ns i=1 L+1[η(x s i )] ns + nt j=1 L-1[η(x t j )] nt + C 1 = 2 (1 + d k (p, q)) + C 1 ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We perform a comprehensive experimental study on diverse real-life domain adaptation problems to evaluate both the effectiveness and scalability of the proposed TDA and TDN models, including sentiment polarity prediction, email spam filtering, newsgroup content classification, and visual object recognition, with a specific focus on the respective effects of unsupervised pre-training, supervised fine-tuning, and optimal two-sample matching. Datasets, codes and configurations used in the evaluation will be made available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Multi-Domain Sentiment Dataset</head><p>The Multi-Domain Sentiment Dataset 1 <ref type="bibr" target="#b1">[2]</ref> has been widely adopted as the benchmark dataset for domain adaptation and sentiment analysis. It contains a collection of product reviews from Amazon.com about four product domains: books (B), dvds (D), electronics (E), and kitchen appliances (K). Each review is assigned with a positive polarity (higher than 3 stars) or with a negative polarity (3 stars or lower) and is represented by the term frequency (TF). Each domain consists of 2,000 labeled reviews and approximately 4,000 unlabeled ones (varying slightly between domains) and the two classes are exactly balanced. Most previous methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b14">[15]</ref> provide results on this dataset based on 12 sentiment transfer tasks: D→B, E→B, K→B, B→D, E→D, K→D, B→E, D→E, K→E, B→K, D→K, E→K, where the notation before arrow corresponds to the source domain and the notation after arrow corresponds to the target domain. Detailed statistics of this dataset are summarized in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Email Spam Filtering Dataset</head><p>The email spam filtering dataset released by ECML/PKDD 2006 Discovery Challenge 2 (Task A) contains 4 separate user inboxes, which can be grouped into private inboxes u1, u2, u3, and public inbox u * . Each private inbox consists of 1,250 spam and 1,250 non-spam emails of private users, and the public inbox consists of 2,000 spam and 2,000 non-spam emails from public domain, and each email is represented by the term frequency (TF). The sample distributions are similar within each group but are significantly different between groups. Thus in our experiments, the cross-domain tasks are constructed between different groups of inboxes, which are u1→u * , u2→u * , u3→u * , u * →u1, u * →u2, and u * →u3. For example, u1→u * denotes the email spam filtering transfer task using u1 as source domain and u * as target domain. Detailed statistics of this dataset are summarized in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Newsgroup Classification Dataset</head><p>The 20-Newsgroups 3 dataset has approximately 20,000 documents distributed evenly in 20 different subcategories. The corpus contains four top categories comp (C), rec (R), sci (S) and talk (T), each with four subcategories detailed in <ref type="bibr" target="#b23">[24]</ref>. We can construct 6 task groups for binary classification by randomly selecting two top categories, resulting in 6 task groups: C → R, C → S, C → T, R → S, R → T, S → T. For each task group A → B, we randomly select two subcategories from A and B respectively to form the source domain and the remaining subcategories form the target domain, resulting in C 2 4 • C 2 4 = 36 transfer tasks, and in total we can generate 6 • 36 = 216 transfer tasks. For fair comparison, the 216 newsgroup tasks are constructed using a preprocessed 20-Newsgroups dataset <ref type="bibr" target="#b23">[24]</ref>, which contains 25,804 features and 15,033 documents, with each document represented by the term frequency-inverse document frequency (TF-IDF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Visual Object Recognition Dataset</head><p>Office-31 <ref type="bibr" target="#b27">[28]</ref> is a standard benchmark dataset for domain adaptation in computer vision, which has 4,652 images in 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from Amazon.com, Webcam (W) and DSLR (D), which are images taken by web camera and digital SLR camera in an office with different environmental and photographing variations. Caltech-256 (C) is a standard database for object recognition with 30,607 images and 256 categories. In the expriments, we adopt the Office-Caltech dataset 4 <ref type="bibr" target="#b28">[29]</ref>, which are comprised of the 10 common categories shared by the Office-31 and Caltech-256 datasets and is widely adopted in transfer learning methods.</p><p>From the dataset we can construct 12 transfer tasks:</p><formula xml:id="formula_33">A → C, W → C, D → C, C → A, W → A, D → A, C → W, A → W, D → W, C → D, A → D, W → D.</formula><p>The dataset is represented with the DeCAF features <ref type="bibr" target="#b48">[49]</ref>, which are the 4,096dimensional FC7-layer hidden activations extracted by the deep convolutional neural network (CNN), i.e. AlexNet <ref type="bibr" target="#b41">[42]</ref>. Detailed statistics of this dataset are summarized in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Protocols</head><p>To fully evaluate the efficacy of the proposed TDA and TDN models, we compare with a wide range of state of the art domain adaptation and deep learning methods. As baseline, we train a linear SVM on the input features of labeled source domain and test it on unlabeled target domain. For cross-domain sentiment classification, Structural Correspondence Learning (SCL) <ref type="bibr" target="#b1">[2]</ref> and Spectral Feature Alignment (SFA) <ref type="bibr" target="#b2">[3]</ref> are the most widely applied methods, hence we investigate them on our datasets. We further compare with Co-Training for Domain Adaptation (CODA) <ref type="bibr" target="#b26">[27]</ref>, which has been shown to produce state of the art performance on multi-domain sentiment dataset based on shallow transfer learning. Since SCL, SFA, and CODA are specifically tailored to textual domains, we also investigate two classical transfer learning methods, Transfer Component Analysis (TCA) <ref type="bibr" target="#b5">[6]</ref> and Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b28">[29]</ref>, which are general methods applicable to various domains. In this regards, we investigate latest general-purpose shallow methods, Domain Adaptation Machine (DAM) <ref type="bibr" target="#b49">[50]</ref> and Transfer Kernel Learning (TKL) <ref type="bibr" target="#b23">[24]</ref>, which have created record performance on both the newsgroup dataset and visual object dataset.</p><p>Besides the shallow learning methods, we also consider the latest deep learning methods for domain adaptation. We choose to compare with the Marginalized Stacked Denoising Autoencoders (mSDA) <ref type="bibr" target="#b14">[15]</ref>, a marginalized variant of the seminal Stacked Denoising Autoencoders (SDA) <ref type="bibr" target="#b12">[13]</ref>, which was the first success of deep learning methods applied for domain adaptation on the multi-domain sentiment dataset. Note that the proposed TDA and TDN models distinguish clearly from mSDA and SDA by further calibrating distributions across domains using optimal two-sample matching, and constitute a unified framework that takes the benefits of both unsupervised pre-training and supervise fine-tuning. Finally, we compare with Deep Adaptation Network (DAN) <ref type="bibr" target="#b32">[33]</ref>, the latest deep transfer learning method that gives the state of the art results on the visual object dataset.</p><p>To perform a fair comparative study, we adopt an identical evaluation protocol for all comparison methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and report the average classification accuracy of these methods on all the transfer tasks. For all methods, if there are tunable hyper-parameters, we select their optimal parameters by cross-validation on labeled source data as <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For the TDA and TDN models, we also follow standard model selection procedures for deep autoencoders <ref type="bibr" target="#b12">[13]</ref> and deep neural networks <ref type="bibr" target="#b41">[42]</ref>. More specifically, the number of hidden units, learning rate, corruption probability c, and MK-MMD penalty λ are all selected using cross-validation on labeled source data. To investigate module-wise efficacy of unsupervised pre-training, supervised fine-tuning, and multi-kernel MMD, we evaluate the single-kernel variants of TDA and TDN, respectively termed TDA SK and TDN SK . Note that, TDA and TDA SK are unsupervised pre-training models that cannot perform classification directly, hence to enable fair comparison, we apply a linear SVM on their lastlayer features as <ref type="bibr" target="#b14">[15]</ref>. Correspondingly, TDN and TDN SK are supervised fine-tuning models that are initialized by TDA and TDA SK respectively, which can readily take the benefits from the unsupervised pre-training models. The proposed models are implemented using the Pylearn2 open package 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Discussion</head><p>We report the experimental results of all methods on applicable transfer tasks and give an in-depth discussion on the respective insights of each method and their interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Result of Sentiment Classification</head><p>The challenge of cross-product sentiment polarity prediction is rooted in the distribution shift, as different words are used to express sentiment in different domains, e.g. words like "blur", "fast", "sharp" are used to comment the electronics products, while they carry no opinion in the books products. We construct 12 transfer tasks of sentiment classification on this dataset. Figure <ref type="figure" target="#fig_6">2</ref> shows the detailed comparison results of different methods, where each group of bars represents 5. https://github.com/lisa-lab/pylearn2 a cross-domain sentiment classification task, and each bar in specific color represents a specific method; the horizontal numbered lines are UpperBound accuracies <ref type="bibr" target="#b14">[15]</ref>, created by SVM trained with input features on target training data and evaluated on target test data following the train/test splits in Table <ref type="table">2</ref>. Note that the target labels are not accessible by all comparison methods during training as there is no labeled target data in unsupervised domain adaptation. For clarity, Table <ref type="table" target="#tab_3">3</ref> compares the dataset-wise average accuracies.</p><p>The proposed TDA and TDN models outperform all the comparison methods on all transfer tasks and significantly boost the performance on 8 out of 12 tasks. The average classification accuracy achieved by TDN on the 12 tasks are 85.99% and the performance boost is 3.61% compared to the best baseline method mSDA. It is impressive to observe that TDA and TDN even outperform the UpperBound (denoted by the numbered lines) on 6 tasks and achieve comparable results on the other 6 tasks. This verifies that the proposed models can learn high-quality transferable representation to enable domain adaptation for sentiment classification.</p><p>To achieve an in-depth understanding of the proposed models, we further present the results of their variants.</p><p>(1) The single-kernel variants TDA SK and TDN SK generally achieve better accuracy than the baseline methods, however, they perform fairly worse than the multi-kernel variants TDA and TDN, which highlights that multi-kernel MMD can bridge the domain discrepancy more effectively than single-kernel MMD. The reason is that multiple kernels with different bandwidths can match both low-order moments and high-order moments to minimize the distribution mismatch <ref type="bibr" target="#b11">[12]</ref>. (2) TDN, which learns both transferable representation and classifier by optimal two-sample matching, further outperforms TDA. Supervised fine-tuning is critical for domain adaptation since labeled data can be fully exploited to enhance the discriminative power of transferable representation and classifier. Note that TDA serves as an indispensable pre-training for TDN, otherwise TDN cannot be successfully trained due to gradient vanishing <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>From the results, we also observe that the four domains of the sentiment dataset can be roughly organized into two groups: domains B and D are similar to each other, as are domains K and E, but the two groups are very different from each other. Therefore, adapting a supervised classifier from domain K to domain E is much easier than adapting it from domain B or domain D. It is interesting to observe that the margin of which TDA and TDN outperform the best comparison method increases with adaptability difficulty. In other words, TDA and TDN perform much better than the comparison methods on the difficult-to-transfer tasks, e.g. E→B, B→K. This suggests that for those highly difficult transfer tasks, it is more important to extract high-quality representation that manifests domain-invariant structures.</p><p>The proposed models establish this goal by taking all advantages of unsupervised pre-training, supervised fine-tuning, and optimal multi-kernel two-sample matching.</p><p>To give a better understanding of the comparison methods, we further discuss their pros and cons. SVM is known to perform fairly well on standard sentiment classification problems, but it may fail when the training data and testing data are sampled from different domains <ref type="bibr" target="#b1">[2]</ref>. TCA and GFK are generic domain adaptation methods, which correct the distribution mismatch using shallow learning architectures (PCA in their cases) but they cannot outperform SVM much, at least for the sentiment tasks if not the case elsewhere. In particular, TCA uses single-kernel MMD to match different distributions, and is less effective than TDA based on MK-MMD. SCL and SFA have been widely recognized as the state of the art domain adaptation methods specifically designed for sentiment polarity prediction. Their effectiveness stems from exploring the domain knowledge of natural languages, that is, identifying domain-shared words as pivot features to construct domain-invariant feature subspace for adaptation. However, it still remains unclear how to identify a good set of pivot features for these two methods. CODA improves SCL and SFA by adapting co-training for iterative sample-and feature-selection of TF-IDF features, based on their relevance to the source and target domains. In general, SCL, SFA, and CODA may be limited to the text domains. While TKL shows state of the art results on text and image classification problems, it gives degenerated performance for the sentiment tasks due to the violation of its power-law distribution assumption on kernel eigenspectrum <ref type="bibr" target="#b23">[24]</ref>. Thus TKL may only work well for some restricted scenarios. As shallow architectures cannot create compact representation to capture abstract domain-invariant knowledge structure <ref type="bibr" target="#b19">[20]</ref>, the shallow domain adaptation methods have been sur- passed by standard deep learning methods SDA and mSDA which do not explicitly consider the domain mismatch <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The proposed TDA and TDN models significantly outperform mSDA by correcting the domain mismatch, which highlights the importance of integrating both deep learning and optimal two-sample matching for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Result of Email Spam Filtering</head><p>The difficulty of cross-inbox email spam filtering is caused by the distribution shift, since different users usually have personalized rules or preferences for detecting email spams. We construct 6 email spam filtering tasks by transferring across public and private groups of inboxes. Note that our tasks are much more difficult than those in <ref type="bibr" target="#b6">[7]</ref>, which performs transfer within the group of private inboxes u1∼u3. This can be seen by comparing the base SVM performance: SVM achieved 70.39% on our tasks while it reported 94.9% on those tasks <ref type="bibr" target="#b6">[7]</ref>. Figure <ref type="figure" target="#fig_7">3</ref> demonstrates the detailed classification accuracy of all the applicable methods and Table <ref type="table" target="#tab_3">3</ref> summarizes their average classification accuracy. The proposed TDN model outperforms the best baseline CODA by very large margin of 8.23%. It is very important to observe that, standard deep learning method mSDA performs much worse than shallow domain adaptation method CODA on this dataset. This reveals that only extracting deep representation without explicitly matching domain distributions may not be good enough for robust domain adaptation, especially when the domain discrepancy is substantially large. Another observation is that CODA consistently outperforms SCL and SFA on this dataset, which further highlights the difficulty in identifying a good set of pivot features for SCL and SFA that behave similarly across domains. Meanwhile, TDA and TDN are generic learning methods that do not need such domain-specific heuristic to engineer the features.</p><p>From the results, we also observe an interesting asymmetry property of domain adaptation: transferring from task A to task B is not identical to transferring from task B to task A. In other words, transferring from public inbox u * to private inboxes u1∼u3 is much easier than transferring from vice versa. This can be explained as that more generic domains constitute more generic concepts which can be more easily adapted to new domains. This suggests that a preferred way for domain adaptation is transferring from generic domains to specific domains, which is consistent with the successful practice of transfer learning from big data such as ImageNet <ref type="bibr" target="#b15">[16]</ref>. The results also show that the proposed TDA and TDN models work even better than the baseline methods when transferring from specific domains to more generic domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Result of Newsgroup Categorization</head><p>The distributions of cross-domain newsgroups are different since the same topic is described by domain-specific words. Newsgroup categorization is very different from sentiment polarity prediction and email spam filtering. Specifically, in sentiment and email tasks, the category of each document can be determined by a few pivot words, which are sentiment words in polarity prediction or spam words in spam filtering. In the newsgroup tasks, however, the category of each document must be determined by considering many content-carrying words, hence it is nontrivial to identify a set of pivot features for newsgroup content classification. Table <ref type="table" target="#tab_3">3</ref> summarize the average accuracy of all applicable methods on all the 216 newsgroup transfer tasks, while for detailed comparison, Figure <ref type="figure">4</ref> also shows the classification accuracies of these methods on the 6 hard-to-transfer tasks. In particular, TKL has been shown to give the state of the art performance on these content-based transfer tasks, although it performs worse on the sentiment and email tasks. Again, TDA and TDN significantly outperform the baseline methods on most tasks, which highlights the advantages of TDA and TDN to be applicable to general-purpose transfer tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Result of Object Recognition</head><p>Object recognition is an important computer perception task for multimedia data mining, which is very different from text mining problems. The distribution shift across visual domains is often caused by substantial variations in pose, occlusion, or illumination. Many domain-specific methods including SCL, SFA, and CODA cannot be applied to visual domains. On the other hand, the deep convolutional neural network (CNN) based methods have created breakthroughs in computer vision fields <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In particular as shown in Figure <ref type="figure">5</ref>, the DAN model proposed by Long et al. <ref type="bibr" target="#b32">[33]</ref> has given the latest state of the art results for object recognition tasks, but it is a domain-specific method not applicable to text domains. It is promising to see that the proposed TDN model can achieve comparable performance as DAN while significantly outperforms all the other comparison methods.</p><p>A defense for DAN is that TDN is based on DeCAF features <ref type="bibr" target="#b48">[49]</ref>, which are also extracted using the deep convolutional neural networks <ref type="bibr" target="#b41">[42]</ref>. Nonetheless, DeCAF is only used as a feature extractor for TDN. Decoupling feature extraction from model learning makes TDN a general transfer learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Empirical Analysis</head><p>We will go deeper into TDA and TDN by investigating the deep consolidation, domain discrepancy reduction, parameter sensitivity, visualization analysis and scalability study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Deep Consolidation</head><p>An advantage of deep learning is the capability to extract hierarchical representation. Thus we are curious to find out whether the transferability of deep representation can be enhanced with more layers in the TDA and TDN models. Figure <ref type="figure" target="#fig_3">6</ref>(a) demonstrates the average classification accuracy of all transfer tasks on the sentiment and email datasets respectively, with varying number of layers increased from 1 to 5. As expected, both mSDA and TDA perform increasingly better with more layers, which extract more abstract feature representations. This verifies that deep architectures can substantially enhance the representation transferability across domains to enable effective domain adaptation. It is interesting to observe that the margin of which TDA outperforms mSDA also increases by using more layers. In other words, TDA can outperform mSDA by an even larger margin when more layers of autoencoders are stacked. This can be explained by the deep consolidation of representation invariance. With more layers stacked, the extracted feature representation will be more compact and nonlinear. This naturally enhances the adaptation capability of MK-MMD, since MK-MMD relies heavily on appropriate nonlinear representation to reduce the domain distribution discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Proxy-A-Distance</head><p>The theoretical results in Ben-David et al. <ref type="bibr" target="#b17">[18]</ref> suggested the Proxy-A-Distance (PAD) as a measure of similarity between two probability distributions. They showed that the PAD between the source and target distributions is a crucial part of a generalization error bound for domain adaptation. They hypothesized that it should be difficult to discriminate between the source and target domains if an effective knowledge transfer is established between them, since this would imply similar feature distributions. In practice, computing exact PAD is intractacle and one has to compute a proxy. The approximate PAD is defined as dA = 2 (1 -2 ), where is the generalization error of a classifier (SVM in our case) trained on the binary classification problem to distinguish input samples between the source and target domains.</p><p>Figure <ref type="figure" target="#fig_3">6</ref>(b) shows the PAD on input features, mSDA features, and TDA features, respectively. Each point in the figure corresponds to the PAD of one transfer task on the sentiment dataset (12 tasks in total), with the 2-dimensional coordinate defined as ( dA (input), dA (deep)). If the point is on the upper (lower) side of line y = x, then the PAD on the deep feature is greater (smaller) than the PAD on the input feature. Figure <ref type="figure" target="#fig_3">6</ref>(b) reveals a surprising observation: the PAD increases on the mSDA representation in 11 out of 12 tasks, which implies that distinguishing two domains becomes easier with the mSDA features. This phenomenon is explained in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> as that the deep representation may disentangle both domain-specific and sentiment-polarity information and learns generally better representation for the input data, which helps both domain discrimination and sentiment prediction. However, following Ben-David et al. <ref type="bibr" target="#b17">[18]</ref>, the representation of mSDA may deteriorate domain adaptation. We conjecture that the representation extracted by standard deep learning is not sufficiently transferable.</p><p>As demonstrated in Figure <ref type="figure" target="#fig_3">6</ref>(b), the PAD decreases on TDA representation on all 12 transfer tasks. Based on the theory of Ben-David et al. <ref type="bibr" target="#b17">[18]</ref>, smaller PAD implies lower generalization error for domain adaptation. Hence TDN is guaranteed with better domain generalization performance. From a theoretical perspective, TDA and TDN are superior to prior deep learning based adaptation methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The superiority of TDA and TDN over TLDA <ref type="bibr" target="#b33">[34]</ref> is that there is no theoretical connection between the generalization bound and the KL-divergence adopted in TLDA. Moreover, TDN is also superior to the shallow adaptation methods without deep learning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b23">[24]</ref>, since these methods cannot abstract representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Parameter Sensitivity</head><p>Deep learners based on denoising autoencoders involve an important hyper-parameter, i.e., the corruption probability c. In addition, the adaptation penalty λ is important for the proposed TDA and TDN models. Although these hyperparameters can be selected using cross-valuation, insensitive parameter performance is desirable for real-life scenarios. Therefore, we conduct sensitivity analysis on both the sentiment and email datasets, where the average classification accuracy of all transfer tasks is computed on each dataset. When testing a specific parameter, we fix other parameters Generally, TDA works best under moderate probability of feature corruption, i.e. c ∈ [0.4, 0.8], which is consistent with mSDA. This suggests that the feature corruption should be neither too small that degenerates the deep representation to trivial identity mapping, nor too large that decreases signalto-noise ratio. Similarly, TDA favors moderate adaptation penalty, i.e. λ ∈ [0.1, 100]. This confirms the motivation of joint deep learning and distribution matching, since a good trade-off of them can enhance effective domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Feature Visualization</head><p>To demonstrate the transferable representation learned by TDA and TDN, we follow <ref type="bibr" target="#b48">[49]</ref> and plot in Figures <ref type="figure" target="#fig_12">9(a</ref>)-9(b) and 9(c)-9(d) the t-SNE embeddings of the images in task A → W with DeCAF features and TDN features, respectively. We can observe that with TDN features, the target points are discriminated much better, and the categories between the source and target are aligned much better. Both these observations can explain the superior performance of TDN over DeCAF. Intuitively, TDN can learn both transferable representation and classifier for robust domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Scalability Analysis</head><p>We show that the proposed models scale linearly by testing SDA <ref type="bibr" target="#b19">[20]</ref> vs. TDA and MLP <ref type="bibr" target="#b20">[21]</ref> vs. TDN on several transfer tasks. The runtime in Figure <ref type="figure">8</ref> shows that TDA (TDN) has a comparable time complexity as standard SDA (MLP), which are linearly scalable for big domain adaptation applications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have proposed a new domain adaptation framework that jointly learns the transferable representation and classifier to enable scalable domain adaptation, taking the benefits of both deep learning and optimal two-sample matching. Our promising results suggest that it is essential to jointly extract highly abstract feature representation and match different distributions in a unified framework. Also, it is beneficial to explore multiple kernel learning to enhance the transferability of two-sample matching methods. Finally, linear-time algorithms are highly expected for kernel-based domain adaptation methods in the presence of big data.</p><p>In the future, we plan to extend our framework to other deep learning methods such as recurrent neural networks, and heterogeneous feature spaces or multiple data sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed deep architectures for scalable domain adaptation: (a) transfer denoising autoencoder (TDA) that learns transferable features via unsupervised pre-training; and (b) transfer deep network (TDN) that learns both transferable features and classifier via supervised fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>based on each minibatch {D ,b s , D ,b t }, where D ,b * (b+1)B i=bB+1 z * , i is the minibatch of the -th hidden representation. Since B-test takes a summation form that can be readily decoupled into the sum of MK-MMD on each mini-batch, i.e. d2 k (D ,b s , D ,b t ), we only need to compute gradient ∂ d2 k (D ,b s ,D ,b t ) ∂θ of each mini-batch {D ,b s , D ,b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 3 for 4 for mini-batch b = 1 to n s /B do 5</head><label>1345</label><figDesc>Deep Adaptation Models: TDA and TDN Input: Data X; corruption level c, penalty λ, #layers l. Output: Transferable representation R and classifier h. / * Unsupervised pre-training with TDA * / 1 Initialize X 1 ← X and parameters {θ, θ } randomly. 2 for layer = 1 to l -1 do epoch t = 1 to T do / * Feed-forward pass omitted * / Update θ , θ of th TDA by SGD (10) (12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>11 for layer = l to 1 do 12 for</head><label>1112</label><figDesc>10 for epoch t = 1 to T do / * Feed-forward pass omitted * / mini-batch b = 1 to n s /B do 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where L(•) is the linear loss function for the Parzen window classifier η, and L +1 [η] -η, L -1 [η] η. By explicitly minimizing MK-MMD in multiple layers of TDN, the representation learned by the proposed TDN model can decrease the upper bound on target risk. The source classifier and the two-sample classifier together provide a way to assess the adaptation performance, and can facilitate model selection. Note that we maximize MK-MMD w.r.t. β (14) to minimize Type II test error, and to help the Parzen window classifier achieve minimal risk of two-sample discrimination in (18).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>DFig. 2 .</head><label>2</label><figDesc>Fig.2. Accuracy of the 12 transfer tasks on the multi-domain sentiment dataset. All methods are built on the training set of one domain and evaluated on the test sets of the other domain. TDA and TDN outperform all baseline methods on all transfer tasks, with 3.61% accuracy boost over mSDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Accuracy of the 6 transfer tasks on the email spam dataset. TDA and TDN boost the performance by 8.23% over the best baseline CODA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Accuracy of 6 difficult-to-transfer tasks selected from the newsgroup dataset. TDA and TDN boost the performance by 2.14% over TKL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Effectiveness verification: (a) deep consolidation of transferable representation, (b) proxy-A-distance for measuring domain discrepancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Parameter sensitivity: (a) corruption level c, and (b) adaptation penalty λ. TDA outperforms baselines when c ∈ [0.4, 0.8], λ ∈ [0.1, 100].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Representation visualization: t-SNE of DeCAF features on source (a) and target (b); t-SNE of TDN features on source (c) and target</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Notations and Their Descriptions Frequently Used in This Paper</figDesc><table><row><cell cols="2">Notation</cell><cell>Description</cell><cell>Notation</cell><cell>Description</cell></row><row><cell>Ds, Dt</cell><cell></cell><cell>source, target</cell><cell>x, x</cell><cell>input: original, corrupted</cell></row><row><cell>d, n</cell><cell></cell><cell>#features, #samples</cell><cell cols="2">z, z activation: original, corrupted</cell></row><row><cell>c</cell><cell cols="3">corruption probability W, b</cell><cell>deep network parameters</cell></row><row><cell cols="4">λ k, β multi-kernel function f adaptation penalty f θ , g θ sup θ</cell><cell>encoder, decoder supervised classifier: softmax</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 Average</head><label>3</label><figDesc>Accuracy (%) of Transfer Tasks on Sentiment (12 Tasks), Email (6 Tasks), Newsgroup (216 Tasks), and Object (12 Tasks) Datasets</figDesc><table><row><cell>Dataset</cell><cell>SVM TCA GFK</cell><cell>SCL</cell><cell>SFA</cell><cell cols="9">CODA DAM TKL mSDA DAN TDA SK mTDA TDA TDN SK TDN</cell></row><row><cell>Sentiment</cell><cell cols="3">77.33 78.39 75.12 80.18 80.75</cell><cell>80.84</cell><cell>79.45 76.48</cell><cell>82.38</cell><cell>-</cell><cell>84.91</cell><cell>85.08</cell><cell>85.38</cell><cell>85.74</cell><cell>85.99</cell></row><row><cell>Email</cell><cell cols="3">70.39 67.35 68.97 79.37 78.37</cell><cell>82.41</cell><cell>79.84 68.46</cell><cell>78.68</cell><cell>-</cell><cell>86.19</cell><cell>88.95</cell><cell>89.56</cell><cell>89.80</cell><cell>90.64</cell></row><row><cell cols="4">Newsgroup 82.05 86.31 88.15 84.25 84.86</cell><cell>87.89</cell><cell>87.37 92.41</cell><cell>88.64</cell><cell>-</cell><cell>91.20</cell><cell>91.99</cell><cell>92.64</cell><cell>93.86</cell><cell>94.55</cell></row><row><cell>Object</cell><cell>85.45 86.64 86.24</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.94 87.81</cell><cell>86.30</cell><cell>90.55</cell><cell>88.65</cell><cell>87.39</cell><cell>89.74</cell><cell>90.16</cell><cell>90.61</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 99, NO. PREPRINTS, 2016</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (61325008, 61502265), China Postdoctoral Science Foundation (2015T80088), National Science and Technology Supporting Program (2015BAH14F02), and Tsinghua National Laboratory (TNList) Special Fund for Big Data Science and Technology. Jianmin Wang and Mingsheng Long are the corresponding authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
		<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flexible transfer learning under support and model shift</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for largescale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
	<note>Learning Internal Representations by Error Propagation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">B-test: A nonparametric, low variance kernel two-sample test</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Covariate shift in hilbert space: A solution via surrogate kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain invariant transfer kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From bias to opinion: a transfer-learning approach to real-time sentiment analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meira</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-domain collaboration recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised representation learning: Transfer learning with deep autoencoders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Int. Joint Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and rkhs-based statistics in hypothesis testing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning with marginalized corrupted features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Marginalized denoising auto-encoders for nonlinear representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hybrid heterogeneous transfer learning through deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conf. on Artif. Intell.</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pylearn2: a machine learning research library</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.4214</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernel choice and classifiability for rkhs embeddings of probability distributions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Kernel choice and classifiability for rkhs embeddings of probability distributions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">He is a full professor at Tsinghua University, where he is the director of the School of Information Science and Technology and the director of the Tsinghua National Laboratory for Information Science and Technology. He is dedicated in teaching and R&amp;D activities in computer graphics, computer-aided design, formal verification of software, and database systems. Prof. Sun has been an academician of the Chinese Academy of Engineering since</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tkde</surname></persName>
		</author>
		<author>
			<persName><surname>Dmkd</surname></persName>
		</author>
		<author>
			<persName><surname>Wwwj</surname></persName>
		</author>
		<author>
			<persName><surname>Sigmod</surname></persName>
		</author>
		<author>
			<persName><surname>Vldb</surname></persName>
		</author>
		<author>
			<persName><surname>Icde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Icml</forename><surname>Sigir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cvpr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He is pursuing the Ph.D. degree in Computer Software at Tsinghua University. His research interests include machine learning, data mining, and information retrieval. Jiaguang Sun received the B.S. degree in Automation Science from Tsinghua University</title>
		<title level="s">where he received the B.E. degree in Electrical Engineering and the Ph.D. degree in Computer Science</title>
		<meeting><address><addrLine>China; China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1970">2012. 2008 and 2014. 2014. 1970. 1999. 2001-2004. 2003. 2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="504" to="518" />
		</imprint>
		<respStmt>
			<orgName>Computer Software from Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>IEEE Transactions on Knowledge and Data Engineering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
