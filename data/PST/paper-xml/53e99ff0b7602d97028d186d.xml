<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft clustering using weighted one-class support vector machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manuele</forename><surname>Bicego</surname></persName>
							<email>bicego@uniss.it</email>
							<affiliation key="aff0">
								<orgName type="department">DEIR</orgName>
								<orgName type="institution" key="instit1">University of Sassari</orgName>
								<orgName type="institution" key="instit2">via Torre Tonda</orgName>
								<address>
									<settlement>Sassari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mario</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
							<email>mario.figueiredo@lx.it.pt</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Instituto de Telecomunicações</orgName>
								<orgName type="institution" key="instit2">Instituto Superior Técnico</orgName>
								<address>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft clustering using weighted one-class support vector machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B016735BD017AA8C732F56730F118003</idno>
					<idno type="DOI">10.1016/j.patcog.2008.07.004</idno>
					<note type="submission">Received 29 August 2007 Received in revised form 6 May 2008 Accepted 12 July 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Soft clustering One-class support vector machines EM-like algorithms Kernel methods Deterministic annealing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new soft clustering algorithm in which each cluster is modelled by a oneclass support vector machine (OC-SVM). The proposed algorithm extends a previously proposed hard clustering algorithm, also based on OC-SVM representation of clusters. The key building block of our method is the weighted OC-SVM (WOC-SVM), a novel tool introduced in this paper, based on which an expectation-maximization-type soft clustering algorithm is defined. A deterministic annealing version of the algorithm is also introduced, and shown to improve the robustness with respect to initialization. Experimental results show that the proposed soft clustering algorithm outperforms its hard clustering counterpart, namely in terms of robustness with respect to initialization, as well as several other stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Kernel-based methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> have emerged in the last decade as a flexible and effective approach for addressing pattern recognition problems. These methods have been largely employed in supervised learning contexts (e.g., support vector classification and regression <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>) but their application in unsupervised learning represents a more recent and less explored trend <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently Camastra and Verri <ref type="bibr" target="#b7">[8]</ref> have presented a hard clustering scheme which uses one-class support vector machines (OC-SVMs) to represent the clusters. An OC-SVM is a binary classification machine which is trained using only "positive examples" (i.e., examples from one class). In the version of Tax and Duin <ref type="bibr" target="#b9">[10]</ref>, training the OC-SVM consists in determining the smallest hyper-sphere containing the training data. In the version of Sch ölkopf et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, the OC-SVM training algorithm works by finding the maximum margin separation between the training points and the origin. Both approaches involve only inner products between pairs of data points, thus being easily kernelizable via the famous "kernel-trick" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. After training, an OC-SVM is able to provide an answer to the question: "Was this new data point generated by the same distribution that generated the training data, or is it an outlier?" In fact, using its soft output, an OC-SVM is also able to provide a soft (or fuzzy) answer to this question, in the form of a real number which expresses a degree of confidence with which a new data point belongs to the same 0031-3203/$ -see front matter © 2008 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2008.07.004 class (i.e., density) as the training data. The OC-SVM was proposed mainly to address (in a non-parametric way) problems of outlier (or novelty) detection.</p><p>The clustering scheme of Camastra and Verri is an iterative procedure, similar to the well-known K-means algorithm <ref type="bibr" target="#b12">[13]</ref>, which learns a set of K OC-SVMs. In each iteration, every data point is assigned to the "nearest" cluster, and then the clusters are updated using the assigned points. Specifically, the distance measure based on which the "nearest" cluster is found is provided by the OC-SVMs. For example, with the Tax and Duin OC-SVM, the "distance" between a point and a given cluster is simply the distance between that point and the center of the hyper-sphere of the corresponding OC-SVM. After all the points have been assigned to the clusters, each OC-SVM is retrained using only the corresponding points and the procedure is repeated until some convergence criterion is met. This corresponds to a hard clustering (K-means-type) scheme since, in each iteration, each pattern is assigned to a single cluster. It is known that iterative hard clustering schemes, namely K-means, are very sensitive to initialization. In Ref. <ref type="bibr" target="#b7">[8]</ref>, the authors skip this problem by manually initializing the method using a subset of points; the resulting method is thus not a fully unsupervised learning procedure.</p><p>Higher robustness to initialization is typically exhibited by soft clustering methods, such as finite mixture fitting using the expectation-maximization (EM) algorithm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which tend to be more effective than K-means in avoiding local minima of the underlying criteria. The robustness to initialization can be further improved by resorting to deterministic annealing strategies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, in which the smoothness of the assignments is initially very high and then slowly decreased until some desired value.</p><p>In this paper we present two contributions to OC-SVM-based clustering:</p><p>• We extend the concept of OC-SVM by defining a novel variant, which we call weighted OC-SVM (WOC-SVM). The WOC-SVM can be trained using a set of points and associated weights, where each weight indicates the importance to be given to the corresponding point. We define weighted versions of both the Tax and Duin and the Sch ölkopf et al. OC-SVM formulations. To the best of our knowledge, no method had been proposed for incorporating weights into OC-SVM training, although this problem had been already addressed in standard two-class (binary) SVMs; see, e.g., Ref. <ref type="bibr" target="#b15">[16]</ref>. • We introduce a soft clustering method based on the WOC-SVM, which can be considered as a soft version of the method of Camastra and Verri <ref type="bibr" target="#b7">[8]</ref>. The proposed method is similar to an EM algorithm for fitting a finite mixture, in which the density of each component is a function of the soft output of the corresponding WOC-SVM. The proposed approach depends on a scale parameter, which can be used to control the softness of the cluster assignments. This parameter opens the door to a deterministic annealing version of the basic scheme, which is shown to be more robust to initialization.</p><p>Experimental results on different UCI ML-Repository datasets demonstrate that the proposed soft clustering algorithm compares favorably with its hard clustering counterpart (an implementation of the method of Camastra and Verri <ref type="bibr" target="#b7">[8]</ref> with similar characteristics), particularly in terms of robustness with respect to initialization. Moreover, when compared with other state-of-the-art methods, its performance is very competitive.</p><p>The remaining sections of the paper are organized as follows. In Section 2, we introduce the weighted versions of the OC-SVM formulations of Tax and Duin and of Sch ölkopf et al. Section 3 contains the description of the new soft clustering algorithm based on the WOC-SVM as well as the description of its deterministic annealing variant; Section 4 reports experiments, and, finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Weighted One-Class Support Vector Machines</head><p>In this section, after briefly reviewing the OC-SVM formulations of Tax and Duin <ref type="bibr" target="#b9">[10]</ref> and of Sch ölkopf et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, namely to establish the necessary notation, we introduce the proposed weighted versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Tax and Duin OC-SVM</head><p>In the OC-SVM formulation of Tax and Duin <ref type="bibr" target="#b9">[10]</ref>, the goal is to find the smallest sphere containing the data points {x 1 , ... , x }, with some relaxation given by the so-called slack variables. This goal is formulated as a constrained convex optimization problem: min R,a, 1 ,...,</p><formula xml:id="formula_0">R 2 + C i=1 i s.t. x i -a 2 R 2 + i , i = 1, ... , , i 0, i = 1, . . . , ,<label>( 1 )</label></formula><p>where R and a are, respectively, the radius and center of the sphere,</p><p>• denotes the Euclidean norm, the i are the slack variables, and C is a trade-off parameter controlling how much the slack variables are penalized. The Wolfe dual of this problem is min 1 ,..., i=1</p><formula xml:id="formula_1">i x i , x i - i=1 j=1 i j x i , x j s.t. 0 i C, i = 1, ... , , i=1 i = 1,<label>(2)</label></formula><p>where •, • is the inner product, and { 1 , ... , } are Lagrange multipliers. Denoting the solution to problem (2) as * 1 , ... , * , the sphere center is given by a = i * i x i , thus the squared distance between a given test point x and a is</p><formula xml:id="formula_2">x -a 2 = x, x -2 i=1 * i x i , x + i=1 j=1 * i * j x i , x j ,<label>( 3 )</label></formula><p>i.e., it only involves inner products. Typically, the decision of whether x belongs to the same class as the training data or not is obtained by comparing xa 2 with some threshold. Clearly xa 2 can be seen as a squared distance measure between x and the class defined by the training data.</p><p>The fact that Eqs. ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_2">3</ref>) only depend on the data via inner products allows using the kernel-trick to obtain a kernelized version of the OC-SVM <ref type="bibr" target="#b9">[10]</ref>: simply replace all the inner products x i , x j in Eqs. ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_2">3</ref>) by the kernel function K(x i , x j ). In the kernel version, the hyper-sphere lives in a high (maybe infinite) dimensional space induced by the kernel <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Sch ölkopf et al. OC-SVM</head><p>In the OC-SVM formulation of Sch ölkopf et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, the idea is to find a hyper-plane w, x + = 0 that separates the data from the origin with maximal margin. This goal is also formulated as a convex problem:</p><formula xml:id="formula_3">min w, 1 ,..., , 1 2 w 2 + 1 i=1 i - s.t. w, x i -i , i = 1, ... , , i 0, i = 1, ... , ,<label>( 4 )</label></formula><p>where the i are the slack variables and controls the amount of penalization incurred by these slack variables. The corresponding Wolfe dual is</p><formula xml:id="formula_4">min 1 ,..., i=1 j=1 i j x i , x j s.t. 0 i 1/( ), i = 1, ... , , i=1 i = 1,<label>(5)</label></formula><p>where { 1 , ... , } are Lagrange multipliers. Denoting the solution of Eq. ( <ref type="formula" target="#formula_4">5</ref>) as * 1 , ... , * , and since w = i * 1 x i , the (directed) distance from a given point x to the separating hyper-plane is given by</p><formula xml:id="formula_5">d(x) = i=1 * i x i , x -. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>Parameter can be obtained from the fact that, for any * i such that 0 &lt; * i &lt; 1/( ), the corresponding data point</p><formula xml:id="formula_7">x i satisfies = j=1 * j x i , x j . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>The decision of whether some point x belongs to the same class as the training data is obtained by comparing d(x) with zero. The function d(x) can be seen as a measure of similarity between its argument and the learnt class. The fact that this formulation only involves inner products also allows for easy kernelization, simply by replacing each inner product x i , x j by the kernel function K(x i , x j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Weighted OC-SVM</head><p>The weighted version of the OC-SVM is able to take into account a set of weights {w 1 , ... , w }, where w i ∈ [0, 1], for i = 1, ... , , indicating the importance assigned to each point of the training set {x 1 , ... , x }.</p><p>Consider first the Tax and Duin formulation. The introduction of weights into the OC-SVM formulation is carried out by letting the penalty on slack variable i (which corresponds to the pattern x i ) be proportional to the weight w i . The rationale is straightforward: if a point x i has a small weight, w i &gt;1, the corresponding slack variable i has a small penalty, thus being able to have a large value, which will allow that point to be far from the center of the sphere, having a weak influence on its location and radius. With this modification, the optimization problem in Eq. ( <ref type="formula" target="#formula_0">1</ref>) becomes min R,a, 1 ,...,</p><formula xml:id="formula_9">R 2 + C i=1 w i i s.t. x i -a 2 R 2 + i , i = 1, . . . , , i 0, i = 1, ... , ,<label>( 8 )</label></formula><p>where the variables R, a, 1 , ... , , and C have the exact same meaning as in Eq. ( <ref type="formula" target="#formula_0">1</ref>). The Lagrangian for problem <ref type="bibr" target="#b7">(8)</ref> is given by</p><formula xml:id="formula_10">L(R, a, 1 , ... , , 1 , ... , , 1 , ... , )) = R 2 - i (R 2 + i -x i -a 2 ) i - i i i + C i i w i ,<label>( 9 )</label></formula><p>where 1 , ... , and 1 , ... , are the Lagrange multipliers associated with the two sets of constraints in Eq. ( <ref type="formula" target="#formula_9">8</ref>). Finally, the dual problem is obtained by minimizing L with respect to R, a, 1 , ... , , and 1 , ... , , which leads to min 1 ,..., i=1</p><formula xml:id="formula_11">i x i , x i - i=1 j=1 i j x i , x j s.t. 0 i w i C, i = 1, ... , , i=1 i = 1. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Notice that the objective function in Eq. ( <ref type="formula" target="#formula_11">10</ref>) is the same as the one in Eq. ( <ref type="formula" target="#formula_1">2</ref>); only the constrains are changed. In particular, each i is constrained to being less than w i C rather than C. This is in agreement with the desired behavior: a weight w i close to zero forces the corresponding i to also be close to zero, thus contributing very weakly to the definition of the distance between any point x and the center of the sphere, which is still given by Eq. ( <ref type="formula" target="#formula_2">3</ref>). This behavior is reenforced by the last constraint (the sum of the i has to be equal to one), since by limiting some of the i to small values forces the others to have larger values to keep the total sum equal to one.</p><p>Finally, notice that to guarantee that the feasible set is not empty, the weights have to satisfy i=1 w i 1 C , otherwise, even if each i equaled its allowed maximum w i C, their sum would be less than one. The exact same approach can be applied to the Sch ölkopf et al. formulation, leading to the dual problem min 1 ,...,</p><formula xml:id="formula_13">i=1 j=1 i j x i , x j s.t. 0 i w i /( ), i = 1, ... , , i=1 i = 1. (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>As in the previous case, the sole change is in the constraints and all the comments in the previous paragraph are still valid. Namely, to guarantee that the feasible set is not empty, the weights have to satisfy</p><formula xml:id="formula_15">i=1 w i .</formula><p>The distance function d(x) and the expression for are, of course, still given by Eqs. ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_7">7</ref>), where * 1 , ... , * are now the solution of Eq. <ref type="bibr" target="#b10">(11)</ref>.</p><p>Finally, as in the original (non-weighted) versions, kernelization is obtained by replacing each inner product x i , x j by the adopted kernel function K(x i , x j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The soft clustering scheme</head><p>In this section, the proposed soft clustering algorithm is presented. The key ideas are that each cluster is represented by one WOC-SVM and that the algorithm has an EM-type structure. After initialization (more on this below), the two following steps are cyclically repeated until some convergence criterion is met (or a maximum number of iterations is reached): the E-type step computes a degree of similarity between every data point and each cluster; the M-type step uses these degrees of similarity as weights to retrain the WOC-SVM representing the clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The similarities</head><p>In the standard EM algorithm for a finite mixture model, the class likelihood represents the "similarity" between some point x and the class, namely how well the point x is modelled by a specific component of the mixture. Likewise, in our algorithm we need a function measuring the closeness of a point to a cluster, given that the cluster is modelled with the WOC-SVM. Of course, unlike in a finite mixture model, the "mixture" of WOC-SVM does not correspond to a generative model of the data. Although the WOC-SVM is intrinsically a classifier (with a binary output), we can consider the "soft" output (namely the output before thresholding) as a measure of the degree with which a given point belongs to the corresponding cluster. The notion that the similarities should behave similarly to cluster likelihoods suggests that the soft outputs of the WOC-SVM can be used as follows to obtain these similarities/"likelihoods".</p><p>• In the Tax and Duin formulation, the WOC-SVM can provide the squared distance between a point x and the center of its sphere, via Eq. ( <ref type="formula" target="#formula_2">3</ref>) or its kernelized version. A natural choice for the similarity between point x and cluster k is thus</p><formula xml:id="formula_16">S TD (x, k) = exp - x -a k 2 , (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where a k is the center of the sphere of the WOC-SVM representing cluster k, and the subscript TD in S TD stands for "Tax and Duin".</p><p>Parameter controls how the similarity scales with the distance, much as the variance of a Gaussian density; its impact will be analyzed in Section 3.3. • In the Sch ölkopf et al. formulation, the WOC-SVM provides, via Eq. ( <ref type="formula" target="#formula_5">6</ref>) or its kernel version, the signed distance between some point x and the hyper-plane associated with cluster k. Since positive (respectively, negative) values correspond to high (respectively, low) similarities, a natural choice for the similarity between point x and cluster k is thus</p><formula xml:id="formula_18">S S (x, k) = exp d k (x) ,<label>(13)</label></formula><p>where d k is the signed distance function ( <ref type="formula" target="#formula_5">6</ref>) corresponding to the WOC-SCM of the k-th cluster and the subscript S in S S stands for "Sch ölkopf et al". Again controls the scale of the distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The algorithm</head><p>With the similarity functions defined above, we are now ready to describe the soft clustering algorithm in detail. Consider a set of points, {x 1 , ... , x }, to be clustered in K groups. The proposed algorithm proceeds as follows:</p><p>• Initialization: For k = 1, ... , K, initialize the k-th WOC-SVM; for example, train each WOC-SVM using all points as positive examples, each with a different random weight. Three different initialization methods are described in the experimental section. Similarly to the EM algorithm for finite mixtures, we need to employ in the algorithm the mixing coefficients k , which we call "cluster weights"; such weights are initialized as k = 1/K, for k = 1, ... , K. • E-type step: Compute the cluster membership weights</p><formula xml:id="formula_19">z k i = k S(x i , k) K r=1 r S(x i , r) for i = 1, ... , , k = 1, . . . , K,<label>(14)</label></formula><p>where S(x i , k) is given by Eq. ( <ref type="formula" target="#formula_16">12</ref>) or ( <ref type="formula" target="#formula_18">13</ref>), computed with the current WOC-SVM of each cluster, and k is the current estimate of the weight of cluster k. Notice that, naturally, k z k i = 1. • M-type step: For k = 1, ... , K, update the k-th WOC-SVM by training it using as positive examples all the points in the dataset, each weighted according to the set of weights {z k 1 , ... , z k }, and update the cluster weight parameters according to</p><formula xml:id="formula_20">k = 1 i=1 z k i . (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>Notice that this definition guarantees, exactly as in EM for mixtures, that the k parameters sum to one:</p><formula xml:id="formula_22">K k=1 k = 1 K k=1 i=1 z k i = 1 i=1 K k=1 z k i 1 =1.</formula><p>• Stopping criterion: If a maximum number of iterations has been reached, or some other convergence criterion is satisfied, stop; otherwise, go back to the step.</p><p>In our implementation, we used as stopping criterion the convergence of the following function f LL , which takes inspiration from the log-likelihood computed in the mixture EM case:</p><formula xml:id="formula_23">f LL = i=1 log ⎛ ⎝ K k=1 k S(x i , k) ⎞ ⎠ . (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>If wanted, a final hard partition can be obtained by assigning each point x i to the cluster arg max</p><formula xml:id="formula_25">k {z k i , k = 1, ... , K},</formula><p>or simply to the closest cluster.</p><p>We would like to stress again that although we have used EM terminology and our algorithm has an EM-type structure, it is not an EM algorithm, namely due to the absence of an underlying probabilistic generative model. For this same reason, we cannot directly import the monotonicity and convergence properties of EM to our algorithm. In the experiments described in the next section, the algorithm almost always converged. In some rare cases, the function continued to oscillate between two values; this behavior is currently under study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The scale parameter and deterministic annealing</head><p>This section investigates the effect on the proposed algorithm of the scale parameter of Eqs. ( <ref type="formula" target="#formula_18">13</ref>) and ( <ref type="formula" target="#formula_16">12</ref>), introducing a variant of the presented clustering scheme exploiting that effect.</p><p>First of all, it is important to notice that controls the "softness" of the clustering scheme. Let us clarify this statement: a soft clustering scheme, by definition, assigns to each pattern a membership vector, which describes its similarities to every cluster. On the other hand, a hard clustering algorithm assigns a pattern just to one cluster (i.e., uses membership vectors with just one non-zero entry). Consider the membership vector for pattern i, obtained by the E-type step, denoted as z i ( ) = [z 1 i ( ), ... , z K i ( )] (where we have used notation that explicitly indicates the dependency on ). Now, noting that we can write exp -</p><formula xml:id="formula_26">x -a 2 = (exp{-x -a 2 }) 1/ , exp d k (x) = (exp{d k (x)}) 1/ , it is easy to show that lim →∞ [z 1 i ( ), ... , z K i ( )] = [ 1 , ... , K ] and lim →0 z k i ( ) = 1 if k = arg max j S(x i , j), 0 otherwise.</formula><p>These limits show that when is very large, the memberships are essentially controlled by the cluster weights k , whereas when becomes very small the membership vector specifies a hard partition. The above described effect suggests the use of a determinist annealing version of the algorithm, controlled by the scale parameter , which is potentially able to improve robustness with respect to initialization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The basic idea is to start the algorithm with a large : in this case the algorithm is working in a regime where the clustering is very soft (since the cluster weights are initialized to k = 1/K), so there are no local minima. Subsequently, is gradually decreased, thus hardening the assignment vectors. A similar approach has been exploited in Ref. <ref type="bibr" target="#b16">[17]</ref>, in order to improve the robustness of the standard EM with respect to initialization, and in Ref. <ref type="bibr" target="#b17">[18]</ref> for vector quantization (equivalently K-means) algorithms.</p><p>The application of this idea to our algorithm leads to the insertion of an outer loop (the annealing loop), as follows:</p><p>(1) Initialization: as in the EM-like algorithm described in Section 3.2. (2) set = max ;</p><p>(3) repeat E-type and M-type steps until convergence; (4) decrease ;</p><p>(5) if &gt; min go to step (3), otherwise stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Basic algorithm: fixed</head><p>The experimental evaluation was based on five well-known real datasets: the Iris dataset, the Wisconsin breast cancer (referred to as WSC), the Wine data set-all from the USC Machine Learning Repository<ref type="foot" target="#foot_0">1</ref> -the Pima Indian diabetes dataset<ref type="foot" target="#foot_1">2</ref> (referred to simply as Pima), and the Biomed dataset. <ref type="foot" target="#foot_2">3</ref>The analyzed methods were implemented in Matlab, except the OC-SVM and WOC-SVM training algorithms. For the OC-SVM, we have used the LIBSVM software <ref type="bibr" target="#b18">[19]</ref>, which implements the version of Sch ölkopf et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The training algorithm for the WOC-SVM was obtained by modifying the OC-SVM code from LIBSVM. When computing the similarity (13), the scale parameter was set to 1.</p><p>In the experiments, we compare our soft clustering method (which we named WOC-SVM soft clustering -WOC-SVM SC) with its hard clustering counterpart-which we call OC-SVM hard clustering (OC-SVM HC). In particular, in order to have a precise and fair comparison with our algorithm, we slightly adapted the algorithm in Ref. <ref type="bibr" target="#b7">[8]</ref> to our case. The main modifications were: first, in our implementation we did not use the parameter , which was used in Ref. <ref type="bibr" target="#b7">[8]</ref> to discard the elements that are too distant in feature space. Second, in Ref. <ref type="bibr" target="#b7">[8]</ref> the initialization was obtained by manually assigning some points to each cluster, whereas in the OC-SVM HC two different automatic initializations were introduced: one initializes each OC-SVM using random points (called "Random Init" in the table), while in the other (called "K-means init" in the table) a preliminary K-means algorithm is run, and the points assigned to each cluster are used to initialize the corresponding OC-SVM. All the experiments use a standard Gaussian radial basis function (GRBF) kernel of width . Different values for parameters and have been tested, choosing those leading to the best results. In particular, for each dataset, the optimal pair ( , ) was the same for all the kernel methods: (0.85, 0.97) for Iris; (0.2, 0.99) for WBC; (0.01, 0.98) for Wine; (0.1, 0.69) for Pima and (0.002, 0.95) for Biomed.</p><p>All the algorithms were run 20 times and we report in Tables <ref type="table">1</ref> and<ref type="table">2</ref> the average, the minimum, and the maximum accuracies obtained over these 20 runs, for the proposed technique and for the corresponding hard clustering version, respectively. Since true labels are known, clustering accuracies could be quantitatively assessed. In particular, given a specific group, an error is considered when a pattern does not belong to the most frequent class inside the group. For the proposed soft clustering method (results shown in Table <ref type="table">1</ref>), we have considered several initialization methods: "Rand Pnts Init", in which each WOC-SVM is initialized using as positive examples a randomly selected subsect of points; "Rand Wghts Init", in which each WOC-SVM is initialized using as positive examples all points with different random weights; and "GMM Init", in which a standard Gaussian mixture model is used to find the clusters, and the likeli-Table <ref type="table">1</ref> Percentage accuracies of the experimental evaluation for the proposed WOC-SVM SCscheme, for three different initialization: initialization using random points ("Rand Pnts Init"), using random weights ("Rand Wghts Init") and using GMM clustering ("GMM Init")  hoods are used as initial weights. From the tables it can be concluded that for random initializations, the proposed approach outperforms its hard version in all dataset (except Pima where it performs in line). Further, for clever initializations (K-means or GMM), the proposed method performs in line (in three cases) or above the hard clustering scheme (in two cases). Moreover, the differences between the maximum and minimum accuracies show that the proposed soft clustering algorithm is more stable and robust with respect to initialization (except in the Pima case). With three of these datasets (Iris, WBC and Wine), it seems that all initializations methods work equally well. For the other two, clever initializations lead to more stable results. The sensitivity of the hard clustering scheme with respect to initialization is confirmed by comparing results on Iris and WBC in Table <ref type="table">2</ref> with the results reported in Ref. <ref type="bibr" target="#b7">[8]</ref>, where the variations were lower than those we found; recall that, in their method, a manual initialization was used. As a general comment, we have to say that parameter settings for kernel methods was quite difficult, confirming the need of proper model selection techniques.</p><p>In order to further assess the performance of the proposed algorithm in comparison with other methods, we include in Table <ref type="table" target="#tab_1">3</ref> the average accuracy values reported in Ref. <ref type="bibr" target="#b7">[8]</ref>, obtained for the Iris and WBC datasets by other recent state-of-the-art algorithms: selforganizing maps (SOM) <ref type="bibr" target="#b19">[20]</ref>, neural gas <ref type="bibr" target="#b20">[21]</ref>, the Ng-Jordan spectral clustering algorithm <ref type="bibr" target="#b21">[22]</ref>. Results of the classical GMM-based clustering and K-means clustering are also reported. It can be observed that the proposed approach is very competitive on these two datasets, achieving the best performance for WBC and the second best for Iris. All the experimental conditions are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deterministic annealing version</head><p>In order to assess the suitability of the presented deterministic annealing version of the algorithm, we repeated the experiments described in the previous subsection, using the same configuration; note that, in order to have a fair comparison, exactly the same initializations were used by both versions of the algorithm. We set max = 5, min = 0.1, and the update rule of step 4 as ← * 0.95. The results obtained are reported in Table <ref type="table" target="#tab_2">4</ref>.</p><p>Comparing Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table">1</ref>, we can observe that in general there is not a substantial improvement in the averaged accuracies. Nevertheless, note that the minimum accuracy is almost always higher when applying the deterministic annealing version, thus showing an increased robustness with respect to initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding remarks and ongoing work</head><p>In this paper we have introduced a soft clustering algorithm based on one-class SVM (OC-SVM) representations of the clusters. The proposed method is based on a weighted version of the OC-SVM (termed WOC-SVM) which we have also introduced in this paper, and is inspired by the OC-SVM-based hard clustering algorithm proposed in Ref. <ref type="bibr" target="#b7">[8]</ref>. Due to the use of WOC-SVM, the algorithm is directly kernelizable, thus constituting a kernel-based soft clustering method. Experimental results reported have shown that the proposed soft clustering algorithm outperforms the hard clustering counterpart, namely in what concerns robustness with respect to initialization. The proposed algorithm performs competitively with several stateof-the-art methods, including the spectral clustering algorithm of Ng and Jordan <ref type="bibr" target="#b21">[22]</ref>. A deterministic annealing version has been also proposed, and shown to be able to provide robustness with respect to initialization.</p><p>As with most kernel-based methods, the performance of our method depends critically on the choice (and tuning of parameters) of the kernel. We are currently investigating approaches to adjust the kernel to the data in a more automatic way. Another research front concerns the development of model selection criteria under which the algorithm can select the number of clusters in the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Average accuracies for different methods on the Iris and WBCdatasets</figDesc><table><row><cell>Method</cell><cell>Iris (%)</cell><cell>WBC (%)</cell></row><row><cell>Self organizing maps a</cell><cell>81.0</cell><cell>96.7</cell></row><row><cell>Neural gas a</cell><cell>91.7</cell><cell>96.1</cell></row><row><cell>Spectral clustering a</cell><cell>84.3</cell><cell>95.5</cell></row><row><cell>K-means a</cell><cell>89.0</cell><cell>96.1</cell></row><row><cell>GMM</cell><cell>89.3</cell><cell>94.6</cell></row><row><cell>Camastra-Verri a</cell><cell>94.7</cell><cell>97.0</cell></row><row><cell>Proposed method</cell><cell>93.3</cell><cell>97.1</cell></row></table><note><p><p><p>a The results are taken from Ref.</p><ref type="bibr" target="#b7">[8]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Percentage accuracies of the experimental evaluation for the deterministic annealing variant of the proposed WOC-SVM SC scheme, described in Section 3.3</figDesc><table><row><cell>Dataset</cell><cell>Accuracy (max-min)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Rand Pnts Init</cell><cell>Rand Wghts Init</cell><cell>GMM Init</cell></row><row><cell>Iris</cell><cell>93.3% (93.3-93.3%)</cell><cell>93.3% (93.3-93.3%)</cell><cell>93.3% (93.3-93.3%)</cell></row><row><cell>WBC</cell><cell>97.1% (97.1-97.1%)</cell><cell>97.1% (97.1-97.1%)</cell><cell>97.1% (97.1-97.1%)</cell></row><row><cell>Wine</cell><cell>96.1% (96.1-96.1%)</cell><cell>96.1% (96.1-96.1%)</cell><cell>96.1% (96.1-96.1%)</cell></row><row><cell>Pima</cell><cell>70.0% (71.0-69.0%)</cell><cell>70.0% (70.5-69.0%)</cell><cell>70.0% (70.0-70.0%)</cell></row><row><cell>Biomed</cell><cell>81.3% (85.6-70.1%)</cell><cell>80.7% (86.1-69.6%)</cell><cell>85.6% (85.6-85.6%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at http://archive.ics.uci.edu/ml/datasets.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Available at www.stats.ox.ac.uk/pub/PRNN/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Available at http://lib.stat.cmu.edu/datasets/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would thank Annalisa Barla for initial discussions on WOC-SVM and Cheng Dong Seon for precious help in adapting the LISBSVM code to the weighted case. The authors would also thank the anonymous reviewers for their helpful comments.</p><p>M. Figueiredo was partially supported by Fundação para a Ciência e Tecnologia (FCT), Portuguese Ministry of Science and Higher Education, under Grant POSC/EEA-SRI/61924/2004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<title level="m">Kernel Methods for Pattern Analysis</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector clustering through proximity graph modelling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Estivill-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chalup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An improved cluster labeling method for support vector clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel kernel method for clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="801" to="805" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic characterization of cluster structures for robust and inductive support vector clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1869" to="1874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support vector domain description</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syt</title>
		<imprint>
			<biblScope unit="page" from="526" to="532" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1443" to="1447" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of finite mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="381" to="396" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. (B)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge with weighted margin support vector machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeedings of the ACM International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>eeedings of the ACM International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deterministic annealing EM algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="271" to="282" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deterministic annealing for clustering, compression, classification, regression, and related optimization problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2210" to="2239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">Software</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural-gas network for vector quantization and its application to time-series prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="558" to="569" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">He has served as a member of the scientific committee of different international conferences, and he is a reviewer for several international conferences and journals. About the Author-MÁRIO FIGUEIREDO holds a PhD (1994) from IST, Technical University of Lisbon, and is now a professor at IST and a researcher at Instituto de Telecomunicações. He is an associate editor of several journals, including IEEE-TPAMI and IEEE-TIP, and was a guest editor of several special issues</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Now he is a researcher at the University of Sassari. His research interests include statistical pattern recognition, hidden Markov models, video analysis and biometrics. Since 2004 he is an associate editor of the ELCVIA international journal, and he was a guest editor of the special issue of Pattern Recognition on. He co-chaired several workshops and has been in program committees of many conferences, including NIPS, ICML, CVPR, EECV, ICIP, ICPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
