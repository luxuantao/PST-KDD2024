<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Capacity of Discrete-Time Memoryless Rayleigh-Fading Channels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ibrahim</forename><forename type="middle">C</forename><surname>Abou-Faycal</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Mitchell</forename><forename type="middle">D</forename><surname>Trott</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shlomo</forename><surname>Shamai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">Laboratory for Information and Decision Systems</orgName>
								<orgName type="institution">Massa-chusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">VANU Inc</orgName>
								<address>
									<postCode>02140</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<addrLine>Cam-bridge</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">MA. He is now with ArrayComm Inc</orgName>
								<address>
									<postCode>95131</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Technion-Is-rael Institute of Technology</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Capacity of Discrete-Time Memoryless Rayleigh-Fading Channels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77C6BAAB5FC38B2C3C463D962E4EC657</idno>
					<note type="submission">received December 29, 1997; revised April 15, 1999.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Capacity</term>
					<term>fading channels</term>
					<term>memoryless fading</term>
					<term>Rayleigh fading</term>
					<term>time-varying channels</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider transmission over a discrete-time Rayleigh fading channel, in which successive symbols face independent fading, and where neither the transmitter nor the receiver has channel state information. Subject to an average power constraint, we study the capacity-achieving distribution of this channel and prove it to be discrete with a finite number of mass points, one of them located at the origin. We numerically compute the capacity and the corresponding optimal distribution as a function of the signal-to-noise ratio (SNR). The behavior of the channel at low SNR is studied and finally a comparison is drawn with the ideal additive white Gaussian noise channel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>with mean zero and variance and , respectively. Equivalently, the amplitude of the fading coefficient is Rayleighdistributed and its phase is uniform. The discrete time index is designed by and we assume that and are sequences of independent and identically distributed (i.i.d.) random variables . The input is average power limited: . Neither the transmitter nor the receiver knows the value of or , but both are assumed to know their statistics exactly.</p><p>Perhaps surprisingly, this basic channel is not as well understood as the discrete-time additive white Gaussian noise channel. In a 1969 technical report <ref type="bibr" target="#b0">[1]</ref>, Richters conjectured that the capacity-achieving input distribution for this channel is discrete, a rather unexpected result for a continuous-alphabet channel under an average power constraint. The conjecture was motivated by analytical arguments but not rigorously proved. The main result of the present paper is a proof of Richters' conjecture. As is the case for the peak-amplitude-constrained Gaussian channels studied by Smith <ref type="bibr" target="#b1">[2]</ref> and later by Shamai and Bar-David <ref type="bibr" target="#b3">[4]</ref>, the input distribution that achieves capacity for the fading channel is discrete with a finite number of mass points. The same result holds for the th-order Rayleigh diversity channel with independent branches, as proved in Appendix III. Another relevant reference to this study is Telatar's work <ref type="bibr" target="#b4">[5]</ref> on the capacity and error exponent of this channel for infinite bandwidth.</p><p>The model ( <ref type="formula">1</ref>) is appropriate in a number of scenarios. For example, a memoryless fading model is reasonable when a narrow-band signal is hopped rapidly over a large set of frequencies, one symbol per hop. The communication security gained by fast hopping comes at the price of decreased capacity; the results developed here help quantify this tradeoff. The model is also appropriate for a slowly time-varying channel in which successive symbols are widely separated in time, as might arise when transmitting opportunistically during guard times in a packet-based system, or when using information-bearing pilot tones for both channel identification and communication. A final motivation for studying this model is to complement the results that apply when fading information is available to the receiver only <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> or to both receiver and transmitter <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Let us note, however, that understanding the memoryless fading channel is only a small step toward understanding fading channels in their most general form. In particular, symbol-rate sampling of a rapidly varying continuous-time fading channel does not lead to the model <ref type="bibr" target="#b0">(1)</ref>. If the time variation in a continuous-time channel is fast enough to cause independent fading from symbol to symbol, there will be significant variation within each symbol, and the output bandwidth will be much larger than the input bandwidth. In such cases, the conversion from continuous to discrete time must be done with care, for example, using several samples per symbol at the channel output. We do not consider this problem here.</p><p>The outline of the paper is as follows. In Section II, we derive a simpler mathematical model for this channel. In Section III, we write the Kuhn-Tucker condition for the capacity-achieving input random variable. We prove, in Section IV, the discrete character of the optimal input, and in Section V, we prove that it has necessarily a mass point at zero. In Section VI, numerical results for the fading channel are compared to the ideal additive white Gaussian channel. Discussions and conclusions in Section VII terminate the paper, and detailed proofs are given in Appendixes I-III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE MODEL</head><p>Since the channel ( <ref type="formula">1</ref>) is stationary and memoryless, the capacity achieving statistics of are also memoryless (i.i.d.), and hence with no loss of generality we suppress the time index.</p><p>Conditioned on the input , the output of the channel (1) is a complex circular Gaussian random variable (since and are independent, and consequently jointly Gaussian) with density <ref type="bibr" target="#b1">(2)</ref> Because the phase of the fading parameter is uniform, the conditional output density involves only the squared amplitude of . Conditioned on the input, the sufficient statistic has a central chi-square distribution with two degrees of freedom <ref type="bibr" target="#b2">(3)</ref> Letting and , we obtain an equivalent channel with nonnegative input , nonnegative output , transition probability <ref type="bibr" target="#b3">(4)</ref> and average power constraint , where . In other words, the output density is conditionally exponential with mean modulated by the channel input.</p><p>Since appears in the above equations only via its square, it is convenient to make the invertible change of variables , so that <ref type="bibr" target="#b4">(5)</ref> with the constraint . Proving that the optimal input is a discrete random variable is equivalent to proving that is discrete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE KUHN-TUCKER CONDITION</head><p>The capacity of an input-constrained memoryless channel is the supremum <ref type="bibr" target="#b5">(6)</ref> of the mutual information between the input and output over all input distribution functions that meet the constraint , where <ref type="bibr" target="#b6">(7)</ref> is the marginal output density induced by . The input may include a mixture of discrete and continuous parts. However, (6) applies only when there exists a conditional density for the channel output given the input, a requirement automatically met for channels with absolutely continuous additive noise.</p><p>For a channel with a continuous alphabet, the supremum in (6) need not be achievable. A sufficient condition for achievability is that there exist a topology for which i) mutual information is continuous in the input distribution function, and ii) the set of input distribution functions that meet the constraint is compact. We establish continuity and compactness for the average-power-constrained fading channel (1) in Appendix I. Using strict convexity, we further show that capacity is achieved by a unique input distribution (ignoring phase).</p><p>The maximization ( <ref type="formula">6</ref>) is not readily solved using the calculus of variations, a methodology better suited to spaces of continuous functions than distribution functions. Instead, in Appendix II (following Smith <ref type="bibr" target="#b1">[2]</ref>) we use the theory of convex optimization to show that an input random variable with distribution function achieves the capacity of an average power-limited channel if and only if there exists a such that <ref type="bibr" target="#b7">(8)</ref> for all , with equality if is in the support of . Equation (8) applies to any average power constrained channel for which mutual information is weak* continuous and weakly differentiable.</p><p>For channels over finite alphabets, (8) reduces to [10, Theorem 4.5.1], with the addition of a Lagrange multiplier arising from the power constraint. Similar generalizations from discrete to continuous alphabets exist for rate distortion theory. (See, for example, <ref type="bibr" target="#b10">[11]</ref> or the summary in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Sec. 4.5]</ref>.) A virtue of ( <ref type="formula">8</ref>) is its exclusive use of densities; the input distribution function appears only indirectly through the output density and the support of . Using the conditional density ( <ref type="formula">5</ref>), ( <ref type="formula">8</ref>) becomes <ref type="bibr" target="#b8">(9)</ref> for all</p><p>. Expanding the term inside the integral yields <ref type="bibr" target="#b9">(10)</ref> for all , with equality if is in the support of . Equation <ref type="bibr" target="#b9">(10)</ref> will be referred to as the "Kuhn-Tucker condition."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE DISCRETE CHARACTER OF</head><p>The optimal input must have exactly one the following properties:</p><p>1) its support contains an interval;</p><p>2) it is discrete, with an infinite number of mass points on some bounded interval; 3) it is discrete and infinite, but with only a finite number of mass points on any bounded interval; or 4) it is discrete with a finite number of mass points.</p><p>We do not have a direct proof that case 4) prevails. Instead, following Smith <ref type="bibr" target="#b1">[2]</ref> and Shamai and Bar-David <ref type="bibr" target="#b3">[4]</ref>, we use the Kuhn-Tucker condition to prove that the first two cases are impossible. We then rule out the third possibility (which does not arise for the peak limited channels considered in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>) using a lower bounding argument. The existence and uniqueness of consequently proves that is discrete with a finite number of mass points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Positive Accumulation Point</head><p>Assume now that case 1) or 2) holds. Then the support of includes a bounded infinite set of distinct points . Equivalently, the support of includes an infinite set of distinct points . The interval is compact, hence by the Bolzano-Weierstrass theorem the set has an accumulation point in . Let us extend the left-hand term of ( <ref type="formula">10</ref>) to the complex domain. Write as , and define <ref type="bibr" target="#b10">(11)</ref> where is the principal branch of the logarithm <ref type="bibr" target="#b12">[13]</ref>. Given this choice, the function is analytic over the domain defined by . On the interval of the real axis, the Kuhn-Tucker condition <ref type="bibr" target="#b9">(10)</ref> states that the function is zero if is in the support of . Therefore, is zero on . We have thus an analytic function over a domain that is zero over a set having a point of accumulation in . The identity theorem <ref type="bibr" target="#b13">[14]</ref> states that is zero over the whole domain . In other words, the Kuhn-Tucker condition <ref type="bibr" target="#b9">(10)</ref> holds with equality for all real , and more generally for all complex . Let us examine carefully the consequences of this result. Equation ( <ref type="formula">10</ref>) can be rewritten as <ref type="bibr" target="#b11">(12)</ref> for all</p><p>. The left-hand side (LHS) is the unilateral Laplace transform of the function , while the right-hand side (RHS) can be recognized as the Laplace transform of <ref type="bibr" target="#b12">(13)</ref> where is Euler's constant. The uniqueness of the Laplace transform for continuous functions of bounded variations <ref type="bibr" target="#b12">[13]</ref> implies that <ref type="bibr" target="#b13">(14)</ref> Therefore, the only function that satisfies ( <ref type="formula">12</ref>) is <ref type="bibr" target="#b14">(15)</ref> where But for any value of the integral over of ( <ref type="formula">15</ref>) is infinite, hence it cannot be a probability density.</p><p>By contradiction, the original assumption on is wrong. The only possibilities remaining are cases 3) and 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Accumulation Point at Zero</head><p>Assume now that case 3) holds, so that is discrete with infinitely many mass points, but with only a finite number in any bounded interval. Equivalently, assume that has discrete support with an accumulation point only at zero. A random variable has at most a countable number of mass points, therefore, the support of can be written as a sequence converging to .</p><p>Let . Then the output probability density is <ref type="bibr" target="#b15">(16)</ref> which implies the obvious lower bound for all and . Therefore,</p><p>for all . Using <ref type="bibr" target="#b16">(17)</ref>, the LHS of ( <ref type="formula">10</ref>) may be lower-bounded as</p><formula xml:id="formula_1">LHS (<label>18</label></formula><formula xml:id="formula_2">) (<label>19</label></formula><formula xml:id="formula_3">)</formula><p>where the term applies as for any fixed . If then this lower bound diverges to as . But the LHS of (10) equals zero on the support of , which, by assumption, contains a point of accumulation at . By contradiction, . The above analysis is valid for all ; since this implies . The Lagrange multiplier in <ref type="bibr" target="#b9">(10)</ref> is nonnegative (see Appendix II), hence we conclude that if the support of has an accumulation point at zero then is zero.</p><p>A Lagrange multiplier is zero when the corresponding constraint is inactive, or, more precisely, when the sensitivity to a change in the constraint is zero. For the fading channel, it means that the power constraint is inactive, which is not sensible.</p><p>The impossibility of can be reasoned more precisely as follows. The capacity of a channel increases monotonically with the power constraint ; the concavity of mutual information in the input distribution (and the linearity of the power constraint) implies that is also concave. The Lagrange multiplier corresponding to a particular capacity-achieving input distribution with power may be interpreted as the slope of a line tangent to at . Thus, by convexity and monotonicity, if for some power constraint then for all . To demonstrate the impossibility of it, therefore, suffices to find a family of input distributions with strictly monotonically increasing mutual information. Such a family may be constructed in any number of ways. One approach is to use a discrete uniform input distribution with input levels located at . By taking sufficiently large it can be shown that the probability of error for a simple minimum-probability-of-error receiver goes to zero, hence, by Fano's inequality, the mutual information approaches . We have assumed that is discrete with an infinite number of mass points, but with only finitely many mass points in any bounded interval. We proved that this is possible only if the Lagrange multiplier in the Kuhn-Tucker condition ( <ref type="formula">10</ref>) is zero. But this is impossible, consequently, the optimal distribution is discrete with a finite set of mass points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. THE EXISTENCE OF AN IMPULSE AT ZERO</head><p>The next natural task is to locate the mass points of . A closed-form solution seems unlikely. However, in this section, we will prove by contradiction that the optimal input random variable has necessarily a mass point at zero. This is not surprising: a zero-level input is "good" for the power constraint and results in the smaller variance at the output and should be preferred to other levels.</p><p>Because is discrete, it has a distribution function where and is the unit step. Assume now that contains no mass point at zero, i.e., . Let us fix the 's and and move downwards. Clearly, the power constraint becomes looser. Proving that the mutual information increases is therefore sufficient to prove that the original density is suboptimal. To establish this result rigorously is a matter of algebra.</p><p>For a discrete input , the mutual information between and is <ref type="bibr" target="#b19">(20)</ref> Differentiating with respect to yields <ref type="bibr" target="#b20">(21)</ref> To simplify (21) further, differentiate (4) <ref type="bibr" target="#b21">(22)</ref> and define . Then ( <ref type="formula">23</ref>)</p><p>Lemma 1: Let be a probability density function with mean . If is strictly monotonically decreasing then <ref type="bibr" target="#b23">(24)</ref> Proof: The function is strictly decreasing, hence is a product of two positive terms for and is a product of two negative terms for . Thus, for , and</p><p>because is the mean of .</p><p>To apply Lemma 1, notice that the mean of is . To show that is monotone, write (26) Since , we have</p><p>The exponentials in <ref type="bibr" target="#b25">(26)</ref> all have positive exponents, hence the ratio -an average of increasing functions-is an increasing function. Therefore, the argument of the logarithm in is decreasing and so is . It follows that the derivative of with respect to is negative for . Thus, the postulated distribution with cannot be a local extremum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. NUMERICAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Capacity Curves</head><p>Having characterized the optimal probability distribution, we can now compute numerically the capacity of an i.i.d. Rayleigh-fading channel as a function of the power constraint. This was attempted previously by Richters <ref type="bibr" target="#b0">[1]</ref>, but more powerful numerical techniques and computational tools are now available and more precise results may be obtained. Moreover, the conjectured capacity-achieving distribution was not provided in Richters' work, and how this distribution varies as a function of the power constraint at low and high signal-to-noise ratios (SNRs) is of some interest.</p><p>To find the capacity as a function of the power constraint , we introduce a Lagrange multiplier and maximize the functional over all input random variables . The multiplier is the slope of a line tangent to the curve at , so by varying between and (which equals when the fading and noise variance are normalized to ) the full curve may be found. The optimal input is discrete with a finite set of mass points, hence for each we must maximize over the number of mass points , their probabilities , and their locations , subject to the constraints for all , and . Unfortunately, though the optimization problem is convex over the space of all input distribution function, there is no reason to believe it remains convex when parameterized by the mass point probabilities and locations. Furthermore, we have no bounds on the number of mass points needed as a function of the power constraint. As a practical matter, these potential difficulties do not arise. Because mutual information is continuous and strictly concave in the input distribution, the optimal input distribution function changes continuously (in the weak* topology) with the SNR . Also, we have found empirically that two mass points are optimal for low SNR and the required number of mass points increases monotonically with SNR. Strong evidence for these conclusions comes from the Kuhn-Tucker condition <ref type="bibr" target="#b9">(10)</ref>, which, being necessary and sufficient for optimality, allows us to establish (up to the resolution of the numerical algorithms) that a local maximum found by a descent method is in fact global.</p><p>To apply the Kuhn-Tucker test to a postulated and , we first compute and , then plot the LHS of (8) as a function of . The resulting graph must be nonnegative and must touch zero at the atoms of , as in Fig. <ref type="figure" target="#fig_0">1</ref>. The curve and the mass point locations and probabilities were computed using a gradient descent method, together with Gauss-Laguerre quadrature to evaluate the necessary integrals. Projected gradients were used to keep the mass point probabilities positive. An alternative optimization method, using a quantized version of Arimoto-Blahut, was too slow to be useful.</p><p>Low SNR: For low values of the SNR the capacityachieving input distribution has only two mass points, and, therefore, amounts to "on-off" keying. One point is always located at zero; the other is easily found with standard one-dimensional optimization techniques, because the power constraint determines the mass point probability as a function of its location. The Kuhn-Tucker condition verifies that the optimized two-point distribution achieves capacity. The optimal mass point locations and probabilities are plotted as a function of in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>As decreases, the probability of the nonzero mass point approaches zero, while its amplitude increases, albeit quite slowly. That the amplitude increases to infinity as is proved by Gallager <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Theorem 8.6.1]</ref>.</p><p>Higher SNR: As SNR increases, does a new mass point appear apart from the others, or does an existing mass point split in two? Fix <ref type="bibr" target="#b14">[15]</ref> considered an analogous problem in rate distortion theory, and concluded from plots of a quantity comparable to Fig. <ref type="figure" target="#fig_0">1</ref> that both possibilities arise. The question of splitting for rate distortion theory was also studied (but not solved) by Rose <ref type="bibr" target="#b15">[16]</ref>, whose results can also be shown by techniques similar to those in <ref type="bibr" target="#b1">[2]</ref>.</p><p>From a numerical standpoint this question is quite challenging, as mutual information is insensitive to the location of a new mass point with near-zero probability. Some insight can be gained from the Kuhn-Tucker condition, which for our problem suggests that mass points never split and that new mass points appear initially at . For our problem, when the SNR reaches a level where a new mass point is needed, the asymptote of the quantity plotted in Fig. <ref type="figure" target="#fig_0">1</ref> switches suddenly from to . We conjecture that the analysis that applies to the case can be modified to prove that all new mass points enter at .</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the capacity (in nats per channel use) of the i.i.d. Rayleigh-fading channel as a function of the power constraint . Figs. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref> show the locations and probabilities of the mass points in the capacity-achieving input distribution. The mass point with lowest probability has the highest amplitude, the mass point with the second lowest probability has the second highest amplitude, and so on. The dashed segments in the figures are conjectured curves; numerical optimization became unstable when the lowest mass point probability dropped below . Interestingly, the location of a new mass point initially moves downward as increases, then moves upward. This peculiar behavior has little engineering consequence, as the probability of the mass point remains negligible until its location begins its upward trend.</p><p>The sensitivity of mutual information to the exact number and location of the mass points appears to be small. Fig. <ref type="figure" target="#fig_5">6</ref>, for example, shows the maximum mutual information achievable when a distribution with two mass points is used in the SNR region where three or four mass points are optimal. At an SNR of there is only a 4% gap to capacity. Though not shown in the figure, moving the nonzero mass point 10% from its optimal location yields a mutual information that is only 0.5% lower.</p><p>Finally, a reference that is worth mentioning is Taricco and Elia's work <ref type="bibr" target="#b17">[18]</ref> which provides bounds that reflect the asymptotic low and high SNR behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the Ideal Gaussian Channel</head><p>In Fig. <ref type="figure" target="#fig_6">7</ref>, the ratio of the capacity of the ideal additive white Gaussian noise channel to the capacity of the i.i.d. Rayleighfading channel is plotted as a function of the power constraint, where the channels are normalized to have the same SNR at the receiver. The graph suggests that the capacity of the fading channel approaches the capacity of the Gaussian channel as . This is indeed the case, as is shown for a continuous-time model in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Theorem 8.6</ref>.1] and for a discrete-time model in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">Example 3]</ref>. Fig. <ref type="figure" target="#fig_6">7</ref> illustrates, however, that the asymptote is approached quite slowly.</p><p>At low SNR, ON-OFF keying with a low duty cycle is optimal, and a capacity-achieving codebook for the fading channel resembles pulse-position modulation. Unlike a Gaussian channel, where energy is spread uniformly over all degrees of freedom, for the fading channel energy becomes more concentrated as   bandwidth increases. At moderate SNR, the optimal input distribution for the fading channel resembles a uniform distribution over uniformly spaced levels. At high SNR, the loss in performance due to fading grows rapidly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the Fading Channel with Side Information Given to the Receiver</head><p>The case when fading information is available to the receiver was studied by Ericson <ref type="bibr" target="#b5">[6]</ref>, and later by Ozarow, Shamai, and Wyner <ref type="bibr" target="#b6">[7]</ref>. The capacity of the channel with perfect channel state information (CSI) is SNR SNR ( <ref type="formula">27</ref>) where</p><p>The dashed line in Fig. <ref type="figure" target="#fig_6">7</ref> plots the ratio of the capacity of the i.i.d. Rayleigh-fading channel with perfect CSI to the capacity of the Rayleigh-fading channel with no CSI. The graph suggests that the harmful effects of i.i.d. fading arise mainly from the consequent lack of knowledge of the channel at the receiver, not from the time-varying SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SUMMARY AND DISCUSSION</head><p>Motivated by previous work done by <ref type="bibr">Smith [2]</ref> and Shamai and Bar-David <ref type="bibr" target="#b3">[4]</ref>, we have proven what Richters <ref type="bibr" target="#b0">[1]</ref> conjectured in his original report-that the capacity-achieving distribution for the discrete-time memoryless Rayleigh-fading channel is discrete with a finite set of mass points. The main results also hold for th-order diversity with independent branches (see Appendix III).</p><p>An immediate direction for future work is the i.i.d. Ricean fading channel, modeled by giving the fading variable in (1) a nonzero mean. The Ricean model is appropriate when there is a line-of-sight path from transmitter to receiver, or when the receiver (and possibly the transmitter) have side information about the fading. We conjecture that the optimal input will again be discrete. Unlike the Rayleigh channel, the Ricean channel has two parameters: the SNR and the ratio of the fading standard deviation to the fading mean. The capacity of this channel when the input is restricted to be Gaussian has been studied by Zhou, Mei, Xu, and Yao <ref type="bibr" target="#b18">[19]</ref>. A block constant fading was assumed, and no side information was available to either the transmitter or receiver. While the result gives some indication for the effect of the direct path, the optimal input distribution might not be Gaussian.</p><p>Good, easily computed bounds on the capacity of the i.i.d. Rayleigh-fading channel at low and high SNR would be useful for engineering design, as the exact capacity is tedious to compute. Some general guidelines on signal set design would also be helpful. The sensitivity of mutual information to the input distribution function is generally small, and the fading model is likely to be only marginally accurate in many scenarios. Is there a simple rule of thumb for approximately selecting the number of mass points, their locations, and their probabilities as a function of SNR?</p><p>Complementary to the case where the receiver has CSI is the case where the CSI is available at the transmitter only. We expect difficulties here, as transmission strategies rather than input distributions should be considered, as is concluded by extrapolating the results of Shannon <ref type="bibr" target="#b19">[20]</ref> to the continuous state space.</p><p>An important and challenging extension is to generalize the study to non-i.i.d. Rayleigh-fading channels, modeled for example by a Gauss-Markov process. A classical receiver attempts to track the channel variations when the fading coefficients are correlated in time. Should an information-theoretic receiver do the same? We expect that the answer is effectively "yes" at high SNR with slow fading, and "no" when the fading is fast. The desired results will be difficult to obtain, as the optimal input process need not be i.i.d. in general. Some work has been done in this direction by Marzeta and Hochwald <ref type="bibr" target="#b20">[21]</ref> who considered an -transmitters and -receivers channel, whose fading is constant for symbols. The capacity of this channel in some simple cases was computed numerically, and the conjecture of a discrete optimal distribution was found to be accurate.</p><p>A most challenging problem is to combine fading memory and input memory. Underwater acoustic channels, for example, combine rapid time variation with long intersymbol interference. Little is known about the capacity of such channels or how to achieve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I THE OPTIMIZATION PROBLEM</head><p>In this appendix, we establish the existence and uniqueness of the capacity-achieving input distribution for the average power-limited Rayleigh-fading channel. Existence is automatic for finite-alphabet channels but not for continuous-alphabet ones. Uniqueness follows from a particular parameterization of the input space that disregards phase. The structure of the existence proof below follows Smith <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, but the details are different; the fading channel has both multiplicative and additive noise, and compactness is more difficult to prove for an average power constraint than for a peak power constraint.</p><p>We establish existence using topological arguments found in optimization theory and probability theory. In optimization theory, one starts by defining the real normed linear space of all bounded continuous functions on . The dual of includes the set of all probability measures. Optimization results are then obtained using the weak* topology on <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Sec. 5.10]</ref>. In probability theory, one starts with the set of probability measures, and defines weak convergence-which is actually the weak* convergence in -on this set. Next, a metric that metrizes weak convergence is defined (e.g., the LÃ©vy metric [23, Sec. III.7]) and optimization is done in the metric topology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Kuhn-Tucker condition (8) for a = 1:2, = 0:099, and p(x) = 0:811242(x) + 0:188749(x 0 2:52) + 0:000009(x 0 5:24).</figDesc><graphic coords="5,55.08,62.28,220.08,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Probability and location of the nonzero mass point.</figDesc><graphic coords="6,134.22,62.28,321.84,153.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Capacity C(a) (nats/channel use) versus SNR a.</figDesc><graphic coords="6,54.42,256.32,218.40,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Optimal nonzero mass point locations as a function of a.</figDesc><graphic coords="6,54.00,489.06,219.36,174.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Corresponding mass point probabilities.</figDesc><graphic coords="6,317.04,459.12,219.36,166.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Capacity for optimal distribution versus two points distribution (nats).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The ratio of Gaussian capacity to Rayleigh capacity with no channel state information (CSI) (solid line), and the ratio of Rayleigh capacity with receiver CSI to Rayleigh capacity with no CSI (dashed line).</figDesc><graphic coords="7,56.28,62.28,217.68,191.04" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank Prof. R. Gallager and A. Lapidoth of the Laboratory for Information and Decision Systems at MIT for numerous helpful discussions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by Fares Foundation, the National Science Foundation under Grant NCR-9314341, and by the fund for the promotion of research at the Technion. The material in this paper was presented at the IEEE International Symposium on Information theory (ISIT'97), Ulm, Germany, June-July 1997.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the topology obtained in both theories is the same, we freely combine the approaches.</p><p>Existence and uniqueness follow from the following basic theorem of optimization. The remainder of the appendix establishes that the conditions of the theorem are met.</p><p>Theorem 1: If is a real-valued, weak* continuous functional on a weak* compact set , then achieves its maximum on . If furthermore is convex, and is strictly concave, then the maximum (29) is achieved by a unique in . Proof: The first statement is given in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Sec. 5.10]</ref>. The second follows from the definition of strict concavity: if the maximum were achieved at two points, then the evaluation of along their convex combination would exceed that maximum, a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Set is Convex and Compact</head><p>Let denote the set of all distribution functions, and let be the distribution functions of nonnegative random variables with a second moment constraint. That is, is the set of distribution functions such that The set is convex. Indeed, for any and , the convex combination is a distribution function (nondecreasing, right continuous, and</p><p>) and is in , because and because the second moment of is the linear combination of the second moments of and . To prove that is weak* compact we first show that the average power constraint makes tight. Then, because the weak* topology on distribution functions is metrizable, Prokhorov's Theorem [23, Sec. III.2] implies that is relatively compact. That is, for every sequence of distribution functions in we can find a subsequence and a distribution function , not necessarily in , such that . Finally, we prove that is in , establishing that is sequentially compact and hence compact (again because the topology is metrizable).</p><p>The set is tight if, for every , there is a such that (32) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mutual Information is Continuous and Strictly Concave</head><p>The weak* topology on distribution functions is metrizable, hence weak* continuity of a function is equivalent to <ref type="bibr">(36)</ref> We prove that this property is satisfied by the mutual information for the Rayleigh-fading channel. For simplicity, denote the mutual information resulting from a specific input distribution function by Let . Note that for and, by inspection of ( <ref type="formula">4</ref>), for all and . Therefore, for all and , (46</p><p>Since is integrable, (41) is established, and is weak* continuous.</p><p>We now show that is weak* continuous in . (This result is obvious for channels with purely additive noise, as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>.)</p><p>The Being the difference of two weak* continuous functions, the mutual information is consequently a weak* continuous function of .</p><p>We now prove is that is a strictly concave function over , and in fact over .</p><p>Lemma 2: The operator that associates to a given an output density is injective.</p><p>Proof: Equation ( <ref type="formula">4</ref>) describes a multiplicative channel (55)</p><p>where is independent of and has a probability density function . To simplify notation, define . Since we have an invertible relationship between and when is nonnegative, to prove injectivity it is sufficient to show that Assume . Then and are equal in distribution. Therefore, and are equal in distribution. Equivalently, and have equal characteristic functions It can be shown that is well-defined and analytic for complex values of on the band <ref type="bibr" target="#b12">[13]</ref>, and, therefore, has isolated zeros on the real axis. This implies that everywhere except at potentially isolated points, but given that characteristic functions are continuous, they are equal everywhere. Thus, and are equal in distribution and hence so are and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall that</head><p>. The function is clearly a strictly concave function of , and since is an injective linear function of , is a strictly concave function of . Since the other term is linear in , is a strictly concave function of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II THE KUHN-TUCKER THEOREM</head><p>The result of this appendix can be obtained using either the local theory of constrained optimization (the generalized Kuhn-Tucker Theorem), or the global one. For simplicity, we have chosen the latter, while the end result can still be called a Kuhn-Tucker condition.</p><p>The following results closely parallel Smith <ref type="bibr" target="#b1">[2]</ref>, and use the idea of weak differentiability defined by Smith <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Lagrangian Theorem</head><p>Theorem 2 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Sec. 8.3]</ref>: Let be a linear vector space, a normed space, a convex subset of , and the positive cone in . Assume that contains an interior point.</p><p>Let be a real-valued concave functional on and a convex mapping from to . Then is said to be weakly differentiable in at , and is the weak derivative in at . If is weakly differentiable in at for all , is said to be weakly differentiable in or simply weakly differentiable.</p><p>Theorem 3: Assume a weakly differentiable functional in a convex set achieves its maximum.</p><p>1) If achieves it maximum at then for all .</p><p>2) If is concave, then for all implies that achieves its maximum at . Proof: See Smith <ref type="bibr" target="#b1">[2]</ref>.</p><p>Let us prove that the functionals and are weakly differentiable in . Define and Then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since</head><p>, we have</p><p>This equality is valid as long as each of the terms in the difference is finite. In our case, this is guaranteed through the power constraint. As for we have (64) These expressions are valid for arbitrary and in , which implies that and are weakly differentiable, and, consequently, so is . Moreover, since is linear in , and is (strictly) concave, is concave. Hence, by Theorem 3, a necessary and sufficient condition for to achieve the supremum in (</p><p>because if is strictly less than , the moment constraint is trivial and is zero by (61), and (67) remains true. Assume now that (69) is true but (70) is false, that is, there exists such that (73) Since all the functions in the above equation are continuous in , the inequality is satisfied strictly on a neighborhood of . By definition of a point of increase, the set has necessarily a nonzero measure . Hence, (74) which is a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX III TH-ORDER DIVERSITY</head><p>Let us generalize our study to a real-valued scalar channel with independent branches. If is the real-valued channel input, the real-valued output of the th branch, and and the fading and additive noise of the th branch, then the channel is described by for (75) We assume that the 's and 's are mutually independent, and zero-mean real Gaussian random variables with variance and , respectively. When we obtain the Rayleigh-fading channel studied in this paper. The case describes a Gaussian fading channel encountered in the study of the voice channel <ref type="bibr" target="#b25">[26]</ref>. When is even and greater than , the model describes an i.i.d. Rayleigh-fading channel with -order diversity. A sufficient statistic for is . Hence, denoting and an equivalent channel is obtained with transition probability (76) which, as a function of the random variable , can be written (77)</p><p>For Theorem 1 of Appendix I to remain valid for the th-order diversity channel, we need to prove that is weak* continuous and strictly concave. The chain of equations proving the weak* continuity of remains valid since can still be upper-bounded by a function that is initially constant and then decaying as . Furthermore, (76) describes a multiplicative channel , where is independent of and has a probability density function . Following Appendix I, it can be shown that is well-defined and analytic for complex values of on the band <ref type="bibr" target="#b12">[13]</ref>. Hence, the same technique may be applied and is strictly concave. We can thus apply the Kuhn-Tucker condition to yield (78) for , with equality if is in the support of , and where . The proof of the finite character of the optimal distribution follows what is presented in the paper. Having a positive accumulation point is absurd, since the only output density that satisfies (78) with equality everywhere has the form , which is not a valid probability density. The accumulation point at zero can then be ruled out using a similar technique. Indeed, can be lower-bounded by (79)</p><p>Hence, (78) cannot be satisfied with equality for arbitrarily close to zero unless the Lagrange multiplier is zero. But, by using a discrete uniform input distribution with input levels located at , it can be proven that . The proof of the existence of an impulse at zero is identical to the one given in the paper. Given that (80) and the mean of is , then Lemma 1 applies and the proof is the same.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communication over fading dispersive channels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Richters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Res. Lab. Electronics</title>
		<imprint>
			<biblScope unit="volume">464</biblScope>
			<date type="published" when="1967-11-30">Nov. 30, 1967</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The information capacity of amplitude and variance-constrained scalar Gaussian channels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="219" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the information capacity of peak and average power constrained Gaussian channels</title>
		<imprint>
			<date type="published" when="1969">1969</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Elec. Eng., Univ. California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The capacity of average and peak-power-limited quadrature Gaussian channels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Shitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bar-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1060" to="1071" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coding and multiaccess for the energy limited Rauleigh fading channel</title>
		<author>
			<persName><forename type="first">E</forename><surname>Telatar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elec. Eng. Comp. Sci, MIT</title>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Masters thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Gaussian channel with slow fading</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ericson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="353" to="355" />
			<date type="published" when="1970-05">May 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Information rates for the two-ray mobile communications channel</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ozarow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai (shitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capacity of fading channels with channel side information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="1986">1986-1992, Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capacity of Markov channels with receiver CSI and delayed feedback</title>
		<author>
			<persName><forename type="first">H</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="761" to="771" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<title level="m">Information Theory and Reliable Communication</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On an extremum problem of information theory</title>
		<author>
			<persName><forename type="first">I</forename><surname>CsiszÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studia Scient. Math. Hung</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<title level="m">Source Coding Theory</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Lamoureux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analyze MathÃ©matique et NumÃ©rique (Cours de l&apos;Ãcole Centrale)</title>
		<imprint>
			<date type="published" when="1992">1992-1993</date>
			<publisher>Ãcole Centrale</publisher>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Complex Variables</title>
		<author>
			<persName><forename type="first">H</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Houghton Mifflin</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rate distortion for squared error distortion measures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Fix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Annu. Allerton Conf. Communications, Control, and Computers</title>
		<meeting>16th Annu. Allerton Conf. Communications, Control, and Computers</meeting>
		<imprint>
			<date type="published" when="1978-10">Oct. 1978</date>
			<biblScope unit="page" from="704" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mapping approach to rate-distortion computation and analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1939" to="1952" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On channel capacity per unit cost</title>
		<author>
			<persName><forename type="first">S</forename><surname>VerdÃº</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1019" to="1030" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Capacity of fading channel with no side information</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1368" to="1370" />
			<date type="published" when="1997-07-31">July 31, 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Channel capacity of fast fading channels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 47th Vehicular Technology Conf. Proc</title>
		<meeting><address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">XXVII</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948-07">July 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Capacity of a mobile-antenna communication link in a Rayleigh flat-fading</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Marzeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Hochwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<title level="m">Optimization by Vector Space Methods</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Shiryaev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">A Course in Probability Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Probability Theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>LoÃ¨ve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voice channel</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory (ISIT&apos;97)</title>
		<meeting>IEEE Int. Symp. Information Theory (ISIT&apos;97)<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">June-July 1997</date>
			<biblScope unit="page">332</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fading Dispersive Communication Channels</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
