<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Learning Approach for Multi-Frame In-Loop Filter of HEVC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Ce</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zulin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Guan</surname></persName>
						</author>
						<title level="a" type="main">A Deep Learning Approach for Multi-Frame In-Loop Filter of HEVC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD141432946ADC9A8D02249DEFF2DE6B</idno>
					<idno type="DOI">10.1109/TIP.2019.2921877</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2921877, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High efficiency video coding</term>
					<term>in-loop filter</term>
					<term>deep learning</term>
					<term>multiple frames</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An extensive study on in-loop filter has been proposed for high efficiency video coding (HEVC) standard to reduce compression artifacts, thus improving coding efficiency. However, in the existing approaches, the in-loop filter is always applied to each single frame, without exploiting the content correlation among multiple frames. In this paper, we propose a multiframe in-loop filter (MIF) for HEVC, which enhances the visual quality of each encoded frame by leveraging its adjacent frames. Specifically, we first construct a large-scale database containing encoded frames and their corresponding raw frames of a variety of content, which can be used to learn the in-loop filter in HEVC. Next, we find that there usually exist a number of reference frames of higher quality and of similar content for an encoded frame. Accordingly, a reference frame selector (RFS) is designed to identify these frames. Then, a deep neural network for MIF (known as MIF-Net) is developed to enhance the quality of each encoded frame by utilizing the spatial information of this frame and the temporal information of its neighboring higherquality frames. The MIF-Net is built on the recently developed DenseNet, benefiting from its improved generalization capacity and computational efficiency. In addition, a novel block-adaptive convolutional layer is designed and applied in the MIF-Net, for handling the artifacts influenced by coding tree unit (CTU) structure in HEVC. Extensive experiments show that our MIF approach achieves averagely 11.621% saving of the Bj√∏ntegaard delta bit-rate (BD-BR) on the standard test set, significantly outperforming the standard in-loop filter in HEVC and other state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, the rapid growth of high definition videos has brought about increasing visual experience, but meanwhile posing the challenge on transmitting or storing the huge amount of video data. Addressing the challenge, the Joint Collaborate Team on Video Coding (JCT-VC) has proposed the high efficiency video coding (HEVC) standard <ref type="bibr" target="#b0">[1]</ref> for video compression. Compared with its predecessor H.264/advanced video coding (AVC) standard <ref type="bibr" target="#b1">[2]</ref>, HEVC can save approximately 50% bit-rate on average. This benefits from an integration of advanced coding techniques, e.g., the flexible quadtree-based structure of coding tree unit (CTU), the increased number of intra-prediction modes and the more precise interpolation for motion compensation. However, various compres-T. Li, M. Xu, R. <ref type="bibr">Yang</ref>  Email: MaiXu@buaa.edu.cn. sion artifacts (e.g., blocking, blurring and ringing artifacts) <ref type="bibr" target="#b2">[3]</ref> still present in compressed videos, especially at low bit-rates. The artifacts mainly result from the block-wise prediction and quantization with limited precision. To alleviate the compression artifacts, in-loop filters were adopted as crucial components in the recent video coding standards, via enhancing the quality of each encoded frame and providing higher-quality reference for its successive frames. Consequently, the coding efficiency can be further improved by adopting the in-loop filters.</p><p>In total, three types of built-in in-loop filters were proposed for the standard HEVC, including deblocking filter (DBF) <ref type="bibr" target="#b3">[4]</ref>, sample adaptive offset (SAO) filter <ref type="bibr" target="#b4">[5]</ref> and adaptive loop filter (ALF) <ref type="bibr" target="#b5">[6]</ref>. These in-loop filters are implemented sequentially in the HEVC. Specifically, DBF is firstly used to remove the blocking artifacts. Then the SAO filter reduces sample distortion by adding an adaptive offset to each sample. In addition, ALF was also considered to be implemented after the SAO filter, which can further minimize the mean square error between the reconstructed frames and the raw frames based on Wiener filter. However, ALF cannot provide visually better quality, and thus it was not adopted in the final version of HEVC. Besides the built-in in-loop filters for the HEVC, some other in-loop filtering methods were also proposed, containing both heuristic and learning-based methods. In heuristic methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, some prior knowledge of video coding is utilized to build a statistical model of compression artifacts, and then a filtering process is derived for enhancing quality of each video frame based on the model. In the most recent years, deep learning has been successfully employed in many areas about data compression, such as video coding <ref type="bibr" target="#b10">[11]</ref>, quality enhancement <ref type="bibr" target="#b11">[12]</ref> and feature encoding <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Also, learning-based methods have successfully enhanced the performance of in-loop filtering <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b19">[20]</ref>. These methods typically adopt convolutional neural networks (CNN) to learn the spatial correlation of content within a frame patch. For example, Dai et al. <ref type="bibr" target="#b15">[16]</ref> introduced a variable-filter-size residue-learning CNN (VRCNN) in place of the standard DBF and SAO at intra-mode. Compared with the single-path CNN, variable filter sizes in <ref type="bibr" target="#b15">[16]</ref> enable feature extraction at different spatial scales, with less network complexity and accelerated training process. Recently, Zhang et al. <ref type="bibr" target="#b19">[20]</ref> have proposed a residual highway CNN (RHCNN) suitable for both intra-and inter-mode video coding, which is employed after the standard SAO. However, none of the above learning-based methods has employed multiple adjacent frames for in-loop filtering in the HEVC. As measured in this paper, high fluctuation of visual quality exists across the encoded frames in the HEVC, and thus a low-quality frame can be enhanced by referring to its adjacent higher-quality frames. Thus, it is possible to further reduce the compression artifacts of each encoded frame in the in-loop filters by using its adjacent frames, inspired by the works in multi-frame super-resolution <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b26">[27]</ref>.</p><p>Based on deep learning, this paper develops a multi-frame in-loop filter (MIF) for HEVC, replacing the original DBF and SAO. Specifically, we first construct a large-scale database for in-loop filtering in HEVC <ref type="foot" target="#foot_0">1</ref> . Our database contains distorted frames and their corresponding raw frames, generated from 200 raw video sequences at four quantization parameter (QP) values. Next, we need to examine the quality fluctuation of encoded frames in the HEVC. To this end, we design a reference frame selector (RFS) to search for higher-quality reference frames given an unfiltered reconstructed frame (URF), which is based on frame quality and content similarity. If RFS provides sufficient reference frames, the URF flows through a deep neural network for MIF (named MIF-Net) to utilize both spatial information within one frame and temporal information across frames. In MIF-Net, the content of each reference frame is first aligned with the URF via motion compensation, and then the URF is enhanced leveraging the information from multiple frames. In the case that no sufficient reference frame is selected by RFS, a simpler deep neural network for in-loop filter (named IF-Net) is used to enhance the URF instead. Both the MIF-Net and IF-Net are built upon the recently developed DenseNet <ref type="bibr" target="#b27">[28]</ref>, benefiting from its great success on the improved generalization capacity and computational efficiency. Also, considering the blocking artifacts highly influenced by the CTU partition structure, the proposed networks are also adaptive to the coding unit (CU) and transform unit (TU) partition in the HEVC, via varying convolutional kernels at different locations of the CU and TU grids. Finally, a mode selection scheme and the corresponding syntax are also designed to select the best mode among the three possible choices (i.e., MIF-Net, IF-Net and the standard in-loop filters), ensuring the overall performance of our approach. Figure <ref type="figure" target="#fig_1">1</ref> shows an example of our MIF approach. Here, the current 182nd frame is encoded as a URF, and then the 177th and 178th encoded frames are selected as its reference, with higher quality and similar content. Then, the URF and two reference frames are input to MIF-Net. As a result, the content with conspicuous artifacts (face and ear behind the bubble) in the URF can be significantly enhanced by MIF-Net, leveraging the information of two reference frames.</p><p>This paper was previously presented in Data Compression Conference 2019 <ref type="bibr" target="#b28">[29]</ref>, with the following improvements. First, the training and validation sets of HIF database are enlarged from 93 sequences in <ref type="bibr" target="#b28">[29]</ref> to 160 sequences. In addition, this paper thoroughly analyzes both frame quality and content similarity of compressed HEVC, as the foundation of our MIF approach. Next, we advance our MIF approach by developing the syntax regulation. Finally, we provide more extensive experimental results with various settings and the ablation study, verifying the effectiveness and the generalization ability of our MIF approach. In brief, the main contributions of this paper are summarized below.</p><p>‚Ä¢ We construct a large-scale database for learning the inloop filter of HEVC, with the potential to facilitate the further research in designing in-loop filters for HEVC encoding.</p><p>‚Ä¢ We investigate the quality fluctuation of encoded frames in HEVC, and design an RFS to find higher-quality reference frames for URFs. ‚Ä¢ We propose an MIF-Net and an IF-Net to prominently enhance the frame quality via utilizing both spatial and temporal information. The rest of this paper is organized as follows. Section II reviews the related works on HEVC in-loop filtering and multiframe super-resolution. Section III presents the constructed HIF database and examines the frame quality fluctuation in the HEVC. In Section IV, we propose the MIF approach for in-loop filtering, and Section V regulates the corresponding syntax. Section VI reports the experimental results to verify the effectiveness of the proposed approach. Finally, Section VII concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In recent years, many in-loop filtering approaches have been proposed to improve the coding efficiency of HEVC by reducing the compression artifacts. Along with the development of HEVC, three built-in in-loop filters were designed, including DBF <ref type="bibr" target="#b3">[4]</ref>, SAO filter <ref type="bibr" target="#b4">[5]</ref> and ALF <ref type="bibr" target="#b5">[6]</ref>. Specifically, DBF, simplified from that in H.264, is adopted as the first in-loop filter of HEVC to remove the blocking artifacts at prediction unit (PU) or TU boundaries. Afterwards, the SAO filter refines samples in both smooth and textured areas. To this end, the SAO filter divides the samples into different categories and then adds an offset to each sample according to the category. In addition, ALF was also considered during the development of HEVC, which estimates suitable filter coefficients using Wiener filter at the encoder-end and then signals the coefficients to the decoder-end. However, it was not adopted in HEVC eventually, since it is unable to produce visually better quality.</p><p>In addition to the above built-in filters of HEVC, some other in-loop filtering methods have also been proposed. These methods can be classified into two categories, i.e, heuristic and learning-based methods. In heuristic methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>, the statistical characteristics of artifacts are modeled according to some prior knowledge (such as textural complexity, and the number of similar frame patches), and a filtering process is then derived based on the model. For example, Matsumura et al. <ref type="bibr" target="#b6">[7]</ref> introduced a non-local means (NLM) filter to HEVC, which takes the weighted mean of non-local similar frame patches for artifact reduction. The non-local design compensates the disadvantage that the pre-existing inloop filters utilize only local information of frames. Ma et al. <ref type="bibr" target="#b8">[9]</ref> developed a group-based in-loop filter to exploit both local and non-local similarities. With the obtained similarities, a reconstructed frame is firstly divided into multiple patch groups, and each group forms a matrix. Then, a soft or hard thresholding is applied to the singular values of the formed matrix, for achieving a sparse representation and meanwhile filtering out compression artifacts. Also based on the singular value decomposition of the group matrix, Zhang et al. <ref type="bibr" target="#b9">[10]</ref> formulated the in-loop filtering as an optimization problem with low-rank constraint on every patch group, and then established an adaptive soft-thresholding model for sparse representation. Although the above heuristic methods have considerably enhanced the coding efficiency, the prior knowledge in these methods need to be manually exploited. Thus, the handcraft feature extraction results in inefficiency to some extent for the above heuristic methods. Meanwhile, it is also intractable to build a multi-variable filtering model, thus leading to limited coding efficiency enhancement in the above methods.</p><p>More recently, a number of learning-based in-loop filters have been proposed for HEVC, to address the shortcomings of heuristic methods. The learning-based methods can automatically learn the extensive features of compression artifacts and optimize the in-loop filters with sufficient trainable parameters. Since the input to the in-loop filter is always a frame patch of two dimensions, these methods typically adopt CNN to learn the spatial correlation of patch content. Specifically, Park et al. <ref type="bibr" target="#b14">[15]</ref> utilized a four-layer super-resolution CNN (SRCNN) <ref type="bibr" target="#b29">[30]</ref> to replace SAO in the encoding process. Dai et al. <ref type="bibr" target="#b15">[16]</ref> introduced a VRCNN in place of DBF and SAO. Compared with a single-path CNN, variable filter sizes in <ref type="bibr" target="#b15">[16]</ref> are helpful to extract features in different spatial scales, with less network complexity and accelerated training process. As we have witnessed tremendous progresses in CNN, some new CNN structures were also applied in in-loop filtering. For example, Kang et al. <ref type="bibr" target="#b16">[17]</ref> proposed a multi-modal/multi-scale CNN to replace the existing DBF and SAO at intra-mode. This architecture mainly contains two convolutional sub-networks with different scales, also exploiting the CU and TU boundaries as input. Meng et al. <ref type="bibr" target="#b18">[19]</ref> developed a multi-channel long-short-term dependency residual network (MLSDRN) for mapping a distorted frame to its associated raw frame, inserted between DBF and SAO. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> investigated the performance of residual units with various internal structures, and proposed an RHCNN to build accurate mappings between the reconstructed frames and their corresponding raw frames. The RHCNN is employed as a high-dimensional filter after SAO, without conflicting the present in-loop filters.</p><p>It is also worth mentioning that the versatile video coding (VVC) standard is being developed by the Joint Video Exploration Team (JVET), as the successor to HEVC. Some proposals for JVET have already investigated deep-learning-based in-loop filters for VVC, containing two main categories, i.e., sequence-dependent and sequence-independent approaches. In the sequence-dependent approaches, a deep neural network model is trained on-line for certain frames and then used for in-loop filter for all frames of the same video sequence. For example, Hsiao et al. <ref type="bibr" target="#b30">[31]</ref> proposed packing co-located luma and chroma patches together and then processing them with a three-layer CNN, for predicting the enhanced patches.</p><p>The CNN is only trained on frames with temporal ID of 0 or 1, for reducing the computation overhead. However, to the best of our knowledge, no existing work has employed multiple adjacent frames for in-loop filtering in the HEVC encoder. In this paper, we find it possible to further reduce compression artifacts of each encoded frame by using its adjacent frames, which is inspired by the existing works in multi-frame quality enhancement and super-resolution presented as follow. Most recently, Yang et al. <ref type="bibr" target="#b36">[37]</ref> have proposed a decoder-end quality enhancement approach for HEVC. In <ref type="bibr" target="#b36">[37]</ref>, a support vector machine (SVM) based peak quality frame (PQF) detector first distinguishes PQFs from others, and then a novel CNN structure is applied to enhance each non-PQF according to its adjacent PQFs after motion compensation. In addition to quality enhancement, more works have been proposed for multi-frame super-resolution. In early years, some traditional signal processing and machine learning methods were proposed in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> for multi-frame superresolution, increasing video resolution with reference to highresolution key frames. Afterwards, deep learning was widely employed in this area. For example, Kappeler et al. <ref type="bibr" target="#b22">[23]</ref> developed a video super-resolution network (VSRnet), where consecutive frames are firstly aligned together via motion compensation and then fed into a CNN that outputs superresolved frames. Later, Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed replacing the VSRnet by a deeper network based on residual learning. Recently, Huang et al. <ref type="bibr" target="#b24">[25]</ref> proposed a bi-directional recurrent convolutional network (BRCN) for efficient multi-frame super- resolution, achieving both better performance and faster speed.</p><p>Besides, <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref> also presented other deep-learningbased super-resolution approaches for videos.</p><p>The above super-resolution methods <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> and the decoder-end quality enhancement approach <ref type="bibr" target="#b36">[37]</ref> are built on the assumption that, the same objects or scenes may appear in several successive frames and thus the content in a low resolution/quality frame can be inferred from its adjacent higher resolution/quality frames. Accordingly we make the first attempt to apply MIF at the encoder-end, which, we infer, has a great potential to improve coding efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATABASE FOR HEVC IN-LOOP FILTER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database Construction</head><p>We construct a large-scale database for HEVC in-loop filter (known as HIF database), to provide sufficient training data for the proposed approach and facilitate the subsequent works. For constructing the HIF database, 182 raw video sequences were collected, consisting of 6 sequences from <ref type="bibr" target="#b37">[38]</ref>, 87 sequences from Xiph.org <ref type="bibr" target="#b38">[39]</ref> and 89 sequences from the Consumer Digital Video Library <ref type="bibr" target="#b39">[40]</ref> in the Video Quality Experts Group (VQEG) <ref type="bibr" target="#b40">[41]</ref>. These 182 sequences can be freely used for research without any commercial purpose. Note that 18 sequences of Classes A ‚àº E from the Joint Collaborative Team on Video Coding (JCT-VC) test set <ref type="bibr" target="#b41">[42]</ref> are also used for evaluating our MIF approach. However, these JCT-VC sequences are protected by copyrights, thus not included in our HIF database. Despite that, our database contains 182 downloadable video sequences, sufficient for training a deeplearning-based in-loop filter. The details about the sequences are listed in Table <ref type="table">I</ref>. Considering that only resolutions in multiples of the minimum CU size (8√ó8 by default) are supported in HM <ref type="bibr" target="#b42">[43]</ref>, the NTSC sequences were cropped to 720√ó480 by removing the bottom edges of the frames. Moreover, the sequences longer than 10 seconds were clipped to be 10 seconds, preventing the over-large video files in our database.</p><p>All the sequences in our HIF database were divided into non-overlapping sets of training (120 sequences), validation (40 sequences) and test (22 sequences). Note that the 22 test sequences were randomly selected from <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref> with five different resolutions and diverse content. The sequences were all encoded by HM 16.5 <ref type="bibr" target="#b42">[43]</ref> at four QPs  <ref type="table">I</ref>, each sub-database contains 51,335 frames, and thus 616,020 frame-wise samples were collected for the whole HIF database. Note that each frame-wise sample can be split into multiple block-wise samples for data augmentation. Also, the position of each block-wise sample within the framewise sample is alterable, further increasing the variety of training samples in practice. Therefore, the HIF database is ready for providing sufficient data for our deep-learning-based MIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Analysis</head><p>In this section, we analyze the quality fluctuation and content similarity of encoded frames, which serves as a premise for the proposed MIF. For such analysis, the default settings of the LDP, LDB and RA configurations are used, where the hierarchical coding of frames with periodical quality fluctuation is an inherent feature. However, as far as we know, the periodical quality fluctuation has not been quantified for designing the in-loop filters of HEVC. The purpose of this section is to quantify the frame quality and content similarity of compressed frames. Firstly, the quality of video frames is measured by peak signal to noise rate (PSNR), and the standard deviation (STD) of PSNR is used to evaluate the quality fluctuation. Figure <ref type="figure" target="#fig_2">2</ref> shows the fluctuation of PSNR in two selected sequences as examples, encoded at various configurations. It can be observed that evident fluctuation always presents across the frames in the same video sequence. For overall analysis, we further calculate the STD of framelevel PSNR for each sequence, as shown in Table <ref type="table">II</ref>. Also, the PSNR increase by the original DBF and SAO in HEVC (named IDS in Table <ref type="table">II</ref>) is provided for comparison. Note that the STD and IDS of PSNR are averaged on all the 160 training and validation sequences. From this table, we can find that the average STD of frame quality is 0.891 dB, 0.882 dB and 0.929 dB under the LDP, LDB and RA configurations, respectively, which are much larger than the 0.064 dB, 0.048 dB and 0.050 dB increase by the DBF and SAO. This indicates the high fluctuation of frames after HEVC encoding at different configurations. Such high fluctuation of frame quality shows the potential to design an MIF that may significantly outperform the original in-loop filters in HEVC.</p><p>Besides quality fluctuation, content similarity is also a crucial factor in the proposed approach, considering that the motion compensation between a URF and its higher-quality reference frames typically works when they share similar content. The similarity is measured by calculating the correlation coefficient (CC) of luminance and chrominance matrices between two frames. Figure <ref type="figure">3</ref> shows the CC curves with standard deviation at various distance of frames in encoding order of HEVC, averaged on all the training and validation sequences. For both luminance (i.e., Y) and chrominance (i.e., U and V) channels, the CC is always positive, revealing the similarity in frame content. In addition, the three CC curves with standard deviation are similar, which implies a coherence of CC among Y, U and V channels. Although the frame distance is calculated based on the encoding order rather than displaying order, the increase of frame distance generally leads to decreasing CC. It can also be observed that in most cases the CC is larger than 0.7 within 10 frames, indicating a prominent similarity in frame content. Thus, adequate similar frames may always be obtained for a URF via searching from its adjacent frames.</p><p>Finally, we analyze the composition of available reference frames for a URF in our MIF, considering both frame quality and content similarity. a certain threshold. The statistics in this figure are averaged on all inter-predicted frames in the training and validation sequences at four QP values {22, 27, 32, 37}. It can be found that on average 9.9, 9.8 and 10.2 previously encoded frames with CC &gt; 0.7 and ‚àÜPSNR &gt; 0.5dB are available for a URF, under the LDP, LDB and RA configurations, respectively. With a tighter constraint of CC &gt; 0.9, there are still 8.0, 7.9 and 8.3 previously encoded frames available with ‚àÜPSNR &gt; 0.5dB under the three configurations, respectively. It can be expected that, the reference frames of substantially higher quality (e.g., ‚àÜPSNR &gt; 2dB) are more helpful. Moreover, Figure <ref type="figure" target="#fig_5">5</ref> illustrates the subjective quality of reference frames for a URF, taking the 69th encoded frame of sequence BalloonRising at QP = 32 as an example. As shown in this figure, totally 42 higher-quality reference frames with CC &gt; 0.7 are available for the URF, and typically the higher PSNR also corresponds to the better subjective visual perception, especially in textured and moving regions (e.g., ropes attached to the hot air balloon). Also, in terms of content, the frames shown in Figure <ref type="figure" target="#fig_5">5</ref> appear to be similar. Therefore, based on both objective and subjective examination, it can be reasonably expected to find an adequate number of reference frames of much higher quality and similar to a URF.  </p><formula xml:id="formula_0">‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED MIF APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework</head><p>The framework of our MIF approach is illustrated in Figure <ref type="figure" target="#fig_7">6</ref>. In the standard HEVC, each raw frame is encoded through intra/inter-mode prediction, discrete transform and quantization. Then, the predicted frame and the residual frame form a URF. Subsequently, the URF is filtered with DBF and SAO for quality enhancement. Different from the standard HEVC, we propose a deep-learning-based in-loop filter to enhance the URF, leveraging information from its neighboring frames. First, RFS selects high-quality and high-correlated frames as reference, to be introduced in Section IV-B. Next, one of the two possible filtering modes is applied to the URF, as described below.</p><p>‚Ä¢ Mode 1: MIF-Net. Assume that M reference frames are needed in MIF-Net. If RFS selects at least M frames, the URF is processed by MIF-Net to generate an enhanced frame. MIF-Net consists of two parts, i.e., motion compensation and quality enhancement. In MIF-Net, each reference frame is first aligned with the URF in terms of content, with a motion compensation network.</p><p>Then, all the aligned reference frames and the URF are fed into a quality enhancement network to output the reconstructed frame, utilizing both spatial and temporal correlation of these frames. Note that the two networks are combined into an end-to-end model, which can be efficiently optimized with intermediate training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design of RFS</head><p>In our approach, RFS is designed to select the reference frames for each URF, serving as a basis of MIF. For the n-th URF (denoted by F U n ) in a video sequence, RFS examines its previous N encoded frames as the reference frame pool, each of which is denoted by</p><formula xml:id="formula_1">F P i (n -N ‚â§ i ‚â§ n -1)</formula><p>. Afterwards, six metrics reflecting quality difference and content similarity are calculated, as shown below.</p><p>‚Ä¢ ‚àÜPSNR Y i,n , ‚àÜPSNR U i,n and ‚àÜPSNR V i,n : PSNR increase of F P i over F U n , for the Y, U and V channels, respectively.</p><formula xml:id="formula_2">‚Ä¢ CC Y i,n , CC U i,n and CC V i,n</formula><p>: the CC values of frame content between F P i and F U n for the Y, U and V channels. Based on the above metrics, the procedure of RFS is shown in Figure <ref type="figure" target="#fig_8">7</ref>. RFS first divides the reference frame pool into valid and invalid reference frames, and then all valid reference frames are fed into RFS-Net to select M frames used for enhancing the visual quality of F U n . To be more specific, a binary value V i,n represents whether a reference frame from the pool is valid. For at least one channel of F P i , if the PSNR increase is positive and the CC value is above a threshold œÑ , i.e., V i,n = 1 in (1), F P i is treated as a valid reference frame.</p><formula xml:id="formula_3">V i,n = Ô£± Ô£≤ Ô£≥ 1, if c‚àà{Y,U,V} (‚àÜPSNR c i,n &gt; 0 ‚àß CC c i,n &gt; œÑ ) 0, otherwise.<label>(1)</label></formula><p>If there exist at least M valid reference frames, the six metrics for each valid reference frame F P i satisfying n-N ‚â§ i ‚â§ n-1 and V i,n = 1 form a 6-dimensional vector, and then they are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RFS-Net</head><p>‚â•M ? Null input to a two-layer fully connected network (termed by RFS-Net) to generate a scalar Ri,n as output, illustrated in Figure <ref type="figure">8</ref>.</p><formula xml:id="formula_4">P - nN F P - nN F P 1 -+ nN F P 1 -+ nN F P 1 - n F P 1 - n F U n F U n F n-N n-N+1 n-1 n</formula><p>Here, Ri,n is a continuous variable representing the potential of F P i being the reference for F U n . A larger Ri,n indicates that F P i has more potential than other reference frames for enhancing F U n . Note that Ri,n is the predicted value by RFS-Net, with the corresponding ground-truth value denoted by R i,n .</p><p>The procedure to generate R i,n and train RFS-Net is presented in the following. Different from a randomly selected training batch for a typical neural network, the samples in one training batch for RFS-Net are extracted from the valid reference frames for only one URF. Such organization of samples is on account that all predicted values { Ri,n |n -N ‚â§ i ‚â§ n -1, V i,n = 1} by RFS are used for enhancing one certain URF F U n , without any consideration of other URFs. In RFS-Net, the ground-truth potential {R i,n |n -N ‚â§ i ‚â§ n -1, V i,n = 1} should reflect the quality of valid reference frames after these frames are aligned with F U n . To achieve the content alignment, we apply the motion compensation network in MIF-Net to each valid reference frame</p><formula xml:id="formula_5">F P i for F U n (satisfying n -N ‚â§ i ‚â§ n -1 and V i,n = 1) to generate a compensated frame F C i .</formula><p>Then, the difference between F C i and the n-th raw frame (denoted by F n ) is able to quantify R i,n , i.e., the ground-truth potential of F P i for enhancing F U n . Here, normalized PSNR is used to calculate R i,n for each valid reference frame, formulated as</p><formula xml:id="formula_6">R i,n = PSNR(F C i , F n ) -¬µ PSNR (F n ) œÉ PSNR (F n ) ,<label>(2)</label></formula><p>where PSNR(‚Ä¢, ‚Ä¢) denotes the PSNR between a compensated frame and its corresponding raw frame, and ¬µ PSNR (F n ) and œÉ PSNR (F n ) denote the mean value and the standard deviation over {PSNR(</p><formula xml:id="formula_7">F C i , F n )|n -N ‚â§ i ‚â§ n -1, V i,n = 1}</formula><p>, respectively. After normalization, the ground-truth values in one batch are with the mean value of 0 and the standard deviation of 1, in accord with the normalized predicted values { Ri,n |n-N ‚â§ i ‚â§ n-1, V i,n = 1}. Therefore, R i,n and Ri,n are of similar scale, and the 2 -loss can be used to measure the difference between them. Considering the whole training batch for enhancing F U n , the loss function of RFS-Net is formulated as</p><formula xml:id="formula_8">L RFS = n-N ‚â§i‚â§n-1 Vi,n=1 (R i,n -Ri,n ) 2 ,<label>(3)</label></formula><p>which is optimized by the Adam algorithm <ref type="bibr" target="#b43">[44]</ref>. Using the trained RFS-Net model, the reference potential for all the valid frames { Ri,n |n -</p><formula xml:id="formula_9">N ‚â§ i ‚â§ n -1, V i,n = 1} can be obtained. Then RFS selects M frames as output, denoted by {F R m,n } M m=1</formula><p>, where the index m indicates that F R m,n is the frame with the m-th highest Ri,n among all valid reference frames. In the exceptional case that the number of valid reference frames is less than M , RFS does not work and IF-Net is used to enhance F U n instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture of MIF-Net</head><p>In our approach, the quality of each URF is enhanced by either MIF-Net or IF-Net, depending on the number of frames selected by RFS. This section mainly focuses on the architecture of MIF-Net, and the difference between IF-Net and MIF-Net is to be specified in Section IV-D. Figure <ref type="figure">9</ref> illustrates the overall architecture of MIF-Net/IF-Net. As shown in this figure, MIF-Net takes a URF F U n and its M reference frames</p><formula xml:id="formula_10">‚Ä¢‚Ä¢‚Ä¢ (6)<label>(12) (1)</label></formula><p>Z-score Norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><formula xml:id="formula_11">, √ñ i n R U , PSNR i n ' V , PSNR i n ' Y , PSNR i n ' Y , CC i n U , CC i n V , CC i n</formula><p>Fig. <ref type="figure">8</ref>. Illustration of RFS-Net. The input is a 6-dimensional vector representing PSNR increase and CC of frame content, followed by 12 hidden nodes and 1 output node generated by the two layers in sequence. Both layers are activated with parametric rectified linear units (PReLU) <ref type="bibr" target="#b44">[45]</ref>, and the output of all samples in the same batch is processed with Z-score normalization, for obtaining the normalized Ri,n .</p><p>{F R m,n } M m=1 as the input, to generate the enhanced frame F E n as the output. MIF-Net synthesizes information from M parallel branches {B m } M m=1 , with each branch B m dealing with the corresponding reference frame F R m,n . In branch B m , the reference frame F R m,n is first aligned with the URF F U n via a motion compensation network, to produce a compensated frame, denoted by F C m,n . Next, F U n and F C m,n are processed with a novel convolutional layer guided by the CTU partitioning structure of F U n (named block-adaptive convolutional layer), to explore low-level features from different sources and merge the features with consideration of the CU and TU partition. Then, the low-level features flow through two successive DenseNet-based units (named dense units) <ref type="bibr" target="#b27">[28]</ref> to extract more comprehensive features within B m . Finally, the extracted features from all the M branches are concatenated together and further processed with two dense units to extract high-level features. For ease of training, the output of the last dense unit (denoted by F ‚àÜ n ) is regarded as a difference frame, and the enhanced frame F E n is the summation of F ‚àÜ n and F U n . The details of MIF-Net components are presented in the following.</p><p>Motion compensation network. In general, the content of a reference frame F R m,n differs from that of F U n due to temporal motion across frames. Therefore, we propose a motion compensation network based on the spatial transformer motion compensation (STMC) model <ref type="bibr" target="#b45">[46]</ref>, for content alignment between F R m,n and F U n , illustrated in Figure <ref type="figure" target="#fig_13">10-(a)</ref>. In <ref type="bibr" target="#b45">[46]</ref>, the STMC model takes both F R m,n and F U n as input to obtain a compensated frame as output, denoted by F STMC m,n . The STMC consists of two paths (√ó4 and √ó2 down-scaling paths) for predicting coarse and fine motion vector (MV) maps between the two input frames, respectively. Each path contains a succession of convolutional layers and an upscaling layer, and the fine MV maps from the √ó2 down-scaling path are applied to F R m,n for outputting F STMC m,n . In the STMC, the √ó2 and √ó4 down-sampling is capable for estimating various scales of motion. However, the accuracy of the STMC is limited due to down-sampling, and the architecture of the STMC can also be improved. To address this issue, we propose a motion compensation network, with the following advancements over the STMC in <ref type="bibr" target="#b45">[46]</ref>.</p><p>‚Ä¢ Besides the √ó2 and √ó4 down-scaling paths, a full-scale path without down-sampling is added to enhance the precision of MV estimation. As shown in Figure <ref type="figure" target="#fig_13">10</ref></p><formula xml:id="formula_12">- (a), F R m,n , F U n , F STMC m,n</formula><p>and the √ó2 MV maps from the STMC are concatenated together and input to this path. Afterwards, they are processed through convolutional layers to generate the final MV maps. All convolutional layers on this path are with stride of 1, keeping the size of feature maps the same as F R m,n and F U n . ‚Ä¢ Inspired by the ResNet <ref type="bibr" target="#b46">[47]</ref>, in total 6 shortcuts are added next to the convolutional layers for higher network capacity and ease to be trained. Note that both identity shortcuts and projection shortcuts are used, depending on the numbers/sizes of feature maps before and after each shortcut.</p><p>‚Ä¢ All rectified linear units (ReLU) <ref type="bibr" target="#b47">[48]</ref> activating convolutional layers in the STMC are replaced by PReLU, to adaptively learn the rectifying parameters <ref type="bibr" target="#b44">[45]</ref>. With the above modifications, the full-scale path outputs two MV maps, M X m,n and M Y m,n , denoting the horizontal and vertical motion of all pixels from F R m,n to F U n . Finally, the compensated frame F C m,n is derived by</p><formula xml:id="formula_13">F C m,n (x, y)=Bil{F R m,n (x + M X m,n (x, y), y + M Y m,n (x, y))},<label>(4)</label></formula><p>where x and y are coordinates of a pixel, and Bil{‚Ä¢} represents the bilinear interpolation considering that the motion may be of non-integer pixels.</p><p>Block-adaptive convolutional layers. In each branch of MIF-Net, the compensated frame F C m,n and the URF F U n are processed with a convolutional layer adaptive to the CU and TU partition in HEVC. The input to this layer is a concatenation of three feature maps, including</p><formula xml:id="formula_14">F C m,n , F U n and F C m,n -F U n . In addition to F C m,n and F U n , F C m,n -F U n is also meaningful. It is because F C m,n -F U n reflects the reliability of F C</formula><p>m,n as a reference frame for F U n , since an overlarge distance between two co-located parts of F C m,n and F U n may indicate ineffective motion compensation at these parts. In this layer, the CU and TU partition for F U n is represented by two feature maps, i.e., C n and T n , respectively. The size of C n or T n is equal to that of F U n and the values in each map are assigned according to the partition structure. If pixel (x, y) is on the boundary of a CU or TU, the corresponding value C n (x, y) or T n (x, y) is set to 1. Otherwise, the value is set to -1. Afterwards, the target of a block-adaptive convolutional layer is to output a certain number of feature maps, providing three feature maps as the input and two feature maps as the guidance. For this problem, we present a guided convolution operation in Algorithm 1, assuming that P I , P G and P O feature maps are used as the input, guidance and output, respectively. This algorithm consists of two main procedures: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><p>Concat. Concat.   ). All feature maps a size of 3√ó2, and the convolutional kernels are 3√ó3 with the stride of 1. The zero-padding is ado receptive fields of convolution, shown by transparent pixels with dashed boundary. In the in feature map and convolutional weights, the luminance of pixels corresponds to their relative valu the color of each pixel in output feature map reveals the main source of information from the inpu An example of convolution with intermediation. Assume that one input and one intermediate fe are used in the convolution, to output one feature map (i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><note type="other">Input feature map</note><p>). All feature maps a size of 3√ó2, and the convolutional kernels are 3√ó3 with the stride of 1. The zero-padding is ado receptive fields of convolution, shown by transparent pixels with dashed boundary. In the in feature map and convolutional weights, the luminance of pixels corresponds to their relative valu the color of each pixel in output feature map reveals the main source of information from the inpu </p><formula xml:id="formula_15">F U n F C , mn F X , m n M Y , m n M R 1, n F R , M n F C , M n F C 1, n F U n F n ÔÅÑ F E n F n C n T<label>(a)</label></formula><p>Concat. An example of convolution with intermediation. Assume that one input and one intermediate feature maps are used in the convolution, to output one feature map (i.e., ). All feature maps are with the size of 3√ó2, and the convolutional kernels are 3√ó3 with the stride of 1. The zero-padding is adopted in the receptive fields of convolution, shown by transparent pixels with dashed boundary. In the intermediate feature map and convolutional weights, the luminance of pixels corresponds to their relative values. Finally, the color of each pixel in output feature map reveals the main source of information from the input.</p><p>An example of convolution with intermediation. Assume that one input and one intermediate feature maps are used in the convolution, to output one feature map (i.e.,</p><p>). All feature maps are with the size of 3√ó2, and the convolutional kernels are 3√ó3 with the stride of 1. The zero-padding is adopted in the receptive fields of convolution, shown by transparent pixels with dashed boundary. In the intermediate feature map and convolutional weights, the luminance of pixels corresponds to their relative values. Finally, the color of each pixel in output feature map reveals the main source of information from the input. For convolutional layers, "p √ó p, q" represents q output channels with p √ó p kernels. Note that the convolutional stride is set to 1 by default, except that explicitly mentioned in certain layers.</p><p>Compared with a typical convolutional layer where the spaceirrelevant weights are shared across the whole feature map, the major advancement of this algorithm lies in the spacerelevant weights generated according to the guidance (see line 5 in Algorithm 1), contributing to a higher network capacity. Moreover, because of only convolution rather than full-connection is added for intermediate map extraction, the number of trainable parameters is not sharply increased, which results in little risk of over-fitting. For each block-adaptive convolutional layer in MIF-Net, P I = 3 and P G = 2 as described above, and the number of output maps is set to be P O = 12.</p><p>Dense units for quality enhancement. In <ref type="bibr" target="#b27">[28]</ref>, Huang et. al. have proposed an efficient variant of CNN, named DenseNet, which introduces different length of connections between the input and output. Compared with a plain CNN or the ResNet <ref type="bibr" target="#b46">[47]</ref>, the efficiency of DenseNet to train a deep network mainly results from the alleviation of vanishing gradients, the encouragement of feature reuse and the reduction of computational complexity. Considering these compelling advantages, totally (2M +2) dense units are adopted in our MIF-Net for quality enhancement, i.e., 2 dense units in each branch and 2 dense units at the end of MIF-Net synthesizing features from all the M branches. Here, all the dense units are with the same structure, as illustrated in Figure <ref type="figure" target="#fig_13">10-(b)</ref>. Each dense unit contains 4 convolutional layers, and before each layer, features from all preceding layers are concatenated together. Thus, a dense unit includes 10 inter-layer connections, much more than a 4-layer plain CNN with only 4 connections. At each layer in the dense unit, the number of output channels is 12, except the last layer in the final dense unit which outputs only 1 channel as the difference frame between F E n and F U n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Difference between MIF-Net and IF-Net</head><p>The previous section has elaborated our MIF-Net in detail. In the following, we introduce the IF-Net as a simpler counterpart of MIF-Net, adopted in the case that no enough reference frames are found for the URF. The only difference between IF-Net and MIF-Net lies in the absence of M reference frames for F U n in IF-Net. Therefore, only the quality enhancement network without motion compensation is adopted in IF-Net, as illustrated with the red arrows in Figure <ref type="figure">9</ref>. Compared with MIF-Net, only one branch B 1 without the compensated frame exists in IF-Net, and the concatenation synthesizing M branches is omitted. Despite the simpleness, a guided convolutional layer and four consecutive dense units still exist Algorithm 1 Guided convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>{F I j } P I j=1 : Input feature maps. {F G k } P G k=1 : Feature maps as the guidance. {w j,l (‚àÜx, ‚àÜy)|1 ‚â§ j ‚â§ P I , 1 ‚â§ l ‚â§ P O , -1 ‚â§ ‚àÜx, ‚àÜy ‚â§ 1}: Convolutional weights. Each element is the weight in the 3√ó3 kernel from the j-th input map to the l-th output map, where ‚àÜx and ‚àÜy are the relative indices within the kernel. Output: {F O l } P O l=1 : Output feature maps. </p><formula xml:id="formula_16">for 1 ‚â§ x ‚â§ width of F O l and 1 ‚â§ y ‚â§ height of F O l do 4:</formula><p>for ‚àÜx ‚àà {-1, 0, 1} and ‚àÜy ‚àà {-1, 0, 1} do 5:</p><p>Compute the modified weight guided by F M l : w G j,l (‚àÜx, ‚àÜy) = w j,l (‚àÜx, ‚àÜy) ‚Ä¢ F M l (x + ‚àÜx, y + ‚àÜy). *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>end for 7:</p><p>Compute the output pixel:</p><formula xml:id="formula_17">F O l (x, y) = P I j=1 1 ‚àÜx=-1 1 ‚àÜy=-1 w G j,l (‚àÜx, ‚àÜy) ‚Ä¢ F I j (x + ‚àÜx, y + ‚àÜy) * 8:</formula><p>end for 9: end for * If (x + ‚àÜx, y + ‚àÜy) is beyond the frame size, the zero padding is adopted.</p><p>in IF-Net, ensuring sufficient network capacity for quality enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training of MIF-Net and IF-Net</head><p>With both motion compensation and quality enhancement in M branches, MIF-Net is an end-to-end deep neural network that may be difficult to be trained by directly minimizing the difference between F E n and F n . To solve this problem, we propose to train MIF-Net with intermediate supervision <ref type="bibr" target="#b48">[49]</ref>, via introducing two loss functions into MIF-Net to optimize the whole network at different stages. First, the difference between F U n and each of M compensated frames F C m,n can measure the performance of the motion compensation network, and thus it is defined as the intermediate loss</p><formula xml:id="formula_18">L INT = 1 M M m=1 F C m,n -F U n 2 2 ,<label>(5)</label></formula><p>where ‚Ä¢ 2 represents the 2 -norm difference between two frames. Next, the 2 -norm difference between F E n and F n indicates the performance of the whole MIF-Net, and the global loss is defined as</p><formula xml:id="formula_19">L GLO = F E n -F n 2 2 .<label>(6)</label></formula><p>Combining the above two loss functions, the loss L for our MIF-Net is the weighted summation of them, formulated as</p><formula xml:id="formula_20">L = Œ± ‚Ä¢ L INT + Œ≤ ‚Ä¢ L GLO .<label>(7)</label></formula><p>Here, Œ± and Œ≤ are changeable weights, and L is optimized by the Adam algorithm <ref type="bibr" target="#b43">[44]</ref>. On account that the optimal performance of quality enhancement relies on the well-trained motion compensation network, the intermediate loss L INT should be optimized with a larger weight by setting Œ± Œ≤ at early stage of training. After L INT converges, we set Œ≤ Œ± instead, in order to emphasize more on optimization of the global loss L GLO . Through the two stages of training, the URF F U n can be significantly enhanced using M selected reference frames. In contrast to MIF-Net, the training procedure of IF-Net is easier, considering the absence of motion compensation. In IF-Net, the trainable parameters in three dense units can be initialized by those in well-trained MIF-Net, with no need to train from scratch. In addition, the loss of IF-Net is the same as L GLO in MIF-Net, which can be directly optimized by the Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SYNTAX REGULATION</head><p>In our MIF approach, some control data about RFS and filtering mode selection should be shared by both the encoder and decoder. Therefore, the corresponding syntax is regulated for each URF F U n , as listed in Table <ref type="table">III</ref>. The details of syntax regulation are presented in the following.</p><p>Syntax for RFS. First, MIF Net on signals whether MIF-Net is adopted, depending on the number of reference frames selected by RFS. If MIF Net on is true, Ref index is activated to represent the indices of M reference frames, selected from the N -frame pool by RFS. To save the bit-rate, only the distance between a reference frame and F U n (i.e., |i -n| satisfying n-N ‚â§ i ‚â§ n-1) is encoded, rather than encoding the absolute frame index n. Consequently, the number of bits for encoding Ref index is M log 2 N .</p><p>Syntax for filtering mode selection. In addition to RFS, the selection of filtering mode is also encoded to signal whether the URF is processed by a proposed network or by the standard in-loop filters. For each channel c ‚àà {Y, U, V}, assume that the size of a frame is W c √ó H c . Considering that the same filtering mode may not be suitable for different patches of a frame, the selection is conducted in p c √ó p c patches. Here, a frame can be divided into Wc is set to true, and otherwise set to false. Considering the adjustable patch width, a smaller p c indicates more refined mode selection prone to better frame quality, but introduces more bit-rate redundancy due to the encoded bits of the syntax Mode c. In contrast, a larger p c means fewer bits for the syntax, while leading to lower frame quality. Therefore, there exists a trade-off to choose a reasonable p c , and the value of p c is discussed in Section VI-A. </p><formula xml:id="formula_21">1 {Ref index[m]} M m=1 (when MIF Net on is true) M log 2 N {{Mode Y[u][v]} H Y p Y u=1 } W Y p Y v=1 {{Mode U[u][v]} H U p U u=1 } W U p U v=1 {{Mode V[u][v]} H V p V u=1 } W V p V v=1 c‚àà{Y,U,V} Wc pc</formula><p>‚Ä¢ Hc pc Note: ‚Ä¢ represents the top integral function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the performance of our MIF approach through experimental results. Section VI-A presents the settings in the experiments. In Section VI-B, we evaluate both objective and subjective performance of our MIF approach at the RA configuration, compared with the HM baseline and two state-of-the-art approaches, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b19">[20]</ref>. In Section VI-C, we further verify the effectiveness and generalization ability of our MIF approach with various settings. Finally, the ablation study is conducted in Section VI-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>Experimental configurations. In our experiments, all approaches for in-loop filtering were incorporated into the HEVC reference software HM 16.5 <ref type="bibr" target="#b42">[43]</ref>. The RA configuration was applied using the file encoder randomaccess main.cfg <ref type="bibr" target="#b49">[50]</ref> for both network training and performance evaluation at four QPs, {22, 27, 32, 37}. The 120 training sequences in our HIF were used to train the networks, and the hyperparameters were tuned over the 40 validation sequences. Note that all video sequences are in YUV format. During the training phase, only the Y channel was input to both MIF-Net and IF-Net. It is because the Y channel is luminance that contains most visual information. Therefore, during the test phase, the trained models on the Y channel were directly used on all three channels. In the test stage, we set the patch width (introduced in Section V) to be p Y = p U = p V = 256. For performance evaluation, the Bj√∏ntegaard delta bit-rate and Bj√∏ntegaard delta PSNR (BD-PSNR) <ref type="bibr" target="#b50">[51]</ref> were measured to assess the rate-distortion (RD) performance. The evaluation was conducted on 40 video sequences in total, containing all 18 sequences of the JCT-VC standard test set <ref type="bibr" target="#b41">[42]</ref> and the 22 test sequences in our HIF database, named as the supplementary test set. Note that the test sequences were non-overlapping with both training and validation sequences. All experiments were conducted on a computer with an Intel (R) Core (TM) i7-7700K CPU @4.2 GHz, 32 GB RAM and the Ubuntu 16.04 (64-bit) operating system. In addition, a GeForce GTX 1080 GPU was used to accelerate the training procedure.</p><p>Network settings. For our approach, one MIF-Net model and one IF-Net model were trained for each evaluated QP, while all QPs shared the same trained RFS-Net model. The tuned hyper-parameters for these networks are listed in Table </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Analysis</head><p>Objective RD performance. First, we evaluate the objective RD performance of our MIF approach in terms of the BD-BR and BD-PSNR, compared with the HM baseline (standard DBF and SAO), a heuristic approach <ref type="bibr" target="#b9">[10]</ref> and a learning-based approach <ref type="bibr" target="#b19">[20]</ref>. For a fair comparison, the models of <ref type="bibr" target="#b19">[20]</ref> were re-trained on our HIF database. Table V tabulates the RD results of all four approaches, in which the original HM without the in-loop filter is used as an anchor. As indicated in Table V-(a), the BD-BR of our MIF approach is -11.621% averaged over the 18 standard test sequences, outperforming -5.031% of the DBF and SAO, -6.295% of <ref type="bibr" target="#b9">[10]</ref> and -9.227% of <ref type="bibr" target="#b19">[20]</ref>. In addition, Table V-(b) shows that the average BD-BR of our approach is -12.607% over the supplementary test set, and it also significantly outperforms those of other three approaches, i.e, -4.449% of the DBF and SAO, -5.746% of <ref type="bibr" target="#b9">[10]</ref> and -9.942% of <ref type="bibr" target="#b19">[20]</ref>. In terms of BD-PSNR, our approach achieves 0.391 dB in the standard test set and 0.502 dB in the supplementary test set, also considerably better than the DBF and SAO (0.162 dB and 0.167 dB), <ref type="bibr" target="#b9">[10]</ref> (0.201 dB and 0.219 dB) and <ref type="bibr" target="#b19">[20]</ref> (0.305 dB and 0.392 dB). In a word, our MIF approach achieves the best RD performance among all four approaches. The possible reasons of such outperformance include: (1) the utilization of multiple adjacent frames, (2) the effective dense blocks and (3) the proposed block-adaptive convolutional layers. Their contributions in the RD gain are to be analyzed in Section VI-D.</p><p>Subjective visual quality. Next, we compare the subjective quality of all four approaches. Figure <ref type="figure" target="#fig_15">11</ref> illustrates some regions of compressed video sequences as examples, compressed at QP = 37 and the RA configuration. For RaceHorses, it can be observed that the edges of the horse tail are severely blurred when compressed by the DBF and SAO, <ref type="bibr" target="#b9">[10]</ref> and   <ref type="bibr" target="#b19">[20]</ref>. In contrast, the horse tail is with clearer edges after being enhanced by our MIF approach. Also, on the pedestrians in PeopleOnStreet and the hand in FourPeople, the blocking artifacts are significantly reduced by our approach, compared with other three approaches. These examples show that our approach is probably with better visual quality of compressed videos, and the quality enhancement may be more observable at moving regions of frames. Moreover, we have also uploaded the bitstream files of 22 test sequences online<ref type="foot" target="#foot_2">3</ref> , encoded by both our MIF approach and the standard HEVC. With the corresponding decoders, the visual quality of all the frames can be observed for our MIF approach. Time complexity. In addition, we analyze the complexity overhead introduced by our MIF approach and other adapted in-loop filters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>. First, the running time to encode one frame in HM <ref type="bibr" target="#b42">[43]</ref>, denoted by T HM , is provided in Table <ref type="table">VI</ref>. Based on this, we have also measured the time overhead T f introduced by each in-loop filter and tabulated the ratio  in the rest of Table <ref type="table">VI</ref>. A larger ratio T f THM indicates relatively more time overhead of an in-loop filter. Note that the results in this table are averaged over all JCT-VC test sequences with the same resolution. Considering that learning-based in-loop filters can be significantly accelerated by a GPU, both the results with and without GPU are provided, for our MIF approach and the RHCNN <ref type="bibr" target="#b19">[20]</ref>. Here, the above learning-based approaches were implemented at the open-source machine learning framework TensorFlow (TM) <ref type="bibr" target="#b52">[53]</ref>. We record the computational time of only using CPU and using CPU+GPU, respectively, for our approach and <ref type="bibr" target="#b19">[20]</ref>. We can observe from Table VI that the heuristic approach <ref type="bibr" target="#b9">[10]</ref> introduces the least time overhead among the three approaches, when implemented with only CPU. However, benefiting from the GPU acceleration, our MIF approach and the RHCNN-based in-loop filter can be drastically accelerated. As a result, our approach with GPU consumes the least time among all configurations in Table VI, which is 2.4 and 23.5 times faster than the RHCNN <ref type="bibr" target="#b19">[20]</ref> with GPU and the heuristic loop filter <ref type="bibr" target="#b9">[10]</ref> with only CPU, respectively. From the above analysis, the proposed MIF is an efficient approach in terms of time complexity, as a learningbased in-loop filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis with Various Settings</head><p>Transfer to LDP configuration. In this section, we first evaluate the RD performance of our MIF approach at the LDP configuration through transfer learning, to verify its generalization ability. The models of MIF-Net and IF-Net at all four QPs {22, 27, 32, 37} were initialized from those of the RA configuration at the corresponding QPs. Then, they were fine-tuned on our HIF database for the LDP configuration. The file encoder lowdelay P main.cfg <ref type="bibr" target="#b49">[50]</ref> was applied during both transfer learning and performance evaluation, while other experimental settings followed those at the RA configuration, as mentioned in Section VI-A. Table <ref type="table" target="#tab_10">VII</ref> shows the RD performance of all four approaches at the LDP configuration. Note that the results are reported over all sequences at different from both the JCT-VT standard test set and our supplementary test set. We can observe from Table VII that our MIF approach achieves -23.341% of BD-BR on average, outperforming -16.567% of the DBF and SAO, -18.934% of <ref type="bibr" target="#b9">[10]</ref> and -19.518% of <ref type="bibr" target="#b19">[20]</ref>. Similar results can also be found in terms of BD-PSNR. In conclusion, the effectiveness and generalization ability of our MIF approach have been verified at the LDP configuration.</p><p>Statistics of frame quality. To better understand the performance of our MIF approach, it is helpful to analyze the statistics of frame quality, for different in-loop filters. In addition to PSNR, the structural similarity (SSIM) <ref type="bibr" target="#b53">[54]</ref> is also added to evaluate the visual quality of video sequences. As tested in <ref type="bibr" target="#b53">[54]</ref>, SSIM has a remarkably better prediction of subjective visual quality than PSNR. Figure <ref type="figure" target="#fig_16">12-(</ref>  the quality fluctuation for the first 100 frames of sequence KristenAndSara as an example, evaluated on our MIF approach, standard HEVC and two state-of-the-art approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>. It can be observed that our MIF approach outperforms other three approaches in terms of overall PSNR and SSIM. Also, the quality fluctuation for our approach is less than that for other approaches. For more comprehensive analysis, we further compare the statistics of PSNR and SSIM for video sequences encoded by four approaches, as shown in Figure <ref type="figure" target="#fig_16">12-(b</ref>). Here, the results are averaged over all 18 JCT-VC test sequences at the RA configuration with QPs {22, 27, 32, 37}. Analyzed from Figure <ref type="figure" target="#fig_16">12</ref>-(b), our approach achieves the mean PSNR of 37.533 dB, considerably higher than that for other three approaches. Moreover, the standard deviation of PSNR for our approach is 0.776 dB, smaller than 0.791 dB of standard HEVC, 0.787 dB of <ref type="bibr" target="#b9">[10]</ref> and 0.786 dB of <ref type="bibr" target="#b19">[20]</ref>. For the mean and standard deviation of SSIM, similar results can be found. The above analysis verifies that our MIF approach can achieve both better overall quality and lower fluctuation of quality for compressed videos. This benefits from the multi-frame design, in which low-quality frames can be significantly enhanced using other higher-quality frames.</p><p>Comparison with learning-based in-loop filters on JVET. Considering that learning-based in-loop filters proposed by JVET have made remarkable achievements, it is also necessary to evaluate their performance. To this end, we compare our MIF approach with two sequence-independent filters for VVC, i.e., the residual weight-sharing CNN <ref type="bibr" target="#b34">[35]</ref> and the dense residual CNN <ref type="bibr" target="#b35">[36]</ref>. Note that the filters <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> were reimplemented in HM <ref type="bibr" target="#b42">[43]</ref> for HEVC. For fair comparison, the models of <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> were both re-trained on our HIF database. Table <ref type="table" target="#tab_10">VIII</ref> shows the RD performance of three approaches at the RA configuration with QPs {22, 27, 32, 37}. We can find in this table that the average BD-BR of our approach is -12.184%, outperforming -7.875% of <ref type="bibr" target="#b34">[35]</ref> and -9.004% of <ref type="bibr" target="#b35">[36]</ref>. In terms of BD-PSNR, there exist similar results. On a closer observation, our approach also performs better than the other two approaches at each resolution of both test sets in Table <ref type="table" target="#tab_10">VIII</ref>, in terms of both BD-BR and BD-PSNR. Therefore, the effectiveness and stability of our MIF approach have been verified, compared with two newly-developed inloop filter approaches in JVET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We further conduct a series of ablation experiments to investigate the effectiveness of major components in our approach. Our ablation study starts from the standard in-loop filter, and then certain components are added step-wise, finally reaching the proposed MIF approach. Figure <ref type="figure" target="#fig_1">13</ref> shows the RD performance at the RA configuration. More details are discussed in the following.</p><p>Plain CNN vs. standard in-loop filter. In the standard DBF and SAO, the filtering procedure is predetermined without any trainable parameter, which tends to be limited in reducing compression artifacts with diverse content. By contrast, we replace the standard DBF and SAO by a plain-CNN-based filter, providing sufficient trainable parameters. For simplicity, the plain CNN is composed of 4 successive convolutional layers <ref type="foot" target="#foot_3">4</ref> for generating the difference frame F ‚àÜ n , without any dense unit, block-adaptive convolutional layer and utilization of multiple frames. We can observe from Figure <ref type="figure" target="#fig_1">13</ref> that the plain CNN improves RD performance by 4.381% with BD-BR saving and 0.147 dB of BD-PSNR increase, compared with the standard DBF and SAO.</p><p>CNN with dense units vs. plain CNN. To analyze the impact of network topology, we substitute the 4 convolutional layers in the plain CNN by 4 successive dense units, i.e., changing the plain CNN into the proposed IF-Net without block-adaptive convolution. Note that the numbers of trainable parameters are the same in both networks with and without dense units, each containing 47, 196 convolutional weights. As can be seen in Figure <ref type="figure" target="#fig_1">13</ref>, the dense units outperform the plain typology of CNN by 0.891% of BD-BR saving and 0.032 dB of BD-PSNR increase, when implemented in the proposed inloop filter. As discussed in <ref type="bibr" target="#b27">[28]</ref>, some possible reasons for the effectiveness of dense units include: (1) the feature reuse to enhance parameter efficiency, (2) the flexible topology with various depth of pathways for easy convergence and (3) the implicit deep supervision that enforces intermediate layers to learn discriminative features.</p><p>IF-Net with vs. without block-adaptive convolution. In HEVC, the flexible CTU partition structure has evident influence on compression artifacts, especially on blocking artifacts. Therefore, a block-adaptive convolutional layer is proposed to handle such artifacts. We evaluate two networks with and without block-adaptive convolution to analyze its effectiveness. In terms of RD performance, the IF-Net with block-adaptive convolution is better than that without it, where the decrease of BD-BR is 0.411% and the increase of PSNR is 0.015 dB. The block-adaptive convolution outperforms typical convolution, because the CTU structure has impact on the compression artifacts in HEVC, especially the blocking artifacts.</p><p>IF-Net and MIF-Net vs. only IF-Net. In our MIF approach, the utilization of multiple frames is a major contribution for enhancing the quality of each URF. Here, we investigate the RD performance of our approach with and without MIF-Net. Note that RFS is enabled when evaluating the performance with MIF-Net. As can be seen in Figure <ref type="figure" target="#fig_1">13</ref>, our approach with both IF-Net and MIF-Net outperforms the setting with only IF-Net (0.907% BD-BR reduction and 0.035 dB BD-PSNR increase). These results verify the effectiveness of leveraging multiple frames for our in-loop filter. The multi-frame design is effective because there always exists considerable quality fluctuation among adjacent frames, and a low-quality frame can be enhanced by its neighboring higher-quality frames.</p><p>From the above analysis, three major configurations of network contribute to the proposed MIF approach, comparing with a plain CNN. Among them, the outperformance is mainly due to the utilization of multiple frames and the efficient dense units. Also, the novel block-adaptive convolution helps to improve the RD performance in a certain extent. Therefore, the reason why such network configurations are beneficial, lies in both the advanced topology itself (i.e., the dense units) and the specific characteristics of the compression artifacts (i.e., the multi-frame design and block-adaptive convolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have proposed a deep-learning-based MIF approach for HEVC. Different from the existing inloop filter approaches based on a single frame, our MIF approach learns to enhance the visual quality of one frame by leveraging multiple adjacent frames. To this end, we first constructed a large-scale HIF database, and found that there normally exist an adequate number of reference frames with both higher quality and similar content for a URF. According to our observation, we design an RFS for selecting these reference frames. Taking advantage of the HIF database, a deep MIF-Net model was proposed to enhance the quality of each URF, which utilizes both the spatial information of this URF and the temporal information of its selected reference frames. The MIF-Net model was constructed by the newly developed DenseNet with improved generalization ability and computational efficiency. Also, a novel block-adaptive convolutional layer was proposed for MIF-Net, considering the blocking artifacts highly influenced by the CTU structure in HEVC. Finally, both objective and subjective experiments demonstrated that our MIF approach significantly outperforms the standard in-loop filter and other state-of-the-art approaches for HEVC. For future works, more various details related to compression artifacts (e.g., skip modes, prediction unit partition, motion vectors and residual frames) may also be utilized, with potential to further improve the performance of in-loop filters. In addition, the implementation of deep neural networks can be accelerated with some techniques <ref type="bibr" target="#b54">[55]</ref>. Thus, another future work is applying these techniques to speed up our MIF approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and Z. Wang are with the School of Electronic and Information Engineering, Beihang University, Beijing, 100191 China. C. Zhu is with the School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Xiyuan Avenue 2006, Western High Tech Zone, Chengdu, 611731, China. Z. Guan is with the School of Cyber Science and Technology, Beihang University, Beijing, 100191 China. Mai Xu is the corresponding author of this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example illustrating quality fluctuation of frames and the proposed MIF approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. PSNR fluctuation of two encoded sequences under various configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 -Fig. 3 .Fig. 4 .</head><label>434</label><figDesc>Fig. 3. Frame content similarity measured by CC at various frame distance. The line represents the average CC for each channel, and the colored foreground indicates the range within one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>CC = 0.993, 0.996, 0.995 ŒîPSNR = 0.243 dB CC = 0.902, 0.877, 0.891 ŒîPSNR = 0.635 dB CC = 0.871, 0.834, 0.849 ŒîPSNR = 0.566 dB CC = 0.983, 0.989, 0.989 ŒîPSNR = 0.707 dB CC = 0.945, 0.947, 0.957 ŒîPSNR = 1.255 dB CC = 0.892, 0.861, 0.874 ŒîPSNR = 1.179 dB CC = 0.925, 0.917, 0.932 ŒîPSNR = 1.239 dB CC = 0.958, 0.961, 0.962 ŒîPSNR = 1.987 dB CC = 0.877, 0.847, 0.859 ŒîPSNR = 1.948 dB CC = 0.975, 0.980, 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Subjective examination of higher-quality reference frames with CC &gt; 0.7 for a URF. The reference frames are grouped by different ranges of ‚àÜPSNR, and the blue font in bracket represents the number of frames for each group. In each frame, the CC values are calculated for Y, U and V channels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>‚Ä¢</head><label></label><figDesc>Mode 2: IF-Net. In the case that no enough reference frames are found for the URF, another deep neural network, IF-Net, is used instead as a simpler counterpart of MIF-Net. In contrast to MIF-Net, IF-Net only takes the URF as input without any consideration of multiple frames. The architecture of IF-Net is similar to that of the quality enhancement network in MIF-Net, and thus most training parameters in IF-Net can be initialized by the trained parameters in MIF-Net. Such design improves the effectiveness of training procedure because it is not necessary to train IF-Net from scratch. In Modes 1 and 2, both MIF-Net and IF-Net are adaptive to the CU and TU partition, in which the parameters of the convolutional kernels are varied with respect to CU and TU partition. More details about the architectures of MIF-Net and IF-Net are introduced in Sections IV-C and IV-D, respectively, and the training protocol is presented in Section IV-E. If MIF-Net/IF-Net fails to improve frame quality, the standard DBF and SAO can also be used as a supplementary mode. Finally, the best mode among the three possible choices (i.e., MIF-Net, IF-Net and the standard in-loop filters) is selected as the actual choice, ensuring the overall performance of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Framework of the proposed MIF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Procedure of RFS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 BFig. 9 .</head><label>19</label><figDesc>Fig. 9. Architecture of MIF-Net/IF-Net. The difference between MIF-Net and IF-Net is shown by different colors of arrows. Note that the background slashes in dense units indicate that the parameters of IF-Net can be initialized with those from MIF-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>only IF-Net only MIF-Net and IF-Net MIF-Net only IF-Net only MIF-Net and IF-Net An example of convolution with intermediation. Assume that one input and one intermediate fe are used in the convolution, to output one feature map (i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Network details. (a) Motion compensation network. (b) Dense unit.For convolutional layers, "p √ó p, q" represents q output channels with p √ó p kernels. Note that the convolutional stride is set to 1 by default, except that explicitly mentioned in certain layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>patches, and the syntax Mode c[u][v] denotes whether a proposed network is used for the (u, v)-th patch. If the quality of a patch processed by MIF-Net or IF-Net is higher than that filtered by the standard DBF and SAO, Mode c[u][v]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparison of subjective visual quality. (a) RaceHorses (Class C). (b) PeopleOnStreet (Class A). (c) FourPeople (Class E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Statistics of frame quality for different approaches. (a) PSNR and SSIM fluctuation for the first 100 frames of sequence KristenAndSara. (b) Statistics of PSNR and SSIM over all 18 JCV-VC sequences at the RA configuration with QPs {22, 27, 32, 37}. "a ¬± b" represents mean value of a with standard deviation of b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1: Process {F G k } P G k=1 with two typical convolutional layers to generate P O intermediate feature maps, denoted by {F M l } P O l=1 . Each layer outputs P O channels, convoluted by 3√ó3 kernels with stride of 1 and activated by PReLU. 2: for 1 ‚â§ l ‚â§ P O do</figDesc><table><row><cell>3:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>INT converged) * Except that the parameters in three dense units of IF-Net are initialized with those from MIF-Net, as mentioned in Section IV-E. * * The batch size equals to the number of valid reference frames for a URF. IV. For training the models of MIF-Net and IF-Net, all frames were segmented into 64√ó64 patches. Considering the efficiency of training, the IF-Net or MIF-Net model at QP = 32 was trained from scratch, while the models at QPs {22, 27, 37} were fine-tuned from the trained model.</figDesc><table><row><cell></cell><cell cols="2">TABLE IV</cell></row><row><cell cols="3">HYPER-PARAMETERS FOR NETWORKS</cell></row><row><cell>Hyper-parameter</cell><cell>RFS-Net</cell><cell>MIF-Net or IF-Net</cell></row><row><cell>Size of ref. frame pool in RFS: N</cell><cell>16</cell><cell>-</cell></row><row><cell>Threshold for CC value in RFS: œÑ</cell><cell>0.3</cell><cell>-</cell></row><row><cell>Num. of selected reference frames: M</cell><cell></cell><cell>2</cell></row><row><cell>Optimization</cell><cell cols="2">Xavier initialization  *  [52] and Adam [44]</cell></row><row><cell>Batch size</cell><cell>‚â§ 16  *  *</cell><cell>16</cell></row><row><cell>Initial learning rate</cell><cell>10 -5</cell><cell>10 -4</cell></row><row><cell>Num. of iterations</cell><cell>10 5</cell><cell>10 6 (from scratch) 2 √ó 10 5 (fine-tunning)</cell></row><row><cell>Changeable weights in MIF-Net: Œ± and Œ≤</cell><cell>-</cell><cell>0.99 &amp; 0.01 (at beginning) 0.01 &amp; 0.99 (after L</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII RD</head><label>VII</label><figDesc>PERFORMANCE OF IN-LOOP FILTERS (LDP CONFIG.)</figDesc><table><row><cell>Source</cell><cell>Class or Resolution</cell><cell cols="2">DBF and SAO BD-BR BD-PSNR (%) (dB)</cell><cell>BD-BR (%)</cell><cell>[10] BD-PSNR (dB)</cell><cell>BD-BR (%)</cell><cell>[20] BD-PSNR (dB)</cell><cell cols="2">Proposed MIF BD-BR BD-PSNR (%) (dB)</cell></row><row><cell></cell><cell>Class A</cell><cell>-17.214</cell><cell>0.642</cell><cell>-20.979</cell><cell>0.813</cell><cell>-20.756</cell><cell>0.803</cell><cell>-24.449</cell><cell>0.971</cell></row><row><cell>JCT-VC test set</cell><cell>Class B Class C Class D</cell><cell>-20.225 -12.787 -8.399</cell><cell>0.515 0.503 0.361</cell><cell>-23.898 -13.698 -8.813</cell><cell>0.617 0.540 0.380</cell><cell>-22.944 -14.543 -9.827</cell><cell>0.593 0.578 0.429</cell><cell>-26.065 -16.954 -11.888</cell><cell>0.677 0.684 0.517</cell></row><row><cell></cell><cell>Class E</cell><cell>-24.048</cell><cell>0.713</cell><cell>-27.577</cell><cell>0.824</cell><cell>-27.277</cell><cell>0.830</cell><cell>-32.198</cell><cell>1.008</cell></row><row><cell></cell><cell>352√ó288</cell><cell>-10.915</cell><cell>0.472</cell><cell>-11.995</cell><cell>0.523</cell><cell>-15.153</cell><cell>0.678</cell><cell>-17.837</cell><cell>0.792</cell></row><row><cell>Supple-</cell><cell>640√ó360</cell><cell>-11.626</cell><cell>0.559</cell><cell>-14.966</cell><cell>0.730</cell><cell>-13.985</cell><cell>0.683</cell><cell>-17.970</cell><cell>0.885</cell></row><row><cell>mentary</cell><cell>720√ó480</cell><cell>-12.548</cell><cell>0.430</cell><cell>-14.029</cell><cell>0.486</cell><cell>-17.049</cell><cell>0.602</cell><cell>-24.553</cell><cell>0.934</cell></row><row><cell>test set</cell><cell>1280√ó720</cell><cell>-24.469</cell><cell>0.474</cell><cell>-26.205</cell><cell>0.515</cell><cell>-26.995</cell><cell>0.540</cell><cell>-31.473</cell><cell>0.645</cell></row><row><cell></cell><cell>1920√ó1080</cell><cell>-23.438</cell><cell>0.758</cell><cell>-27.184</cell><cell>0.907</cell><cell>-26.653</cell><cell>0.861</cell><cell>-30.021</cell><cell>0.986</cell></row><row><cell cols="2">Average</cell><cell>-16.567</cell><cell>0.543</cell><cell>-18.934</cell><cell>0.634</cell><cell>-19.518</cell><cell>0.660</cell><cell>-23.341</cell><cell>0.810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>a) illustrates</figDesc><table><row><cell cols="2">PSNR (dB)</cell><cell>38 38.5 39</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>37.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frame order</cell></row><row><cell></cell><cell></cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSIM</cell><cell cols="2">0.956 0.958</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.954</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frame order</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Proposed MIF</cell><cell cols="2">NALF (TCSVT 2017)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RHCNN (TIP 2018)</cell><cell cols="2">Standard DBF and SAO</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.533 ¬± 0.776</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.433 ¬± 0.786</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">37.346 ¬± 0.787</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">37.318 ¬± 0.791</cell></row><row><cell></cell><cell></cell><cell>36.5</cell><cell>37</cell><cell>37.5</cell><cell>38</cell><cell>38.5</cell><cell>39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR (dB)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9315 ¬± 0.0073</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9303 ¬± 0.0075</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.9295 ¬± 0.0075</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.9292 ¬± 0.0075</cell></row><row><cell></cell><cell></cell><cell>0.92</cell><cell>0.925</cell><cell>0.93</cell><cell>0.935</cell><cell>0.94</cell><cell>0.945</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SSIM</cell></row><row><cell></cell><cell></cell><cell cols="2">Proposed MIF</cell><cell></cell><cell cols="2">NALF (TCSVT 2017)</cell></row><row><cell></cell><cell></cell><cell cols="3">RHCNN (TIP 2018)</cell><cell cols="2">Standard DBF and SAO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2921877, IEEE Transactions on Image Processing Fig. 13. RD performance of ablation study. The results are obtained over all 18 sequences in the JCT-VT test set, compared at the RA configuration with QPs {22, 27, 32, 37}.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell></row><row><cell>-11.621</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.391</cell></row><row><cell>-10.714</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.356</cell></row><row><cell>-10.303</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.341</cell></row><row><cell cols="2">-9.412</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.309</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>-5.031</cell><cell></cell><cell></cell><cell>0.162</cell><cell></cell><cell></cell></row><row><cell>-12</cell><cell>-10</cell><cell>-8</cell><cell>-6</cell><cell>-4</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell cols="3">BD-BR (%)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">BD-PSNR (dB)</cell><cell></cell></row><row><cell cols="3">IF-Net and MIF-Net</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Plain CNN</cell><cell></cell></row><row><cell cols="4">IF-Net (with block-adaptive conv.)</cell><cell></cell><cell></cell><cell cols="3">Standard DBF and SAO</cell></row><row><cell cols="5">IF-Net (without block-adaptive conv.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at: https://github.com/tianyili2017/HIF-Database</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this paper, each mentioned QP represents the QP configured before encoding (equal to the QP of the first I-frame in a sequence), despite the QP may fluctuate during encoding.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Available at: https://github.com/tianyili2017/HIF-Database</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The plain CNN contains 3 layers (each outputting 48 channels) and 1 layer (outputting 1 channel) in sequence. All layers are convoluted by 3 √ó 3 kernels with stride of 1, followed by the PReLU<ref type="bibr" target="#b44">[45]</ref> activation.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
			<date type="published" when="2012-12">Dec 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of the H.264/AVC video coding standard</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="560" to="576" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey of hybrid MC/DPCM/DCT video coding distortions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0165-1684(98)00128-5</idno>
		<ptr target="http://dx.doi.org/10.1016/S0165-1684(98)00128-5" />
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="247" to="278" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HEVC deblocking filter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fuldseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Narroschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Der Auwera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1746" to="1754" />
			<date type="published" when="2012-12">Dec 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sample adaptive offset in the HEVC standard</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1755" to="1764" />
			<date type="published" when="2012-12">Dec 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive loop filtering for video coding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamakage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chujoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karczewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JSTSP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="934" to="945" />
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In-loop filter based on non-local means filter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mtsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bandoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jozawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joint Collaborative Team on Video Coding, document Rec. JCTVC-E</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<date type="published" when="2011-03">Mar. 2011</date>
			<pubPlace>Geneva, Swizerland</pubPlace>
		</imprint>
	</monogr>
	<note>ITU-T SG</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quadtree-based non-local Kuan&apos;s filtering in video compression</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1044" to="1055" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlocal in-loop filter: The way toward next-generation video coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2016-04">Apr 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-rank-based nonlocal adaptive loop filter for high-efficiency video compression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2177" to="2188" />
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing complexity of HEVC: A deep learning approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5044" to="5059" />
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing quality for HEVC compressed videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep hashing for scalable image search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2352" to="2367" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep binary descriptor with multi-quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CNN-based in-loop filtering for coding efficiency improvement</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE IVMSP Workshop</title>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network approach for post-processing in HEVC intra coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10132</biblScope>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal/multi-scale convolutional neural network based in-loop filter design for next generation video codec</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE ICIP</title>
		<imprint>
			<date type="published" when="2017-09">Sept 2017</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial-temporal residue network based in-loop filter for video coding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE VCIP</title>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new HEVC in-loop filter based on multi-channel long-short-term dependency residual networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 DCC</title>
		<imprint>
			<date type="published" when="2018-03">March 2018</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Residual highway convolutional neural networks for in-loop filtering in HEVC</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3827" to="3841" />
			<date type="published" when="2018-08">Aug 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super-resolution of video using key frames and motion estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE ICIP</title>
		<imprint>
			<date type="published" when="2008-10">Oct 2008</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video super-resolution algorithm using bi-directional overlapped block motion compensation and on-thefly dictionary training</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="274" to="285" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video superresolution with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video superresolution via motion compensation and deep residual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="762" />
			<date type="published" when="2017-12">Dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video super-resolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end learning of video superresolution with motion compensation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A densenet based approach for multi-frame in-loop filter in HEVC</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 DCC</title>
		<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ECCV, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural network loop filter</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0159</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive convolutional neural network loop filter</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0566</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A result of convolutional neural network filter</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0872</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional neural network filter (CNNF) for intra frame</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0351</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cnn-based in-loop filter proposed by ustc</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0510</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Test results of dense residual convolutional neural network based in-loop filter</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JVET-M0508</title>
		<meeting><address><addrLine>Marrakech, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-frame quality enhancement for compressed video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Region-of-interest based conversational HEVC coding with hierarchical perception model of face</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JSTSP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="489" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><surname>Xiph</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf" />
		<title level="m">Xiph.org video test media</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Consumer digital video library</title>
		<author>
			<persName><surname>Cdvl</surname></persName>
		</author>
		<author>
			<persName><surname>Org</surname></persName>
		</author>
		<ptr target="https://www.cdvl.org/index.php" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">VQEG video datasets and organizations</title>
		<author>
			<persName><surname>Vqeg</surname></persName>
		</author>
		<ptr target="https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparison of the coding efficiency of video coding standards including high efficiency video coding (HEVC)</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1669" to="1684" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jct-Vc</forename></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Hm Software</surname></persName>
		</author>
		<ptr target="https://hevc.hhi.fraunhofer.de/svn/svnHEVCSoftware/tags/HM-16.5/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Accessed 5-Nov.-2016</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IWAENC</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2011">2016. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Common test conditions and software reference configurations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Collaborative Team on Video Coding, document Rec. JCTVC-L1100</title>
		<imprint>
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Calculation of average PSNR difference between RDcurves</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bj√∏ntegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITU-T, VCEG-M33</title>
		<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">TensorFlow</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016-05">2016. May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
