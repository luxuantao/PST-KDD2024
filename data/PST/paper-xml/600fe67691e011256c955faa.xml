<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiasing Pre-trained Contextualised Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-23">23 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
							<email>kaneko-masahiro@ed.tmu.ac.jp</email>
						</author>
						<author>
							<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
							<email>danushka@liverpool.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Metropolitan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<settlement>Amazon</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Debiasing Pre-trained Contextualised Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-23">23 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.09523v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In comparison to the numerous debiasing methods proposed for the static noncontextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token-or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pretrained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contextualised word embeddings have significantly improved performance in numerous natural language processing (NLP) applications <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Liu et al., 2019;</ref><ref type="bibr" target="#b7">Clark et al., 2020)</ref> and have established as the de facto standard for input text representations. Compared to static word embeddings <ref type="bibr" target="#b31">(Pennington et al., 2014;</ref><ref type="bibr" target="#b28">Mikolov et al., 2013)</ref> that represent a word by a single vector in all contexts it occurs, contextualised embeddings use dynamic context dependent vectors for representing a word in a specific context. Unfortunately however, it has been shown that, similar to their non-contextual counterparts, contextualised text embeddings also encode various types of unfair biases <ref type="bibr" target="#b40">(Zhao et al., 2019;</ref><ref type="bibr" target="#b4">Bordia and Bowman, 2019;</ref><ref type="bibr" target="#b27">May et al., 2019;</ref><ref type="bibr" target="#b36">Tan and Celis, 2019;</ref><ref type="bibr" target="#b3">Bommasani et al., 2020;</ref><ref type="bibr" target="#b22">Kurita et al., 2019)</ref>. This is a worrying situation because such biases can easily propagate to the downstream NLP applications that use contextualised text embeddings.</p><p>Different types of unfair and discriminative biases such as gender, racial and religious biases have been observed in static word embeddings <ref type="bibr" target="#b2">(Bolukbasi et al., 2016;</ref><ref type="bibr">Zhao et al., 2018a;</ref><ref type="bibr" target="#b33">Rudinger et al., 2018;</ref><ref type="bibr">Zhao et al., 2018b;</ref><ref type="bibr" target="#b15">Elazar and Goldberg, 2018;</ref><ref type="bibr" target="#b19">Kaneko and Bollegala, 2019)</ref>. As discussed later in ? 2 different methods have been proposed for debiasing static word embeddings such as projection-based methods <ref type="bibr" target="#b19">(Kaneko and Bollegala, 2019;</ref><ref type="bibr">Zhao et al., 2018b;</ref><ref type="bibr" target="#b2">Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b32">Ravfogel et al., 2020)</ref> and adversarial methods <ref type="bibr" target="#b39">(Xie et al., 2017;</ref><ref type="bibr" target="#b17">Gonen and Goldberg, 2019)</ref>. In contrast, despite multiple studies reporting that contextualised embeddings to be unfairly biased, methods for debiasing contextualised embeddings are relatively under explored <ref type="bibr" target="#b9">(Dev et al., 2020;</ref><ref type="bibr" target="#b29">Nadeem et al., 2020;</ref><ref type="bibr" target="#b30">Nangia et al., 2020)</ref>. Compared to static word embeddings, debiasing contextualised embeddings is significantly more challenging due to several reasons as we discuss next.</p><p>First, compared to static word embedding models where the semantic representation of a word is limited to a single vector, contextualised embedding models have a significantly large number of parameters related in complex ways. For example, BERT-large model <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> contains 24 layers, 16 attention heads and 340M parameters. Therefore, it is not obvious which parameters are responsible for the unfair biases related to a partic-ular word. Because of this reason, projection-based methods, popularly used for debiasing pre-trained static word embeddings, cannot be directly applied to debias pre-trained contextualised word embeddings.</p><p>Second, in the case of contextualised embeddings, the biases associated with a particular word's representation is a function of both the target word itself and the context in which it occurs. Therefore, the same word can show unfair biases in some contexts and not in the others. It is important to consider the words that co-occur with the target word in different contexts when debiasing a contextualised embedding model.</p><p>Third, pre-training large-scale contextualised embeddings from scratch is time consuming and require specialised hardware such as GPU/TPU clusters. On the other hand, fine-tuning a pre-trained contextualised embedding model for a particular task (possibly using labelled data for the target task) is relatively less expensive. Consequently, the standard practice in the NLP community has been to share<ref type="foot" target="#foot_0">1</ref> pre-trained contextualised embedding models and fine-tune as needed. Therefore, it is desirable that a debiasing method proposed for contextualised embedding models can be applied as a fine-tuning method. In this view, counterfactual data augmentation methods <ref type="bibr" target="#b44">(Zmigrod et al., 2019;</ref><ref type="bibr" target="#b18">Hall Maudslay et al., 2019;</ref><ref type="bibr" target="#b40">Zhao et al., 2019)</ref> that swap gender pronouns in the training corpus for creating a gender balanced version of the training data are less attractive when debiasing contextualised embeddings because we must retrain those models on the balanced corpora, which is more expensive compared to fine-tuning.</p><p>Using gender-bias as a running example, we address the above-mentioned challenges by proposing a debiasing method that fine-tunes pre-trained contextualised word embeddings<ref type="foot" target="#foot_1">2</ref> . Our proposed method retains the semantic information learnt by the contextualised embedding model with respect to gender-related words, while simultaneously removing any stereotypical biases in the pre-trained model. In particular, our proposed method is agnostic to the internal architecture of the contextualised embedding method and we apply it to debias different pre-trained embeddings such as BERT, RoBERTa <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b23">(Lan et al., 2020)</ref>, DistilBERT <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref> and ELEC-TRA <ref type="bibr" target="#b7">(Clark et al., 2020)</ref>. Moreover, our proposed method can be applied at token-level or at sentencelevel, enabling us to debias at different granularities and on different layers in the pre-trained contextualised embedding model.</p><p>Following prior work, we compare the proposed debiasing method in two sentence-level tasks: Sentence Encoder Association Test (SEAT; <ref type="bibr" target="#b27">May et al., 2019)</ref> and Multi-genre co-reference-based Natural Language Inference (MNLI; <ref type="bibr" target="#b9">Dev et al., 2020)</ref>. Experimental results show that the proposed method not only debiases all contextualised word embedding models compared, but also preserves useful semantic information for solving downstream tasks such as sentiment classification <ref type="bibr" target="#b35">(Socher et al., 2013)</ref>, paraphrase detection <ref type="bibr" target="#b13">(Dolan and Brockett, 2005)</ref>, semantic textual similarity measurement <ref type="bibr" target="#b6">(Cer et al., 2017)</ref>, natural language inference <ref type="bibr" target="#b8">(Dagan et al., 2005;</ref><ref type="bibr" target="#b0">Bar-Haim et al., 2006)</ref> and solving Winograd schema <ref type="bibr" target="#b24">(Levesque et al., 2012)</ref>. We consider gender bias as a running example throughout this paper and evaluate the proposed method with respect to its ability to overcome gender bias in contextualised word embeddings, and defer extensions to other types of biases to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior work on debiasing word embeddings can be broadly categorised into two groups depending on whether they consider static or contextualised word embeddings. Although we focus on contextualised embeddings in this paper, we first briefly describe prior work on debiasing static embeddings for completeness of the discussion.</p><p>Bias in Static Word Embeddings: <ref type="bibr" target="#b2">Bolukbasi et al. (2016)</ref> proposed a post-processing approach that projects gender-neutral words into a subspace, which is orthogonal to the gender direction defined by a list of gender-definitional words. However, their method ignores gender-definitional words during the subsequent debiasing process, and focus only on words that are not predicted as gender-definitional by a classifier. Therefore, if the classifier erroneously predicts a stereotypical word as gender-definitional, it would not get debiased. <ref type="bibr">Zhao et al. (2018b)</ref> modified the original GloVe <ref type="bibr" target="#b31">(Pennington et al., 2014)</ref> objective to learn gender-neutral word embeddings (GN-GloVe) from a given corpus. Unlike the above-mentioned methods, <ref type="bibr" target="#b19">Kaneko and Bollegala (2019)</ref> proposed GP-GloVe, a post-processing method to preserve gender-related information with autoencoder <ref type="bibr" target="#b20">(Kaneko and Bollegala, 2020)</ref>, while removing discriminatory biases from stereotypical cases.</p><p>Adversarial learning <ref type="bibr" target="#b39">(Xie et al., 2017;</ref><ref type="bibr" target="#b15">Elazar and Goldberg, 2018;</ref><ref type="bibr" target="#b25">Li et al., 2018)</ref> for debiasing first encode the inputs and then two classifiers are jointly trained -one predicting the target task (for which we must ensure high prediction accuracy) and the other for protected attributes (that must not be easily predictable). <ref type="bibr" target="#b15">Elazar and Goldberg (2018)</ref> showed that although it is possible to obtain chance-level development-set accuracy for the protected attributes during training, a post-hoc classifier trained on the encoded inputs can still manage to reach substantially high accuracies for the protected attributes. They conclude that adversarial learning alone does not guarantee invariant representations for the protected attributes. <ref type="bibr" target="#b32">Ravfogel et al. (2020)</ref> found that iteratively projecting word embeddings to the null space of the gender direction to further improve the debiasing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmarks for biases in Static Embeddings:</head><p>Word Embedding Association Test (WEAT; <ref type="bibr" target="#b5">Caliskan et al., 2017)</ref> quantifies various biases (e.g. gender, race and age) using semantic similarities between word embeddings. Word Association Test (WAT) measures gender bias over a large set of words <ref type="bibr" target="#b14">(Du et al., 2019</ref>) by calculating the gender information vector for each word in a word association graph created in the Small World of Words project <ref type="bibr">(SWOWEN;</ref><ref type="bibr" target="#b12">Deyne et al., 2019)</ref> by propagating masculine and feminine words via a random walk <ref type="bibr" target="#b43">(Zhou et al., 2003)</ref>. SemBias dataset <ref type="bibr">(Zhao et al., 2018b)</ref> contains three types of word-pairs: (a) Definition, a gender-definition word pair (e.g. hero -heroine), (b) Stereotype, a gender-stereotype word pair (e.g., manager -secretary) and (c) None, two other word-pairs with similar meanings unrelated to gender (e.g., jazz -blues, pencil -pen). It uses the cosine similarity between the gender directional vector, ( # ? he -# ? she), and the offset vector (a -b) for each word pair, (a, b), in each set to measure gender bias. WinoBias <ref type="bibr">(Zhao et al., 2018a)</ref> uses the ability to predict gender pronouns with equal probabilities for gender neutral nouns such as occupations as a test for the gender bias in embeddings.</p><p>Bias in Contextualised Word Embeddings: <ref type="bibr" target="#b27">May et al. (2019)</ref> extended WEAT using templates to create a sentence-level benchmark for evaluating bias called SEAT. In addition to the attributes proposed in WEAT, they proposed two additional bias types: angry black woman and double binds (when a woman is doing a role that is typically done by a man that woman is seen as arrogant). They show that compared to static embeddings, contextualised embeddings such as BERT, GPT and ELMo are less biased. However, similar to WEAT, SEAT also only has positive predictive ability and cannot detect the absence of a bias. <ref type="bibr" target="#b3">Bommasani et al. (2020)</ref> evaluated the bias in contextualised embeddings by first distilling static embeddings from contextualised embeddings and then using WEAT tests for different types of biases such as gender (male, female), racial (White, Hispanic, Asian) and religion (Christianity, Islam). They found that aggregating the contextualised embedding of a particular word in different contexts via averaging to be the best method for creating a static embedding from a contextualised embedding. <ref type="bibr" target="#b40">Zhao et al. (2019)</ref> showed that contextualised ELMo embeddings also learn gender biases present in the training corpus. Moreover, these biases propagate to a downstream coreference resolution task. They showed that data augmentation by swapping gender helps more than neutralisation by a projection. They obtain the embedding of two input sentences with reversed gender from ELMo, and obtain the debiased embedding by averaging them. It can only be applied to feature-based embeddings, so it cannot be applied to fine-tuning based embeddings like BERT. We directly debias the contextual embeddings. Additionally, data augmentation requires re-training of the embeddings, which is often costly compared to fine-tuning. <ref type="bibr" target="#b22">Kurita et al. (2019)</ref> created masked templates such as " is a nurse" and used BERT to predict the masked gender pronouns. They used the log-odds between male and female pronoun predictions as an evaluation measure and showed that BERT to be biased according to it. <ref type="bibr" target="#b21">Karve et al. (2019)</ref> learnt conceptor matrices using class definitions in the WEAT and used the negated conceptors to debias ELMo and BERT. Although their method was effective for ELMo, the results on BERT were mixed. This method can only be applied to context-independent vectors, and it requires the creation of static embeddings from BERT and ELMo as a pre-processing step for debi- asing the context-dependent vectors. Therefore, we do not compare against this method in the present study, where we evaluate on context-dependent vectors.</p><p>Dev et al. ( <ref type="formula">2020</ref>) used natural language inference (NLI) as a bias evaluation task, where the goal is to ascertain if one sentence (i.e. premise) entails or contradictions another (i.e. hypothesis), or if neither conclusions hold (i.e. neutral). The premisehypothesis pairs are constructed to elicit various types of discriminative biases. They showed that orthogonal projection to gender direction <ref type="bibr" target="#b10">(Dev and Phillips, 2019</ref>) can be used to debias contextualised embeddings as well. However, their method can be applied only to the noncontextualised layers (ELMo's Layer 1 and BERT's subtoken layer). In contrast, our proposed method can be applied to all layers in a contextualised embedding and outperforms their method on the same NLI task. And our debiasing approach does not require taskdependent data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Debiasing Contextualised Embeddings</head><p>We propose a method for debiasing pre-trained contextualised word embeddings in a fine-tuning setting that simultaneously (a) preserves the semantic information in the pre-trained contextualised word embedding model, and (b) removes discriminative gender-related biases via an orthogonal projection in the intermediate (hidden) layers by operating at token-or sentence-levels. Fine-tuning allows debiasing to be carried out without requiring large amounts of tarining data or computational resources. Our debiasing method is independent of model architectures or their pre-training methods, and can be adapted to a wide range of contextualised embeddings as shown in ? 4.3.</p><p>Let us define two types of words: attribute words (V a ) and target words (V t ). For example, in the case of gender bias, attribute words consist of multiple word sets such as feminine (e.g. she, woman, her) and masculine (e.g. he, man, him) words, whereas target words can be occupations (e.g. doctor, nurse, professor), which we expect to be gender neutral. We then extract sentences that contain an attribute or a target word. Sentences contain more than one attribute (or target) words are excluded to avoid ambiguities. Let us denote the set of sentences extracted for an attribute or a target word w by ?(w). Moreover, let A = w?Va ?(w) and T = w?Vt ?(w) be the sets of sentences containing respectively all of the attribute and target words. We require that the debiased contextualised word embeddings preserve semantic information w.r.t. the sentences in A, and remove any discriminative biases w.r.t. the sentences in T .</p><p>Let us consider a contextualised word embedding model E, with pre-trained model parameters ? e . For an input sentence x, let us denote the embedding of token w in the i-th layer of E by E i (w, x; ? e ). Moreover, let the total number of layers in E to be N . In our experiments, we consider different types of encoder models such as E. To formalise the requirement that the debiased word embedding E i (t, x; ? e ) of a target word t ? V t must not contain any information related to a protected attribute a, we consider the inner-product between the noncontextualised embedding v i (a) of a and E i (t, x; ? e ) as a loss L i given by (1).</p><formula xml:id="formula_0">L i = t?Vt x??(t) a?Va v i (a) E i (t, x; ? e ) 2 (1)</formula><p>Here, v i (a) is computed by averaging the contextualised embedding of a in the i-th layer of E over all sentences in ?(a) following <ref type="bibr" target="#b3">Bommasani et al. (2020)</ref> and is given by (2).</p><formula xml:id="formula_1">v i (a) = 1 |?(a)| x??(a) E i (a, x; ? e )<label>(2)</label></formula><p>Here, |?(a)| denotes the total number of sentences in ?(a). If a word is split into multiple sub-tokens, we compute the contextualised embedding of the word by averaging the contextualised embeddings of its constituent sub-tokens. Minimising the loss L i defined by (1) with respect to ? e forces the hidden states of E to be orthogonal to the protected attributes such as gender.</p><p>Although removing discriminative biases in E is our main objective, we must ensure that simultaneously we preserve as much useful information that is encoded in the pre-trained model for the downstream tasks. We model this as a regulariser where we measure the squared 2 distance between the contextualised word embedding of a word w in the i-th layer in the original model, parametrised by ? pre , and the debiased model as in (3).</p><formula xml:id="formula_2">Lreg = x?A w?x N i=1 ||Ei(w, x; ?e) -Ei(w, x; ?pre)|| 2 (3)</formula><p>The overall training objective is then given by (4) as the linearly weighted sum of the two losses defined by ( <ref type="formula">1</ref>) and (3).</p><formula xml:id="formula_3">L = ?L i + ?L reg<label>(4)</label></formula><p>Here, coefficients ?, ? ? [0, 1] satisfy ? + ? = 1.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a contextualised word embedding model typically contains multiple layers. It is not obvious which hidden states of E are best for calculating L i for the purpose of debiasing. Therefore, we compute L i for different layers in a particular contextualised word embedding model in our experiments. Specifically, we consider three settings: debiasing only the first layer, last layer or all layers. Moreover, L i can be computed only for the target words in a sentence x as in (1), or can be summed up for all words in w ? x (i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t?Vt x??(t)</head><p>w?x v i (a) E i (w, x; ? e ) 2 ).</p><p>We refer to the former as token-level debiasing and latter sentence-level debiasing. Collectively this gives us six different settings for the proposed debiasing method, which we evaluate experimentally in ? 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used SEAT <ref type="bibr" target="#b27">(May et al., 2019)</ref> 6, 7 and 8 to evaluate gender bias. We use NLI as a downstream evaluation task and use the Multi-Genre Natural Language Inference data (MNLI; <ref type="bibr" target="#b38">Williams et al., 2018)</ref> for training and development following <ref type="bibr" target="#b9">Dev et al. (2020)</ref>. In NLI, the task is to classify a given hypothesis and premise sentence-pair as entailing, contradicting, or neutral. We programmatically generated the evaluation set following <ref type="bibr" target="#b9">Dev et al. (2020)</ref> by filling occupation words and gender words in template sentences. The templates take the form "The subject verb a/an object." and the created sentence-pairs are assumed to be neutral. We used the word lists created by <ref type="bibr">Zhao et al. (2018b)</ref> for the attribute list of feminine and masculine words. As for the stereotype word list for target words, we use the list created by <ref type="bibr" target="#b19">Kaneko and Bollegala (2019)</ref>. Using News-commentary-v15 corpus<ref type="foot" target="#foot_2">3</ref> was extract 11023, 42489 and 34148 sentences respectively for Feminine, Masculine and Stereotype words. We excluded sentences with more than 128 tokens in training data. We randomly sampled 1,000 sentences from each type of extracted sentences as development data.</p><p>We used the GLEU benchmark <ref type="bibr" target="#b37">(Wang et al., 2018)</ref> to evaluate whether the useful information in the pre-trained embeddings is retrained after debiasing. To evaluate the debiased models with minimal effects due to task-specific fine-tuning, we used the following small-scale training data: Stanford Sentiment Treebank (SST-2; <ref type="bibr" target="#b35">Socher et al., 2013)</ref>, Microsoft Research Paraphrase Corpus (MRPC; <ref type="bibr" target="#b13">Dolan and Brockett, 2005)</ref>, Semantic Textual Similarity Benchmark (STS-B; <ref type="bibr" target="#b6">Cer et al., 2017)</ref>, Recognising Textual Entailment (RTE; <ref type="bibr" target="#b8">Dagan et al., 2005;</ref><ref type="bibr" target="#b0">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b16">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b1">Bentivogli et al., 2009)</ref>, and Winograd Schema Challenge (WNLI; <ref type="bibr" target="#b24">Levesque et al., 2012)</ref>. We evaluate the performance of the contextualised embeddings on the corresponding development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>We used <ref type="bibr">BERT (bert-base-uncased;</ref><ref type="bibr" target="#b11">Devlin et al., 2019)</ref>, RoBERTa (roberta-base; <ref type="bibr" target="#b26">Liu et al., 2019)</ref>, <ref type="bibr">ALBERT (albert-base-v2;</ref><ref type="bibr" target="#b23">Lan et al., 2020)</ref>, Distil-BERT (distilbert-base-uncased; <ref type="bibr" target="#b34">Sanh et al., 2019)</ref> and ELECTRA (electra-small-discriminator; <ref type="bibr" target="#b7">Clark et al., 2020)</ref> in our experiments. <ref type="foot" target="#foot_3">4</ref> Distil-BERT has 6 layers and the others 12. We used the development data in SEAT-6 for hyperparameter tuning. The hyperparameters of the models, except the learning rate and batch size, are set to their default values as in run glue.py. Using greedy search, the learning rate was set to 5e-5 and the batch size to 32 during debiasing. Optimal values for ? = 0.2 and ? = 0.8 were found by a greedy search in [0, 1] with 0.1 increments. For the GLEU and MNLI experiments, we set the learning rate to 2e-5 and the batch size to 16. Experiments were conducted on a GeForce GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Debiasing vs. Preserving Information</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results on SEAT and GLEU where original denotes the pre-trained contextualised models prior to debiasing. We see that original models other than ELECTRA contain significant levels of gender biases. Overall, the all-token method that conducts token-level debiasing across all layers performs the best. Prior work has shown that biases are learned at each layer <ref type="bibr" target="#b3">(Bommasani et al., 2020)</ref> and it is important to debias all layers. Moreover, we see that debiasing at token-level is more efficient compared to at the sentence-level. This is because in token-level debiasing, the loss is computed only on the target word and provides a more direct debiasing update for the target word than in the sentence-level debiasing, which sums the losses over all tokens in a sentence.</p><p>To test the importance of carefully selecting the target words considering the types of biases that we want to remove from the embeddings, we implement a random baseline where we randomly select target and attribute words from V a ? V t and perform all-token debiasing. We see that random debiases BERT to some extent but is not effective on other models. This result shows that the proposed debiasing method is not merely a regularisation technique that imposes constraints on any arbitrary set of words, but it is essential to carefully select the target words used for debiasing.</p><p>The results on GLEU show that BERT, Distil-BERT and ELECTRA compared to the original embeddings, the debiased embeddings report comparable performances in most settings. This confirms that the proposed debiasing method preserves sufficient semantic information contained in the original embeddings that can be used to learn accurate prediction models for the downstream NLP tasks.<ref type="foot" target="#foot_4">5</ref> However, the performance of RoBERTa and ALBERT decrease significantly compared to their original versions after debiasing. We suspect that these models are more sensitive to fine-tuning and hence lose their pre-trained information during the debiasing process. We defer the development of techniques to address this issue to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measuring Bias with Inference</head><p>Following <ref type="bibr" target="#b9">Dev et al. (2020)</ref>, we use the multigenre co-reference-based natural language inference (MNLI) dataset for evaluating gender bias. This dataset contains sentence triples where a premise must be neutral in entailment w.r.t. two hypotheses. If the predictions made by a classifier that uses word embeddings as features deviate from neutrality, it is considered as biased. Given a set containing M test instances, let the entailment predictor's probabilities for the m-th instance for entail, neutral and contradiction labels be respectively e m , n m and c m . Then, they proposed the following measures to quantify the bias: </p><formula xml:id="formula_4">:? = 1[n m ? ? ],</formula><p>where we used ? = 0.7 following <ref type="bibr" target="#b9">Dev et al. (2020)</ref>. For an ideal (bias-free) embedding, all three measure would be 1. In Table <ref type="table" target="#tab_1">2</ref>, we compare our proposed method against the noncontextualised debiasing method proposed by <ref type="bibr" target="#b9">Dev et al. (2020)</ref> where they debias Layer 1 of BERT-large model using an orthogonal projection to the gender direction during training and evaluation. In addition to the above-mentioned measures, we also report the entailment accuracy on the matched (in-domain) and mismatched (crossdomain) denoted respectively by MNLI-m and MNLI-mm in Table <ref type="table" target="#tab_1">2</ref> to evaluate the semantic information preserved in the embeddings after debiasing.</p><p>We see that the proposed method outperforms noncontextualised debiasing <ref type="bibr" target="#b9">(Dev et al., 2020)</ref> in NN and T:0.7, and its performance of the MNLI task is comparable to the original embeddings. This result further confirms that the proposed method can not only debias well but can also preserve the pre-trained information. Moreover, it is consistent with the results reported in Table <ref type="table" target="#tab_0">1</ref> and shows that debiasing all layers is more effective than only the first layer as done by <ref type="bibr" target="#b9">Dev et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Importance of Debiasing All Layers</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we investigated the bias for the final layer, but it is known that the contextualised embeddings are learned at each layer <ref type="bibr" target="#b3">(Bommasani et al., 2020)</ref>. Therefore, to investigate whether by debiasing in each layer we are able to remove the biases of the entire contextualised embeddings, we evaluate the debiased embeddings at each layer on SEAT 6, 7, 8 datasets and report the averaged metrics for all-token, first-token and last-token methods in Table <ref type="table" target="#tab_2">3</ref>. We see that, on average, fitsttoken and last-token methods have more bias than all-token. Therefore, we conclude that It is not enough to debias only the first and last layers even in DistilBERT, which has a small number of layers. These results show that biases in the entire contextualised embedding cannot be reliably removed by debiasing only some selected layers, but rather the importance of debiasing all layers consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualizing Debiasing Results</head><p>To further illustrate the effect of debiasing using the proposed all-token method, we visualise the similarity scores of a stereotypical word with feminine and masculine dimensions as follows. First, for each target word t, its hidden state, E i (t, x) in the i-th layer of the model E in a sentence x is computed. Next, we average those hidden states across all sentences in the dataset that contain t to obtain ?i (t) = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|T |</head><p>x?T E i (t, x). Likewise, we compute ?i (f ) and ?i (m) respectively for each feminine (f ) and masculine (m) word. Next, we compute, s f i , the cosine similarity between each ?i (f ) and the feminine vector v i (f ), and the cosine similarity, s m i , between each ?i (f ) and the masculine vector v i (f ). s f i and s m i , respectively, are averaged over all layers in a contextualised embedding model to obtain s f Avg and s m Avg , which represent how much gender information each gender word contains on average.</p><p>We then compute the cosine similarity, s t,f i , between each stereotype word's averaged embedding, ?i (t) and the feminine vector v i (f ). Similarly, we compute the cosine similarity s t,m i between each stereotype word's averaged embedding ?i (t) and the masculine vector v i (m). We then average s t,f and s t,m over the layers in E respectively, to compute s t,f</p><p>Avg and s t,m Avg , which represent how much gender information each stereotype word contains on average. Finally, we visualise the normalised female and male gender scores given respectively by s t,f Avg /s f Avg and s t,m Avg /s m Avg . For example, a zero s t,f Avg /s f Avg value indicates that t does not contain female gender related information, whereas a value of one indicates that it contains all information about the female gender. Figure <ref type="figure" target="#fig_2">2</ref> shows each stereotype word with its normalised female ad male gender scores respectively in x and y axises. For a word, a yellow circle denotes its original embeddings, and the blue triangle denotes the result of debiasing using the all-token method.</p><p>We see that with the original embeddings, stereotypical words of are distributed close to one, indicating that they are highly gender-specific. On the other hand, we see that the debiased BERT, DistilBERT and ELECTRA have similar word distributions compared to the original embeddings respectively, with an overall movement towards zero. On the other hand, for RoBERTa, debiased embeddings are mainly distributed from zero to around one compared to the original embeddings. Moreover, for ALBERT, the debiased embeddings are close to zero, but unlike the original distribution, the debiased embeddings are mainly clustered around zero. This shows that RoBERTa and AL-BERT do not retain structure of the original distribution after debiasing. While ALBERT overdebiases pre-trained embeddings of stereotypical words, RoBERTa under-debiases them. This trend was already confirmed on the downstream evaluation tasks conducted in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a debiasing method for pre-trained contextualised word embeddings, operating at token-or sentence-levels. Our experimental results showed that the proposed method effectively debiases discriminative gender-related biases, while preserving useful semantic information in the pretrained embeddings. The results showed that the downstream task was more effective in debias than the previous studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Types of hidden states in E considered in the proposed method. The blue boxes in the middle correspond to the hidden states of the target token.</figDesc><graphic url="image-1.png" coords="4,81.92,62.81,198.43,182.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>neutral = max(e m , n m , c m )]; and (3) Threshold ? (T:? ): T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plot of gender information of hidden states for original and debiased stereotype words.</figDesc><graphic url="image-5.png" coords="8,161.47,190.49,135.82,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Gender bias of contextualised embeddings on SEAT. ? denotes significant bias effects at ? &lt; 0.01.</figDesc><table><row><cell>Model</cell><cell>Layer</cell><cell>Unit</cell><cell>SEAT-6</cell><cell>SEAT-7</cell><cell>SEAT-8</cell><cell># ?</cell><cell>SST-2</cell><cell>MRPC</cell><cell>STS-B</cell><cell>RTE</cell><cell>WNLI</cell><cell>Avg</cell></row><row><cell></cell><cell>all</cell><cell>token sent</cell><cell>0.68  ? 1.13  ?</cell><cell>-0.09 0.34</cell><cell>0.60  ? 0.12</cell><cell>2 1</cell><cell>92.1 91.9</cell><cell>85.6 82.6</cell><cell>83.1 80.0</cell><cell>60.0 54.2</cell><cell>53.5 40.8</cell><cell>74.9 69.9</cell></row><row><cell>BERT</cell><cell>last</cell><cell>token sent</cell><cell>1.02  ? 1.51  ?</cell><cell>-1.18 -0.60</cell><cell>0.47  ? 1.52  ?</cell><cell>2 2</cell><cell>92.2 92.3</cell><cell>86.9 84.6</cell><cell>82.3 82.9</cell><cell>58.1 62.1</cell><cell>56.3 56.3</cell><cell>75.2 75.6</cell></row><row><cell></cell><cell>first</cell><cell>token sent</cell><cell>0.88  ? 0.94  ?</cell><cell>0.33 0.32</cell><cell>0.86  ? 0.97  ?</cell><cell>2 2</cell><cell>92.4 91.9</cell><cell>87.1 86.1</cell><cell>82.6 83.0</cell><cell>62.1 63.9</cell><cell>50.7 46.5</cell><cell>75.0 74.3</cell></row><row><cell></cell><cell cols="2">original</cell><cell>1.04  ?</cell><cell>0.18</cell><cell>0.81  ?</cell><cell>2</cell><cell>92.8</cell><cell>86.7</cell><cell>82.4</cell><cell>60.6</cell><cell>56.3</cell><cell>75.8</cell></row><row><cell></cell><cell cols="2">random</cell><cell>1.16  ?</cell><cell>-0.08</cell><cell>-0.29</cell><cell>1</cell><cell>92.2</cell><cell>87.4</cell><cell>81.9</cell><cell>63.2</cell><cell>54.9</cell><cell>75.9</cell></row><row><cell></cell><cell>all</cell><cell>token sent</cell><cell>0.51  ? 1.27  ?</cell><cell>0.15 0.86  ?</cell><cell>0.02 1.14  ?</cell><cell>1 3</cell><cell>78.1 80.3</cell><cell>81.6 82.8</cell><cell>73.7 74.4</cell><cell>53.8 50.9</cell><cell>56.3 56.3</cell><cell>68.7 68.9</cell></row><row><cell>RoBERTa</cell><cell>last</cell><cell>token sent</cell><cell>1.17  ? 0.98  ?</cell><cell>-0.60 0.75  ?</cell><cell>0.45  ? 0.87  ?</cell><cell>2 3</cell><cell>79.9 69.5</cell><cell>83.7 81.5</cell><cell>74.1 72.9</cell><cell>52.3 52.7</cell><cell>56.3 56.3</cell><cell>69.3 66.6</cell></row><row><cell></cell><cell>first</cell><cell>token sent</cell><cell>1.15  ? 1.21  ?</cell><cell>0.26 0.32</cell><cell>0.54  ? 0.50  ?</cell><cell>2 2</cell><cell>77.8 79.0</cell><cell>81.1 82.5</cell><cell>74.5 74.5</cell><cell>54.5 51.6</cell><cell>56.3 56.3</cell><cell>68.8 68.8</cell></row><row><cell></cell><cell cols="2">original</cell><cell>1.21  ?</cell><cell>1.34  ?</cell><cell>1.01  ?</cell><cell>3</cell><cell>93.8</cell><cell>91.2</cell><cell>89.8</cell><cell>71.8</cell><cell>56.3</cell><cell>80.6</cell></row><row><cell></cell><cell cols="2">random</cell><cell>1.39  ?</cell><cell>0.40  ?</cell><cell>0.39  ?</cell><cell>3</cell><cell>73.4</cell><cell>82.5</cell><cell>73.9</cell><cell>53.4</cell><cell>49.3</cell><cell>66.5</cell></row><row><cell></cell><cell>all</cell><cell>token sent</cell><cell>0.16 0.18</cell><cell>0.02 -0.05</cell><cell>0.18 -0.77</cell><cell>0 0</cell><cell>78.1 77.3</cell><cell>80.5 81.7</cell><cell>67.5 69.9</cell><cell>54.9 46.9</cell><cell>56.3 56.3</cell><cell>67,5 66.4</cell></row><row><cell>ALBERT</cell><cell>last</cell><cell>token sent</cell><cell>0.83  ? 0.69  ?</cell><cell>-1.15 -0.06</cell><cell>-0.76 -0.10</cell><cell>1 1</cell><cell>77.8 78.3</cell><cell>81.2 80.1</cell><cell>68.9 71.3</cell><cell>47.3 55.2</cell><cell>56.3 56.3</cell><cell>66.3 68,2</cell></row><row><cell></cell><cell>first</cell><cell>token sent</cell><cell>0.09 0.25</cell><cell>0.28 0.60  ?</cell><cell>0.97  ? 1.18  ?</cell><cell>1 2</cell><cell>77.9 75.9</cell><cell>81.6 81.3</cell><cell>70.0 70.1</cell><cell>52.0 53.1</cell><cell>56.3 54.9</cell><cell>67,6 67,1</cell></row><row><cell></cell><cell cols="2">original</cell><cell>0.30</cell><cell>0.48  ?</cell><cell>1.12  ?</cell><cell>2</cell><cell>92.2</cell><cell>89.9</cell><cell>87.7</cell><cell>70.0</cell><cell>56.3</cell><cell>79.2</cell></row><row><cell></cell><cell cols="2">random</cell><cell>0.41  ?</cell><cell>0.34</cell><cell>1.08  ?</cell><cell>2</cell><cell>78.2</cell><cell>79.9</cell><cell>71.8</cell><cell>47.3</cell><cell>56.3</cell><cell>66.7</cell></row><row><cell></cell><cell>all</cell><cell>token sent</cell><cell>0.70  ? 1.34  ?</cell><cell>-0.83 1.01  ?</cell><cell>-0.66 0.97  ?</cell><cell>1 3</cell><cell>90.4 91.4</cell><cell>87.8 83.3</cell><cell>80.8 78.8</cell><cell>56.0 57.4</cell><cell>42.3 53.5</cell><cell>71.5 72.9</cell></row><row><cell>DistilBERT</cell><cell>last</cell><cell>token sent</cell><cell>1.11  ? 1.57  ?</cell><cell>-0.03 -1.34</cell><cell>1.38  ? 0.27</cell><cell>2 1</cell><cell>90.9 90.8</cell><cell>88.5 90.2</cell><cell>80.3 80.9</cell><cell>55.6 58.5</cell><cell>38.0 43.7</cell><cell>70.7 72.8</cell></row><row><cell></cell><cell>first</cell><cell>token sent</cell><cell>1.19  ? 1.19  ?</cell><cell>0.59  ? 0.60  ?</cell><cell>0.52  ? 0.55  ?</cell><cell>3 3</cell><cell>90.8 91.1</cell><cell>90.8 90.9</cell><cell>80.4 80.1</cell><cell>55.2 55.2</cell><cell>38.0 36.6</cell><cell>71.0 70.8</cell></row><row><cell></cell><cell cols="2">original</cell><cell>1.26  ?</cell><cell>0.31</cell><cell>0.74  ?</cell><cell>2</cell><cell>90.8</cell><cell>89.3</cell><cell>80.6</cell><cell>56.0</cell><cell>38.0</cell><cell>70.9</cell></row><row><cell></cell><cell cols="2">random</cell><cell>1.35  ?</cell><cell>0.66  ?</cell><cell>-0.25</cell><cell>2</cell><cell>91.1</cell><cell>89.1</cell><cell>80.5</cell><cell>56.3</cell><cell>40.8</cell><cell>71.6</cell></row><row><cell></cell><cell>all</cell><cell>token sent</cell><cell>0.33 0.42  ?</cell><cell>0.10 0.21</cell><cell>0.15 0.33</cell><cell>0 1</cell><cell>90.3 90.7</cell><cell>87.7 87.1</cell><cell>79.4 79.5</cell><cell>52.7 52.3</cell><cell>57.7 54.9</cell><cell>73.6 72.9</cell></row><row><cell>ELECTRA</cell><cell>last</cell><cell>token sent</cell><cell>0.55  ? 0.50  ?</cell><cell>0.07 0.42  ?</cell><cell>0.24 0.32  ?</cell><cell>1 3</cell><cell>90.8 90.5</cell><cell>87.3 87.3</cell><cell>79.8 80.1</cell><cell>51.6 54.5</cell><cell>46.5 40.8</cell><cell>71.2 70.6</cell></row><row><cell></cell><cell>first</cell><cell>token sent</cell><cell>0.31 0.29</cell><cell>0.10 0.22</cell><cell>0.33 0.30</cell><cell>0 0</cell><cell>90.4 90.4</cell><cell>86.9 87.6</cell><cell>79.7 79.7</cell><cell>53.1 53.4</cell><cell>56.3 56.3</cell><cell>73.4 73.5</cell></row><row><cell></cell><cell cols="2">original</cell><cell>0.16</cell><cell>0.46  ?</cell><cell>0.04</cell><cell>1</cell><cell>90.5</cell><cell>87.9</cell><cell>80.4</cell><cell>54.5</cell><cell>46.5</cell><cell>72.0</cell></row><row><cell></cell><cell cols="2">random</cell><cell>0.43  ?</cell><cell>0.49  ?</cell><cell>-0.22</cell><cell>2</cell><cell>90.4</cell><cell>87.7</cell><cell>78.5</cell><cell>51.3</cell><cell>54.9</cell><cell>72.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Debias results for BERT in MNLI.</figDesc><table><row><cell>Model</cell><cell cols="3">MNLI-m MNLI-mm NN FN T:0.7</cell></row><row><cell>Dev et al. (2020)</cell><cell>80.8</cell><cell>81.1</cell><cell>85.5 97.3 88.3</cell></row><row><cell>all-token</cell><cell>80.7</cell><cell>81.2</cell><cell>87.8 96.8 89.3</cell></row><row><cell>original</cell><cell>80.8</cell><cell>81.0</cell><cell>82.3 96.4 83.2</cell></row><row><cell>random</cell><cell>80.5</cell><cell>81.1</cell><cell>85.8 96.4 87.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Averaged scores over all layers in an embedding debiased at token-level, measured on SEAT tests.</figDesc><table><row><cell>Model</cell><cell cols="4">Layer SEAT-6 SEAT-7 SEAT-8</cell></row><row><cell></cell><cell>all</cell><cell>0.44</cell><cell>0.25</cell><cell>0.46</cell></row><row><cell>BERT</cell><cell>last</cell><cell>0.56</cell><cell>0.12</cell><cell>0.47</cell></row><row><cell></cell><cell>first</cell><cell>0.52</cell><cell>0.22</cell><cell>0.49</cell></row><row><cell></cell><cell>all</cell><cell>0.59</cell><cell>0.23</cell><cell>0.61</cell></row><row><cell>RoBERTa</cell><cell>last</cell><cell>0.73</cell><cell>0.24</cell><cell>0.65</cell></row><row><cell></cell><cell>first</cell><cell>0.69</cell><cell>0.28</cell><cell>0.59</cell></row><row><cell></cell><cell>all</cell><cell>0.46</cell><cell>0.48</cell><cell>0.24</cell></row><row><cell>ALBERT</cell><cell>last</cell><cell>1.15</cell><cell>0.26</cell><cell>0.60</cell></row><row><cell></cell><cell>first</cell><cell>0.54</cell><cell>0.89</cell><cell>0.95</cell></row><row><cell></cell><cell>all</cell><cell>0.66</cell><cell>-0.16</cell><cell>0.37</cell></row><row><cell>DistilBERT</cell><cell>last</cell><cell>0.88</cell><cell>0.19</cell><cell>0.35</cell></row><row><cell></cell><cell>first</cell><cell>0.90</cell><cell>0.40</cell><cell>0.52</cell></row><row><cell></cell><cell>all</cell><cell>0.21</cell><cell>0.02</cell><cell>0.18</cell></row><row><cell>ELECTRA</cell><cell>last</cell><cell>0.34</cell><cell>0.20</cell><cell>0.21</cell></row><row><cell></cell><cell>first</cell><cell>0.28</cell><cell>0.13</cell><cell>0.34</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/transformers/ pretrained_models.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Code and debiased embeddings: https://github. com/kanekomasahiro/context-debias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.statmt.org/wmt20/ translation-task.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We used https://github.com/huggingface/ transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Although on WNLI all-token debiasing improves performance for DistilBERT and ELECTRA compared to the respective original models, this is insignificant as WNLI contains only 146 test instances.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno>TAC&apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proc Text Analysis Conference</title>
		<meeting>Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.431</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4758" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying and reducing gender bias in word-level language models</title>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.10555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</title>
		<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On Measuring and Mitigating Biased Inferences of Word Embeddings</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attenuating bias in word vectors</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18">2019. 2019, 16-18 April 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="879" to="887" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The &quot;small world of words&quot; english word association norms for over 12,000 cue words</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>De Deyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Storms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="987" to="1006" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring human gender stereotypes with word association test</title>
		<author>
			<persName><forename type="first">Yupei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1635</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6133" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial Removal of Demographic Attributes from Text Data</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">The Third PASCAL Recognizing Textual Entailment Challenge</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="609" to="614" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">It&apos;s all in the name: Mitigating gender bias with name-based counterfactual data substitution</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Hall Maudslay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1530</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5266" to="5274" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gender-preserving debiasing for pre-trained word embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1641" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoding improves pre-trained word embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1699" to="1713" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conceptor debiasing of word representations evaluated on WEAT</title>
		<author>
			<persName><forename type="first">Saket</forename><surname>Karve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3806</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.11942</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Piyush Sharma, and Radu Soricut</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR&apos;12</title>
		<meeting>the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR&apos;12</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards robust and privacy-preserving text representations</title>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On measuring social biases in sentence encoders</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Rudinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representation in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffery</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Elisa</forename><surname>Celis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13230" to="13241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Controllable invariance through adversarial feature learning</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Gender-Neutral Word Embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Navin Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
