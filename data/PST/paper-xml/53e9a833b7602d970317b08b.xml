<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Computation of Frequent and Top-k Elements in Data Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Metwally</surname></persName>
							<email>metwally@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Divyakant</forename><surname>Agrawal</surname></persName>
							<email>agrawal@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amr</forename><surname>El</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Computation of Frequent and Top-k Elements in Data Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A22E56B9C0B29B9FEA95186C9E20B13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an integrated approach for solving both problems of finding the most popular k elements, and finding frequent elements in a data stream. Our technique is efficient and exact if the alphabet under consideration is small. In the more practical large alphabet case, our solution is space efficient and reports both top-k and frequent elements with tight guarantees on errors. For general data distributions, our top-k algorithm can return a set of k elements, where k ≈ k, which are guaranteed to be the top-k elements; and we use minimal space for calculating frequent elements. For realistic Zipfian data, our space requirement for the frequent elements problem decreases dramatically with the parameter of the distribution; and for top-k queries, we ensure that only the top-k elements, in the correct order, are reported. Our experiments show significant space reductions with no loss in accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, online monitoring of data streams has emerged as an important data management problem. This new key research topic has its foundations and applications in many domains, including databases, data mining, algorithms, networking, theory and statistics. However, new challenges have emerged. Due to their vast sizes, some stream types should be mined fast before being deleted forever. Generally, the alphabet is too large to keep exact information for all elements. Conventional database, and mining techniques, though effective with stored data, are deemed impractical in this setting.</p><p>This work is primarily motivated by the setting of Internet advertising commissioners, who represent the middle persons between Internet publishers, and Internet advertisers. The file systems are bombarded continuously by streams of various types: advertisement rendering, clicks, sales, and leads; and each type is handled differently. For instance, before rendering an advertisement for a user, the clicks stream summary structure should be queried to determine what advertisements would suit the user's profile. If the user's profile indicates that (s)he is not a frequent "clicker", then this user, most probably, will not click any displayed advertisement. Thus, it can be more profitable to show Pay-Per-Impression advertisements, which generate revenue on rendering them. On the other hand, if the user's profile was found to be one of the frequent profiles, then, there is a good chance that this user will click some of the advertisements shown and potentially generate a sale/lead transaction. In this case, Pay-Per-Click advertisements should be displayed. Choosing what advertisements to display entails retrieving the top advertisement categories for this specific user profile.</p><p>From the above example we are motivated to solve two problems simultaneously. We would like to know if the user's profile is frequent in the click stream, and we need to identify the top advertisements for this specific profile. That is, we need to solve both the frequent elements and the top-k problems.</p><p>The problems of finding frequent<ref type="foot" target="#foot_0">1</ref> and top-k elements are closely related, yet, to the best of our knowledge, no integrated solution has been proposed. In this paper, we propose an integrated approach for solving both problems of finding the top-k elements, and finding frequent elements in a data stream. Our Space-Saving algorithm reports both top-k and frequent elements with tight guarantees on errors. For general data distributions, Space-Saving answers top-k queries by returning a set of k elements, where k ≈ k, which are guaranteed to be the topk elements; and we use minimal space for calculating frequent elements. For realistic Zipfian data, our space requirement for the frequent elements problem decreases dramatically with the parameter of the distribution; and for top-k queries, we ensure that only the top-k elements, in the correct order, are reported.</p><p>The rest of the paper is organized as follows. Section 2 highlights the related work. In Section 3, we introduce our Space-Saving algorithm, and its associated data structure, followed by a discussion of query processing in Section 4. We comment on our experimental results in Section 5, and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Formally, given an alphabet, A, a frequent element, E i , is an element whose frequency, or number of hits, F i , in a stream S of a given size N , exceeds a user specified support φN , where 0 ≤ φ ≤ 1; whereas the top-k elements are the k elements with highest frequencies. Since the space requirements for exact solutions of these problems are impractical <ref type="bibr" target="#b2">[3]</ref>, other relaxations of the original problems were proposed. The FindCandidateTop(S, k, l) problem was proposed in <ref type="bibr" target="#b2">[3]</ref> to ask for l elements among which the top-k elements are concealed, with no guarantees on the rank of the remaining (l-k) elements. The FindApproxTop(S, k, ) <ref type="bibr" target="#b2">[3]</ref> is a more practical approximation for the top-k problem. The user asks for a list of k elements such that every element, E i , in the list has</p><formula xml:id="formula_0">F i &gt; (1 -)F k ,</formula><p>where is a user-defined error, and F 1 ≥ F 2 ≥ . . . ≥ F |A| , such that E k is the element with k th rank. The Hot Items 2 problem is a special case of the frequent elements problem, proposed in <ref type="bibr" target="#b12">[13]</ref>, that asks for k elements, each of which has frequency more than N k+1 . This extends the early work done in <ref type="bibr" target="#b1">[2]</ref>, and <ref type="bibr" target="#b7">[8]</ref> for identifying a majority element. The most popular variation of the frequent elements problem, finding the -Deficient Frequent Elements <ref type="bibr" target="#b10">[11]</ref>, asks for all the elements with fequency more than (φ -)N .</p><p>Several algorithms <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> have been proposed to handle the top-k, the frequent elements problems, and their variations. These techniques can be classified into counter-based, and sketch-based techniques.</p><p>Counter-based techniques keep an individual counter for each element in the monitored set, a subset of A. The counter of a monitored element, e i , is updated when e i occurs in the stream. If there is no counter kept for the observed ID, it is either disregarded, or some algorithm-dependent action is taken.</p><p>For solving the -Deficient Frequent Elements, algorithms Sticky Sampling, and Lossy Counting were proposed in <ref type="bibr" target="#b10">[11]</ref>. The algorithms cut the stream into rounds. Though simple and intuitive, they suffer from zeroing too many counters at rounds' boundaries, and thus, they free space before it is really needed. In addition, answering a frequent elements query entails scanning all counters.</p><p>Demaine et al. proposed the Frequent algorithm to solve the Hot Items problem in <ref type="bibr" target="#b5">[6]</ref>. Their algorithm, a re-discovery of the algorithm in <ref type="bibr" target="#b12">[13]</ref>, outputs a list of k elements with no guarantee on which elements, if any, have frequency more than N k+1 . The same algorithm was proposed independently by Karp et al. in <ref type="bibr" target="#b9">[10]</ref>. Frequent extends the early work done in <ref type="bibr" target="#b1">[2]</ref>, and <ref type="bibr" target="#b7">[8]</ref> for finding a majority item, using only one counter. Frequent <ref type="bibr" target="#b5">[6]</ref> keeps k counters to monitor k elements. If a monitored element is observed, its counter is incremented, else all counters are decremented. In case any counter reaches 0, it is assigned the next observed element. When the algorithm terminates, the monitored elements are the candidate frequent elements. <ref type="bibr" target="#b5">[6]</ref> proposed a lightweight data structure that can decrement all counters in O (1) operations. The sampling algorithm Probabilistic-InPlace <ref type="bibr" target="#b5">[6]</ref>, which is similar to Sticky Sampling <ref type="bibr" target="#b10">[11]</ref>, solves FindCandidateTop(S, k, m 2 ). When queried, the algorithm returns the upper half of the counters, in the hope that they are the correct top-k. Again, the algorithm deletes half the counters at rounds' boundaries, which is Ω(|distinct values of the deleted counters|). In general, counter-based techniques exhibit fast per-item processing.</p><p>Sketch-based techniques do not monitor a subset of elements, rather provide, with less stringent guarantees, frequency estimation for all elements using bit-maps of counters. Usually, each element is hashed into the space of counters using a family of hash functions, and the hashed-to counters are updated for every hit of this element. Those "representative" counters are then queried for the element frequency with less accuracy, due to hashing collisions.</p><p>The CountSketch algorithm, proposed in <ref type="bibr" target="#b2">[3]</ref>, solves the FindApproxTop(S, k, ) problem, with success probability (1 -δ). Its bottleneck is estimating the frequency of the element by finding the median of its representative counters.</p><p>The GroupTest algorithm, proposed in <ref type="bibr" target="#b4">[5]</ref>, answers queries about Hot Items, with a constant probability of failure, δ. A novel algorithm, FindMajority, was first devised to detect the majority element, assuming elements' IDs to be 1 . . . |A|. Then GroupTest, a probabilistic generalization, was devised that employs several independent copies of FindMajority. GroupTest is generally accurate. However, its space complexity is large, and it offers no information about elements' frequencies or relative order. The Multistage filters approach proposed in <ref type="bibr" target="#b6">[7]</ref>, which was also independently proposed in <ref type="bibr" target="#b8">[9]</ref>, is very similar to GroupTest.</p><p>Sketch-based techniques monitor all elements. However, a hit entails expensive calculations. They do not offer guarantees about relative order or estimated frequencies, and their space usage are not bounded by the size of the alphabet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarizing the Data Stream</head><p>The algorithms described in Section 2 handle individual problems. The main difficulty in devising an integrated solution is that queries of one type cannot serve as a pre-processing step for the other type of queries. For instance, the frequent elements receiving 1% or more of the total hits might constitute the top-100 elements, some of them, or none. In order to use frequent elements queries to pre-process the stream for a top-k query, several frequent elements queries have to be issued to reach a lower bound on the frequency of the k th element; and in order to use top-k queries to pre-process the stream for a frequent elements query, several top-k queries have to be issued to reach an upper bound on the number of frequent elements. To offer an integrated solution, we have generalized both problems to accurately estimate the frequencies of significant<ref type="foot" target="#foot_2">3</ref> elements, and store these frequencies in an always-sorted structure. We, then, devise a generalized algorithm for the generalized problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Space-Saving Algorithm</head><p>In this section, we propose our counter-based Space-Saving algorithm and its associated Stream-Summary data structure. The underlying idea is to maintain partial information of interest; i.e., we monitor only m elements. We update the counters in a way that accurately estimates the frequencies of the significant elements, and we use a lightweight data structure that keeps the elements sorted by their estimated frequencies. In an ideal situation, any significant element, E i , with rank i, that has received F i hits, should be accommodated in the i th counter. However, due to errors in estimating the frequencies of the elements, the order of the elements in the data structure might not reflect their exact ranks. For this reason, we will denote the counter at the i th position in the data structure count i . The counter count i estimates the frequency f i , of some element e i . If the i th position in the data structure has the right element, i.e., the element with the i th rank, E i , then e i = E i , and count i is an estimation of F i .</p><p>The algorithm is straightforward. If we observe an element, e, that is monitored, we just increment its counter. If e is not monitored, give it the benefit of doubt, and replace e m , the element that currently has the least estimated hits, min, with e. Assign count m the value min + 1. For each monitored element e i , we keep track of its over-estimation, ε i , resulting from the initialization of its counter when it was inserted into the list. That is, when starting to monitor e i , set ε i to the value of the evicted counter. The algorithm is depicted in Figure <ref type="figure" target="#fig_0">1</ref>.  In general, the top elements among non-skewed data are of no great significance. Hence, we concentrate on skewed datasets. The basic intuition is to make use of the skewed property of the data, since we expect a minority of the elements, the more frequent ones, to get the majority of the hits. Frequent elements will reside in the counters of bigger values, and will not be distorted by the ineffective hits of the infrequent elements, and thus, will never be replaced out of the monitored counters. Meanwhile, the numerous infrequent elements will be striving to reside on the smaller counters, whose values will grow slower than those of the larger counters. In addition, if the skew remains, but the popular elements change overtime, the algorithm adapts automatically. The elements that are growing more popular will gradually be pushed to the top of the list as they receive more hits. If one of the previously popular elements lost its popularity, it will receive less hits. Thus, its relative position will decline, as other counters get incremented, and it might eventually get dropped from the list.</p><p>Even if the data is not skewed, the errors in the counters will be inversely proportional to the number of counters, as shown later. Keeping only a moderate number of counters will guarantee very small errors. This is because the more counters we keep, the less it is probable to replace elements, and thus, the smaller the over-estimation errors in counters' values.</p><p>To implement this algorithm, we need a data structure that cheaply increments counters without violating their order, and that ensures constant time retrieval. We propose the Stream-Summary data structure for these purposes.</p><p>In a Stream-Summary, all elements with the same counter value are linked together in a linked list. They all point to a parent bucket. The value of the parent bucket is the same as the counters' value of all of its elements. Every bucket points to exactly one element among its child list, and buckets are kept in a doubly linked list, sorted by their values. Initially, all counters are empty, and are attached to a single parent bucket with value 0. The elements can be stored in a hash table for constant amortized access cost, or in an associative memory for constant worst case access cost. The Stream-Summary can be sequentially traversed as a sorted list, since the buckets' list is sorted. In case it is feasible to keep counters for all elements, Stream-Summary can be used to report both the most and the least significant elements. Reporting the least significant elements can be useful in some contexts, but it is beyond the scope of this paper.</p><p>The algorithm for counting elements' hits using Stream-Summary is straightforward. When an element's counter is updated, its bucket's neighbor with the larger value is checked. If it has a value equal to the new value of the element, then the element is detached from its current list, and is inserted in the child list of this neighbor. Otherwise, a new bucket with the correct value is created, and is attached to the bucket list in the right position; and this element is attached to this new bucket. The old bucket is deleted if it points to an empty child list. The worst case scenario costs 10 pointer assignments, and one heap operation. Stream-Summary is motivated by the work done in <ref type="bibr" target="#b5">[6]</ref>. However, to look up a value of a counter using the data structure in <ref type="bibr" target="#b5">[6]</ref>, it takes O (m), while Stream-Summary looks values up in Θ(1), for online queries about specific elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Properties of the Space-Saving Algorithm</head><p>To prove the space bounds in Section 4, we analyze some properties of Space-Saving, which will help us establish our space bounds. For space limitations, all proofs are omitted, and the reader is referred to the full version <ref type="bibr" target="#b11">[12]</ref>.</p><p>The strength behind our simple algorithm is that we keep the information until the space is absolutely needed, and we do not initialize counters in batches like other counter-based algorithms. This is what allowed us to prove these properties about the proposed algorithm.</p><p>A pivotal factor in our analysis is the value of min. The value of min is highly dynamic since it is dependent on the permutation of elements in S. We give an illustrative example. If m = 2, and = 4. S = X, Z, Y, Y yields min = 1, while S = X, Y, Y, Z yields min = 2. Although it would be very useful to quantify min, we do not want to involve the order in which hits were received in our analysis, because predicating the analysis on all possible stream permutations will be intractable. Thus, we establish an upper bound on min.</p><p>Lemma 1. The minimum counter value, min, is less than or equal to N m . We assume that the number of distinct elements in S to be more than m. Thus, all m counters are occupied. Otherwise, all counts are exact. We are interested in min since it represents an upper bound on the over-estimation in any counter in Stream-Summary. Moreover, any element e i , with frequency f i &gt; min, is guaranteed to be monitored, as shown next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.</head><p>An element e i with f i &gt; min, must exist in the Stream-Summary.</p><p>We can infer an interesting general rule about the over-estimation of elements' counters. For any element E i , with rank i ≤ m, the frequency F i , is no more than count i , the counter occupying the i th position in the Stream-Summary. Theorem 2. Whether or not E i occupies the i th position, count i ≥ F i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Processing Queries</head><p>In this section, we discuss query processing using the Stream-Summary data structure. We also analyze the space requirements for both the general case, where no data distribution is assumed, and the more interesting Zipfian case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Frequent Elements</head><p>In order to answer queries about the frequent elements, we sequentially traverse Stream-Summary as a sorted list until an element with frequency less than the user support is reached. Thus, we report frequent elements in Θ(|frequent elements|). If for each reported element e i , count i -ε i &gt; φN, then the algorithm guarantees that all, and only the frequent elements are reported. This guarantee is conveyed through the boolean parameter guaranteed. The number of counters, m, should be specified by the user according to the data properties, the required error rate and/or the available memory on the server. The QueryFrequent algorithm is given in Figure <ref type="figure" target="#fig_2">3</ref>. The General Case. We will analyze the space requirements for the general case of the data distribution. Theorem 3. Assuming no specific data distribution, or user-supplied support, to find all frequent elements with error , Space-Saving uses a number of counters that is bounded by min(|A|, 1 ). Any element, e i , with frequency f i &gt; N is guaranteed to be in the Stream-Summary.</p><p>Zipf Distribution Analysis. Assuming Zipfian data <ref type="bibr" target="#b13">[14]</ref>, with parameter α,</p><formula xml:id="formula_1">F i = N i α ζ(α) , where ζ(α) = |A| i=1 1</formula><p>i α converges to a small constant inversely proportional to α, except for α ≤ 1. For instance, ζ(1) ≈ ln(1.78|A|). We assume α ≥ 1, to ensure that the data is worth analyzing. As noted before, we do not expect the popular elements to be of great importance if the data is weakly skewed. Theorem 4. Assuming Zipfian data with parameter α, to calculate the frequent elements with error rate , Space-Saving uses only min(|A|, 1 1 α , 1 ) counters. This is regardless of the stream permutation.</p><p>Having established the bounds of Space-Saving for both the general, and the Zipf distributions, we compare them to other algorithms. We also comment on some practical issues, that can not be directly inferred from the bounds.</p><p>Comparison with Similar Work. The above bounds are better than those guaranteed in <ref type="bibr" target="#b10">[11]</ref>. Sticky Sampling has a bound of 2 log( 1 φδ ). Lossy Counting has a bound of 1 log( N ). Furthermore, Space-Saving has a better bound than GroupTest <ref type="bibr" target="#b4">[5]</ref>, whose bound is O ( 1 φ log( 1 δφ ) log(|A|)), which is less scalable than ours. For example, for N = 10 6 , |A| = 10 4 , φ = 10 -1 , = 10 -2 , and δ = 10 -1 , we need 100 counters, Sticky Sampling needs 700 counters, Lossy Counting needs 1000 counters, and GroupTest needs C * 930 counters, where C ≥ 1.</p><p>Frequent <ref type="bibr" target="#b5">[6]</ref> has a similar bound in the general case. Using m counters, the elements' under-estimation error is bounded by N -1  m . Although this is close to the theoretical under-estimation error bound, as proved in <ref type="bibr" target="#b0">[1]</ref>, there is no straightforward feasible extension of the algorithm to track the under-estimation error for each counter. In addition, every observation of a non-monitored element increases the errors for all the monitored elements, since their counters get decremented. Therefore, elements of higher frequency are more error prone, and thus, it is still difficult to guess the frequent elements, which is not the case for Space-Saving.</p><p>Even more, the structure proposed in <ref type="bibr" target="#b5">[6]</ref> is built and queried in a way that does not allow the user to specify an error threshold, . Thus, the algorithm has only one parameter, the support φ, which increases the number of false positives dramatically, as will be clear from the experimental results in Section 5.</p><p>The number of counters used in GroupTest <ref type="bibr" target="#b4">[5]</ref> depends on the failure probability, δ, as well as the support, φ. Thus, it does not suffer from the singlethreshold drawback of Frequent. However, it does not output frequencies at all, and reveals nothing about the relative order of the elements. In addition, its assumption that elememts' IDs are 1 . . . |A| can only be enforced by building an indexed lookup table that maps every ID to a unique number in the range 1 . . . |A|. Thus, in practice , GroupTest needs O (|A|) space, which is infeasible in most cases. Meanwhile, we only require the m IDs to fit in memory.</p><p>For the Zipfian case, we make no comparison to other works, since we are not aware of a similar analysis. For the numerical example given above, if α = 2, we would need only 10 counters, instead of 100, to guarantee the same error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Top-k Elements</head><p>For the top-k elements, the algorithm can output the first k elements. An element, e i , is guaranteed to be among the top-k if its guaranteed number of hits, count i -ε i , exceeds count k+1 , the over-estimated number of hits for the element in position k + 1. We call the results to have guaranteed top-k if by looking at the results only, we can tell that the reported top-k elements are correct. Space-Saving reports a guaranteed top-</p><formula xml:id="formula_2">k if ∀ i≤k , count i -ε i ≥ count k+1 .</formula><p>That is, all the reported k elements are guaranteed to be among the top-k.</p><p>All guaranteed top-i subsets, for all i, can be reported in Θ(m), by iterating on all the counters 1 . . . m -1. At each iteration, i, the min ∀ j≤i (count j -ε j ) is compared to count i+1 . The first i elements are guaranteed to be the top-i elements if this minimum is no smaller than count i+1 . The algorithm guarantees the top-m if in addition to this condition, ε m = 0; which is only true if the number of distinct elements in the stream is at most m.</p><p>Similarly, we call the top-k to have guaranteed order if ∀ i≤k , count i -ε i ≥ count i+1 . That is, in addition to having guaranteed top-k, the order of elements among the top-k elements are guaranteed to hold, if the guaranteed hits for every element in the top-k are more than the over-estimated hits of the next element. Thus, the order is guaranteed if the algorithm guarantees the top-i, ∀ i≤k . This is the first algorithm that can give guarantees about its output. For top-k queries, we can guarantee which reported elements are among the top-k. Even if we cannot guarantee all the top-k elements, we can guarantee top-k elements, where k ≈ k. For the case of Zipfian data, we guarantee that k = k, as shown later in the section. The algorithm QueryTop-k is given in Figure <ref type="figure">4</ref>. on the data distribution, Space-Saving can guarantee the reported top-k, or a subset of them, to be correct; while CountSketch does not offer any guarantees.</p><p>We can assume that field experts know whether the data is skewed enough to be considered Zipf <ref type="bibr" target="#b0">(1)</ref> or not. Even if this is not applicable, we can start by analyzing a sample from the data, and then re-size the structure accordingly.</p><p>In the case of Zipf Distribution, the bound of <ref type="bibr" target="#b2">[3]</ref> is</p><formula xml:id="formula_3">O (k log( N δ )). For α &gt; 1, the bound of Space-Saving is O ( k α 1 α k).</formula><p>Only when α = 1, the space complexity is O (k 2 log(|A|)), and thus, Space-Saving is better for cases of skewed data, long streams/windows, and has a 0-failure probability. In addition, we preserve the order of the top-k elements. For example, for N = 10 6 , |A| = 10 4 , k = 10, α = 2, and δ = 10 -1 our space requirements are only 66 counters, while <ref type="bibr" target="#b2">[3]</ref> needs C * 230 counters, where C 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zipf Alpha</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head><p>Space-Saving GroupTest Frequent (c) Recall for Frequent Elements (100,000 Hits) on Synthetic Data We conducted a set of experiments, using both real and synthetic data. For space constraints, we summarize our synthetic data results here. The real data experimental results agree with those presented here, and the reader is referred to <ref type="bibr" target="#b11">[12]</ref> for a full analysis on both the synthetic and real data experimental results. We generated several synthetic Zipfian datasets with the zipf parameter, α, varying from 0, which is uniform, to 3, which is highly skewed, on a fixed interval of 1  2 . The size of each dataset, N , is 10 7 hits. This set of experiments measure how the algorithms adapt to, and make use of data skew.</p><formula xml:id="formula_4">1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0.</formula><p>The algorithms were run on a Pentium IV 2.66 GHz, with 1.0 GB RAM. The stream was input and processed by each algorithm, and then a query was issued, and we recorded the recall, the number of correct elements found as a percentage of the number of actual correct elements; and the precision, the number of correct elements found as a percentage of the entire output <ref type="bibr" target="#b4">[5]</ref>. We also measured the run time and space used by each algorithm, which indicates the capability to deal with high-speed streams, and to reside on servers with limited memories.</p><p>For the frequent elements problem, we compared our results with those of GroupTest <ref type="bibr" target="#b4">[5]</ref>, and Frequent <ref type="bibr" target="#b5">[6]</ref>. For GroupTest, and Frequent, we used the C code available on the web-site of the first author of <ref type="bibr" target="#b4">[5]</ref>. Space-Saving, GroupTest, and Frequent were queried for the frequent elements with support, φ, of 10 -2 . We set , the error, to be one hundredth of φ, the required support; and δ, the failing probability, to be 0.01. Although Frequent ran up to four times faster than Space-Saving, as clear from Figure <ref type="figure">5</ref>(a), its results were not competitive in terms of precision. Since it is not possible to specify an parameter for the algorithm, its precision was very low in all the runs. When the Zipf parameter was 0.0, and 0.5, the algorithm reported 28, and 19 elements, respectively, and actually there were no elements satisfying the support. For the rest of the experiments in Figure <ref type="figure">5</ref>(b), the precision achieved by Frequent ranged from 0.053 to 0.216. The space used ranged from one fifth to five times the space of Space-Saving, as shown in Figure <ref type="figure">5(d)</ref>. It is interesting to note that as the data became more skewed, the space advantage of Space-Saving increased, while Frequent was not able to exploit the data skew to reduce its space requirements. Compared to GroupTest, from Figure <ref type="figure">5</ref>(a), Space-Saving ran 1.5 to 2 times faster than GroupTest. The precision of the proposed algorithm was always 1; while GroupTest precision depended on α, with a precision of 0.83 when α = 1, as sketched in Figure <ref type="figure">5(b)</ref>. The recalls of both algorithms were constant at 1, as clear from Figure <ref type="figure">5(c)</ref>. The advantage of Space-Saving is clear in Figure <ref type="figure">5(d)</ref>, which shows that Space-Saving achieved a reduction in the space used by a factor ranging from 2 up to 60.</p><p>For the top-k problem, we implemented Probabilistic-InPlace <ref type="bibr" target="#b5">[6]</ref>, and CountSketch <ref type="bibr" target="#b2">[3]</ref>. The Space-Saving, CountSketch, and Probabilistic-InPlace algorithms were used to identify the top-100 elements. For CountSketch, we set the probability of failure, δ, to 0.01. Both the Space-Saving, and the Probabilistic-InPlace were allowed the same number of counters; and thus, their run time and space usages were comparable, as clear from Figure <ref type="figure" target="#fig_4">6</ref>(a), and Figure <ref type="figure" target="#fig_4">6(d)</ref>, respectively. The precision of Probabilistic-InPlace increased from 0.02 to 0.36 as the skew increased; and finally reached 1, when α ≥ 2.5, as indicated in Figure <ref type="figure" target="#fig_4">6(b)</ref>. On the contrary, from Figure <ref type="figure" target="#fig_4">6</ref>(c), the recall of Probabilistic-InPlace was very high throughout the entire range of α. The precision and recall of Space-Saving were constant at 1. From Figure <ref type="figure" target="#fig_4">6</ref>(a), the time reductions of Space-Saving over CountSketch ranged from a factor of 30, to 82. Although we used a hidden factor of 16, as indicated earlier, CountSketch failed to attain a recall and precision of 1, for all the experiments. CountSketch had a very low precision and recall The performance gap increased with the data skew.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This paper has devised an integrated approach for solving an interesting family of problems in data streams. The Stream-Summary data structure was proposed, and utilized by the Space-Saving algorithm to guarantee strict error bounds for approximate counts of elements, using very limited space. We showed that Space-Saving can handle both the frequent elements and top-k problems because it efficiently estimates the elements' frequencies. The memory requirements were analyzed with special attention to the case of skewed data. We validated the theoretical analysis by experimental evaluation. This is the first algorithm, to the best of our knowledge, that guarantees the order of the top-k elements. Even when it cannot guarantee the top-k, the algorithm outputs guaranteed top-k elements, where k ≈ k.</p><p>In practice, if the alphabet is too large, like in the case of IP addresses, only a subset of this alphabet is observed in the stream, and not all the 2 32 addresses. Our space bounds are actually a function of the number of distinct elements observed in the stream. However, in our analysis, we have assumed that the entire alphabet is observed in the stream, which is the worst case for Space-Saving. Yet, our space bounds are still better than those of other algorithms.</p><p>The main practical strengths of Space-Saving is that it can use whatever space is available on the server to estimate the elements' frequencies, and provide guarantees on its results whenever possible. Even when analysts are not sure about the appropriate parameters, the algorithm can run in the available memory, and the results can be analyzed for further adaptation. It is interesting that running the algorithm on the available space ensures that more important elements are less susceptible to noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Space-Saving Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Example 1 .</head><label>21</label><figDesc>Fig. 2. Space-Saving updates to a Stream-Summary data structure as elements are observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm:Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Reporting Frequent Elements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance Comparison for the Top-k Problem Using Synthetic Zipfian Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Zipf Alpha Recall Space-Saving GroupTest Frequent (d) Space Used for Frequent Elements (100,000 Hits) on Synthetic Data</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>180000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>160000</cell><cell></cell><cell>168260</cell><cell>168260</cell><cell></cell><cell>168260</cell><cell>168260</cell><cell>168260</cell><cell>168260</cell><cell>168260</cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>140000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>120000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80000</cell><cell>58460</cell><cell>78460</cell><cell></cell><cell></cell><cell>67756</cell><cell></cell><cell></cell></row><row><cell>1 0.2 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20000 40000 60000</cell><cell></cell><cell>13760</cell><cell cols="2">13760 38240</cell><cell>13760</cell><cell>16588 13760</cell><cell>5636 13760</cell><cell>2796 13760</cell><cell>13760</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell>0</cell><cell></cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Zipf Alpha Space Used (Bytes) Space-Saving GroupTest Frequent</head><label></label><figDesc></figDesc><table><row><cell>Fig. 5. Performance Comparison for the Frequent Elements Problem Using Synthetic</cell></row><row><cell>Zipfian Data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The term "Heavy Hitters" was also used in<ref type="bibr" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The term "Hot Items" was coined later in<ref type="bibr" target="#b4">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The significant elements are interesting elements that can be output in meaningful queries about top-k or frequent elements.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>1 k+1 -1 |A| )) ≈ O ( k 2 log( N δ )).Even more, depending</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by NSF under grants EIA 00-80134, NSF 02-09112, and CNF 04-23336. Part of this work was done while the first author was at ValueClick, Inc.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The algorithm consists of two loops. The first loop outputs the top-k candidates. At each iteration the order of the elements reported so far is checked. If the order is violated, order is set to false. At the end of the loop, the top-k candidates are checked to be the guaranteed top-k, by checking that all of these candidates have guaranteed hits that exceed the overestimated counter of the k + 1 element, count k+1 . If this does not hold, the second loop is executed to search for the next k , where k ≈ k, such that the top-k are guaranteed.</p><p>The algorithm can also be implemented in a way that only outputs the first k elements, or that outputs k elements, such that k is the closest possible to k, regardless of whether k is greater than k, or vice versa. Throughout the rest of the paper, we assume that the algorithm outputs only the first k elements. Next, we look at the space requirements for solving this problem.</p><p>The General Case. We start by considering data which is not as skewed as Zipf of parameter 1. We deal with skewed data later. We also restrict the discussion to the relaxed version, FindApproxTop(S, k, ) <ref type="bibr" target="#b2">[3]</ref>, which is finding a list of k elements, each of which has frequency more than (1 -)F k .</p><p>Theorem 5. Regardless of the data distribution, to solve the FindApproxTop(S, k, ) problem, Space-Saving uses min(|A|, N F k ) counters. Any element with frequency more than (1 -)F k is guaranteed to be monitored.</p><p>For the general case, we were not able to solve the exact problem, and we only considered a relaxed version, since it is widely accepted that solving the exact problem requires Θ(|A|) space <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zipf Distribution</head><p>Analysis. To answer exact top-k queries, can be automatically set less than F k -F k+1 . Thus, we guarantee correctness, and order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 6. Assuming the data is Zipfian with parameter</head><p>. This is regardless of the stream permutation. Also, the order among the top-k elements is preserved.</p><p>To the best of our knowledge, this is the first work to look at the space bounds for solving the exact problem, in the case of Zipfian data, with guaranteed results. Having established the bounds of Space-Saving for both the general, and the Zipf distributions, we compare these bounds to other algorithms.</p><p>Comparison with Similar Work. These bounds are better than the bounds guaranteed by the best known algorithm, CountSketch <ref type="bibr" target="#b2">[3]</ref>, for a large range of practical values of the parameters |A|, , and k. CountSketch solves the relaxed version of the problem, FindApproxTop(S, k, ), with failure probability δ, using</p><p>), with a large constant hidden in the big-O notation, according to <ref type="bibr" target="#b2">[3]</ref>, and <ref type="bibr" target="#b4">[5]</ref>. The bound of Space-Saving for the relaxed problem is N F k , with a 0-failure probability. For instance, for N = 10 6 , |A| = 10 4 , k = 100, and = δ = 10 -1 , and a uniformly distributed data, we require 10 3 counters, while CountSketch needs C * 2.3 * 10 7 counters, where C 1, which is more than the entire stream. In addition, Space-Saving guarantees that any element, e i , whose f i &gt; (1 -)F k belongs to the Stream-Summary, and does not simply output a random k selection of these elements.</p><p>In case of a non-Zipf distribution, or a weakly skewed Zipf distribution with α &lt; 1, for all i ≥ k, we will assume that</p><p>This assumption is justified. Since we are assuming a non-skewed distribution, the top few elements have a less significant share in the stream than in the case of Zipf(1), and thus, less frequent elements will have a larger share. Using this assumption, we rewrite the bound of Space-Saving as O ( k * log(N ) ); while the bound in <ref type="bibr" target="#b2">[3]</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bounds for Frequency Estimation of Packet Streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kranakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Colloquium on Structural Information and Communication Complexity</title>
		<meeting>the 10th International Colloquium on Structural Information and Communication Complexity</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Fast Majority Vote Algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<idno>1981- 32</idno>
		<imprint>
			<date type="published" when="1981-02">February 1981</date>
			<pubPlace>Austin</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Computing Science, University of Texas</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding Frequent Items in Data Streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Colloquium on Automata, Languages and Programming</title>
		<meeting>the 29th International Colloquium on Automata, Languages and Programming</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding Hierarchical Heavy Hitters in Data Streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Very Large Data Bases</title>
		<meeting>the 29th International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="464" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Whats Hot and Whats Not: Tracking Most Frequent Items Dynamically</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Symposium on Principles of Databse Systems</title>
		<meeting>the 22nd Symposium on Principles of Databse Systems</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="296" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frequency Estimation of Internet Packet Streams with Limited Space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lpez-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual European Symposium on Algorithms</title>
		<meeting>the 10th Annual European Symposium on Algorithms</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="348" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">New Directions in Traffic Measurement and Accounting: Focusing on the Elephants, Ignoring the Mice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="313" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding a Majority Among N Votes: Solution to Problem 81-5</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="376" to="379" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamically Maintaining Frequent Items over A Data Stream</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<meeting>the Twelfth International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Simple Algorithm for Finding Frequent Elements in Streams and Bags</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="55" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximate Frequency Counts over Data Streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Very Large Data Bases</title>
		<meeting>the 28th International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="346" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient Computation of Frequent and Top-k Elements in Data Streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metwally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Abbadi</surname></persName>
		</author>
		<idno>2005-23</idno>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Santa Barbara</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Repeated Elements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science of Computer Programming</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="1982-11">November 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human Behavior and The Principle of Least Effort</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
