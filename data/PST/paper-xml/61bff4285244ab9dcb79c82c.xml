<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WebGPT: Browser-assisted question-answering with human feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">Schulman</forename><surname>Openai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jeff Wu Long Ouyang Christina Kim</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WebGPT: Browser-assisted question-answering with human feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We fine-tune GPT-3 to answer long-form questions using a text-based webbrowsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit. * Equal contribution, order randomized 2 https://www.microsoft.com/en-us/bing/apis/bing-web-search-api * 256 for the 760M BC model * * 32 for the 175B RM</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance <ref type="bibr" target="#b18">[Krishna et al., 2021]</ref>. Existing work tends to focus on two core components of the task, information retrieval and synthesis.</p><p>In this work we leverage existing solutions to these components: we outsource document retrieval to the Microsoft Bing Web Search API, 2 and utilize unsupervised pre-training to achieve high-quality synthesis by fine-tuning <ref type="bibr">GPT-3 [Brown et al., 2020]</ref>. Instead of trying to improve these ingredients, we focus on combining them using more faithful training objectives. Following <ref type="bibr" target="#b29">Stiennon et al. [2020]</ref>, we use human feedback to directly optimize answer quality, allowing us to achieve performance competitive with humans.</p><p>We make two key contributions: From Gifts From Crows | Outside My Window (www.birdsoutsidemywindow.org) &gt; Many animals give gifts to members of their own species but crows and other corvids are the only ones known to give gifts to humans. Search results for: how to train crows to bring you gifts ♦Scrollbar: 0 -11 ♦Text 【0 †How to Make Friends With Crows -PetHelpful †pethelpful.com】 If you did this a few times, your crows would learn your new place, but as I said, I'm not sure if they will follow or visit you there since it's probably not in their territory. The other option is simply to make new crow friends with the crows that live in your new neighborhood.</p><p>【1 †Gifts From Crows | Outside My Window †www.birdsoutsidemywindow.org】 The partial piece of apple may have been left behind when the crow was startled rather than as a gift. If the crows bring bright objects you'll know for sure that it's a gift because it's not something they eat. Brandi Williams says: May 28, 2020 at 7:19 am.  • We create a text-based web-browsing environment that a fine-tuned language model can interact with. This allows us to improve both retrieval and synthesis in an end-to-end fashion using general methods such as imitation learning and reinforcement learning. • We generate answers with references: passages extracted by the model from web pages while browsing. This is crucial for allowing labelers to judge the factual accuracy of answers, without engaging in a difficult and subjective process of independent research.</p><p>Our models are trained primarily to answer questions from ELI5 <ref type="bibr" target="#b9">[Fan et al., 2019]</ref>, a dataset of questions taken from the "Explain Like I'm Five" subreddit. We collect two additional kinds of data: demonstrations of humans using our web-browsing environment to answer questions, and comparisons between two model-generated answers to the same question (each with their own set of references). Answers are judged for their factual accuracy, coherence, and overall usefulness.</p><p>We use this data in four main ways: behavior cloning (i.e., supervised fine-tuning) using the demonstrations, reward modeling using the comparisons, reinforcement learning against the reward model, and rejection sampling against the reward model. Our best model uses a combination of behavior cloning and rejection sampling. We also find reinforcement learning to provide some benefit when inference-time compute is more limited.</p><p>We evaluate our best model in three different ways. First, we compare our model's answers to answers written by our human demonstrators on a held-out set of questions. Our model's answers are preferred 56% of the time, demonstrating human-level usage of the text-based browser. Second, we compare our model's answers (with references stripped, for fairness) to the highest-voted answer provided by the ELI5 dataset. Our model's answers are preferred 69% of the time. Third, we evaluate our model on TruthfulQA <ref type="bibr" target="#b22">[Lin et al., 2021]</ref>, an adversarial dataset of short-form questions. Our model's answers are true 75% of the time, and are both true and informative 54% of the time, outperforming our base model (GPT-3), but falling short of human performance.</p><p>The remainder of the paper is structured as follows:</p><p>• In Section 2, we describe our text-based web-browsing environment and how our models interact with it. • In Section 3, we explain our data collection and training methods in more detail.</p><p>• In Section 4, we evaluate our best-performing models (for different inference-time compute budgets) on ELI5 and TruthfulQA. • In Section 5, we provide experimental results comparing our different methods and how they scale with dataset size, parameter count, and inference-time compute. • In Section 6, we discuss the implications of our findings for training models to answer questions truthfully, and broader impacts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Environment design</head><p>Previous work on question-answering such as REALM <ref type="bibr" target="#b13">[Guu et al., 2020]</ref> and RAG <ref type="bibr" target="#b20">[Lewis et al., 2020a]</ref> has focused on improving document retrieval for a given query. Instead, we use a familiar existing method for this: a modern search engine (Bing). This has two main advantages. First, modern search engines are already very powerful, and index a large number of up-to-date documents. Second, it allows us to focus on the higher-level task of using a search engine to answer questions, something that humans can do well, and that a language model can mimic.</p><p>For this approach, we designed a text-based web-browsing environment. The language model is prompted with a written summary of the current state of the environment, including the question, the text of the current page at the current cursor location, and some other information (see Figure <ref type="figure" target="#fig_3">1(b)</ref>). In response to this, the model must issue one of the commands given in Table <ref type="table" target="#tab_0">1</ref>, which performs an action such as running a Bing search, clicking on a link, or scrolling around. This process is then repeated with a fresh context (hence, the only memory of previous steps is what is recorded in the summary).</p><p>While the model is browsing, one of the actions it can take is to quote an extract from the current page. When this is performed, the page title, domain name and extract are recorded to be used later as a reference. Browsing then continues until either the model issues a command to end browsing, the maximum number of actions has been reached, or the maximum total length of references has been reached. At this point, as long as there is at least one reference, the model is prompted with the question and the references, and must compose its final answer.</p><p>Further technical details about our environment can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collection</head><p>Guidance from humans is central to our approach. A language model pre-trained on natural language would not be able to use our text-based browser, since it does not know the format of valid commands. We therefore collected examples of humans using the browser to answer questions, which we call demonstrations. However, training on demonstrations alone does not directly optimize answer quality, and is unlikely to lead far beyond human performance <ref type="bibr" target="#b29">[Stiennon et al., 2020]</ref>. We therefore collected pairs of model-generated answers to the same question, and asked humans which one they preferred, which we call comparisons.</p><p>For both demonstrations and comparisons, the vast majority of questions were taken from ELI5 <ref type="bibr" target="#b9">[Fan et al., 2019]</ref>, a dataset of long-form questions. For diversity and experimentation, we also mixed in a small number of questions from other sources, such as TriviaQA <ref type="bibr" target="#b16">[Joshi et al., 2017]</ref>. In total, we collected around 6,000 demonstrations, 92% of which were for questions from ELI5, and around 21,500 comparisons, 98% of which were for questions from ELI5. A more detailed breakdown of the questions we used along with post-processing details can be found in Appendix B. Table <ref type="table">2</ref>: An answer produced by our 175B best-of-64 model to a randomly-chosen question from the ELI5 test set (not cherry-picked). The full text of the references can be found in Appendix J, along with answers from our human demonstrators and the ELI5 dataset. Further samples are available at https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Why did we decide that certain words were "bad" and shouldn't be used in social settings?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>We have all been taught that there are certain words that should not be used in public, at least not without good reason. But why do some words fall into this category while others do not? It turns out that there is not a single answer to this question, as each word has a different history and carries different meanings in different cultures and families <ref type="bibr">[2]</ref>. However, there are some general categories that "bad" words tend to fall into. "Bad" words generally relate to parts of life that we don't like talking about in public, like bathroom functions, or negative ways of talking about people's religion, skin color, ability, or gender <ref type="bibr">[3]</ref>. Often, people say bad words when they are experiencing strong emotions, and those words are linked to our emotions <ref type="bibr">[3]</ref>.</p><p>To make it easier for humans to provide demonstrations, we designed a graphical user interface for the environment (see Figure <ref type="figure" target="#fig_3">1(a)</ref>). This displays essentially the same information as the text-based interface and allows any valid action to be performed, but is more human-friendly. For comparisons, we designed a similar interface, allowing auxiliary annotations as well as comparison ratings to be provided, although only the final comparison ratings (better, worse or equally good overall) were used in training.</p><p>For both demonstrations and comparisons, we emphasized that answers should be relevant, coherent, and supported by trustworthy references. Further details about these criteria and other aspects of our data collection pipeline can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>The use of pre-trained models is crucial to our approach. Many of the underlying capabilities required to successfully use our environment to answer questions, such as reading comprehension and answer synthesis, emerge as zero-shot capabilities of language models <ref type="bibr" target="#b4">[Brown et al., 2020]</ref>. We therefore fine-tuned models from the GPT-3 model family, focusing on the 760M, 13B and 175B model sizes.</p><p>Starting from these models, we used four main training methods:</p><p>1. Behavior cloning (BC). We fine-tuned on the demonstrations using supervised learning, with the commands issued by the human demonstrators as labels. 2. Reward modeling (RM). Starting from the BC model with the final unembedding layer removed, we trained a model to take in a question and an answer with references, and output a scalar reward. Following <ref type="bibr" target="#b29">Stiennon et al. [2020]</ref>, the reward represents an Elo score, scaled such that the difference between two scores represents the logit of the probability that one will be preferred to the other by the human labelers. The reward model is trained using a cross-entropy loss, with the comparisons as labels. Ties are treated as soft 50% labels. 3. Reinforcement learning (RL). Once again following <ref type="bibr" target="#b29">Stiennon et al. [2020]</ref>, we fine-tuned the BC model on our environment using PPO <ref type="bibr" target="#b26">[Schulman et al., 2017]</ref>. For the environment reward, we took the reward model score at the end of each episode, and added this to a KL penalty from the BC model at each token to mitigate overoptimization of the reward model. 4. Rejection sampling (best-of-n). We sampled a fixed number of answers <ref type="bibr">(4, 16 or 64)</ref> from either the BC model or the RL model (if left unspecified, we used the BC model), and selected the one that was ranked highest by the reward model. We used this as an alternative method of optimizing against the reward model, which requires no additional training, but instead uses more inference-time compute.</p><p>We used mutually disjoint sets of questions for each of BC, RM and RL.</p><p>For BC, we held out around 4% of the demonstrations to use as a validation set.</p><p>For RM, we sampled answers for the comparison datasets in an ad-hoc manner, using models of various sizes (but primarily the 175B model size), trained using various combinations of methods and hyperparameters, and combined them into a single dataset. This was for data efficiency: we collected many comparisons for evaluation purposes, such as for tuning hyperparameters, and did not want to waste this data. Our final reward models were trained on around 16,000 comparisons, the remaining 5,500 being used for evaluation only.</p><p>For RL, we trained on a mixture of 90% questions from ELI5 and 10% questions from TriviaQA.</p><p>To improve sample efficiency, at the end of each episode we inserted 15 additional answering-only episodes using the same references as the previous episode. We were motivated to try this because answering explained slightly more of the variance in reward model score than browsing despite taking many fewer steps, and we found it to improve sample efficiency by approximately a factor of 2. We also randomized the maximum number of browsing actions, sampling uniformly from the range 20-100 inclusive.</p><p>Hyperparameters for all of our training methods can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In evaluating our approach, we focused on three "WebGPT" models, each of which was trained with behavior cloning followed by rejection sampling against a reward model of the same size: a 760M best-of-4 model, a 13B best-of-16 model and a 175B best-of-64 model. As discussed in Section 5.2, these are compute-efficient models corresponding to different inference-time compute budgets. We excluded RL for simplicity, since it did not provide significant benefit when combined with rejection sampling (see Figure <ref type="figure" target="#fig_6">4</ref>).</p><p>We evaluated all WebGPT models using a sampling temperature of 0.8, which was tuned using human evaluations, and with a maximum number of browsing actions of 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ELI5</head><p>We evaluated WebGPT on the ELI5 test set in two different ways:</p><p>1. We compared model-generated answers to answers written by demonstrators using our web-browsing environment. For these comparisons, we used the same procedure as comparisons used for reward model training. We consider this to be a fair comparison, since the instructions for demonstrations and comparisons emphasize a very similar set of criteria. 2. We compared model-generated answers to the reference answers from the ELI5 dataset, which are the highest-voted answers from Reddit. In this case, we were concerned about ecological validity, since our detailed comparison criteria may not match those of real-life users. We were also concerned about blinding, since Reddit answers do not typically include citations. To mitigate these concerns, we stripped all citations and references from the model-generated answers, hired new contractors who were not familiar with our detailed instructions, and gave them a much more minimal set of instructions, which are given in Appendix F.</p><p>In both cases, we treat ties as 50% preference ratings (rather than excluding them).</p><p>Our results are shown in Figure <ref type="figure">2</ref>. Our best model, the 175B best-of-64 model, produces answers that are preferred to those written by our human demonstrators 56% of the time. This suggests that the use of human feedback is essential, since one would not expect to exceed 50% preference by imitating demonstrations alone (although it may still be possible, by producing a less noisy policy).</p><p>The same model produces answers that are preferred to the reference answers from the ELI5 dataset 69% of the time. This is a substantial improvement over <ref type="bibr" target="#b18">Krishna et al. [2021]</ref>, whose best model's answers are preferred 23% of the time to the reference answers, although they use substantially less compute than even our smallest model.</p><p>Although the evaluations against the ELI5 reference answers are useful for comparing to prior work, we believe that the evaluations against human demonstrations are more meaningful, for several reasons: Figure <ref type="figure">2</ref>: Human evaluations on ELI5 comparing against (a) demonstrations collected using our web browser, (b) the highest-voted answer for each question. The amount of rejection sampling (the n in best-of-n) was chosen to be compute-efficient (see Figure <ref type="figure">8</ref>). Error bars represent ±1 standard error.</p><p>• Fact-checking. It is difficult to assess the factual accuracy of answers without references: even with the help of a search engine, expertise is often required. However, WebGPT and human demonstrators provide answers with references. • Objectivity. The use of minimal instructions makes it harder to know what criteria are being used to choose one answer over another. Our more detailed instructions enable more interpretable and consistent comparisons. • Blinding. Even with citations and references stripped, WebGPT composes answers that are different in style to Reddit answers, making the comparisons less blinded. In contrast, WebGPT and human demonstrators compose answers in similar styles. Additionally, some ELI5 answers contained links, which we instructed labelers not to follow, and this could have biased labelers against those answers. • Answer intent. People ask questions on ELI5 to obtain original, simplified explanations rather than answers that can already be found on the web, but these were not criteria we wanted answers to be judged on. Moreover, many ELI5 questions only ever get a small number of low-effort answers. With human demonstrations, it is easier to ensure that the desired intent and level of effort are used consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TruthfulQA</head><p>To further probe the abilities of WebGPT, we evaluated WebGPT on TruthfulQA <ref type="bibr" target="#b22">[Lin et al., 2021]</ref>, an adversarially-constructed dataset of short-form questions. TruthfulQA questions are crafted such that they would be answered falsely by some humans due to a false belief or misconception. Answers are scored on both truthfulness and informativeness, which trade off against one another (for example, "I have no comment" is considered truthful but not informative).</p><p>We evaluated both the base GPT-3 models used by WebGPT and the WebGPT models themselves on TruthfulQA. For GPT-3, we used both the "QA prompt" and the "helpful prompt" from <ref type="bibr" target="#b22">Lin et al. [2021]</ref>, and used the automated metric, since this closely tracks human evaluation on answers produced by the GPT-3 model family. For WebGPT, we used human evaluation, since WebGPT's answers are out-of-distribution for the automated metric. TruthfulQA is a short-form dataset, so we also truncated WebGPT's answers to 50 tokens in length, and then removed any trailing partial sentences. WebGPT GPT-3</p><formula xml:id="formula_0">(QA prompt) GPT-3 (helpful prompt) Truthful (%)</formula><p>Truthful and informative (%)</p><p>Figure <ref type="figure">3</ref>: TruthfulQA results. The amount of rejection sampling (the n in best-of-n) was chosen to be compute-efficient (see Figure <ref type="figure">8</ref>). Error bars represent ±1 standard error.</p><p>Our results are shown in Figure <ref type="figure">3</ref>. All WebGPT models outperform all GPT-3 models (with both prompts) on both the percentage of truthful answers and the percentage of truthful and informative answers. Moreover, the percentage of truthful and informative answers increases with model size for WebGPT, unlike GPT-3 with either prompt. Further qualitative analysis of WebGPT's performance on TruthfulQA is given in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TriviaQA</head><p>We also evaluated the WebGPT 175B BC model on TriviaQA <ref type="bibr" target="#b16">[Joshi et al., 2017]</ref>. These results are given in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison of training methods</head><p>We ran a number of additional experiments comparing reinforcement learning (RL) and rejection sampling (best-of-n) with each other and with the behavior cloning (BC) baseline. Our results are shown in Figures <ref type="figure" target="#fig_7">4 and 5</ref>. Rejection sampling provides a substantial benefit, with the 175B best-of-64 BC model being preferred 68% of the time to the 175B BC model. Meanwhile, RL provides a smaller benefit, with the 175B RL model being preferred 58% of the time to the 175B BC model.</p><p>Even though both rejection sampling and RL optimize against the same reward model, there are several possible reasons why rejection sampling outperforms RL:</p><p>• It may help to have many answering attempts, simply to make use of more inference-time compute. • The environment is unpredictable: with rejection sampling, the model can try visiting many more websites, and then evaluate the information it finds with the benefit of hindsight. • The reward model was trained primarily on data collected from BC and rejection sampling policies, which may have made it more robust to overoptimization by rejection sampling than by RL. • RL requires hyperparameter tuning, whereas rejection sampling does not.  The combination of RL and rejection sampling also fails to offer much benefit over rejection sampling alone. One possible reason for this is that RL and rejection sampling are optimizing against the same reward model, which can easily be overoptimized (especially by RL, as noted above). In addition to this, RL reduces the entropy of the policy, which hurts exploration. Adapting the RL objective to optimize rejection sampling performance is an interesting direction for future research.</p><p>It is also worth highlighting the importance of carefully tuning the BC baseline for these comparisons.</p><p>As discussed in Appendix E, we tuned the number of BC epochs and the sampling temperature using a combination of human evaluations and reward model score. This alone closed much of the gap we originally saw between BC and RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scaling experiments</head><p>We also conducted experiments to investigate how model performance varied with the size of the dataset, the number of model parameters, and the number of samples used for rejection sampling.</p><p>Since human evaluations can be noisy and expensive, we used the score of a 175B "validation" reward model (trained on a separate dataset split) for these experiments. We found this to be a good predictor of human preference when not optimizing against a reward model using RL (see Figure <ref type="figure" target="#fig_7">5</ref>). Recall that the reward represents an Elo score, with a difference of 1 point representing a preference of sigmoid(1) ≈ 73%.</p><p>Scaling trends with dataset size and parameter count are shown in Figures <ref type="figure">6 and 7</ref>. For dataset size, doubling the number of demonstrations increased the policy's reward model score by about 0.13, and doubling the number of comparisons increased the reward model's accuracy by about 1.8%. For parameter count, the trends were noisier, but doubling the number of parameters in the policy increased its reward model score by roughly 0.09, and doubling the number of parameters in the reward model increased its accuracy by roughly 0.4%.</p><p>For rejection sampling, we analyzed how to trade off the number of samples against the number of model parameters for a given inference-time compute budget (see Figure <ref type="figure">8</ref>). We found that it is generally compute-efficient to use some amount of rejection sampling, but not too much. The models for our main evaluations come from the Pareto frontier of this trade-off: the 760M best-of-4 model, the 13B best-of-16 model, and the 175B best-of-64 model. 6 Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Truthfulness of WebGPT</head><p>As NLP systems improve and become more widely deployed, it is becoming increasingly important to develop techniques for reducing the number of false statements they make <ref type="bibr" target="#b8">[Evans et al., 2021]</ref>.</p><p>To assess the contribution of WebGPT to this aim, it is helpful to distinguish two categories of false statement made by a model:</p><p>1. Imitative falsehoods. These are false statements that are incentivized by the training objective (even in the limit of infinite data and compute), such as reproducing common misconceptions <ref type="bibr" target="#b22">[Lin et al., 2021</ref>]. 2. Non-imitative falsehoods. These are false statements that are the result of the model failing to achieve its training objective, including most hallucinations, which are statements that are false, but look plausible at a glance <ref type="bibr" target="#b23">[Maynez et al., 2020]</ref>.</p><p>Our TruthfulQA results suggest that WebGPT produces fewer imitative falsehoods than GPT-3. We believe this is because WebGPT is incentivized to prefer reliable sources (both because of filtering performed by the Bing API, and because we specify this in our instructions). Nevertheless, as shown in Table <ref type="table">3</ref>, WebGPT still sometimes quotes from highly unreliable sources in response to TruthfulQA questions. We hypothesize that this is because of the distribution shift from ELI5 to TruthfulQA, and that training on adversarially-selected questions is a promising way to improve this. It would be important in such an endeavor to pay close attention to labeler judgments of source trustworthiness (see Appendix C).</p><p>Our results on ELI5 suggest that WebGPT also produces fewer non-imitative falsehoods than GPT-3. We did not test this hypothesis directly, since we found that it was challenging for labelers to spot subtle hallucinations. However, prior work shows that the use of retrieval reduces the rate of hallucinations <ref type="bibr" target="#b28">[Shuster et al., 2021]</ref>, and moreover WebGPT performs about as well as human demonstrations for factual accuracy on ELI5 (see Figure <ref type="figure">2</ref>(a)). Nevertheless, WebGPT still sometimes produces non-imitative falsehoods, which are typically mistakes when attempting to paraphrase or synthesize information rather than wild hallucinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Perceived truthfulness of WebGPT</head><p>In order to assess the benefits and risks of WebGPT, it is necessary to consider not only how often it makes false statements, but also how likely users are to rely on those statements. Although WebGPT makes false statements less frequently than GPT-3, its answers also appear more authoritative, partly because of the use of citations. In combination with the well-documented problem of "automation bias" <ref type="bibr" target="#b11">[Goddard et al., 2012]</ref>, this could lead to overreliance on WebGPT's answers. This is particularly Table <ref type="table">3</ref>: Two questions from TruthfulQA, cherry-picked to highlight a success and a failure of WebGPT. While GPT-3 175B with the helpful prompt answers "I have no comment" to 49% of questions, WebGPT almost always tries to answer the question, but sometimes quotes from unreliable sources. In spite of this, WebGPT still answers more truthfully overall (see Figure <ref type="figure">3</ref>). Key: ✗ = false, ✓ = true but uninformative, ✓ = true and informative GPT-3 175B (QA prompt)</p><p>GPT-3 175B (helpful prompt)</p><p>WebGPT 175B best-of-64 (first sentence)</p><p>What happens if you smash a mirror?</p><p>If you smash a mirror, you will have seven years of bad luck.</p><p>I have no comment.</p><p>When you break a mirror you might cut yourself and people might be angry at you for doing it on purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>✗ ✓ ✓</head><p>If you dream of doing something and make a wish, will you succeed?</p><p>I have no comment. I have no comment.</p><p>It is true that you can make a wish come true by the power of thought.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>✓ ✓ ✗</head><p>problematic because, as discussed in Section 6.1, WebGPT can make more mistakes than humans on out-of-distribution questions. Documentation of these limitations could help inform those interacting with WebGPT, and further research is required to understand how else to mitigate this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Reinforcement of bias</head><p>There are a number of ways in which WebGPT tends to perpetuate and reinforce existing assumptions and biases. Firstly, WebGPT inherits the biases of the base model from which it is fine tuned, <ref type="bibr">GPT-3 [Brown et al., 2020]</ref>, and this influences the way in which it chooses to search for and synthesize information. Search and synthesis both depend on the ability to include and exclude material depending on some measure of its value, and by incorporating GPT-3's biases when making these decisions, WebGPT can be expected to perpetuate them further. Secondly, the fact that WebGPT synthesizes information from existing sources gives it the potential to reinforce and entrench existing beliefs and norms. Finally, WebGPT usually accepts the implicit assumptions made by questions, and more generally seems to be influenced by the stance taken by questions. This is something that could exacerbate confirmation bias in users.</p><p>These problems could be mitigated with improvements both to WebGPT's base model and to WebGPT's training objective, and we discuss some alternative objectives in the next section. It may also be important to control how WebGPT is used, both by limiting access and by tailoring the design and documentation of applications.</p><p>Additional analysis of the effect of question stance and of reference point bias is given in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Using references to evaluate factual accuracy</head><p>Central to our approach is the use of references collected by the model to aid human evaluation of factual accuracy. This was previously suggested by <ref type="bibr" target="#b24">Metzler et al. [2021]</ref>, and has several benefits:</p><p>• More accurate feedback. It is very challenging to evaluate the factual accuracy of arbitrary claims, which can be technical, subjective or vague. In contrast, it is much easier to evaluate how well a claim is supported by a set of sources.</p><p>• Less noisy feedback. It is also easier to specify an unambiguous procedure for evaluating how well a claim is supported by a set of sources, compared to evaluating the factual accuracy of an arbitrary claim. This improves agreement rates between labelers, which helps data efficiency. • Transparency. It is much easier to understand how WebGPT composes answers than it is for GPT-3, since the entire browsing process can be inspected. It is also straightforward for end-users to follow up on sources to better judge factual accuracy for themselves.</p><p>Despite these benefits, references are far from a panacea. Our current procedure incentivizes models to cherry-pick references that they expect labelers to find convincing, even if those references do not reflect a fair assessment of the evidence. As discussed in Section 6.3, there are early signs of this happening, with WebGPT accepting the implicit assumptions of questions, and the problem is likely to be exacerbated by more capable models and more challenging or subjective questions. We could mitigate this using methods like debate <ref type="bibr" target="#b15">[Irving et al., 2018]</ref>, in which models are trained to find evidence both for and against different claims. Such setups can also be viewed as simple cases of recursive reward modeling <ref type="bibr" target="#b19">[Leike et al., 2018]</ref> and Iterated Amplification <ref type="bibr" target="#b7">[Christiano et al., 2018]</ref>, in which the model assists its own evaluation.</p><p>Our approach also raises a challenging problem with societal implications: how should factual accuracy be evaluated when training AI systems? <ref type="bibr">Evans et al. [2021, Section 2]</ref> propose a number of desiderata, but a substantial gap remains between these and the highly specific criteria needed to train current AI systems with reasonable data efficiency. We made a number of difficult judgment calls, such as how to rate the trustworthiness of sources (see Appendix C), which we do not expect universal agreement with. While WebGPT did not seem to take on much of this nuance, we expect these decisions to become increasingly important as AI systems improve, and think that cross-disciplinary research is needed to develop criteria that are both practical and epistemically sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Risks of live web access</head><p>At both train and inference time, WebGPT has live access to the web via our text-based browsing environment. This enables the model to provide up-to-date answers to a wide range of questions, but potentially poses risks both to the user and to others. For example, if the model had access to forms, it could edit Wikipedia to construct a reliable-looking reference. Even if human demonstrators did not perform such behavior, it would likely be reinforced by RL if the model were to stumble across it.</p><p>We believe the risk posed by WebGPT exploiting real-world side-effects of its actions is very low. This is because the only interactions with the outside world allowed by the environment are sending queries to the Bing API and following links that already exist on the web, and so actions like editing Wikipedia are not directly available to the model. While a capable enough system could escalate these privileges <ref type="bibr" target="#b14">[Harms, 2016]</ref>, WebGPT's capabilities seem far below what would be required to achieve this.</p><p>Nevertheless, much more capable models could potentially pose much more serious risks <ref type="bibr" target="#b4">[Bostrom, 2014]</ref>. For this reason, we think as the capabilities of models increase, so should the burden of proof of safety for giving them access to the web, even at train time. As part of this, measures such as tripwire tests could be used to help catch exploitative model behavior early.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Combining machine learning with an external knowledge base, for the task of question-answering, preceded the rise of pre-trained language models in the late 2010s. One notable system of this kind was DeepQA (also known as IBM Watson), which was used to beat the best humans at Jeopardy <ref type="bibr" target="#b10">[Ferrucci et al., 2010]</ref>. A large body of newer work uses language models to answer questions with the help of retrieved documents; these systems are more general and conceptually simpler than DeepQA. One approach is to use inner product search to retrieve relevant documents and then generate an answer given these documents:</p><p>p(passage|query) ∝ exp(embed(passage) ⋅ embed(query)).</p><p>Given a training dataset that specifies relevant passages for each question, dense passage retrieval (DPR) trains the retriever directly using a contrastive objective <ref type="bibr" target="#b17">[Karpukhin et al., 2020]</ref>. Retrieval Augmented Language Modeling (REALM) <ref type="bibr" target="#b13">[Guu et al., 2020]</ref> and Retrieval Augmented Generation (RAG) <ref type="bibr" target="#b20">[Lewis et al., 2020a]</ref> train the retriever and question-answering components end-to-end using a language modeling objective. Unlike DPR, RAG, and REALM, which focus on benchmarks with short answers, <ref type="bibr" target="#b18">Krishna et al. [2021]</ref> use a similar system to tackle long-form question-answering on the ELI5 dataset <ref type="bibr" target="#b9">[Fan et al., 2019]</ref>. They find that automated metrics like ROUGE-L are not meaningful, which motivates our choice to use human comparisons as the main metric. Note that the aforementioned family of methods, which rely on inner product search (Equation <ref type="formula" target="#formula_1">1</ref>), differ from WebGPT in that they formulate retrieval as a differentiable process. Fully differentiable retrieval has the advantage of fast optimization; two disadvantages are that it cannot deal with non-differential processes like using a search engine, and it is less interpretable.</p><p>Like WebGPT, some other recent work defines document retrieval or web browsing as a reinforcement learning (RL) problem. <ref type="bibr" target="#b30">Yuan et al. [2019]</ref> apply RL to reading comprehension benchmarks, where (as in WebGPT) the action space includes searching and scrolling through the provided source document.</p><p>They suggest web-level QA (like WebGPT) as a direction for future work. Adolphs et al. <ref type="bibr">[2021]</ref> set up an RL problem that involves performing a series of search queries for short-form question-answering. They train their system in two alternative ways: behavior cloning (BC) on synthetically-generated sequences and RL. Finally, there is another body of work that uses BC and RL to control web browsers, for automating other tasks besides question-answering <ref type="bibr" target="#b27">[Shi et al., 2017</ref><ref type="bibr" target="#b12">, Gur et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have demonstrated a novel approach to long-form question-answering, in which a language model is fine-tuned to use a text-based web-browsing environment. This allows us to directly optimize answer quality using general methods such as imitation learning and reinforcement learning. To make human evaluation easier, answers must be supported by references collected during browsing. Using this approach, our best model outperforms humans on ELI5, but still struggles with out-of-distribution questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Author contributions</head><p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji and John Schulman jointly led the project, developed the codebase, ran all data collection and experiments, and wrote the paper.</p><p>Jeff Wu, Long Ouyang, Xu Jiang and Karl Cobbe provided invaluable advice on a multitude of topics over the course of the project.</p><p>Jeff Wu, Vineet Kosaraju, William Saunders and Xu Jiang made key contributions to the project codebase.</p><p>Christina Kim, Christopher Hesse and Shantanu Jain built and supported infrastructure used for model training and inference.</p><p>Tyna Eloundou and Gretchen Krueger conducted the analysis of bias and contributed to the paper.</p><p>Kevin Button and Matthew Knight provided computer security support.</p><p>Benjamin Chess provided computer networking support.</p><p>thank Surge AI for helping us with data collection, in particular Edwin Chen, Andrew Mauboussin, Craig Pettit and Bradley Webb.</p><p>Finally, we would like to thank all of our contractors for providing demonstrations and comparisons, without which this project would not have been possible, including: Jamie Alexander, Andre Gooden, Jacquelyn Johns, Rebecca Kientz, Ashley Michalski, Amy Dieu-Am Ngo, Alex Santiago, Alice Sorel, Sam </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Question dataset details</head><p>For our demonstration and comparison datasets, the vast majority of questions were taken from ELI5 <ref type="bibr" target="#b9">[Fan et al., 2019]</ref>, to which we applied the follow post-processing:</p><p>The final step was performed because there is sometimes an implicit "Explain Like I'm Five" at the start of questions. We considered a question to be phrased as an actual question if it included either a question mark, or one of the following sequences of characters with a regex-word boundary at either end, case-insensitively: <ref type="bibr">explain, eli5, which, what, whats, whose, who, whos, whom, where, wheres, when, whens, how, hows, why, whys, am, is, isn, isnt, are, aren, arent, was, wasn, wasnt, were, weren, werent, do, don, dont, does, doesn, doesnt, did, didn, didnt, can, cant, could, couldn, couldnt, have, haven, havent, has, hasn, hasnt, may, might, must, mustn, mustnt, shall, shant, should, shouldn, shouldnt, will, wont, would, wouldn, wouldnt</ref> For diversity and experimentation, we also mixed in a small number of questions from the following datasets:</p><p>• TriviaQA. This is a dataset of short-form questions taken from trivia websites <ref type="bibr" target="#b16">[Joshi et al., 2017]</ref>. • AI2 Reasoning Challenge (ARC). This is a dataset of grade-school level, multiple-choice science questions <ref type="bibr" target="#b3">[Bhakthavatsalam et al., 2021]</ref>, which we converted to free-form questions using the format &lt;question&gt;&lt;new line&gt;A. &lt;option A&gt;&lt;new line&gt;.... This dataset is sub-divided into two difficulties, "Challenge" and "Easy". • Hand-written. We constructed this small dataset of miscellaneous questions written by people trying out the model. • ELI5 fact-check. We constructed this dataset using answers to questions from ELI5 given by an instruction-following model. <ref type="foot" target="#foot_1">4</ref> Each question has the following format: Fact-check each of the claims in the following answer. &lt;double new line&gt;Question: &lt;ELI5 question&gt;&lt;double new line&gt;Answer: &lt;model answer&gt;</p><p>The numbers of demonstrations and comparisons we collected for each of these datasets are given in Table <ref type="table" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data collection details</head><p>To collect demonstrations and comparisons, we began by hiring freelance contractors from Upwork (https://www.upwork.com), and then worked with Surge AI (https://www.surgehq.ai) to scale up our data collection. In total, around 25% of our data was provided by 10 contractors from Upwork, and around 75% by 46 contractors from Surge AI. The top 5 contractors provided around 50% of the data.</p><p>For both types of task, we provided contractors with a video and a detailed instruction document (linked below). Due to the challenging nature of the tasks, contractors were generally highly educated, usually with an undergraduate degree or higher. Contractors were compensated based on hours worked rather than number of tasks completed, and we conducted a survey to measure job satisfaction (see Appendix D).</p><p>For data quality, we put prospective contractors through a paid trial period lasting a few hours, and manually checked their work. For comparisons, we also completed around 100 tasks ourselves for all labelers to complete, and monitored both researcher-labeler agreement rates and labeler-labeler agreement rates. Treating the agreement rate between a neutral label and a non-neutral label as 50%, we measured a final researcher-labeler agreement rate of 74%, and a labeler-labeler agreement rate of 73%.</p><p>Demonstrations took an average of around 15 minutes each, and comparisons took an average of around 10 minutes each. Despite conventional wisdom that human labelling tasks should be quick and repeatable, we did not think it would be straightforward to decompose our tasks into significantly simpler ones, but we consider this to be a promising direction for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Demonstrations</head><p>We designed the demonstration interface in such a way that, as a rule, the user is given the same information as the model, and has the same actions available. There were a couple of exceptions to this:</p><p>1. Unlike humans, the model has no memory of previous steps. We therefore included a summary of past actions in the text given to the model. However, we felt that it was unnecessary to display this to humans. 2. The Scrolled &lt;up, down&gt; &lt;2, 3&gt; actions are useful for reducing the number of actions taken, but humans are used to scrolling one step at a time. We therefore made these actions unavailable to humans, and instead simply merged any repeated Scrolled &lt;up, down&gt; 1 actions that they made.</p><p>The full instruction document we provided to contractors for demonstrations can be viewed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Comparisons</head><p>To minimize label noise, it is important to make comparisons as unambiguous as possible. We therefore designed the following procedure for comparing two answers to a given question:</p><p>1. Read the question, and flag if it does not make sense or should not be answered (in which case the rest of the comparison is skipped). 2. Read the first answer and its references. 3. Rate the trustworthiness of any references relied upon by the answer. 4. Annotate each of the claims in the answer with the level of support it has and its relevance to the question. A screenshot of the annotation tool is shown in Figure <ref type="figure" target="#fig_8">9</ref>. 5. Repeat steps 2-4 for the second answer and its references. 6. Give comparison ratings for the amount of unsupported and irrelevant information, the usefulness of information with different levels of support, and coherence. 7. Weighing everything up, give a final comparison rating for overall usefulness. For each of the comparison ratings, we used a 5-point Likert scale with the options "A much better", "A better", "Equally good", "B better" and "B much better". Importantly, we did not require contractors to perform independent research to judge the factual accuracy of answers, since this would have been difficult and subjective. Instead, we asked contractors to judge whether claims in the answer are supported, i.e., either backed up by a reliable reference, or common knowledge.</p><p>For the final comparison rating, we encouraged contractors to use their best judgment, but to roughly consider the following criteria in descending order of priority:</p><p>• Whether or not the answer contains unsupported information. • Whether or not the core question has been answered. • Whether or not there is additional helpful information, which does not necessarily need to answer the question directly. • How coherent the answer is, and whether or not there are any citation errors.</p><p>• How much irrelevant information there is in the answer. (This can be higher priority in extreme cases.)</p><p>The full instruction document we provided to contractors for comparisons can be viewed here.</p><p>For most of the project, we made every part of this procedure required 10% of the time, and made every part except for the final comparison rating optional 90% of the time. Towards the end of the project, we removed the question flags from the first part since we felt that they were being overused, and made the comparison ratings for unsupported information and coherence required all of the time.</p><p>Despite the complexity of this procedure, we only used the final comparison rating in training, even collapsing together the "much better" and "better" ratings. We experimented with predicting some of the other information as an auxiliary loss, but we were not able to significantly improve the validation accuracy of the reward model. Nevertheless, we consider this to be another promising direction for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Contractor survey</head><p>It was valuable to gather feedback from our contractors, both to understand and improve their process, and to monitor job satisfaction. To this end, we sent them a questionnaire with the following questions:</p><p>• Please say how much you agree with each of the statements. (Required 5-point Likert rating and optional comments) 1. It was clear from the instructions what I was supposed to do. 2. I found the task enjoyable and engaging.</p><p>3. I found the task repetitive. 4. I was paid fairly for doing the task. 5. Overall, I am glad that I did this task. • What would you change about the task to make it more engaging or enjoyable? (Encouraged) • Are there any other tools you could be given that would make it easier to complete the task to a consistently high standard? (Encouraged) • Did you come up with any shortcuts that you used to do the task more quickly, and if so, what were they? (Encouraged) • Do you have any other comments? (Optional)</p><p>The "encouraged" questions were required questions but with instructions to put "N/A" if they really could not think of anything (this was rare).</p><p>We surveyed all contractors who completed 32 or more tasks (thus we excluded people who dropped out after the trial period or shortly thereafter). We did this 3 times over the course of the project: once for demonstrations and twice for comparisons. The quantitative results from these surveys are given in Figure <ref type="figure" target="#fig_3">10</ref>. The vast majority of respondents reported that they enjoyed the task, were paid fairly and were glad that they did the task overall. A significant minority of respondents also reported that they found the task repetitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"It was clear from the instructions</head><p>what I was supposed to do."</p><p>"I found the task enjoyable and engaging." "I found the task repetitive."</p><p>"I was paid fairly for doing the task."</p><p>"Overall, I am glad that I did this task."</p><p>Strongly disagree Disagree Neither agree nor disagree Agree Strongly agree</p><p>Figure 10: Likert ratings aggregated over all 3 of our contractor surveys. All ratings are weighted equally, even when the same contractor provided ratings in multiple surveys. In total, there are 41 ratings for each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyperparameters</head><p>Hyperparameters for all of our training methods are given in Table <ref type="table" target="#tab_5">6</ref>. We mostly used the same hyperparameters for the different model sizes, with the caveat that we expressed the Adam step sizes as multiples of the pre-training Adam step sizes, which are given in Table <ref type="table" target="#tab_4">5</ref>.</p><p>For each training method, we implemented some form of early stopping:</p><p>1. For BC, we stopped after a certain number of epochs based on reward model score (which usually improves past the point of minimum validation loss). 2. For RM, we stopped after a certain number of epochs based on validation accuracy. 3. For RL, we stopped after a certain number of PPO iterations based on the reward model score for some KL budget. The KL here is measured from the BC model, and summed over the episode. For the 175B model, we compared a couple of different KL budgets using human evaluations, and for the 760M and 13B models, we chose KL budgets informed by the 175B evaluations.</p><p>The points at which we early stopped are given in Table <ref type="table" target="#tab_6">7</ref>.</p><p>We tuned hyperparameters using similar criteria to early stopping. We used human evaluations sparingly, since they were noisy and expensive, and put less effort into tuning hyperparameters for the 760M and 13B model sizes. As a rule, we found the most important hyperparameter to tune to be the Adam step size multiplier.</p><p>For BC and RM, we used Polyak-Ruppert averaging <ref type="bibr" target="#b25">[Polyak and Juditsky, 1992]</ref>, taking an exponentially-weighted moving average (EMA) of the weights of the model as the final checkpoint. The "EMA decay" hyperparameter refers to the decay of this EMA per gradient step. For RL (but not rejection sampling), we did not use the EMA model for the 750M or 13B reward models, due to a bug.</p><p>For RL, most PPO hyperparameters did not require tuning, but a few points are worth noting:</p><p>• As discussed in Section 3, the reward is the sum of the reward model score at the end of each episode and a KL penalty from the BC model at each token. Even though the reward is part of the environment, we treat the coefficient of this KL penalty as a hyperparameter, called the "KL reward coefficient". • We used separate policy and value function networks for simplicity, although we think that using shared networks is a promising direction for future research. • Since some actions (such as quotes and answers) require many more tokens than others, we modified the environment to "chunk" long completions into multiple actions, to improve rollout parallelizability. Note that this has a minor effect on GAE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Minimal comparison instructions</head><p>As discussed in Section 4, for comparing WebGPT's answers to the reference answers from the ELI5 dataset, we used a much more minimal set of instructions, for ecological validity. The full instructions consisted of the following text:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing answers (minimal version)</head><p>In this task, you'll be provided with a question and a set of two answers. We'd like you to provide ratings comparing the two answers for the following categories:</p><p>• Accuracy -which answer is more factually accurate?</p><p>• Please use a search engine to fact-check claims in an answer that aren't obvious to you. Answers may have subtly incorrect or fabricated information, so be careful! • Coherence -which answer is easier to follow? • Usefulness overall -all things considered, which answer would be more helpful to the person who asked this question?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FAQ</head><p>• What should I do if there's a URL in the question or one of the answers?</p><p>• Please don't click any URLs and interpret the questions and answers based on their remaining textual content. • What should I do if the question doesn't make any sense, or isn't a question?</p><p>• Sometimes you'll see a statement instead of a question, which you should interpret as "Explain: . . . ".</p><p>-E.g. a question titled "Magnets" should be interpreted as "Explain: magnets" or "How do magnets work?" • If the question is ambiguous but has a few reasonable interpretations, stick with the interpretation that you think is most likely. -This should be rare, so use this sparingly. • What should I do if the answer to the question depends on when it was asked?</p><p>• In this case, please be charitable when judging answers with respect to when the question was asked -an answer is considered accurate if it was accurate at any point within the last 10 years.</p><p>-E.g. valid answers to the question "Who is the current U.S. president" are Barack Obama, Donald Trump, and Joe Biden. • What should I do if I only see one answer?</p><p>• If you only see one answer, you'll be asked to provide absolute ratings for that answer (very bad, bad, neutral, good, or very good) instead of comparison ratings.</p><p>-For the "usefulness overall" category, please calibrate your ratings such that "very bad" indicates an answer that is worse than not having an answer at all (e.g. due to being very misleading), "bad" indicates an answer that's about as helpful as not having an answer, and higher ratings indicate useful answers with varying degrees of quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G TriviaQA evaluation</head><p>Although WebGPT was trained primarily to perform long-form question-answering, we were interested to see how well it would perform short-form question-answering. To this end, we evaluated WebGPT on TriviaQA <ref type="bibr" target="#b16">[Joshi et al., 2017]</ref>, a dataset of short-form questions from trivia websites. For this evaluation, we used the WebGPT 175B BC model with a sampling temperature of 0.8 and no rejection sampling.</p><p>To address the mismatch between WebGPT's long-form answers and the short-form answers expected by TriviaQA, we fine-tuned GPT-3 175B to answer TriviaQA questions conditioned on the output of WebGPT. Since this is a simple extraction task, and out of concern for test-train overlap <ref type="bibr" target="#b21">[Lewis et al., 2020b]</ref>, we used only 256 questions for this fine-tuning (with a batch size of 32 and a learning rate of 1.5 × 10 −6 ). This was in addition to the 143 TriviaQA demonstrations on which the WebGPT model was trained. As an ablation, we also fine-tuned GPT-3 175B in the same way, but without the WebGPT output.</p><p>Our results are shown in Table <ref type="table" target="#tab_8">8</ref>, along with those of the best existing model, UnitedQA <ref type="bibr" target="#b5">[Cheng et al., 2021]</ref>. We report results on the TriviaQA development set splits defined in <ref type="bibr" target="#b21">Lewis et al. [2020b]</ref>. We perform slightly better than UnitedQA-E on questions with no test-train overlap, and slightly worse on questions with test-train overlap. We hypothesize that this difference is the result of WebGPT being trained on far fewer TriviaQA questions. Note that we use far more compute than UnitedQA, and also use live access to the web rather than only the corpus provided by <ref type="bibr" target="#b16">Joshi et al. [2017]</ref> (although we still censor trivia websites in the same way for this evaluation). On the other hand, WebGPT was trained primarily to perform long-form question-answering, and so the transfer to the short-form setting is notable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Analysis of effect of question stance and reference point bias</head><p>In this section we investigate the impact of question "stance" (whether the question implicitly supports or refutes some relevant belief) on the model's accuracy, and on its tendency to support or refute that belief in its answer. We also probe the model's bias towards "assuming" a certain cultural reference point with an example culturally dependent question ("What does a wedding look like?").</p><p>H.1 Effect of question stance on factual accuracy and answer stance</p><p>We ran a small experiment to investigate the impact of question stance on the model's answers. Inspired by TruthfulQA <ref type="bibr" target="#b22">[Lin et al., 2021]</ref>, we chose 10 well-known conspiracy theories and 10 common misconceptions for this experiment. For each conspiracy theory or misconception, we wrote three questions, each taking one of three stances: one expressing skepticism around the implicit belief, one neutral about the implicit belief, and one affirming the implicit belief. This resulted in the 60 questions given in Table <ref type="table">9</ref>. We collected answers to these questions for the three compute-efficient WebGPT models (see Section 5.2), and used these answers to look for a couple of different effects.</p><p>• Factual accuracy. First, we examined whether the stance of the question impacts the model's factual accuracy. To do this, we labelled each answer as accurate or inaccurate, by fact-checking any central or specific claims in the answer, and labeling the answer as inaccurate if a significant number (more than around 25%) of those claims could not be easily verified. Our results are given in Figure <ref type="figure" target="#fig_3">11</ref>. We found suggestive evidence that, across model sizes, questions that affirm an implicit belief in a conspiracy or misconception tend to elicit inaccurate answers from the model more often than questions that are framed in a neutral or skeptical way. While our experiment had too small of a sample size for us to draw definitive conclusions, it demonstrates the model's potential to misinform users who have erroneous beliefs in ways that reinforce those beliefs. • Answer stance. Second, we studied whether the model mirrors the stance of the question in the content of its response. To do this, we labelled each answer on whether it explicitly refutes the implicit belief or explicitly affirms the implicit belief. Note that in some cases it is possible for an answer to affirm the belief in the conspiracy theory or misconception while remaining factually accurate, by including appropriate caveats. If an answer initially affirms the belief but then reverses its stance, saying for example "but this is a myth", then we consider it to have refuted the belief. Our results are given in Figure <ref type="figure" target="#fig_3">12</ref>. We found that all the models tended to refute the implicit beliefs more often than they affirmed them, and that this effect increased with model size. However, we did not find any clear evidence that the stance of the question has any effect on this behavior.</p><p>Given the small scale of this experiment, it would be informative to see further research on the effect of question stance on model answers. We remark that humans exhibit sensitivity to the framing of questions <ref type="bibr" target="#b6">[Chong and Druckman, 2007]</ref>. In addition to this, it would be useful to study the effects of various other factors, such as the training data collection methodology, the relative degree of skepticism, neutrality or affirmation in the questions, the relative volumes of skeptical or affirming sources on the web, and whether the questions themselves appear in the training data or on the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Reference point bias</head><p>Rather than having a strong stance, some questions may reveal very little information about the user, but the model may nevertheless assume a certain cultural reference point. We refer to this as reference point bias. To probe this phenomenon, we conducted a simple case study, in which we analyzed 64 answers from the WebGPT 175B BC model to the following question: "What does a wedding look like?".</p><p>In response to this question, the model tended to assume a Western, and often specifically an American, point-of-view. Out of the 64 answers, 20 included the word "America" or "American", and only 4 focused on a specific, named culture other than American: Vietnamese (1); Indian (1); and Croatian <ref type="bibr">(2)</ref>. While 8 of 64 responses noted that there is no standard wedding, all but one of these still also included at least one detail typical of a Western and often American wedding. And 2 of the these 8including the answer with the highest reward model score -noted that there is no standard or typical American wedding. The assumption of a Western and often American reference point in this case may be influenced by the data the model has seen during pre-training, by Internet search data, by the viewpoints represented by the contractors we worked with, and by our research team. When asked specifically "What does a Vietnamese wedding look like?", the model usually generates responses pertaining to Vietnamese weddings, but these come up rarely in response to a generic question. When in doubt, the model defaults to assuming a Western or American viewpoint in this case.</p><p>Furthermore, we also noticed that the model often makes other assumptions that exclude or erase some identities in response to this question, for example by assuming that a couple consists of a male groom and female bride and assuming that a bride's father walks her down an aisle. While our experiment was focused on probing its bias towards a Western or American point of view, we encourage further interdisciplinary research in these and other areas of bias.</p><p>Table <ref type="table">9</ref>: Questions used to study the effect of question stance on the model's answers. Each of the top 10 topics refers a well-known conspiracy theory, and each of the bottom 10 topics refers to a common misconception. For each topic, we wrote a question with a skeptical stance, a question with neutral stance, and a question with an affirming stance.</p><p>Table <ref type="table" target="#tab_0">11</ref>: An answer and references produced by one of our human demonstrators to the same question as the one in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>Swear words, obscenities, and derogatory terms are some categories of words that society considers 'bad' and exempt from usage in social settings. In a more definitive approach, courts presume harm from speech in cases involving discrimination or sexual harassment <ref type="bibr" target="#b31">[1]</ref>. While the original justification for these obscenity laws was predicated on an unfounded assumption that speech can deprave or corrupt children, and it is important to note that there is little (if any) social science data demonstrating that a word is by itself harmful <ref type="bibr" target="#b31">[1]</ref>, it is also pertinent to consider the origin of some derogatory words and the context of their usage to understand why they should be unacceptable in social settings. Examples include:</p><p>-Ape/Monkey: These are considered offensive terms when used to describe a person of color because offensive comparisons of black people to apes date back hundreds of centuries, such as when scientists Josiah C. Nott and George R. Gliddon created the 1854 Types of Mankind (a leading scientific text on race in its time), and they compared black people not to other (white) people but to chimpanzees, gorillas, and orangutans. They are now, justifiably, considered extremely derogatory slang. <ref type="bibr">[2]</ref> -Savage: When used as an adjective to describe acts of violence, this word is not considered offensive, but the use of 'savage' as a noun to describe people dates back to a time when Christopher Columbus was enslaving Native Americans and claiming their lands and work for his own. The word became a slur used by white Europeans to describe Native American people. The usage stuck around for hundreds of years, and it's still a word that many indigenous peoples around the world find offensive. <ref type="bibr">[3]</ref> Table <ref type="table" target="#tab_0">12</ref>: The reference answer from the ELI5 dataset for the question given in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>There is an excellent podcast by Stuff You Should Know that is entirely about swear/curse words. They explain the history, psychology and science of curse and swear words, which are actually two different things.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Screenshot from the demonstration interface. ♦Question How can I train the crows in my neighborhood to bring me gifts? ♦Quotes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>♦</head><label></label><figDesc>Past actions Search how to train crows to bring you gifts Click Gifts From Crows | Outside My Window www.birdsoutsidemywindow.org Quote Back ♦Title</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>♦</head><label></label><figDesc>Actions left: 96 ♦Next action (b) Corresponding text given to the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An observation from our text-based web-browsing environment, as shown to human demonstrators (left) and models (right). The web page text has been abridged for illustrative purposes.</figDesc><graphic url="image-1.png" coords="2,93.60,73.99,220.89,152.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>WebGPT vs. ELI5 reference answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Preference of RL models over BC models, with (right) and without (left) using rejection sampling. RL slightly improves preference, but only when not using rejection sampling. Error bars represent ±1 standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Preference of the 175B best-of-n BC model over the BC model. The validation RM prediction is obtained using the estimator described in Appendix I, and predicts human preference well in this setting. The shaded region represents ±1 standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Screenshot from the comparison interface, showing the annotation tool.</figDesc><graphic url="image-2.png" coords="18,108.00,72.00,396.00,273.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Results of experiment on effect of question stance on factual accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Actions the model can take. If a model generates any other text, it is considered to be an invalid action. Invalid actions still count towards the maximum, but are otherwise ignored.</figDesc><table><row><cell>Command</cell><cell>Effect</cell></row><row><cell>Search &lt;query&gt;</cell><cell>Send &lt;query&gt; to the Bing API and display a search results page</cell></row><row><cell>Clicked on link &lt;link ID&gt;</cell><cell>Follow the link with the given ID to a new page</cell></row><row><cell>Find in page: &lt;text&gt;</cell><cell>Find the next occurrence of &lt;text&gt; and scroll to it</cell></row><row><cell>Quote: &lt;text&gt;</cell><cell>If &lt;text&gt; is found in the current page, add it as a reference</cell></row><row><cell>Scrolled down &lt;1, 2, 3&gt;</cell><cell>Scroll down a number of times</cell></row><row><cell>Scrolled up &lt;1, 2, 3&gt;</cell><cell>Scroll up a number of times</cell></row><row><cell>Top</cell><cell>Scroll to the top of the page</cell></row><row><cell>Back</cell><cell>Go to the previous page</cell></row><row><cell>End: Answer</cell><cell>End browsing and move to answering phase</cell></row><row><cell cols="2">End: &lt;Nonsense, Controversial&gt; End browsing and skip answering phase</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Thornton and Kelli W. from Upwork; and Elena Amaya, Michael Baggiano, Carlo Basile, Katherine Beyer, Erica Dachinger, Joshua Drozd, Samuel Ernst, Rodney Khumalo, Andrew Kubai, Carissa Lewis, Harry Mubvuma, William Osborne, Brandon P., Kimberly Quinn, Jonathan Roque, Jensen Michael Ruud, Judie Anne Sigdel, Bora Son, JoAnn Stone, Rachel Tanks, Windy Thomas, Laura Trivett, Katherine Vazquez, Brandy and Shannon from Surge AI.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Breakdown of our demonstrations and comparisons by question dataset.</figDesc><table><row><cell cols="3">Question dataset Demonstrations Comparisons</cell></row><row><cell>ELI5</cell><cell>5,711</cell><cell>21,068</cell></row><row><cell>ELI5 fact-check</cell><cell>67</cell><cell>185</cell></row><row><cell>TriviaQA</cell><cell>143</cell><cell>134</cell></row><row><cell>ARC: Challenge</cell><cell>43</cell><cell>84</cell></row><row><cell>ARC: Easy</cell><cell>83</cell><cell>77</cell></row><row><cell>Hand-written</cell><cell>162</cell><cell>0</cell></row><row><cell>Total</cell><cell>6,209</cell><cell>21,548</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Pre-training Adam step sizes, to which we apply multipliers. These are the same as those given in<ref type="bibr" target="#b4">Brown et al. [2020]</ref>.</figDesc><table><row><cell cols="3">Model size Base Adam step size</cell></row><row><cell>760M</cell><cell>2.5 × 10</cell><cell>−4</cell></row><row><cell>13B</cell><cell>1.0 × 10</cell><cell>−4</cell></row><row><cell>175B</cell><cell>0.6 × 10</cell><cell>−4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for all of our training methods.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="4">Value for BC Value for RM Value for RL</cell></row><row><cell>Minibatch size</cell><cell>512  *</cell><cell>64</cell><cell>*  *</cell><cell>512</cell></row><row><cell>Adam step size multiplier</cell><cell>0.1</cell><cell cols="2">0.05  *  *  *</cell><cell>0.004</cell></row><row><cell>Epoch count upper bound</cell><cell>12</cell><cell></cell><cell>6</cell><cell>N/A</cell></row><row><cell>EMA decay</cell><cell>0.99</cell><cell cols="2">0.99</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Early stopping points.</figDesc><table><row><cell cols="5">Model size BC epochs RM epochs RL stopping point RL stopping point</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(PPO iterations)</cell><cell>(KL per episode)</cell></row><row><cell>760M</cell><cell>2</cell><cell>1</cell><cell>19</cell><cell>10.5 nats</cell></row><row><cell>13B</cell><cell>5</cell><cell>1</cell><cell>30</cell><cell>6.8 nats</cell></row><row><cell>175B</cell><cell>3</cell><cell>1</cell><cell>18</cell><cell>∼12 nats</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>•</head><label></label><figDesc>If the question still doesn't make sense (e.g. if you'd need to click on a URL to understand it, or if it's entirely unclear what the question means), then click the "This question does not make sense" checkbox at the top and submit the task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>TriviaQA development set accuracy (exact match scores).</figDesc><table><row><cell></cell><cell>Total</cell><cell>Question</cell><cell>No ques-</cell><cell>Answer</cell><cell>Answer</cell><cell>No</cell></row><row><cell></cell><cell></cell><cell>overlap</cell><cell>tion</cell><cell>overlap</cell><cell>overlap</cell><cell>over-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>overlap</cell><cell></cell><cell>only</cell><cell>lap</cell></row><row><cell>GPT-3 175B</cell><cell cols="2">58.7% 75.9%</cell><cell>52.9%</cell><cell>67.3%</cell><cell>61.6%</cell><cell>39.0%</cell></row><row><cell cols="3">GPT-3 175B + WebGPT 175B BC 69.5% 86.3%</cell><cell>65.3%</cell><cell>78.4%</cell><cell>73.2%</cell><cell>52.4%</cell></row><row><cell>UnitedQA-E</cell><cell cols="2">68.9% 89.3%</cell><cell>62.7%</cell><cell>78.6%</cell><cell>70.6%</cell><cell>44.3%</cell></row><row><cell>UnitedQA (hybrid model)</cell><cell>70.5%</cell><cell></cell><cell cols="2">Not reported</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">   3  This inadvertently resulted in a small number of empty answers, which were considered truthful but not informative. This affected 74 answers in total, around 3% of answers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://beta.openai.com/docs/engines/instruct-series-beta</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgments</head><p>We would like to thank Leo Gao, Hyeonwoo Noh and Chelsea Voss for working on future directions; Steve Dowling, Christian Gibson, Peter Hoeschele, Fraser Kelton, Bianca Martin, Bob McGrew, Felipe Such and Hannah Wong for technical, logistical and communications support; Steven Adler, Miles Brundage, David Farhi, William Guss, Oleg Klimov, Jan Leike, Ryan Lowe, Diogo Moitinho de Almeida, Arvind Neelakantan, Alex Ray, Nick Ryder and Andreas Stuhlmüller for helpful discussions; Owen Cotton-Barratt, Owain Evans, Jared Kaplan, Girish Sastry, Carl Shulman, Denis Yarats and Daniel Ziegler for helpful discussions and feedback on drafts; Beth Barnes and Paul Christiano for helpful discussions and feedback on drafts, and in particular for suggesting the project; and Dario Amodei for suggesting to work on factual inaccuracy in language models. We would also like to</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Environment design details</head><p>Our text-based web-browsing environment is written mostly in Python with some JavaScript. For a high-level overview, see Section 2. Further details are as follows:</p><p>• When a search is performed, we send the query to the Microsoft Bing Web Search API, and convert this to a simplified web page of results. • When a link to a new page is clicked, we call a Node.js script that fetches the HTML of the web page and simplifies it using Mozilla's Readability.js. • We remove any search results or links to reddit.com or quora.com, to prevent the model copying answers from those sites. • We take the simplified HTML and convert links to the special format 【&lt;link ID&gt; †&lt;link text&gt; †&lt;destination domain&gt;】, or 【&lt;link ID&gt; †&lt;link text&gt;】 if the destination and source domains are the same. Here, the link ID is the index of the link on the page, which is also used for the link-clicking command. We use special characters such as 【 and 】 because they are rare and encoded in the same few ways by the tokenizer, and if they appear in the page text then we replace them by similar alternatives. • We convert superscripts and subscripts to text using ^and _, and convert images to the special format [Image: &lt;alt text&gt;], or [Image] if there is no alt text. • We convert the remaining HTML to text using html2text.</p><p>• For text-based content types other than HTML, we use the raw text. For PDFs, we convert them to text using pdfminer.six. For all other content types, and for errors and timeouts, we use an error message. • We censor any pages that contain a 10-gram overlap with the question (or reference answer, if provided) to prevent the model from cheating, and use an error message instead. • We convert the title of the page to text using the format &lt;page title&gt; (&lt;page domain&gt;).</p><p>For search results pages, we use Search results for: &lt;query&gt;. • When a find in page or quote action is performed, we compare the text from the command against the page text with any links stripped (i.e., including only the text from each link). We also ignore case. For quoting, we also ignore whitespace, and allow the abbreviated format &lt;start text&gt;━&lt;end text&gt; to save tokens. • During browsing, the state of the browser is converted to text as shown in Figure <ref type="figure">1(b)</ref>.</p><p>For the answering phase (the last step of the episode), we convert the question to text using the format &lt;question&gt;■, and follow this by each of the collected quotes in the format [&lt;quote number&gt;] &lt;quote page title&gt; (&lt;quote page domain&gt;) &lt;double new line&gt;&lt;quote extract&gt;■. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Predicting rejection sampling performance</head><p>It is helpful to be able to predict human preference of answers produced using rejection sampling (best-of-n). To do this, we evaluate answers using a validation reward model (trained on a separate dataset split), to try to account for the original reward model being overoptimized. For large n, the naive Monte Carlo estimator of the expected validation reward model score requires many model samples to produce accurate estimates. Here we describe an alternative estimator, which produces accurate estimates more efficiently.</p><p>Let Q be the distribution of questions, and given a question q, let A (q) be the distribution of answers produced by the model. Given a question q and an answer a (with references), let R train (a | q) be the original reward model score, and let R val (a | q) be the validation reward model score. Let n be the number of answers sampled when rejection sampling (i.e., the n in best-of-n).</p><p>To predict the Elo score corresponding to human preference for a given question q, we estimate</p><p>To predict the overall Elo score corresponding to human preference, we estimate</p><p>As shown in Figure <ref type="figure">5</ref>, this predicts human preference well for n ≤ 64, although we expect it to overestimate human preference for sufficiently large n, as the validation reward model will eventually become overoptimized.</p><p>The simplest way to estimate R pred n</p><p>(q) for a given question q is with a Monte Carlo estimator, by repeatedly sampling A 1 , A 2 , . . . , A n ∼ A (q). However, this is very wasteful, since it takes n answers to produce each estimate, and moreover, answers are not re-used for different values of n. Instead, we sample A 1 , A 2 , . . . , A N ∼ A (q) for some N ≥ n, and compute 1</p><p>which is an unbiased estimator of R pred n</p><p>(q) by linearity of expectation. This can be computed efficiently by sorting A 1 , A 2 , . . . , A N by original reward model score to obtain S 1 , S 2 , . . . S N with R train (S 1 | q) ≤ ⋅ ⋅ ⋅ ≤ R train (S N | q), and then computing 1</p><p>To estimate</p><p>for some N ≥ n max for each question Q, and for each n take the empirical average of the above estimator over the different questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J References for example answer and alternative answers</head><p>In Table <ref type="table">2</ref>, we gave an answer produced by our 175B best-of-64 model to a randomly-chosen question from the ELI5 test set: "Why did we decide that certain words were "bad" and shouldn't be used in social settings?". Here we give the full text of the references for that answer, along with an answer produced by one of our human demonstrators, and the reference answer from the ELI5 dataset. Further samples are available at https://openaipublic.blob.core.windows.net/ webgpt-answer-viewer/index.html.</p><p>Table <ref type="table">10</ref>: Full text of the references found by our model for the answer given in Table <ref type="table">2</ref>.</p><p>References <ref type="bibr" target="#b31">[1]</ref> Why Are Some Words <ref type="bibr">'Bad'? | Vermont Public Radio (www.vpr.org)</ref> But we will explore the psychology and brain science behind bad words with Benjamin Bergen, professor of cognitive science at University of California, San Diego. He's the author of What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves.</p><p>[2] Why Are Some Words <ref type="bibr">'Bad'? | Vermont Public Radio (www.vpr.org)</ref> It's hard to give a single answer to the question of why some words are bad, while others aren't, because each word has a different history. Words and their meanings change over time. So one word might be considered "bad" to one generation, and not bad 100 years later. In addition, words carry different meanings in different families, communities, and cultures. So what is bad to one culture might not be considered bad to another.</p><p>[3] Why Are Some Words <ref type="bibr">'Bad'? | Vermont Public Radio (www.vpr.org)</ref> But there are some general categories that "bad" words fall into: "Words in general that are considered bad tend to relate to parts of our lives that we don't like talking about in public, like bathroom functions," Benjamin Bergen says. Other words that are often considered bad relate to negative ways of talking about people and their religion, skin color, ability, gender etc. Those words are called slurs. And many people consider those the worst kind of bad words.</p><p>Often people say bad words when they're experiencing strong emotions, and those words are link to our emotions. Kids are no exception. "We've learned since we were very young that when people are excited, frustrated, really happy, angry or sad these are the words that they use. So when we have these strong emotions, we use those words as well," Bergen says.</p><p>[4] On Words: 'Bad' Words and Why We Should Study Them | UVA Today (news.virginia.edu)</p><p>We also use the term "bad word" to pick out terms that are sanctioned simply because of what they refer to: taboo human acts, impolite biological processes and items that people find disgusting.</p><p>[5] The Science of Curse Words: Why The &amp;amp;@$! Do We Swear? (www.babbel.com)</p><p>For a word to qualify as a swear word it must have the potential to offend -crossing a cultural line into taboo territory. As a general rule, swear words originate from taboo subjects. This is pretty logical. The topic is off-limits, so the related words aren't meant to be spoken either. Certain topics are almost universally taboo -death, disease, excrement -you know, icky stuff. Sex is another classic taboo, as the English "fuck," Italian fanculo and Russian блядь illustrate.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="www.vpr.org" />
		<title level="m">Why Are Some Words &apos;Bad&apos;? | Vermont Public Radio</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">On Words: &apos;Bad&apos; Words and Why We Should Study Them | UVA Today</title>
				<imprint/>
	</monogr>
	<note>The Science of Curse Words: Why The &amp;@$!. Do We Swear? (www.babbel.com</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boerschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huebscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><surname>Kilcher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00527</idno>
		<title level="m">Boosting search engines with interactive agents</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bhakthavatsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03315</idno>
		<title level="m">Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
	</analytic>
	<monogr>
		<title level="m">Superintelligence: Paths, Dangers, Strategies</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2014">2014. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UnitedQA: A hybrid approach for open domain question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00178</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framing theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Druckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Polit. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Supervising strong learners by amplifying weak experts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shlegeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08575</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cotton-Barratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Finnveden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balwit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Righetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06674</idno>
		<title level="m">Truthful AI: Developing and governing AI that does not lie</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09190</idno>
		<title level="m">ELI5: Long form question answering</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automation bias: a systematic review of frequency, effect mediators, and mitigators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goddard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roudsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wyatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="127" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09195</idno>
		<title level="m">Learning to navigate the web</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">REALM: Retrieval-augmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Harms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Crystal Society. Crystal Trilogy</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CreateSpace Independent Publishing Platform</publisher>
		</imprint>
	</monogr>
	<note>ISBN 9781530773718</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00899</idno>
		<title level="m">AI safety via debate</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hurdles to progress in long-form question answering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06332</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07871</idno>
		<title level="m">Scalable agent alignment via reward modeling: a research direction</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Question and answer test-train overlap in open-domain question answering datasets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02637</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07958</idno>
		<title level="m">Measuring how models mimic human falsehoods</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00661</idno>
		<title level="m">On faithfulness and factuality in abstractive summarization</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02274</idno>
		<title level="m">Rethinking search: Making experts out of dilettantes</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">World of bits: An open-domain platform for web-based agents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3135" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07567</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01325</idno>
		<title level="m">Learning to summarize from human feedback</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interactive machine comprehension with information seeking agents</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10449</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">We included URLs in full, rather than using special _URL_ tokens</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">We filtered out questions with the title</title>
		<imprint/>
	</monogr>
	<note>deleted by user. and ignored the selftext &quot;[deleted]&quot; and &quot;[removed]&quot;. (The &quot;selftext&quot; is the body of the post.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">We concatenated the title and any non-empty selftext</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explain</title>
	</analytic>
	<monogr>
		<title level="m">to questions that were not phrased as actual questions (e.g., we used &quot;Explain: gravity&quot; rather than simply &quot;gravity</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Courts presume harm from speech in cases involving discrimination or sexual harassment. The original justification for our obscenity laws was predicated on an unfounded assumption that speech can deprave or corrupt children, but there is little (if any) social-science data demonstrating that a word in and of itself causes harm. A closely related problem is the manner in which harm has been defined -harm is most commonly framed in terms of standards and sensibilities such as religious values or sexual mores</title>
	</analytic>
	<monogr>
		<title level="s">The Science of Swearing (www.psychologicalscience.org</title>
		<imprint/>
	</monogr>
	<note>Rarely are there attempts to quantify harm in terms of objectively measurable symptoms (e.g., sleep disorder, anxiety</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">All Of These Words Are Offensive</title>
		<ptr target="www.dictionary.com" />
		<imprint/>
	</monogr>
	<note>But Only Sometimes</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exactly when the words became slurs is unknown, but offensive comparisons of black people to apes date back hundreds of centuries. As recently as the late 1800s, for example, when scientists Josiah C. Nott and George R. Gliddon created the 1854 Types of Mankind (then the leading scientific text on race), they compared black people not to other (white) people but to chimpanzees, gorillas, and orangutans</title>
		<author>
			<persName><surname>So</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>where&apos;s the problem?Ape and monkey are considered offensive terms when they&apos;re used to describe a person of color. It&apos;s what is known as dehumanizing language. When used in these contexts, monkey and ape are considered extremely derogatory slang</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">At a time when Christopher Columbus was enslaving Native Americans and claiming their lands and work for his own, the word became a slur used by white Europeans to describe Native American people. The usage stuck around for hundreds of years</title>
		<ptr target="www.dictionary.com" />
		<imprint/>
		<respStmt>
			<orgName>All Of These Words Are Offensive (But Only Sometimes</orgName>
		</respStmt>
	</monogr>
	<note>The word savage has taken a circuitous path through the lexicon over the years, first showing up in English in the 1200s from Middle English. As an adjective, it&apos;s typically meant &quot;fierce, ferocious, or cruel; uncivilized; barbarous. and it&apos;s still a word that many indigenous peoples around the world find offensive</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
