<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BTB-X: A Storage-Effective BTB Organization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Truls</forename><surname>Asheim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
							<idno type="ORCID">0000-0001-6525-0762</idno>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
							<idno type="ORCID">0000-0001-6306-304X</idno>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BTB-X: A Storage-Effective BTB Organization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LCA.2021.3109945</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Server</term>
					<term>microarchitecture</term>
					<term>branch target buffer (BTB)</term>
					<term>instruction cache</term>
					<term>prefeteching Ã‡</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many contemporary applications feature multi-megabyte instruction footprints that overwhelm the capacity of branch target buffers (BTB) and instruction caches (L1-I), causing frequent front-end stalls that inevitably hurt performance. BTB is crucial for performance as it enables the front-end to accurately resolve the upcoming execution path and steer instruction fetch appropriately. Moreover, it also enables highly effective fetch-directed instruction prefetching that can eliminate many L1-I misses. For these reasons, commercial processors allocate vast amounts of storage capacity to BTBs. This letter aims to reduce BTB storage requirements by optimizing the organization of BTB entries.</p><p>Our key insight is that today's BTBs store the full target address for each branch, yet the vast majority of dynamic branches have short offsets requiring just a handful of bits to encode. Based on this insight, we organize the BTB as an ensemble of smaller BTBs, each storing offsets within a particular range. Doing so enables a dramatic reduction in storage for target addresses. We also compress tags to reduce the tag storage cost. Our final design, called BTB-X, uses an ensemble of five BTBs with compressed tags that enables it to track 2.8x more branches than a conventional BTB with the same storage budget.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CONTEMPORARY server applications feature massive instruction footprints stemming from deeply layered software stacks. These footprints may far exceed the capacity of the branch target buffer (BTB) and instruction cache (L1-I), resulting in the so-called frontend bottleneck. BTB misses may lead to wrong-path execution, triggering a pipeline flush when misspeculation is detected. Such pipeline flushes not only throw away tens of cycles of work but also expose the fill latency of the pipeline. Similarly, L1-I misses cause the core front-end to stall for tens of cycles while the miss is being served from lower-level caches.</p><p>BTB stands at the center of a high-performance core front end for three key reasons: it determines the instruction stream being fetched, it identifies branchs for the branch predictor, and it affects the L1-I hit rate. Specifically, by identifying control flow divergences, the BTB ensures that the branch predictor can make predictions for upcoming conditional branches. For predicted-taken and unconditional branches, the BTB supplies targets to which instruction fetch should be redirected. Finally, the BTB together with the direction predictor enables an important class of instruction prefetchers called fetch-directed instruction prefetchers (FDIP) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, which rely on the BTB to discover L1-I prefetch candidates.</p><p>Considering the criticality of capturing the large branch working sets of modern workloads, commercial CPUs feature BTBs with colossal capacities, a trend also observed by <ref type="bibr" target="#b4">[5]</ref>. Thus, IBM z-series processors <ref type="bibr" target="#b2">[3]</ref>, AMD Zen-2 <ref type="bibr" target="#b10">[11]</ref>, and ARM Neoverse N1 <ref type="bibr" target="#b7">[8]</ref> feature 24K-entry, 8.5K-entry, and 6K-entry BTBs. With each BTB entry requiring 10 bytes or more (Section 2), BTB storage costs can easily reach into tens and even hundreds of KBs. Indeed, the Samsung Exynos M6 mobile processor allocates a staggering 529KB of onchip storage to BTBs <ref type="bibr" target="#b3">[4]</ref>. While such massive BTBs are effective at capturing branch working sets, they do so at staggering area costs.</p><p>This work seeks to reduce BTB storage requirements by increasing its branch density, defined as branches per KB of storage. To that end, we aim to reorganize individual BTB entries to minimize their storage cost. Our key insight is that branch offsets, defined as delta between the address of the branch instruction and that of its target, are unequally distributed but tend to require significantly fewer bits to represent than full target addresses. Our analysis reveals that 37% of dynamic branches require only 7 bits or fewer for offset encoding, while a meager 1% of branches need 25 bits or more to store their offsets.</p><p>Based on this insight, we propose to store offsets in the BTB rather than full target addresses, which can be up to 64 bits long depending on the size of virtual address space. To accommodate the varied distribution of branch offsets, we partition the BTB into several smaller BTBs, each storing only those branches whose target offsets can be encoded with a certain number of bits. Because the target field accounts for over half of each entry's storage budget in a conventional BTB (Fig. <ref type="figure">1</ref>), this optimization brings significant storage savings. We further observe that the tag field is the second-largest contributor to each BTB entry's storage requirement. To reduce this cost, we propose compressing the tags through the use of hashing.</p><p>Our final design, called BTB-X, uses an ensemble of five BTBs, each with 16-bit tags. The BTBs differ only in the number of bits they allocate for branch target offsets. Our evaluation shows that BTB-X can track over 2.8x more branches than a conventional BTB with the same storage budget. Conversely, BTB-X can accommodate the same number of branches as existing BTBs while requiring 2.8x less storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Branch Target Buffer (BTB)</head><p>BTB is used in the core front-end to identify whether a program counter (PC) corresponds to a branch instruction before the instruction itself is even fetched. As depicted in Fig. <ref type="figure">1</ref>, each BTB entry is composed of tag, type, and target fields. BTB is indexed with the lower order PC bits and tag field of the indexed entry is compared with the higher order PC bits. A match indicates that the PC belongs to a branch instruction. The type field of the indexed BTB entry determines whether the branch is a call, return, conditional, or unconditional branch. The branch type determines whether the branch direction (taken/not taken) needs to be predicted and where its target address is found. Call, return, and unconditional branches are always taken, whereas for conditional branches, a direction predictor is used to predict their direction. If the branch is predicted to be taken, target field in the BTB entry provides the address for the next instruction, except for returns. This is because a given function can be called from different call sites; as such, the return address is call-site dependent. Therefore, a return address stack (RAS) is typically employed to record return addresses at call-sites. On a function call, the call instruction pushes the return address to RAS, which is later popped by the corresponding return instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Cost of a BTB Miss</head><p>A BTB miss for a branch instruction means that the branch is undetected and the front-end continues to fetch instructions sequentially. Whether or not the sequential path is the correct one depends on the actual direction of the missed branch. Unless the missed branch is a conditional branch that is not taken, the Truls Asheim and Rakesh Kumar are with the Norwegian University of Science and Technology, 7491 Trondheim, Norway. E-mail: {truls.asheim, rakesh.kumar}@ntnu.no. Boris Grot is with the University of Edinburgh, Edinburgh EH8 9YL, U.K. E-mail: boris.grot@ed.ac.uk. sequential path is incorrect. When the wrong path is eventually detected by the core, the instructions after the branch that missed in the BTB are flushed, fetch is redirected to the branch target and pipeline is filled with correct-path instructions. BTB misses are thus highly deleterious to performance as they result in a loss of tens of cycles of work and expose the pipeline fill latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BTB's Role in Instruction Prefetching</head><p>Fetch-directed instruction prefetchers are a class of powerful L1-I prefetchers that intrinsically rely on a BTB. These prefetchers are highly effective and, when coupled with a sufficiently large BTB, outperform the winner of the recently-concluded Instruction Prefetching Championship <ref type="bibr" target="#b1">[2]</ref>, as reported by Ishii et al. <ref type="bibr" target="#b4">[5]</ref>. Variants of these prefetchers have been adopted in commercial products, for example in IBM z15 <ref type="bibr" target="#b9">[10]</ref>, ARM Neoverse N1 <ref type="bibr" target="#b7">[8]</ref> etc.</p><p>Fig. <ref type="figure">2</ref> shows a canonical organization of a fetch-directed instruction prefetcher (FDIP) <ref type="bibr" target="#b8">[9]</ref>. As originally proposed, FDIP decouples the branch-prediction unit and the fetch engine via the fetch target queue (FTQ). This decoupling allows the branch prediction unit to run ahead of the fetch engine and discover prefetch candidates by predicting the control flow far into the future. With FDIP, each cycle, the branch prediction unit identifies and predicts branches to anticipate upcoming execution path and inserts corresponding instruction addresses into the FTQ. Consequently, the FTQ contains a stream of anticipated instruction addresses to be fetched by the core. The prefetch engine scans the FTQ to identify prefetch candidates and issue prefetch requests.</p><p>For FDIP to be effective, the BTB needs to accommodate the branch working set, otherwise frequent BTB misses will cause FDIP to prefetch the wrong path as FTQ will be filled with wrong path instruction addresses. This is one of the key reasons why commercial processors deploy massive BTBs, as also observed by <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BTB-X</head><p>To reduce the overall storage cost, this work seeks to minimize the storage requirements of the costliest fields making up each BTB entry, i.e. target and tag, through two ideas: partitioning and hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partitioned BTB</head><p>As Fig. <ref type="figure">1</ref> shows, the largest contributor to storage cost is the target field, which stores the branch target address. For instance, in the ARMv8 ISA, which uses a 32-bit fixed length instruction encoding, the target address is 46 bits long with a 48-bit virtual address space.</p><p>Our key insight is that targets of most branches lie relatively close in the virtual address space to the branch itself. As a result, encoding the distance to the target, in the form of an offset from the branch instruction, instead of a full target address, can provides drastic storage savings.</p><p>Fig. <ref type="figure" target="#fig_1">3</ref> plots the distribution of offsets in the branch working sets of our workload traces. Offsets are calculated in instruction words, which are 32 bits in the ARM v8 ISA. The data includes both conditional and unconditional branches; hence, it comprehensively covers the full branch working set. The X-axis shows the number of bits required to encode the offset, while the Y-axis plots the frequency of occurrence. Note that, in addition to bits for encoding the offset, an additional bit is required for the direction of the offset (forward/backward).</p><p>As the figure shows, short offsets dominate the distribution with 37% of branches requiring only seven bits or fewer for their offsets. A further 30% of branches only require between 8 and 14-bits to represent their offsets. The reason why such a high fraction of offsets is short is that conditional branches dominate the dynamic branch working set, and they tend to have short offsets <ref type="bibr" target="#b5">[6]</ref>. This is because conditional branches generally guide the control flow only inside a function; meanwhile, software engineering principles favor small functions, thus restricting conditional branch offsets to short distances.</p><p>Perhaps surprisingly, Fig. <ref type="figure" target="#fig_1">3</ref> also shows that very few branches require a large number of bits to encode their offset. Indeed, a meagre 1% of branches requires 25 bits or more for their offset encoding. The sum of these results indicates that reserving space for the full 46-bit target address results in an appalling under-utilization of BTB storage, since 99% of branches need at most half the number of bits needed to represent the full target address if offsets are used instead.</p><p>Based on these insights, we propose to partition a single logical BTB into multiple physically-separate BTBs. The BTBs differ amongst themselves only in the size of the offset. When the branch prediction unit queries an address, all BTB partitions are accessed in parallel, hence presenting a logical equivalent of a monolithic BTB. If the core queries the BTB with n addresses per cycles, each BTB-X partition must be accessed with all n addresses. Fig. <ref type="figure" target="#fig_2">4</ref> shows the BTB partitions used by our proposed BTB organization, called BTB-X. It uses five different BTBs with offset field sizes of 0, 7, 14, 24 and 46 bits. The BTB with no offset field (i.e., 0bit offset) tracks only return instructions. Recall from Section 2 that return instructions read their target address from RAS; as such,   there is no need to allocate space for targets of returns in the BTB. Further, as all instructions in this BTB are returns, it does not require the branch type field either. Other branches are allocated entries in one of the remaining four BTBs based on the minimum number of bits required to encode their offsets. For example, if a branch requires 10 bits for encoding its target offset, it is allocated an entry in the BTB with target offset field size of 14 bits.</p><p>We further make use of the data in Fig. <ref type="figure" target="#fig_1">3</ref> to size each of the BTBs. Because very few branches require more than 24 bits to encode their target offsets, the BTB with the 46-bit offset field is allocated the fewest entries. Meanwhile, the BTBs corresponding to 7-, 14-, and 24-bit offset are allocated a similar number of entries, as the frequency of 1-7 bit, 8-14 bit, and 15-24 bit offsets is about same -37%, 30% and 32% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tag Compression</head><p>Tags comprise the second largest source of storage overhead in each BTB entry, requiring 39 bits in the baseline design. To further reduce the storage requirement, BTB-X uses a compressed 16-bit tag in all of its BTBs. Our compression scheme maintains the 8 loworder bits same as in the full tag. The remaining bits of the full tag are folded, using the XOR operator, in blocks of eight to compute the 8 higher-order bits for the compressed tag. As our evaluation shows, the performance impact of this scheme is negligible as the hashing function (folded XOR) preserves most of the entropy found in the high-order bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Applicability to Basic-Block-Based BTBs</head><p>While this work describes BTB-X in the context of an instructionbased BTB organization (i.e., the BTB is accessed using individual instruction addresses), our insights and design are equally applicable to basic-block-based BTBs (BB-BTBs) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>. BB-BTBs are similar to instruction-based ones but are accessed using a basicblock address. Because existing BB-BTB designs store full branch targets and offsets, they would benefit from optimizations described in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We use ChampSim <ref type="bibr" target="#b0">[1]</ref>, an open-source trace-driven simulator, to evaluate the efficacy of BTB-X on server and client workload traces from IPC-1 <ref type="bibr" target="#b1">[2]</ref>. We warm up microarchitectural structures for 50M instructions and collect statistics over the next 50M. The microarchitectural parameters for the modeled processor are listed in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Storage Breakdown</head><p>The storage requirements for a conventional BTB for different number of BTB entries are presented in Table <ref type="table" target="#tab_2">2</ref> assuming a 48-bit virtual address space. We increase the number of sets in the BTB to increase the number of entries while keeping the associativity same (8-way). Notice that the entry size reduces by one bit while doubling the number of entries. This is because the tag size reduces as more bits are needed to index the BTB.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents the allocation of the storage budget among the five BTB-X partitions. For this analysis, the storage budget is capped at that of a 1K-entry conventional BTB. As the table shows, the partition for 46-bit offsets gets the smallest amount of storage as very few branches need to be allocated there. Meanwhile, the remaining partitions get relatively more storage with a roughly similar number of entries in each partition.</p><p>When presented with a larger storage budget, we follow the same strategy for scaling up BTB-X as for a conventional BTB. Thus, we double the number of sets in each BTB partition to double the capacity while maintaining the associativity (i.e., 0-bit and 7-bit offset partitions are 6-way, others are 5-way).</p><p>Table <ref type="table">4</ref> shows the number of entries that a conventional BTB and BTB-X can accommodate for several storage budgets. As is evident from the table, for a given storage budget, BTB-X can store about 2.8x more entries than the conventional BTB. Note that since the number of sets have to be a power of 2, we are not able to precisely match the storage of conventional BTB and BTB-X -the conventional BTB gets a slightly higher storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance</head><p>To assess the effectiveness of BTB-X, we compare its performance to that of a conventional BTB across different storage budgets. Recall from Section 2 that a larger BTB can deliver two distinct benefits: 1) reduce the incidence of pipeline flushes by detecting branches in the upcoming control flow and 2) facilitate instruction prefetching when coupled with FDIP. Thus, we compare the performance gains achieved by the two competing BTB designs by evaluating them with FDIP. Fig. <ref type="figure" target="#fig_3">5</ref> presents the performance gains obtained on server and client traces. Each bar in the figure shows the contribution to performance of having fewer pipeline flushes and from better instruction prefetching stemming from larger BTB capacities. The results   are normalized to the performance of a core with a 1K-entry conventional BTB (10.875KB storage budget) and no instruction prefetching.</p><p>As the figure shows, BTB-X provides significantly higher overall performance than the conventional BTB for equal storage budgets of up to several tens of kilobytes. The performance advantage of BTB-X is particularly pronounced on server traces whose large instruction footprints pressure the BTB and L1-I. For instance, BTB-X provides 63% performance gain over the baseline compared to 38% of conventional BTB with 21.5KB storage budget. At large BTB storage budgets, the branch working sets of many workloads start to fit in the available BTB capacity, at which point the performance gap between the two designs diminishes.</p><p>A key take-away from the figure is that BTB-X provides same or higher performance than the conventional BTB even when BTB-X is given just half the storage budget of its conventional counterpart. For example, in Fig. <ref type="figure" target="#fig_3">5a</ref>, the conventional BTB improves performance by 38% with a 21.5KB budget whereas BTB-X provides a 44% improvement with just 10.875KB of storage. The reason for this phenomenon is that BTB-X accommodates 2.8x more entries than a conventional BTB of equal storage budget; thus, halving BTB-X's budget still gives a capacity advantage over the conventional design.</p><p>Ignoring instruction prefetching and looking exclusively at performance gains stemming from reduced pipeline flushes, the trends are similar to above. For storage budgets of up to several tens of KBs, BTB-X outperforms a conventional BTB even with half of the latter's storage budget. For instance, Fig. <ref type="figure" target="#fig_3">5a</ref> (blue segments of the bars) shows that BTB-X provides 13% gain with a 10.875KB budget whereas a conventional BTB with twice the budget (21.5KB) gains only 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Tag Compression</head><p>For assessing the performance loss due to compressed tags, we compare the performance of BTB-X with 16-bit tags versus full tags for the smallest BTB size (10.875 KB). We focus on the smallest BTB as it is likely to suffer the highest degree of aliasing due to tag compression. Our results show that, full tags provide 38.21% performance gain, geo-mean across server and client traces, over the baseline compared to 38.16% with compressed tags, a difference of only 0.05%. This indicates that our tag compression scheme is able to preserve the entropy of higher-order bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The multi-megabyte instruction footprints of contemporary server applications cause frequent BTB and L1-I misses, which have become major performance limiters. Because BTB capacity greatly affects front-end performance in terms of flush rate and the efficacy of fetch-directed instruction prefetching, commercial products allocate tens to hundreds of KBs of storage to BTBs. To reduce the BTB storage requirements, this paper introduced an optimized BTB organization. The proposed design, BTB-X, leverages our insight that branch target offsets vary but tend to be much shorter than full target addresses. BTB-X uses an ensemble of five BTBs, each storing offsets of a different length, and also compresses the tags to track 2.8x more branches than a conventional BTB with an equal storage budget. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. BTB entry composition in a conventional BTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distribution of branch target offsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. BTB entry composition for BTB-X partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Performance gain for conventional BTB and BTB-X (both with FDIP) on (a) server and (b) client traces. Baseline is no-prefetch 1K-entry conventional BTB. X-axis is storage for a 1K-, 2K-, 4K-, 8K-, and 16K-entry conventional BTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 30 Mar. 2021; revised 3 June 2021; accepted 21 June 2021. Date of publication 3 Sept. 2021; date of current version 4 Oct. 2021.</figDesc><table /><note>(Corresponding author: Rakesh Kumar.) Digital Object Identifier no. 10.1109/LCA.2021.3109945</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Microarchitectural Parameters</cell></row><row><cell>Core</cell><cell>6-wide OoO, 128-entry FTQ, 128 reservation stations,</cell></row><row><cell></cell><cell>352-entry ROB, 128-entry load queue, 72-entry store queue</cell></row><row><cell>Branch Predictor</cell><cell>Hashed Perceptron</cell></row><row><cell>L1-I</cell><cell>32 KB, 8-way, 4 cycle latency, 8 MSHRs</cell></row><row><cell>L1-D</cell><cell>48 KB, 12-way, 5 cycle latency, 16 MSHRs</cell></row><row><cell>L2</cell><cell>512 KB, 8-way, 14/15 cycle latency, 32 MSHRs</cell></row><row><cell>LLC</cell><cell>2MB, 16-way, 34/35 cycle latency, 64 MSHRs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Storage Breakdown for Conventional BTB</cell><cell></cell></row><row><cell>Entries</cell><cell>Organization</cell><cell>Entry size (bits)</cell><cell>Total (bytes)</cell></row><row><cell>1K</cell><cell>128-set, 8-way</cell><cell>87</cell><cell>10.875K</cell></row><row><cell>2K</cell><cell>256-set, 8-way</cell><cell>86</cell><cell>21.5K</cell></row><row><cell>4K</cell><cell>512-set, 8-way</cell><cell>85</cell><cell>42.5K</cell></row><row><cell>8K</cell><cell>1024-set, 8-way</cell><cell>84</cell><cell>84K</cell></row><row><cell>16K</cell><cell>2048-set, 8-way</cell><cell>83</cell><cell>166K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 Storage</head><label>3</label><figDesc>Breakdown for BTB-X. The Storage Budget is Comparable to That of a 1K-Entry Conventional BTB</figDesc><table><row><cell>Partition</cell><cell>Entry size</cell><cell>Entries</cell><cell>Storage</cell></row><row><cell>0-bit offset</cell><cell>16-bits</cell><cell>768</cell><cell>1.5KB</cell></row><row><cell>7-bit offset</cell><cell>25-bits</cell><cell>768</cell><cell>2.34KB</cell></row><row><cell>14-bit offset</cell><cell>32-bits</cell><cell>640</cell><cell>2.5KB</cell></row><row><cell>24-bit offset</cell><cell>42-bits</cell><cell>640</cell><cell>3.28KB</cell></row><row><cell>46-bit offset</cell><cell>64-bits</cell><cell>80</cell><cell>0.625KB</cell></row><row><cell>Total</cell><cell></cell><cell>2,896</cell><cell>10.25KB</cell></row><row><cell></cell><cell>TABLE 4</cell><cell></cell><cell></cell></row><row><cell cols="4">Storage and Entries in Conventional BTB and BTB-X</cell></row><row><cell cols="2">Conventional BTB</cell><cell>BTB-X</cell><cell></cell></row><row><cell>Storage</cell><cell>Entries</cell><cell>Storage</cell><cell>Entries</cell></row><row><cell>10.875KB</cell><cell>1K</cell><cell>10.25KB</cell><cell>2,896</cell></row><row><cell>21.5KB</cell><cell>2K</cell><cell>20.5KB</cell><cell>5,792</cell></row><row><cell>42.5KB</cell><cell>4K</cell><cell>41KB</cell><cell>11,584</cell></row><row><cell>84KB</cell><cell>8K</cell><cell>82KB</cell><cell>23,168</cell></row><row><cell>166KB</cell><cell>16K</cell><cell>164KB</cell><cell>46,336</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:56:46 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">IEEE COMPUTER ARCHITECTURE LETTERS, VOL. 20, NO. 2, JULY-DECEMBER 2021 Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:56:46 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the Research Council of Norway (NFR) Under Grant 302279 to NTNU and by UK EPSRC Under Grant EP/M001202/1 to the University of Edinburgh.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 06:56:46 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Chapsim</surname></persName>
		</author>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint>
			<date type="published" when="2021-09-13">Sep. 13, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The 1st instruction prefetching championship</title>
		<ptr target="https://research.ece.ncsu.edu/ipc/" />
		<imprint>
			<date type="published" when="2021-09-13">Sep. 13, 2021</date>
		</imprint>
		<respStmt>
			<orgName>NC State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two level bulk preload branch prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bonanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 19th Int. Symp. High Perform</title>
				<meeting>IEEE 19th Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evolution of the Samsung Exynos CPU microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 47th Annu</title>
				<meeting>47th Annu</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Re-establishing fetch-directed instruction prefetching: An industry perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp</title>
				<meeting>IEEE Int. Symp</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. High Perform. Comput. Archit</title>
				<meeting>IEEE Int. Symp. High Perform. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Archit. Support Program</title>
				<meeting>23rd Int. Conf. Archit. Support Program</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The arm neoverse N1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2020-04">Mar./Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annu. ACM/IEEE Int. Symp. Microarchit</title>
				<meeting>32nd Annu. ACM/IEEE Int. Symp. Microarchit</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The IBM z15 processor chip set</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saporito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Hot Chips Symp</title>
				<meeting>Hot Chips Symp</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The AMD &quot;zen 2&quot; processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2020-04">Mar./Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
