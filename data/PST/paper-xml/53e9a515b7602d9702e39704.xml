<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fairness and Throughput in Switch on Event Multithreading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ron</forename><surname>Gabor</surname></persName>
							<email>rongabor@post</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shlomo</forename><surname>Weiss</surname></persName>
							<email>weiss@eng.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
							<email>avi.mendelson@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fairness and Throughput in Switch on Event Multithreading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The need to reduce power and complexity will increase the interest in Switch on Event multithreading (coarse grained multithreading). Switch On Event multithreading is a low power and low complexity mechanism to improve processor throughput by switching threads on execution stalls. Fairness may, however, become a problem in a multithreaded processor. Unless fairness is properly handled, some threads may starve while others consume all of the processor cycles. Heuristics that were devised in order to improve fairness in Simultaneous Multithreading are not applicable to Switch On Event multithreading. This paper defines the fairness metric using the ratio of the individual threads' speedups, and shows how it can be enforced in Switch On Event multithreading. Fairness is controlled by forcing additional thread switch points. These switch points are determined dynamically by runtime estimation of the single threaded performance of each of the individual threads. We analyze the impact of the fairness enforcement mechanism on throughput. We present simulation results of the performance of Switch on Event multithreading. Switch on Event multithreading achieves an average speedup over single thread of 24% when no fairness is enforced. In this case, over a third of our runs achieved poor fairness in which one thread ran extremely slowly (10 to 100 times slower than its single thread performance) while the other thread's performance was hardly affected. By using the proposed mechanism we can guarantee fairness of 1/4, 1/2 and 1 for a small performance loss of 2.2%, 3.7% and 7.2% respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last two decades, different architectures were introduced to support multiple threads on a single die (chip). These architectures can be classified into three classes:</p><p>? Chip Multi-Processor (CMP) -multiple processors (on die) that share some of the memory hierarchy; e.g. IBM's Power4 <ref type="bibr" target="#b41">[42]</ref> and Intel Duo <ref type="bibr" target="#b15">[16]</ref>.</p><p>? Simultaneous Multi-Threading (SMT) in which instructions from multiple threads are fetched, executed and retired on each cycle, sharing most of the resources in the core <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>; e.g. IBM's Power5 <ref type="bibr" target="#b22">[23]</ref> and Intel's P4 <ref type="bibr" target="#b29">[30]</ref>.</p><p>? Switch on Event (SOE, coarse grained multithreading) in which instructions from a single thread are fetched, executed and retired, while an event, such as a long latency memory operation event is used to efficiently switch between the different threads <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43]</ref>; e.g. IBM's RS64 IV <ref type="bibr" target="#b5">[6]</ref> and Intel's Montecito <ref type="bibr" target="#b30">[31]</ref>.</p><p>A conclusive comparison of these architectures is by no means a trivial task since it involves many design and implementation details and therefore is out of the scope of this paper. In general SOE is simple to implement, and can easily be extended to a high number of threads. Not only does simpler implementation mean lower design effort <ref type="bibr" target="#b4">[5]</ref>, but it usually also means lower power.</p><p>There is an ongoing trend towards lower power and complexity of microprocessors. All major microprocessor vendors are going to chip multiprocessors with simple cores rather than complex superscalars <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. In order to improve throughput and power efficiency, more cores and threads are squeezed into a single processor die <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40]</ref>. Asymmetric cores integration can further improve power to performance ratio by integrating simple cores with larger, more complex ones <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. SOE is extremely important to this 'simple-cores' trend, as it can increase number of threads at a relatively small power, area and complexity costs. Given SOE's importance, we feel it deserves further research and study.</p><p>In all the current multi-threaded architectures fairness is a major problem. Lack of fairness is usually caused when resources are unfairly shared between the different threads. Unfair execution can cause, for example, serious responsiveness problems, in which some threads run extremely slowly. CMP based architectures are mainly exposed to fairness in accessing the shared caches in the memory hierarchy. SMT has to handle the fairness among most of the resources of the machine. Handling fairness in CMP is limited to memory access while handling it in SMT is micro- architectural dependent. Fairness in SOE, as will be shown in this paper, can be handled at the architectural level, freeing the microarchitecture from having to deal with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>The following example demonstrates the fairness problem in SOE. Consider the simple two threads case illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The example shows how each of the two threads is executing by itself on the processor, as well as how they run together, using SOE which switches threads on last level cache misses. Let us assume one thread (thread 2) has many more last-level cache misses than the other (thread 1). When executed alone on the processor, each of the two threads will suffer from a certain stall for each miss, as it has nothing to execute while the miss is being resolved (external memory access latency). When executed together in SOE, however, these thread stalls are used for execution of instructions from the other thread. From each thread's perspective, each of these execution stalls ends when the other thread encounters a miss. This means that although in the case of single thread, miss latency is effectively constant (memory access latency), in SOE each thread sees a different miss latency whose length is determined by the other thread's execution. This causes, in our scenario, for the slower thread to become much slower, while the faster thread gets most of the execution cycles. In this example, SOE improved throughput but caused unfair execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Many studies were made on SOE and its variants (coarse grained multithreading). Most of these studies dealt with throughput improvements. Farrens and Pleszkun <ref type="bibr" target="#b14">[15]</ref> introduced Blocked Multithreading (BMT) as part of their evaluation of techniques for improving processor throughput. In BMT the active thread is switched off whenever it is blocked. Gupta et al. <ref type="bibr" target="#b17">[18]</ref> evaluated coarse grain multithreading as part of their latency reducing and tolerating techniques. They evaluated it along with coherent caches, relaxed memory consistency models and prefetching techniques. Eickemeyer et al. <ref type="bibr" target="#b12">[13]</ref> studied SOE throughput in server workloads, and showed that SOE achieves its maximum throughput using three threads. Haskins et al. <ref type="bibr" target="#b19">[20]</ref> introduced differential multithreading (dMT), a variant of BMT which also handles misses in instructions and data caches. They showed that dMT can substantially reduce the cost and complexity of microprocessors. A complexity related study proposed to add SOE on top of SMT in order to increase the number of threads with low complexity overhead <ref type="bibr" target="#b46">[47]</ref>. None of these studies dealt with fairness between threads. Our approach can be applied to any SOE mechanism, such as BMT or dMT, to improve execution fairness.</p><p>Coarse grained multithreading (or SOE) has been implemented in several commercial processors, such as IBM's RS64 IV <ref type="bibr" target="#b5">[6]</ref> and Intel's Montecito <ref type="bibr" target="#b30">[31]</ref>. Montecito preferred SOE over SMT due to the already high IPC (instructions per cycle) and execution units utilization achieved in single thread runs, which implies low potential for SMT. In Montecito's SOE scheme, each thread gets its fair share of the memory hierarchy caches. There is, however, no guarantee for fairness in its threads execution. Several research projects studied SOE multithreading <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>, but none of them dealt with the fairness problem. A survey of SOE research projects and commercial machines can be found in Ungerer et al. <ref type="bibr" target="#b47">[48]</ref> explicit multithreading survey.</p><p>Fair cache sharing between multiple co-scheduled threads has been shown to be a potential cause of serious problems such as threads starvation. Cache sharing can be extremely unfair, for example, when a thread with high miss rate and poor locality constantly causes evictions of other thread's data that will be required soon after. Dynamic and static resources partitioning schemes have been proposed to improve fairness in caches and other resources sharing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Simple time sharing is used at the operating system level to ensure an equal share of time for each thread. Various methods were suggested to manage the time sharing for fairness with prioritization and real-time constraints <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. We deal with the applicability of time sharing to SOE in Section 6.</p><p>Fairness of threads execution was studied in the context of SMT <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. Raasch and Reinhardt <ref type="bibr" target="#b34">[35]</ref> showed that resource partitioning in SMT improves threads' execution fairness. For example, statically partitioned ROB improves fairness compared to competitive sharing. Luo et al. <ref type="bibr" target="#b28">[29]</ref> used fetch policy as a heuristic to prioritize the different threads, in order to improve fairness. Both approaches are applicable to SMT but not to SOE. SOE maintains a single active thread in the pipeline. Hence, resource partitioning will not improve fairness and fine grained fetch prioritization will require frequent pipeline flushes in order to switch threads (severely harming performance). Several attempts were made to suggest a single metric that combines both fairness and throughput. Snavely et al. <ref type="bibr" target="#b38">[39]</ref> used weighted speedup in order to measure the goodness of their operating system level job scheduling. Weighted speedup is the sum of the individual threads' speedups 1 . This metric considers each instruction executed by a low IPC thread as being worth more. It is useful for increasing throughput in a fair manner when selecting a small number of jobs from a larger pool (OS tasks scheduling). Luo et al. <ref type="bibr" target="#b28">[29]</ref> defined fairness as the harmonic mean of the speedups of the individual threads. The speedup they use is the throughput (IP C) of each individual thread when run in multithreaded mode, compared to its throughput when executed alone on the processor. They attempt to capture both throughput and fairness in a single metric. We further discuss these metrics in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Paper Overview</head><p>No attempt has been made so far in the relevant literature to analyze or control fairness in SOE multithreading. This paper fills this gap. We provide a mechanism for fairness enforcement and analyze it. The suggested mechanism uses hardware counters and a feedback loop that monitors fairness by estimating the individual threads' IP C, had each of them been executed alone on the processor. Fairness between threads is enforced by inducing additional thread switches in order to balance threads execution. For simplicity, the rest of the paper uses last-level cache misses as the event that causes thread switches. The suggested approach is applicable to any detectable long latency stall. This paper makes the following contributions:</p><p>Analytical Model: we provide an analytical model and analyze SOE fairness and throughput tradeoffs. The analytical model shows the effects of the induced thread switches. It enables throughput calculation given workload characteristics such as miss distribution and threads performance when executed alone on the processor. The main benefit of the model is the estimation method, which is essential for enforcing fairness. Fairness Enforcement in SOE: we present a low overhead mechanism for fairness enforcement in SOE multithreading. The mechanism tracks run-time fairness by estimating the individual threads' performance, had they been executed alone on the processor. Thread switches are induced when necessary in order to enforce the desired fairness level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model and Fairness Definition</head><p>This section presents an analytical model for SOE fairness and throughput. The model provides a fairness estima- tion method which is used by the mechanism for fairness enforcement at run-time.</p><p>Sketching the mechanism roughly, we estimate the single thread performance of the individual threads had each of them been executed alone, while they are running in SOE multithreading. The estimation is based on the measurement of the throughput of each thread while it is running under SOE in addition to last level cache misses which would have stalled the thread, had it been executed alone. We can then estimate the speedup of each thread by dividing its actual SOE throughput by the estimated single thread throughput. Our proposed mechanism induces additional thread switches in order to make sure that the speedups are similar for all of the threads. We define fairness as the ratio between the speedup of the individual threads. We compute the necessary "instructions per switch" quota that needs to be maintained in order to guarantee a minimum specified level of fairness. This Instructions Quota is then maintained using deficit counting (as explained in section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Program Behavior Model</head><p>In order to present a simple and meaningful analytical model of the fairness problem, let us assume that the execution of single thread applications can be viewed as a sequence of instructions delimited by long latency last-level cache misses. Let IP M (Instructions Per Miss) be the average number of useful instructions executed between two consecutive misses, and CP M (Cycles Per Miss) be the average number of cycles between those misses. When a thread executes alone on the processor, each miss causes an execution stall of M iss lat cycles (M iss lat is the average memory access time in processor cycles). IP M j and CP M j (IP M and CP M of thread j) are illustrated in Figure <ref type="figure" target="#fig_1">2</ref> for two threads both when running together (using SOE), and when executed alone.</p><p>Let IP C ST j be the average number of useful instructions executed per cycle (retired instructions per cycle) in thread j when executed alone on the processor. As shown in Equation 1, IP C ST j can be calculated by dividing IP M j , by the average of the total number of cycles corresponding to these instructions 2 , which is CP M j + M iss lat .</p><formula xml:id="formula_0">IP C ST j = IP M j CP M j + M iss lat<label>(1)</label></formula><p>Let IP C SOE j be the average number of useful instructions executed per cycle in thread j when running together with other threads using SOE. As shown in Equation <ref type="formula" target="#formula_1">2</ref>, IP C SOE j can be calculated by dividing IP M j by the average total number of cycles of a whole switch-on-event round (until thread j gains execution again). A whole round is calculated by summing the CP M of all the threads together with their corresponding Switch lat cycles (the average overhead per thread switch).</p><formula xml:id="formula_1">IP C SOE j = IP M j k (CP M k + Switch lat )<label>(2)</label></formula><p>It should be noted that Equation 2 holds as long as misses that cause thread switches are resolved by the time their threads are running again. This is obviously true if there are sufficient threads available. 2 We deliberately ignore out-of-order issues such as useful work done while cache misses are being resolved or the fact that several cache misses may be pending (overlapping of cache misses). This is done in order to simplify the model. It should be noted that our empirical results indicate that the analytical model gives adequate approximation. It should also be noted that our implementation only counts the first miss in a each group of overlapped misses (see Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fairness Metric</head><p>A system is fair if all the threads experience an equal slowdown compared to their performance had they been executed alone <ref type="bibr" target="#b28">[29]</ref>. We define a perfectly fair system as:</p><formula xml:id="formula_2">?j, k IP C SOE j IP C ST j = IP C SOE k IP C ST k<label>(3)</label></formula><p>Following Equation <ref type="formula" target="#formula_2">3</ref>, a fairness metric can be defined as the minimum ratio between the slowdowns of any two threads running in the system:</p><formula xml:id="formula_3">F airness ? min j,k ? ? ? ? IP C SOE j IP C ST j IP C SOE k IP C ST k ? ? ? ? = min j,k IP C SOE j ? IP C ST k IP C ST j ? IP C SOE k (4)</formula><p>It should be noted that this definition of fairness is more strict than the harmonic mean used by Luo et al. <ref type="bibr" target="#b28">[29]</ref>. It uses the maximum and minimum speedups for fairness calculation, which guarantees that these values will not have reduced impact on fairness due to averaging. In other words, enforcing fairness using our definition is guaranteed to improve harmonic mean based fairness, but not vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fairness Enforcement</head><p>According to the definition in Equation <ref type="formula">4</ref>, 0 ? F airness ? 1. Perfect fairness is achieved when F airness = 1. Fairness decreases with the metric value, until it reaches 0 which is complete unfairness (one of the threads is completely starved -not running at all).</p><p>Substituting Equation <ref type="formula" target="#formula_0">1</ref>and Equation 2 into Equation 4 results in Equation <ref type="formula" target="#formula_4">5</ref>:</p><formula xml:id="formula_4">F airness ? min j,k CP M j + M iss lat CP M k + M iss lat<label>(5)</label></formula><p>Equation 5 shows that unless we modify our SOE scheme, fairness will be defined by CP M , which is a characteristic of the running threads.</p><p>In order to control fairness in SOE, we must define forced switch points, not necessarily caused by last-level cache misses. Let IP Sw j be the average number of instructions executed from thread j, before it is switched out. Similarly, let CP Sw j be the average number of cycles thread j has executed before it is switched out (if thread j switches out only due to last level cache misses then IP Sw j = IP M j and CP Sw j = CP M j ). Using IP Sw j and CP Sw j modifies Equation <ref type="formula" target="#formula_1">2</ref>:</p><formula xml:id="formula_5">IP C SOE j = IP Sw j k (CP Sw k + Switch lat )<label>(6)</label></formula><p>Substituting Equation <ref type="formula" target="#formula_5">6</ref>and Equation 1 into equation 4 we get:</p><formula xml:id="formula_6">F airness ? min j,k IP Sw j IP Sw k IP M k (CP M j + M iss lat ) IP M j (CP M k + M iss lat ) (7)</formula><p>We define the parameter F to be the target fairness that we wish to achieve from the system (0 ? F ? 1). Based on our definition of fairness from Equation 7, a system achieves the required fairness F if it satisfies Equation <ref type="formula" target="#formula_7">8</ref>.</p><formula xml:id="formula_7">F ? min j,k IP Sw j IP Sw k IP M k (CP M j + M iss lat ) IP M j (CP M k + M iss lat )<label>(8)</label></formula><p>Let CP M min be the minimal value of CP M of all threads, CP M min = min j CP M j . Setting IP Sw j for each thread as shown in Equation <ref type="formula">9</ref>is guaranteed to satisfy Equation 8 <ref type="foot" target="#foot_0">3</ref> . <ref type="formula">9</ref>is the main result of the analytical model. It is used by the fairness enforcement mechanism in order to calculate IP Sw j for each thread that, when enforced, will achieve the required fairness.</p><formula xml:id="formula_8">IP Sw j = min IP M j , IP C ST j F (CP M min + M iss lat ) (9) Equation</formula><p>Our SOE scheme switches threads in order to achieve an average of IP Sw j instructions per switch. It switches on last-level cache misses in addition to forced switches due to IP Sw j . Hence, there is no way to increase IP Sw j to a value greater than IP M j (that is why we use min in Equation 9).</p><p>As shown in Equation <ref type="formula">4</ref>, fairness is the minimum ratio of speedups between any two threads in the system. When used as a parameter F, it sets the allowed speedup ratio. For example, calculating IP Sw j for all threads using F = 1/2 will guarantee a maximum speedup ratio of 2. This means that the ratio of speedups between the threads that have the highest and lowest speedup will be at most 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fairness and Throughput</head><p>Using IP Sw j to control fairness sets new thread switches. These switches are defined by the IP Sw j quota and not necessarily by last-level cache misses. In other words, there are forced thread switches, that cause some execution stall (thread switch latency), which are not hiding any other long latency stall (such as the last-level cache miss). This means that fairness enforcement will affect the throughput.</p><p>We can measure throughput as the IP C SOE , which is the IP C of all of the threads when executed together using SOE. As shown in Equation <ref type="formula">10</ref>, IP C SOE can be calculated by dividing the total average number of instructions </p><formula xml:id="formula_9">F = 0 F = 1 F = 1/2 j (thread num) 1 2</formula><formula xml:id="formula_10">IP C SOE = k IP Sw k k (CP Sw k + Switch lat ) = k IP C SOE k (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2</head><p>This example demonstrates the use of fairness enforcement using the method described in Section 2.3. Consider the case of two threads sharing a processor using SOE, switching on last-level cache miss events. Let us assume that instructions run at a rate of 2.5 instructions per cycle on both threads excluding miss stalls (IP C no miss = 2.5). Memory access latency on a last level cache miss is 300 cycles. Thread switch latency is 25 cycles. The first thread has a miss every 15,000 instructions (6,000 cycles), the second every 1,000 instructions (400 cycles). Table <ref type="table" target="#tab_1">2</ref> shows the performance of the two threads when running alone (IP C ST j ) and when running together in SOE mode (IP C SOE j ). As shown, in the latter case, the first thread's IP C drops by a factor of 1.02, while the other thread's IP C drops by 9.2. This is clearly unfair, as the faster thread (thread 1), whose performance is hardly affected, causes the other thread to slow down by a factor of 9. This gives a fairness metric of 0.11. However, when fairness is enforced to 1, the first thread is forced to switch every 1, 667 instructions (on average), instead of only on cache misses which occur every 15, 000 instructions (IP M 1 = 15, 000).</p><p>In this example thread 2 is almost starved when no fairness is enforced (F = 0). When F = 1 (fairness enforced to 1), both threads are forced to equal slowdown compared to their IP C ST j . When F = 1/2, slowdown is allowed to differ by at most a factor of 2. </p><formula xml:id="formula_11">IP C no miss = [IP C no miss1 , IP C no miss2 ], IP M = [IP M 1 , IP M 2 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Fairness and Throughput Analysis</head><p>When fairness is not enforced, every thread switch hides a memory access stall (last-level cache miss). However, when fairness is enforced, additional thread switches are induced in order to maintain the required fairness. Intuitively, these forced thread switches, which cause a Switch lat cycles stall, reduce the throughput of SOE <ref type="foot" target="#foot_1">4</ref> .</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the effect of fairness enforcement on throughput in a two threads model. It shows throughput degradation due to fairness enforcement, of thread pair combinations with different IP C no miss (performance excluding misses) and IP M . The figure shows that in most cases fairness enforcement cause throughput degradation. As shown by the lines of IP C no miss = [2.5, 2.5], when IP C no miss is similar for both threads, throughput degrades by up to 4%. However, when the two threads do not have the same IP C no miss , throughput can degrade by up to 15% or can actually improve by up to 10%. The throughput increase in the IP C no miss = [2, 3] cases is explained by noting that fairness enforcement biases the execution towards the thread with the higher IP C no miss , hence, improves throughput.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows a tradeoff between F (enforced fairness) and throughput (for M iss lat = 300, Switch lat = 25). As shown, in some situations, forcing fairness increases the throughput, while in other cases using a fairness (F) of less than 1 allows balancing the fairness requirement with the throughput degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head><p>In order to enforce fairness between threads, the processor should calculate IP Sw j for each thread based on its runtime characteristics and regulate the switch points in order for the average instructions per switch to be equal to the required IP Sw j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Runtime Calculation of IP Sw j</head><p>In order to calculate IP Sw j (Equation <ref type="formula">9</ref>), two thread characteristics must be obtained for each thread: IP M j and CP M j . In addition M iss lat must be known. All of these can be obtained using three hardware counters per thread. These counters should count instructions retired, last-level cache misses and cycles while the threads are running in SOE:</p><p>Instrs j : instructions retired from thread j. Cycles j : total number of cycles from the retirement of the first instruction after thread j switches in, until it is switched out (this is the actual number of cycles the thread was running, excluding the switch overhead). M isses j : number of last level cache misses encountered while running thread j. In order to minimize inaccuracies caused by overlapping of misses (several clustered pending misses on a specific thread caused by out-of-order execution) we count only misses that actually cause a thread switch (non-resolved misses that are encountered on retirement of the next-to-retire instruction).</p><p>The characteristics of the threads can be computed from these counters, as shown in Equation <ref type="formula" target="#formula_12">11</ref>, Equation 12 and Equation <ref type="formula" target="#formula_14">13</ref>. Average M iss lat can be either a predefined parameter or a measured statistic.</p><formula xml:id="formula_12">IP M j = Instrs j max(M isses j , 1)<label>(11)</label></formula><formula xml:id="formula_13">CP M j = Cycles j max(M isses j , 1)<label>(12)</label></formula><formula xml:id="formula_14">IP C ST j = IP M j CP M j + M iss lat<label>(13)</label></formula><p>IP Sw j can be calculated using the hardware counters every ? cycles. The calculated IP Sw j values will be used during the following ? cycles. In other words, hardware counters of each ? cycles are used as an estimation for the following ? cycles. ? should be set to a value large enough to allow good statistical averaging, but not too large in order to allow performance phases to be accurately tracked.</p><p>In rare cases, where a thread does not encounter any miss in ? cycles, a value of M isses j = 1 is used to estimate its performance. This decreases IP C ST j estimation, however, it is still good enough for our purposes.</p><p>There are several alternative implementations for calculating IP Sw j every ? cycles. It can be done in hardware, using injected instruction flows (flows of instructions that are injected into the pipeline by the hardware as a result of an event) or using interrupts. We found ? = 250, 000 cycles to be large enough for our statistical purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Maintaining IP Sw j using Deficit Counts</head><p>Fairness mechanism is expected to maintain IP Sw j instructions on average, between switches for thread j. However, simply forcing a thread switch every IP Sw j instructions will not produce the expected average instructions per switch, as threads are switched on last-level cache misses as well. In order to achieve this average, we are using a per thread Deficit Counter, for dynamically adjusting the switch points. Deficit Counters are used to hold the quota 'left-over'. This leftover is caused by misses encountered before the quota is fully used up. The leftover increases the quota of the thread the next time it is switched in. This deficit mechanism is done in a similar manner to Deficit-Round-Robin mechanism <ref type="bibr" target="#b36">[37]</ref>.</p><p>The hardware maintains a per thread Deficit Counter. Initially, the Deficit Counter is set to 0 for all of the threads. Deficit Counter of thread j is incremented by IP Sw j every time thread j is switched in. On retirement of each instruction, the corresponding Deficit Counter is decremented. A thread is switched out when its Deficit Counter reaches 0, or when a last-level cache miss is encountered.</p><p>Deficit counting ensures that when a miss event causes a switch before IP Sw j instruction quota was reached, the next instructions quota for that thread will be larger. This compensates for the shorter previous quota, that ended by a miss. The deficit mechanism ensures that on average the thread will run for the required IP Sw j instructions between switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simulation Methodology</head><p>An out-of-order processor was simulated using a detailed cycle accurate execution driven simulator. The processor is derived from the P6 micro-architecture <ref type="bibr" target="#b18">[19]</ref>. The simulator uses Long Instruction Traces (LITs) <ref type="bibr" target="#b37">[38]</ref>. LITs are not actually traces (they do not contain an instruction or execution trace). Each LIT contains an architectural checkpoint (state snapshot) in addition to injectable external events. Each checkpoint consists of a memory image and processor state (registers). Injectable external events include interrupts, IO and DMA events. Using LIT enables full accurate simulation of the application as well as of interrupts, operating system and DMA side effects. These tools and methodology were extensively used for detailed simulation in many studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Machine Configuration</head><p>We simulated an out-of-order core, with first level instruction and data caches, a unified 2nd level cache (L2), a pipelined bus and a constant latency memory (as shown   in Figure <ref type="figure" target="#fig_3">4</ref>). Table <ref type="table" target="#tab_3">3</ref> summarizes machine configuration parameters. Structure sizes were based on Intel's disclosure of their latest processor core <ref type="bibr" target="#b26">[27]</ref>, and were slightly increased to reflect our view on a future version of that processor. Memory latency was set to 300 cycles, which is 75ns at 4GHz processor frequency. High level views of the simulated processor pipeline as well as the memory hierarchy are shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>We have modified the simulator to support SOE multithreading, switching on L2 cache misses (last-level cache misses). Misses induced by load instructions as well as i/d TLB page walks are tracked by flagging them in the ROB (Re-Order Buffer). A thread switch is triggered when the head of the ROB (the next instruction or micro-operation to be retired) is flagged as handling a miss which has not been resolved <ref type="foot" target="#foot_2">5</ref> . A thread switch causes draining of instructions from the RS (Reservation Station), ROB and LB (load buffers) and is simulated as a 6 cycles long draining. Structures such as iTLB, dTLB, caches and branch prediction history are shared, and are not flushed on switch. This is required in order to maintain performance after thread switch events <ref type="bibr" target="#b30">[31]</ref>. The store buffer keeps dispatching retired stores even after a flush, but will not forward their data if they are not from the same thread.</p><p>Switch latency is defined as the number of cycles from the start of the switch until the first instruction of the next thread reaches the pipe stage in which the switch was triggered (retirement). The switch latency is not constant (depends for example on the instruction that was switched in), and usually accumulates to around 25 cycles. We set ? to 250, 000 cycles. ? is the period used for sampling hardware counters and recalculating fairness parameters. In order to ensure that all threads are run in every ? cycles, each thread is limited to a certain amount of time, before it is forced to switch out. We refer to this value as the maximum cycles quota per thread. This value should be less than ?/N , where N is the number of threads. A value less then ?/N ensures that all threads execute in any given ? cycles. The maximum cycles quota deals with the rare cases where threads do not encounter any miss in ? cycles. It should be noted that the maximum cycles quota for a thread switch should be large enough so that the quotaforced switches are relatively rare and do not cause performance degradation. We used 50,000 cycles as the maximum cycles quota per thread. Spec CPU2000 benchmarks <ref type="bibr" target="#b9">[10]</ref> were simulated on a two threads SOE configuration. Caches were warmed up using 10,000,000 instructions from each thread. Threads were simulated until both of them completed at least 6,000,000 instructions. The first 1,000,000 simulated instructions were not included in the results (statistics), and were used to warm up the internal micro-architecture state (internal structures such as the branch prediction tables as well as the fairness mechanism state). When the same benchmark was run on both threads, the two threads were offset by 1,000,000 instructions.</p><p>We used 16 combinations of benchmarks, out of which 8 combinations were of the same benchmark executed on both threads. Each combination was simulated using SOE without fairness (F = 0), and with F = 1, 1/2 and 1/4. In addition, we simulated each benchmark alone on the processor, in order to obtain its real IP C ST j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Detailed Examination</head><p>We use a representative two threads combination in order to gain insight into the fairness enforcement mechanism. We use a combination of gcc and eon applications. This combination requires active fairness enforcement without which the gcc thread almost starves, while eon runs on the processor most of the time. estimated performance using hardware counters. Equation 13 is used to estimate the performance. The figure shows that hardware counters can be effectively used to estimate single thread performance of each thread (IP C ST j ), while they are both running in SOE.</p><p>As shown in Figure <ref type="figure">5</ref> (top), the estimated IP C ST j closely tracks the real IP C ST j . It is usually slightly lower than the real IP C ST j , due to several issues that may slow down each thread's execution in between misses. When a thread executes alone, the out of order mechanism uses some of the miss-stall cycles for out-of-order execution. These speculatively executed instructions are retired after the miss is resolved. Needless to say, these executions are not possible in SOE, which uses the stall time for the execution of the other thread. Another factor which reduces the performance of the individual threads is resource sharing (e.g. branch predictor tables). Sharing of resource reduces their effective size, as seen by each thread, when both threads are running together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Fairness Enforcement. Figure 5 (middle) shows</head><p>the individual thread estimated speedups and the actual achieved fairness (bottom), when the two threads run under SOE. When fairness is not enforced, the 2nd thread (eon) executes most of the time, which causes the 1st thread (gcc) to almost starve. When fairness is enforced to 1/4, the 2nd thread's (eon) speedup is not allowed to exceed the 1st thread's (gcc) speedup by a factor of more than four. Now ) with and without fairness enforcement, as well as IP C ST j of each of the two threads when executed alone.</p><p>the speed of gcc is 20 times faster than its speed without fairness enforcement.</p><p>The speedup plot shows one occurrence in which the 2nd thread gets higher speedup, followed by a slightly higher speedup of the 1st thread. This can be seen on the plots at 6,000,000 cycles when no fairness is enforced, and on 7,000,000 cycles when fairness is enforced to 1/4. This is most likely due to a short performance phase change, in which the estimation based on the previous ? (250,000) cycles isn't accurate. This causes a short unfair execution. In long runs, however, the effects of these short unfair periods average towards an average of good fairness. The time difference of 1,000,000 cycles indicates that phase change occurred in the 2nd thread (eon), which gets slower when fairness is enforced (had the phase change been in the other thread, which gets faster when fairness is enforced, the occurrence would have moved to an earlier cycle in the fair scenario).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Fairness and Throughput</head><p>The throughput of different thread combinations is shown in Figure <ref type="figure">6</ref>. IP C SOE with and without fairness enforcement, as well as IP C ST j for both threads, are shown. IP C SOE is shown as stacked performance of the two threads (Equation <ref type="formula">10</ref>). The average speedup of SOE over single thread 6 is 24%, 21%, 19% and 15%, for no fairness enforcement (F = 0), F = 1/4, 1/2 and 1.</p><p>Usually, as shown in Figure <ref type="figure">6</ref>, fairness enforcement has only negligible effect on the throughput when IP C ST j of the two threads is roughly the same (e.g. lucas:applu, bzip2b:bzip2b). The greater the difference between performance of the individual threads, the more thread switches will be required in order to enforce fairness. Induced switches cause throughput degradation as enforced fairness F increases (e.g. galgel:gcc, apsi:swim).   Figure <ref type="figure" target="#fig_6">7</ref> shows throughput degradation caused by fairness enforcement, as well as number of forced thread switches in 1000 cycles. It shows throughput normalized to the throughput without fairness enforcement. Forced switches are caused by fairness enforcement (they do not hide any memory access). Enforcing fairness of 1, 1/2 or 1/4 causes an average throughput degradation of 7.2%, 3.7% and 2.2% respectively. Enforcing perfect fairness (F = 1) can cause a large performance degradation, even in cases where the same application is executed on both threads. Using fairness of 1/2 or 1/4, allows the thread speedups to have some difference (slack), which reduces performance degradation. Throughput degradation is the combined effect of the overhead caused by induced thread switches and the fact that usually slower threads (threads with lower IP C no miss ) are allowed to execute more instructions (at the expense of the faster thread). As shown, there is a high correlation between the number of forced thread switches and the effect on the throughput (performance).</p><p>Figure <ref type="figure">8</ref> (left) shows achieved fairness with and without fairness enforcement. The F = 0 line shows the achieved fairness of the two threads without any attempt to enforce it. The lines with F = 1/4, 1/2 and 1 show the fairness when it is enforced to be 1/4, 1/2 and 1 respectively. Simulation runs are ordered by their achieved fairness when no fairness is enforced. As shown in the figure, even in the most unfair cases fairness enforcement achieves a fairness close to F (the target fairness being enforced). On runs which are also fair without fairness enforcement, the mechanism has small effect on the achieved fairness. that when achieved fairness is greater than F (the target fairness being enforced), it will be truncated to F. This gets rid of the biasing effect of runs that are fair even without fairness enforcement (these runs that would have otherwise biased the averages towards 1). There is no truncation for the case of F = 0. The results show that achieved fairness is highly accurate, although its accuracy degrades as stricter fairness is being enforced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Simple heuristics for fairness improvements, such as fetch prioritization or simple time sharing, are ineffective for SOE. Fetch prioritization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> is a fine grained prioritization approach appropriate for SMT. It prioritizes fetches from the different threads. This can result in changing the thread from which instructions are fetched every few cycles. This is applicable to SMT, where the front end maintains several threads and fetches are based on prioritization. SOE, however, maintains a single fetch thread and has to flush the pipeline on each such thread switch. Fetch prioritization will result in frequent pipeline stalls which will severely impair SOE performance. Simple time sharing is ineffective for producing high fairness with small performance degradation. When the time sharing quota is small, high fairness can be achieved at the cost of high performance degradation (due to frequent pipeline flushes). When the quota is large, performance is maintained but fair execution is only rarely achieved. Consider the scenario in Example 2. Forcing an average of 400 cycles between thread switches will produce poor fairness. Although time will be equally divided between the two threads, their individual speedups (</p><formula xml:id="formula_15">IP C SOE j IP C ST j</formula><p>) will be 0.5 and 0.8, and the achieved fairness will be 0.5 0.8 = 0.6. On the other hand, as shown in Table <ref type="table" target="#tab_1">2</ref>, by using the proposed algorithm the speedup of both threads can be adjusted to 0.63 ( 1 1.59 ) and the achieved fairness in that simple example will be 0.63 0.63 = 1.0. Several attempts have been made to suggest a single metric that combines both fairness and throughput. Luo et al. <ref type="bibr" target="#b28">[29]</ref> used the harmonic mean of the individ-ual threads' speedups. Snavely et al. <ref type="bibr" target="#b38">[39]</ref> suggested using weighted speedup which is the sum of the individual threads' speedups (W S = N i=1 (IP C SOE j /IP C ST j )). We found both metrics to give insufficient insight into either throughput or fairness, and the harmonic mean to be biased toward fairness. We prefer the use of two metrics, one for fairness and one for throughput (IPC). Each of them clearly captures its purpose. Using two metrics allows the user to subjectively balance between fairness and throughput. We believe that fairness should be controlled in such a way that maximum performance is maintained while guaranteeing that no thread starves. This is not possible with either the weighted speedup or the harmonic mean metric. Our fairness metric is based on speedups of the individual threads (similar to the approach used by Luo et al.) and clearly encapsulates fairness (but not throughput).</p><p>The empirical results shown in this paper demonstrate that fairness can be effectively maintained using the proposed mechanism. The results also indicate that attempting to maintain perfect fairness (F = 1) has negative effects on performance and in many cases on the achieved fairness as well. Both performance and the achieved fairness are affected by the large number of forced switches introduced by the attempt to maintain perfect fairness. These thread switches reduce the accuracy of the IP C ST estimation, on which fairness enforcement mechanism is based. In extreme cases, this can lead to achieving worse fairness than when attempting to enforce a less strict one (e.g. gcc:gcc or mgrid:mgrid in Figure <ref type="figure">8</ref>).</p><p>Increasing the enforced fairness (F) increases the number of induced thread switches. This increases the total thread switch overhead as well as usually allowing the slower thread to execute more instructions. This usually decreases throughput but increases the achieved fairness. Hence there is a tradeoff between fairness and throughput. The balance point between fairness and throughput depends on the user's preferences. There is no objective metric that combines both throughput and fairness to allow such balance point analysis. According to our simulation results, using F ? 0.5 is a reasonable compromise. This maintains high throughput and guarantees that no thread starves.</p><p>The analytical model shows that enforcing fairness can actually improve throughput. We did not encounter such a case in our simulation runs. Fairness will increase throughput when its enforcement biases execution towards threads that have a higher IP C no miss . This means that lower CP M threads also have higher IP C no miss . This occurs for short periods of time, but not enough to increase the average throughput in any of our runs.</p><p>The SOE mechsnism presented in this paper may be expanded by using various events to trigger thread switches. For example, L1 misses (which may hit or miss the L2 cache) can cause a thread switch to hide L1 miss latency.</p><p>Another example are explicit instructions that can trigger thread switches 7 .</p><p>We used a constant predefined value for miss latency (300 cycles) to represent the average memory access latency. Some switch events may have variable latency, whose average is hard to predict (e.g. L1 miss). In these cases, event's latency should be monitored using hardware counters. For example, in order to determine L1 miss latency, a hardware counter could count the total number of cycles used for L1 misses handling. On every ? cycles, when fairness parameters are recalculated, M iss lat should also be calculated, using the hardware counter divided by the number of L1 misses. This method can be used to support multiple event types with variable latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>SOE is a low power and low complexity multithreading solution, that improves processor utilization and throughput. It hides execution stalls, such as last level cache misses, by switching threads on the detection of such stalls. This paper presented a fairness enforcement mechanism for SOE, forcing thread switch points based on architectural level runtime fairness tracking. Fairness tracking is done by estimating the single thread performance of individual threads while they are running using SOE multithreading, based on a model developed in this paper. The overhead of the fairness mechanism is very low. It requires the use of a few hardware counters, which exist anyhow in most modern processors, and the addition of hardware or software hooks (e.g. interrupts) for IP Sw j calculation.</p><p>Simulation results show that the suggested fairness enforcement mechanism works well for a variety of applications. SOE achieves an average speedup over single thread of 24% when no fairness is enforced. In this case, over a third of our runs achieved poor fairness in which one thread ran extremely slowly (10 to 100 times slower than its single thread performance) while the other thread's performance was hardly affected. By using the proposed mechanism we guarantee fairness of F = 1/4, 1/2 and 1 for a performance loss of 2.2%, 3.7% and 7.2% respectively. It should be noted that by loosing a very small amount of performance, we guarantee thread execution fairness.</p><p>When thread execution is unfair, the fairness mechanism improves fairness by forcing additional thread switch points. Fairness enforcement, when not required (threads run in a fair fashion even without any enforcement), has negligible effect on the execution. Thread switches caused by fairness enforcement usually cause a performance loss, due to the switch latency they incur. Enforcing strict fairness is not necessary, a reasonable compromise is to en- 7 An example of such instruction in X86 instruction set is pause. This instruction hints that a short execution pause can be done. Pause is usually used in busy wait loops, that wait for external events. force a fairness of 0.5 or less in order to reduce the impact of thread switches and maintain a high throughput.</p><p>The fairness mechanism is based on estimating the single thread performance of the individual threads, while running in SOE. This estimated performance is used to calculate the achieved fairness, to induce thread switches in case the achieved fairness needs to be improved. The extension of our method to SMT is by no means a trivia task due to the difficulty in estimating the performance of the individual threads while they are running in SMT. Extension of this work to SMT is being considered and remains as a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Intuitive example of unfair execution in SOE. Ex1 marks execution of instructions from thread 1, Ex2 from thread 2, M marks last level cache misses and Sw denotes thread switch overheads. When both threads run together using SOE (bottom), the 2nd thread runs extremely slowly while the 1st thread's performance is hardly affected by the multithreading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Representation of single thread runs for two threads (top) and SOE run of the same two threads (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The effect of fairness enforcement on throughput, using a two threads analytical model. Legend's notation: IP C no miss = [IP C no miss1 , IP C no miss2 ], IP M = [IP M 1 , IP M 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Processor pipeline and memory hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 1 . 1 .Figure 5 .</head><label>115</label><figDesc>Figure 5. IP C ST j estimation using hardware counters while running in SOE multithreading with fairness enforcement (top). The estimated speedup of the individual threads when run with and without fairness enforcement (middle). The achieved ratio between thread speedups -achieved fairness (bottom). Fairness is enforced to 1/4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6</head><label></label><figDesc>Speedup of SOE over single thread =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. SOE throughput degradation due to fairness enforcement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 (F = 1 F = 1/ 2 F = 1/ 4 FFigure 8 .</head><label>81248</label><figDesc>Figure7shows throughput degradation caused by fairness enforcement, as well as number of forced thread switches in 1000 cycles. It shows throughput normalized to the throughput without fairness enforcement. Forced switches are caused by fairness enforcement (they do not hide any memory access). Enforcing fairness of 1, 1/2 or 1/4 causes an average throughput degradation of 7.2%, 3.7% and 2.2% respectively. Enforcing perfect fairness (F = 1) can cause a large performance degradation, even in cases where the same application is executed on both threads. Using fairness of 1/2 or 1/4, allows the thread speedups to have some difference (slack), which reduces performance degradation. Throughput degradation is the combined effect of the overhead caused by induced thread switches and the fact that usually slower threads (threads with lower IP C no miss ) are allowed to execute more instructions (at the expense of the faster thread). As shown, there is a high correlation between the number of forced thread switches and the effect on the throughput (performance).Figure8(left) shows achieved fairness with and without fairness enforcement. The F = 0 line shows the achieved fairness of the two threads without any attempt to enforce it. The lines with F = 1/4, 1/2 and 1 show the fairness when it is enforced to be 1/4, 1/2 and 1 respectively. Simulation runs are ordered by their achieved fairness when no fairness is enforced. As shown in the figure, even in the most unfair cases fairness enforcement achieves a fairness close to F (the target fairness being enforced). On runs which are also fair without fairness enforcement, the mechanism has small effect on the achieved fairness.Figure8(right) show the average achieved fairness.It shows the average and standard deviation of min(F, Achieved f airness ).Using min(F, Achieved f airness ) in our calculation ensures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Abbreviations and symbols.</head><label>1</label><figDesc></figDesc><table><row><cell>1 Weighted ? N</cell><cell>speedup</cell><cell>is</cell><cell>defined</cell><cell>as</cell><cell>W S</cell><cell>=</cell></row></table><note><p>i=1 (realized IPC job i /single-threaded IPC job i ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Example of two threads SOE with and without fairness enforcement. executed</head><label>2</label><figDesc>by all threads in a single SOE threads round, by the respective number of cycles. Finally, Equation 6 can be used to show that IP C SOE = k IP C SOE</figDesc><table><row><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell></row></table><note><p>k :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 . Simulated machine parameters</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Thread Switch Trigger</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">(when next uop is pending on a detected miss)</cell><cell>Memory</cell></row><row><cell cols="2">Instr Fetch Decode Instr</cell><cell>Alloc Rename/ Uop (write to RS) Issue Uop</cell><cell>Dispatch and Schedule</cell><cell>Execute</cell><cell>Retire</cell><cell>L2 Cache (unified) Bus</cell></row><row><cell>ALU</cell><cell cols="2">ALU</cell><cell></cell><cell></cell><cell></cell><cell>Cache L1 Data</cell><cell>L1 Instrs Cache</cell></row><row><cell>Load (L1 hit) Load</cell><cell>AGU</cell><cell>Data Cache Access (hit)</cell><cell></cell><cell></cell><cell></cell><cell>Out-Of-Order Core Memory Hierarchy</cell></row><row><cell>(L2 miss)</cell><cell>AGU</cell><cell>Data Cache Access (miss)</cell><cell cols="2">L2 Cache Access (miss)</cell><cell cols="2">Memory Access</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>This can be proven algebraically.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>It will be shown later that additional forced switches can actually increase the throughput.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>This triggering scheme allows overlapping of clustered misses executed using the out-of-order (prefetching effect).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The MIT Alewife machine: architecture and performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA-22</title>
		<meeting>of ISCA-22</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">April: a processor architecture for multiprocessing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3a</biblScope>
			<biblScope unit="page" from="104" to="114" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perceptron-based branch confidence estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koltur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Refaai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA-10</title>
		<meeting>of HPCA-10</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The impact of performance asymmetry in emerging multicore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA-32</title>
		<meeting>of ISCA-32</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="506" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">uComplexity: Estimating processor design effort</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bazeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Mesa-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-38</title>
		<meeting>of MICRO-38</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multithreaded PowerPC processor for commercial servers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Borkenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kunkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="885" to="898" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Power-sensitive multithreaded architecture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2000 IEEE International Conference on Computer Design (ICCD &apos;00)</title>
		<meeting>of the 2000 IEEE International Conference on Computer Design (ICCD &apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">199</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamically controlled resource allocation in SMT processors</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-37</title>
		<meeting>of MICRO-37</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A stateless, content-directed data prefetching mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Standard performance evaluation corporation</title>
		<ptr target="http://www.spec.org/cpu2000/" />
	</analytic>
	<monogr>
		<title level="m">Spec CPU2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>CPU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximizing CMP throughput with mediocre cores</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT-14</title>
		<meeting>of PACT-14</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Borrowed-virtual-time (BVT) scheduling: supporting latency-sensitive threads in a general-purpose scheduler</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM symposium on Operating systems principles</title>
		<meeting>of the 7th ACM symposium on Operating systems principles</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation of multithreaded processors and thread-switch policies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Squillante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on High Performance Computing (ISHPC &apos;97)</title>
		<meeting>of the International Symposium on High Performance Computing (ISHPC &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="75" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prophet/critic hybrid branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">250</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strategies for achieving improved processor throughput</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA-18</title>
		<meeting>of ISCA-18</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to Intel Core Duo processor architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gochman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multithreaded processor designed for distributed shared memory systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gruenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ungerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1997 Advances in Parallel and Distributed Computing Conference (APDC &apos;97)</title>
		<meeting>of the 1997 Advances in Parallel and Distributed Computing Conference (APDC &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">206</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparative evaluation of latency reducing and tolerating techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="263" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intel&apos;s P6 uses decoupled superscalar design</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessor Report</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inexpensive throughput enhancement in small-scale embedded microprocessors with block multithreading: Extensions characterization, and tradeoffs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th IEEE International Performance, Computing, and Communications Conference</title>
		<meeting>of the 20th IEEE International Performance, Computing, and Communications Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ros ?u, and M.-C. Ros ?u. CPU reservations and time constraints: efficient, predictable scheduling of independent activities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="198" to="211" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Cell processor architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kahle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-38</title>
		<meeting>of MICRO-38</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">IBM Power5 chip: A dual-core multithreaded processor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-37</title>
		<meeting>of MICRO-37</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fair cache sharing and partitioning in a chip multiprocessor architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT-13</title>
		<meeting>of PACT-13</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Niagara: A 32way multithreaded Sparc processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kongetira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aingaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Krewell</surname></persName>
		</author>
		<title level="m">AMD vs. Intel in dual-core duel. Microprocessor Report</title>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intel looks to core for success</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heterogeneous chip multiprocessors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Balancing throughput and fairness in SMT processors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gummaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>of the International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hyper-threading technology architecture and microarchitecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Montecito: A dual-core, dualthread Itanium processor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcnairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Software-controlled multithreading using informing memory operations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramkissoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th International Symposium on High-Performance Computer Architecture (HPCA &apos;00)</title>
		<meeting>of the 6th International Symposium on High-Performance Computer Architecture (HPCA &apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA-9</title>
		<meeting>of HPCA-9</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">129</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Applications of thread prioritization in SMT processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Raasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Multithreaded Execution And Compilation</title>
		<meeting>of the Workshop on Multithreaded Execution And Compilation</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The impact of resource partitioning on SMT processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Raasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT-12</title>
		<meeting>of PACT-12</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-core to the masses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rattner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of PACT-14</title>
		<meeting>of PACT-14</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient fair queueing using deficit round-robin</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shreedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="385" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Performance analysis and validation of the Intel Pentium4 processor on 90nm technology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattwandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling with priorities for a simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems</title>
		<meeting>of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Chip multithreading: opportunities and challenges</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HPCA-11</title>
		<meeting>of HPCA-11</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="248" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An instruction fetch policy handling l2 cache misses in smt processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th International Conference on High-Performance Computing in Asia-Pacific Region</title>
		<meeting>of the 8th International Conference on High-Performance Computing in Asia-Pacific Region</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="519" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sinharoy. POWER4 system microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="26" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The effectiveness of multiple hardware contexts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thekkath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="328" to="337" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Handling long-latency loads in a simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-34</title>
		<meeting>of MICRO-34</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA-23</title>
		<meeting>of ISCA-23</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: maximizing on-chip parallelism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;98: 25 years of the international symposia on Computer architecture</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="533" to="544" />
		</imprint>
	</monogr>
	<note>selected papers</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Balanced multithreading: Increasing throughput via a low cost multithreading hierarchy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICRO-37</title>
		<meeting>of MICRO-37</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey of processors with explicit multithreading</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ungerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="63" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
