<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimized Assistive Human-Robot Interaction Using Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hamidreza</forename><surname>Modares</surname></persName>
							<email>modares@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Arlington Research Institute</orgName>
								<address>
									<addrLine>Fort Worth</addrLine>
									<postCode>76118-7115</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Isura</forename><surname>Ranatunga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Arlington Research Institute</orgName>
								<address>
									<addrLine>Fort Worth</addrLine>
									<postCode>76118-7115</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Frank</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Arlington Research Institute</orgName>
								<address>
									<addrLine>Fort Worth</addrLine>
									<postCode>76118-7115</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Dan</forename><forename type="middle">O</forename><surname>Popa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Arlington Research Institute</orgName>
								<address>
									<addrLine>Fort Worth</addrLine>
									<postCode>76118-7115</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimized Assistive Human-Robot Interaction Using Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BBB37E6409C18B978E43D3AF1AF57CEA</idno>
					<idno type="DOI">10.1109/TCYB.2015.2412554</idno>
					<note type="submission">received December 11, 2014; accepted March 6, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive impedance control</term>
					<term>human-robot interaction (HRI)</term>
					<term>neuro-adaptive control</term>
					<term>reinforcement learning (RL)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An intelligent human-robot interaction (HRI) system with adjustable robot behavior is presented. The proposed HRI system assists the human operator to perform a given task with minimum workload demands and optimizes the overall human-robot system performance. Motivated by human factor studies, the presented control structure consists of two control loops. First, a robot-specific neuro-adaptive controller is designed in the inner loop to make the unknown nonlinear robot behave like a prescribed robot impedance model as perceived by a human operator. In contrast to existing neural network and adaptive impedance-based control methods, no information of the task performance or the prescribed robot impedance model parameters is required in the inner loop. Then, a task-specific outer-loop controller is designed to find the optimal parameters of the prescribed robot impedance model to adjust the robot's dynamics to the operator skills and minimize the tracking error. The outer loop includes the human operator, the robot, and the task performance details. The problem of finding the optimal parameters of the prescribed robot impedance model is transformed into a linear quadratic regulator (LQR) problem which minimizes the human effort and optimizes the closed-loop behavior of the HRI system for a given task. To obviate the requirement of the knowledge of the human model, integral reinforcement learning is used to solve the given LQR problem. Simulation results on an x-y table and a robot arm, and experimental implementation results on a PR2 robot confirm the suitability of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I NDUSTRIAL robots have been successfully used to per- form repetitive tasks with a high precision. However, there are tasks that are less structured and too complex to fully automate and thus cannot be totally performed by robots. Moreover, evaluation of the performance and fine tuning by the human are sometimes necessary, which makes it impossible to fully replace humans with robots. Therefore, human-robot interaction (HRI) systems are developed in industry to take advantages of the abilities of both humans and robots. In fact, humans and robots have complementary advantages and skills. Humans' strength lies in their abilities in reasoning, thinking, and acting when faced with unforeseen events, while robots are able to work in extremely risky environments with guaranteed performance. Other than industry, the HRI systems have also been employed in many other areas such as cooperative manipulation tasks <ref type="bibr" target="#b0">[1]</ref>, surgery <ref type="bibr" target="#b1">[2]</ref>, robot-assisted rehabilitation therapies <ref type="bibr" target="#b2">[3]</ref>, and elsewhere to assist the humans in their daily life.</p><p>Unlike ordinary industrial robotics where the environment is structured and known to them, in HRI systems, the robots interact with humans who may potentially have very different skills and capabilities. Therefore, it is desired to develop human-robot systems that are capable of adapting themselves to the level of the skill of the human operator to assist the human operator to accomplish a given task with minimum workload demands and to achieve a perfect closed-loop behavior of the human-robot system. This requires the design of an intelligent adaptive controller for the HRI system.</p><p>Adaptive robot controllers have been widely used to provide highly effective controllers in yielding guaranteed position control for industrial robot manipulators <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b8">[9]</ref>. However, when the robot manipulator is in contact with an object or a human, it must be able to control not only positions, but also forces. Impedance control <ref type="bibr" target="#b9">[10]</ref> provides an effective method for the control of both position and force simultaneously. Various impedance control methodologies have been developed in the literature to make a robot follow a desired trajectory while operating in physical contact with objects. Adaptive impedance control techniques using neural networks (NNs) have also been developed to tune the impedance model based on various considerations <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>All these mentioned adaptive control methods and adaptive impedance control methods are based on tracking error dynamics, and/or making the error dynamics have a prescribed impedance characteristic. The objective of trajectory following with an error dynamics having prescribed impedance properties often restricts the applications of these approaches in HRI systems. For modern interactive HRI systems to be capable of performing a wide range of tasks successfully, it is required to include the effects of both robot and human dynamics. Human performance neuropsychological and human factors studies have shown that in coordinated motion with a robot, human 2168-2267 c 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. IEEE TRANSACTIONS ON CYBERNETICS learning has two components <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. The operator learns a robot-specific inverse dynamics model to compensate for the nonlinearities of the robot <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and simultaneously he learns a feedback control component that is specific to the successful performance of the task. These foundations can be incorporated in the design of the human-robot control system to include the effects of both robot and human dynamics, and their interactions in a task-specific outer control loop.</p><p>Recently, various adaptive impedance methods have been developed to improve the performance and safety of the HRI system. Adjustment of the impedance parameters based on various considerations such as the stability <ref type="bibr" target="#b21">[22]</ref>, the input torque <ref type="bibr" target="#b22">[23]</ref>, minimizing a cost function <ref type="bibr" target="#b23">[24]</ref>, and incorporating the human intention <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref> were considered. These methods, however, have not taken into account the skill differences of different operators. To overcome this problem, an approach, namely human adaptive mechatronics, is presented in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>. This method takes into account the skill differences of the operators by adjusting the impedance of the robot according to the identified operator's model dynamics.</p><p>Identifying the operator's model dynamics has been studied by several researchers <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The impedance parameters in human adaptive mechatronics approach are tuned based on a Lyapunov function to assure stability. But, stability is a bare minimum requirement for a controlled system, and it is desired to tune the impedance parameters to optimize the long-term performance of the system. References <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> performed the adjustment of the impedance based on estimation of the operator's impedance parameters. These techniques did not concern to optimize the overall long-term performance of the human-robot systems. Li et al. <ref type="bibr" target="#b34">[35]</ref> and Wang et al. <ref type="bibr" target="#b35">[36]</ref> developed an adaptive impedance method for HRI systems to find the optimal parameters of the robot impedance model.</p><p>In this paper, a novel approach is presented to develop an intelligent HRI system with adjustable robot behavior that assists the human operator to perform a given task using the minimum effort and achieves an optimal performance. In accordance with human factors studies, the proposed method has two control loops. A robot-specific inner loop is designed to make the robot with unknown dynamics behave like a simple prescribed robot impedance model as perceived by a human operator. Next, a task-specific outer loop is developed to find the optimal parameters of the prescribed robot impedance model. In the outer loop, the problem of finding the optimal parameters of the prescribed robot impedance model is formulated as a linear quadratic regulator (LQR) control problem <ref type="bibr" target="#b36">[37]</ref> such that both tracking errors and human operator effort are minimized. Reinforcement learning (RL) <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b41">[42]</ref> is used to solve the given LQR problem to obviate the requirement of the knowledge of the human model.</p><p>The contributions of this paper are as follows.</p><p>1) An inner-loop controller is designed to make the nonlinear unknown robot dynamics behave like a prescribed robot impedance model. This is more general than standard trajectory following. The proposed inner-loop controller does not require either task information or the specific prescribed robot impedance model parameters. This enables us to decouple the design of the robot-specific inner loop from the design of the task-specific outer-loop controller.</p><p>2) The problem of designing the optimal parameters of the prescribed robot impedance model is transformed into a LQR problem in a task-specific outer-loop control design. These parameters are determined by minimizing a performance function in terms of the human control effort and the tracking error. 3) A RL technique is employed to solve the task-specific LQR problem online in real time. 4) The proposed approach does not restrict the robot to a trajectory following task, because it leaves the task-specific details to the design of the outer loop which incorporates the human operator. The rest of this paper is organized as follows. The next section presents the overall structure of the proposed control design method for the HRI systems. Both inner-and outer-loop control designs are briefly discussed. Sections III and IV discuss the inner-and outer-loop designs, respectively, in detail. Section V presents the simulation results, Section VI shows the experimental implementation results, and finally Section VII concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HRI CONTROL STRUCTURE OVERVIEW</head><p>In this section, the importance of designing an assistive HRI system is first discussed and then, the structure of the proposed assistive HRI control system developed in this paper is overviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Assistive HRI</head><p>Robots interacting with humans should be able to assist the human by adjusting themselves to the level of the skills of the human and compensating for possible human mistakes due to fatigue, stress, etc. This increases the performance of the overall HRI system in terms of safety, human force, and precision. In simple mechanical systems, for instance, a skillful operator is able to handle faster movements and thus achieve better performance using a small damping coefficient. Therefore, the damping coefficient should be tuned so that the response of the system is adapted to the level of the skill of the human operator.</p><p>In order to design such an intelligent HRI system, one major issue is that of how to best design a learning-based controller for the HRI system so that the assistance and safety are provided to the human using minimum knowledge about the human and robot dynamics, while compromise on performance is avoided. To design such an optimized assistive HRI system, the control structure proposed here decouples the robot-specific design from the task-specific design. The robot-specific inner-loop control makes the robot behave like a prescribed impedance model as perceived by the human. No task information is required in this step. In a task-specific outer loop, that includes the human operator and task information, the optimal prescribed impedance model is found. The overview of proposed method is detailed in the next section. Note that, there are many types of tasks in HRI systems. In this paper, we focus on trajectory following tasks. The desired trajectory in which the HRI system must follow is sometimes known a priori. For example, in robotic therapy, the patient is guided along a predetermined desired trajectory path. Pointto-point tasks with the goal of reaching a desired point in the workspace are also popular in human-in-the-loop systems. In these types of tasks, the desired point is determined a priori. In some applications of HRI systems, the desired trajectory is not given a priori and it is adjusted or planned online using various techniques such as estimation of human motion intention <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>. Although we focus on the trajectory following tasks in this paper, the proposed methodology can be extended to more general tasks. This is because no trajectory information is required in the inner loop and this leaves the freedom to incorporate task information in an outer-loop task-specific design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Structure of the Proposed Assistive HRI System</head><p>The HRI design here is motivated by the human factors studies <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> which states that the human learns a robot-specific inverse dynamics model to compensate for the nonlinearities of the robot, and simultaneously a feedback control component that is specific to the successful performance of the task. First, a robot torque controller is provided to avoid the need for the operator to learn a robot-specific model. Second, assistive inputs are provided to augment the operator's control effort so that the operator performs a given task with minimum workload demands and maximum performance.</p><p>To achieve these goals, the proposed method has two control loops. The first loop is a robot-specific inner loop which does not require any information of the task (see Fig. <ref type="figure" target="#fig_0">1</ref>). The second loop is a task-specific outer loop which includes the human operator dynamics, the robot, and the task performance details (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>The robot-specific inner-loop controller is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The objective is to make the unknown robot manipulator dynamics behave like a prescribed robot impedance model as perceived by a human operator. Therefore, the human only needs to interact with the simplified impedance model. To compensate for the unknown robot nonlinearities, an adaptive NN controller is employed. This is not the same as the bulk of the work in robot impedance control and NN control, which is directed toward making a robot follow a prescribed trajectory, and/or causing the trajectory error dynamics to follow a prescribed impedance model. No trajectory information is needed for the inner-loop design. This leaves the  freedom to incorporate task information in an outer-loop design. The robot-specific inner-loop controller is shown in Fig. <ref type="figure" target="#fig_0">1</ref> called model reference neuro-adaptive control. This is because an adaptive NN controller is developed to make the robot dynamics, from the human force to the robot motion, behave like the prescribed reference impedance model.</p><p>The task-specific outer-loop controller is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Given the robot-specific inner-loop design, the optimal parameters of the prescribed robot impedance model are determined in this task-specific outer loop to guarantee motion tracking and also assist the human to perform the task with minimum effort. This design must take into account the unknown human dynamics as well as the desired overall performance of the human-robot system, which depends on the task. Fig. <ref type="figure" target="#fig_2">3</ref> shows the overall schematic of the proposed two-loop control design method for the HRI system. The details of the inner-and outer-loop designs are given in Sections III and IV, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ROBOT-SPECIFIC INNER-LOOP CONTROL DESIGN: MODEL REFERENCE NEURO-ADAPTIVE CONTROLLER</head><p>In this section, the design of the inner-loop controller is given in Fig. <ref type="figure" target="#fig_0">1</ref>. The aim of the inner-loop controller is to make the robot manipulator behave like a prescribed robot impedance model. The proposed inner-loop control method has two main differences from the existing adaptive impedance control methods. First, in contrast to trajectory following based methods, the proposed method is a robot-specific method that does not require a reference motion trajectory. That is, the proposed method minimizes the model-following error between the output of the prescribed robot impedance model and the motion of the robot without needing task information. Second, in the proposed method, the designed control torque does not require any knowledge of the prescribed robot impedance model. This enables us to decouple the design of the robot-specific inner-loop controller from the design of the task-specific outer-loop control design.</p><p>Consider the dynamical model of robot manipulator in Cartesian space <ref type="bibr" target="#b45">[46]</ref> M(q) ẍ + C(q, q) ẋ + F c (q) + G(q) + τ d = τ + K h f h <ref type="bibr" target="#b0">(1)</ref> with</p><formula xml:id="formula_0">M = J -T M * J -1 , C = J -T (C * -M * J -1 J) J -1 , M = J -T M * J -1 , F c = J -T F * , G = J -T G *</formula><p>, and τ = J -T τ * , where q ∈ R n is the vector of generalized joint coordinates, n is the number of joints, x ∈ R n is the end-effector Cartesian position, the control input force is τ = J -T τ * with τ * is the vector of generalized torques acting at the joints, M * ∈ R n×n is the symmetric positive definite mass (inertia) matrix, C * (q, q) q ∈ R n×1 is the vector of Coriolis and centripetal forces, F * c (q) ∈ R n×1 is the Coulomb friction term, G * (q) ∈ R n×1 is the vector of gravitational torques, τ d is a general nonlinear disturbance, f h is the human control effort, K h is a gain, and J is the Jacobian matrix.</p><p>It is assumed that the robot manipulator dynamics in (1) are unknown.</p><p>Remark 1: Note that in the dynamics (1), it is assumed that the human force is sensed by a force sensor and is amplified by a gain K h before applying to the robot. For example, for the x-y table example in <ref type="bibr" target="#b17">[18]</ref>, the force generated by the hand to the grip is measured by a force sensor and is magnified before it is applied to the stage. As shown in Section V, this gain is employed to help the human to minimize the tracking error and maximize the performance. It is shown in Remark 5 that if the amplification of the human force is not possible for a specific application, the proposed method can still be used.</p><p>Consider the prescribed robot impedance model</p><formula xml:id="formula_1">M ẍm + B ẋm + K x m = K h f h + l(x d ) ≡ l( f h , x d ) (2)</formula><p>in Cartesian space, where x m is the output of the prescribed robot impedance model, M, B, and K are the desired inertia, damping, and stiffness parameter matrices, respectively. These parameters are specified in the task-specific outer control loop design in Section IV. The auxiliary input l(x d ) is a trajectory dependent input and is also designed in Section IV. Design Objective: The aim is to design the force τ in (1) to make the unknown robot dynamics (1) from the human force f h to the Cartesian coordinates x behave like the prescribed robot impedance model <ref type="bibr" target="#b1">(2)</ref>. That is, it is desired to make the following model-following error go to zero: e = x mx.</p><p>(3)</p><p>Note that, this is not a trajectory-following error. Therefore, this is a model-following design, and not a trajectory-following design, in contrast to most work on robot torque control <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>. No task information is required in this section. All task-specific details are taken into account in the next section.</p><p>It is now required to design a control torque τ to make the robot behave like the prescribed robot impedance model <ref type="bibr" target="#b1">(2)</ref>.</p><p>Consider the control torque</p><formula xml:id="formula_2">τ = ŴT φ VT z + K v r -v(t) -K h f (4)</formula><p>where v(t) is a robustifying signal to be specified, K v is the control gain, and</p><formula xml:id="formula_3">r = ė + 1 e + 2 ε (5)</formula><p>is the sliding mode error with</p><formula xml:id="formula_4">ε = t 0 e(τ ) dτ. (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>Finally</p><formula xml:id="formula_6">ĥ(z) = ŴT φ VT z (7)</formula><p>is a NN with z = [q, q, ẋm , ẍm , e, ė, ε] T the input to the NN, Ŵ and V the NN weights, and φ(z) the vector of activation functions. As is shown in the proof of Theorem 1, the NN controller in ( <ref type="formula">4</ref>) is used to compensate for the unknown robot function h defined as</p><formula xml:id="formula_7">h(z) = M(q)(ẍ m + 1 ė + 2 e) + C(q, q)(ẋ m + 1 e + 2 ε) + F c (q) + G(q). (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>The NN universal approximation property specifies that any unknown continuous function can be approximated on a compact set using a two-layer NN to any arbitrary precision <ref type="bibr" target="#b45">[46]</ref>. That is, for the continuous function h(z) on a compact set z ∈ , one has</p><formula xml:id="formula_9">h(z) = W T φ V T z + ε(z) (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where V is a matrix of first-layer weights, W is a matrix of second-layer weights, and ε is the NN functional approximation error. The ideal weight vectors W and V are unknown and is approximated online. Therefore, h(z) is approximated as <ref type="bibr" target="#b6">(7)</ref> with Ŵ and V the estimations of W and V, respectively. Define</p><formula xml:id="formula_11">Z = W 0 0 V (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>and Ẑ equivalently. Assumption 1: The ideal NN weights are bounded by a constant scalar so that</p><formula xml:id="formula_13">Z ≤ Z B . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The following theorem shows that the proposed control input τ given by (4) guarantees the boundedness of the model-following error e and the NN weights.</p><p>Theorem 1: Consider the robot manipulator dynamics (1) and the prescribed robot impedance model <ref type="bibr" target="#b1">(2)</ref>. Let the control input be chosen as <ref type="bibr" target="#b3">(4)</ref>. Let Assumption 1 hold. Let the update rule for the NN weights be given by</p><formula xml:id="formula_15">Ẇ = F φ r T -F φ VT z r T -k F r Ŵ (12) V = G z φ Ŵr T r T -k G r V (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>where φ = φ( VT z), φ = dφ(y)/dy| y= VT z , F = F T &gt; 0, G = G T &gt; 0, and k &gt; 0 is a small design parameter. Let the robustifying term be</p><formula xml:id="formula_17">v(t) = -K z Ẑ + Z B (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where K z &gt; 0. Then, e(t) in (3) and the NN estimated weights are uniformly ultimately bounded.</p><p>Proof: By differentiating (3) with respect to time one has ė = ẋm -ẋ, or equivalently ẋ = ẋm -ė. Differentiating ẋ gives ẍ = ẍm -ë. Considering the sliding mode tracking error r defined in <ref type="bibr" target="#b4">(5)</ref>, one has ė = r -1 e -2 ε. Differentiating ė gives ë = ṙ -1 ė -2 e. Using these expressions in (1) yields</p><formula xml:id="formula_19">M(q)(ẍ m -(ṙ -1 ė -2 e)) + C(q, q)(ẋ m -(r -1 e -2 ε)) + F c (q) + G(q) + τ d = τ + K h f h . (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>This gives the sliding mode error dynamics M(q)ṙ = -C(q, q)r + h(q, q, ẋm , ẍm , e, ė, ε)</p><formula xml:id="formula_21">+ τ d -τ -K h f h (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>with h defined in <ref type="bibr" target="#b7">(8)</ref>. The robot manipulator dynamics ( <ref type="formula">1</ref>) is assumed to be unknown and therefore h in ( <ref type="formula" target="#formula_21">16</ref>) is unknown and approximated online by <ref type="bibr" target="#b6">(7)</ref>. Then, the closed-loop filtered error dynamics <ref type="bibr" target="#b15">(16)</ref> becomes</p><formula xml:id="formula_23">M(q)ṙ = -C(q, q)r + ŴT φ VT z + τ d -τ -K h f h + h<label>(17)</label></formula><p>where h = h-ĥ is the estimation error. Substituting τ from (4) in <ref type="bibr" target="#b16">(17)</ref> gives</p><formula xml:id="formula_24">M(q)ṙ = -C(q, q)r -K v r + τ d + h + v(t). (<label>18</label></formula><formula xml:id="formula_25">)</formula><p>The remainder of the proof is the same as <ref type="bibr" target="#b45">[46]</ref> and thus is only outlined here. A Lyapunov function is defined as</p><formula xml:id="formula_26">L = 1 2 r T M(q)r + tr WT F -1 W + tr ṼT F -1 Ṽ<label>(19)</label></formula><p>where the weight estimation errors are W = W -Ŵ and Ṽ = V -V, and it is shown using ( <ref type="formula">12</ref>)-( <ref type="formula" target="#formula_17">14</ref>) and ( <ref type="formula" target="#formula_23">17</ref>) that the Lyapunov function derivative is negative outside a compact set. This guarantees the boundedness of the filtered tracking error r as well as the NN weights. Specific bounds on r and the NN weights are given in <ref type="bibr" target="#b45">[46]</ref>. Note that the proposed controller (4) is composed of four parts. The first part is a nonlinear compensator consisting of a NN controller to compensate for unknown function h defined in <ref type="bibr" target="#b7">(8)</ref>. The second part of the controller is a stabilizing Proportional Integral Derivative controller that stabilizes the model-following error e. The third part is a robust term that is designed to achieve robustness against uncertainties. Finally, the last part is used to compensate for the human input K h f h . Fig. <ref type="figure" target="#fig_3">4</ref> shows the detailed schematic of the proposed inner-loop controller. We call this model reference neuro-adaptive control because the NN adaptive controller causes the robot dynamics to behave like the prescribed impedance model <ref type="bibr" target="#b1">(2)</ref>. This is in contrast to NN torque control work, which seek to make the robot motion x(t) follow a prescribed trajectory.</p><p>Remark 2: Function h(z) in ( <ref type="formula" target="#formula_7">8</ref>) does not contain the parameters M, D, and K of the prescribed robot impedance model ( <ref type="formula">2</ref>). This means that the NN does not need to estimate the impedance model. This is in contrast to methods that design the torque τ in (1) to guarantee following a desired trajectory, and demand that the trajectory tracking error dynamics follow a prescribed impedance model <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Our design of the robot-specific inner-loop controller is independent from any task objectives. All task-specific information is considered in the outer-loop design in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TASK-SPECIFIC OUTER-LOOP ADAPTIVE IMPEDANCE CONTROL: FINDING OPTIMAL PARAMETERS OF THE PRESCRIBED ROBOT IMPEDANCE MODEL</head><p>In this section, the design of the outer task loop controller is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It was shown in Section III that the robot-specific controller of Theorem 1 makes the nonlinear unknown robot (1) behave like the simple prescribed robot impedance model (2) as perceived by the human operator. In this section, the parameters of the prescribed robot impedance model given in (2) are optimized to assist the human to perform a given task with minimum effort and to minimize a tracking error. To this end, the problem of optimizing the parameters of the prescribed robot impedance model is transformed into a LQR problem and then RL is used to solve the given problem without requiring the human dynamics model.</p><p>Design Objective: The aim of the task-specific outer-loop controller is to find the optimal values of the prescribed impedance parameters B, K, the human gain K h (or M if K h = 1), and the auxiliary input l(x d ) in (2) to minimize the human control effort f h and optimize the tracking performance depending on the task.</p><p>The dynamics of the human model and the interaction of the robot and human are considered in this outer-loop control design. It was shown in <ref type="bibr" target="#b25">[26]</ref> that the human dynamics change during the task learning process. After learning, an expert human operator is characterized by a simple linear transfer characteristic. Therefore, the human impedance model is assumed to be</p><formula xml:id="formula_27">(K d s + K p ) f h = k e e d (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>where K d , K p , and k e are unknown gains. These parameters vary from one individual to another and depend on the specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task-Specific Outer-Loop Control Method: LQR Approach</head><p>The block diagram of the outer-loop task controller is sketched in Fig. <ref type="figure" target="#fig_1">2</ref> and shown in detail in Fig. <ref type="figure" target="#fig_4">5</ref>. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, in addition to the adaptive impedance loop that specifies the optimal impedance parameters, an assistive feedforward input and a human force gain are employed to help the human to minimize the tracking error. The feedforward term l(x d ) in ( <ref type="formula">2</ref>) is designed to make the steady-state tracking error go to zero. The human gain K h and the optimal values of the prescribed impedance parameters K and B in (2) are determined to minimize the human effort and the tracking error for a given task.</p><p>In the following, it is shown how the problem of finding optimal values of B, K, and K h is transformed into a LQR problem, and how these parameters are obtained by solving an algebraic Riccati equation (ARE).</p><p>Define the tracking error</p><formula xml:id="formula_29">e d = x d -x m ∈ R n (<label>21</label></formula><formula xml:id="formula_30">)</formula><p>and</p><formula xml:id="formula_31">ēd = e T d ėT d T = xd -x ∈ R 2n (22) with x = x T m ẋT m T ∈ R 2n<label>(23)</label></formula><p>and</p><formula xml:id="formula_32">xd = x T d ẋT d T ∈ R 2n . (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>Based on this tracking error, define the performance index</p><formula xml:id="formula_34">J = ∞ t ēT d Q d ēd + f T h Q h f h + u T e R u e dτ (<label>25</label></formula><formula xml:id="formula_35">)</formula><p>where</p><formula xml:id="formula_36">Q d = Q T d &gt; 0, Q h = Q T h &gt; 0, R = R T &gt; 0,</formula><p>and u e is the feedback control input which depends linearly on the tracking error ēd and the human effort f h . Then</p><formula xml:id="formula_37">u e = K 1 ēd + K 2 f h . (<label>26</label></formula><formula xml:id="formula_38">)</formula><p>It is shown in Theorem 2 that the control input ( <ref type="formula" target="#formula_37">26</ref>) has two components. The first component, i.e., K 1 tunes the prescribed impedance parameters B and K and the second component, i.e., K 2 tunes the human control gain K h (or M if K h = 1).</p><p>Remark 3: Note that by minimizing the performance index <ref type="bibr" target="#b24">(25)</ref>, both tracking error ēd and human effort f h are minimized.</p><p>By defining the augmented state</p><formula xml:id="formula_39">X = ēd f h ∈ R 3n (<label>27</label></formula><formula xml:id="formula_40">)</formula><p>the performance index ( <ref type="formula" target="#formula_34">25</ref>) can be written as</p><formula xml:id="formula_41">J = ∞ t X T Q X + u T e R u e dτ (<label>28</label></formula><formula xml:id="formula_42">)</formula><p>where</p><formula xml:id="formula_43">Q = diag(Q d , Q h ) and u e = K X with K = [ K 1 K 2 ].</formula><p>The dynamics of the system with augmented state ( <ref type="formula" target="#formula_39">27</ref>) are now given. Using ( <ref type="formula">2</ref>), one has</p><formula xml:id="formula_44">ẋ = 0 I n×n 0 0 x + 0 I n×n u ≡ A q x + B q u (<label>29</label></formula><formula xml:id="formula_45">)</formula><p>where x is defined in <ref type="bibr" target="#b22">(23)</ref>, and</p><formula xml:id="formula_46">u = M-1 -K q x + K h f h + M-1 l(x d ) (<label>30</label></formula><formula xml:id="formula_47">)</formula><p>with</p><formula xml:id="formula_48">K q = K B (<label>31</label></formula><formula xml:id="formula_49">)</formula><p>where K q ∈ R n×2n , B, K, and M are the prescribed impedance model in <ref type="bibr" target="#b1">(2)</ref>. On the other hand, based on the human model ( <ref type="formula" target="#formula_27">20</ref>), we have</p><formula xml:id="formula_50">K d s + K p f = k e e d (<label>32</label></formula><formula xml:id="formula_51">)</formula><p>which can be written in time domain as</p><formula xml:id="formula_52">K d ḟh + K p f h = k e e d (<label>33</label></formula><formula xml:id="formula_53">)</formula><p>or equivalently</p><formula xml:id="formula_54">ḟh = -K -1 d K p f h + k e K d,0 ēd ≡ A h f h + E h ēd<label>(34)</label></formula><p>where</p><formula xml:id="formula_55">K d,0 = [K -1 d 0] ∈ R n×2n</formula><p>and ēd is defined in <ref type="bibr" target="#b21">(22)</ref>. The following theorem shows how the problem of finding the optimal parameters of the prescribed impedance model and the human gain are obtained by solving a LQR problem.</p><p>Remark 4: Note that in <ref type="bibr" target="#b17">[18]</ref>, the human transfer function from e d to f h was considered as</p><formula xml:id="formula_56">G(s) = K d s + K p Ts + 1 . (<label>35</label></formula><formula xml:id="formula_57">)</formula><p>For this case, A h and B h in <ref type="bibr" target="#b33">(34)</ref> become</p><formula xml:id="formula_58">A h = -T -1 and E h = T -1 [K p K d ].</formula><p>Theorem 2: Consider the prescribed robot impedance model <ref type="bibr" target="#b1">(2)</ref>. Based on dynamics in ( <ref type="formula" target="#formula_44">29</ref>) and <ref type="bibr" target="#b33">(34)</ref>, define augmented matrices A and B by</p><formula xml:id="formula_59">A = A q 0 E h A h , B = B q 0 . (<label>36</label></formula><formula xml:id="formula_60">)</formula><p>Define</p><formula xml:id="formula_61">K = K q K h ∈ R n×3n (<label>37</label></formula><formula xml:id="formula_62">)</formula><p>as the matrix of the impedance parameters and the human gain. Then, the optimal value of K which minimizes the performance index ( <ref type="formula" target="#formula_34">25</ref>) is given by</p><formula xml:id="formula_63">K = -M R -1 B T P (<label>38</label></formula><formula xml:id="formula_64">)</formula><p>where P is the solution to the ARE</p><formula xml:id="formula_65">0 = A T P + PA + P BR -1 B T P + Q. (<label>39</label></formula><formula xml:id="formula_66">)</formula><p>Then, the optimal feedback control is given by</p><formula xml:id="formula_67">u e = M-1 K e d + M-1 B ėd + M-1 K h f h . (<label>40</label></formula><formula xml:id="formula_68">)</formula><p>Proof: Manipulating <ref type="bibr" target="#b29">(30)</ref> gives</p><formula xml:id="formula_69">u = M-1 K q ēd + K h f h + M -1 l(x d ) -K q xd ≡ u e + u d (<label>41</label></formula><formula xml:id="formula_70">)</formula><p>where ēd and xd are defined in ( <ref type="formula">22</ref>) and ( <ref type="formula" target="#formula_32">24</ref>), and</p><formula xml:id="formula_71">u e = M-1 K q ēd + K h f h (<label>42</label></formula><formula xml:id="formula_72">)</formula><p>is a feedback control input, and</p><formula xml:id="formula_73">u d = M -1 l(x d ) -K q xd (<label>43</label></formula><formula xml:id="formula_74">)</formula><p>is a feedforward control input. The steady state or the feedforward term is used to guarantee perfect tracking. That is, in the steady state one has</p><formula xml:id="formula_75">ẋd = A q xd + B q u d (<label>44</label></formula><formula xml:id="formula_76">)</formula><p>where xd is defined in <ref type="bibr" target="#b23">(24)</ref>. Therefore</p><formula xml:id="formula_77">l(x d ) = M u d + K q xd = M B -1 q ẋd -A q xd + K q xd . (<label>45</label></formula><formula xml:id="formula_78">)</formula><p>Taking derivative of ēd and using ( <ref type="formula" target="#formula_44">29</ref>) and <ref type="bibr" target="#b43">(44)</ref>, and some manipulations gives</p><formula xml:id="formula_79">ėd = A q ēd + B q u e . (<label>46</label></formula><formula xml:id="formula_80">)</formula><p>Using the augmented state <ref type="bibr" target="#b26">(27)</ref>, and using <ref type="bibr" target="#b33">(34)</ref> and ( <ref type="formula" target="#formula_79">46</ref>) one has</p><formula xml:id="formula_81">Ẋ = ėd ḟh = A q 0 E h A h ēd f h + B q 0 u e ≡ AX + Bu e . (<label>47</label></formula><formula xml:id="formula_82">)</formula><p>The control input u e in terms of the augmented state can be written as</p><formula xml:id="formula_83">u e = M-1 K q ēd + K h f h = M-1 K X. (<label>48</label></formula><formula xml:id="formula_84">)</formula><p>Finding the optimal feedback control (48) to minimize the performance index <ref type="bibr" target="#b24">(25)</ref> subject to the augmented system (47) is a LQR problem and its solution is given by <ref type="bibr" target="#b36">[37]</ref> u</p><formula xml:id="formula_85">* e = -R -1 B T PX (<label>49</label></formula><formula xml:id="formula_86">)</formula><p>where P is the solution to the Riccati equation <ref type="bibr" target="#b38">(39)</ref>. Equating the right-hand sides of ( <ref type="formula" target="#formula_83">48</ref>) and ( <ref type="formula" target="#formula_85">49</ref>) yields</p><formula xml:id="formula_87">K = K q K h = -MR -1 B T P. (<label>50</label></formula><formula xml:id="formula_88">)</formula><p>This completes the proof. Remark 5: The K vector defined in (37) includes both parameters <ref type="bibr" target="#b30">(31)</ref> of the robot impedance model and the gain K h of the human force. Therefore, the solution to the formulated LQR problem gives the optimal values of the prescribed impedance model parameters and the gain of the human operator force. If the human gain cannot be magnified for a specific HRI application, i.e., if K h = 1, then based on (48) one can set the coefficient of f h in the control input as M-1 and then find M instead of K h . That is, if K h = 1 and M is unknown, then (50) becomes K = [ M-1 K q M-1 ] = -R -1 B T P, which gives unknown parameters of the impedance model <ref type="bibr" target="#b1">(2)</ref>.</p><p>Remark 6: The outer-loop control design consists of two components: 1) an adaptive impedance component which finds the optimal values of the parameters <ref type="bibr" target="#b30">(31)</ref> of the prescribed impedance model and 2) an assistive component including the human force gain K h and the feedforward term l(x d ) to help the human to minimize the tracking error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Optimal Parameters of the Prescribed Impedance Model Using Integral Reinforcement Learning</head><p>Solving <ref type="bibr" target="#b38">(39)</ref> requires the knowledge of the matrix A in <ref type="bibr" target="#b35">(36)</ref> and consequently the knowledge of the human model. Several model-free RL algorithms have been developed to solve the optimal control of linear systems without requiring any knowledge of the system dynamics [47]- <ref type="bibr" target="#b51">[52]</ref>. In this paper, the off-policy integral RL (IRL) algorithm <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b51">[52]</ref> is used to solve the given LQR problem. The IRL is an iterative policy iteration algorithm for solving (39) that consists of two iteration steps: 1) policy evaluation and 2) policy improvement. In the policy evaluation step, the value function related to a fixed policy is evaluated using an IRL Bellman equation [see <ref type="bibr" target="#b51">(52)</ref>] which does not involve the system dynamics. In the policy improvement step, an improved policy is found using the value obtained in the policy evaluation step.</p><p>To ensure sufficient exploration of the state space, which is crucial for a proper convergence to the optimal value function, a small exploratory probing noise consisting of sinusoids of varying frequencies is added to the control input to satisfy persistently exciting (PE) qualitatively <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Consider the system (47) explored by a known time-varying probing signal e τ</p><formula xml:id="formula_89">Ẋ = AX + B [u e + e τ ].<label>(51)</label></formula><p>The IRL Bellman equation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> uses only the information given by measuring the system state and an integral of the utility function in finite reinforcement intervals to evaluate a control policy. The IRL Bellman equation for the given LQR problem for the system (51) including probing noise is given, for time interval t &gt; 0, by <ref type="bibr" target="#b51">[52]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X(t) T PX(t)</head><formula xml:id="formula_90">+ t+ t t [2X(t) T PB e τ ] dτ = t+ t t X(t) T Q X(t) + u T e R u e dτ + X(t + t) T P X(t + t). (<label>52</label></formula><formula xml:id="formula_91">)</formula><p>This equation explicitly contains the probing noise and is called an off-policy Bellman equation. Using <ref type="bibr" target="#b51">(52)</ref> for the policy evaluation step and an update law in form of <ref type="bibr" target="#b48">(49)</ref> to IEEE TRANSACTIONS ON CYBERNETICS Algorithm 1 Online IRL Algorithm for Outer-Loop Control Design Initialization: Start with an admissible control input u 0 = K 0 1 X Policy evaluation: Given a control policy u i , find P i using the off-policy Bellman equation</p><formula xml:id="formula_92">X(t) T P i X(t) + t+ t t [2X(t) T P i B e τ ] dτ = t+ t t X(t) T Q X(t) + u T e R u e dτ + X(t + t) T P i X(t + t). (<label>53</label></formula><formula xml:id="formula_93">)</formula><p>Policy improvement: update the control input using</p><formula xml:id="formula_94">u i+1 e = -R -1 B T 1 P i X. (<label>54</label></formula><formula xml:id="formula_95">)</formula><p>find an improved policy, the following exploratory IRL-based algorithm is obtained for solving <ref type="bibr" target="#b38">(39)</ref>. Note that the probing signal e τ in (51) must be applied during learning to assure convergence of Algorithm 1. After convergence, however, the probing noise is no longer required and can be removed.</p><p>Remark 7: Note that for the LQR problem, since the system is linear and the performance function is quadratic, the optimal solution is unique and is found by solving the ARE <ref type="bibr" target="#b38">(39)</ref>. It is shown in <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b51">[52]</ref> that the off-policy IRL Algorithm 1 converges to the global optimal solution found by solving the ARE <ref type="bibr" target="#b38">(39)</ref>, provided that the probing noise is PE. Explicitly including the probing noise in the IRL Bellman equation <ref type="bibr" target="#b51">(52)</ref> means that the algorithm converges with no bias, as shown in <ref type="bibr" target="#b51">[52]</ref>.</p><p>Remark 8: The solution for P i in the policy evaluation step ( <ref type="formula" target="#formula_92">53</ref>) is generally carried out in a least squares (LSs) sense. In fact, ( <ref type="formula" target="#formula_92">53</ref>) is a scalar equation and P i is a symmetric n × n matrix with n(n + 1)/2 independent elements and therefore at least n(n + 1)/2 data sets are required before (53) can be solved using LS. Consequently, the computational complexity of computing P i depends on the size of the system.</p><p>Remark 9: Note that Algorithm 1 solves the ARE (39) and does not require knowledge of the A matrix which contains knowledge of the human dynamics. In fact, the information of A is embedded in the online measurement of system data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION RESULTS</head><p>In this section, the proposed RL-based optimized assistive control and the method of <ref type="bibr" target="#b17">[18]</ref> are applied to an x-y table and their simulation results are compared. Then, the proposed method is simulated on a two-link planar robot arm, as a more complicated example.</p><p>Example 1 (x-y Table ): In order to compare the proposed method to the method presented in <ref type="bibr" target="#b17">[18]</ref>, a simulation is conducted for a haptic interface system consists of a two degreeof-freedom planer xy-stage. It is assumed that the human operator moves a grip attached to the xy-stage and a pointer on a monitor shows the movement of the position of the grip. The x-and y-axis of the stage are mechanically independent Fig. <ref type="figure">6</ref>. Tracking error for the proposed method and the method of <ref type="bibr" target="#b17">[18]</ref>. and each is driven by a drive motor. A point-to-point task is considered by setting a target point in the work space <ref type="bibr" target="#b17">[18]</ref>. The dynamics of the stage is</p><formula xml:id="formula_96">m p ẍp + d p ẋp = τ + k h f h . (<label>55</label></formula><formula xml:id="formula_97">)</formula><p>The subscripts of x and y are omitted in (55) because controllers of the x-and y-axis can be designed separately <ref type="bibr" target="#b17">[18]</ref>. To perform the simulation, similar to <ref type="bibr" target="#b17">[18]</ref>, the human transfer function is considered as <ref type="bibr" target="#b34">(35)</ref> with K p = 779, K d = 288, and T = 0.18. The goal is to move x p to a desired point x d which is assumed to be origin here. That is, x d = 0.</p><p>The method presented in <ref type="bibr" target="#b17">[18]</ref> first identifies the human dynamics and then finds a set of parameters for the stage based on a stability criterion. In the following, to simplify the simulation, we assumed that the human dynamics are known when applying the method of <ref type="bibr" target="#b17">[18]</ref>. It is shown that although the method proposed herein does not require to know or to identify the human dynamics, it gives a better performance.</p><p>Based on (2), consider the prescribed impedance model as</p><formula xml:id="formula_98">m ẍm + d ẋm + k x m = f h . (<label>56</label></formula><formula xml:id="formula_99">)</formula><p>To do a fair comparison, the initial values of the impedance model are chosen as m = 50, d = 50, and k = 0 for both methods. The methods of <ref type="bibr" target="#b17">[18]</ref> and the IRL Algorithm 1 are then performed to tune the impedance parameters and improve the performance. The sampling time is chosen as t = 0.02. At the end of the simulation, the improved impedance parameters are found as m = 25.41, d = 68.83, and k = 0 for the method of <ref type="bibr" target="#b17">[18]</ref>. For Algorithm 1, a number of ten samples are collected to perform a LS in each iteration. After six iterations (i.e., 1.2 s), the optimal impedance parameters are found as m = 3.21, d = 113.91, and k = 44.32. Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref> show the performance of the HRI system in terms of the tracking error and the required human force for three different cases: 1) the HRI system with fixed initial impedance model; 2) the HRI system tuned by the method of <ref type="bibr" target="#b17">[18]</ref>; and 3) the HRI system optimized by Algorithm 1. It is seen after the learning that the proposed method is faster, has a better performance and requires less effort to perform the task. This is because the proposed method found the optimal set of impedance parameters faster. By contrast, although the method of <ref type="bibr" target="#b17">[18]</ref> improves the performance compared to the HRI system with fixed impedance model, it does not converge to an optimal impedance model. These results confirm that the proposed method is faster that the method of <ref type="bibr" target="#b17">[18]</ref> and leads to a better performance. Fig. <ref type="figure">7</ref>. Required human effort for the proposed method and the method of <ref type="bibr" target="#b17">[18]</ref>. Example 2 (Two-Link Planar Robot Arm): The proposed method is now applied to a two-link robot arm. The length of the links are L 1 = 1 m and L 2 = 1 m. The masses of the rigid links are m 1 = 0.8 kg and m 2 = 2.3 kg. The gravitational acceleration is g = 9.8 m/s 2 . In the following, it is first shown how the proposed inner-loop control design method makes the robot behave like a prescribed impedance model regardless of its impedance parameters. Then, it is shown how to find optimal parameters of the prescribed impedance parameters to make the tracking error in an optimal manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inner Loop</head><p>It is shown here how a robot two-link planar robot arm behaves like a given impedance model using the inner-loop control design method presented in Section III. The human force is assumed sinusoidal in both directions in this section. The simulations are performed for two different sets of impedance gains to show that the robot behaves like the impedance model regardless of the impedance model parameters.</p><p>Case 1: The first matrix gains for the impedance model in (2) are chosen as Figs. <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> show that the trajectories of the prescribed impedance model are very close to the trajectories of the robot arm for both cases. These results confirm that the robot   behaves like the impedance model regardless of the impedance parameters. This enables us to tune the impedance parameters in the outer loop to minimize the tracking error and the human workload. Note that since the initial conditions are known, we start both robot and prescribed impedance model from the same initial conditions.</p><formula xml:id="formula_100">M = 1 0 0 1 , B = 5 0 0 5 , K = 2 0 0 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Outer Loop</head><p>The results of the proposed outer-loop controller method are now presented. It is shown that the proposed online Algorithm 1 gives the same set of parameters as the one found by solving the ARE <ref type="bibr" target="#b38">(39)</ref>. The performance of the proposed method is verified on a trajectory tracking task. The mass matrix for the prescribed impedance model is set to the identity of appropriate dimension and it is assumed that the desired trajectory to be followed by the robot is x d = [sin(t), cos(t)]. It is also assumed that the human admittance parameters are K d = 10, K p = 20, and h = 1. Note that, these parameters are generally not constant and may vary from human to human. Then, the solution of the ARE <ref type="bibr" target="#b38">(39)</ref> and consequently the control gain <ref type="bibr" target="#b37">(38)</ref> are given as   The proposed controller is now implemented to this HRI system. Figs. <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> show the results of this experiment. Fig. <ref type="figure" target="#fig_12">16</ref> shows that the trajectory of the robot tracks the trajectory of the prescribed impedance model in the inner loop. Fig. <ref type="figure" target="#fig_13">17</ref> shows the outer-loop controller performance. At the beginning, the prescribed impedance model is initialized with a set of nonoptimal parameters and thus the performance of the overall systems is not satisfactory. However, after a short time of interaction between the human and robot, the outer-loop controller learns the optimal parameters for the prescribed impedance model and therefore the HRI system tracks the desired trajectory successfully. Fig. <ref type="figure" target="#fig_14">18</ref> shows how the human force is reduced after the learning is performed and the optimal set of the prescribed impedance model is found by the outer-loop controller. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>A novel HRI control design method is presented inspired by the human factors studies. The proposed control structure has two control loops. The first loop is an inner-control loop which makes the unknown nonlinear robot look like a prescribed robot impedance model. In contrast to the previous trajectory tracking-based methods, the proposed inner loop does not require the knowledge of the task or the prescribed impedance parameters. This decomposes the robot-specific control design from the task specific design. The second loop is a task-specific loop which includes the human, the robot, and their interaction and finds the optimal parameters of the prescribed impedance parameters to assist the human to perform the task with less effort and optimal performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Robot-specific inner-loop model following control design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Task-specific outer-loop control design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overall two-loop control design method for the adaptive HRI system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Model reference neuro-adaptive inner-loop controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Human-robot interface in the task-specific outer loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Trajectory of the robot arm and the prescribed impedance model in x-direction for case 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Case 2 :</head><label>2</label><figDesc>The second matrix gains for the impedance model in (2) are chosen as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Trajectory of the robot arm and the prescribed impedance model in y-direction for case 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Trajectory of the robot arm and the prescribed impedance model in x-direction for case 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Trajectory of the robot arm and the prescribed impedance model in y-direction for case 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>IEEE TRANSACTIONS ON CYBERNETICSThe matrices A and B in<ref type="bibr" target="#b35">(36)</ref> then become</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. PR2 robot and the experimental setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>.Inner-loop results: the trajectory of the prescribed impedance control (red) versus the robot trajectory (black).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Outer-loop results: the trajectory of the robot (red) versus the desired trajectory (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Human force.</figDesc><graphic coords="11,72.99,53.51,202.80,172.68" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation under Grant IIS-1208623, in part by the Office of naval Research under Grant N00014-13-1-0562, in part by the Air Force Office of Scientific Research European Office of Aerospace Research and Development under Grant 13-3055, and in part by Army Research Office under Grant W911NF-11-D-0001. This paper was recommended by Associate Editor T.-H. S. Li.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>507.364 0.000 7.076 -0.000 3.378 -0.000 0.000 507.364 0.000 7.076 -0.000 3.378 7.075 0.000 7.170 -0.000 0.046 0.000 -0.000 7.076 -0.000 7.170 -0.000 0.046 3.378 -0.000 0.046 -0.000 99.995 -0.000 -0.000 3.378 0.000 0.046 -0.000 99.995</p><p>K * = -70.758 0.000 -71.704 0.000 0.458 0.000 0.000 -70.758 0.000 -71.704 -0.000 0.458 <ref type="bibr">(58)</ref> and consequently, the optimal gain matrices for the prescribed impedance model and the human force gain becomes B = 71.704 -0.000 -0.000 71.704 , K = 70.75 80.000 -0.000 70.758 K h = 0.458 0.000 0.000 0.458 .</p><p>It is now shown that the proposed online IRL Algorithm 1 gives the same set of the optimal parameters and consequently the same performance as the offline method, but without requiring the knowledge of the human dynamics. To initialize the value function in Algorithm 1, we assume that K d = Kd + K d and K p = Kp + K p , where Kd and Kp are nominal parameters for an expert human and K d and K p changes from one human to another. Note that matrix A q in A [see <ref type="bibr" target="#b35">(36)</ref>] is known and so we can solve the ARE (39) with matrix A containing only nominal values of the human dynamics. This gives us a very appropriate initial value for the value function kernel matrix P.</p><p>Fig. <ref type="figure">12</ref> shows the convergence of the control gain parameters to their optimal values given in (58) using online Algorithm 1. Note that, Algorithm 1 converges fast after only two iterations because we initialized the value function kernel matrix in an appropriate way. The final gain and kernel matrix found by Algorithm 1 are (59) Fig. <ref type="figure">12</ref>. Convergence of the prescribed impedance gains to their optimal values using online Algorithm 1.</p><p>13. Trajectory of the robot arm and the desired trajectory in x-direction. By comparing (59) to (58), it is obvious that the solution found by Algorithm 1 is the same as one found by solving the ARE <ref type="bibr" target="#b38">(39)</ref>. Figs. <ref type="figure">13</ref> and<ref type="figure">14</ref> show the results of the proposed method during and after learning. It can be seen that the system states follow the desired trajectories very fast after the learning is finished, because of the proper gains chosen for the prescribed impedance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL IMPLEMENTATION RESULTS</head><p>In this section, a practical experiment is conducted on a PR2 robot at the University of Texas at Arlington Research Institute. Fig. <ref type="figure">15</ref> shows the PR2 robot and the experimental setup. In this experiment, the human operator holds the gripper of the PR2 to perform point-to-point motion between red and blue points along the y-axis, as can be seen in Fig. <ref type="figure">15</ref>. Human force is measured using an ATI Mini40 FT sensor attached between the gripper and forearm of the PR2. The controller is implemented using the real-time controller manager framework of the PR2 in ROS Groovy. The real-time loop on the PR2 runs at 1000 Hz and communicates with the sensors and actuators on an EtherCAT network. Isura Ranatunga (S'09) received the B.Sc. degree from the University of Texas at Arlington, Arlington, TX, USA, in 2010, where he is currently pursuing the Ph.D. degree with a focus on robotics and automation, both in electrical engineering.</p><p>He is a Graduate Research Assistant with the University of Texas at Arlington Research Institute, Fort Worth, TX, USA, and the Next Generation Systems Research Group. His current research interests include force control, physical human-robot interaction, bipedal walking, adaptive robot control, and autonomous navigation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The role of roles: Physical cooperation between humans and robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mörtl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1656" to="1674" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The first national examination of outcomes and trends in robotic surgery in the United States</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Talamini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Coll. Surg</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lower-limb movement assistance through wearable robots: State of the art and challenges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amirat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Robot</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural Network Control of Robot Manipulators and Nonlinear Systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yesildirek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Taylor and Francis</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust neural-fuzzy-network control for robot manipulator including actuator dynamics</title>
		<author>
			<persName><forename type="first">R.-J</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1328" to="1349" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive control of robot manipulators with neural network based compensation of frictional uncertainties</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ciliz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="167" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adaptive Neural Network Control of Robotic Manipulators</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>World Scientific</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural network-based sliding mode adaptive control for robot manipulators</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="2377" to="2384" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive neural controller for space robot system with an attitude controlled base</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Borm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2333" to="2340" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Impedance control: An approach to manipulation. I-Theory. II-Implementation. III-Applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASME Trans. J. Dyn. Syst. Meas. Control</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1985-03">Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Impedance control of robot manipulators using adaptive neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Control Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="452" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive impedance control based on dynamic recurrent fuzzy neural network for upper-limb rehabilitation robot</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Control Autom</title>
		<meeting>IEEE Int. Conf. Control Autom<address><addrLine>Christchurch, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="1376" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural network based adaptive impedance control of constrained robots</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2002 IEEE Int. Symp. Intell. Control</title>
		<meeting>2002 IEEE Int. Symp. Intell. Control<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="615" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion learning and adaptive impedance for robot control during physical interaction with humans</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="4326" to="4332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive impedance control of a robotic orthosis for gait rehabilitation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural network impedance force control of robot manipulator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="461" />
			<date type="published" when="1998-06">Jun. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An optimal control model of human response. Part II: Prediction of human performance in a complex task</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Kleinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Levison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive impedance control to enhance human skill on a haptic interface system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Control Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visuomotor feedback gains upregulate during the learning of novel dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model-free reinforcement learning of impedance control in stochastic environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Auton. Mental Develop</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="330" to="341" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tracking control properties of human-robotic systems based on impedance control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="535" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Safe, stable and intuitive control for physical human-robot interaction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Duchaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3383" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal variable impedance control for a robot and its application to lifting an object with a human</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ikeura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moriguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mizutani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IEEE Int</title>
		<meeting>11th IEEE Int</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="500" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frequency-shaped impedance control for safe human-robot interaction in reference tracking application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ASME Trans. Mechatronics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1907" to="1916" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-robot collaboration based on motion intention estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ASME Trans. Mechatronics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1007" to="1014" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Assisting control in human adaptive mechatronics-single ball juggling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiratori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Control Appl</title>
		<meeting>IEEE Int. Conf. Control Appl<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Virtual internal model following control of robot arms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kosuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yokoyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Raleigh, NC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1549" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human adaptive mechatronics (HAM) for haptic system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th IEEE Annu. Conf. Ind. Electron</title>
		<meeting>30th IEEE Annu. Conf. Ind. Electron<address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="647" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">dynamic assist control on haptic system for human adaptive mechatronics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th IEEE Conf. Decis. Control Eur. Control Conf</title>
		<meeting>44th IEEE Conf. Decis. Control Eur. Control Conf<address><addrLine>Seville, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
			<biblScope unit="page" from="4596" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The nature of the operator&apos;s response in manual control and its implications for controller design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inst. Elect. Eng. IIA</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Engineering aspects of the human being as a servo-mechanism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ragazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Meeting of the American Psychological Association</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Operability of joysticktype steering device considering human arm impedance characteristics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oyabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obinata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variable impedance control based on estimation of human arm stiffness for human-robot cooperative calligraphic task</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsumugiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yokogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="644" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variable mechanical stiffness control based on human stiffness estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mitsantisuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katsura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Mechatronics, Istanbul, Turkey</title>
		<meeting>IEEE Int. Conf. Mechatronics, Istanbul, Turkey</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="731" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Impedance control for multi-point human-robot interaction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Asian Control Conf. (ASCC)</title>
		<meeting>8th Asian Control Conf. (ASCC)<address><addrLine>Kaohsiung, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="1187" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Continuous critic learning for robot control in physical human-robot interaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Tee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Control Autom. Syst. (ICCAS)</title>
		<meeting>13th Int. Conf. Control Autom. Syst. (ICCAS)<address><addrLine>Gwangju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">Oct. 2013</date>
			<biblScope unit="page" from="833" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Syrmos</surname></persName>
		</author>
		<title level="m">Optimal Control</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Optimal Control: Approximate Dynamic Programming, 4th ed</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Athena Scientific</publisher>
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the Curses of Dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reinforcement learning and feedback control</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="105" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Vamvoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimal Adaptive Control and Differential Games by Reinforcement Learning Principles</title>
		<title level="s">Control Engineering Series</title>
		<meeting><address><addrLine>Stevenage, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>IET</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An HMM approach to realistic haptic human-robot interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Joint Eurohaptics Conf. Symp. Haptic Interfaces Virtual Environ</title>
		<meeting>3rd Joint Eurohaptics Conf. Symp. Haptic Interfaces Virtual Environ<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="374" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human-intent detection and physically interactive control of a robot without force sensors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Erden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tomiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="382" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural-network-based human intention estimation for physical human-robot interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Ubiquitous Robot. Ambient Intell</title>
		<meeting>Int. Conf. Ubiquitous Robot. Ambient Intell<address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="390" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Abdallah</surname></persName>
		</author>
		<title level="m">Robot Manipulator Control: Theory and Practice</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimal tracking control of unknown discrete-time linear systems using input-output measured data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kiumarsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Linear quadratic tracking control of partially-unknown continuous-time systems using reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3051" to="3056" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive optimal control for continuous-time linear systems based on policy iteration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pastravanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abu-Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="477" to="484" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural network approach to continuous-time direct adaptive optimal control for partially unknown nonlinear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2699" to="2704" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning for continuous-time input-affine nonlinear systems with simultaneous invariant explorations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Integral reinforcement learning and experience replay for adaptive control of partially-unknown continuous-time systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adaptive optimal control of unknown constrained-input systems using policy iteration and neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Modares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi-Sistani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1513" to="1525" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
