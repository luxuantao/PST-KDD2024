<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Speech Translation with Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-17">17 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
							<email>yuchen.liu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
							<email>xionghao05@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc. No</orgName>
								<address>
									<addrLine>10, Shangdi 10th Street</addrLine>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
							<email>hezhongjun@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc. No</orgName>
								<address>
									<addrLine>10, Shangdi 10th Street</addrLine>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<email>jjzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc. No</orgName>
								<address>
									<addrLine>10, Shangdi 10th Street</addrLine>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc. No</orgName>
								<address>
									<addrLine>10, Shangdi 10th Street</addrLine>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<email>cqzong@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Speech Translation with Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-17">17 Apr 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1904.08075v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech recognition</term>
					<term>Speech translation</term>
					<term>Knowledge distillation</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years. Compared to conventional pipepine systems, end-to-end ST models have advantages of lower latency, smaller model size and less error propagation. However, the combination of speech recognition and text translation in one model is more difficult than each of these two tasks. In this paper, we propose a knowledge distillation approach to improve ST model by transferring the knowledge from text translation model. Specifically, we first train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation. Experiments on English-French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the instruction of teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conventional speech translation system is a pipeline of two main components: an automatic speech recognition (ASR) model which provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, <ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5]</ref>. This pipeline system usually suffers from time delay, parameter redundancy and error accumulation. In contrast, end-to-end ST, based on an encoderdecoder architecture with attention mechanism, is more compact and efficient. It can directly generate translations from raw audio and jointly optimize parameters on the final goal. Therefore, this model has become a new trend in speech translation research studies <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11]</ref>.</p><p>However, despite appealing advantages of end-to-end ST model, its performance is generally inferior. One of the important reasons is due to extremely scarce data which includes speech in source language paired with text in target language. Previous studies resort pretraining or multi-task learning approaches to improve the translation quality. They either pretrain ASR task on high-resource data <ref type="bibr" target="#b9">[11]</ref>, or use multi-task learning to train ST model with ASR or MT model simultaneously <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10]</ref>. Nevertheless, they only gain limited improvements and do not take full advantage of text data. We notice that the performance between end-to-end ST and MT model exists a huge gap, thus how to utilize MT model to help instruct end-to-end ST model is of great significance.</p><p>It is a challenge to train an end-to-end ST model directly from speech signal without text guidance while achieving comparable performance as text translation model. Given that text translation models are superior to ST model, we consider ST model can be improved by leveraging knowledge distillation. In knowledge distillation, there is usually a big teacher model with a small student model. It has been shown that the output probabilities of teacher model are smooth, which are easier for student model to learn from than ground-truth text <ref type="bibr" target="#b10">[12]</ref>. Thus, a student model can be taught by imitating the behaviour of teacher model, such as output probabilities <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>, hidden representation <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, or generated sequence <ref type="bibr" target="#b14">[16]</ref>, and alleviate the performance gap between itself and the teacher model.</p><p>In this paper, we present a method based on knowledge distillation for end-to-end ST model to learn knowledge from text translation model. We first train a text translation model on parallel text data (regarded as teacher) and then an end-to-end ST model (regarded as student) is trained by learning from groundtruth translations and the outputs of teacher model simultaneously. Experiments conducted on 100h English-French Augmented LibriSpeech corpus and 542h English-Chinese TED corpus show that it is possible to train a compact end-to-end speech translation model on both similar and dissimilar language pairs. With the instruction of teacher model, end-to-end ST model can gain significant improvements, approaching to the traditional pipeline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>End-to-end model has already become a dominant paradigm in machine translation task, which adopts an encoder-decoder architecture and generates target words from left to right at each step [1, <ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b3">5]</ref>. This model has also achieved promising results in ASR fields <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b15">17]</ref>. Recent works purpose a further attempt to combine these two tasks together by building an end-to-end speech-to-text translation without the use of source language translation during learning or decoding.</p><p>Anastasopoulos et al. <ref type="bibr" target="#b4">[6]</ref> use k-means clustering to cluster repeated audio patterns and automatically align spoken words with their translations. Duong et al. <ref type="bibr" target="#b5">[7]</ref> focus on the alignment between speech and translated phrase but not to directly predict the final translations. Bérard et al. <ref type="bibr" target="#b6">[8]</ref> give the first proof of the potential for end-to-end speech-to-text translation without using source language. They further conduct experimetns on a larger English-to-French dataset and pre-train encoder and decoder which improves performance <ref type="bibr" target="#b8">[10]</ref>. Weiss et al. <ref type="bibr" target="#b7">[9]</ref> also use multi-task learning and show that end-to-end model can outperform a cascade of independently trained pipeline system on Fisher Callhome Spanish-English speech translation task. Bansal et al. <ref type="bibr" target="#b9">[11]</ref> find pretraining encoder on higher-resource language ASR training data can achieve gains in low-resource speech translation system. However, these work mainly focus on pretraining acoustic encoder and do not take full advantage of text data.</p><p>Knowledge distillation is first adopted to apply for model compression, the main idea of which is to train a smaller student model to mimic a larger teacher model, by minimizing the loss between the teacher and student predictions. It has soon been applied to a variety of tasks, like image classification <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20]</ref>, speech recognition <ref type="bibr" target="#b10">[12]</ref> and natural language processing <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b19">21]</ref>. The teacher and student model in conventional knowledge distillation usually handle the same task, while in our method the teacher model and student model have different input modalities where teacher uses text as input and student uses speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Models</head><p>In this paper, we apply end-to-end models with the same architecture for all three tasks (ASR, ST and MT). The model architecture is similar with Transformer <ref type="bibr" target="#b3">[5]</ref>, which is the state-of-art model in MT task. Recently, this model also begins to be used in ASR task, showing a decent performance <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23]</ref>. In this section, we first describe the core architecture of Transformer and then show how this model is applied to ASR/ST and MT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Core Module of Transformer</head><p>Transformer is an encoder-decoder architecture which entirely relies on self-attention mechanism including scaled dot-product attention and multi-head attention. It consists of N stacked encoder and decoder layers. Each encoder layer has two blocks, which is a self-attention block followed by a feed-forward block. Decoder layer has the same architecture with encoder layer except an extra encoder-decoder attention block to perform attention over the output of the top encoder layer. Residual connection and layer normalization are employed around each block. In addition, the self-attention block in the decoder is modified with mask to prevent present positions attending to future positions during training.</p><p>To be detailed, multi-head attention technique is applied in self-attention and encoder-decoder attention blocks to obtain information from different representation subspaces at different positions. Each head is corresponding to a scaled dot-product attention, which operates on query Q, key K and value V:</p><formula xml:id="formula_0">Attention(Q, K, V) = softmax( QK T √ d k )V (1)</formula><p>where d k is the dimension of the key. Then the output values are concatenated,</p><formula xml:id="formula_1">MultiHead(Q, K, V) = Concat(head1, • • • , head h )W O where headi = Attention(QW Q i , KW K i , VW V i )<label>(2)</label></formula><p>where the Position-wise feed-forward block is composed of two linear transformations with a ReLU activation in between.</p><formula xml:id="formula_2">W Q i ∈ R d model ×dq , W K i ∈ R d model ×d k , W V i ∈ R d model</formula><formula xml:id="formula_3">FFN(x) = max(0, xW1 + b1)W2 + b2<label>(3)</label></formula><p>where the weights</p><formula xml:id="formula_4">W1 ∈ R d model ×d f f , W2 ∈ R d f f ×d model and the biases b1 ∈ R d f f , b2 ∈ R d model .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ASR/ST Model</head><p>The ASR/ST model is shown in the left part of Figure <ref type="figure" target="#fig_0">1</ref>, whose input is a series of discrete-time speech signal. We first use log-Mel filterbank to convert raw speech signal into a sequence of acoustic features and then apply mean and variance normalization. To prevent the GPU memory overflow and produce approximate hidden representation length against target length, we apply frame stack and downsample similar to <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref>. The final acoustic feature sequence is S = (s1, s2, • • • , sn) with dimension of d f ilterbank × num stack . Then the feature sequence is fed into a linear transformation with a normalization layer to map with model dimension d model . In addition, positional encodings are added to the feature sequence in order to enable the model to attend by relative positions. This sequence is finally treated as the input into Transformer model. Other parts are the same with Transformer model. For ASR the input to decoder is source language text, while the input to decoder in ST is target language text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MT Model</head><p>We also use Transformer to train a baseline MT model, as shown in the right part of Figure <ref type="figure" target="#fig_0">1</ref>. The difference between MT model and ASR/ST model is the input to the encoder. In MT model, X = (x1, x2, • • • , xn) is a sequence of tokens, representing source sentence. We embed the words in sequence X into a real continuous space with the dimension of R d model , which can be fed into a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Knowledge Distillation</head><p>Training an end-to-end ST model is considerably difficult than MT model. The accuracy of the later model is usually much higher than the former. Therefore, we present MT model as a teacher to teach ST model. Here we give a description of the idea of knowledge distillation.</p><p>Denote D = (s, x, y) as the corpus of triple data corresponding to speech signal, transcription in source language and its translation. The log-likelihood loss of ST model can be formulated as follows:</p><formula xml:id="formula_5">LST(D; θ) = − (s,y)∈D logP (y|s; θ) (4) logP (y|s; θ) = N t=1 |V | k=1 1(yt = k)logP (yt = k|y&lt;t, s; θ)<label>(5)</label></formula><p>where s is the acoustic feature sequence of source speech signal, y is the target translated sentence, N is the length of the output sequence, |V | is the vocabulary size of the output language, yt is the t-th output token, 1(yt = k) is an indicator function which indicates whether the output token is equal to the groundtruth.</p><p>We denote the output distribution of teacher model for token yt as Q(yt|y&lt;t, x; θT ), and x is the source transcribed sentence which corresponds to speech signal s. Then the cross entropy between the distributions of teacher and student is:</p><formula xml:id="formula_6">LKD(D; θ, θT ) = − (x,y)∈D N t=1 |V | k=1 Q(yt = k|y&lt;t, x; θT ) logP (yt = k|y&lt;t, x; θ)<label>(6)</label></formula><p>In distillation loss, the student not only matches the output of ground-truth, but also the output probabilities of teacher model, which is more smooth and yields smaller variance in gradients <ref type="bibr" target="#b10">[12]</ref>. Then the total loss function is,</p><formula xml:id="formula_7">LALL(D; θ; θT ) = (1−λ)LST(D; θ)+λLKD(D; θ, θT ) (7)</formula><p>where λ is a hyper-parameter to trade off these two loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on Augmented LibriSpeech which is collected by <ref type="bibr" target="#b24">[26]</ref> and available for free. This corpus is built by automatically aligning e-books in French with English utterances of LibriSpeech, which contains 236 hours of speech in total. They provide quadruplet: English speech signal, English transcription, French text translations from alignment of e-books and Google Translate references. Following <ref type="bibr" target="#b8">[10]</ref>, We only use the 100 hours clean train set for training, with 2 hours development set and 4 hours test set, which corresponds to 47,271, 1071 and 2048 utterances respectively. To be consistent with their settings, we also double the training size by concatenating the aligned references with the Google Translate references.</p><p>To verify whether the end-to-end speech translation model can handle on dissimilar language pairs, we build a corpus in English-Chinese direction. The raw data (including video, subtitles and timestamps) are crawled from TED website<ref type="foot" target="#foot_0">1</ref> . For each talk, we build a wav audio file extracted from video by ffmpeg<ref type="foot" target="#foot_1">2</ref> . We also collect its corresponding transcript and save in txt format. We divide each audio file into small segments based on timestamps instead of voice activity detection (VAD), because it eliminates the influence of improper fragments and guarantees each utterance containing complete semantic information, which is important for translation. In the end, we totally get 317,088 utterances (∼542 hours). Development and test sets are split according to the partition in IWSLT. We use dev2010 as development set and tst2015 as test set, which has 835 utterances (∼1.48 hours) and 1,223 utterances (∼2.37 hours) respectively. The remaining data are put into training set. We will release this dataset to public as a benchmark soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Our acoustic features are 80-dimensional log-Mel filterbanks extracted with a step size of 10ms and window size of 25ms and extended with mean subtraction and variance normalization. The features are stacked with 3 frames to the left and downsample to a 30ms frame rate. For text data, we lowercase all the texts, tokenize and apply normalize punctuations with the Moses scripts <ref type="foot" target="#foot_2">3</ref> . For Augmented LibriSpeech corpus, we apply BPE <ref type="bibr" target="#b25">[27]</ref> on the combination of English and French text to obtain subword units. The number of merge operations in BPE is set to 8K, resulting in a shared vocabulary with 8,159 subwords. For TED English-Chinese, the merge number is 30K, and vocabulary size are 28,912 and 30,000, respectively. We report case-insensitive BLEU scores <ref type="bibr" target="#b26">[28]</ref> by multi-bleu.pl script for the evaluation of ST and MT tasks and use word error rates (WER) to evaluate ASR task.</p><p>Because the size of Augmented LibriSpeech is relatively small, we set the hidden size d model = 256, the filter size in feed-forward layer d f f = 1024, the head number h = 8, the residual dropout and attention dropout are 0.1. For TED English-Chinese, we set the hidden size d model = 512 with the filter size d f f = 2048. MT model, as a teacher model, can use bigger parameters. We use 512 hidden sizes, 2048 filter sizes with 8 heads.The number of encoder layers and decoder layers in above models are all set to 6. We train our models with Adam optimizer <ref type="bibr" target="#b27">[29]</ref> with β1 = 0.9, β2 = 0.98 and = 10 −9 on 2 NVIDIA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results for the ASR and MT tasks on Augmented LibriSpeech. It can be seen that Transformer model significantly outperforms in both ASR and MT tasks, with 0.92 WER reduction and 4.1 BLEU scores improvement in beam search compared to <ref type="bibr" target="#b8">[10]</ref>. We contribute it to the superior performance of Transformer model which is good at modeling long distance in sequence-to-sequence tasks, especially for MT tasks. Contrary to <ref type="bibr" target="#b8">[10]</ref> which uses characters as output units, we consider subword units can also obtain improvements.</p><p>For ST task, we have four settings. The pipeline model uses ASR outputs as MT inputs, where ASR model and MT model are described above. The end-to-end model is directly trained on source speech signal paired with target text translations. The pre-trained model is identical to end-to-end model, but it is initialized with ASR and MT models. Knowledge distillation (KD) is our method which uses MT model as teacher model to instruct end-to-end ST model.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, all four settings surpass the results in <ref type="bibr" target="#b8">[10]</ref>. Noticing that there exists a huge gap between the performance of the end-to-end ST model and MT model, even if the end-to-end ST model is pretrained, thus we conduct knowledge distillation to instruct ST model with MT model. The result shows that this method can bring significant improvement on the BLEU score which increases from 14.30 to 17.02. With the instruction of MT model, the performance gap is alleviated, approaching to the pipeline system. It demonstrates the effectiveness of our method.</p><p>We also conduct experiments on English-Chinese to verify our methods. Table <ref type="table" target="#tab_2">3</ref> presents the results of MT and ST models. Pipeline model combines both the ASR (WER is 18.2%) and MT models. It is difficult to train end-to-end ST model from random initialization parameters, for the reordering between dissimilar language pairs is difficult to align with frame based speech representations. The end-to-end ST model here is pretrained with ASR. With knowledge distillation, it can obtain significant simprovements, proving the generality of our method. Although end-to-end ST does not outperform pipeline system, it shows the potential to implement a compact end-toend model even on dissimilar language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>To evaluate the effect of teacher model, we explore different hyper-parameters λ of the distillation loss on Augmented Lib-riSpeech. With λ increasing, ST will pay more attention to the teacher model. When λ equals 0, it is the pre-trained end-toend model; when λ is 1, it will ignore ground-truth and only learn from the teacher. As Table <ref type="table" target="#tab_3">4</ref> shows, the performance becomes better with the increasing of λ. End-to-end ST obtains We further analyze how knowledge from MT model helps ST through visualizations of the encoder-decoder attention. Figure <ref type="figure" target="#fig_2">2</ref> shows an example. The attentions of ASR (a) and MT (c) models have more confident than ST model. Each output token in the former two model concentrates on specific frames or tokens, especially for MT model, while the attention in ST (b) model tends to be smoothed out across many input frames. However, with the help of MT model, the attention of ST model with KD (d) becomes more concentrated. For example, the speech frames l = 45 ∼ 55 are corresponding to "was talking" in ASR (a), which can be translated to "se parlait" in French (c). The attention in ST model with KD has more weights on frames l = 45 ∼ 55 than that in original ST model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we present knowledge distillation method to improve the end-to-end ST model by transferring the knowledge from MT model. Experiments on two language pairs demonstrate that with the instruction of MT model, end-to-end ST model gain significant improvements. Although the endto-end ST does not outperform pipeline system, it shows the potential to come close in performance. In the future we will utilize other knowledges like the outputs from ASR model to further improve the performance of ST model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture of our method. The left part is ST model, regarded as a student model, whose input is speech signal. The right part is MT model, regarded as a teacher model, whose input is the source sentence corresponding to the input of student model. The output of both student model and teacher model is target sentence. The top part is distillation loss, where the student model not only matches the ground-truth, but also the output probabilities of the teacher model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The visualizations of attention in different models. (a), (b), (c), (d) are the encoder-decoder attention of ASR, endto-end ST, MT and end-to-end ST with KD, respectively.</figDesc><graphic url="image-8.png" coords="4,335.18,443.84,75.28,61.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[1] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," in Proc. ICLR, 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ASR and MT results on test set of Augmented Lib-riSpeech.</figDesc><table><row><cell>LibriSpeech</cell><cell>Method</cell><cell cols="2">WER(↓) BLEU(↑)</cell></row><row><cell>Bérard [10]</cell><cell>greedy beam search</cell><cell>19.9 17.9</cell><cell>19.2 18.8</cell></row><row><cell>Ours</cell><cell>greedy beam search</cell><cell>21.46 16.98</cell><cell>21.35 22.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ST results on Augmented LibriSpeech test. KD denotes knowledge distillation.</figDesc><table><row><cell>LibriSpeech</cell><cell>Method</cell><cell cols="3">greedy beam ensemble</cell></row><row><cell></cell><cell>Pipeline</cell><cell>14.6</cell><cell>14.6</cell><cell>15.8</cell></row><row><cell>Bérard [10]</cell><cell>End-to-end Pre-trained</cell><cell>12.3 12.6</cell><cell>12.9 13.3</cell><cell>15.5</cell></row><row><cell></cell><cell>Pipeline</cell><cell>15.75</cell><cell>17.85</cell><cell>18.4</cell></row><row><cell>Ours</cell><cell>End-to-end Pre-trained</cell><cell>10.19 13.89</cell><cell>13.15 14.30</cell><cell>17.8</cell></row><row><cell></cell><cell>KD</cell><cell>14.96</cell><cell>17.02</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MT and ST results on TED English-Chinese test.</figDesc><table><row><cell>TED</cell><cell>MT</cell><cell cols="2">Pipeline End-to-end</cell><cell>KD</cell></row><row><cell cols="2">BLEU 27.08</cell><cell>22.28</cell><cell>16.80</cell><cell>19.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effect of teacher model weight on ST results.</figDesc><table><row><cell>λ</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell cols="7">BLEU 14.30 15.68 16.73 16.62 16.93 17.02</cell></row><row><cell cols="7">the best performance when it only learns the output distributions</cell></row><row><cell cols="2">of teacher model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.ted.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://ffmpeg.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.statmt.org/moses/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mohammad Norouzi</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno>arXiv preprint arXix:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Googles neural machine translation system: bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stateof-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An unsupervised probability model for speech-to-translation alignment of low-resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An attentional model for speech translation without transcription</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on End-to-end Learning for Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01431</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01802</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge distillation in generations: More tolerant teachers educate better students</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03235</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Syllable-based sequenceto-sequence speech recognition with the transformer in mandarin chinese</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kraif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
