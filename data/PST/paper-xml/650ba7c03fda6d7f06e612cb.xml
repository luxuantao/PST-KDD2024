<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-printed OPENCHAT: ADVANCING OPEN-SOURCE LANGUAGE MODELS WITH MIXED-QUALITY DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-20">20 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory of Brain and Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sijie</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianyuan</forename><surname>Zhan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<addrLine>5 01</addrLine>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory of Brain and Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<addrLine>5 01</addrLine>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-printed OPENCHAT: ADVANCING OPEN-SOURCE LANGUAGE MODELS WITH MIXED-QUALITY DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-20">20 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.11235v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, there have been notable advancements in Large Language Models (LLMs), such as <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> and Chinchilla <ref type="bibr" target="#b13">(Hoffmann et al., 2022)</ref>, demonstrating impressive performance in various downstream natural language processing (NLP) tasks <ref type="bibr">(Zhao et al., 2023)</ref>. Despite the remarkable success of GPT-4, the specific techniques employed in its development remain shrouded in mystery. To gain a deeper understanding of the underlying technical aspects and to promote the widespread adoption of LLMs, a series of open-source base language models have emerged, especially LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref> and LLaMA2 <ref type="bibr">(Touvron et al., 2023b)</ref>. Building upon the released base language models, there are typically two methods to align these base models to specific abilities, including supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT).</p><p>The first line of methods <ref type="bibr" target="#b5">(Chiang et al., 2023;</ref><ref type="bibr" target="#b27">Taori et al., 2023)</ref> use SFT to enhance instruction following abilities. Most existing methods primarily focus on designing SFT datasets. Some studies <ref type="bibr" target="#b5">(Chiang et al., 2023;</ref><ref type="bibr" target="#b11">Geng et al., 2023)</ref> collect user-shared conversations as well as human feedback datasets from the public web, while others <ref type="bibr">(Xu et al., 2023a;</ref><ref type="bibr" target="#b9">Ding et al., 2023)</ref> develop frameworks for automatically gathering extensive open-domain instructions spanning various difficulty levels. However, these constructed SFT datasets are generally mixed with limited expert data and a large proportion of sub-optimal data due to the high cost of human labor and API requests. Naturally, it is not advisable to indiscriminately feed all these mixed conversations to the base model, as the low-quality data are likely to negatively impact learning <ref type="bibr" target="#b40">(Zhou et al., 2023;</ref><ref type="bibr" target="#b35">Xu et al., 2022)</ref>. However, this is largely neglected in previous methods, which often treat all training data equally.</p><p>To allow LLMs to go beyond modeling the training data distribution, recent LLMs <ref type="bibr">(OpenAI, 2023;</ref><ref type="bibr">Touvron et al., 2023b)</ref> adopt RLFT to align better with the human desired behaviors, especially API-based models. The well-known reinforcement learning from human feedback (RLHF) method <ref type="bibr" target="#b21">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b7">Christiano et al., 2017;</ref><ref type="bibr">Bai et al., 2022b)</ref> first collects plenty of highquality preference feedback from human annotators to fit one or multiple reward models (typically also trained based on smaller LLMs), and then uses RL to maximize the estimated reward. The involvement of learning and optimizing with extra reward models using RL brings considerable computational and stability issues. Some recent studies <ref type="bibr" target="#b26">(Rafailov et al., 2023;</ref><ref type="bibr" target="#b36">Yuan et al., 2023)</ref> partly address this problem by avoiding fitting the reward model and fusing preference modeling and LLM fine-tuning into a single-stage training process. However, all existing RLHF methods require highquality pairwise or ranking-based preference data for preference modeling, which inevitably require expensive human expert annotations <ref type="bibr" target="#b2">(Casper et al., 2023)</ref>.</p><p>To address the aforementioned limitations, we propose a new framework, named OpenChat, to advance the open-source language model fine-tuning with mixed-quality data as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Here, we consider the general non-pairwise (nor ranking-based) SFT training data, consisting of a small amount of expert data and a large proportion of easily accessible sub-optimal data, without any preference labels. Specifically, we propose the Conditioned-RLFT (C-RLFT), which enables leveraging mixed-quality training data with very coarse-grained reward labels. The reward label can be as simple as a relative value differentiating different classes of data sources, i.e., GPT-4 or GPT-3.5. We derive C-RLFT based on the KL-regularized RL framework <ref type="bibr" target="#b14">(Jaques et al., 2019;</ref><ref type="bibr" target="#b15">Korbak et al., 2022)</ref>, which maximizes the reward while penalizing the difference between the fine-tuned policy and a reference policy. However, to remedy the imperfect reward signal, we learn the fine-tuned LLM itself as a class-conditioned policy (i.e., conditioning data source classes with distinct prompt tokens), and regularize it with a better and more informative class-conditioned reference policy instead of the original pre-trained LLM. The optimal policy for this RL problem can be shown as equivalent to a class-conditioned reward-weighted regression problem, which can be easily solved through single-stage supervised learning. C-RLFT provides several particularly desirable features for open-source LLM fine-tuning. First, it allows for simple and RL-free training, largely removing the complexities and instabilities in typical RLHF fine-tuning. Second, it has extremely low requirements for the quality of the reward and does not need costly human feedback collection.</p><p>Despite being simple and lightweight, our proposed OpenChat with C-RLFT achieves great performance in a series of benchmark evaluations. Specifically, we leverage the ShareGPT conversations dataset<ref type="foot" target="#foot_0">1</ref> following Vicuna <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref> and use llama-2-13b as the base model. It is worth noting that our proposed method can be applied to any mixed-quality datasets and arbitrary base language models. We conduct extensive experiments on three standard benchmarks to assess instruction following ability, including Alpaca-Eval <ref type="bibr" target="#b17">(Li et al., 2023)</ref>, MT-bench <ref type="bibr" target="#b36">(Zheng et al., 2023)</ref> and Vicuna-bench <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref>. The results demonstrate that openchat-13b significantly surpasses previous 13b open-source language models and can even outperform gpt-3.5-turbo in all three benchmarks. Furthermore, we also use AGIEval <ref type="bibr" target="#b39">(Zhong et al., 2023)</ref> to prove the generalization, where openchat-13b also achieves the top-1 average accuracy among all 13b open-source language models. Finally, we design a series of ablation studies and analyses to validate the contribution of different components, and performance consistency, providing insights into the effectiveness and robustness of OpenChat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Given a conversation dataset D = {(x i , y i )}, where x i indicates the instruction, y i is its corresponding response, the pre-trained language model ? 0 (y|x) can be regarded as a probability distribution mapping from instructions to responses. There are two lines of research to adapt the pre-trained language model ? 0 (y|x) to a fine-tuned language model ? ? (y|x) with desirable features, including supervised fine-tuning and reinforcement learning fine-tuning.</p><p>Supervised Fine-tuning (SFT). This line of methods <ref type="bibr">(Xu et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b9">Ding et al., 2023)</ref> directly uses a high-quality conversation dataset D to fine-tune the pre-trained language model ? 0 (y|x) using supervised learning, i.e., maximum likelihood estimates (MLE):</p><formula xml:id="formula_0">J SFT (?) = E (x,y)?D [log ? ? (y|x)]<label>(1)</label></formula><p>where ? ? is initialized from ? 0 . To ensure the fine-tuning performance, SFT requires the conversation dataset D to have very high quality, because SFT treats all training data uniformly <ref type="bibr" target="#b40">(Zhou et al., 2023;</ref><ref type="bibr" target="#b3">Chen et al., 2023)</ref>. However, the collection of high-quality SFT datasets can be very expensive. Most existing open-source LLMs <ref type="bibr" target="#b5">(Chiang et al., 2023;</ref><ref type="bibr" target="#b11">Geng et al., 2023;</ref><ref type="bibr">Xu et al., 2023a;</ref><ref type="bibr" target="#b9">Ding et al., 2023)</ref> fine-tune their models using conversation datasets that likely contain a large proportion of sub-optimal data due to high costs of human labor or API requests, inevitably leading to a certain level of performance degeneration.</p><p>Reinforcement Learning Fine-tuning (RLFT). Another intuitive approach to align LLMs is through RL, which models rewards according to human preference feedbacks <ref type="bibr" target="#b21">(Ouyang et al., 2022;</ref><ref type="bibr">Bai et al., 2022a;</ref><ref type="bibr" target="#b26">Rafailov et al., 2023)</ref> or pre-defined classifiers <ref type="bibr" target="#b31">(Wu et al., 2023)</ref>, and fine-tune LLMs to maximize the reward. The reward r(x, y), either modeled explicitly or implicitly, assigns high values on desirable responses and low values on bad ones to guide the alignment of the finetuned LLM. A popular RL framework for fine-tuning LLMs is the KL-regularized RL <ref type="bibr" target="#b14">(Jaques et al., 2019;</ref><ref type="bibr" target="#b15">Korbak et al., 2022;</ref><ref type="bibr" target="#b26">Rafailov et al., 2023)</ref>, which adds an additional KL penalty to constrain the fine-tuned LLM ? ? (y|x) to stay close to the base pre-trained LLM ? 0 (y|x). This has been shown beneficial to avoid distribution collapse as compared to na?vely maximize reward using RL <ref type="bibr" target="#b15">(Korbak et al., 2022)</ref>. The RL objective of this series of RLFT models can be typically formulated as follows:</p><formula xml:id="formula_1">J RLFT (?) = E y?? ? [r(x, y)] -?D KL (? ? , ? 0 )<label>(2)</label></formula><p>In existing RLFT methods, high-quality reward signals play a crucial role in ensuring improved LLM fine-tuning performance. This, however, requires collecting considerable amounts of costly pairwise (or ranking-based) human preference feedback, which poses a major challenge in the development of many open-source language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPENCHAT</head><p>In this section, we introduce the OpenChat framework, which provides a new possibility to fine-tune open-source LLMs using easily collectible and mixed-quality training data without any preference labels. More specifically, we consider the setting where we are given a pre-trained LLM ? 0 , a small set of high-quality/expert conversation data D exp , and a larger medium-quality or sub-optimal conversation dataset D sub , we aim to fine-tune an LLM policy ? ? based on ? 0 using only data from D exp D sub . Taking the most popular SFT dataset ShareGPT used in Vicuna <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref> as an example, the distinct data sources from GPT-4 and GPT-3.5 can be regarded as D exp and D sub , as the overall quality of GPT-3.5 conversations generally falls short when compared to that of <ref type="bibr">GPT-4 conversations (OpenAI, 2023;</ref><ref type="bibr" target="#b17">Li et al., 2023)</ref>, where detailed comparison can be found in Sec. 5.1.</p><p>Obviously, it is not possible to derive accurate and fine-grained reward signals solely based on D exp and D sub . However, it should be noted that the quality difference between D exp and D sub itself can serve as implicit or weak reward signals. To make use of this coarse-grained reward information, we provide a new insight that by regularizing ? ? with a better and more informative class-conditioned reference policy ? c instead of the original base pre-trained LLM ? 0 . We are likely to compensate for the potential deficiencies in the rewards and achieve good fine-tuning performance. In the following, we describe the details of OpenChat and its core algorithm -C-RLFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CLASS-CONDITIONED DATASET AND REWARDS</head><p>Given the SFT conversation datasets D exp D sub with different quality levels, we can replenish them with distinct sources as class labels (e.g., c i ? {GPT-4, GPT-3.5}) and construct a classconditioned dataset D c = {(x i , y i , c i )}. We use ? c (y|x, c) to denote class-conditioned distribution over instructions x and responses y in the class-conditioned dataset D c , which can be perceived similarly as the behavior policy of a dataset in offline RL literature <ref type="bibr" target="#b16">(Levine et al., 2020)</ref>, with the difference that ? c is now a class-conditioned policy.</p><p>According to the different overall quality with respect to class labels, we can naturally encode coarse-grained rewards r c (x, y) in D c as follows:</p><formula xml:id="formula_2">r c (x i , y i ) = 1, if (x i , y i ) ? D exp (e.g., c i = GPT-4), ?, if (x i , y i ) ? D sub (e.g., c i = GPT-3.5) (? &lt; 1).<label>(3)</label></formula><p>where we regard GPT-4 conversations as expert data D expert , and GPT-3.5 conversations as suboptimal data D sub . Meanwhile, we set ? &lt; 1 to guide the fine-tuned model to favor more of the high-quality responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FINE-TUNING VIA C(ONDITIONED)-RLFT</head><p>As the rewards r c (x, y) in Eq. ( <ref type="formula" target="#formula_2">3</ref>) are very coarse-grained, to reliably use them in RLFT, we need to provide additional sources of information to remedy their deficiencies. Here, we introduce C-RLFT, which is inspired by the insight from the goal-conditioned supervised learning in offline RL, that by conditioning on proper information in a supervised goal/outcome-conditioned policy, it is possible to recover optimized performance <ref type="bibr" target="#b4">(Chen et al., 2021;</ref><ref type="bibr" target="#b10">Emmons et al., 2021)</ref>. C-RLFT contains two key ingredients: 1) fine-tuning the LLM as a class-conditioned policy ? ? (y|x, c), and 2) regularizing ? ? with respect to the class information augmented reference policy ? c rather than the original base reference policy ? 0 in the KL-regularized RL framework.</p><p>Class-conditioned policy. Instead of directly fine-tuning an LLM from the pre-trained model ? 0 as in existing methods, we model the LLM to be fine-tuned as a class-conditioned policy ? ? (y|x, c). This can be easily implemented by conditioning each example from different data sources using distinct initial prompt tokens as shown below.</p><p>[GPT-4 Prompt] GPT4 User: Question&lt;|end of turn|&gt;GPT4 Assistant:</p><p>[GPT-3.5 Prompt] GPT3 User: Question&lt;|end of turn|&gt;GPT3 Assistant:</p><p>To differentiate speakers, we introduce a new &lt;|end of turn|&gt; special token at the end of each utterance, following <ref type="bibr" target="#b40">Zhou et al. (2023)</ref>. The &lt;|end of turn|&gt; token functions similarly to the EOS token for stopping generation while preventing confusion with the learned meaning of EOS during pretraining. We further discuss distinct prompt tokens in different positions in App. A.</p><p>Policy optimization. To compensate for the coarse-grained reward information r c (x, y), we modify the original KL-regularized RL objective Eq. ( <ref type="formula" target="#formula_1">2</ref>) and instead optimize the following problem:</p><formula xml:id="formula_3">J C-RLFT (?) = E y?? ? [r c (x, y)] -?D KL (? ? , ? c ) (4)</formula><p>The idea is to use the higher-quality and more informative class-conditioned behavior policy ? c of D c for regularization, rather than the pre-trained model ? 0 . We adopt this design for the following reasons. First, for most existing open-source pre-trained LLMs, their performance in many cases is still inferior to API-based models. This means that even the D sub data collected from GPT-3.5 are likely to have higher quality than ? 0 . Second, ? c contains additional data source information, which can provide extra information to help differentiate the quality of data.</p><p>Following prior works <ref type="bibr" target="#b25">(Peters &amp; Schaal, 2007;</ref><ref type="bibr" target="#b24">Peng et al., 2019;</ref><ref type="bibr" target="#b15">Korbak et al., 2022;</ref><ref type="bibr" target="#b26">Rafailov et al., 2023)</ref>, it can be shown that the optimal solution to the above KL-regularized reward maximization objective takes the following form (see App. B for detailed derivation):</p><formula xml:id="formula_4">? * (y|x, c) ? ? c (y|x, c) exp 1 ? r c (x, y)<label>(5)</label></formula><p>We can thus extract the optimized policy ? ? by minimizing the KL divergence between ? * under the class-conditioned dataset D c <ref type="bibr" target="#b19">(Nair et al., 2020;</ref><ref type="bibr" target="#b15">Korbak et al., 2022)</ref>:</p><formula xml:id="formula_5">? ? =arg min ? E (x,c)?Dc [D KL (? * (?|x, c)?? ? (?|x, c))] =arg min ? E (x,c)?Dc [E y?? * [-log ? ? (y|x, c)]] =arg max ? E (x,y,c)?Dc exp 1 ? r c (x, y) log ? ? (y|x, c)<label>(6)</label></formula><p>The last step is obtained by plugging the closed form ? * in Eq. ( <ref type="formula" target="#formula_4">5</ref>) and using the fact that ? c is exactly the class-conditioned behavior distribution of D c . This suggests that the fine-tuned policy ? ? can be learned through a simple reward-weighted regression objective with the class-conditioned dataset D c . This learning objective provides a remarkably simple scheme to fine-tune open-source LLMs. It does not require accurate reward labels, but uses conditioning to differentiate good and inferior model behaviors. Moreover, after initializing ? ? with ? 0 , we no longer need to load ? 0 during training, while most RLHF methods using PPO for policy optimization <ref type="bibr" target="#b21">(Ouyang et al., 2022;</ref><ref type="bibr">Bai et al., 2022a)</ref> still need to maintain ? 0 to compute the KL penalty during fine-tuning. This enables C-RLFT to save a considerable amount of computation resources during training.</p><p>Model inference. During the inference phase, we assume that our C-RLFT method has learned to distinguish expert and sub-optimal data distributions. Considering that we aim to exclusively generate high-quality responses for our fine-tuned class-conditioned ? ? , we use the same specific prompts that were employed in GPT-4 conversations during the training phase as below:</p><p>[Inference Prompt] GPT4 User: Question&lt;|end of turn|&gt;GPT4 Assistant:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUPS</head><p>Mixed-quality Data. Following Vicuna <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref>, we adopt a widely-used SFT dataset, the ShareGPT dataset. The ShareGPT dataset consists of approximately 70k user-shared conversations, including around 6k expert conversations generated by GPT-4 and the remaining sub-optimal conversations from GPT-3.5. We perform experiments to assess their varying quality in Sec. 5.1.</p><p>Benchmarks. To evaluate the instruction-following ability, we employ the three most widely recognized benchmarks, including AlpacaEval <ref type="bibr" target="#b17">(Li et al., 2023)</ref>, MT-bench <ref type="bibr" target="#b36">(Zheng et al., 2023)</ref> and Vicuna-bench <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref>. Additionally, to verify generalization, we perform all English tasks in AGIEval <ref type="bibr" target="#b39">(Zhong et al., 2023)</ref> using zero-shot settings, which presents a collection of humancentric standardized exams. More details can be found in App. C.</p><p>Baselines. We evaluate the most popular API-based and open-source LLMs: (1) gpt-4 (Ope-nAI, 2023) and gpt-3.5-turbo <ref type="bibr" target="#b21">(Ouyang et al., 2022)</ref>  Metrics. We employ three metrics following the official implementations: (1) Win rate: This metric is employed for pairwise comparisons. Given a question and two answers generated by the tested model and the target model, the LLM evaluator needs to compare these two models. The tested model receives 1 point for a win, 0.5 points for a tie, and 0 points for a loss. (2) Score: This metric is applied for single-answer grading in MT-bench, where the LLM evaluator directly judges the generated answer with a score varying from 1 to 10. (3) Accuracy: This metric is used by AGIEval for multiple-choice exam questions.</p><p>Implementation Details. The openchat-13b is based on the llama-2-13b <ref type="bibr">(Touvron et al., 2023b)</ref>. We fine-tune the model for 5 epochs on the ShareGPT dataset using the AdamW optimizer with a sequence length of 4,096 tokens and an effective batch size of 200k tokens. Given that the reward weight term in Eq. ( <ref type="formula" target="#formula_5">6</ref>) (exp(r c /?)) remains constant within a class, we simplify the process by assigning a unit weight to D exp and the weight of 0.1 to D sub . The AdamW optimizer's hyperparameters are set as follows: ? 1 = 0.9, ? 2 = 0.95, ? = 10 -5 , and weight decay of 0.1. We employ a cosine learning rate schedule with a maximum learning rate of 6.7?10 -5 , which decays to 10% of the maximum value. The hyperparameters remain consistent with the base model pretraining settings following <ref type="bibr">Touvron et al. (2023b)</ref>. However, we scale the learning rate proportionally to the square root of the batch size, following the theoretical analysis provided by <ref type="bibr" target="#b12">Granziol et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>In the first set of results, we compare the win-rate (%) performance of openchat-13b and other popular LLMs on three standard benchmarks to assess the instruction-following ability. The results are presented in Table <ref type="table" target="#tab_0">1</ref>. Among the API-based LLMs, the win rate of gpt-4 significantly outperforms all other models, demonstrating that gpt-4 maintains obvious advantages.</p><p>The open-source language model llama-2-chat-70b, which employs both SFT and RLHF, is another powerful instruction-following model which surpasses claude and gpt-3.5-turbo. However, guanaco-65b and guanaco-33b lag behind other models larger than 13b. Regarding the series of 13b models, the open-source language models based on llama-13b, including vicuna-v1.1-13b, wizardlm-v1.0-13b and ultralm-13b, achieve approximately 50% average win rates across the three benchmarks. Meanwhile, the open-source language models based on llama-2-13b generally exhibit higher average win rates.  Although prior works <ref type="bibr">(OpenAI, 2023;</ref><ref type="bibr">Zhao et al., 2023)</ref> have widely confirmed that GPT-4 demonstrates superior capabilities on a broad range of tasks compared to GPT-3.5, we further analyze the quality of collected GPT-3.5 and GPT-4 conversations in the ShareGPT dataset to validate our assumption of mixed-quality data. Specifically, we randomly sample 128 conversations from each data source. Then gpt-4 serves as the automatic evaluator, scoring the responses following <ref type="bibr" target="#b36">Zheng et al. (2023)</ref>. As illustrated in Figure <ref type="figure">3</ref>, GPT-4 conversations contain more high-quality conversations and exhibit a higher overall score. The detailed scoring settings can be found in App. F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDIES</head><p>We conduct an ablation study on the key components of the openchat-13b model to ascertain their individual contributions to the overall performance, including the coarse-grained rewards and the class-conditioned policy. Additionally, we introduce vicuna-v1.5-13b which SFT on about 125k ShareGPT data as a baseline. The results of ablation studies are detailed in Table <ref type="table">2</ref>. Without coarse-grained rewards, the training phase treats different data sources equally, leading to performance decline. Similarly, without a class-conditioned policy, language models lack explicit signals to discern between expert and sub-optimal data, significantly reducing performance. We also conduct only SFT on about 70k ShareGPT data, resulting in a performance drop to 52.7%. This</p><p>Pre-printed Table <ref type="table">2</ref>: Ablation studies of coarse-grained rewards (reward) and class-conditioned policy (condition) to openchat-13b.</p><p>indicates the significant contribution of both main components to model performance. Notably, expanding the SFT dataset from 70k to 125k has less impact on performance improvement than our proposed components, particularly the class-conditioned policy.  Secondly, given the significant impact of the classconditioned policy, we further explore its effects on model performance during the inference phases by examining the influence of class-conditioned prompt tokens. In the inference phase, we use the GPT-4 prompt to induce openchat-13b to generate high-quality responses. Here we further verify the impacts of different inference prompts by replacing the GPT-4 prompt with the GPT-3.5 prompt. The comparison results, illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>, reveal a substantial performance decline when using the GPT-3.5 prompt instead of the GPT-4 prompt. This suggests that our openchat-13b model can distinguish the quality of different data sources based on our class-conditioned policy, and it further indicates that the representation space of GPT-4 is superior to that of GPT-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">EVALUATORS CONSISTENCY</head><p>Figure <ref type="figure">6</ref>: The consistency between GPT-3.5 and Claude-2 in AlpacaEval benchmark.</p><p>While all benchmarks evaluate the agreement between humans and automatic evaluators, we also consider the self-enhancement bias as discussed by <ref type="bibr" target="#b36">Zheng et al. (2023)</ref>, where self-enhancement bias indicates that automatic evaluators may favor their own generated answers. To address this, we employ two additional automatic evaluators, gpt-3.5 and claude-2, alongside the official evaluator gpt-4, to verify the consistency of evaluators on AlpacaEval. The results between gpt-3. In this section, we investigate the impact of varying data sizes on model performance. Specifically, we sub-sample one class in GPT-3.5 or GPT-4 with the ratio varying from 60% to 100% in 10% increments, while keeping the other class unchanged. It is worth noting that the total number of GPT-3.5 data is more than ten times larger than that of GPT-4. The results are shown in Fig. <ref type="figure">7</ref>. Firstly, we observe that the overall decline in both average performances is relatively modest, indicating that our openchat-13b is robust to variation in data size. Secondly, although the number of GPT-4 data points changes much less than GPT-3.5, the effect of varying GPT-4 data size is even more pronounced. This phenomenon demonstrates that expert data, while limited in quantity, is extremely important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>Large Language Models. Recent years have witnessed significant advancements in LLMs, with models such as GPT-4 (OpenAI, 2023), PaLM <ref type="bibr" target="#b6">(Chowdhery et al., 2022)</ref>, and others comprising hundreds of billions or more parameters. This surge in LLMs extends beyond API-based models, as a suite of open-source language models like LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref>, LLaMA-2 <ref type="bibr">(Touvron et al., 2023b)</ref>, and Falcon <ref type="bibr" target="#b22">(Penedo et al., 2023)</ref> have emerged. This paper primarily focuses on the LLaMA base models, which are among the most popular open-source language models.</p><p>Supervised Fine-tuning for LLMs. A considerable body of work has been dedicated to enhancing large base language models through SFT. For instance, Alpaca <ref type="bibr" target="#b27">(Taori et al., 2023)</ref> uses self-instruct <ref type="bibr" target="#b30">(Wang et al., 2022)</ref> to generate 52k instruction-following demonstrations via text-davinci-003. This instruction data has been extensively applied in subsequent studies, such as Koala <ref type="bibr" target="#b11">(Geng et al., 2023)</ref>. <ref type="bibr" target="#b23">Peng et al. (2023)</ref> follow Alpaca's setup but replace GPT-4 as the distillation teacher. WizardLM <ref type="bibr">(Xu et al., 2023a)</ref> introduces Evol-Instruct, a technique that rewrites Alpaca's initial instruction data into more complex instructions, thereby enhancing the model's instruction-following capabilities. Other studies, such as UltraChat <ref type="bibr" target="#b9">(Ding et al., 2023)</ref> and Baize <ref type="bibr">(Xu et al., 2023b)</ref>, have designed frameworks to obtain large-scale datasets of instructional conversations. Vicuna <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref>, another popular variant, is the first to adopt ShareGPT with 70k user-shared ChatGPT conversations. Unlike previous SFT studies that treat all training data uniformly, we strive to maximize the use of mixed-quality data.</p><p>Reinforcement Learning Fine-tuning for LLMs. To better align with preferences beyond SFT, RLFT methods have been proposed. The most well-known method is RLHF <ref type="bibr" target="#b21">(Ouyang et al., 2022)</ref>, which involves collecting preference feedback from humans to train reward models. Subsequently, Proximal Policy Optimization (PPO) is used to train the target LLM to maximize the reward given.</p><p>Most API-based LLMs, such as <ref type="bibr">GPT-4, ChatGPT (OpenAI, 2023)</ref>, and open-source models like Llama-2-chat series <ref type="bibr">(Touvron et al., 2023b)</ref>, utilize RLHF techniques. However, RLHF is a complex and unstable process that involves training a reward model and the LLM with an RL objective. As a result, simpler alternatives like DPO <ref type="bibr" target="#b26">(Rafailov et al., 2023)</ref> and RRHF <ref type="bibr" target="#b36">(Yuan et al., 2023)</ref> have been proposed. DPO trains the LLM to predict and maximize reward simultaneously in a one-staged manner, while RRHF uses a ranking loss to encourage preferred answer output. Considering that the preference data is costly to collect, our method uses easily collectible and mixed-quality training data without any preference labels to finetune LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present OpenChat, an innovative framework featuring the Conditioned-RLFT method, tailored to advance open-source language models with mixed-quality data. Our model, openchat-13b, delivers the highest average performance on extensive benchmarks among all 13b open-source language models, demonstrating notable advantages such as simplicity, RL-free training, and minimal reward quality requirements. Despite these encouraging results, we acknowledge potential research areas for further improvement. Firstly, our assumption of different quality according to data sources may be overly simplistic, and the assigned coarse-grained rewards could be more finely tuned to reflect the actual quality of each data point. Secondly, while our model primarily focuses on enhancing instruction-following capabilities, exploring the application of OpenChat towards improving the reasoning abilities of LLMs offers a promising avenue for future work.</p><p>A EFFECTS OF CLASS-CONDITIONED PROMPT TOKENS We further detect the effects of class-conditioned prompt tokens on model performance during the training phase.</p><p>Our designed class-conditioned prompt tokens in different positions are shown in Table <ref type="table">3</ref>. we attempt three distinct initial prompt tokens in different positions: before speaker, before assistant, and beginning. The results are shown in Fig. <ref type="figure" target="#fig_5">8</ref>. We observe that putting the conditioned prompt tokens in every turn (either before the speaker or before the assistant) performs similarly, but adding the condition only once at the beginning of the conversation performs much worse. This may be due to LLMs tend to forget the prompt at the beginning when the context is long or during subsequent turns. <ref type="bibr">Touvron et al. (2023b)</ref> also observe the gradual loss of multi-turn consistency when the system prompt is put at the beginning. Therefore, we repeat the condition prompt every turn, to improve the effect of class-conditioned policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources Types Conditioned Prompts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-4</head><p>Before speaker GPT4 User: Question&lt;|end of turn|&gt;GPT4 Assistant:</p><p>Before assistant User: Question&lt;|end of turn|&gt;GPT4 Assistant:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beginning</head><p>Assistant is GPT4&lt;|end of turn|&gt;User: Question&lt;|end of turn|&gt;Assistant:</p><p>GPT-3.5</p><p>Before speaker GPT3 User: Question&lt;|end of turn|&gt;GPT3 Assistant:</p><p>Before assistant User: Question&lt;|end of turn|&gt;GPT3 Assistant:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beginning</head><p>Assistant is GPT3&lt;|end of turn|&gt;User: Question&lt;|end of turn|&gt;Assistant:</p><p>Table <ref type="table">3</ref>: The attempted conditioned prompts during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DERIVATION OF THE OPTIMAL POLICY IN C-RLFT</head><p>The goal of C-RLFT is to find the optimal KL-regularized conditional policy. This optimization problem can be formulated as:</p><formula xml:id="formula_6">max ? E y?? [r c (x, y)] -?D KL (?, ? c )<label>(7)</label></formula><p>To ensure ? is a valid probability distribution, we add the normalization constraint and the optimization problem becomes:</p><formula xml:id="formula_7">max ? E y?? [r c (x, y)] -?D KL (?, ? c ) (8) s.t. y ?(y|x, c)dy = 1<label>(9)</label></formula><p>We can obtain the optimal solution of this constrained optimization problem by solving its Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian of this problem is:</p><formula xml:id="formula_8">L(?, ?) = E y?? [r c (x, y)] -?D KL (?, ? c ) + ?(1 - y ?(y|x, c)dy)<label>(10)</label></formula><p>Following the KKT conditions, we take derivatives of L with respect to ? and ?, and set them to zero:</p><formula xml:id="formula_9">?L ?? = r c (x, y) + ? log ?(y|x, c) -? log ? c (y|x, c) + ? -? = 0 (11) ?L ?? = 1 - y ?(y|x, c)dy = 0<label>(12)</label></formula><p>Solving these equations gives us the optimal policy ? * :</p><formula xml:id="formula_10">? * (y|x, c) = 1 Z(x, c) ? c (y|x, c) exp 1 ? r c (x, y) (13) Z(x, c) = y ? c (y|x, c) exp 1 ? r c (x, y) dy<label>(14)</label></formula><p>where Z(x, c) is a normalization term ensuring that ? * (y|x, c) is a valid probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF BENCHMARKS</head><p>This section provides the specifics of the benchmarks employed in our study:</p><p>? AlpacaEval <ref type="bibr" target="#b17">(Li et al., 2023)</ref>: This benchmark primarily assesses the model's ability to comprehend and execute user instructions. It incorporates a test set of 805 user instructions, collected from a diverse array of sources, and corresponding reference responses from text-davinci-003. ? MT-bench <ref type="bibr" target="#b36">(Zheng et al., 2023)</ref>: MT-bench presents a rigorous multi-turn benchmark designed to test both conversational and instruction-following capabilities. It includes 80 high-quality multiturn questions that span eight distinct topics: writing, roleplay, extraction, reasoning, mathematics, coding, knowledge I (STEM), and knowledge II (humanities/social science). ? Vicuna-bench <ref type="bibr" target="#b5">(Chiang et al., 2023)</ref>: This benchmark evaluates the proficiency of large language models across eight question categories: generic, knowledge, roleplay, commonsense, Fermi problems, counterfactual scenarios, coding, mathematics, and writing. ? AGIEval <ref type="bibr" target="#b39">(Zhong et al., 2023)</ref>: AGIEval is a collection of human-centric standardized tests aimed at gauging the problem-solving abilities of language models. We include all English multiplechoice tasks in our evaluation, which encompass general college admission tests (SAT, AQuA-RAT), law school admission tests (LSAT), and civil service exams (LogiQA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MODEL INFORMATION</head><p>Table <ref type="table">4</ref> presents the detailed specifications of the models used in our study, including the base models, context length, finetuning methods, and datasets employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Base Model Context Finetuning Data</head><p>Larger than 13b</p><formula xml:id="formula_11">gpt-4 - 8k SFT + RLFT - claude - 9k SFT + RLFT - gpt-3.5-turbo - 4k SFT + RLFT - llama-2-chat-70b llama-2-70b 4k SFT + RLFT ?27k high-quality SFT data + ?2.9M preference guanaco-65b llama-65b 2k SFT ?9k OASST1 guanaco-33b llama-33b 2k SFT ?9k OASST1 Equal to 13b vicuna-v1.1-13b llama-13b 2k SFT ?70k ShareGPT wizardlm-v1.0-13b llama-13b 2k SFT ?70k gpt-3.5-turbo ultralm-13b llama-13b 2k SFT ?1.5M UltraChat llama-2-chat-13b llama-2-13b 4k SFT + RLFT ?27k high-quality SFT data + ?2.9M preference vicuna-v1.5-13b llama-2-13b 4k SFT ?125k ShareGPT wizardlm-v1.2-13b llama-2-13b 4k SFT ?250k gpt-3.5-turbo openchat-13b (ours) llama-2-13b 4k C-RLFT ?70k ShareGPT</formula><p>Table <ref type="table">4</ref>: The details of the proposed OpenChat series models and other popular language models. The RLFT, SFT, and C-RLFT indicate reinforcement learning fine-tuning, supervised fine-tuning, and conditioned-RLFT proposed in our paper, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E AGIEVAL RESULTS</head><p>Table <ref type="table">5</ref> presents the comprehensive results of AGIEval performance. All models are assessed using the official AGIEval zero-shot prompt and answer matching as described in <ref type="bibr" target="#b39">Zhong et al. (2023)</ref>. Table <ref type="table">5</ref>: Zero-shot performance on AGIEval benchmark.</p><p>For conversation models (excluding llama-2-13b), we utilize the corresponding conversation templates and set the zero-shot prompt as the user's question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F CONVERSATION QUALITY SCORING</head><p>We employ gpt-4 to automatically score the quality of conversations in the ShareGPT dataset on a scale of 1-10. For multi-turn conversations, each assistant's response is scored independently, taking into account the context of previous turns. The conversation's score is the average of all turn scores. The scoring prompt template is shown in Table <ref type="table" target="#tab_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring prompt template [Instruction]</head><p>Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the conversation displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "Rating: <ref type="bibr">[[5]</ref>]".</p><p>[  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G EVALUATORS CONSISTENCY</head><p>The correlations between arbitrary two models among gpt-4, gpt-3.5 and claude-2 are shown in Fig. <ref type="figure">9</ref>.</p><p>Figure <ref type="figure">9</ref>: The consistency of three evaluators, GPT-4, GPT-3.5, Claude-2 in AlpacaEval benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H CASE STUDY</head><p>vicuna-13b-v1.5 [Turn #1] Craft an intriguing opening paragraph for a fictional short story. The story should involve a character who wakes up one morning to find that they can time travel.</p><p>As the sun began to rise, Sarah stirred in her bed, the sound of birds chirping outside her window. But as she opened her eyes, she was not greeted by the familiar sight of her bedroom. Instead, she found herself standing in the middle of a bustling city street, the sound of cars and people rushing past her. Confused and disoriented, she looked around, trying to make sense of where she was and how she got there. And then she saw it -a shimmering aura around her, and a faint buzzing in her ears. It was then that she realized she had time traveled.</p><p>[Turn #2] Summarize the story with three bullet points using only nouns and adjectives, without verbs. * Sarah wakes up in a bustling city street * She discovers she has time traveled * She is disoriented and confused ultralm-13b [Turn #1] Craft an intriguing opening paragraph for a fictional short story. The story should involve a character who wakes up one morning to find that they can time travel.</p><p>It was a typical morning when the alarm screeched through the room, jolting the sleeper awake. As they rubbed their eyes and reached for their glasses, they noticed something peculiar. The clock on the bedside table had stopped ticking. Yet, the room was bathed in a soft, golden light, and the air felt different. Suddenly, the sleeper felt a strange sensation in their chest, and before they knew it, they were hurtling through time.</p><p>[Turn #2] Summarize the story with three bullet points using only nouns and adjectives, without verbs. As I lay in bed, I felt a strange sensation wash over me, like the whisper of a breeze on a summer day. I opened my eyes to find myself in a world that was both familiar and yet, utterly foreign. The clock on my nightstand read 6:45 AM, but something felt off. The light pouring through the window seemed warmer, more golden than it should be. And then, I felt it -the sudden jolt of time travel. I blinked, and when I opened my eyes again, I was somewhere else entirely. The world outside my window was not the one I had left just moments before. I sat up, rubbing my eyes, trying to make sense of the strange sensation that had washed over me. How had I traveled through time? And where was I now? The possibilities were endless, and I couldn't wait to explore this new world and all its wonders.</p><p>[Turn #2] Summarize the story with three bullet points using only nouns and adjectives, without verbs. Sure, here are three bullet points summarizing the story using only nouns and adjectives, without verbs: * Character wakes up with strange sensation * Golden light pours in through window * Unfamiliar world outside with endless possibilities openchat-13b [Turn #1] Craft an intriguing opening paragraph for a fictional short story. The story should involve a character who wakes up one morning to find that they can time travel.</p><p>As the first rays of sunlight peeked through the window, Sarah closed her eyes, relishing the warmth that caressed her face. She never could resist the allure of a beautiful morning, but this particular day was about to become anything but ordinary. With a start, she sat up in bed, her heart pounding, as she realized she was somewhere else, somewhere she had never been before. Confused and exhilarated, Sarah discovered that she had the extraordinary ability to time travel, her very first morning blessed -or perhaps cursed -with the power to traverse the fabric of time.</p><p>[Turn #2] Summarize the story with three bullet points using only nouns and adjectives, without verbs. *Sarah, a time traveler *Mysterious morning adventure *Discovery of extraordinary ability Question How long will it take to walk around the world, assuming that would be possible?</p><p>vicuna-13b-v1.5 It is not possible to walk around the world, as the distance around the globe is much greater than the distance across it. The circumference of the Earth at the equator is approximately 40,075 kilometers (24,901 miles), while the diameter is about 12,742 kilometers (7,918 miles).</p><p>Assuming you could walk continuously without rest, it would take approximately 1,900 years to walk around the world at an average walking speed of 5 kilometers (3.1 miles) per hour. However, this is just an estimate and does not take into account factors such as the terrain, altitude, and other conditions that would affect walking speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ultralm-13b</head><p>Walking around the world is not possible as the Earth is not a perfect sphere and has different land masses making it impossible to walk continuously without gaps. However, if we assume that there were no land masses and the Earth was a perfect sphere, it would take approximately 43,288.89 miles (69,617.05 kilometers) to walk around the equator. If we were to walk around the circumference of the Earth, it would take approximately 24,901.26 miles (40,074.91 kilometers). However, if we were to walk around the world in a straight line, it would take approximately 24,859.82 miles (40,008.06 kilometers) as the Earth is not a perfect sphere and has an elliptical shape. These distances are based on the equator circumference and the circumference of the Earth, respectively. The time it would take to walk around the world would depend on the individual's walking speed and other factors such as stops for rest and food.</p><p>wizardlm-13b-v1.2 As an AI, I'm assuming you're asking about walking around the Earth's equator. The distance around the Earth's equator at the equator itself is about 40,075 kilometers (24,901 miles). However, the actual distance would be much greater if you were to walk along a route that follows coastlines, navigates through mountains, and crosses other obstacles. Now, let's consider that a person walks at a moderate pace of about 5 kilometers per hour. Walking around the world would then take approximately 8,015 hours or approximately 333,600 minutes.</p><p>To convert this into days, you would divide the total number of minutes by 24 (hours in a day): 333,600 minutes / 24 = 14,000 days.</p><p>So, if you were to walk around the world, it would take you approximately 14,000 days or approximately 38.89 years, assuming you walk continuously without rest and no obstacles. This is an estimate based on the distance around the Earth's equator and does not take into account real-world challenges like mountains, oceans, and other obstacles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>llama-2-chat-13b</head><p>Thank you for your question! While it is not possible to walk around the entire world, as there are many areas that are not accessible by foot, such as oceans and other bodies of water, it's important to note that even if it were possible, it would be a very long and challenging journey.</p><p>The circumference of the Earth at the equator is approximately 40,075 kilometers (24,901 miles). However, it's important to consider that walking is not a viable mode of transportation for long distances, as it would be extremely time-consuming and would require a tremendous amount of physical endurance.</p><p>Furthermore, it's important to note that walking through certain areas of the world, such as deserts or other remote regions, could be dangerous due to harsh weather conditions, lack of resources, and other hazards.</p><p>Therefore, it's not possible to provide an accurate estimate of how long it would take to walk around the world, as it is not a feasible or safe journey. It's important to prioritize safe and accessible forms of transportation when traveling, and to plan your journeys with careful consideration of your physical limitations and the resources available to you. Is there anything else I can help with?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>openchat-13b</head><p>It is possible to walk around the world, though it would be a continuous, grueling journey that would take several years, if not a decade or more, depending on the route chosen, individual physical capabilities, and other factors. The distance around the world varies depending on the route, but a commonly used measure is the equatorial circumference of the Earth, which is approximately 24,901.48 miles (40,075.01 kilometers).</p><p>Walking at a moderate pace of around 3 miles (4.8 kilometers) per hour, it would take an average person nearly 3 years and 3 months to walk around the world (assuming no breaks or rest days). However, this is just an estimate, and the actual time required would depend on various factors, such as:</p><p>1. Route: The distance around the world varies depending on the route taken. For example, the shortest distance is along the Equator, while the longest distance is along the meridians of longitude. 2. Terrain: Walking through mountainous or heavily forested areas may be slower and more challenging than walking on flat terrain. 3. Weather: Adverse weather conditions, such as extreme heat or cold, high winds, or heavy rainfall, can make walking more difficult and slow down progress. 4. Fitness and endurance: An individual's physical fitness, endurance, and ability to withstand the rigors of constant walking will affect their pace and overall journey time. 5. Nutrition and rest: Proper nutrition and adequate rest are essential for maintaining physical and mental stamina. Taking regular breaks and rest days will prolong the journey.</p><p>Keep in mind that walking around the world is an extraordinary feat that would require extensive preparation, both physically and logistically. It's essential to consider the many challenges and potential hazards involved before embarking on such an adventure.</p><p>Table <ref type="table">8</ref>: Comparison of models on AlpacaEval benchmark. Specifically, the models ultralm-13b and llama-2-chat-13b fail to provide a direct answer, while vicuna-13b-v1.5 and wizardlm-13b-v1.2 offer estimates with significant discrepancies. openchat-13b provides a reasonable estimate, demonstrating the varying capabilities of models in addressing complex hypothetical questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed framework OpenChat with Conditioned-RLFT to advance the open-source language model fine-tuning with mixed-quality data, comparing to previous supervised fine-tuning (SFT) method and reinforcement learning fine-tuning (RLFT) method. MLE and RL denote maximum likelihood estimates and reinforcement learning, respectively.</figDesc><graphic url="image-1.png" coords="2,108.00,81.86,395.98,63.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The MT-bench score (a) and AGIEval Accuracy (b) of the proposed openchat-13b and other popular open-source language models, where the detailed performance of AGIEval in App. E.</figDesc><graphic url="image-2.png" coords="7,108.00,89.42,217.80,121.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of GPT-4 and GPT-3.5 conversations' representations in only SFT and openchat-13b.</figDesc><graphic url="image-5.png" coords="8,108.00,255.85,396.00,93.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of class-conditioned prompt tokens during inference phase.</figDesc><graphic url="image-6.png" coords="8,345.60,524.60,158.40,114.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 7: Effects of subsampling a specific class of data.</figDesc><graphic url="image-8.png" coords="9,385.20,296.27,118.80,97.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effects of class-conditioned prompt tokens during training phase.</figDesc><graphic url="image-9.png" coords="14,345.60,120.99,158.40,96.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="16,108.00,578.70,395.99,139.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The win-rate (%) performance of the proposed openchat-13b and other popular open-source language models. The competitors are text-davinci-003 in AlpacaEval, and gpt-3.5-turbo in both MT-bench and Vicuna-bench. The bold scores denote the best performance, and the underline scores indicate the second-best performance.</figDesc><table><row><cell>Models</cell><cell>Base Models</cell><cell>Method</cell><cell cols="4">AlpacaEval MT-bench Vicuna-bench Average</cell></row><row><cell></cell><cell></cell><cell cols="2">Larger than 13b</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gpt-4</cell><cell>-</cell><cell>SFT + RLFT</cell><cell>95.3</cell><cell>82.5</cell><cell>90.0</cell><cell>89.3</cell></row><row><cell>llama-2-chat-70b</cell><cell cols="2">llama-2-70b SFT + RLFT</cell><cell>92.7</cell><cell>60.0</cell><cell>87.5</cell><cell>80.1</cell></row><row><cell>claude</cell><cell>-</cell><cell>SFT + RLFT</cell><cell>88.4</cell><cell>65.0</cell><cell>76.3</cell><cell>76.6</cell></row><row><cell>gpt-3.5-turbo</cell><cell>-</cell><cell>SFT + RLFT</cell><cell>86.1</cell><cell>50.0</cell><cell>50.0</cell><cell>62.0</cell></row><row><cell>guanaco-65b</cell><cell>llama-65b</cell><cell>SFT</cell><cell>71.8</cell><cell>40.6</cell><cell>49.4</cell><cell>53.9</cell></row><row><cell>guanaco-33b</cell><cell>llama-33b</cell><cell>SFT</cell><cell>66.0</cell><cell>40.6</cell><cell>54.4</cell><cell>53.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Equal to 13b</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vicuna-v1.1-13b</cell><cell>llama-13b</cell><cell>SFT</cell><cell>70.4</cell><cell>29.4</cell><cell>45.0</cell><cell>48.3</cell></row><row><cell cols="2">wizardlm-v1.0-13b llama-13b</cell><cell>SFT</cell><cell>75.3</cell><cell>33.1</cell><cell>44.4</cell><cell>50.9</cell></row><row><cell>vicuna-v1.5-13b</cell><cell cols="2">llama-2-13b SFT</cell><cell>78.8</cell><cell>37.2</cell><cell>47.1</cell><cell>54.4</cell></row><row><cell>ultralm-13b</cell><cell>llama-13b</cell><cell>SFT</cell><cell>80.6</cell><cell>37.2</cell><cell>50.0</cell><cell>55.9</cell></row><row><cell cols="3">wizardlm-v1.2-13b llama-2-13b SFT</cell><cell>89.2</cell><cell>53.1</cell><cell>80.6</cell><cell>74.3</cell></row><row><cell>llama-2-chat-13b</cell><cell cols="2">llama-2-13b SFT + RLFT</cell><cell>81.1</cell><cell>55.3</cell><cell>86.9</cell><cell>74.4</cell></row><row><cell>openchat-13b</cell><cell cols="2">llama-2-13b C-RLFT</cell><cell>89.5</cell><cell>57.5</cell><cell>85.0</cell><cell>77.3</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p>are highly advanced LLMs developed by OpenAI; (2) claude</p>(Bai et al., 2022a)  </p>is helpful and harmless assistants by Anthropic; (3) llama-2-chat</p>(Touvron et al., 2023b)  </p>series models are the most frequently used open-source models with SFT and RLHF; (4) wizardlm</p>(Xu et al., 2023a)</p>, guanaco</p><ref type="bibr" target="#b8">(Dettmers et al., 2023)</ref></p>, ultralm</p><ref type="bibr" target="#b9">(Ding et al., 2023)</ref> </p>and vicuna</p><ref type="bibr" target="#b5">(Chiang et al., 2023)</ref> </p>are among the well-known opensource LLMs with SFT. More details are shown in App. D.</p>Automatic Evaluators. To mitigate the cost of human annotations, we follow the official evaluators according to each benchmark. Specifically, AlpacaEval employs alpaca eval gpt, while MT-bench and Vicuna-bench use gpt-4. It is worth noting that these benchmarks have already computed the human agreements to ensure reliability. Additionally, we further introduce gpt-3.5 and claude-2 to eliminate the potential self-enhancement bias in Sec. 5.4.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Conversation evaluation prompts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Craft an intriguing opening paragraph for a fictional short story. The story should involve a character who wakes up one morning to find that they can time travel. It was just another ordinary day, a day like any other, or so it seemed. But for John, it was about to become the most extraordinary day of his life. As he groggily opened his eyes, he was hit with a wave of dizziness, a feeling he had never experienced before. Confused and disoriented, he rubbed his eyes, hoping it was just a side effect of his late night binge-watching of Doctor Who. But as he looked around, he realized that something was very wrong. His room seemed different, and the sunlight streaming in through the window was at an angle he had never seen before. It was then that he realized he had somehow managed to time travel.</figDesc><table><row><cell></cell><cell>Time Traveler:</cell></row><row><cell></cell><cell>-Unexpectedly travels through time</cell></row><row><cell></cell><cell>-Struggles to control powers</cell></row><row><cell></cell><cell>-Encounters historical figures and events</cell></row><row><cell cols="2">wizardlm-13b-v1.2 [Turn #1] [Turn #2] Summarize the story with three bullet points using only nouns and adjectives, without</cell></row><row><cell></cell><cell>verbs.</cell></row><row><cell></cell><cell>* Character: John</cell></row><row><cell></cell><cell>* Time Travel</cell></row><row><cell></cell><cell>* Doctor Who</cell></row><row><cell></cell><cell>* Sunlight</cell></row><row><cell></cell><cell>* Room</cell></row><row><cell></cell><cell>* Day</cell></row><row><cell>llama-2-chat-13b</cell><cell>[Turn #1] Craft an intriguing opening paragraph for a fictional short story. The story should involve</cell></row><row><cell></cell><cell>a character who wakes up one morning to find that they can time travel.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note><p>Comparison of models on MT-bench. Only llama-2-chat-13b and openchat-13b successfully adhered to the user's request in the second turn.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The ShareGPT dataset is collected from https://sharegpt.com/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank <rs type="person">Changling Liu</rs> in <rs type="institution">GPT Desk Pte. Ltd., Qiying Yu at Tsinghua University</rs>, <rs type="person">Baochang Ma</rs>, and <rs type="person">Hao Wan</rs> in 01.AI company for their resource support. We would like to express our gratitude to <rs type="person">Jianxiong Li</rs> and <rs type="person">Peng Li</rs> at <rs type="affiliation">Tsinghua University</rs> for their valuable discussion. Finally, we are also grateful to the developers of the following projects, which have contributed significantly to our research: Llama, self-instruct, FastChat (Vicuna), Alpaca, and StarCoder.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Constitutional ai: Harmlessness from ai feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Open problems and fundamental limitations of reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xander</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Krendl</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?my</forename><surname>Scheurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Rando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Freire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15217</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Lichang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalpa</forename><surname>Gunaratna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08701</idno>
		<title level="m">Training a better alpaca with fewer data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15084" to="15097" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-03-30-vicuna/" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from human preferences. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Qlora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14314</idno>
		<title level="m">Efficient finetuning of quantized llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enhancing chat language models by scaling high-quality instructional conversations</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14233</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rvs: What is essential for offline rl via supervised learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10751</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Koala: A dialogue model for academic research</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://bair.berkeley.edu/blog/2023/04/03/koala/" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Blog post</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning rates as a function of batch size: A random matrix theory approach to neural network training</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Granziol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Zohren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:226281826" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="173" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Way off-policy batch deep reinforcement learning of implicit human preferences in dialog</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">Hanwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00456</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RL with KL penalties is better viewed as Bayesian inference</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.77</idno>
		<ptr target="https://aclanthology.org/2022.findings-emnlp.77" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="1083" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Alpacaeval: An automatic evaluator of instruction-following models</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/alpaca_eval" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Awac: Accelerating online reinforcement learning with offline datasets</title>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03277</idno>
		<title level="m">Instruction tuning with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00177</idno>
		<title level="m">Advantage-weighted regression: Simple and scalable off-policy reinforcement learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reinforcement learning by reward-weighted regression for operational space control</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18290</idno>
		<title level="m">Direct preference optimization: Your language model is secretly a reward model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">R</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cant?n</forename><surname>Cristian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michael Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><surname>Scialom</surname></persName>
		</author>
		<idno>ArXiv, abs/2307.09288</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">259950998</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01693</idno>
		<title level="m">Fine-grained human feedback gives better rewards for language model training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://github.com/hanxiao/bert-as-service" />
		<title level="m">bert-as-service</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Wizardlm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12244</idno>
		<title level="m">Empowering large language models to follow complex instructions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminator-weighted offline imitation learning from suboptimal demonstrations</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyuan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiling</forename><surname>Qin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24725" to="24742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rrhf: Rank responses to align language models with human feedback without tears</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiran</forename><surname>Huang</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.05302</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">258059818</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05685</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Agieval: A human-centric benchmark for evaluating foundation models</title>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiduo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06364</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11206</idno>
		<title level="m">Less is more for alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
