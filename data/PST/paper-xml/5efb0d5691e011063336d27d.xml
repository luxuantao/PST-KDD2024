<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 BERTOLOGY MEETS BIOLOGY: INTERPRETING ATTENTION IN PROTEIN LANGUAGE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 BERTOLOGY MEETS BIOLOGY: INTERPRETING ATTENTION IN PROTEIN LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at [supplementary-material].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The study of proteins, the fundamental macromolecules governing biology and life itself, has led to remarkable advances in understanding human health and the development of disease therapies. The decreasing cost of sequencing technology has enabled vast databases of naturally occurring proteins <ref type="bibr" target="#b12">(El-Gebali et al., 2019a)</ref>, which are rich in information for developing powerful machine learning models of protein sequences. For example, sequence models leveraging principles of co-evolution, whether modeling pairwise or higher-order interactions, have enabled prediction of structure or function <ref type="bibr" target="#b50">(Rollins et al., 2019)</ref>.</p><p>Proteins, as a sequence of amino acids, can be viewed precisely as a language and therefore modeled using neural architectures developed for natural language. In particular, the Transformer <ref type="bibr" target="#b61">(Vaswani et al., 2017)</ref>, which has revolutionized unsupervised learning for text, shows promise for similar impact on protein sequence modeling. However, the strong performance of the Transformer comes at the cost of interpretability, and this lack of transparency can hide underlying problems such as model bias and spurious correlations <ref type="bibr" target="#b40">(Niven &amp; Kao, 2019;</ref><ref type="bibr" target="#b58">Tan &amp; Celis, 2019;</ref><ref type="bibr" target="#b29">Kurita et al., 2019)</ref>. In response, much NLP research now focuses on interpreting the Transformer, e.g., the subspecialty of "BERTology" <ref type="bibr" target="#b49">(Rogers et al., 2020)</ref>, which specifically studies the BERT model <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>.</p><p>In this work, we adapt and extend this line of interpretability research to protein sequences. We analyze Transformer protein models through the lens of attention, and present a set of interpretability methods that capture the unique functional and structural characteristics of proteins. We also perform a joint, cross-layer probing analysis of attention weights and embeddings, showing that layers of the model in which knowledge is accrued in embeddings may be far removed from layers in which this information is leveraged in the attention mechanism. Finally, we present a visualization of attention contextualized within three-dimensional protein structure.</p><p>Our analysis reveals that attention captures high-level structural properties of proteins, connecting amino acids that are spatially close in three-dimensional structure, but apart in the underlying sequence (Figure <ref type="figure">1a</ref>). We also find that attention targets binding sites, a key functional component of proteins (Figure <ref type="figure">1b</ref>). Further, we show how attention is consistent with a classic measure of similarity between amino acids-the substitution matrix. Finally, we demonstrate that attention captures progressively higher-level representations of structure and function with increasing layer depth.</p><p>(a) Attention in head 12-4, which targets amino acid pairs that are close in physical space (see inset subsequence 117D-157I) but lie apart in the sequence. Example is a de novo designed TIMbarrel (5BVL) with characteristic symmetry.</p><p>(b) Attention in head 7-1, which targets binding sites, a key functional component of proteins.</p><p>Example is HIV-1 protease (7HVP). The primary location receiving attention is 27G, a binding site for protease inhibitor small-molecule drugs.</p><p>Figure <ref type="figure">1</ref>: Examples of how specialized attention heads in a Transformer recover protein structure and function, based solely on language model pre-training. Orange lines depict attention between amino acids (line width proportional to attention weight; values below 0.1 hidden). Heads were selected based on correlation with ground-truth annotations of contact maps and binding sites. Visualizations based on the NGL Viewer <ref type="bibr" target="#b52">(Rose et al., 2018;</ref><ref type="bibr" target="#b51">Rose &amp; Hildebrand, 2015;</ref><ref type="bibr" target="#b39">Nguyen et al., 2017)</ref>.</p><p>In contrast to NLP, which aims to automate a capability that humans already have-understanding natural language-protein modeling also seeks to shed light on biological processes that are not fully understood. Thus we also discuss how interpretability can aid scientific discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: PROTEINS</head><p>In this section we provide background on the biological concepts discussed in later sections. Amino acids. Just as language is composed of words from a shared lexicon, every protein sequence is formed from a vocabulary of amino acids, of which 20 are commonly observed. Amino acids may be denoted by their full name, e.g. Proline, a 3-letter abbreviation, e.g. Pro, or a single-letter code, e.g. P.</p><p>Substitution matrix. While word synonyms are encoded in a thesaurus, proteins that are similar in structure or function are captured in a substitution matrix, which scores pairs of amino acids on how readily they may be substituted for one another while maintaining protein viability. One common substitution matrix is BLOSUM <ref type="bibr" target="#b20">(Henikoff &amp; Henikoff, 1992)</ref>, which is derived from co-occurrence statistics of amino acids in aligned protein sequences.</p><p>Protein structure. Though a protein may be abstracted as a sequence of amino acids, it represents a physical entity with a well-defined three-dimensional structure (Figure <ref type="figure">1</ref>). Secondary structure describes the local segments of proteins; two commonly observed types are the alpha helix and beta sheet. Tertiary structure encompasses the large-scale formations that determine the overall shape and function of the protein. One way to characterize tertiary structure is by a contact map, which describes the pairs of amino acids that are in contact (within 8 angstroms of one another) in the folded protein structure but lie apart (by at least 6 positions) in the underlying sequence <ref type="bibr" target="#b45">(Rao et al., 2019)</ref>.</p><p>Binding sites. Proteins may also be characterized by their functional properties. Binding sites are protein regions that bind with other molecules (proteins, natural ligands, and small-molecule drugs) to carry out a specific function. For example, the HIV-1 protease is an enzyme responsible for a critical process in replication of HIV <ref type="bibr" target="#b7">(Brik &amp; Wong, 2003)</ref>. It has a binding site, shown in Figure <ref type="figure">1b</ref>, that is a target for drug development to ensure inhibition.</p><p>Post-translational modifications. After a protein is translated from RNA, it may undergo additional modifications, e.g. phosphorylation, which play a key role in protein structure and function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Model. We demonstrate our interpretability methods on five Transformer models that were pretrained through language modeling of amino acid sequences. We primarily focus on the BERT-Base model from TAPE <ref type="bibr" target="#b45">(Rao et al., 2019)</ref>, which was pretrained on Pfam, a dataset of 31M protein sequences <ref type="bibr" target="#b13">(El-Gebali et al., 2019b)</ref>. We refer to this model as TapeBert. We also analyze 4 pre-trained Transformer models from ProtTrans <ref type="bibr" target="#b14">(Elnaggar et al., 2020)</ref>: ProtBert and ProtBert-BFD, which are 30-layer, 16-head BERT models; ProtAlbert, a 12-layer, 64-head ALBERT <ref type="bibr">(Lan et al.) model, and ProtXLNet, a 30-layer, 16-head XLNet (Yang et al., 2019)</ref> model. ProtBert-BFD was pretrained on BFD <ref type="bibr" target="#b56">(Steinegger &amp; Söding, 2018)</ref>, a dataset of 2.1B protein sequences, while the other ProtTrans models were pretrained on UniRef100 <ref type="bibr" target="#b57">(Suzek et al., 2014)</ref>, which includes 216M protein sequences. A summary of these 5 models is presented in Appendix A.1.</p><p>Here we present an overview of BERT, with additional details on all models in Appendix A.2. BERT inputs a sequence of amino acids x = (x 1 , . . . , x n ) and applies a series of encoders. Each encoder layer outputs a sequence of continuous embeddings (h</p><formula xml:id="formula_0">( ) 1 , . . . , h ( )</formula><p>n ) using a multi-headed attention mechanism. Each attention head in a layer produces a set of attention weights α for an input, where α i,j &gt; 0 is the attention from token i to token j, such that j α i,j = 1. Intuitively, attention weights define the influence of every token on the next layer's representation for the current token. We denote a particular head by &lt;layer&gt;-&lt;head_index&gt;, e.g. head 3-7 for the 3rd layer's 7th head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention analysis.</head><p>We analyze how attention aligns with various protein properties. For properties of token pairs, e.g. contact maps, we define an indicator function f (i, j) that returns 1 if the property is present in token pair (i, j) (e.g., if amino acids i and j are in contact), and 0 otherwise. We then compute the proportion of high-attention token pairs (α i,j &gt; θ) where the property is present, aggregated over a dataset X:</p><formula xml:id="formula_1">p α (f ) = x∈X |x| i=1 |x| j=1 f (i, j) • 1 αi,j &gt;θ x∈X |x| i=1 |x| j=1 1 αi,j &gt;θ (1)</formula><p>where θ is a threshold to select for high-confidence attention weights. We also present an alternative, continuous version of this metric in Appendix B.1.</p><p>For properties of individual tokens, e.g. binding sites, we define f (i, j) to return 1 if the property is present in token j (e.g. if j is a binding site). In this case, p α (f ) equals the proportion of attention that is directed to the property (e.g. the proportion of attention focused on binding sites).</p><p>When applying these metrics, we include two types of checks to ensure that the results are not due to chance. First, we test that the proportion of attention that aligns with particular properties is significantly higher than the background frequency of these properties, taking into account the Bonferroni correction for multiple hypotheses corresponding to multiple attention heads. Second, we compare the results to a null model, which is an instance of the model with randomly shuffled attention weights. We describe these methods in detail in Appendix B.2.</p><p>Probing tasks. We also perform probing tasks on the model, which test the knowledge contained in model representations by using the them as inputs to a classifier that predicts a property of interest <ref type="bibr" target="#b62">(Veldhoen et al., 2016;</ref><ref type="bibr" target="#b10">Conneau et al., 2018;</ref><ref type="bibr" target="#b0">Adi et al., 2016)</ref>. The performance of the probing classifier serves as a measure of the knowledge of the property that is encoded in the representation. We run both embedding probes, which assess the knowledge encoded in the output embeddings of each layer, and attention probes <ref type="bibr" target="#b46">(Reif et al., 2019;</ref><ref type="bibr" target="#b9">Clark et al., 2019)</ref>, which measure the knowledge contained in the attention weights for pairwise features. Details are provided in Appendix B.3.</p><p>Datasets. For our analyses of amino acids and contact maps, we use a curated dataset from TAPE based on ProteinNet <ref type="bibr" target="#b2">(AlQuraishi, 2019;</ref><ref type="bibr" target="#b17">Fox et al., 2013;</ref><ref type="bibr" target="#b5">Berman et al., 2000;</ref><ref type="bibr" target="#b38">Moult et al., 2018)</ref>, which contains amino acid sequences annotated with spatial coordinates (used for the contact map analysis). For the analysis of secondary structure and binding sites we use the Secondary Structure dataset <ref type="bibr" target="#b45">(Rao et al., 2019;</ref><ref type="bibr" target="#b5">Berman et al., 2000;</ref><ref type="bibr" target="#b38">Moult et al., 2018;</ref><ref type="bibr" target="#b27">Klausen et al., 2019)</ref> from TAPE. We considered a more fine-grained taxonomy of secondary structure with three categories: Helix, Strand, and Turn/Bend, with the last two belonging to the higher-level beta sheet category (Sec. 2). We used this taxonomy in to study how the model understood structurally distinct regions of beta sheets. We obtained token-level binding site and protein modification labels from the Protein Data Bank <ref type="bibr" target="#b5">(Berman et al., 2000)</ref>. For analyzing attention, we used a random subset of 5000 sequences   <ref type="figure">-e</ref>). The heatmaps show the proportion of high-confidence attention weights (α i,j &gt; θ) from each head that connects pairs of amino acids that are in contact with one another. In TapeBert (a), for example, we can see that 45% of attention in head 12-4 (the 12th layer's 4th head) maps to contacts. The bar plots show the maximum value from each layer. Note that the vertical striping in ProtAlbert (b) is likely due to cross-layer parameter sharing (see Appendix A.3).</p><p>from the training split of the respective datasets (note that none of the aforementioned annotations were used in model training). For the diagnostic classifier, we used the respective training splits for training and the validation splits for evaluation. See Appendix B.4 for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental details</head><p>We exclude attention to the [SEP] delimiter token, as it has been shown to be a "no-op" attention token <ref type="bibr" target="#b9">(Clark et al., 2019)</ref>, as well as attention to the [CLS] token, which is not explicitly used in language modeling. We only include results for attention heads where at least 100 high-confidence attention arcs are available for analysis. We set the attention threshold θ to 0.3 to select for high-confidence attention while retaining sufficient data for analysis. We truncate all protein sequences to a length of 512 to reduce memory requirements. <ref type="foot" target="#foot_0">1</ref>We note that all of the above analyses are purely associative and do not attempt to establish a causal link between attention and model behavior <ref type="bibr" target="#b65">(Vig et al., 2020;</ref><ref type="bibr" target="#b19">Grimsley et al., 2020)</ref>, nor to explain model predictions <ref type="bibr" target="#b24">(Jain &amp; Wallace, 2019;</ref><ref type="bibr" target="#b67">Wiegreffe &amp; Pinter, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WHAT DOES ATTENTION UNDERSTAND ABOUT PROTEINS?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PROTEIN STRUCTURE</head><p>Here we explore the relationship between attention and tertiary structure, as characterized by contact maps (see Section 2). Secondary structure results are included in Appendix C.1.</p><p>Attention aligns strongly with contact maps in the deepest layers. Figure <ref type="figure" target="#fig_0">2</ref> shows how attention aligns with contact maps across the heads of the five models evaluated<ref type="foot" target="#foot_1">2</ref> , based on the metric defined in Equation 1. The most aligned heads are found in the deepest layers and focus up to 44.7% (TapeBert), 55.7% (ProtAlbert), 58.5% (ProtBert), 63.2% (ProtBert-BFD), and 44.5% (ProtXLNet) of attention on contacts, whereas the background frequency of contacts among all amino acid pairs in the dataset is 1.3%. Figure <ref type="figure">1a</ref> shows an example of the induced attention from the top head in TapeBert. We note that the model with the single most aligned head-ProtBert-BFD-is the largest model (same size as ProteinBert) at 420M parameters (Appendix A.1) and it was also the only model pre-trained on the Figure <ref type="figure">3</ref>: Proportion of attention focused on binding sites across five pretrained models. The heatmaps show the proportion of high-confidence attention (α i,j &gt; θ) from each head that is directed to binding sites. In TapeBert (a), for example, we can see that 49% of attention in head 11-6 (the 11th layer's 6th head) is directed to binding sites. The bar plots show the maximum value from each layer. largest dataset, BFD. It's possible that both factors helped the model learn more structurally-aligned attention patterns. Statistical significance tests and null models are reported in Appendix C.2.</p><p>Considering the models were trained on language modeling tasks without any spatial information, the presence of these structurally-aware attention heads is intriguing. One possible reason for this emergent behavior is that contacts are more likely to biochemically interact with one another, creating statistical dependencies between the amino acids in contact. By focusing attention on the contacts of a masked position, the language models may acquire valuable context for token prediction.</p><p>While there seems to be a strong correlation between the attention head output and classically-defined contacts, there are also differences. The models may have learned differing contextualized or nuanced formulations that describes amino acid interactions. These learned interactions could then be used for further discovery and investigation or repurposed for prediction tasks similar to how principles of coevolution enabled a powerful representation for structure prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BINDING SITES AND POST-TRANSLATIONAL MODIFICATIONS</head><p>We also analyze how attention interacts with binding sites and post-translational modifications (PTMs), which both play a key role in protein function.</p><p>Attention targets binding sites throughout most layers of the models. Figure <ref type="figure">3</ref> shows the proportion of attention focused on binding sites (Eq. 1) across the heads of the 5 models studied. Attention to binding sites is most pronounced in the ProtAlbert model (Figure <ref type="figure">3b</ref>), which has 22 heads that focus over 50% of attention on bindings sites, whereas the background frequency of binding sites in the dataset is 4.8%. The three BERT models (Figures <ref type="figure">3a, 3c, and 3d</ref>) also attend strongly to binding sites, with attention heads focusing up to 48.2%, 50.7%, and 45.6% of attention on binding sites, respectively. Figure <ref type="figure">1b</ref> visualizes the attention in one strongly-aligned head from the TapeBert model. Statistical significance tests and a comparison to a null model are provided in Appendix C.3.</p><p>ProtXLNet (Figure <ref type="figure">3e</ref>) also targets binding sites, but not as strongly as the other models: the most aligned head focuses 15.1% of attention on binding sites, and the average head directs just 6.2% of attention to binding sites, compared to 13.2%, 19.8%, 16.0%, and 15.1% for the first four models in Figure <ref type="figure">3</ref>. It's unclear whether this disparity is due to differences in architectures or pre-training objectives; for example, ProtXLNet uses a bidirectional auto-regressive pretraining method (see Appendix A.2), whereas the other 4 models all use masked language modeling objectives. Why does attention target binding sites? In contrast to contact maps, which reveal relationships within proteins, binding sites describe how a protein interacts with other molecules. These external interactions ultimately define the high-level function of the protein, and thus binding sites remain conserved even when the sequence as a whole evolves <ref type="bibr" target="#b26">(Kinjo &amp; Nakamura, 2009)</ref>. Further, structural motifs in binding sites are mainly restricted to specific families or superfamilies of proteins <ref type="bibr" target="#b26">(Kinjo &amp; Nakamura, 2009)</ref>, and binding sites can reveal evolutionary relationships among proteins <ref type="bibr" target="#b31">(Lee et al., 2017)</ref>. Thus binding sites may provide the model with a high-level characterization of the protein that is robust to individual sequence variation. By attending to these regions, the model can leverage this higher-level context when predicting masked tokens throughout the sequence.</p><p>Attention targets PTMs in a small number of heads. A small number of heads in each model concentrate their attention very strongly on amino acids associated with post-translational modifications (PTMs). For example, Head 11-6 in TapeBert focused 64% of attention on PTM positions, though these occur at only 0.8% of sequence positions in the dataset. <ref type="foot" target="#foot_2">3</ref> Similar to our discussion on binding sites, PTMs are critical to protein function <ref type="bibr" target="#b53">(Rubin &amp; Rosen, 1975)</ref> and thereby are likely to exhibit behavior that is conserved across the sequence space. See Appendix C.4 for full results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CROSS-LAYER ANALYSIS</head><p>We analyze how attention captures properties of varying complexity across different layers of TapeBert, and compare to a probing analysis of embeddings and attention weights (see Section 3).</p><p>Attention targets higher-level properties in deeper layers. As shown in Figure <ref type="figure">4</ref>, deeper layers focus relatively more attention on binding sites and contacts (high-level concept), whereas secondary structure (low-to mid-level concept) is targeted more evenly across layers. The probing analysis of attention (Figure <ref type="figure">5</ref>, blue) similarly shows that knowledge of contact maps (a pairwise feature)  The embedding probes (Figure <ref type="figure">5</ref>, orange) also show that the model first builds representations of local secondary structure in lower layers before fully encoding binding sites and contact maps in deeper layers. However, this analysis also reveals stark differences in how knowledge of contact maps is accrued in embeddings, which accumulate this knowledge gradually over many layers, compared to attention weights, which acquire this knowledge only in the final layers in this case. This example points out limitations of common layerwise probing approaches that only consider embeddings, which, intuitively, represent what the model knows but not necessarily how it operationalizes that knowledge.</p><formula xml:id="formula_2">A C D E F G H I K L M N P Q R S T V W Y A C D E F G H I K L M N P Q R S T V W Y 0.4 0.2 0.0 0.2 0.4 0.6 0.8 A C D E F G H I K L M N P Q R S T V W Y A C D E F G H I K L M N P Q R S T V W Y<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AMINO ACIDS AND THE SUBSTITUTION MATRIX</head><p>In addition to high-level structural and functional properties, we also performed a fine-grained analysis of the interaction between attention and particular amino acids.</p><p>Attention heads specialize in particular amino acids. We computed the proportion of TapeBert's attention to each of the 20 standard amino acids, as shown in Figure <ref type="figure" target="#fig_2">6</ref> for two example amino acids.</p><p>For 16 of the amino acids, there exists an attention head that focuses over 25% of attention on that amino acid, significantly greater than the background frequencies of the corresponding amino acids, which range from 1.3% to 9.4%. Similar behavior was observed for ProtBert, ProtBert-BFD, ProtAlbert, and ProtXLNet models, with 17, 15, 16, and 18 amino acids, respectively, receiving greater than 25% of the attention from at least one attention head. Detailed results for TapeBert including statistical significance tests and comparison to a null model are presented in Appendix C.5.</p><p>Attention is consistent with substitution relationships. A natural follow-up question from the above analysis is whether each head has "memorized" specific amino acids to target, or whether it has actually learned meaningful properties that correlate with particular amino acids. To test the latter hypothesis, we analyze whether amino acids with similar structural and functional properties are attended to similarly across heads. Specifically, we compute the Pearson correlation between the distribution of attention across heads between all pairs of distinct amino acids, as shown in Figure <ref type="figure" target="#fig_3">7</ref> (left) for TapeBert. For example, the entry for Pro (P) and Phe (F) is the correlation between the two heatmaps in Figure <ref type="figure" target="#fig_2">6</ref>. We compare these scores to the BLOSUM62 substitution scores (Sec. 2) in Figure <ref type="figure" target="#fig_3">7</ref> (right), and find a Pearson correlation of 0.73, suggesting that attention is moderately consistent with substitution relationships. Similar correlations are observed for the ProtTrans models: 0.68 (ProtBert), 0.75 (ProtBert-BFD), 0.60 (ProtAlbert), and 0.71 (ProtXLNet). As a baseline, the randomized versions of these models (Appendix B.2) yielded correlations of -0.02 (TapeBert), 0.02 (ProtBert), -0.03 (ProtBert-BFD), -0.05 (ProtAlbert), and 0.21 (ProtXLNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PROTEIN LANGUAGE MODELS</head><p>Deep neural networks for protein language modeling have received broad interest. Early work applied the Skip-gram model <ref type="bibr" target="#b36">(Mikolov et al., 2013)</ref> to construct continuous embeddings from protein sequences <ref type="bibr" target="#b3">(Asgari &amp; Mofrad, 2015)</ref>. Sequence-only language models have since been trained through autoregressive or autoencoding self-supervision objectives for discriminative and generative tasks, for example, using LSTMs or Transformer-based architectures <ref type="bibr" target="#b1">(Alley et al., 2019;</ref><ref type="bibr" target="#b4">Bepler &amp; Berger, 2019;</ref><ref type="bibr" target="#b45">Rao et al., 2019;</ref><ref type="bibr" target="#b48">Rives et al., 2019)</ref>. TAPE created a benchmark of five tasks to assess protein sequence models, and ProtTrans also released several large-scale pretrained protein Transformer models <ref type="bibr" target="#b14">(Elnaggar et al., 2020)</ref>. Riesselman et al. ( <ref type="formula">2019</ref>); <ref type="bibr" target="#b34">Madani et al. (2020)</ref> trained autoregressive generative models to predict the functional effect of mutations and generate natural-like proteins.</p><p>From an interpretability perspective, <ref type="bibr" target="#b48">Rives et al. (2019)</ref> showed that the output embeddings from a pretrained Transformer can recapitulate structural and functional properties of proteins through learned linear transformations. Various works have analyzed output embeddings of protein models through dimensionality reduction techniques such as PCA or t-SNE <ref type="bibr" target="#b14">(Elnaggar et al., 2020;</ref><ref type="bibr" target="#b6">Biswas et al., 2020)</ref>. In our work, we take an interpretability-first perspective to focus on the internal model representations, specifically attention and intermediate hidden states, across multiple protein language models. We also explore novel biological properties including binding sites and post-translational modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">INTERPRETING MODELS IN NLP</head><p>The rise of deep neural networks in ML has also led to much work on interpreting these so-called black-box models. This section reviews the NLP interpretability literature on the Transformer model, which is directly comparable to our work on interpreting Transformer models of protein sequences.</p><p>Interpreting Transformers. The Transformer is a neural architecture that uses attention to accelerate learning <ref type="bibr" target="#b61">(Vaswani et al., 2017)</ref>. In NLP, transformers are the backbone of state-of-the-art pre-trained language models such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. BERTology focuses on interpreting what the BERT model learns about language using a suite of probes and interventions <ref type="bibr" target="#b49">(Rogers et al., 2020)</ref>. So-called diagnostic classifiers are used to interpret the outputs from BERT's layers <ref type="bibr" target="#b62">(Veldhoen et al., 2016)</ref>. At a high level, mechanisms for interpreting BERT can be placed into three main categories: interpreting the learned embeddings <ref type="bibr" target="#b15">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b66">Wiedemann et al., 2019;</ref><ref type="bibr" target="#b35">Mickus et al., 2019;</ref><ref type="bibr" target="#b0">Adi et al., 2016;</ref><ref type="bibr" target="#b10">Conneau et al., 2018</ref>), BERT's learned knowledge of syntax <ref type="bibr" target="#b32">(Lin et al., 2019;</ref><ref type="bibr" target="#b33">Liu et al., 2019;</ref><ref type="bibr" target="#b59">Tenney et al., 2019;</ref><ref type="bibr" target="#b23">Htut et al., 2019;</ref><ref type="bibr" target="#b21">Hewitt &amp; Manning, 2019;</ref><ref type="bibr" target="#b18">Goldberg, 2019)</ref>, and BERT's learned knowledge of semantics <ref type="bibr" target="#b59">(Tenney et al., 2019;</ref><ref type="bibr" target="#b16">Ettinger, 2020)</ref>.</p><p>Interpreting attention specifically. Interpreting attention on textual sequences is a wellestablished area of research <ref type="bibr" target="#b67">(Wiegreffe &amp; Pinter, 2019;</ref><ref type="bibr" target="#b69">Zhong et al., 2019;</ref><ref type="bibr" target="#b8">Brunner et al., 2020;</ref><ref type="bibr" target="#b21">Hewitt &amp; Manning, 2019)</ref>. Past work has been shown that attention correlates with syntactic and semantic relationships in natural language in some cases <ref type="bibr" target="#b9">(Clark et al., 2019;</ref><ref type="bibr" target="#b64">Vig &amp; Belinkov, 2019;</ref><ref type="bibr" target="#b23">Htut et al., 2019)</ref>. Depending on the task and model architecture, attention may have less or more explanatory power for model predictions <ref type="bibr" target="#b24">(Jain &amp; Wallace, 2019;</ref><ref type="bibr" target="#b55">Serrano &amp; Smith, 2019;</ref><ref type="bibr" target="#b43">Pruthi et al., 2020;</ref><ref type="bibr" target="#b37">Moradi et al., 2019;</ref><ref type="bibr" target="#b60">Vashishth et al., 2019)</ref>. Visualization techniques have been used to convey the structure and properties of attention in Transformers <ref type="bibr" target="#b61">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b28">Kovaleva et al., 2019;</ref><ref type="bibr" target="#b22">Hoover et al., 2019;</ref><ref type="bibr" target="#b63">Vig, 2019)</ref>. Recent work has begun to analyze attention in Transformer models outside of the domain of natural language <ref type="bibr">(Schwaller et al., 2020;</ref><ref type="bibr" target="#b41">Payne et al., 2020)</ref>.</p><p>Our work extends these methods to protein sequence models by considering particular biophysical properties and relationships. We also present a joint cross-layer probing analysis of attention weights and layer embeddings. While past work in NLP has analyzed attention and embeddings across layers, we believe we are the first to do so in any domain using a single, unified metric, which enables us to directly compare the relative information content of the two representations. Finally, we present a novel tool for visualizing attention embedded in three-dimensional structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>This paper builds on the synergy between NLP and computational biology by adapting and extending NLP interpretability methods to protein sequence modeling. We show how a Transformer language model recovers structural and functional properties of proteins and integrates this knowledge directly into its attention mechanism. While this paper focuses on reconciling attention with known properties of proteins, one might also leverage attention to uncover novel relationships or more nuanced forms of existing measures such as contacts maps, as discussed in Section 4.1. In this way, language models have the potential to serve as tools for scientific discovery. But in order for learned representations to be accessible to domain experts, they must be presented in an appropriate context to facilitate discovery. Visualizing attention in the context of protein structure (Figure <ref type="figure">1</ref>) is one attempt to do so.</p><p>We believe there is the potential to develop such contextual visualizations of learned representations in a range of scientific domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MODEL OVERVIEW</head><p>A.1 PRE-TRAINED MODELS Stacked Encoder: BERT uses a stacked-encoder architecture, which inputs a sequence of tokens x = (x 1 , ..., x n ) and applies position and token embeddings followed by a series of encoder layers. Each layer applies multi-head self-attention (see below) in combination with a feedforward network, layer normalization, and residual connections. The output of each layer is a sequence of contextualized embeddings</p><p>Self-Attention: Given an input x = (x 1 , . . . , x n ), the self-attention mechanism assigns to each token pair i, j an attention weight α i,j &gt; 0 where j α i,j = 1. Attention in BERT is bidirectional.</p><p>In the multi-layer, multi-head setting, α is specific to a layer and head. The BERT-Base model has 12 layers and 12 heads. Each attention head learns a distinct set of weights, resulting in 12 x 12 = 144 distinct attention mechanisms in this case.</p><p>The attention weights α i,j are computed from the scaled dot-product of the query vector of i and the key vector of j, followed by a softmax operation. The attention weights are then used to produce a weighted sum of value vectors:</p><formula xml:id="formula_4">Attention(Q, K, V ) = softmax QK T √ d k V<label>(2)</label></formula><p>using query matrix Q, key matrix K, and value matrix V , where d k is the dimension of K. In a multi-head setting, the queries, keys, and values are linearly projected h times, and the attention operation is performed in parallel for each representation, with the results concatenated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 OTHER TRANSFORMER VARIANTS</head><p>ALBERT: The architecture of ALBERT differs from BERT in two ways: (1) It shares parameters across layers, unlike BERT which learns distinct parameters for every layer and (2) It uses factorized embeddings, which allows the input token embeddings to be of a different (smaller) size than the hidden states. The original version of ALBERT designed for text also employed a sentence-order prediction pretraining task, but this was not used on the models studied in this paper.</p><p>XLNet: Instead of the masked-language modeling pretraining objective use for BERT, XLNet uses a bidirectional auto-regressive pretraining method that considers all possible orderings of the input factorization. The architecture also adds a segment recurrence mechanism to process long sequences, as well as a relative rather than absolute encoding scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENTAL DETAILS B.1 ALTERNATIVE ATTENTION AGREEMENT METRIC</head><p>Here we present an alternative formulation to Eq. 1 based on an attention-weighted average. We define an indicator function f (i, j) for property f that returns 1 if the property is present in token pair (i, j) (i.e., if amino acids i and j are in contact), and zero otherwise. We then compute the proportion of attention that matches with f over a dataset X as follows:</p><formula xml:id="formula_5">p α (f ) = x∈X |x| i=1 |x| j=1 f (i, j)α i,j (x) x∈X |x| i=1 |x| j=1 α i,j (x)<label>(3)</label></formula><p>where α i,j (x) denotes the attention from i to j for input sequence x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 STATISTICAL SIGNIFICANCE TESTING AND NULL MODELS</head><p>We perform statistical significance tests to determine whether any results based on the metric defined in Equation 1 are due to chance. Given a property f , as defined in Section 3, we perform a twoproportion z-test comparing (1) the proportion of high-confidence attention arcs (α i,j &gt; θ) for which f (i, j) = 1, and (2) the proportion of all possible pairs i, j for which f (i, j) = 1. Note that the first proportion is exactly the metric p α (f ) defined in Equation 1 (e.g. the proportion of attention aligned with contact maps). The second proportion is simply the background frequency of the property (e.g. the background frequency of contacts). Since we extract the maximum scores over all of the heads in the model, we treat this as a case of multiple hypothesis testing and apply the Bonferroni correction, with the number of hypotheses m equal to the number of attention heads.</p><p>As an additional check that the results did not occur by chance, we also report results on baseline (null) models. We initially considered using two forms of null models: (1) a model with randomly initialized weights. and (2) a model trained on randomly shuffled sequences. However, in both cases, none of the sequences in the dataset yielded attention weights greater than the attention threshold θ. This suggests that the mere existence of the high-confidence attention weights used in the analysis could not have occurred by chance, but it does not shed light on the particular analyses performed. Therefore, we implemented an alternative randomization scheme in which we randomly shuffle attention weights from the original models as a post-processing step. Specifically, we permute the sequence of attention weights from each token for every attention head. To illustrate, let's say that the original model produced attention weights of (0.3, 0.2, 0.1, 0.4, 0.0) from position i in protein sequence x from head h, where |x| = 5. In the null model, the attention weights from position i in sequence x in head h would be a random permutation of those weights, e.g., (0.2, 0.0, 0.4, 0.3, 0.1). Note that these are still valid attention weights as they would sum to 1 (since the original weights would sum to 1 by definition). We report results using this form of baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PROBING METHODOLOGY</head><p>Embedding probe. We probe the embedding vectors output from each layer using a linear probing classifier. For token-level probing tasks (binding sites, secondary structure) we feed each token's output vector directly to the classifier. For token-pair probing tasks (contact map) we construct a pairwise feature vector by concatenating the elementwise differences and products of the two tokens' output vectors, following the TAPE<ref type="foot" target="#foot_4">4</ref> implementation.</p><p>We use task-specific evaluation metrics for the probing classifier: for secondary structure prediction, we measure F1 score; for contact prediction, we measure precision@L/5, where L is the length of the protein sequence, following standard practice <ref type="bibr" target="#b38">(Moult et al., 2018)</ref>; for binding site prediction, we measure precision@L/20, since approximately one in twenty amino acids in each sequence is a binding site (4.8% in the dataset).</p><p>Attention probe. Just as the attention weight α i,j is defined for a pair of amino acids (i, j), so is the contact property f (i, j), which returns true if amino acids i and j are in contact. Treating the attention weight as a feature of a token-pair (i, j), we can train a probing classifier that predicts the contact property based on this feature, thereby quantifying the attention mechanism's knowledge of that property. In our multi-head setting, we treat the attention weights across all heads in a given layer as a feature vector, and use a probing classifier to assess the knowledge of a given property in the attention weights across the entire layer. As with the embedding probe, we measure performance of the probing classifier using precision@L/5, where L is the length of the protein sequence, following standard practice for contact prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 DATASETS</head><p>We used two protein sequence datasets from the TAPE repository for the analysis: the ProteinNet dataset <ref type="bibr" target="#b2">(AlQuraishi, 2019;</ref><ref type="bibr" target="#b17">Fox et al., 2013;</ref><ref type="bibr" target="#b5">Berman et al., 2000;</ref><ref type="bibr" target="#b38">Moult et al., 2018)</ref> and the Secondary Structure dataset <ref type="bibr" target="#b45">(Rao et al., 2019;</ref><ref type="bibr" target="#b5">Berman et al., 2000;</ref><ref type="bibr" target="#b38">Moult et al., 2018;</ref><ref type="bibr" target="#b27">Klausen et al., 2019)</ref>. The former was used for analysis of amino acids and contact maps, and the latter was used for analysis of secondary structure. We additionally created a third dataset for binding site and post-translational modification analysis from the Secondary Structure dataset, which was augmented with binding site and PTM annotations obtained from the Protein Data Bank's Web API. <ref type="foot" target="#foot_5">5</ref> We excluded any sequences for which annotations were not available. The resulting dataset sizes are shown in Table <ref type="table" target="#tab_4">2</ref>. For the analysis of attention, a random subset of 5000 sequences from the training split of each dataset was used, as the analysis was purely evaluative. For training and evaluating the diagnostic classifier, the full training and validation splits were used.       Table <ref type="table">3</ref>: Amino acids and the corresponding maximally attentive heads in the standard and randomized versions of TapeBert. The differences between the attention percentages for TapeBert and the background frequencies of each amino acid are all statistically significant (p &lt; 0.00001) taking into account the Bonferroni correction. See Appendix B.2 for details. The bolded numbers represent the higher of the two values between the standard and random models. In all cases except for Glutamine, which was the amino acid with the lowest top attention proportion in the standard model (7.1), the standard TapeBert model has higher values than the randomized version. </p><formula xml:id="formula_6">2 4 -7 2 1 -1 2 2 3 -3 2 5 -1 0 2 4 -1 5 2 1 -1 4 2 6 -2 4 -1 3 2 4 -1 1 5 -1 0 Top heads 0 20 40 Attention % (c) ProtBert 2 6 -1 2 5 -1 4 2 5 -4 2 6 -2 2 6 -1 3 2 4 -1 2 6 -4 2 6 -1 2 2 7 -1 5 2 3 -1 Top heads 0 20 40 Attention % (d) ProtBert-BFD 1 1 -1 6 9 -9 1 2 -3 1 4 -6 1 6 -9 1 3 -1 2 1 9 -1 3 1 7 -1 0 1 7 -6<label>1</label></formula><formula xml:id="formula_7">1 1 -3 1 1 -8 1 1 -1 2 1 2 -1 1 3 -6 1 2 -7 1 1 -1 6 -1 1 9 -4<label>9</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Agreement between attention and contact maps across five pretrained Transformer models from TAPE (a) and ProtTrans (b-e). The heatmaps show the proportion of high-confidence attention weights (α i,j &gt; θ) from each head that connects pairs of amino acids that are in contact with one another. In TapeBert (a), for example, we can see that 45% of attention in head 12-4 (the 12th layer's 4th head) maps to contacts. The bar plots show the maximum value from each layer. Note that the vertical striping in ProtAlbert (b) is likely due to cross-layer parameter sharing (see Appendix A.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Each plot shows the percentage of attention focused on the given property, averaged over all heads within each layer. The plots, sorted by center of gravity (red dashed line), show that heads in deeper layers focus relatively more attention on binding sites and contacts, whereas attention toward specific secondary structures is more even across layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Percentage of each head's attention focused on amino acids Pro (left) and Phe (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Pairwise attention similarity (left) vs. substitution matrix (right) (codes in App. C.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Percentage of each head's attention that is focused on Turn/Bend secondary structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Top 10 heads (denoted by &lt;layer&gt;-&lt;head&gt;) for each model based on the proportion of attention focused on binding sites [95% conf. intervals]. Differences between attention proportions and the background frequency of binding sites (orange dashed line) are all statistically significant (p &lt; 0.00001). Bonferroni correction applied for both confidence intervals and tests (see App. B.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Top-10 heads most focused on binding sites for null models. See Appendix B.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Percentage of each head's attention that is focused on post-translational modifications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 19 :</head><label>19</label><figDesc>Figure19: Percentage of each head's attention that is focused on the given amino acid, averaged over a dataset(cont.)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>provides an overview of the five pre-trained Transformer models studied in this work. The models originate from the TAPE and ProtTrans repositories, spanning three model architectures: BERT, ALBERT, and XLNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Summary of pre-trained models analyzed, including the source of the model, the type of Transformer used, the number of layers and heads, the total number of model parameters, the source of the pre-training dataset, and the number of protein sequences in the pre-training dataset.</figDesc><table><row><cell>Source</cell><cell>Name</cell><cell>Type</cell><cell cols="4">Layers Heads Params Train Dataset # Seq</cell></row><row><cell>TAPE</cell><cell>TapeBert</cell><cell>BERT</cell><cell>12</cell><cell>12</cell><cell>94M Pfam</cell><cell>31M</cell></row><row><cell cols="2">ProtTrans ProtBert</cell><cell>BERT</cell><cell>30</cell><cell>16</cell><cell>420M Uniref100</cell><cell>216M</cell></row><row><cell cols="3">ProtTrans ProtBert-BFD BERT</cell><cell>30</cell><cell>16</cell><cell>420M BFD</cell><cell>2.1B</cell></row><row><cell cols="2">ProtTrans ProtAlbert</cell><cell>ALBERT</cell><cell>12</cell><cell>64</cell><cell>224M Uniref100</cell><cell>216M</cell></row><row><cell cols="2">ProtTrans ProtXLNet</cell><cell>XLNet</cell><cell>30</cell><cell>16</cell><cell>409M Uniref100</cell><cell>216M</cell></row><row><cell cols="4">A.2 BERT TRANSFORMER ARCHITECTURE</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Datasets used in analysis Percentage of each head's attention that is focused on Helix secondary structure. Percentage of each head's attention that is focused on Strand secondary structure.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell>Train size Validation size</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ProteinNet</cell><cell></cell><cell>25299</cell><cell>224</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Secondary Structure</cell><cell>8678</cell><cell>2170</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Binding Sites / PTM</cell><cell>5734</cell><cell>1418</cell></row><row><cell cols="13">C ADDITIONAL RESULTS OF ATTENTION ANALYSIS</cell></row><row><cell cols="9">C.1 SECONDARY STRUCTURE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>15% 30% 45% 60%</cell><cell cols="2">2 4 6 8 10 12 Layer</cell><cell cols="2">2 4 6 8 10 12 Head % Attention</cell><cell cols="2">0 50% Max</cell><cell>0% 20% 40% 60%</cell><cell>Layer 2 4 6 8 10 12</cell><cell cols="4">% Attention 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 Head</cell><cell>Max 0 50%</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) TapeBert</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) ProtAlbert</cell></row><row><cell cols="2">0% 20% 40% 60% 80%</cell><cell cols="2">Layer</cell><cell cols="2">2 4 6 8 10 12 14 16 Head 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 % Attention</cell><cell cols="2">0 50% Max</cell><cell></cell><cell>20% 40% 60% 80%</cell><cell>Layer</cell><cell cols="2">2 4 6 8 10 12 14 16 Head 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 % Attention</cell><cell>0 50% Max</cell><cell>0% 20% 40% 60%</cell><cell>Layer</cell><cell>2 4 6 8 10 12 14 16 Head 2 4 6 8 10 12 14 16 20 22 24 % Attention 30 28 26</cell><cell>0 50% Max</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) ProtBert</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) ProtBert-BFD</cell><cell>(e) ProtXLNet</cell></row><row><cell></cell><cell cols="4">Figure 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">94% of sequences had length less than 512. Experiments performed on single 16GB Tesla V-100 GPU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Heads with fewer than 100 high-confidence attention weights across the dataset are grayed out.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">This head also targets binding sites (Fig.3a) but at a percentage of</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="49" xml:id="foot_3">49%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://github.com/songlab-cal/tape</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">http://www.rcsb.org/pdb/software/rest.do</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 CONTACT MAPS: STATISTICAL SIGNIFICANCE TESTS AND NULL MODELS</head> <ref type="table">1 2 -4  1 2 -1 2  1 2 -1 1  1 2 -2 8 -5 1 2 -7 9 -7 6 -1 1 9 -8 1</ref> <p>Top heads  <ref type="table">1 1 -6 9 -7  1 1 -1 2  1 1 -3  1 0 -3 4 -7 5 -8 9 -4 5 -1 9 -1</ref>   <ref type="table">2 -2 8  1 -1 1  2 -1 9  2 -3 4  1 0 -5 6  4 -2 9  3 -3 4  3 -2 9  1 0 -1 7  6 -1 7</ref> Top heads</p><p>Top heads</p><p>Top heads   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04207[cs.CL].</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">589333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ProteinNet: a standardized data set for machine learning of protein structure</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Rk</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">John</forename><surname>Helen M Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zukang</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Talapady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helge</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><forename type="middle">N</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-n protein engineering with data-efficient deep learning</title>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Esvelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.01.23.917682</idno>
		<ptr target="https://www.biorxiv.org/content/early/2020/08/31/2020.01.23.917682" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HIV-1 protease: Mechanism and drug discovery</title>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Brik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Huey</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organic &amp; Biomolecular Chemistry</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On identifiability in Transformers</title>
		<author>
			<persName><forename type="first">Gino</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJg1f6EFDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What does BERT look at? An analysis of BERT&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BlackBoxNLP@ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">L L</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisanna</forename><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky995</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2019-01">January 2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L L</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisanna</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C E</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky995</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ProtTrans: Towards cracking the language of life&apos;s code through self-supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06225</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SCOPe: Structural classification of proteins-extended, integrating scop and astral data and classification of new structures</title>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Marc</forename><surname>Chandonia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D304" to="D309" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Assessing BERT&apos;s syntactic abilities</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05287</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why attention is not explanation: Surgical intervention and causal reasoning about neural models</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Grimsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elijah</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">R S</forename><surname>Bursten</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.lrec-1.220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
				<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J G</forename><surname>Henikoff</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.89.22.10915</idno>
		<ptr target="https://www.pnas.org/content/89/22/10915" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
				<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="10915" to="10919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">exBERT: A visual analysis tool to explore learned representations in transformers models</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05276</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Do attention heads in BERT track syntactic dependencies?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12246</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is not Explanation</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02131630" />
	</analytic>
	<monogr>
		<title level="m">ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comprehensive structural classification of ligand-binding motifs in proteins</title>
		<author>
			<persName><forename type="first">Akira</forename><surname>Kinjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruki</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schantz Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Closter Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamilla</forename><surname>Kjaergaard Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><forename type="middle">Isabell</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Soenderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bent</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of BERT</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4365" to="4374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07337</idno>
		<title level="m">Measuring bias in contextualized word representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.1909.11942</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global organization of a binding site network gives insight into evolution and structure-function relationships of proteins</title>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janez</forename><surname>Konc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusanka</forename><surname>Janezic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2017">11652. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open sesame: Getting inside BERT&apos;s linguistic knowledge</title>
		<author>
			<persName><forename type="first">Yongjie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nelson F Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08855</idno>
		<title level="m">Linguistic knowledge and transferability of contextual representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ProGen: Language modeling for protein generation</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Ssu</forename><surname>Raphael R Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What do you mean</title>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Mickus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kees</forename><surname>Van Deemeter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05758</idno>
	</analytic>
	<monogr>
		<title level="m">BERT? Assessing BERT as a distributional semantics model</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interrogating the explanatory power of attention in neural machine translation</title>
		<author>
			<persName><forename type="first">Pooya</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5624</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-5624" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
				<meeting>the 3rd Workshop on Neural Generation and Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-Round XII</title>
		<author>
			<persName><forename type="first">John</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Tramontano</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25415</idno>
		<ptr target="http://doi.wiley.com/10.1002/prot.25415" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NGLview-interactive molecular graphics for Jupyter notebooks</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx789</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btx789" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1241" to="1242" />
			<date type="published" when="2017">12 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Srouji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16012</idno>
		<title level="m">Dian Ang Yap, and Vineet Kosaraju. Bert learns (and teaches) chemistry</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dissecting contextual word embeddings: Architecture and representation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to deceive with attention-based explanations</title>
		<author>
			<persName><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.07913" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An analysis of encoder representations in Transformerbased machine translation</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5431</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5431" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with TAPE</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and measuring the geometry of BERT</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Dalché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8594" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accelerating protein design using autoregressive generative models</title>
		<author>
			<persName><forename type="first">Jung-Eun</forename><surname>Adam J Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elana</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aashish</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">757252</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12327</idno>
		<title level="m">A primer in BERTology: What we know about how BERT works</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inferring protein 3D structure from deep mutation scans</title>
		<author>
			<persName><forename type="first">Nathan J</forename><surname>Rollins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><forename type="middle">P</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">J</forename><surname>Poelwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Stiffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1170</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">NGL Viewer: a web application for molecular visualization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Hildebrand</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkv402</idno>
		<ptr target="https://doi.org/10.1093/nar/gkv402" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W576" to="W579" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">NGL viewer: web-based molecular graphics for large complexes</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Alexander S Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Valasatava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Prlić</surname></persName>
		</author>
		<author>
			<persName><surname>Rose</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty419</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/bty419" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3755" to="3758" />
			<date type="published" when="2018-05">05 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Protein phosphorylation</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ora</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.bi.44.070175.004151</idno>
		<ptr target="https://doi.org/10.1146/annurev.bi.44.070175.004151" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biochemistry</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="831" to="887" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised attention-guided atom-mapping</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Louis</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Laino</surname></persName>
		</author>
		<idno type="DOI">10.26434/chemrxiv.12298559.v1</idno>
		<ptr target="https://chemrxiv.org/articles/Unsupervised_Attention-Guided_Atom-Mapping/12298559" />
	</analytic>
	<monogr>
		<title level="j">ChemRxiv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1282</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1282" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-04964-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">2542. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">and the UniProt Consortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu739</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btu739" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Elisa</forename><surname>Celis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13230" to="13241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Attention interpretability across NLP tasks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11218</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Diagnostic classifiers revealing how neural networks process hierarchical structure</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Veldhoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoCo@NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A multiscale visualization of attention in the Transformer model</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analyzing the structure of attention in a Transformer language model</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
				<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="63" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Causal mediation analysis for interpreting neural NLP: The case of gender bias</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Does BERT make any sense? Interpretable word sense disambiguation with contextualized embeddings</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10430</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Dalché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Fine-grained sentiment analysis with faithful attention</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06870</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
