<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective Transfer Machine for Personalized Facial Expression Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
						</author>
						<title level="a" type="main">Selective Transfer Machine for Personalized Facial Expression Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F5582BFCA605BA6730F9AAA697202D4</idno>
					<idno type="DOI">10.1109/TPAMI.2016.2547397</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2547397, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2547397, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression analysis</term>
					<term>personalization</term>
					<term>domain adaptation</term>
					<term>transfer learning</term>
					<term>Support Vector Machine (SVM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic facial action unit (AU) and expression detection from videos is a long-standing problem. The problem is challenging in part because classifiers must generalize to previously unknown subjects that differ markedly in behavior and facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) from those on which the classifiers are trained. While some progress has been achieved through improvements in choices of features and classifiers, the challenge occasioned by individual differences among people remains. Person-specific classifiers would be a possible solution but for a paucity of training data. Sufficient training data for person-specific classifiers typically is unavailable. This paper addresses the problem of how to personalize a generic classifier without additional labels from the test subject. We propose a transductive learning method, which we refer as a Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific mismatches. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. We compared STM to both generic classifiers and cross-domain learning methods on four benchmarks: CK+ [44], GEMEP-FERA [67], RU-FACS <ref type="bibr" target="#b3">[4]</ref> and GFT [57]. STM outperformed generic classifiers in all.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A UTOMATIC facial AU detection confronts a number of challenges. These include changes in pose, scale, illumination, occlusion, and individual differences in face shape, texture, and behavior. Face shape and texture differ between and within sexes; they differ with ethnic and racial backgrounds, age or developmental level, exposure to the elements, and in the base rates with which they occur. For example, some people smile broadly and frequently; others rarely or only with smile controls, which counteract the upward pull of the zygomatic major on the lip corners. These and other sources of variation represent considerable challenges for computer vision. Then there is the challenge of automatically detecting facial actions that require significant training and expertise in humans <ref type="bibr" target="#b66">[67]</ref>.</p><p>To address these challenges, previous work has focused on identifying optimal feature representations and classifiers. Interested readers may refer to <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref> for comprehensive reviews. While improvements have been achieved, a persistent shortcoming of existing systems is that they fail to generalize well to previously unseen, or new, subjects. One way to cope with this problem is to train and test separate classifiers on each subject (i.e., person-specific classifier). Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows a real example of how a simple linear person-specific classifier can separate the positive samples of AU12 (lip corner puller, seen in smiling) from the negative ones. When ample training data are available, a person-specific classifier approaches an ideal classifier, one that best separates actions for the test subject.</p><p>A problem with person-specific classifiers is that sufficient quantity of training data is usually unavailable. In part for this reason, most approaches seek to use training data from multiple subjects in the hope to compensate for subject biases. However, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), when a classifier is trained on all training subjects and tested on an unknown subject, its generalizability may disappoint. When a classifier is trained and tested in this manner, we refer it as to generic classifier. Because person-independent classifiers typically are not feasible, generic classifiers are most commonly used. We propose that impaired generalizability occurs in part because of individual differences among subjects. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates this phenomenon on real data in a 3-D eigenspace. One can observe that when the data are interpreted as positive and negative classes in Fig. <ref type="figure" target="#fig_1">2</ref>(a), they could be very difficult to separate without overfitting. When the data are interpreted as subjects in Fig. <ref type="figure" target="#fig_1">2</ref>(b), the grouping effect becomes clear and echoes with our conjecture on individual differences. In this example, these differences include sex, skin color and texture, illumination, and other ways in which people vary. Our guiding hypothesis is that such person-specific bias causes standard generic classifiers to perform worse on some subjects than others <ref type="bibr" target="#b27">[28]</ref>.</p><p>To mitigate the influence of individual biases, this paper explores the idea of personalizing a generic classifier for facial expression analysis. Given a common observation that test videos usually come from only a single subject, we assume the test distribution can be approximated by certain frames from training subjects. The problem of personalizing a generic classifier then is formulated as training a classifier on selected training samples, while reducing the discrepancy between distributions of selected training samples and test ones. In this way, generic classifiers can adapt to an unseen test subject without test labels. We term this transductive approach Selective Transfer Machine (STM). The major contributions of this work include:</p><p>• Based on both qualitative observations and empirical findings, individual differences attenuate AU detection. To address this problem, we introduce Selective Transfer Machine (STM). STM is a personalization approach that reduces mismatch between feature distributions of training and test subjects. We propose an effective and robust procedure to optimize STM in its primal form. • Considering that many applications afford labeled test data, we introduce a useful extension of STM, termed L-STM, to make use of labeled target data. This extension shows considerable performance improvement in situations for which some labeled test data exist. • To evaluate STM, we conduct comprehensive experiments using within-subject, cross-subject, and cross-dataset scenarios on four benchmark datasets. We test STM for both AU detection and detection of holistic expressions. • For test subjects, some training samples are more instrumental than others. We can identify those training samples using STM. The effectiveness of STM scales as the number of training subjects increases. This paper is organized as follows. Sec. 2 reviews related work. Secs. 3-5 describes the STM model, optimization algorithm, and theoretical rationale. Sec. 6 introduces L-STM, an STM extension that utilizes labeled test data. Sec. 7 considers similarities and differences between STM and related methods. Sec. 8 evaluates STM and alternatives for AU and holistic expression detection. Sec. 9 concludes the paper with remarks and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our approach lies at the intersection between facial expression analysis and cross-domain adaptation. Below we briefly discuss each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Facial expression analysis</head><p>Automatic facial expression analysis entails at least three steps: Face tracking and registration, feature extraction, and learning classifiers. This section reviews recent advances in each. Tracking and registration: Tracking and registration of nonrigid facial features is a long-standing problem in computer vision. The goal of tracking is to detect facial landmarks (e.g., eyes) in each frame. For facial landmark detections, Parametrized Appearance Models (PAM) are among the most popular methods. PAM include the Lucas-Kanade method <ref type="bibr" target="#b42">[43]</ref>, Active Appearance Models (AAM) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b46">[47]</ref>, Constrained Local Models (CLM) <ref type="bibr" target="#b14">[15]</ref>, and, more recently, Zface <ref type="bibr" target="#b33">[34]</ref> and Supervised Descent Method <ref type="bibr" target="#b73">[74]</ref>. Once facial landmarks are located, the registration step aims to align the face image to remove 3D rigid head motion, so features can be geometrically normalized. A similarity transformation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b85">[86]</ref> registers faces with respect to an averaged face. A Delaunay triangulation can be also applied with a backward piecewise affine warping to extract features in areas not explicitly tracked. This two-step registration proves to preserve better shape variation in appearance than by geometric normalization alone.</p><p>Feature extraction: With advances in tracking and registration, there has been renewed emphasis on biologically inspired features and temporal variation. As summarized in Table <ref type="table" target="#tab_1">1</ref>, current approaches to feature extraction may be broadly divided into four types: geometric, appearance, dynamic, and fusion. Geometric features contain information about shape and locations of permanent facial features, such as eyes or nose. Standard approaches rely on detecting fiducial facial points <ref type="bibr" target="#b44">[45]</ref>, a connected face <ref type="bibr" target="#b60">[61]</ref>, landmark coordinates <ref type="bibr" target="#b14">[15]</ref>, or face component shape parameterization <ref type="bibr" target="#b44">[45]</ref>. Geometric features have performed well for many but not all AU detection tasks. They have difficulty detecting subtle expressions and are highly vulnerable to registration error <ref type="bibr" target="#b15">[16]</ref>.</p><p>Appearance features, which often are biologically inspired, afford increased robustness to tracking and registration error. Appearance features represent skin texture and its permutations and have been widely applied to facial expression analysis. Representative methods include SIFT <ref type="bibr" target="#b85">[86]</ref>, DAISY <ref type="bibr" target="#b85">[86]</ref>, Gabor jets <ref type="bibr" target="#b3">[4]</ref>, LBP <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b83">[84]</ref>, Bag-of-Words model <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, compositional <ref type="bibr" target="#b76">[77]</ref> and others <ref type="bibr" target="#b71">[72]</ref>. Dynamic features, a newly popular technique, encodes temporal information during the feature extraction stage. Examples include optical flow <ref type="bibr" target="#b31">[32]</ref>, bag of temporal words <ref type="bibr" target="#b61">[62]</ref>, volume LBP/LPQ <ref type="bibr" target="#b81">[82]</ref>, Gabor motion energy <ref type="bibr" target="#b72">[73]</ref>, and others. Fusion approaches incorporate multiple features, e.g., Multiple Kernel Learning (MKL) <ref type="bibr" target="#b57">[58]</ref>, and have yet to prove superior to other approaches <ref type="bibr" target="#b66">[67]</ref>.</p><p>Classifiers: Two main trends have been pursued when designing classifiers for facial expression analysis, as summarized in Table <ref type="table" target="#tab_2">2</ref>. One trend, static modeling, typically tackles the Fusion Multiple feature kernels 2012 <ref type="bibr" target="#b57">[58]</ref> problem as discriminative classification and evaluates each frame independently. Representative approaches include Neural Network <ref type="bibr" target="#b37">[38]</ref>, Adaboost <ref type="bibr" target="#b3">[4]</ref>, SVMs <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b82">[83]</ref>, and Deep Networks <ref type="bibr" target="#b41">[42]</ref>. Due to lack of temporal consistency, static models tend to produce non-smooth results. To address this issue, temporal modeling, the other trend, captures temporal transition between contiguous frames. For instance, Dynamic Bayesian Network (DBN) with appearance features <ref type="bibr" target="#b64">[65]</ref> was proposed to model AU co-occurrence. Other variants of DBN include Hidden Markov Models <ref type="bibr" target="#b58">[59]</ref> and Conditional Random Fields (CRF) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b67">[68]</ref>.</p><p>As an alternative, Simon et al. <ref type="bibr" target="#b60">[61]</ref> proposed a structural-output SVM that detects AUs as temporal segments. To model relations between segments, Rudovic et al. <ref type="bibr" target="#b51">[52]</ref> considered ordinal information in CRF. More recently, Ding et al. <ref type="bibr" target="#b20">[21]</ref> proposed a hybrid approach that integrates frame-based, segment-based, and transition-based tasks in a sequential order. Interested readers are referred to <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b66">[67]</ref> for more complete surveys. Common to all these approaches is the assumption that training and test data are drawn from the same distribution. However, as Fig. <ref type="figure" target="#fig_1">2</ref> shows, they could suffer from individual differences, causing poor generalizability to an unseen subject. STM makes no such assumption. Instead, it seeks a personalized classifier by re-weighting training samples according to their distribution mismatch with test samples. Several studies merged into this direction could be found in <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b77">[78]</ref>- <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-domain adaptation</head><p>Our approach is motivated by an increasing concern about dataset shift in the object detection literature. In real-world data, labels of interest often occur infrequently and features vary markedly between and within datasets. These factors contribute to significant biases in object categorization <ref type="bibr" target="#b65">[66]</ref>. Saenko et al. <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b53">[54]</ref> proposed to reduce the discrepancy between features by learning metric transformation. Aytar and Zisserman <ref type="bibr" target="#b1">[2]</ref> regularized the training of a new object class by transferring pre-learned models. Chattopadhyay et al. <ref type="bibr" target="#b11">[12]</ref> proposed to learn a combination of source classifiers that matches the target labels. Because these techniques use a supervised approach in which one or more labeled instances are required from the target domain, they are illsuited to new domains or subjects for which no prior knowledge is available. In contrast, our approach is unsupervised and thus better geared to the generalization to new domains or subjects. Closer to our approach is a special case in unsupervised domain adaptation known as covariate shift <ref type="bibr" target="#b62">[63]</ref>. In covariate shift, train and test domains follow different distributions but the label distributions remain the same. Given a source and a target domain, Domain Invariant Projection (DIP) <ref type="bibr" target="#b2">[3]</ref> finds a domaininvariant space in which training and test data share a similar distribution. Similarly, Subspace Alignment (SA) <ref type="bibr" target="#b24">[25]</ref> represents each domain as a subspace and then learns a mapping function that aligns the sources to the target one. However, learning the projection or mapping is unsupervised, and thus it is unclear how source labels can be incorporated.</p><p>On the other hand, Dudík et al. <ref type="bibr" target="#b23">[24]</ref> infer the re-sampling weights through maximum entropy density estimation without target labels. Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> measures the discrepancy between two different distributions in terms of expectations of empirical samples. Without estimating densities, Transductive SVM (T-SVM) <ref type="bibr" target="#b35">[36]</ref> simultaneously learns a decision boundary and maximizes the margin in the presence of unlabeled patterns. Domain adaptation SVM <ref type="bibr" target="#b5">[6]</ref> extends T-SVM by progressively adjusting the discriminant function toward the target domain. SVM-KNN <ref type="bibr" target="#b80">[81]</ref> labels a single query using an SVM trained on its k neighborhood of the training data. Each of these methods uses either all or a portion of the training data. STM learns to re-weight training instances, which reduces the influence of irrelevant data.</p><p>Considering distribution mismatch, Kernel Mean Matching (KMM) <ref type="bibr" target="#b30">[31]</ref> directly infers re-sampling weights by matching training and test distributions. Following this idea, Yamada et al. <ref type="bibr" target="#b74">[75]</ref> estimated relative importance weights and learned from re-weighted training samples for 3D human pose estimation. See <ref type="bibr" target="#b49">[50]</ref> for further review. These methods take a two-step approach that first estimates the sampling weights and then trains a reweighted classifier or regressor. In contrast, STM jointly optimizes both the sampling weights and the classifier parameters and hence preserves the discriminant property of the new decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SELECTIVE TRANSFER MACHINE (STM)</head><p>This section describes the proposed Selective Transfer Machine (STM) for personalizing a generic classifier. Unlike previous cross-domain methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b75">[76]</ref>, STM requires no labels from a test subject. We will use Support Vector Machine (SVM) as classifier due to its popularity for AU detection <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b60">[61]</ref>.</p><p>Problem formulation: Recent research and applications in automatic facial expression analysis involve video, which provides a wide sampling of facial appearance change. We assume the distribution of a subject's appearance can be estimated by certain video frames. Based on this assumption, the main idea of STM is to re-weight training samples (i.e., frames) to form a distribution closer to the test distribution. Classifiers trained on the re-weighted training samples are likely to generalize to the test subject.</p><p>Let us denote the training set as D tr = {x i , y i } ntr i=1 , y i ∈ {+1, -1} (see notation 1 ). For notational simplicity, we stack 1 in each data vector x i to compensate for the offset, i.e., x i ∈ R d+1 . We formulate STM as minimizing the objective:</p><formula xml:id="formula_0">g(f, s) = min f,s R f (D tr , s) + λΩ s (X tr , X te ),<label>(1)</label></formula><p>where R f (D Penalized SVM: The first term in STM, R f (D tr , s), is the empirical risk of a penalized SVM, where each training instance is weighted by its relevance to the test data. In the following, we denote X ≡ X tr for notational simplicity unless further referred. The linear penalized SVM has the target decision function in the form f (x) = w x and minimizes:</p><formula xml:id="formula_1">R w (D tr , s) = 1 2 w 2 + C ntr i=1 s i L p (y i , w x i ),<label>(2)</label></formula><p>where L p (y, •) = max(0, 1 -y•) p (p = 1 stands for hinge loss and p = 2 for quadratic loss). In general, L could be any loss function. The unconstrained linear SVM in (2) can be extended to a nonlinear version by introducing a kernel matrix K ij := k(x i , x j ) corresponding to a kernel function k induced from some nonlinear feature mapping ϕ(•). Using the representer theorem <ref type="bibr" target="#b10">[11]</ref>, the nonlinear decision function can be represented f (x) = ntr i=1 β i k(x i , x), yielding the nonlinear penalized SVM:</p><formula xml:id="formula_2">R β (D tr , s) = 1 2 β Kβ + C ntr i=1 s i L p (y i , k i β),<label>(3)</label></formula><p>where β ∈ R ntr is the expansion coefficient and k i is the ith column of K. Unlike most standard solvers, we train the penalized SVM in the primal due to its simplicity and efficiency. Through the unconstrained primal problems, we applied Newton's method with quadratic convergence <ref type="bibr" target="#b10">[11]</ref>. Details are given in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution mismatch:</head><p>The second term in STM, Ω s (X tr , X te ), imitates domain mismatch and aims to find a re-weighting function that minimizes the discrepancy between the training and the test distributions. In previous cross-domain learning methods, the re-weighting function may be computed by separately estimating the densities and then the weights (e.g., <ref type="bibr" target="#b63">[64]</ref>). However, this strategy could be prone to error while taking the ratio of estimated densities <ref type="bibr" target="#b63">[64]</ref>.</p><p>Here we adopt the Kernel Mean Matching (KMM) <ref type="bibr" target="#b30">[31]</ref> method to reduce the difference between the means of the training 1. Bold capital letters denote a matrix X; bold lower-case letters denote a column vector x. x i represents the ith column of the matrix X. All nonbold letters represent scalars. x j denotes the scalar in the jth element of x. In ∈ R n×n is an identity matrix. and the test distributions in the Reproducing Kernel Hilbert Space H. KMM computes the instance re-weighting s i that minimizes:</p><formula xml:id="formula_3">Ω s (X tr , X te ) = 1 n tr ntr i=1 s i ϕ(x tr i )- 1 n te nte j=1 ϕ(x te j ) 2 H .<label>(4)</label></formula><p>Introducing κ i := ntr nte nte j=1 k(x tr i , x te j ), i = 1, . . . , n tr , that captures the closeness between training and each test sample, solving s in (4) can be rewritten as a quadratic programming (QP):</p><formula xml:id="formula_4">min s 1 2 s Ks -κ s,<label>(5)</label></formula><formula xml:id="formula_5">s. t. s i ∈ [0, B], ntr i=1 s i -n tr ≤ n tr ,</formula><p>where B defines a scope bounding discrepancy between probability distributions P tr and P te (B = 1000 in our case). For B → 1, one obtains an unweighted solution where all s i = 1. The second constraint ensures the weighted samples to be close to a probability distribution <ref type="bibr" target="#b30">[31]</ref>. Observe in (5) that larger κ i leads to larger s i when the objective is minimized. This matches our intuition to put higher selection weights on the training samples that are more likely to resemble the test distribution.</p><p>A major benefit from KMM is a direct importance estimation without estimating training and test densities. Compared to existing approaches, with proper tuning of kernel bandwidth, KMM shows the lowest importance estimation error and robustness to input dimension and the number of training samples, as suggested in <ref type="bibr" target="#b63">[64]</ref>. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates its effect on a synthetic data. As shown, KMM can estimate the ideal fitting well, while standard Ordinary Least Square (OLS) and Weighted OLS (WOLS) with training/test ratio lead to suboptimal prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION FOR STM</head><p>To solve Eq. ( <ref type="formula" target="#formula_0">1</ref>), we adopt the Alternate Convex Search <ref type="bibr" target="#b25">[26]</ref> that alternates between solving the decision function f and the selective coefficient s. Note that the objective in ( <ref type="formula" target="#formula_0">1</ref>) is biconvex: Convex in f when s is fixed (f is quadratic and L p is convex), and convex in s when f is fixed (since K 0). Under these conditions, the alternate optimization approach is guaranteed to monotonically decrease the objective function. Because the function is bounded below, it will converge to a critical point. Algorithm 1 summarizes the STM algorithm. Once the optimization is done, f is applied to perform the inference for test images. Below we detail the two steps in the alternate algorithm.</p><p>Minimizing over s: Denote the training losses as p i := L p (y i , f (x i )), i = 1, . . . , n tr . The optimization over s can be rewritten into the following QP:</p><formula xml:id="formula_6">min s 1 2 s Ks + ( C λ p -κ) s<label>(6)</label></formula><formula xml:id="formula_7">s. t. 0 ≤ s i ≤ B, n tr (1 -) ≤ ntr i=1 s i ≤ n tr (1 + ).</formula><p>Since K 0 by definition, ( <ref type="formula" target="#formula_6">6</ref>) has only one global optimum. To make the algorithm numerically stable, we add a ridge σ on the diagonal so that K σI ntr (σ = 10 -8 in our case). Note that the procedure here is different from the original KMM in terms of weight refinement: In each iteration s will be refined through the training loss p from the penalized SVM. This effect can be observed from minimizing the second term in (6): Larger p leads to smaller s to keep the objective small. This effectively reduces the selection weights of incorrectly classified training samples. On the contrary, KMM uses no label information and thus is incapable of refining importance weights. Introducing training losses helps preserve the discriminant property of the new decision boundary and hence leads to a more robust personalized classifier. From this perspective, KMM can be treated as a special case as the first iteration in the STM framework.</p><p>Fig. <ref type="figure">5</ref> illustrates the iterative effects on a synthetic example for learning a personalized classifier. In it#1, the hyperplane estimated by KMM is unreliable because it is completely unsupervised. On the other hand, STM simultaneously considers training loss and the weightings, and thus encourages the associated training samples to be well classified. As can be observed, as the iterations proceed, the STM separation hyperplane approaches toward the ideal one for the target data.</p><p>Minimizing over f : Let sv indicate the index set of support vectors, and n sv the number of support vectors. In the case of training loss 2 being quadratic, the gradient and Hessian of the linear penalized SVM in (2) can be written as:</p><formula xml:id="formula_8">∇ w = w + 2CXSI 0 (X w -y),<label>(7)</label></formula><formula xml:id="formula_9">H w = I d + 2CXSI 0 X ,<label>(8)</label></formula><p>where S = diag(s) ∈ R ntr×ntr denotes the re-weighting matrix, y ∈ R ntr the label vector, and I 0 ∈ R ntr×ntr the proximity identity matrix with the first n sv diagonal elements being 1 and the rest being 0. Similarly, the gradient with respect to the expansion coefficient β in (3) can be derived as:</p><formula xml:id="formula_10">∇ β = Kβ + 2CKSI 0 (Kβ -y),<label>(9)</label></formula><formula xml:id="formula_11">H β = K + 2CKSI 0 K. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Given the gradients and Hessians, the penalized SVM can be optimized by standard Newton's method or conjugate gradient. Differentiable Huber loss: The L 1 (hinge) loss in standard SVMs are not differentiable, hampering its gradient and Hessian to be explicitly expressed and computed. Instead, we use the Huber loss <ref type="bibr" target="#b10">[11]</ref> as a differentiable surrogate, i.e., L 1 (y i , f</p><formula xml:id="formula_13">(x i )) ≈ L H (y i sign(f (x i ))</formula><p>). Note that any differential convex loss, e.g., logistic loss and exponential loss, can be directly incorporated. The Huber loss can be defined as follows:</p><formula xml:id="formula_14">L H (a) =    0 if a &gt; 1 + h, (1+h-a) 2 4h if |1 -a| ≤ h, 1 -a otherwise,<label>(11)</label></formula><p>where h is a parameter of choice. Fig. <ref type="figure" target="#fig_4">4</ref> shows the influnce of h in comparison to the L 1 and L 2 loss. As can be observed, L H approaches the hinge loss when h → 0. As indicated in <ref type="bibr" target="#b10">[11]</ref>, there is no clear reason to prefer the hinge loss because replacing the hinge loss with Huber loss does not influence much the results.</p><p>With the differentiable Huber loss, the gradient and Hessian with Huber loss for the penalized linear SVM can be obtained:</p><formula xml:id="formula_15">∇ w = w + C 2h XSI 0 X w -(1 + h)y -CXSI 1 y,<label>(12)</label></formula><formula xml:id="formula_16">H w = I d + C 2h XSI 0 X ,<label>(13)</label></formula><p>and for the penalized nonlinear SVM:</p><formula xml:id="formula_17">∇ β = Kβ + C 2h KSI 0 [Kβ -(1 + h)y] -KI 1 y,<label>(14)</label></formula><formula xml:id="formula_18">H β = K + C 2h KSI 0 K,<label>(15)</label></formula><p>where I 1 ∈ R ntr×ntr denotes the proximity identity matrix with the first n sv diagonal elements being 0, followed by n (the number of points in the linear part of the Huber loss) elements of ones, and the rest being 0. With the derived gradient and Hessian, we are able to optimize for f with quadratic convergence using standard Newton method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL RATIONALE</head><p>This section analyzes two important properties of STM, biconvexity and boundedness, based on the techniques developed for biconvex optimization <ref type="bibr" target="#b29">[30]</ref>. Then we justify the convergence of the Alternate Convex Search algorithm, which we used for solving STM, in terms of both objective value and optimization variables. Circles and squares denote positive and negative classes, respectively. Note that it#1 indicates the result of KMM <ref type="bibr" target="#b30">[31]</ref>. STM improves separation relative to generic SVM as early as the first iteration and converges toward the ideal hyperplane by the 12 th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Properties of STM</head><p>We start by showing that STM is a biconvex problem. Proof. Denote the decision variable of f as w ∈ W ⊆ R d and the selection coefficient s ∈ S ⊆ R ntr , where W and S are two nonempty convex sets. Let Z ⊆ W × S be the solution set on W × S; Z w and Z s be the subsets when w and s are given respectively. Because Z s is convex for every w ∈ W (w and L p are convex; s i ∈ [0, B] are non-negative) and Z w is convex for every s ∈ S (Ω s is QP and K 0), the solution set Z is a biconvex set. Hence STM can be rewritten in the standard form of biconvex optimization problem <ref type="bibr" target="#b0">[1]</ref>: min w,s {g(w, s) : (w, s) ∈ Z}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property 2. (Boundedness)</head><p>The STM optimization problem in Problem (1) is bounded from below.</p><p>Proof. The boundedness can be observed from two aspects: (1) R f is bounded due to the quadratic term in f and non-negative s and L p . ( <ref type="formula" target="#formula_1">2</ref>) Ω s is bounded since K is positive semi-definite.</p><p>Following the same proof line, the above properties can be also shown for nonlinear STM defined with Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Algorithm</head><p>The following analysis mimics directly Sec. 4 in <ref type="bibr" target="#b29">[30]</ref>. We present the key steps for proving the convergence and refer to more details on this style of proof in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Alternate Convex Search: To solve the biconvex STM problem, a standard and popular approach is to exploit its convex substructure. We used the Alternate Convex Search (ACS) algorithm <ref type="bibr" target="#b70">[71]</ref>, a special case of Block-Relaxation Methods, by alternatively solving the convex subproblems. For explanation convenience, we recall the ACS algorithm in Algorithm 2.</p><p>Denote z = (w, s) as the solution variable. As mentioned in Sec. 4, STM can be seen as initializing s 0 using KMM, or simply as a vector of ones, and then solve the classifier w 1 as an unweighed SVM. As will be discussed below and in Sec. 8.5, the permutation of order does not influence the convergence. For Step 4, there are several ways to determine the stopping criterion. Here, we used the relative decrease of z compared to the last iteration. Below we discuss the convergence properties in terms of objective value (i.e., the difference between g(z t ) and g(z t-1 ) of two consecutive iterations t and t -1), and the variables (i.e., the difference between z t and z t-1 ). Step 2: Solve the convex optimization problem for fixed w t : s t+1 ← min s {g(w t , s), s ∈ Z wt };</p><formula xml:id="formula_19">5</formula><p>Step 3: Solve the convex optimization problem for fixed s t+1 : w t+1 ← min w {g(w, s t+1 ), w ∈ Z st+1 };</p><formula xml:id="formula_20">6</formula><p>Step 4: Set z t+1 ← (w t+1 , s t+1 );</p><formula xml:id="formula_21">7 Set t ← t + 1; 8 end</formula><p>Convergence: Recall that W and S are two non-empty sets, and Z ⊆ W × S is a biconvex set on W × S. We firstly show the convergence of the sequence of objective value {g(z t )} t∈N , and then convergence of the sequence of the variables {z t } t∈N .</p><p>Theorem 1. Let the STM objective function be g : Z → R. Then the sequence of objective value {g(z t )} t∈N generated by ACS converges monotonically.</p><p>Proof. The sequence {g(z t )} t∈N generated by Algorithm 2 decreases monotonically, since g(w * , s * ) ≤ g(w, s * ), ∀w ∈ Z s * and g(w * , s * ) ≤ g(w * , s), ∀s ∈ Z w * . In addition, Property 2 shows g is bounded from below. According to Theorem 4.5 in <ref type="bibr" target="#b29">[30]</ref>, the sequence {g(z t )} t∈N converges to a limit real value.</p><p>Theorem 1 only tells the convergence of the sequence {g(z t )} t∈N but not of the sequence {z t } t∈N . See Example 4.3 in <ref type="bibr" target="#b29">[30]</ref> where {g(z t )} t∈N converge but {z t } t∈N diverge. The following states the condition for convergence of {z t } t∈N . Theorem 2. Let W and S be closed sets, and z t = (w t , s t ) t∈N where w t ∈ W and s t ∈ S. The sequence of variables {z t } t∈N generated by ACS converge to z * ∈ W × S.</p><p>Proof. This can be proved using Theorem 4.7 in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">STM WITH LABELED TARGET DATA (L-STM)</head><p>As discussed above, STM requires no labels from the target subject to obtain the personalized classifier. Nevertheless, in many problems one might collect partially labeled data from the target domain, or acquire additional guidance with a few manual labels. Such labels can be considered as the only reference to the target subject and aid the determination of the personalized classifier. This section describes an inductive extension of STM, termed L-STM, to adapt target labels for personalizing a classifier. Given the target data and their labels as </p><formula xml:id="formula_22">D L = {x L j , y L j } n L j=1 , y L j ∈ {+1, -1}, 0 ≤ n L ≤ n te ,</formula><formula xml:id="formula_23">min f,s R f (D tr , s)+λΩ s (X tr , X te )+λ L Ω L (D L ),<label>(16)</label></formula><p>where λ L &gt; 0 is a tradeoff parameter. A choice of large λ L makes sure the labeled target data are correctly classified. The goal of Ω L (D L ) is to regulate the classification quality on the labeled target data. In this paper, we define</p><formula xml:id="formula_24">Ω L (D L ) = n L j=1 L p (y L j , f (x L j )).</formula><p>Note that an L 2 loss here is analogous to the regularization in Least Square SVM <ref type="bibr" target="#b68">[69]</ref>, which performs comparably with SVM using the hinge loss and has been shown to relate to a ridge regression approach for binary classification, such as our task at hand. Because Ω L (D L ) is convex in f , problem <ref type="bibr" target="#b15">(16)</ref> is still a biconvex optimization problem, and thus the ACS algorithm can be directly applied.</p><p>We show that solving problem ( <ref type="formula" target="#formula_23">16</ref>) is equivalent to solving the original STM using a training set augmented with weighted labeled target data. We demonstrate the use of L 2 loss on linear SVM, while different choices of loss functions (e.g., L 1 ) and classifier types (e.g., nonlinear SVM) can be applied. Specifically, updating for s remains the same process. For updating w, one can again use Newton's method by associated gradient and Hessian:</p><formula xml:id="formula_25">∇ w = w + X S( X w -y),<label>(17)</label></formula><formula xml:id="formula_26">H w = I d + X S X ,<label>(18)</label></formula><p>where X = X tr |X L is the augmented set with labeled target data, S = 2CSI 0 0 0 λ L I n L is the augmented re-weighting matrix, and y = y y L is the augmented labels. The above equivalence is useful particularly for the scenario of AU detection, where the unlabeled videos are usually abundant with limited laborious FACS coding. L-STM allows users to add just a few frames to alleviate false detections significantly.  NA T-SVM <ref type="bibr" target="#b16">[17]</ref> non-convex KMM <ref type="bibr" target="#b30">[31]</ref> convex DA-SVM <ref type="bibr" target="#b5">[6]</ref> non-convex DT-MKL <ref type="bibr" target="#b21">[22]</ref> jointly convex optional DAM <ref type="bibr" target="#b22">[23]</ref> convex optional STM (proposed) bi-convex optional : included, : omitted, NA: not applicable third and fourth rows illustrate the detection of generic SVM and KMM. Both approaches produced many false detections due to the person-specific biases and the lack of weight refinement. STM, on the fifth row, greatly reduced false positives and produced a better F1 score. The last row shows the detection using L-STM with two misclassified frames from STM with correct labels. Using the label information on the target domain, L-STM boosted ∼10% F1 score by using labels from only two frames. As we observed empirically, the more the labeled target data are introduced, the better L-STM approaches the ideal classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION OF RELATED WORK</head><p>A few related efforts use personalized modeling for facial expression analysis, e.g., AU intensity estimation <ref type="bibr" target="#b52">[53]</ref>. STM differs from them in how it accomplishes personalization. Chang and Huang <ref type="bibr" target="#b7">[8]</ref> introduced an additional face recognition module and trained a neural network on the combination of face identities and facial features. Romera-Paredes et al. <ref type="bibr" target="#b50">[51]</ref> applied multi-task learning to learn a group of linear models and then calibrated the models toward the target subject using target labels. By contrast, STM requires neither a face recognition module nor target labels. Motivated by covariate shift <ref type="bibr" target="#b62">[63]</ref>, Chen et al. <ref type="bibr" target="#b13">[14]</ref> proposed transductive and inductive transfer algorithms for learning personspecific models. In their transductive setting, KL-divergence was used to estimate sample importance. However, STM models the domain mismatch using KMM <ref type="bibr" target="#b30">[31]</ref>, which with proper tuning, as implied in <ref type="bibr" target="#b63">[64]</ref>, yields better estimation.</p><p>The most related work is transductive transfer learning, which seeks to address domain shift problems without target labels. Table <ref type="table" target="#tab_4">3</ref> summarizes the comparison. DT-MKL <ref type="bibr" target="#b21">[22]</ref> simultaneously minimizes the MMD criterion <ref type="bibr" target="#b4">[5]</ref> and a multi-kernel SVM. DAM <ref type="bibr" target="#b22">[23]</ref> leverages a set of pre-trained base classifiers and solves for a test classifier that shares similar predictions with the base classifiers on unlabeled data. However, similar to T-SVM <ref type="bibr" target="#b35">[36]</ref> and SVM-KNN <ref type="bibr" target="#b80">[81]</ref>, these methods treat training data uniformly. By contrast, KMM <ref type="bibr" target="#b30">[31]</ref> and STM consider importance re-weighting, properly adjusting the importance for each training instance to move the decision function toward test data. KMM performs re-weighting only once while STM does so in an iterative manner. From this perspective, KMM can be viewed as an initialization of STM (see Sec. 4). In addition, STM uses training loss to refine instance weights in successive steps, thus being able to correct sub-optimal weights. DA-SVM <ref type="bibr" target="#b5">[6]</ref> refines instance weights as a quadratic function decaying with iterations. However, DA-SVM may fail to converge due to its non-convexity, while STM is formulated as a bi-convex problem and thus assures convergence. Moreover, STM can be extended to tackle labeled target data, which greatly improves the performance. Multi-person social interaction Frame-by-frame -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTS</head><p>STM was evaluated in datasets that afforded inclusion of both posed and unposed facial expression, frontal versus variable pose, complexity (e.g., interview versus 3-person interaction), and differences in numbers of subjects, the amount of video per subject, and men and women of diverse ethnicity. These factors are among the individual differences that adversely affect classifier performance in previous work <ref type="bibr" target="#b27">[28]</ref>. To evaluate STM with respect to alternative approaches and scenarios, it was compared with a generic classifier, person-specific classifiers, and cross-domain classifiers using within-subject, cross-subject, and cross-dataset scenarios. Operational parameters for STM included initialization order, parameter choice, and domain size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Dataset Description</head><p>We tested the algorithms on four diverse datasets that involve posed, acted, or spontaneous expressions, and vary in video quality, length, annotation, the number of subjects, and context, as summarized in Table <ref type="table" target="#tab_5">4</ref> and illustrated in Fig. <ref type="figure" target="#fig_11">7</ref>.</p><p>(1) The extended Cohn-Kanade (CK+) dataset <ref type="bibr" target="#b43">[44]</ref> contains brief (approximately 20 frames on average) videos of posed and un-posed facial expressions of men and women of various ethnic backgrounds. Videos begin with a expression and finish at the apex, or peak, which is annotated for AU and for holistic expression. Changes in pose and illumination are relatively small. Posed expressions from 123 subjects and 593 videos were used. Because STM requires some number of frames to estimate a test distribution, it is necessary to modify coding in CK+. In specific, we assume the last one-third frames share the same AU labels. We note that this may introduce some errors, compared to related methods that use only the peak frame for classification.</p><p>(2) The GEMEP-FERA dataset [67] consists of 7 portrayed emotion expressions by 10 trained actors. Actors were instructed to utter pseudo-linguistic phoneme sequences or a sustained vowel and display pre-selected facial expressions. Head pose is primarily frontal with some fast movements. Each video is annotated with AUs and holistic expressions. We used the GEMEP-FERA training set, which comprises 7 subjects (three of them men) and 87 videos.</p><p>(3) RU-FACS dataset <ref type="bibr" target="#b3">[4]</ref> consists of video-recorded interviews of 100 young adults of varying ethnicity. Interviews are approximately 2.5 minutes in duration. Head pose is frontal with small to a moderate out-of-plane rotation. AU are coded if the intensity is greater than 'A', i.e., lowest intensity on a 5-point scale. We had access to 34 of the interviews, of which video from 5 subjects could not be processed for technical reasons. Thus, the experiments reported here were conducted with data from 29 participants with more than 180,000 frames in total.</p><p>(4) GFT <ref type="bibr" target="#b56">[57]</ref> consists of social interaction between 720 previously unacquainted young adults that were assembled into groups of three persons each and observed over the course of a 30minute group formation task. Two minutes of AU-annotated video from 14 groups (i.e., 42 subjects) was used in the experiments for a total of approximately 302,000 frames. Head pose varies over a range of about plus/minus 15-20 degrees <ref type="bibr" target="#b27">[28]</ref>. For comparability with RU-FACS, we included AU 6, 9, 12, 14, 15, 20, 23 and 24.</p><p>Out of these datasets, CK+ is the most controlled, followed by GEMEP-FERA. Both include annotation for holistic expression and AU. GEMEP-FERA introduces variations in spontaneous expressions and large head movements but contains only 7 subjects. RU-FACS and GFT are both unposed and vary in complexity. RU-FACS is an interview context; GFT is a social interaction over a longer duration with greater variability. The first sets of experiments focus on CK+, GEMEP, and RU-FACS. GFT figures primarily in experiments on domain transfer between datasets and on the influence of numbers of subjects on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Settings</head><p>Face tracking &amp; registration: For CK+, FERA, and GFT, 49 landmarks were detected and tracked using the Supervised Descent Method (SDM) <ref type="bibr" target="#b73">[74]</ref>. For RU-FACS, we used available AAM detection and tracking of 68 landmarks. Tracked landmarks were registered to a 200×200 template shape.</p><p>Feature extraction: Given a registered facial image, SIFT descriptors were extracted using 36×36 patches centered at selected landmarks (9 on the upper face and 7 on the lower face), because AUs occur only in local facial regions. The dimensionality of the descriptors was reduced by preserving 98% PCA energy.</p><p>AU selection &amp; evaluation: Positive samples were taken as frames with an AU presence and negative samples as frames without an AU. We selected the 8 most commonly observed AUs across all datasets. To provide a comprehensive evaluation, we report both Area Under the ROC Curve (AUC) and F1 score. As AUC was originally designed for balanced binary classification tasks, F1 score, as the harmonic mean of precision and recall, could be more meaningful for imbalanced data, such as AUs.</p><p>Dataset split &amp; validation: A leave-one-subject-out protocol was used. For each AU, we iteratively chose one subject for test  and the remaining subjects for training and validation. For all iterations, we first identified the range of λ ∈ {2 -10 , ...2 10 } and C ∈ {2 -10 , ..., 2 10 } for which F1 score on the validation set was greatest. Then, we chose ones for which C was small. That is, we sought the parameters that maximize F1-score while preserving large margin of the decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Action Unit (AU) Detection</head><p>We evaluated STM with generic and alternative approaches using three scenarios for AU detection: within-subject, cross-subject, and cross-dataset. We report results separately for each scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Within-subject AU detection</head><p>A natural comparison with STM is a classifier trained on a single subject, also known as a Person-Specific (PS) classifier. A PS classifier can be defined in at least two ways. One, the more common definition, is a classifier trained and tested on the same subject. We refer to this usage as PS 1 . The other definition, referred to as PS 2 or quasi-PS, is a classifier that has been tested on a subject included in the training set. The GEMEP-FERA competition <ref type="bibr" target="#b66">[67]</ref> defined PS in this way. An SVM trained with PS 2 (PS 2 -SVM) is sometimes considered to be a generic classifier (e.g., <ref type="bibr" target="#b44">[45]</ref>). In our usage, we reserve the term "generic classifier" to the case in which training and test subjects are independent.</p><p>Here we compared STM with both PS 1 -SVM and PS 2 -SVM, and summarize the results in Table <ref type="table" target="#tab_6">5</ref>  <ref type="table">6</ref> shows the selection percentage of STM upon initialization and convergence. Each row sums to 1 and represents a test subject; each entry within one row denotes the percentage of selected samples from each training subject. For example, (a) shows the initialization phase that, when testing on Subject 2, 26% of training samples were selected from Subject 1. Upon convergence, as (b) shows, STM selected most training samples that belong to the target subject (higher diagonal value). Note that the selection percentages along the diagonal do not sum to 100% due to insufficient training samples for the target subject. However, STM was able to select relevant training samples, even from different subjects, to alleviate the mismatch between training and test distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Cross-subject AU detection</head><p>Using a cross-subject scenario, i.e., training and test subjects are independent in all iterations (a.k.a., leave-one-subject-out), we compared STM against various types of methods. Unsupervised domain adaptation methods are closest to STM. For comparisons we included Kernel Mean Matching (KMM) <ref type="bibr" target="#b30">[31]</ref>, Domain Adaptation SVM (DA-SVM) <ref type="bibr" target="#b5">[6]</ref>, and Subspace Alignment (SA) <ref type="bibr" target="#b24">[25]</ref>. Multiple source domain adaptation methods serve as another natural comparison by treating each training subject as one source domains; we compared to the state-of-the-art DAM <ref type="bibr" target="#b22">[23]</ref>. For baseline methods, we compared with linear SVMs and semi-supervised Transductive SVM (T-SVM) <ref type="bibr" target="#b16">[17]</ref>. T-SVM, KMM, DAM and SA were implemented per the respective author's webpage. Because STM requires no target labels, methods that use target labels for adaptation (e.g., <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b53">[54]</ref>) were not included.</p><p>All methods were compared in CK+ and RU-FACS with a few exceptions in CK+. In CK+, SA was ruled out because too few frames were available per subject to compute meaningful subspaces. DAM was also omitted in CK+ because it would be problematic to choose negative samples given the structure of the data (i.e., pre-segmented positive examples). In training, a Gaussian kernel was used with bandwidth set as the median distance between pairwise samples. For KMM and STM we set B = 1000 so that none of s i reached the upper bound, and</p><formula xml:id="formula_27">= √ ntr-1 √</formula><p>ntr . As reported in <ref type="bibr" target="#b30">[31]</ref>, when B was reduced to the point where a small percentage of the s i reached B, empirically performance either did not change, or worsened. For T-SVM we used <ref type="bibr" target="#b16">[17]</ref> since the original T-SVM <ref type="bibr" target="#b35">[36]</ref> solves an integer programming and thus unscalable to our problem that consists hundreds of thousands of frames. For fairness, we used linear SVMs in all cases. In DA-SVM, we used LibSVM <ref type="bibr" target="#b6">[7]</ref> as discussed in Sec. 4, τ = 0.5 and β = 0.03. For SA, we obtained the dimension of subspaces d max using their theoretical bound with γ = 10 6 and δ = 0.1; SA with both NN and SVM classifiers were reported. Following <ref type="bibr" target="#b22">[23]</ref>, we tuned DAM using C = 1, λ L = λ D1 = λ D2 = 1; β was set as the median of computed MMD value <ref type="bibr" target="#b4">[5]</ref>; the threshold for virtual labels were cross-validated in {0.01, 0.1, 0.5, 1}. Linear SVMs were used as base classifiers. Note that, because these alternative methods are not optimized for our task, their performance might be improved by searching over a wider range of parameters. Discussion: Tables <ref type="table" target="#tab_8">7</ref> and<ref type="table">8</ref> show results on AUC and F1 scores. A linear SVM served as a generic classifier. For semisupervised learning, T-SVM performed similarly to SVM in RU-FACS, but worse than SVM in CK+. An explanation is because in CK+ the negative (neutral) and positive (peak frames) samples are easier to separate than consecutive frames in RU-FACS. For transductive transfer learning, KMM performed worse than the generic classifier, because KMM estimates sample weights without label information. On the other hand, SA combined with both Nearest Neighbor (NN) and LibSVM led to unsatisfactory performance compared to above methods. This is because SA obtained an optimal transformation through linear subspace representation, which could be improper due to the non-linearity of our data. In addition, SA weighted all training samples equally, and thus suffered from biases caused by individual differences (as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>). Although SA+SVM performed better in AUC, its low F1 score tells a likely overfitting (low precision or recall). The proposed STM outperformed alternative approaches in general. For AUC in RU-FACS, STM had the highest averaged score about 6% higher over the 2nd highest, and the highest scores in all but 2 AUs. For F1, STM had the highest averaged score about 12 points higher than the nearest alternative, and the highest F1 score of all but AU4. For CK+, STM achieved 91% AUC on average, slightly better than the best-published result 90.5% <ref type="bibr" target="#b40">[41]</ref>, although the results may not be directly comparable due to different choices of features and registration. It is also noteworthy that we tested the last one-third of a video that could contain low intensities, while <ref type="bibr" target="#b40">[41]</ref> tested only on peak frames with the highest intensity. On the other hand, STM may be benefited from additional frames due to more information.</p><p>Unlike STM that uses a penalized SVM, T-SVM and SA considered neither re-weighting for training instances nor weight refinement for irrelevant samples, such as noises or outliers. On the other hand, DA-SVM extends T-SVM by progressively labeling test patterns and removing labeled training patterns. Not surprisingly, DA-SVM showed better performance than KMM and T-SVM, because it selected relevant samples for training and thus obtained a better classifier. However, similar to T-SVM, DA-SVM did not update the re-weightings using label information. Moreover, it is not always guaranteed to converge to a correct solution.</p><p>In our experiments, we faced the situation where DA-SVM failed to converge due to a large amount of samples lying within the margin bounds. In contrast, STM is a biconvex formulation, and therefore guaranteed to converge to a critical point and outperform existing approaches (details in Sec. 4).</p><p>As for multi-source domain adaptation, DAM overall performed comparably in AUC, but significantly worse than STM in F1. There are at least three explanations. First, AUs are by nature imbalanced: Simply predicting all samples as negative could yield high AUC for infrequent AUs (such as AUs 4), yet zero precision and recall for F1 score. Second, similar to person-specific classifiers, training samples for each subject are typically insufficient to estimate the true distribution (as discussed in Sec. 8.3.1). Using such limited training samples for each subject, therefore, limits the power of base classifiers and the final prediction in DAM. Finally, DAM uses MMD to estimate inter-subject distance, which could be inaccurate due to insufficient samples or sampling bias (e.g., some subjects have more expressions than others).</p><p>Although in Table <ref type="table" target="#tab_8">7</ref>(a) STM achieved slightly worse in AUC, STM showed a better improvement in F1 metric, which better suits our imbalanced detection task. A major reason that limits STM's improvement is because GEMEP-FERA comprises limited subjects and training samples, and thus hinders STM from selecting and receiving proper supports from the training samples. This can be also explained by the findings of selection ability in Sec. 8.3.1. When the number of subjects and training samples increase, as illustrated by the CK+ and the RU-FACS datasets in Tables <ref type="table">8</ref> and<ref type="table" target="#tab_8">7</ref>, STM is able to gain contributions from the selected data, and thus the improvement becomes clearer. Overall STM achieves the most competitive performance due to the properties of instance re-weighting, weight refinement, and convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3">Cross-dataset AU detection</head><p>Detecting AUs across datasets is challenging because of differences in acquisition and participant characteristics and behavior. As shown in Fig. <ref type="figure" target="#fig_11">7</ref>, participant characteristics, context, background, illumination, camera parameters, compression schemes are among the differences that may bias features. Generic SVMs fail to address such differences. Secs. 8.3.1 and 8.3.2 have shown the effectiveness of STM on within-dataset experiments involving within-subject and across-subject scenarios. This section aims to justify that STM can attain not only subject adaptation but can be naturally extended for cross-dataset adaptation. Specifically, we performed two experiments, RU-FACS→GEMEP-FERA and GFT→RU-FACS, using the same settings described above. Table <ref type="table" target="#tab_9">9</ref> shows the results. One can observe that cross-domain approaches outperformed a generic SVM in most cases. It is not surprising because a generic SVM does not model the biases between datasets. That is, in the cross-dataset scenario, the training and test distributions are more likely different than in withindataset scenario, causing an SVM to fail to transfer the knowledge from one dataset to another. Among the cross-domain methods, STM consistently outperforms the others. Observe STM gained improvement over SVM in Table <ref type="table" target="#tab_8">7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Holistic Expression Detection</head><p>Taking into account of individual differences, STM showed improvement for AU detection. In this experiment, we ask whether the same could be found for holistic expression detection. We used the major benchmarks CK+ <ref type="bibr" target="#b43">[44]</ref> and FERA emotion subchallenge <ref type="bibr" target="#b66">[67]</ref> for this experiment, and the same settings in Sec. 8.2, except for that the labels were replaced as holistic expressions. Similar to <ref type="bibr" target="#b66">[67]</ref>, we utilized every frame of a video to train and test our algorithm. Because each video has only a single expression label instead of a frame-by-frame labeling, F1 score is meaningless in this experiment. For CK+, 327 out of the original 593 videos were given a nominal expression label based on the 7 basic and discrete expressions: Anger, Contempt, Disgust, Fear, Happy, Sadness, and Surprise. For GEMEP-FERA, 289 portrayals were retained one out of the five expression states: Anger, Fear, Joy, Sadness, and Relief. The training set included 7 actors with 3∼5 instances of each expression per actor. We evaluated on the training set, which contains a total of 155 videos. STM was also compared to alternative approaches discussed in Sec. 8.3.2.</p><p>Table <ref type="table" target="#tab_1">10</ref>(a) shows the results from CK+. Note that DA-SVM is unavailable in this experiment because it failed to converge to a final classifier due to insufficient test data, recalling that we used the last one-third frames of each video for test. One can observe that a generic SVM performed fairly well because positive (peak expressions) and negative samples (neutral faces) are relatively easy to separate in CK+. KMM and T-SVM resulted in suboptimal results due to the lack of a weight-refinement step, and thus were unable to rectify badly estimated weights for learning the final classifier (see discussions in Sec. 7). This effect becomes obvious when there is insufficient test data, such as this experiment. On the other hand, STM considers the labels for weight refinement and performed similarly as well as a generic SVM.</p><p>Table <ref type="table" target="#tab_1">10</ref>(b) presents our results on GEMEP-FERA, which served as a larger and more challenging benchmark for evaluating the holistic expression detection performance. In this experiment, each test video consists of tens of frames, and thus enables DA-SVM to converge in most cases. The generic SVM performed poorly due to large variations in this dataset, such as head movements and spontaneous expressions. Without the ability to select meaningful training samples, the generic classifier suffered from the individual differences. Other cross-domain methods alleviated the person-specific biases and produced better results. Overall STM achieved the most satisfactory performance. This serves as evidence that when training data grow larger and more complex, the improvement of STM becomes clearer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.1">Initialization order</head><p>A potential concern of STM is that the initialization order could affect the convergence property and performance. To evaluate this, we examined the initialization order with w 0 (STM w ) and with s 0 (STM s ). Standard two-stage approach, i.e., solving the selection coefficients first and then the penalized SVM (e.g., <ref type="bibr" target="#b30">[31]</ref>), can be interpreted as STM w , as discussed in Sec. 4. To validate convergence property of STM, we randomized 10 initialization sets for STM w and STM s respectively. Upon the convergence of STM, we computed their objective differences in consecutive iterations (g(z t+1 ) -g(z t )), and the absolute sum of variable difference ( z t+1 -z t 1 ). For the cases where STM took fewer iterations to converge, we set the difference of later iterations to 0.</p><p>Fig. <ref type="figure" target="#fig_13">8</ref>(a) shows the curve of mean and standard deviation of differences across the iterations of STM w and STM s . Note that the differences were scaled for visualization convenience. The random initial value was reflected in the first iteration and made a major difference with the value of the second iteration. One can observe that in STM w and STM s , both the objective value and difference between consecutive variables decreased at each step and toward convergence, as theoretically detailed in Sec. 5. Note that, although the resulting solution was slightly different due to different initialization, the performance remains the same as both converge to a critical point. We observed so by comparing the confusion matrices during the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.2">Parameter choice</head><p>Recall that training STM involves two parameters: C for the tradeoff between maximal margin and training loss, and λ for the tradeoff between the SVM empirical risk and the domain mismatch. This section examines the sensitivity of performance with respect to different parameter choices. Specifically, we ran the experiment of detecting AU12 on the CK+ dataset with the parameters ranges C ∈ {2 -10 , ..., 2 10 } and λ ∈ {2 -10 , ..., 2 10 }. Following the experiment settings in Sec. 8.2, we used the leaveone-subject-out protocol and computed an averaged F1 score for evaluating the performance. We used Gaussian kernel with a fixed bandwidth as the median distance between sample points. Fig. <ref type="figure" target="#fig_13">8(c</ref>) illustrates the contour plot of F1 score v.s. different parameter pairs in terms of (log 2 (C), log 2 (λ)). As can be observed, the performance scatters evenly in most region of the plot, showing that STM is robust to the parameter choices when their values are reasonable. The performance decayed when both (C, λ) become extremely small (&lt; 2 -6 ), as shown in the bottom left of the plot. This is not surprising because smaller values of C and λ imply less emphasis on training loss and personalization. Note that with large enough λ, STM does not need large C to achieve comparable F1, providing an explanation that personalization helps avoid imposing large C and hence avoid overfitting. As a general guideline for choosing parameters, we suggest a small value of C with a reasonable λ (thus encouraging a large-margin decision boundary with reasonable distribution mismatch).</p><p>We note that cross validation (CV) for domain adaptation methods is difficult and remains an open research issue. As also mentioned in <ref type="bibr" target="#b63">[64]</ref>, this issue becomes vital in a conventional scenario where the number of training samples is much smaller than the number of test samples. However, in our case, we always have much more training samples than test samples, and thus, the CV process is less biased under covariate shift. In addition, as can be seen in Fig. <ref type="figure" target="#fig_1">2</ref> of <ref type="bibr" target="#b63">[64]</ref>, with proper σ (kernel bandwidth) and standard CV, KMM consistently reaches lower error than the KL-divergence-based CV <ref type="bibr" target="#b63">[64]</ref>. This serves as a justification for KMM's ability to estimate importance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.3">Domain size</head><p>The intuition for STM to work better in facial expression analysis is a judicious selection of training samples. The availability of richer diversity grants STM a broader knowledge to select better candidates that match the test distribution. This experiment examines performance changes w.r.t. diversities of the source domain, for which we evaluated by the domain size or the number of training subjects. Intuitively, the larger number of training subjects, the more diverse the training domain is, and thus the more likely STM could perform better. We compared STM to a generic SVM (with cross-validation) to contrast the performance.</p><p>This experiment was performed on AU 12 using the RU-FACS dataset. A subset from 3 to 27 training subjects was randomly picked as a shrunk domain. The leave-one-subject-out protocol and F1 score were used following Sec. 8.2. Fig. <ref type="figure" target="#fig_14">9</ref>(a) illustrates the effects of #training subjects on averaged F1 scores. For each domain size, the mean and standard deviation were computed on F1 scores over all test subjects. Test subjects without true positives were ignored because their precision and F1 scores were not computable. One can observe that, as #training subjects grew, STM achieved higher F1 scores, and also performed more consistently with lower standard deviation. This observation imitates Sec. 8.3.2, where a source domain with poor diversity was shown to limit STM's performance. On the other hand, generic classifier improved when #training subjects arose to 12. However, with more training subjects being introduced, its performance was slightly lowered due to the biases caused by individual differences. Note that, because the training subjects were downsampled in a randomized manner, it is possible that STM achieved better performance on a domain with less training subjects.</p><p>As another justification, we examined the effects of domain size on the GFT dataset <ref type="bibr" target="#b56">[57]</ref>, which contains a larger number of subjects and more intensive facial expressions than RU-FACS. The GFT dataset records videos of real-life social interactions among three-person groups in less constrained contexts. Videos were recorded using separate wall-mounted cameras facing each subject; Fig. <ref type="figure" target="#fig_13">8(e</ref>) shows exemplar frames. The videos include moderate-to-large head rotations and frequent occlusions; facial movements are spontaneous and unscripted. We selected 50 videos with around 3 minutes each (5400 frames).</p><p>Following the same procedure, we randomly picked a subset of subjects varying from 4 to 49 as the shrunk domains. Note that for subject 4 (the 4th row), there is no F1 score because AU 12 was absent. One can observe that for 6 outlier subjects (e.g., rows <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48)</ref>, their F1 scores remained low even as the number of subjects was increased. This result suggests that these subjects share no or few instances in the feature space. Visual inspection of their data was consistent with this hypothesis. The outliers were ones with darker skin color, asymmetric smiles or relatively large head pose variations. Thus, for these subjects STM could offer no benefit. This finding suggests the need to include greater heterogeneity in training subjects. When these subjects were omitted, as shown in Fig. <ref type="figure" target="#fig_14">9</ref>(c), the F1 scores are markedly higher. The influence of the domain size becomes clear and replicates Fig. <ref type="figure" target="#fig_14">9</ref>(a). It is interesting to note that, for classifiers, the performance increased until 24 training subjects and then drops abruptly. This observation serves as another evidence that individual differences (introduced by increasing number of training subjects) could bias generic classifiers.</p><p>Between these two experiments, generally the averaged F1 score in GFT is higher than in RU-FACS. At least two factors may have accounted for this difference. One is that participants in GFT may have been less inhibited and more expressive. In RU-FACS, subjects were motivated to convince an examiner of their veridicality. They knew that they would be penalized if they were not believed. In the three-person social interaction of GFT, there were no such negative contingencies. Subjects may have felt more relaxed and become more expressive. More intense AUs are more easily detected. The other factor is that inter-observer reliability of the ground truth FACS labels was likely much higher for GFT than for RU-FACS. Kappa coefficients for GFT were exceptionally good. While reliability for RU-FACS is not available, we know from past confirmation-coding that inter-observer agreement was not as high. Less error in the GFT ground truth would contribute to more accurate classifier performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Discussion</head><p>In above experiments, we have evaluated STM against alternative methods in many scenarios: Within-subject (Sec. 8.3.1), across-subject (Sec. 8.3.2), across-dataset (Sec. 8.3.3), and holistic expression detection (Sec. 8.4). We also analyzed STM on its initialization order, and sensitivity to parameters and domain size (Sec. 8.5). STM consistently outperformed a generic SVM and most transfer learning methods. The advantage of STM is clearest in GFT, where the variety of subjects are more extensive, and slightly so, in RU-FACS. The results indicate a more obvious improvement in F1 than in AUC, in large complex datasets than in posed datasets, in cross-dataset scenario than in within-dataset scenario, and with more training subjects than with fewer ones.</p><p>STM has some limitations. For example, it suffers from the lack of training subjects or crucial mismatch between training  and test distributions, which are known as common drawbacks in unsupervised domain adaptation methods. For a theoretical analysis in terms of performance v.s. the number of samples, Corollary 1.9 in KMM <ref type="bibr" target="#b28">[29]</ref> reaches a transductive bound for an estimated risk of a re-weighted task, given the assumptions of linear loss and data being iid. However, it remains unclear how to theoretically analyze STM's performance in terms the number of test samples, because STM involves nonlinear loss functions and the data are from real-world videos (non-iid).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION AND FUTURE WORK</head><p>Based on the observation of individuals differences, this paper proposed Selective Transfer Machine (STM) for personalized facial expression analysis. We showed that STM translates to a biconvex problem, and proposed an alternate algorithm with a primal solution. In addition, we introduced L-STM, an extension of STM that exhibited significant improvement when labeled test data are available. Our results on both AU and holistic expression detection suggested that STM is capable of improving test performance by selecting training samples that form a close distribution to a test one. Experiments using within-subject, cross-subject, and cross-dataset scenarios revealed two insights: <ref type="bibr" target="#b0">(1)</ref> Training data matter unevenly for test data, and (2) extending the variety of training subjects brings more value in improving performance.</p><p>It is worth noting that STM can be extended to other classifiers with convex decision functions and losses, such as logistic regression. This is a direct outcome of Property 1 in Sec. 5.1. However, for non-convex cases, such as random forest, local minimum could cause worse performance. We leave extensions to nonconvex classifiers as a focus of future work. Moreover, improving STM's training speed could be another direction due to the QP for solving s. Finally, while this study focuses evaluations on facial expressions, STM could be applied to other fields where objectspecific issues are involved, e.g., object or activity recognition.</p><p>Acknowledgments: The authors would like thank many anonymous reviewers for constructive feedback. Research reported in this paper was supported in part by the National Institutes</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the proposed Selective Transfer Machine (STM): (a) 2D PCA projection of positive (squares) and negative (circles) samples for a given AU (in this case AU 12 or lip-corner raiser) for 3 subjects. An ideal classifier separates AU 12 nearly perfectly for each subject. (b) A generic classifier trained on all 3 subjects generalizes poorly to a new person (i.e., test subject) due to individual differences between the 3subject training set and the new person. STM personalizes a generic classifier and reliably separates an AU for a new subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of samples from the RU-FACS dataset [4] in 3D eigenspace: colors/markers indicate different (a) positive/negative classes, and (b) subjects (best viewed in color).</figDesc><graphic coords="2,434.12,43.70,129.46,145.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Fitting a line to a quadratic function using KMM and other reweighting methods. The larger size (more red) of training data, the more weight KMM adopted. As can be observed, KMM puts higher weights in the training samples closer to the test ones. Compared to standard OLS or WOLS, KMM allows to better approximation for the test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 4 5</head><label>14</label><figDesc>Selective Transfer Machine Input : X tr , X te , parameters C, λ Output: Inferred test labels y for test data 1 Initialize training loss p ← 0; 2 while not converged do 3 Update the instance-wise re-weighting s by solving the QP in (6); Update the decision function f and training loss p by solving the penalized SVM in (2) or (3); Infer test labels by y ← f (X te )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Loss functions: (a) L 1 and L 2 loss, and (b) Huber loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>it# 1 (Fig. 5 .</head><label>15</label><figDesc>Fig. 5. Comparisons of a generic SVM, personalized STM, and ideal classifier for synthetic data. The left-most figure shows the convergence curve of the objective value where STM converges in 12 iterations. Figures it#1,4,8,12 with training/test accuracy (Tr% and Te%) show the corresponding hyperplanes in at each iteration. Grey (shaded) dots denote training data, and white (unshaded) dots denote test data. Circles and squares denote positive and negative classes, respectively. Note that it#1 indicates the result of KMM<ref type="bibr" target="#b30">[31]</ref>. STM improves separation relative to generic SVM as early as the first iteration and converges toward the ideal hyperplane by the 12 th iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Property 1 .</head><label>1</label><figDesc>(Bi-convexity) Selective Transfer Machine (STM) in (1) is a biconvex optimization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2 :</head><label>2</label><figDesc>Alternate Convex Search Algorithm 1 Step 1: Choose a starting point z 0 ← (w 0 , s 0 ) ∈ Z; 2 Set t ← 0; 3 while not converged do 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of different methods on the RU-FACS dataset. Light yellow (dark green) indicates AU 12 presense (absense) of Subject 12. The numbers in the parentheses are F1 scores. Two misclassified frames of STM were chosen and fed into L-STM with correct labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>we formulate L-STM by introducing an additional regularization term Ω L (D L ) to (1):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig. 6 illustrates the benefits of L-STM over different methods. Light yellow (dark green) indicates positive (negative) frames for AU 12 on Subject 12 of the RU-FACS dataset. Top two rows show the ground truth and the detection result of the ideal classifier, respectively. The numbers in the parentheses indicate the F1 score. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Example images from (a) CK+ [44], (b) GEMEP-FERA [67], and (c) RU-FACS [4] datasets.</figDesc><graphic coords="8,337.24,247.95,68.40,51.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8 .</head><label>8</label><figDesc>Analysis experiments: (a)-(b) Objective and variable differences between iterations with initialization w 0 (STMw) and s 0 (STMs), respectively. (c) Performance versus parameter choices. (d) Per-subject F1 score v.s. # training subjects. (e) Exemplar images of the GFT dataset [57].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 (</head><label>9</label><figDesc>b) shows the F1 scores with respect to the number of training subjects. One can observe the averaged F1 score increases with #training subjects, although the standard deviation fluctuates. To study the fluctuation, we broke down the averaged F1 into individual subjects corresponding to different training sizes, as shown in Fig. 8(d). Each row represents a test video; each column represents one number of training subjects (ranging from 4 to 49).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance versus domain size: The averaged and standard deviation of F1 score on (a) RU-FACS. (b) and (c) show the F1 scores on the GFT dataset before and after removing the outlier subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Representative feature extraction methods</cell><cell></cell></row><row><cell>Type</cell><cell>Feature</cell><cell cols="2">Year Reference</cell></row><row><cell></cell><cell>Shape model parametrization</cell><cell>2012</cell><cell>[45]</cell></row><row><cell>Geometric</cell><cell>Geometry of facial components</cell><cell>2010</cell><cell>[85]</cell></row><row><cell></cell><cell>Landmark locations</cell><cell>2006</cell><cell>[45]</cell></row><row><cell></cell><cell>Active facial patches</cell><cell>2012</cell><cell>[84]</cell></row><row><cell></cell><cell>SIFT/DAISY</cell><cell>2011</cell><cell>[86]</cell></row><row><cell></cell><cell cols="2">Discrete Cosine Transform (DCT) 2011</cell><cell>[27]</cell></row><row><cell>Appearance</cell><cell>Local Phase Quantization (LPQ) Local Binary Patterns (LBP)</cell><cell>2011 2009</cell><cell>[35] [59], [67]</cell></row><row><cell></cell><cell>Hist. of Oriented Gradient (HOG)</cell><cell>2009</cell><cell>[48]</cell></row><row><cell></cell><cell>Gabor</cell><cell>2006</cell><cell>[4], [41]</cell></row><row><cell></cell><cell>Raw pixels</cell><cell>2000</cell><cell>[37]</cell></row><row><cell></cell><cell>Longitudinal expression atlases</cell><cell>2012</cell><cell>[33]</cell></row><row><cell></cell><cell>Gabor motion energy</cell><cell>2010</cell><cell>[73]</cell></row><row><cell>Dynamic</cell><cell>Bag of Temporal Words (BoTW)</cell><cell>2010</cell><cell>[61]</cell></row><row><cell></cell><cell>Volume LBP (LBP-TOP)</cell><cell>2007</cell><cell>[82]</cell></row><row><cell></cell><cell>Optical flow</cell><cell>2005</cell><cell>[32]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Representative classifiers</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell>Classifier</cell><cell cols="2">Year Reference</cell></row><row><cell></cell><cell>Deep Networks</cell><cell>2013</cell><cell>[42]</cell></row><row><cell>Static</cell><cell>Support Vector Machine (SVM) AdaBoost</cell><cell>2007 2005</cell><cell>[45] [4]</cell></row><row><cell></cell><cell>Neural Network (NN)</cell><cell>2005</cell><cell>[38]</cell></row><row><cell></cell><cell>Conditional Random Field (CRF)</cell><cell>2009</cell><cell>[9]</cell></row><row><cell>Temporal</cell><cell cols="2">Gaussian process Dynamic Bayesian Network (DBN) 2007 2009</cell><cell>[13] [65], [70]</cell></row><row><cell></cell><cell>Isomap embedding</cell><cell>2006</cell><cell>[10]</cell></row><row><cell>Hybrid</cell><cell>Cascade of Tasks (CoT)</cell><cell>2013</cell><cell>[21]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>tr , s) is the SVM empirical risk defined on the decision function f , and training set D tr with each instance weighted by s ∈ R ntr . Each entry s i corresponds to a positive weight for a training sample x i . Ω s (X tr , X te ) measures training and test distribution mismatch as a function of s. The lower the value of Ω s , the more similar the training and the test distributions are. λ &gt; 0 is a tradeoff between the risk and the distribution mismatch. The goal of the STM is to jointly optimize the decision function f as well as the selective coefficient s, such that the resulting classifier can alleviate person-specific biases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 Compare</head><label>3</label><figDesc>STM with related transductive transfer learning methods</figDesc><table><row><cell>Methods</cell><cell>Importance re-weight</cell><cell>Weight refine</cell><cell>Convexity</cell><cell>Labeled target data</cell></row><row><cell>SVM-KNN [81]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Detailed content of different datasets</figDesc><table><row><cell>Datasets</cell><cell>#Subjects</cell><cell>#Videos</cell><cell>#Frames/video</cell><cell>Content</cell><cell>AU annotation</cell><cell>Expression annotation</cell></row><row><cell>CK+ [44]</cell><cell>123</cell><cell>593</cell><cell>∼20</cell><cell>Neutral→peak</cell><cell>Per video</cell><cell>Per video</cell></row><row><cell>GEMEP-FERA [67]</cell><cell>7</cell><cell>87</cell><cell>20∼60</cell><cell>Acting</cell><cell>Frame-by-frame</cell><cell>Per video</cell></row><row><cell>RU-FACS [4]</cell><cell>34</cell><cell>34</cell><cell>5000∼8000</cell><cell>Interview</cell><cell>Frame-by-frame</cell><cell>-</cell></row><row><cell>GFT [57]</cell><cell>720</cell><cell>720</cell><cell>∼60,000</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 6</cell></row><row><cell></cell><cell cols="6">Within-subject AU detection with STM and PS classifiers</cell><cell>Selection percentage of STM for different subjects</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell></cell><cell></cell><cell>F1 Score</cell><cell></cell></row><row><cell cols="4">AU PS 1 -SVM PS 2 -SVM STM</cell><cell cols="3">PS 1 -SVM PS 2 -SVM STM</cell></row><row><cell>1</cell><cell>48.0</cell><cell>72.4</cell><cell>79.2</cell><cell>45.0</cell><cell>54.8</cell><cell>61.9</cell></row><row><cell>2</cell><cell>46.5</cell><cell>71.1</cell><cell>80.2</cell><cell>45.9</cell><cell>55.7</cell><cell>64.3</cell></row><row><cell>4</cell><cell>62.6</cell><cell>61.9</cell><cell>66.5</cell><cell>46.6</cell><cell>40.7</cell><cell>60.4</cell></row><row><cell>6</cell><cell>70.3</cell><cell>80.0</cell><cell>86.4</cell><cell>60.2</cell><cell>69.7</cell><cell>78.5</cell></row><row><cell>7</cell><cell>47.5</cell><cell>54.3</cell><cell>72.4</cell><cell>49.4</cell><cell>55.3</cell><cell>58.4</cell></row><row><cell>12</cell><cell>65.7</cell><cell>74.0</cell><cell>72.3</cell><cell>69.5</cell><cell>70.4</cell><cell>72.6</cell></row><row><cell>15</cell><cell>41.4</cell><cell>64.0</cell><cell>70.5</cell><cell>44.5</cell><cell>49.0</cell><cell>56.0</cell></row><row><cell>17</cell><cell>32.6</cell><cell>70.3</cell><cell>61.7</cell><cell>25.0</cell><cell>40.3</cell><cell>36.3</cell></row><row><cell>Av.</cell><cell>51.8</cell><cell>68.5</cell><cell>73.6</cell><cell>48.3</cell><cell>54.5</cell><cell>61.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. In all, PS 1 -SVM shows the lowest AUC and F1. This outcome likely occurred because of the relatively small number of samples for individual subjects. Lack of sufficient training data for individual subjects is a common problem for person-specific classifiers. It is likely that PS 1 -SVM would have performed the best if the amount training data from the same subject is large enough. PS 2 -SVM achieved better AUC and F1 because it saw more training subjects. Overall, STM consistently outperformed both PS classifiers. Selection ability of STM: Recall that PS 2 includes samples of the test subject in both training and test sets. Could STM improve PS 2 performance by selecting proper training samples? To answer this question, we employed PS 2 to investigate STM's ability to select relevant training samples with respect to the test subject. Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Cross-subject AU detection on RU-FACS dataset. "SA (NN|SVM)" indicates SA with NN and SVM, respectively.</figDesc><table><row><cell>AUC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9</head><label>9</label><figDesc>Cross-dataset AU detection: (a) RU-FACS→GEMEP-FERA, and (b) GFT→RU-FACS ("A→B" represents for training on dataset A and test on B).</figDesc><table><row><cell>(a)</cell><cell></cell><cell>AUC</cell><cell></cell><cell></cell><cell></cell><cell cols="2">F1 Score</cell><cell>(b)</cell><cell></cell><cell>AUC</cell><cell>F1 Score</cell></row><row><cell cols="2">AU SVM KMM</cell><cell>T-SVM</cell><cell cols="3">DA-SVM STM SVM KMM</cell><cell>T-SVM</cell><cell>DA-SVM STM</cell><cell cols="2">AU SVM KMM</cell><cell>T-SVM</cell><cell>DA-SVM STM SVM KMM</cell><cell>T-SVM</cell><cell>DA-SVM STM</cell></row><row><cell>1</cell><cell cols="7">44.7 48.8 43.7 56.9 63.2 46.3 46.4 41.8 46.1 50.4</cell><cell>1</cell><cell cols="2">45.8 63.6 70.3 71.2 73.7 23.7 29.8 26.6 31.8 38.6</cell></row><row><cell>2</cell><cell cols="7">52.8 70.5 52.1 52.3 74.0 47.4 54.2 38.6 45.4 54.6</cell><cell>2</cell><cell cols="2">46.4 62.8 68.5 68.2 71.7 21.3 25.4 19.4 32.1 30.2</cell></row><row><cell>4</cell><cell cols="7">52.7 55.4 54.2 52.7 58.6 57.1 57.1 40.2 42.9 57.4</cell><cell>4</cell><cell cols="2">56.9 60.1 59.1 47.2 61.7 18.3 24.5 20.7 28.5</cell></row><row><cell>6</cell><cell cols="7">73.5 55.2 77.1 79.9 83.4 60.7 55.2 52.8 56.3 72.7</cell><cell>6</cell><cell cols="2">65.5 73.9 81.5 74.1 93.3 42.2 46.8 30.4 38.7 61.4</cell></row><row><cell>12</cell><cell cols="7">56.8 60.1 70.9 76.1 78.1 67.7 67.7 63.5 62.6 71.5</cell><cell>12</cell><cell cols="2">65.3 72.1 76.3 80.9 90.3 43.2 47.6 45.8 56.8 62.2</cell></row><row><cell>15</cell><cell cols="7">55.1 52.1 59.3 60.2 58.6 31.5 32.8 29.7 26.4 41.1</cell><cell>14</cell><cell cols="2">57.2 54.8 53.7 70.2 72.2 25.8 23.8 25.9 29.7 36.2</cell></row><row><cell>17</cell><cell cols="7">44.3 41.1 39.1 46.2 52.7 27.3 27.1 24.3 24.6 31.4</cell><cell>15</cell><cell cols="2">56.9 61.8 64.2 65.5 80.4 23.7 30.3 28.2 29.9 37.8</cell></row><row><cell>Av.</cell><cell cols="7">54.3 54.8 56.6 60.6 66.9 48.3 48.6 41.6 43.5 54.2</cell><cell>17</cell><cell cols="2">52.4 54.5 64.8 72.6 72.6 30.8 31.5 32.3 38.9 39.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Av.</cell><cell cols="2">55.8 62.9 67.3 68.7 77.0 28.6 32.5 28.7 34.7 41.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Expression detection with AUC on (a) CK+ and (b) GEMEP-FERA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Expression</cell><cell cols="6">SVM KMM T-SVM DA-SVM STM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Anger</cell><cell>95.1</cell><cell>85.3</cell><cell>76.1</cell><cell></cell><cell>-</cell><cell>96.4</cell><cell></cell><cell></cell></row><row><cell>(a) CK+</cell><cell>Contempt Disgust Fear Happy Sadness</cell><cell>96.9 94.5 96.6 99.4 94.5</cell><cell>94.5 81.6 92.7 93.9 76.0</cell><cell>88.8 84.2 84.9 86.7 78.7</cell><cell></cell><cell>-----</cell><cell>96.9 96.0 95.5 98.9 93.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Surprise</cell><cell>97.3</cell><cell>64.5</cell><cell>81.8</cell><cell></cell><cell>-</cell><cell>97.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Av.</cell><cell>96.3</cell><cell>84.1</cell><cell>83.0</cell><cell></cell><cell>-</cell><cell>96.4</cell><cell></cell><cell></cell></row><row><cell>GEMEP-FERA</cell><cell>Expression Anger Fear Joy Relief Sadness</cell><cell cols="6">SVM KMM T-SVM DA-SVM STM 31.1 66.5 70.4 78.8 78.6 31.9 81.4 64.5 83.9 85.5 90.2 33.5 78.9 71.1 95.0 20.4 74.8 76.8 87.9 88.4 73.4 80.2 77.1 74.7 84.8</cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell>Av.</cell><cell>49.4</cell><cell>67.3</cell><cell>73.5</cell><cell cols="2">79.3</cell><cell>86.5</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 6, NO.1, NOVEMBER 2013  </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of Health (NIH) under Award Number R01MH096951, the National Science Foundation (NSF) under the grant RI-1116583, and Army Research Laboratory Collaborative Technology Alliance Program under cooperative agreement W911NF-10-2-0016. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bi-convexity and bi-martingales</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="180" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic recognition of facial actions in spontaneous expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="35" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A dasvm classification technique and a circular validation strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="770" to="787" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Personalized facial expression recognition in indoor environments</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning partially-observed hidden conditional random fields for facial expression recognition</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manifold based analysis of facial expression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="605" to="614" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training a support vector machine in the primal</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1155" to="1178" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Source domain adaptation and its application to early detection of fatigue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Switching gaussian process dynamic models for simultaneous composite motion tracking and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning person-specific models for facial expression and action unit recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aragones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1964" to="1970" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person-independent facial expression detection using constrained local models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In the pursuit of effective affective computing: The relationship between features and registration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1006" to="1016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale transductive svms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1687" to="1712" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analysis of Humans: Looking at People</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">377</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial action unit event detection by cascade of tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources: A domain-dependent regularization approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="518" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias in maximum entropy density estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dudık</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A global optimization algorithm (gop) for certain classes of nonconvex nlpsxi. theory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Floudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; chemical engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1397" to="1417" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A common framework for real-time emotion recognition and facial action unit detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spontaneous facial expression in unscripted social interactions can be measured automatically</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sayette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Obtaining person-specific images in a public venue</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>uS Patent 7,561,723</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Biconvex sets and optimization with biconvex functions: a survey and extensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klamroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods of Operations Research</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="407" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Covariate shift by kernel mean matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
	<note>Dataset shift in machine learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Affect recognition from face and body: early fusion vs. late fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3437" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic facial expression recognition using longitudinal facial expression atlases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d videos in real-time</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal affect recognition in learning environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The computer expression recognition toolbox (CERT)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AU-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Investigating spontaneous facial action recognition through aam representations of the face</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
	<note>Face recognition</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A model of the perception of facial expressions of emotion by humans: Research overview and perspectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1589" to="1608" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hog-based decision tree for facial expression classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gañán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Machine analysis of facial expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="377" to="416" />
		</imprint>
	</monogr>
	<note>Face recognition</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transfer learning to account for idiosyncrasy in face and body expressions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De C Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Watson</surname></persName>
		</author>
		<editor>AFGR</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kernel conditional ordinal random fields for temporal segmentation of facial action units</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Context-sensitive dynamic ordinal regression for intensity estimation of facial action units</title>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="958" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Alcohol and group formation a multimodal investigation of the effects of alcohol on emotion and social bonding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sayette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Dimoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Fairbairn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Moreland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="869" to="878" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Facial action recognition combining heterogeneous features via multikernel learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="993" to="1005" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nonparametric discriminant HMM and application to facial expression recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploring bag of words architectures in the facial expression domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Action unit detection with segment-based svms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Facial action unit recognition by exploiting their dynamic and semantic relationships</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1699" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Metaanalysis of the first facial expression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fully automatic recognition of the temporal phases of facial actions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Benchmarking least squares support vector machine classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dedene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Capturing complex spatio-temporal relations among facial muscles for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Minimization of a non-separable objective function subject to disjoint constraints</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hurter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="657" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Social Emotions in Nature and Artifact</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Facial expression recognition using gabor motion energy filters</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploring facial expressions with compositional features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Personalized modeling of facial action unit intensity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for personalized facial emotion recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Confidence preserving machine for facial action unit detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Svm-knn: Discriminative nearest neighbor classification for visual category recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of facial events</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Dynamic cascades with bidirectional bootstrapping for action unit detection in spontaneous facial behavior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">He has over 150 publications in referred journals and conferences and is Associate Editor at IEEE TPAMI. He has organized and co-organized several workshops and has given tutorials at international conferences on component analysis. Jeffrey F. Cohn is Professor of Psychology and Psychiatry at the University of Pittsburgh and Adjunct Professor of Computer Science at the Robotics Institute at Carnegie Mellon University. He leads interdisciplinary and inter-institutional efforts to develop advanced methods of automatic analysis and synthesis of facial expression and prosody; and applies those tools to research in human emotion, social development, non-verbal communication, psychopathology, and biomedicine</title>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu ; Tainan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiwan</forename><surname>Currently</surname></persName>
		</author>
		<ptr target="http://humansensing.cs.cmu.edu" />
	</analytic>
	<monogr>
		<title level="m">He has served as Co-Chair of the 2008 IEEE International Conference on Automatic Face and Gesture Recognition (FG2008), the 2009 International Conference on Affective Computing and Intelligent Interaction (ACII2009), the Steering Committee for IEEE International Conference on Automatic Face and Gesture Recognition, and the 2014 International Conference on Multimodal Interfaces</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>De</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994. 1996, and 2002. 2014</date>
		</imprint>
		<respStmt>
			<orgName>National Cheng Kung University ; Robotics Institute, Carnegie Mellon University ; D degrees in Electronic Engineering from La Salle School of Engineering at Ramon Llull University ; Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>IEEE Transactions in Affective Computing (TAC)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
