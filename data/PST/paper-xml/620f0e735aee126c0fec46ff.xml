<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer for Graphs: An Overview from Architecture Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-17">17 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
							<email>erxue.min@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">National Centre for Text Mining</orgName>
								<orgName type="institution" key="instit2">The University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Runfa</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kangfei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">National Centre for Text Mining</orgName>
								<orgName type="institution" key="instit2">The University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer for Graphs: An Overview from Architecture Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-17">17 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.08455v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Transformer model, which has achieved great success in many artificial intelligence fields, has demonstrated its great potential in modeling graph-structured data. Till now, a great variety of Transformers has been proposed to adapt to the graph-structured data. However, a comprehensive literature review and systematical evaluation of these Transformer variants for graphs are still unavailable. It's imperative to sort out the existing Transformer models for graphs and systematically investigate their effectiveness on various graph tasks. In this survey, we provide a comprehensive review of various Graph Transformer models from the architectural design perspective. We first disassemble the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and 3) Improved Attention Matrix from Graphs. Furthermore, we implement the representative components in three groups and conduct a comprehensive comparison on various kinds of famous graph data benchmarks to investigate the real performance gain of each component. Our experiments confirm the benefits of current graph-specific modules on Transformer and reveal their advantages on different kinds of graph tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph is a kind of data structure that structurally depicts a set of objects (nodes) with their relationships (edges). As a unique non-Euclidean data structure, graph analysis focuses on tasks such as node classification <ref type="bibr" target="#b21">[Yang et al., 2016]</ref>, link prediction <ref type="bibr" target="#b23">[Zhang and Chen, 2018]</ref>, and clustering <ref type="bibr" target="#b0">[Aggarwal, 2010]</ref>. Recent research on analyzing graphs using deep learning has attracted more and more attention due to the rich expressive power of deep learning models. Graph Neural Networks (GNNs) <ref type="bibr" target="#b7">[Kipf and Welling, 2016]</ref>, as a kind of deep learning-based method, are recently becoming a widely-used graph analysis tools due to their convincing performance. Most of the current GNNs are based on the Message Passing paradigm <ref type="bibr" target="#b4">[Gilmer et al., 2017]</ref>, the expressive power of which is bounded by the Weisfeiler-Lehamn isomorphism hierarchy <ref type="bibr" target="#b8">[Maron et al., 2019;</ref><ref type="bibr">Xu et al., 2018a]</ref>. Worse still, as pointed out by <ref type="bibr" target="#b7">[Kreuzer et al., 2021]</ref>, GNNs suffer from the over-smoothing problem due to repeated local aggregation, and the over-squashing problem due to the exponential computation cost with the increase of model depth.</p><p>Several studies <ref type="bibr" target="#b24">[Zhao and Akoglu, 2019;</ref><ref type="bibr" target="#b13">Rong et al., 2019;</ref><ref type="bibr">Xu et al., 2018b]</ref> try to address such problems. Nevertheless, none of them seems to be able to eliminate these problems from the Message Passing paradigm.</p><p>On the other hand, Transformers and its variants, as a powerful class of models, are playing an important role in various areas including Natural Language Processing (NLP) <ref type="bibr" target="#b19">[Vaswani et al., 2017]</ref>, Computer Vision (CV) <ref type="bibr" target="#b3">[Forsyth and Ponce, 2011]</ref>, Time-series Analysis <ref type="bibr" target="#b5">[Hamilton, 2020]</ref>, Audio Processing <ref type="bibr" target="#b12">[Purwins et al., 2019]</ref>, etc. Moreover, recent years have witnessed many successful Transformer variants in modeling graphs. These models have achieved competitive or even superior performance against GNNs in many applications, such as Quantum Property Prediction <ref type="bibr" target="#b6">[Hu et al., 2021]</ref>, Catalysts Discovery <ref type="bibr" target="#b2">[Chanussot* et al., 2021]</ref> and Recommendation Systems <ref type="bibr" target="#b10">[Min et al., 2022]</ref>. The literature reviews and systematic evaluations for these Transformer variants are, however, lacking.</p><p>This survey gives an overview of the current research progress on incorporating Transformers in graph-structured data. Concretely, we provide a comprehensive review of over 20 Graph Transformer models from the architectural design perspective. We first dismantle the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNNs as Auxiliary Modules, i.e., directly inject GNNs into Transformer architecture. Specifically, according to the relative position between GNN layers and Transformer layers, existing GNN-Transformer architectures can be categorized into three types: (a) build Transformer blocks on top of GNN blocks, (b) stack GNN blocks and Transformer blocks in each layer, (c) parallel GNN blocks and Transformer blocks in each layer. 2) Improved Positional Embedding from Graphs, i.e., compress the graph structure into positional embedding vectors and add them to the input before it is fed to the vanilla Transformer model. This graph positional embedding can be derived from the structural information of graphs, such as degree and centrality. 3) Improved Attention Matrices from Graphs, i.e., inject graph priors into the attention computation via graph bias terms, or restrict a node only attending to local neighbours in the graph, which can be computationally formulated as an attention masking mechanism.</p><p>Additionally, in order to investigate the effectiveness of existing models in various kinds of graph tasks, we implement the representative components in three groups and conduct comprehensive ablation studies on six popular graph-based benchmarks to uniformly test the real performance gain of each component. Our experiments indicate that: 1). Current models incorporating the graph information can improve the performance of Transformer on both graph-level and nodelevel tasks. 2). Utilizing GNN as auxiliary modules and improving attention matrix from graphs generally contributes more performance gains than encoding graphs into positional embeddings. 3). The performance gain on graph-level tasks is more significant than that on node-level tasks. 4) Different kinds of graph tasks enjoy different group of models.</p><p>The rest of this survey is organized as follows. We first review the general vanilla Transformer Architecture in Section 2. Section 3 summarizes existing works about the Transformer variant on Graphs and systematically categorizes these methods into three groups. In Section 4, comprehensive ablation studies are conducted to verify the effectiveness and compatibility of these proposed models. In Section 5, we conclude this survey and discuss several future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transformer Architecture</head><p>Transformer <ref type="bibr" target="#b19">[Vaswani et al., 2017]</ref> architecture was first applied to machine translation. In the paper, Transformer is introduced as a novel encoder-decoder architecture built with multiple blocks of self-attention. Let X ? R n?d to be the input of each Transformer layer, where n is number of tokens, d is the dimension of each token, then one block layer can be a function f ? : R n?d ? R n?d with f ? (X) =: Z defined by:</p><formula xml:id="formula_0">A = 1 ? d XQ(XK) ,<label>(1)</label></formula><formula xml:id="formula_1">X = SoftMax(A)(XV),<label>(2)</label></formula><formula xml:id="formula_2">M = LayerNorm 1 ( XO + X),<label>(3)</label></formula><formula xml:id="formula_3">F = ?(MW 1 + b 1 )W 2 + b 2 , (4) Z = LayerNorm 2 (M + F),<label>(5)</label></formula><p>where Equation 1, Equation 2, and Equation 3 are the attention computation; while Equation 4 and Equation <ref type="formula" target="#formula_3">5</ref>are the position-wise feed-forward network (FFN) layers.</p><p>Here, Softmax(?) refers to the row-wise softmax function, LayerNorm(?) refers to layer normalization function <ref type="bibr" target="#b0">[Ba et al., 2016]</ref>, and ? refers to the activation function.</p><formula xml:id="formula_4">Q, K, V, O ? R d?d , W 1 ? R d?d f , b 1 ? R d f , W 2 ? R d f ?d , b 2 ? R d are</formula><p>trainable parameters in the layer. Furthermore, it is common to consider multiple attention heads to extend the self-attention to Multi-head Self-attention  <ref type="bibr" target="#b17">Shiv and Quirk, 2019]</ref> [ <ref type="bibr" target="#b21">Wang et al., 2019</ref>] U2GNN <ref type="bibr" target="#b11">[Nguyen et al., 2019]</ref> HeGT <ref type="bibr" target="#b21">[Yao et al., 2020</ref>] Graformer <ref type="bibr" target="#b15">[Schmitt et al., 2020]</ref> PLAN <ref type="bibr" target="#b6">[Khoo et al., 2020]</ref> UniMP <ref type="bibr" target="#b16">[Shi et al., 2020]</ref> GTOS <ref type="bibr" target="#b22">[Cai and</ref><ref type="bibr">Lam, 2020] Graph Trans [Dwivedi and</ref><ref type="bibr">Bresson, 2020]</ref> Grover <ref type="bibr">[Rong et al.</ref>, 2020] Graph-BERT <ref type="bibr" target="#b23">[Zhang et al., 2020</ref> </p><formula xml:id="formula_5">(MHSA). Specifically, Q, K, V are decomposed into H heads with Q (h) , K (h) , V (h) ? R d?d h with d = H h=1 d h ,</formula><p>and the matrices X (h) ? R n?d h from attention heads are concatenated to obtain X. In this case, Equation 1 and Equation 2 respectively become:</p><formula xml:id="formula_6">A (h) = 1 ? d XQ (h) (XK (h) ) ,<label>(6)</label></formula><formula xml:id="formula_7">X = H h=1 (SoftMax(A (h) )XV (h) ).<label>(7)</label></formula><p>The multi-head mechanism enables the model to implicitly learn representation from different aspects. Apart from the attention mechanism, the paper uses sine and cosine functions with different frequencies as positional embedding to distinguish the position of each token in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transformer Architecture for Graphs</head><p>The self-attention mechanism in the standard Transformer actually considers the input tokens as a fully-connected graph, which is agnostic to the intrinsic graph structure among the data. Existing methods that enable Transformer to be aware of topological structures are generally categorized into three groups: 1) GNNs as auxiliary modules in Transformer (GA), 2) Improved positional embedding from graphs (PE), 3) Improved attention matrices from graphs (AT). We summarize relevant literature in terms of these three dimensions in Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GNNs as Auxiliary Modules in Transformer</head><p>The most direct solution of involving structural knowledge to benefit from global relation modeling of self-attention is The first architecture is most-frequently adopted among the three options. For example, GraphTrans <ref type="bibr" target="#b6">[Jain et al., 2021]</ref> adds a Transformer subnetwork on top of a standard GNN layer. The GNN layer performs as a specialized architecture to learn local representations of the structure of a node's immediate neighbourhood, while the Transformer subnetwork computes all pairwise node interactions in a position-agnostic fashion, empowering the model global reasoning capability. GraphTrans is evaluated on graph classification task from biology, computer programming and chemistry, and achieves consistent improvement over benchmarks.</p><p>Grover <ref type="bibr" target="#b14">[Rong et al., 2020]</ref> consists of two GTransformer modules to represent node-level features and edge-level features respectively. In each GTransformer, the inputs are first fed into a tailored GNNs named dyMPN to extract vectors as queries, keys and values from nodes of the graph, followed by standard multi-head attention blocks. This bi-level information extraction framework enables the model to capture the structural information in molecular data and make it possible to extract global relations between nodes, enhancing the representational power of Grover.</p><p>GraphiT <ref type="bibr" target="#b9">[Mialon et al., 2021]</ref> also falls in the first architecture, which adopts one Graph Convolutional Kernel Network (GCKN) <ref type="bibr" target="#b3">[Chen et al., 2020]</ref> layer to produce a structureaware representation from original features, and concatenate them as the input of Transformer architecture. Here, GCKNs is a multi-layer model that produces a sequence of graph feature maps similar to a GNN. Different from GNNs, each layer of GCKNs enumerates local sub-structures at each node, encodes them using a kernel embedding, and aggregates the sub-structure representations as outputs. These representations in a feature map carry more structural information than traditional GNNs based on neighborhood aggregation.</p><p>Mesh Graphormer <ref type="bibr">[Lin et al., 2021]</ref> follows the second architecture by stacking a Graph Residual Block (GRB) on a multi-head self-attention layer as a Transformer block to model both local and global interactions among 3D mesh vertices and body joints. Specifically, given the contextualized features M generated by multi-head self-attentions (MHSA) in Equation <ref type="formula" target="#formula_2">3</ref>, Mesh Graphormer improves the local interactions using a graph convolution in each Transformer block as:</p><formula xml:id="formula_8">M = GraphConv(A G , M; W G ) = ?(A G XW G ). (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where A G ? R n?n denotes the adjacency matrix of a graph and W G denotes the trainable parameters. ?(?) implies the non-linear activation function. Mesh Graphormer also implements another two variants: building GRB before MHSA and building those two blocks in parallel, but they perform worse than the proposed one. Graph-BERT <ref type="bibr" target="#b23">[Zhang et al., 2020]</ref> adopts the third architecture by utilizing a graph residual term in each attention layer as follows:</p><formula xml:id="formula_10">M = M + G-Res(X, X r , A G ),<label>(9)</label></formula><p>where the notation G-Res (X, X r , A G ) represents the graph residual term introduced in <ref type="bibr" target="#b23">[Zhang and Meng, 2019</ref>] and X r is the raw features of all nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improved Positional Embeddings from Graphs</head><p>Although combining graph neural networks and Transformer has shown effectiveness in modeling graph-structured data, the best architecture to incorporate them remains an issue and requires heavy hype-parameter searching. Therefore, it is meaningful to explore a graph-encoding strategy without adjustment of the Transformer architecture. Similar to the positional encoding in Transformer for sequential data such as sentences, it is also possible to compress the graph structure into positional embedding (PE) vectors and add them to the input before it is fed to the actual Transformer model:</p><formula xml:id="formula_11">X = X + f map (P),<label>(10)</label></formula><p>where X ? R n?d is the matrix of input embeddings, P ? R n?dp represents the graph embedding vectors, and f map : R dp ? R d is a transformation network to align the dimension of both vectors. The graph positional embedding P is normally generated from the adjacent matrix A G ? R n?n .</p><p>[Dwivedi and Bresson, 2020] adopt Laplacian eigenvectors as P in Graph Transformer. For each graph in the dataset, they pre-compute the Laplacian eigenvectors , which are defined by the factorization of the graph Laplacian matrix:</p><formula xml:id="formula_12">U T ?U = I -D -1/2 A G D -1/2 , (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where D is the degree matrix, and ?, U correspond to the eigenvalues and eigenvectors respectively. They use eigenvectors of the k smallest non-trivial eigenvalues as the positional embedding, with P ? R n?k in this case. Since these eigenvectors have multiplicity occurring due to the arbitrary sign of eigenvectors, they randomly flip the sign of the eigenvectors during training.</p><p>[Hussain et al., 2021] employs pre-computed SVD vectors of the adjacent matrix as the positional embeddings P. They use the largest r singular values and corresponding left and right singular vectors to represent the positional encoding.</p><formula xml:id="formula_14">A G SVD ? U?V T = (U ? ?) ? (V ? ?) T = ? VT ,<label>(12)</label></formula><formula xml:id="formula_15">P = ? V,<label>(13)</label></formula><p>where U, V ? R n?r contain the r left and right singular vectors corresponding to the top r singular values in the diagonal matrix ? ? R r?r , denotes concatenation operator along columns. They also randomly flip the signs during training as a form of data augmentation to avoid over-fitting, since similar to Laplacian eigenvectors, the signs of corresponding pairs of left and right singular vectors can be arbitrarily flipped.</p><p>Different from Eigen PE and SVD PE, which attempt to compress the adjacent matrix into dense positional embeddings, there exist some heuristic methods that encode specific structural information from the extraction of the adjacent matrix. For example, Graphormer <ref type="bibr" target="#b22">[Ying et al., 2021]</ref> uses the degree centrality as an additional signal to the neural network. To be specific, Graphformer assigns each node two real-valued embedding vectors according to its indegree and outdegree. For the i-th node, the degree-aware representation is denoted as:</p><formula xml:id="formula_16">x = x + z - deg -(vi) + z + deg + (vi) ,<label>(14)</label></formula><p>where z -, z + ? R d are learnable embedding vectors specified by the indegree deg -(v i ) and outdegree deg + (v i ) respectively. For undirected graphs, deg -(v i ) and deg + (v i ) could be unified to be deg(v i ). By using the centrality encoding in the input, the softmax attention will catch the node importance signal in queries and keys of the Transformer. Graph-BERT <ref type="bibr" target="#b23">[Zhang et al., 2020]</ref> introduces three types of PE to embed the node position information, i.e., an absolute WL-PE which represents different codes labeled by the Weisfeiler-Lehman algorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs. <ref type="bibr" target="#b1">[Cai and Lam, 2020]</ref> applies graph transformer to tree-structured abstract meaning representation (AMR) graph. It adopts a distance embedding for each node by encoding the minimum distance from the root node as a flag of the importance of the corresponding concept in the wholesentence semantics. <ref type="bibr" target="#b7">[Kreuzer et al., 2021]</ref> proposes a learned positional encoding that can utilize the full Laplacian spectrum to learn the position of each node in a given graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improved Attention Matrices from Graphs</head><p>Although node positional embedding is a convenient practice to inject graph priors into Transformer architectures, the progress of compressing graph structure into fixed-sized vectors suffers from information loss, which might limit their effectiveness. For this sake, another group of works attempts to improve the attention matrix computation based on graph information:</p><p>A</p><formula xml:id="formula_17">=f G att (X, A G , E; Q, K, W 1 ), M =f M (X, A, A G , E; V, W 2 ), (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>where X is the input features, A G is the adjacent matrix of the graph, E ? R n?n?de is the edge features if available, Q, K, V are the attention parameters, W 1 , W 2 are extra graph encoding parameters.</p><p>One line of models adapts self-attention mechanism to GNN-like architectures by restricting a node only attending to local node neighbours in the graph, which can be computationally formulated as an attention masking mechanism:</p><formula xml:id="formula_19">A = ( 1 ? d XQ(XK) ) A G ,<label>(16)</label></formula><p>where A G ? R n?n is the adjacent matrix of the graph. In <ref type="bibr">[Dwivedi and Bresson, 2020]</ref>, A G ij = 1 if there is an edge between i-th node and j-th node. Given the simple and generic nature of this architecture, they obtain competitive performance against standard GNNs on graph datasets. In order to encode edge features, they also extend Equation <ref type="formula" target="#formula_19">16</ref>as:</p><formula xml:id="formula_20">A = ( 1 ? d XQ(XK) ) A G E W E ,<label>(17)</label></formula><p>where W E ? R de?d is the parameter matrix, E is the edge embedding matrix from the previous layer which is updated based on A, we do not elaborate these details for simplicity.</p><p>One possible extension of this practice is masking the attention matrices of different heads with different graph priors. In the original multi-head self-attention blocks, different attention heads implicitly attend to information from different representation subspaces of different nodes. While in this case, using the graph-masking mechanism to enforce the heads explicitly attend to different subspaces with graph priors further improves the model representative capability for graph data. For example, <ref type="bibr" target="#b21">[Yao et al., 2020]</ref> computes the attention based on the extended Levi graph. Since Levi graph is a heterogeneous graph that contains different types of edges. They first group all edge types into a single one to get a homogeneous subgraph referred to as connected subgraph. The connected subgraph is actually an undirected graph that contains the complete connected information in the original graph. Then they split the input graph into multiple subgraphs according to the edge types. Besides learning the directly connected relations, they introduce a fully-connected subgraph to learn the implicit relationships between indirectly connected nodes. Multiple adjacent matrices are assigned to different attention heads to learn a better representation for AMR task. <ref type="bibr" target="#b10">[Min et al., 2022</ref>] adopts a similar practice, which carefully designs four types of interaction graphs for modeling neighbourhood relations in CTR prediction task: induced subgraph, similarity graph, cross-neighbourhood graph, and complete graph. And they use the masking mechanism to encode these graph priors to improve neighbourhood representation.</p><p>GraphiT <ref type="bibr" target="#b9">[Mialon et al., 2021]</ref> extends the adjacent matrix to a kernel matrix, which is more flexible to encode various graph kernels. Besides, they use the same matrix for keys and queries following the recommendation of <ref type="bibr" target="#b19">[Tsai et al., 2019]</ref> to reduce parameters without hurting the performance in practice, and adopt a degree matrix to reduce the overwhelming influence of highly connected graph components. The update equation can be formulated as:</p><formula xml:id="formula_21">A = ( 1 ? d XQ(XQ) ) K r , X = SoftMax(A)(XV), M = LayerNorm(D -1 2 X + X),<label>(18)</label></formula><p>where D ? R n?n is the diagonal matrix of node degrees, K r ? R n?n is the kernel matrix on the graph, which is used as diffusion kernel and p-step random walk kernel.</p><p>Another line of models attempts to add soft graph bias to attention scores. <ref type="bibr">Graphormer [Ying et al., 2021]</ref> proposes a novel Spatial Encoding mechanism. Concretely, they consider a distance function ?(v i , v j ), which measures the spatial relation between nodes v i and v j in the graph. They select ?(v i , v j ) as the shortest path distance (SPD) between v i and v j . If they are not connected, the output of ? is set as a special value, i.e., -1. They assign each feasible value of ? a learnable scale parameter as a graph bias term. So the update rule is:</p><formula xml:id="formula_22">A = ( 1 ? d XQ(XK) ) + B s . (<label>19</label></formula><formula xml:id="formula_23">)</formula><p>B s is the bias matrix, where</p><formula xml:id="formula_24">B s ij = b ?(vi,v k )</formula><p>is the learnable scalar indexed by ?(v i , v k ), and shared across all layers. In order to handle graph structure with edge features, they also design an edge feature bias term. Specifically, for each ordered node pair (v i , v j ), they search (one of) the shortest path SP ij = (e 1 , e 2 , . . . , e N ) from v i to v j , and then compute an average of dot-products of the edge features and a learnable embedding along the path. Combined with the above spatial bias, the unnormalized attention can be modified as:</p><formula xml:id="formula_25">A = ( 1 ? d XQ(XK) ) + B s + B c (20)</formula><p>where B c is the edge feature bias matrix.</p><formula xml:id="formula_26">B c ij = 1 N N n=1 x en (w E n )</formula><p>, where x en is the feature of the n-th edge e n in SP ij , w E n ? R de is the n-th weight embedding, and d e is the dimensionality of edge feature.</p><p>Gophormer <ref type="bibr" target="#b24">[Zhao et al., 2021]</ref> proposes proximityenhanced multi-head attention (PE-MHA) to encode multihop graph information. Specifically, for a node pair v i , v j , M views of structural information is encoded as a proximity encoding vector, denoted as ? ij ? R M , to enhance the attention mechanism. The proximity-enhanced attention score A ij is defined as:</p><formula xml:id="formula_27">A ij = ( 1 ? d x i Q(x j K) ) + ? ij b ,<label>(21)</label></formula><p>where b ? R M is the learnable parameters that compute the bias of structural information. The proximity encoding is calculated by M structural encoding functions defined as:</p><formula xml:id="formula_28">? ij = Concat(? m (v i , v j )|m ? 0, 1, . . . , M -1),<label>(22)</label></formula><p>where each structural encoding function ? m (?) encodes a view of structural information. <ref type="bibr">PLAN [Khoo et al., 2020</ref>] also proposes a structure aware self-attention to model the tree structure of rumour propagation in social media. The modified attention calculation can be defined as:</p><formula xml:id="formula_29">A ij = 1 ? d (x i Q(x j K) ) + a K ij<label>(23)</label></formula><formula xml:id="formula_30">M i = n j=1 SoftMax(A ij )(x j V + a V ij )<label>(24)</label></formula><p>Both a V ij and a K ij are learnable parameter vectors that represent one of the five possible structural relationships between the pair of the tweets (i.e. parent, child, before, after and self). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluations</head><p>We conduct an extensive evaluation to study the effectiveness of different methods. On the basis of standard Transformer, methods in the three groups, auxiliary GNN modules (GA), positional embeddings (PE), and improved attention (AT) are compared. Since most methods are composed of more than one graph-specific module and are trained with various tricks, it is difficult to evaluate the effectiveness of each module in a fair and independent way. In our experiments, we extract the representative modules of existing models and evaluate their performance individually. For GA methods, we compare the three architectures described in Section 3.1. In both alternately and parallel settings, the GNNs and Transformer layers are combined before the FFN layers. PE methods include degree embedding <ref type="bibr" target="#b22">[Ying et al., 2021]</ref>, Laplacian Eigenvectors <ref type="bibr">[Dwivedi and Bresson, 2020]</ref> and SVD vectors <ref type="bibr">[Hussain et al., 2021]</ref>. AT methods contain spatial bias (SPB) <ref type="bibr" target="#b22">[Ying et al., 2021]</ref>, proximity-enhanced multi-head attention (PMA) <ref type="bibr" target="#b24">[Zhao et al., 2021]</ref>, attention masked with 1-hop adjacent matrix (Mask-1) <ref type="bibr">[Dwivedi and Bresson, 2020]</ref>. We also mask attention with different hops of the adjacent matrix, denoted as (Mask-n), inspired by the multi-head masking mechanisms in <ref type="bibr" target="#b21">[Yao et al., 2020;</ref><ref type="bibr" target="#b10">Min et al., 2022]</ref>. We evaluate the methods on both graphlevel tasks on small graphs and node-level tasks on a single large graph. The details of the tasks and datasets are listed in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings and Implementation Details</head><p>To ensure a consistent and fair evaluation, we fix the scale of the Transformer architecture in three levels: small, middle, and large, whose configurations are listed in Table <ref type="table" target="#tab_3">3</ref>. The remaining hyper-parameters are fixed to their empirical value as shown in Table <ref type="table" target="#tab_5">5</ref>. For node-level tasks in large-scale graphs, we adopt the shadow k-hop sampling <ref type="bibr" target="#b22">[Zeng et al., 2020]</ref> to generate a subgraph for a target node, to which the graphaware modules are applied. We calculate the required inputs of all modules based on the subgraph instead of the original graph because most of them are not computationally scalable to large graphs. We set the maximum sampling hop to be 2 and the maximum neighbourhood per node to be 10. For PE methods, we select the embedding size from {3, 4, 5}. For GA methods, we select the GNN type from GCN <ref type="bibr" target="#b7">[Kipf and Welling, 2016]</ref>, GAT <ref type="bibr" target="#b20">[Velivckovic et al., 2017]</ref> and GIN [Xu  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Table <ref type="table" target="#tab_4">4</ref> reports the performance of ten graph-specific modules from three categories on six datasets of graph-level and node-level tasks, respectively. We summarize the main observations as follows:</p><p>-Not surprisingly, in most cases, the evaluated graphspecific modules of Transformer lead to to better performance. For example, on molpcba, we observe at most a 56% performance improvement compared with the vanilla Transformer. This observation confirms the effectiveness of graph-specific modules on the various kinds of graph tasks.</p><p>-The improvement on graph-level tasks is more significant than that on node-level tasks. This may be due to the graph sampling process when dealing with a single large graph.</p><p>The sampled induced subgraphs fail to keep the original graph intact, incurring variance and information loss in the learning process.</p><p>-GA and AT methods bring more benefits than PE methods.</p><p>In conclusion, PE's weaknesses are twofold. First, as introduced in Section 3.2, PE does not contain intact graph information. Second, since PE is only fed into the input layer of the network, the graph-structural information would decay layer by layer across the model, leading to a degeneration of the performance.</p><p>-It seems that different kinds of graph tasks enjoy different group of models. GA methods achieve the best performance in more than half of the cases (5 out of 9 cases) of node-level tasks. More significantly, AT methods achieve the best performance in almost all the cases (8 out of 9 cases) of graph-level tasks. We conjecture that GA methods are able to better encode the local information of the sampled induced subgraphs in node-level tasks, while AT methods are suitable for modeling the global information of the single graphs in graph-level tasks.</p><p>In summary, our experiments confirm the value of the existing graph-specific module on Transformer and reveal their advantages for a variety of graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Directions</head><p>In this survey, we present a comprehensive review of the current progress of applying Transformer model on graphs.</p><p>Based on the injection part of graph data and graph model in Transformer, we classify the existing graph-specific modules into three typical groups: GNNs as auxiliary modules, improved positional embeddings from graphs, and improved attention matrices from graphs. To investigate the real performance gains under a fair setup, we implement the representative modules from three groups and compare them on six benchmarks with different tasks. We hope our systematic review and comparison will foster understanding and stimulate new ideas in this area.</p><p>In spite of the current successes, we believe several directions would be promising to investigate further and to start from in the future, including:</p><p>-New paradigm of incorporating the graph and the Transformer. Most studies treat the graphs as strong prior to modifying the Transformer model. There is a great interest to develop the new paradigm that not just takes graphs as a prior, but also better reflects the properties of graphs.</p><p>-Extending to other kinds of graphs. Existing Graph Transformer models mostly focus on homogeneous graphs, which consider the node type and edge type as the same. It is also important to explore their potential on other forms of graphs, such as heterogeneous graphs and hypergraphs.</p><p>-Extending to large-scale graphs. Most existing methods are designed for small graphs, which might be computationally infeasible for large graphs. As illustrated in our experiments, directly applying them to the sampled subgraphs would impair performance. Therefore, designing salable Graph-Transformer architecture is essential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three types of GNN-as-Auxiliary-Modules architecture.to combine graph neural networks with Transformer architecture. Generally, according to the relative postion between GNN layers and Transformer layers, existing Transformer architectures with GNNs are categorized into three types as illustrated in Figure1:(1) building Transformer blocks on top of GNN blocks, (2) alternately stacking GNN blocks and Transformer blocks, (3) parallelizing GNN blocks and Transformer blocks.The first architecture is most-frequently adopted among the three options. For example, GraphTrans<ref type="bibr" target="#b6">[Jain et al., 2021]</ref> adds a Transformer subnetwork on top of a standard GNN layer. The GNN layer performs as a specialized architecture to learn local representations of the structure of a node's immediate neighbourhood, while the Transformer subnetwork computes all pairwise node interactions in a position-agnostic fashion, empowering the model global reasoning capability. GraphTrans is evaluated on graph classification task from biology, computer programming and chemistry, and achieves consistent improvement over benchmarks.Grover<ref type="bibr" target="#b14">[Rong et al., 2020]</ref> consists of two GTransformer modules to represent node-level features and edge-level features respectively. In each GTransformer, the inputs are first fed into a tailored GNNs named dyMPN to extract vectors as queries, keys and values from nodes of the graph, followed by standard multi-head attention blocks. This bi-level information extraction framework enables the model to capture the structural information in molecular data and make it possible to extract global relations between nodes, enhancing the representational power of Grover.GraphiT<ref type="bibr" target="#b9">[Mialon et al., 2021]</ref> also falls in the first architecture, which adopts one Graph Convolutional Kernel Network (GCKN)<ref type="bibr" target="#b3">[Chen et al., 2020]</ref> layer to produce a structureaware representation from original features, and concatenate them as the input of Transformer architecture. Here, GCKNs is a multi-layer model that produces a sequence of graph feature maps similar to a GNN. Different from GNNs, each layer of GCKNs enumerates local sub-structures at each node, encodes them using a kernel embedding, and aggregates the sub-structure representations as outputs. These representations in a feature map carry more structural information than traditional GNNs based on neighborhood aggregation.MeshGraphormer [Lin et al., 2021]  follows the second</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">: A summary of papers that applied Transformers on graph-</cell></row><row><cell cols="2">structured data. GA: GNNs as Auxiliary Modules; PE: Improved</cell></row><row><cell cols="2">Positional Embedding from Graphs; AT: Improved Attention Matrix</cell></row><row><cell>from Graphs.</cell><cell></cell></row><row><cell>Method</cell><cell>GA PE AT Code</cell></row><row><cell>[Zhu et al., 2019]</cell><cell></cell></row><row><cell>[</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Details of the datasets.</figDesc><table><row><cell>Dataset Type</cell><cell>Name</cell><cell cols="3">#Graph #Nodes(Avg.) #Edges(Avg.)</cell><cell>Task type</cell><cell>Metric</cell></row><row><cell></cell><cell>ZINC</cell><cell>12,000</cell><cell>23.2</cell><cell>49.8</cell><cell>Regression</cell><cell>MAE</cell></row><row><cell>Graph-level</cell><cell cols="2">ogbg-molhiv 41,127</cell><cell>25.5</cell><cell>27.5</cell><cell cols="2">Binary cla. ROC-AUC</cell></row><row><cell></cell><cell cols="2">ogbg-molpcba 437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>Binary cla.</cell><cell>AP</cell></row><row><cell></cell><cell>Flickr</cell><cell>1</cell><cell>89,250</cell><cell>899,756</cell><cell cols="2">Multi-class cla. Accuracy</cell></row><row><cell>Node-level</cell><cell>ogbg-arxiv</cell><cell>1</cell><cell>169,343</cell><cell cols="3">1,166,243 Multi-class cla. Accuracy</cell></row><row><cell></cell><cell>ogbg-product</cell><cell>1</cell><cell>2,449,029</cell><cell cols="3">61,859,140 Multi-class cla. Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameters of various Transformer sizes.</figDesc><table><row><cell>Transformer Size</cell></row><row><cell>Small Middle Large</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on graph-level tasks. For each column, the values with underline are the best results in each group. The values in bold are the best results across the groups.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>graph-level tasks</cell><cell></cell><cell></cell><cell>node-level tasks</cell></row><row><cell></cell><cell></cell><cell>ZINC(MAE?)</cell><cell>molhiv(ROC-AUC?)</cell><cell>molpcba(AP?)</cell><cell>Flickr(Acc?)</cell><cell>arixv(Acc?)</cell><cell>product(Acc?)</cell></row><row><cell></cell><cell></cell><cell cols="5">Small Middle Large Small Middle Large Small Middle Large Small Middle Large Small Middle Large Small Middle Large</cell></row><row><cell cols="7">TF vanilla 0.6689 0.6700 0.6699 0.7466 0.7230 0.7269 0.1624 0.1673 0.1676 0.5270 0.5279 0.5214 0.5539 0.5571 0.5598 0.7887 0.7887 0.7956</cell></row><row><cell></cell><cell cols="6">before 0.4700 0.4809 0.5169 0.6758 0.7339 0.7182 0.2105 0.1989 0.2269 0.5352 0.5369 0.5272 0.5608 0.5590 0.5614 0.7953 0.7888 0.8012</cell></row><row><cell>GA</cell><cell cols="6">alter 0.3771 0.3412 0.2956 0.7200 0.7086 0.7433 0.2474 0.2417 0.2244 0.5374 0.5357 0.5162 0.5599 0.5555 0.5592 0.7905 0.7915 0.8057</cell></row><row><cell></cell><cell cols="6">parallel 0.3803 0.2749 0.2984 0.7138 0.7750 0.7603 0.2345 0.2444 0.2205 0.5370 0.5379 0.5209 0.5647 0.5600 0.5529 0.7878 0.7896 0.7999</cell></row><row><cell></cell><cell cols="6">degree 0.5364 0.5364 0.5435 0.7506 0.6818 0.7357 0.1672 0.1646 0.1650 0.5291 0.5250 0.5133 0.5551 0.5618 0.5502 0.7920 0.7913 0.7947</cell></row><row><cell>PE</cell><cell>eig</cell><cell cols="5">0.6031 0.6074 0.6166 0.7407 0.7279 0.7351 0.2194 0.2087 0.2131 0.5253 0.5278 0.5257 0.5575 0.5637 0.5658 0.7893 0.7887 0.8017</cell></row><row><cell></cell><cell>svd</cell><cell cols="5">0.5811 0.5462 0.5400 0.7350 0.7155 0.7275 0.1648 0.1663 0.1767 0.5281 0.5317 0.5203 0.5614 0.5663 0.5706 0.7856 0.7893 0.8007</cell></row><row><cell></cell><cell cols="6">SPB 0.5122 0.4878 0.6100 0.7065 0.7589 0.7255 0.2409 0.2524 0.2621 0.5368 0.5364 0.5234 0.5503 0.5605 0.5576 0.7921 0.7918 0.7984</cell></row><row><cell>AT</cell><cell cols="6">PMA 0.6194 0.6057 0.5963 0.6902 0.7054 0.7314 0.2115 0.1956 0.2518 0.5240 0.5288 0.5204 0.5567 0.5571 0.5492 0.7877 0.7853 0.7945 Mask-1 0.2861 0.2772 0.2894 0.7610 0.7753 0.7960 0.2573 0.2662 0.1594 0.5295 0.5300 0.5236 0.5598 0.5559 0.5583 0.7923 0.7917 0.7963</cell></row><row><cell></cell><cell cols="6">Mask-n 0.3906 0.3596 0.4765 0.7286 0.7423 0.7128 0.2619 0.2577 0.2380 0.5359 0.5349 0.5348 0.5593 0.5603 0.5576 0.7935 0.7917 0.8016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The training hyper-parameters.</figDesc><table><row><cell>attention dropout</cell><cell>0.1 FFN dropout</cell><cell>0.1</cell></row><row><cell>maximum steps</cell><cell>1e+6 warm-up steps</cell><cell>4e+4</cell></row><row><cell>peak learning rate</cell><cell>2e-4 batch size</cell><cell>256</cell></row><row><cell>weight decay value</cell><cell>1e-3 lr decay</cell><cell>Linear</cell></row><row><cell>gradient clip norm</cell><cell cols="2">5.0 Adam , ?1, ?2 1e-8, 0.9, 0.99</cell></row><row><cell cols="3">et al., 2018a]. Our learning framework is built on PyTorch</cell></row><row><cell cols="3">1.8, and PyTorch Geometric 2.0, and the code is public at</cell></row><row><cell cols="2">https://github.com/qwerfdsaplking/Graph-Trans.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal ; Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
	</analytic>
	<monogr>
		<title level="m">Layer normalization</title>
		<editor>
			<persName><surname>Ba</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2010">2010. 2010. 2016. 2016</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName><forename type="first">Lam</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open catalyst 2020 (oc20) dataset and community challenges</title>
		<author>
			<persName><forename type="first">*</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Catalysis</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dwivedi and Bresson, 2020] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>Prentice hall</publisher>
			<date type="published" when="2011">2020. 2020. 2020. 2011. 2011. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1970" to="1981" />
		</imprint>
		<respStmt>
			<orgName>Forsyth and Ponce</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><surname>Gilmer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamilton</forename><surname>Douglas</surname></persName>
		</author>
		<title level="m">Time series analysis</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<idno>arXiv:2108.03348</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2021. 2021. 2021. 2021. 2021. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8783" to="8790" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Kreuzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:2104.00272</idno>
	</analytic>
	<monogr>
		<title level="m">Lijuan Wang, and Zicheng Liu. Mesh graphormer</title>
		<imprint>
			<publisher>Kevin Lin</publisher>
			<date type="published" when="2016">2016. 2016. 2021. 2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rethinking graph transformers with spectral attention. Lin et al., 2021</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Mialon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">Gr?goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Masked transformer for neighhourhood-aware click-through rate prediction</title>
		<author>
			<persName><forename type="first">Min</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13311</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal graph transformer selfattention networks</title>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11855</idno>
		<editor>Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning for audio signal processing</title>
		<author>
			<persName><surname>Purwins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02835</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Schmitt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09242</idno>
		<title level="m">Philipp Dufter, Iryna Gurevych, and Hinrich Sch?tze. Modeling graph structure via relative position for text generation from knowledge graphs</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers</title>
		<author>
			<persName><forename type="first">Quirk</forename><surname>Shiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Philipp Th?lke and Gianni de Fabritiis. Equivariant transformers for neural network based molecular potentials</title>
		<author>
			<persName><forename type="first">De</forename><surname>Th?lke</surname></persName>
		</author>
		<author>
			<persName><surname>Fabritiis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformer dissection: A unified understanding of transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11775</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Petar Velivckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><surname>Velivckovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tianming Wang, and Xiaojun Wan. Heterogeneous graph transformer for graph-tosequence learning</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00383</idno>
		<idno>arXiv:1810.00826</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><surname>Xu</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Shaowei Yao</publisher>
			<date type="published" when="2016">2019. 2019. 2018. 2018. 2018. 2016. 2016. 2020. 2020</date>
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Self-attention with structural position representations</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Ying</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<idno>arXiv:2012.01380</idno>
		<title level="m">Do transformers really perform bad for graph representation? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2021. 2021. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep graph neural networks with shallow subgraph samplers</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gresnet: Graph residual network for reviving deep gnns from suspended animation</title>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05729</idno>
		<idno>arXiv:2001.05140</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Congying</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2019. 2019. 2020</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph-bert: Only attention is needed for learning graph representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gophormer: Ego-graph transformer for node classification</title>
		<author>
			<persName><forename type="first">Akoglu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Pairnorm</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<idno>arXiv:1909.00136</idno>
	</analytic>
	<monogr>
		<title level="m">Modeling graph structure in transformer for better amr-to-text generation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2021. 2021. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tackling oversmoothing in gnns</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
