<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disconnected Recurrent Neural Networks for Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Baoxin</forename><surname>Wang</surname></persName>
							<email>bxwang2@iflytek.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint Laboratory of HIT and iFLYTEK</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disconnected Recurrent Neural Networks for Text Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text categorization is a fundamental and traditional task in natural language processing (NLP), which can be applied in various applications such as sentiment analysis <ref type="bibr" target="#b23">(Tang et al., 2015)</ref>, question classification <ref type="bibr" target="#b35">(Zhang and Lee, 2003)</ref> and topic classification <ref type="bibr" target="#b24">(Tong and Koller, 2001)</ref>. Nowadays, one of the most commonly used methods to handle the task is to represent a text with a low dimensional vector, then feed the vector into a softmax function to calculate the probability of each category. Recurrent neural network (RNN) and convolutional neural network (CNN) are two kinds of neural networks usually used to represent the text.</p><p>RNN can model the whole sequence and capture long-term dependencies <ref type="bibr" target="#b3">(Chung et al., 2014)</ref>. However, modeling the entire sequence sometimes case1: One of the seven great unsolved mysteries of mathematics may have been cracked by a reclusive Russian. case2: A reclusive Russian may have cracked one of the seven great unsolved mysteries of mathematics.</p><p>Table <ref type="table">1</ref>: Examples of topic classification can be a burden, and it may neglect key parts for text categorization <ref type="bibr" target="#b31">(Yin et al., 2017)</ref>. In contrast, CNN is able to extract local and position-invariant features well <ref type="bibr" target="#b20">(Scherer et al., 2010;</ref><ref type="bibr" target="#b4">Collobert et al., 2011)</ref>. Table <ref type="table">1</ref> is an example of topic classification, where both sentences should be classified as Science and Technology. The key phrase that determines the category is unsolved mysteries of mathematics, which can be well extracted by CNN due to position-invariance. RNN, however, doesn't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves.</p><p>In this paper, we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network (DRNN). Concretely, we disconnect the information transmission of RNN and limit the maximal transmission step length as a fixed value k, so that the representation at each step only depends on the previous k − 1 words and the current word. In this way, DRNN can also alleviate the burden of modeling the entire document. To maintain the position-invariance, we utilize max pooling to extract the important information, which has been suggested by <ref type="bibr" target="#b20">Scherer et al. (2010)</ref>.</p><p>Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units. Therefore, the maximal transmission step length can also be consid-ered as the window size in CNN. Another difference to CNN is that DRNN can increase the window size k arbitrarily without increasing the number of parameters.</p><p>We also find that there is a trade-off between position-invariance and long-term dependencies in the DRNN. When the window size is too large, the position-invariance will disappear like RNN. By contrast, when the window size is too small, we will lose the ability to model long-term dependencies just like CNN. We find that the optimal window size is related to the type of task, but affected little by training dataset sizes. Thus, we can search the optimal window size by training on a small dataset.</p><p>We conduct experiments on seven large-scale text classification datasets introduced by <ref type="bibr" target="#b36">Zhang et al. (2015)</ref>. The experimental results show that our proposed model outperforms the other models on all of these datasets.</p><p>Our contributions can be concluded as follows:</p><p>1. We propose a novel model to incorporate position-variance into RNN. Our proposed model can both capture long-term dependencies and local information well.</p><p>2. We study the effect of different recurrent units, pooling operations and window sizes on model performance. Based on this, we propose an empirical method to find the optimal window size.</p><p>3. Our proposed model outperforms the other models and achieves the best performance on seven text classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep neural networks have shown great success in many NLP tasks such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b25">Tu et al., 2016</ref><ref type="bibr">), reading comprehension (Hermann et al., 2015)</ref>, sentiment classification <ref type="bibr" target="#b23">(Tang et al., 2015)</ref>, etc. Nowadays, nearly most of deep neural networks models are based on CNN or RNN. Below, we will introduce some important works about text classification based on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Networks</head><p>CNN has been used in natural language processing because of the local correlation and position-invariance. <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> first utilize 1D CNN in part of speech (POS), named entity recognition (NER) and semantic role labeling (SRL). <ref type="bibr" target="#b14">Kim (2014)</ref> proposes to classify sentence by encoding a sentence with multiple kinds of convolutional filters. To capture the relation between words, <ref type="bibr" target="#b13">Kalchbrenner et al. (2014)</ref> propose a novel CNN model with a dynamic k-max pooling. <ref type="bibr" target="#b36">Zhang et al. (2015)</ref> introduce an empirical exploration on the use of character-level CNN for text classification. Shallow CNN cannot encode long-term information well. Therefore, <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> propose to use very deep CNN in text classification and achieve good performance. Similarly, <ref type="bibr" target="#b11">Johnson and Zhang (2017)</ref> propose a deep pyramid CNN which both achieves good performance and reduces training time.</p><p>Recurrent Neural Networks RNN is suitable for handling sequence input like natural language. Thus, many RNN variants are used in text classification. <ref type="bibr" target="#b23">Tang et al. (2015)</ref> utilize LSTM to model the relation of sentences. Similarly, <ref type="bibr" target="#b30">Yang et al. (2016)</ref> propose hierarchical attention model which incorporates attention mechanism into hierarchical GRU model so that the model can better capture the important information of a document. <ref type="bibr" target="#b27">Wang and Tian (2016)</ref> incorporate the residual networks <ref type="bibr" target="#b7">(He et al., 2016)</ref> into RNN, which makes the model handle longer sequence. <ref type="bibr" target="#b29">Xu et al. (2016)</ref> propose a novel LSTM with a cache mechanism to capture long-range sentiment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid model</head><p>Some researchers attempt to combine the advantages of CNN and RNN. <ref type="bibr" target="#b28">(Xiao and Cho, 2016)</ref> extract local and global features by CNN and RNN separately. <ref type="bibr" target="#b15">(Lai et al., 2015)</ref> firstly model sentences by RNN, and then use CNN to get the final representation. <ref type="bibr" target="#b21">Shi et al. (2016)</ref> replace convolution filters with deep LSTM, which is similar to what is proposed in this paper. The main differences are as follows. Firstly, they regard their models as CNN and set a small window size of 3, while we propose to use a large window size. We argue that small window size makes the model lose the ability to capture long-term dependencies. Secondly, we utilize max pooling but not mean pooling, because max pooling can maintain position-invariance better <ref type="bibr" target="#b20">(Scherer et al., 2010)</ref>. Finally, our DRNN model is more general and can make use of different kinds of recurrent units. We find that using GRU as recurrent units outperforms LSTM which is utilized by <ref type="bibr" target="#b21">Shi et al. (2016)</ref>.</p><formula xml:id="formula_0">w 1 w 2 w 3 w 4 h 1 h 2 h 3 h 4 RNN (a) RNN p w 1 w 2 w 3 w 4 p h 1 h 2 h 3 h 4 RNN RNN RNN RNN (b) DRNN w 1 w 2 w 3 w 4 h 1 h 2 h 3 h 4 p p Conv Conv Conv Conv (c) CNN</formula><p>Figure <ref type="figure">1</ref>: Three model architectures. In order to ensure the consistency of the hidden output, we pad k − 1 zero vectors on the left of the input sequence for DRNN and CNN. Here window size k is 3.</p><formula xml:id="formula_1">3 Method 3.1 Recurrent Neural Network (RNN)</formula><p>RNN is a class of neural network which models a sequence by incorporating the notion of time step <ref type="bibr" target="#b16">(Lipton et al., 2015)</ref>. Figure <ref type="figure">1</ref>(a) shows the structure of RNN. Hidden states at each step depend on all the previous inputs, which sometimes can be a burden and neglect the key information <ref type="bibr" target="#b31">(Yin et al., 2017)</ref>. A variant of RNN has been introduced by Cho et al. ( <ref type="formula">2014</ref>) with the name of gated recurrent unit (GRU). GRU is a special type of RNN, capable of learning potential long-term dependencies by using gates. The gating units can control the flow of information and mitigate the vanishing gradients problem. GRU has two types of gates: reset gate r t and update gate z t . The hidden state h t of GRU is computed as</p><formula xml:id="formula_2">h t = (1 − z t ) h t−1 + z t ht<label>(1)</label></formula><p>where h t−1 is the previous state, ht is the candidate state computed with new input information and is the element-wise multiplication. The update gate z t decides how much new information is updated. z t is computed as follows:</p><formula xml:id="formula_3">z t = σ(W z x t + U z h t−1 )<label>(2)</label></formula><p>here x t is the input vector at step t. The candidate state ht is computed by</p><formula xml:id="formula_4">ht = tanh(Wx t + U(r t h t−1 )) (3)</formula><p>where r t is the reset gate which controls the flow of previous information. Similarly to the update gate, the reset gate r t is computed as:</p><formula xml:id="formula_5">r t = σ(W r x t + U r h t−1 )<label>(4)</label></formula><p>We can see that the representation of step t depends upon all the previous input vectors. Thus, we can also express the tth step state shown in Equation ( <ref type="formula">5</ref>).</p><formula xml:id="formula_6">h t = GRU (x t , x t−1 , x t−2 , ..., x 1 )</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Disconntected Recurrent Neural Networks (DRNN)</head><p>To reduce the burden of modeling the entire sentence, we limit the distance of information flow in RNN. Like other RNN variants, we feed the input sequence into an RNN model and generate an output vector at each step. One important difference from RNN is that the state of our model at each step is only related to the previous k − 1 words but not all the previous words. Here k is a hyperparameter called window size that we need to set.</p><p>Our proposed model DRNN is illustrated in Figure 1(b). Since the output at each step only depends on the previous k − 1 words and current word, the output can also be regarded as a representation of a phrase with k words. Phrases with the same k words will always have the same representation no matter where they are. That is, we incorporate the position-invariance into RNN by disconnecting the information flow of RNN.</p><p>Similarly, we can get the state h t as follows:</p><formula xml:id="formula_7">h t = RN N (x t , x t−1 , x t−2 , ..., x t−k+1 ) (6)</formula><p>Here k is the window size, and RN N can be naive RNN, LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>, GRU or any other kinds of recurrent units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Convolutional Neural</head><p>Network (CNN)</p><p>DRNN can be considered as a special 1D CNN which replace the convolution filters with recur- </p><formula xml:id="formula_8">c t = [x t , x t−1 , x t−2 , ..., x t−k+1 ]<label>(7)</label></formula><p>here, we concatenate k word vectors and generate vector c t . Then we can get the output of convolution as follows:</p><formula xml:id="formula_9">h t = Wc t + b (8)</formula><p>where W is a set of convolution filters and b is a bias vector. Then a pooling operation can be applied after the convolutional layer and generate a fixed size vector <ref type="bibr" target="#b14">(Kim, 2014)</ref>. Similarly to RNN and DRNN, we can also represent the context vector of CNN as followings:</p><formula xml:id="formula_10">h t = Conv(x t , x t−1 , x t−2 , ..., x t−k+1 )<label>(9)</label></formula><p>Obviously, the parameters of convolution filters W increase as the window size k increases. By contrast, for DRNN the parameters do not increase with the increase of window size. Hence, DRNN can mitigate overfitting problem caused by the increase of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DRNN for Text Classification</head><p>DRNN is a general model framework, which can be used for a variety of tasks. In this paper, we only discuss how to apply DRNN in text categorization.</p><p>We utilize GRU as recurrent units of DRNN and get the context representation of each step. Every   <ref type="bibr">2015)</ref> after DRNN, so that the model can alleviate the internal covariate shift problem. To get the text representation vector, we apply max pooing after MLP layer to extract the most important information and position-invariant features <ref type="bibr" target="#b20">(Scherer et al., 2010)</ref>. Finally, We feed the text representation vector into an MLP with rectified linear unit (ReLU) activation and send the output of MLP to a softmax function to predict the probability of each category. We use cross entropy loss function as follows:</p><formula xml:id="formula_11">H(y, ŷ) = i y i log ŷi (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where ŷi is the predicted probability and y i is the true probability of class i.</p><p>To alleviate the overfitting problem, we apply dropout regularization <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> in DRNN model. Dropout is usually applied in the input and output layers but not the hidden states of RNN, because the number of previous states is variable <ref type="bibr" target="#b33">(Zaremba et al., 2014)</ref>. In contrast, our DRNN model has a fixed window size for output at each step, so we also apply dropout in the hidden states. In this paper, we apply dropout in the input layer, output layer, and hidden states. The Figure <ref type="figure">3</ref> shows the difference to apply dropout between  <ref type="bibr" target="#b36">Zhang et al. (2015)</ref>. We summarize the datasets in Table <ref type="table" target="#tab_2">2</ref>. AG corpus is news and DBPedia is an ontology which comes from the Wikipedia. Yelp and Amazon corpus are reviews for which we should predict the sentiment. Here P. means that we only need to predict the polarities of the dataset, while F. indicates that we need predict the star number of the review. Yahoo! Answers (Yah. A.) is a question answering dataset. We can see that these datasets contain various domains and sizes, which would be credible to validate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We tokenize all the corpus with NLTK's tokenizer <ref type="bibr" target="#b1">(Bird and Loper, 2004)</ref>. We limit the vocabulary size of each dataset as shown in Table <ref type="table">3</ref>. The words not in vocabulary are replaced with a special token UNK. Table <ref type="table">3</ref> also shows the window sizes that we set for these datasets.</p><p>We utilize the 300D GloVe 840B vectors (Pennington et al., 2014) as our pre-trained word embeddings. For words that do not appear in GloVe, we average the vector representations of 8 words around the word in training dataset as its word vector, which has been applied by <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref>. When training our model, word embeddings are updated along with other parameters.</p><p>We use Adadelta <ref type="bibr" target="#b34">(Zeiler, 2012)</ref> to optimize all the trainable parameters. The hyperparameter of Adadelta is set as Zeiler (2012) suggest that is 1e − 6 and ρ is 0.95. To avoid the gradient explosion problem, we apply gradient norm clipping <ref type="bibr" target="#b17">(Pascanu et al., 2013)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Table <ref type="table">4</ref> shows that our proposed model significantly outperforms all the other models in 7 datasets. DRNN does not have too many hyperparameters. The main hyperparameter is the window size which can be determined by an empirical method.</p><p>The top block shows the traditional methods and some other neural networks which are not based on RNN or CNN. The linear model <ref type="bibr" target="#b36">(Zhang et al., 2015)</ref> achieves a strong baseline in small datasets, but performs not well in large data. Fast-Text <ref type="bibr" target="#b12">(Joulin et al., 2017)</ref> and region embedding methods <ref type="bibr" target="#b19">(Qiao et al., 2018)</ref> achieve comparable performance with other CNN and RNN based models.</p><p>The RNN based models are listed in the second block and CNN based models are in the third block. The D-LSTM <ref type="bibr" target="#b32">(Yogatama et al., 2017)</ref> is a discriminative LSTM model. Hierarchical attention network (HAN) <ref type="bibr" target="#b30">(Yang et al., 2016)</ref> is a hierarchical GRU model with attentive pooling. We can see that very deep CNN (VDCNN) <ref type="bibr" target="#b5">(Conneau et al., 2017)</ref>   Char-CRNN <ref type="bibr" target="#b28">(Xiao and Cho, 2016)</ref> in the fourth block is a model which combines positioninvariance of CNN and long-term dependencies of RNN. Nevertheless, they do not achieve great improvements over other models. They first utilize convolution operation to extract position-invariant features, and then use RNN to capture long-term dependencies. Here, modeling the whole sequence with RNN leads to a loss of position-invariance. Compared with their model, our model can better maintain the position-invariance by max pooling <ref type="bibr" target="#b20">(Scherer et al., 2010)</ref>. Table <ref type="table">4</ref> shows that our model achieves 10-50% relative error reduction compared with char-CRNN in these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with RNN and CNN</head><p>In this section, we compare DRNN with CNN, GRU and LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>. To make these models comparable, we im- plement these models with the same architecture shown in Figure <ref type="figure">2</ref>. We just replace the DRNN with CNN or RNN. we firstly compare DRNN with CNN on AG dataset. Figure <ref type="figure" target="#fig_1">4</ref> shows that DRNN performs far better than CNN. In addition, the optimal window size of CNN is 3, while for DRNN the optimal window size is 15. It indicates that DRNN can model longer sequence as window size increases. By contrast, simply increasing the window size of CNN only results in overfitting. That is also why <ref type="bibr" target="#b5">Conneau et al. (2017)</ref> design complex CNN models to learn long-term dependencies other than simply increase the window size of convolution filters.</p><p>In addition, we also compare our model with GRU and LSTM. The experimental results are shown in Table <ref type="table" target="#tab_5">5</ref>. Our model DRNN achieves much better performance than GRU and LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>To investigate why DGRU performs better than CNN and GRU, we do some error analysis on Yelp P. dataset. Table <ref type="table" target="#tab_6">6</ref> shows two examples which have been both case1: I love Hampton Inn but this location is in serious need of remodeling and some deep cleaning. Musty smell everywhere. case2: Pretty good service, but really busy and noisy!! It gets a little overwhelming because the sales people are very knowledgeable and bombard you with useless techy information to I guess impress you?? Anyways I bought the Ipad 3 and it is freaking awesome and makes up for the store. I would give the Ipad 3 a gazillion stars if I could. I left it at home today and got really sad when I was driving away. Boo Hoo!!  Considering the first example, CNN may extract some key phrases such as I love and misclassifies the example as P ositive, while GRU can model long sequence and capture the information after but. For the second example, however, GRU still captures the information after but and neglects the key phrases such as pretty good service and freaking awesome, which leads to the wrong classification. DGRU can both extract the local key features such as pretty good service and capture long-term information such as the sentence after but, which makes it perform better than GRU and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Component Analysis</head><p>Recurrent Unit In this part, we study the impact of different recurrent units on the effectiveness of DRNN. We choose three types of recurrent units: naive RNN, LSTM and GRU which have been compared by <ref type="bibr" target="#b3">Chung et al. (2014)</ref>. We carry out the experiments with different window sizes to eliminate the impact of window sizes. All the experiments in this part are conducted on the AG dataset.</p><p>We find that the disconnected naive RNN performs just a little worse than disconnected LSTM (DLSTM) and disconnected GRU (DGRU) when the window size is lower than 5. However, when the window size is more than 10, its performance decreases rapidly and the error rate becomes even more than 20%. We believe that it is due to vanishing gradient problem of naive RNN.</p><p>From Figure <ref type="figure">5</ref>(a), we can see that window sizes affect the performance of DGRU and DLSTM. DGRU achieves the best performance when the window size is 15, while the best window size for DLSTM is 5. The performance of DGRU is always better than DLSTM no matter what the window size is. We also find that the DGRU model converges faster than DLSTM in the process of training. Therefore, we apply GRU as recurrent units of DRNN in this paper for all the other experiments.</p><p>Pooling Method Pooling is a kind of method to subsample the values to capture more important information. In NLP, pooling can also convert a variable-length tensor or vector into a fixed-length   one, so that it can be dealt with more easily. There're several kinds of pooling methods such as max pooling, mean pooling and attentive pooling <ref type="bibr" target="#b6">(dos Santos et al., 2016)</ref>.</p><p>We still conduct the experiments on AG dataset. Figure <ref type="figure">5</ref>(b) shows the experimental results of three pooling methods along with different window sizes. From Figure <ref type="figure">5</ref>(b), we can see that the DRNN model with max pooling performs better than the others. This may be because that max pooling can capture position-invariant features better <ref type="bibr" target="#b20">(Scherer et al., 2010)</ref>. We find attentive pooling is not significantly affected by window sizes. However, the performance of mean pooling becomes worse as the window becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Window size analysis</head><p>In this section, we mainly study what factors affect the optimal window size. In addition to the recurrent units and pooling methods discussed above, we believe the optimal window size may be also related to the amount of training data and the type of task.</p><p>In order to study the factors that affect the optimal window size, we conduct experiments on three datasets: AG, DBP and Yelp Polarity. To eliminate the influence of differrnt training data sizes, we conduct experiments with the same training data size. From Figure <ref type="figure" target="#fig_5">6</ref>(a) we can see that the type of task has a great impact on the optimal window size. For AG and DBPedia, the optimal window size is 15. However, for Yelp P. the optimal window size is 40 or even larger. The result is intuitive, because sentiment analysis such as Yelp often involves long-term dependencies <ref type="bibr" target="#b23">(Tang et al., 2015)</ref>, while topic classification such as AG and DBPedia relys more on the key phrases.</p><p>From Figure <ref type="figure" target="#fig_5">6</ref>(b) and Figure <ref type="figure" target="#fig_5">6</ref>(c) we can see the effect of different training data sizes on the optimal window size. Surprisingly, the effect of different training data sizes on the optimal window size seems little. We can see that for both DBPedia and Yelp corpus, the trend of error rate with the window size is similar. This shows that the number of training data has little effect on the choice of the optimal window size. It also provides a good empirical way for us to choose the optimal window size. That is, conducting experiments on a small dataset first to select the optimal window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we incorporate position-invariance into RNN, so that our proposed model DRNN can both capture key phrases and long-term dependencies. We conduct experiments to compare the effects of different recurrent units and pooling operations. In addition, We also analyze what factors affect the optimal window size of DRNN and present an empirical method to search it. The experimental results show that our proposed model outperforms CNN and RNN models, and achieve the best performance in seven large-scale text classification datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DGRU compared with CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5: Component comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Different training sets of DBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Different training sets of Yelp P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Window size analysis. For better comparing the trends of different tasks, (a) shows the error reduction rates with different window sizes. (b) and (c) show the error rates of DBP. and Yelp P. with different training set numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset information. Here SA refers to sentiment analysis, and QA refers to question answering.</figDesc><table><row><cell></cell><cell>AG</cell><cell>DBP</cell><cell cols="5">Yelp P. Yelp F. Yah. A. Amz. F. Amz. P.</cell></row><row><cell>Tasks</cell><cell cols="2">News Ontology</cell><cell>SA</cell><cell>SA</cell><cell>QA</cell><cell>SA</cell><cell>SA</cell></row><row><cell>Train dataset</cell><cell>120k</cell><cell>560k</cell><cell>560k</cell><cell>650k</cell><cell>1.4M</cell><cell>3.6M</cell><cell>3M</cell></row><row><cell>Test dataset</cell><cell>7.6k</cell><cell>70k</cell><cell>38k</cell><cell>50k</cell><cell>60k</cell><cell>400k</cell><cell>650k</cell></row><row><cell>Average Lengths</cell><cell>45</cell><cell>55</cell><cell>153</cell><cell>155</cell><cell>112</cell><cell>93</cell><cell>91</cell></row><row><cell>Classes Number</cell><cell>4</cell><cell>14</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>5</cell><cell>2</cell></row><row><cell>RNN and DRNN.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Experimental Settings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets Introduction</cell><cell cols="3">We use 7 large-scale</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">text classification datasets which are proposed by</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. The batch size is set to 128 and all the dimensions of input vectors and hidden</figDesc><table><row><cell cols="3">Corpus Window size Vocabulary size</cell></row><row><cell>AG</cell><cell>15</cell><cell>100k</cell></row><row><cell>DBP.</cell><cell>15</cell><cell>500k</cell></row><row><cell>Yelp P.</cell><cell>20</cell><cell>200k</cell></row><row><cell>Yelp F.</cell><cell>20</cell><cell>200k</cell></row><row><cell>Yah. A.</cell><cell>20</cell><cell>500k</cell></row><row><cell>Amz. F.</cell><cell>15</cell><cell>500k</cell></row><row><cell>Amz. P.</cell><cell>15</cell><cell>500k</cell></row><row><cell cols="3">Table 3: Experimental settings</cell></row><row><cell cols="2">states are set to 300.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>performs well in large datasets. However, VDCNN is a CNN model with 29 convolutional layers, which needs to be tuned more carefully. By contrast, our proposed model can achieveModelsAG DBP. Yelp P. Yelp F. Yah. A. Amz. F. Amz. P.</figDesc><table><row><cell cols="9">Linear model (Zhang et al., 2015) 7.64 1.31</cell><cell>4.36</cell><cell>40.14</cell><cell>28.96</cell><cell>44.74</cell><cell>7.98</cell></row><row><cell cols="6">FastText (Joulin et al., 2017)</cell><cell></cell><cell>7.5</cell><cell>1.4</cell><cell>4.3</cell><cell>36.1</cell><cell>27.7</cell><cell>39.8</cell><cell>5.4</cell></row><row><cell cols="6">Region.emb (Qiao et al., 2018)</cell><cell></cell><cell>7.2</cell><cell>1.1</cell><cell>4.7</cell><cell>35.1</cell><cell>26.3</cell><cell>39.1</cell><cell>4.7</cell></row><row><cell cols="7">D-LSTM (Yogatama et al., 2017)</cell><cell>7.9</cell><cell>1.3</cell><cell>7.4</cell><cell>40.4</cell><cell>26.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">HAN (Yang et al., 2016)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.2</cell><cell>36.4</cell><cell>-</cell></row><row><cell cols="6">char-CNN (Zhang et al., 2015)</cell><cell></cell><cell cols="2">9.51 1.55</cell><cell>4.88</cell><cell>37.95</cell><cell>28.80</cell><cell>40.43</cell><cell>4.93</cell></row><row><cell cols="6">word-CNN (Zhang et al., 2015)</cell><cell></cell><cell cols="2">8.55 1.37</cell><cell>4.60</cell><cell>39.58</cell><cell>28.84</cell><cell>42.39</cell><cell>5.51</cell></row><row><cell cols="6">VDCNN (Conneau et al., 2017)</cell><cell></cell><cell cols="2">8.67 1.29</cell><cell>4.28</cell><cell>35.28</cell><cell>26.57</cell><cell>37.00</cell><cell>4.28</cell></row><row><cell cols="9">char-CRNN (Xiao and Cho, 2016) 8.64 1.43</cell><cell>5.51</cell><cell>38.18</cell><cell>28.26</cell><cell>40.77</cell><cell>5.87</cell></row><row><cell cols="3">DRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5.53 0.81</cell><cell>2.73</cell><cell>30.85</cell><cell>23.74</cell><cell>35.57</cell><cell>3.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Table 4: Error rates (%) on seven datasets</cell></row><row><cell></cell><cell>6.8</cell><cell></cell><cell>DGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6.6</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error rate (%)</cell><cell>6.0 6.2 6.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5.6</cell><cell>3 5</cell><cell>10</cell><cell>15</cell><cell>20 Window size</cell><cell>30</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with RNN and CNN. Table shows the error rate (%) on three datasets.</figDesc><table><row><cell cols="3">Models AG DBP. Yelp P.</cell></row><row><cell>CNN</cell><cell>6.30 1.13</cell><cell>4.08</cell></row><row><cell>GRU</cell><cell>6.25 0.96</cell><cell>3.41</cell></row><row><cell cols="2">LSTM 6.20 0.90</cell><cell>3.20</cell></row><row><cell cols="2">DRNN 5.53 0.81</cell><cell>2.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Examples of error analysis. The case 1 is a negative review and case 2 is a positive review. The first example is misclassified by CNN and classified correctly by GRU. The second one is just the contrary. DGRU classify both examples correctly.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Program of China (No. 2016YFC0800806). I would like to thank Jianfeng Li, Shijin Wang, Ting liu, Guoping Hu, Shangmin Guo, Ziyue Wang, Xiaoxue Wang and the anonymous reviewers for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics</title>
				<meeting>the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName><forename type="first">Cıcero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName><forename type="first">John</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new method of region embedding for text classification</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep lstm based feature mapping for query classification</title>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
				<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11">2001. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<title level="m">Modeling coverage for neural machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent residual learning for sequence classification</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="938" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00367</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01923</idno>
		<title level="m">Comparative study of cnn and rnn for natural language processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generative and discriminative text classification with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Question classification using support vector machines</title>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
				<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
