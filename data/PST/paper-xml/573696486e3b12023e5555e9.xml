<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Visual Semantic Relationships for Efficient Visual Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xshua@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Visual Semantic Relationships for Efficient Visual Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DFA9A214F94D949604276B2DFEE4674</idno>
					<idno type="DOI">10.1109/TBDATA.2016.2515640</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2016.2515640, IEEE Transactions on Big Data</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual concept relationship</term>
					<term>hashing</term>
					<term>image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate how to establish the relationship between semantic concepts based on the large-scale real-world click data from image commercial engine, which is a challenging topic because the click data suffers from the noise such as typos, the same concept with different queries, etc. We first define five specific relationships between concepts. We then extract some concept relationship features in textual and visual domain to train the concept relationship models. The relationship of each pair of concepts will thus be classified into one of the five special relationships. We study the efficacy of the conceptual relationships by applying them to augment imperfect image tags, i.e., improve representative power. We further employ a sophisticated hashing approach to transform augmented image tags into binary codes, which are subsequently used for content-based image retrieval task. Experimental results on NUS-WIDE dataset demonstrate the superiority of our proposed approach as compared to state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed the explosive growth of Web multimedia data driven by the advance of fast network, massive storage, digital cameras, etc. How to effectively and efficiently process these data has become a critical yet challenging problem in modern search engines. Exploring semantic relationships between concepts has been a promising research topic in this area, since it has been widely applied on natural language processing, object detection, and multimedia retrieval <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. However, important as it can be, a rapid and robust approach that can discover the complex concept relationship in a large-scale realworld network environment has not been investigated in the industry and the academia. Nonetheless, the concept relationship is derived from higher level of knowledge of human cognition, hence it is not easily described.</p><p>In recent years, great research endeavors have been dedicated to initial artificial attempts, such as Cyc project <ref type="bibr" target="#b3">[4]</ref> and WordNet project <ref type="bibr" target="#b4">[5]</ref>. However, it is extremely time-consuming and labor-extensive to manually to design such network for distinguishing the complex concept relationship and to construct the network by linguistic experts. Although great efforts have been cost to enlarge the scale of concept network, the concept database is still limited and difficult to update compared to the overall informa-tion on the Web. In order to cope with the above problems, some approaches have been proposed to automatically explore the relationship by means of measuring the difference of two concepts. These approaches cost little human effort and can cover far more concepts in the Web. Bussmann <ref type="bibr" target="#b5">[6]</ref> proposed the componential analysis to distinguish the meaning of concepts through structured sets of semantic features. The performance of componential analysis depends on the definition of sense components, however, to define these sense components needs a high level of language knowledge and to date there is no uniform standard. Lei et al. <ref type="bibr" target="#b6">[7]</ref> proposed Flickr distance to measure the similarity between concepts by the squared root of jensen-shannon divergence between the corresponding visual language models. However, they only measured the distance of pairwise concepts and cannot recognize the category of concept relationship, hence such approach is limitedly applied in a variety of application environment. Therefore, it is promising to come out with an approach to recognize different types of relationships, sucha as hyponymy and meronymy.</p><p>Moreover, to facilitate large-scale image retrieval and other multimedia-related applications <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b12">[13]</ref>, efficient indexing techniques are in great demand as well. Hashing techniques have been showing its extremely high efficiency and effectiveness in various large-scale applications, such as multimedia search, multimedia annotation, etc. The typical procedure of hashing is to first transform the original representation of data into low-dimensional features (realvalued) using dimensionality reduction techniques; and then the low-dimensional features are binarized using zero thresholding or other sophisticated learning-based approaches. The benefits of hashing techniques are two-fold: 1) computational efficiency is high, i.e., only XOR operation is required to compute Hamming distance of data; and 2) storage requirement is low, i.e., we can simply compress one image into 4 bytes, which makes it possible to load an entire large-scale database into main memory for computation.</p><p>Inspired by the aforementioned analysis and our previous work on visual relationships construction <ref type="bibr" target="#b13">[14]</ref>, in this paper, we propose a novel framework to facilitate large-scale image retrieval with visual semantic relationship modelling as well as sophisticated hashing technique. Specifically, we first define five types of specific semantic relationships between concepts, and model them by extracting textual and visual knowledge from a precious click log dataset collected from a commercial image search engine. An 8-dimensional feature is constructed by considering both textual and visual information. Click log information of multimedia search is able to reflect user search intent, which is beneficial for subsequent multimedia-related applications. Then, we employ these learned relationship models to determine which kind of relationship a pair concepts belongs to and quantify the estimated relationships. Furthermore, we construct a conceptual relationships set, which is used to augment user-provided image tags, which are often imperfect. With the augmented image tag feature, an effective hashing approach is exploited to transform image data into binary codes, which is finally used to support multimedia applications, such as contentbased image retrieval.</p><p>The contributions of the paper are summarized as follows:</p><p>• We assume that concept is composed of visual words, which represents the appearance characteristics of concept. We model the concept by the probability distribution of visual words and the similarity between concepts, as well as the difference of probability distribution. • To the best of our knowledge, our work is one of the first to give multifold definitions of the concept relationship in visual domain and perform a comprehensive study on the role of concept relationship features in classifying the concept relationship. • Different concept similarity metrics have different capacity to describe the relationship between concepts. We extract different concept relationship features from three levels, including textual expression of concepts, visual images of concepts and the co-occurrence frequency of concepts labeled on the same image. • We design an approach to augment image tag with the derived conceptual relationships, and a sophisticated hashing method is employed to transform images into binary codes, which can support efficient image search in large scale dataset. The rest of the paper is organized as follows. Section 2 presents some related works on conceptual relationships and hashing techniques. Section 3 elaborates the details of the proposed framework, including modelling concept relationships, image tag augmentation and transforming image into binary codes. In section 4, we report extensive experimental results on a large-scale real-world image corpus -NUS-WIDE, followed by conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we simply review recent research works on measurement of concept relationship (or distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.1">Google Distance</head><p>Google distance is proposed to compute the distance between two concepts derived from the number of hits returned by Google search engine when querying both concepts. The concept relationship with the same or similar meanings in a natural language sense tend to be close in units of Google distance, while concepts with dissimilar meanings tend to be farther apart. The normalized Google distance derives from the earlier normalized compression distance <ref type="bibr" target="#b14">[15]</ref>. NGD can ideally measures the textual conceptual relationship, i.e. the same and similarity, when the pairwise concepts frequently occur in the same web page. However, NGD is insufficient to measure the concept relationship among daily life, since the knowledge of concept relationship stems from human perception and 80% of human cognition comes from visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.2">Flickr Distance</head><p>In order to make up for the lack of Normalized Google Distance, Wu et al. <ref type="bibr" target="#b6">[7]</ref> proposed the Flickr Distance to explore the relationship between two concepts in visual domain by measuring the square root of Jensen-Shannon divergence between the visual language models corresponding to these two concepts. If two concepts more frequently appear in the same image, the relationship is more similarity and the Flickr distance is small, otherwise dissimilarity and large. Since the Flickr distance measures the similarity between two concepts based on the images in visual domain, it is in accord with the human cognition. However, the Flickr distance only measures the similarity but fails to consider the type of relationship between two concepts. Thus it is not quite reasonable to directly apply the Flickr distance to the real-world environment. In addition that the Flickr distance is established on the Flickr data, However, Flickr data is very noisy. The computational cost of Flickr distance is quite expensive, so the Flickr distance is inadequate in the large-scale environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.3">Semantic Networks</head><p>Semantic network is a graph structure for representing knowledge in patterns of interconnected nodes, which represent concepts, and arcs, which represent relations. Sowa et al. <ref type="bibr" target="#b15">[16]</ref> proposed the semantic network for artificial intelligence and machine translation. According to types of relations, Semantic networks can be divided into definitional network, assertional network, implicational network, executable network, learning network and hybrid network, one of the most commonly used is the definitional network. Definitional network emphasizes "is-a" relation or inheritance relation between concepts, such as WordNet. In the WordNet, nodes represent concept set with synonymous relation and arcs represent inheritance relation between nodes. WordNet is mainly used to help computer emulates the human capability for external knowledge cognition, and is widely used in data mining, expert system and artificial intelligence field.</p><p>Compared with these existing methods, our framework has been verified on real-world scenarios with state-of-the-art techniques, such as hashing. We introduce these techniques in the following part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hashing Techniques</head><p>In recent years, hashing techniques have become popular in indexing high-dimensional data, such as image, video. It helps to facilitate efficient and effective multimedia applications <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Generally, there are two promising directions: data-independent and datadependent.</p><p>Among data-independent approaches, the family of Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b16">[17]</ref> has been widely employed in reality. The key idea is to create hash functions by random projections. Several follow-up works have been manipulating the distance measures, such as and kernel similarity <ref type="bibr" target="#b25">[26]</ref>, p-norm distances <ref type="bibr" target="#b26">[27]</ref> and Mahalanobis metric <ref type="bibr" target="#b27">[28]</ref>. But they all suffer from the problem that longer code length is needed to achieve good performance.</p><p>To overcome the above issues, data-dependent (or learning-based) hashing paradigms have been extensively developed. These kinds of methods are capable of capturing intrinsic data characteristics, as well as generate more effective yet shorter binary codes. Apart from using random methods, data-dependent hashing automatically learns compact codes from data itself. The representative linear algorithms contains unsupervised PCA Hashing <ref type="bibr" target="#b19">[20]</ref>, Iterative Quantization (ITQ) <ref type="bibr" target="#b18">[19]</ref>, Isotropic Hashing <ref type="bibr" target="#b28">[29]</ref>, etc., and supervised Minimal Loss Hashing (MLH) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, Semi-Supervised Hashing (SSH) <ref type="bibr" target="#b19">[20]</ref>, LDA Hashing <ref type="bibr" target="#b31">[32]</ref>, Ranking-Based Supervised Hashing <ref type="bibr" target="#b32">[33]</ref>, FastHash <ref type="bibr" target="#b33">[34]</ref>, etc. Moveover, non-linear coding approaches are also well developed. For instance, Binary Reconstructive Embedding (BRE) <ref type="bibr" target="#b34">[35]</ref>, Random Maximum Mar-gin Hashing (RMMH) <ref type="bibr" target="#b35">[36]</ref>, Kernel-Based Supervised Hashing (KSH) <ref type="bibr" target="#b21">[22]</ref>, kernel ITQ <ref type="bibr" target="#b18">[19]</ref> are several typical representatives in the early stage. Another classic hashing method is spectral hashing (SH) <ref type="bibr" target="#b17">[18]</ref>, which takes local data structural information into account. After that, more sophisticated ideas are developed to enhance this category, such as Anchor Graph Hashing (AGH) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, Inductive Manifold Hashing (IMH) <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUAL SEMANTIC RELATIONSHIPS</head><p>In this section, we first define and model relationships between concepts, and then determine specific relationship of each pair of concepts with the assistance of both textual clue and visual characteristics from click log data of image search engine. Further, we design an effective procedure to augment image textual features, which are originally noisy and incomplete, with the derived concept pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modelling Concept Relationships</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Definition of Relationships</head><p>We first define five types of concept relationships as follows: leftmargin=*</p><p>• Complete Similarity (CS) is used to determine whether two concepts have the same meaning, i.e., synonym, such as "Justin Bieber and J.B.", "Beijing and Peking", etc. • Type Similarity (TS) is the relationship indicating two concepts belong to the same object. For instance, Husky and Bulldog are both specific types of dog; and Goldfish and Angelfish are both species of fish. </p><formula xml:id="formula_0">T =&lt; I, q, c &gt;<label>(1)</label></formula><p>where the triad T indicates that image I is clicked c times in the search results of query q within a year.</p><p>Clickture contains 40 million unique images and 73.6 million unique queries. There is also a lite version of Clickture, namely Clickture-Lite, contained 1 million images, 11.7 million queries and 23.1 million triads. We define eight types of features to characterize relationships between concepts as follows.</p><p>leftmargin=*</p><p>• Visual Word (VW). We assume that the number of visual words is limited and different concept relationships have different visual word distributions. Suppose we are given a database of image-concept pairs {(x i , y i )}| m i=1 . In each pair, x i is the image feature, such as bag-of-visualword based on SIFT, and y i is the concept. We assume that visual words {vw 1 , vw 2 , . . . , vw k } are generated by k-means. Denote image set as X = [x 1 , x 2 , . . . , x m ], then we compute conceptual feature by the divergence between distributions of visual words:</p><formula xml:id="formula_1">g(x) = argmin i=1,2,...,k x -vw i .<label>(2)</label></formula><p>• Spatial Visual Word (SVW) Visual Word does not consider the spatial dependence between each other. By considering the spatial relative position and co-occurrence frequency between different visual words, we propose to use spatial visual word to express the spatial relative position of local feature. Image I can be expressed by visual word and spatial dependence as below:</p><formula xml:id="formula_2">I = {(vw 1 , vw 2 , |x 1 -x 2 |, |y 1 -y 2 |), (vw 1 , vw 3 , |x 1 -x 3 |, |y 1 -y 3 |), . . .}.<label>(3)</label></formula><p>Even without considering the spatial relative position of visual word, the number of visual phrase is still C 2 n . In order to the computational efficiency of the consideration, we quantify the horizontal and vertical of visual phrase and combining the redundant visual phrases to reduce the number of visual phrases. Eventually we will choose the k maximum frequency of the visual phrases in all the concept.</p><formula xml:id="formula_3">x =    0, |x i -x j | ≤ 1 3 width 1, 1 3 width &lt; |x i -x j | ≤ 2 3 width 2, |x i -x j | &gt; 1 3 width y =    0, |y i -y j | ≤ 1 3 height 1, 1 3 height &lt; |y i -y j | ≤ 2 3 height 2, |y i -y j | &gt; 1 3 height<label>(4)</label></formula><p>• Edit Distance (ED) It is a way of quantifying how dissimilar two strings (e.g., query) are to one another by counting the minimum number of operations,such as insert, delete and substitute, required to transform one string into the other. Edit Distance is regarded as one dimensional conceptual feature to measure the conceptual relationship between the queries of two concepts. • Earth Movers Distance (EMD) The EMD <ref type="bibr" target="#b7">[8]</ref> is widely used in content-based image retrieval to compute distances between the color histograms of two digital images. The EMD between concepts c i and c j is defined as follows:</p><formula xml:id="formula_4">EM D(c i , c j ) = 1 N × N x∈Ωc i ,y∈Ωc j , emd(x, y) (5)</formula><p>where emd(•, •) is the distance between the color histograms of two images. Ω c is the top n images of concept c ordered by click count. Since the computational cost of EMD is super-cubic, the N is set to 50. EM D(c i , c j ) is the one dimensional conceptual feature.</p><p>• Jensen Kmeans (JK) is computed by measuring the Jensen-Shannon divergence between two visual words' probability distributions generated by k-means. For each visual word probability distribution P (c) can be computed as</p><formula xml:id="formula_5">P (c) = num of vw i N (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where N is the total number of visual word in the concept c, and num of vw i is the number of vw i in concept c. JK can be computed as the Jensen-Shannon divergence between two concepts:</p><formula xml:id="formula_7">JK(c i ||c j ) = n i=1 p i1 log p i1 + p i2 log p i2<label>2</label></formula><formula xml:id="formula_8">-( p i1 + p i2 2 ) log( p i1 + p i2 2 ) .<label>(7)</label></formula><p>• Cosine Kmeans (CK) is computed by measuring the cosine similarity between two visual word probability distributions generated by kmeans.</p><p>For each visual word probability distribution P (c) can be computed as above. The cosine similarity can be computed as follow:</p><formula xml:id="formula_9">CS(c i , c j ) = n i=1 pi 1 pi 2 n i=1 p 2 i 1 n i=1 p 2 i 2 CS dist (c i , c j ) = 1 -CS(c i , c j )<label>(8)</label></formula><p>where CS dist (•, •) represents the the cosine distance between two concepts. Note that Jensen Prase Kmenas (JPK) and Cosine Prase Kmeans(CPK) JPK and CPK are very similar with JK and CK, except that JPK and CPK use visual words and visual phrases to substitute visual words in the JK and CK. • Jensen Hash (JH) is computed the same as Jensen Kmeans(JK), except for the way of visual words generated. The Visual Word(VW) is generated by Bins Hash coding <ref type="bibr" target="#b10">[11]</ref> as below:</p><formula xml:id="formula_10">V W ik = 0, if v ik &gt; mean k 1, otherwise,<label>(9)</label></formula><p>where V W ik represents the kth dimensional feature of visual word of image i. • Cosine Hash (CH) is computed the same as Cosine Similarity (CS), except for the way of visual word generated. The Visual Word (VW) is generated by Bins Hash coding as follows:</p><formula xml:id="formula_11">V W ik = 0, ifv ik &gt; mean k 1, otherwise<label>(10)</label></formula><p>where V W ik represents the kth dimensional feature of visual word of image i. • Exemplar Similarity (ES) is computed by measuring the similarity between the exemplars of two concepts. Exemplar can be generated by performing k-means on the image feature set of this concept and one centroid is one exemplar (k N ). Denote Ω c as the representative exemplar set of concept c. Then the exemplar similarity between concepts c 1 and c 2 is defined as follows:</p><formula xml:id="formula_12">Sim e (c i , c j ) = exp(- 1 k × k x∈Ωc i ,y∈Ωc j x -y 2 σ 2 ) (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where σ is set to the median value of all pairwise Euclidean distances between exemplars of different concepts.</p><p>• Normalized Google Distance (NGD) is proposed to calculate the distance between two concepts in the search results from Google search engine. Hence, we make some slight changes to let it more suitable for our problem. The Google distance is defined as follow:</p><formula xml:id="formula_14">N GD(c i , c j ) = max(log f (c i ), log f (c j )) -logf (c i , c j ) log N -min(log f (c i ), log f (c j ))<label>(12</label></formula><p>) where N GD(c i , c j ) represents the Normalized Google distance between concepts c i and c j . f (c i ), f (c j ) and f (c i , c j ) denote the number of images labeled c i , c j , both c i and c j , respectively. N is the total number of images in the click data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modelling Visual Semantic Relationships</head><p>In this part, we employ the aforementioned conceptual features and Clickture data to train relationship classifiers.</p><p>For exploring the relationship between these concepts, we extract 8-dimensional conceptual features and manually annotate the type of relationship between two concepts. If we have concepts, the size of data set is (n × (n -1)/2). The data set can express as follows:    x = {ED, EDM, N GD, ES, CK, JK, CH, JH} y = {r|r ∈ {CS, tS, HH, P R, U R}} training data = {(x i , y i ), i = 1, 2, ..., n(n -1)/2}</p><p>(13) The conceptual relationship y represents the five specific relationships between any two concepts asorder by the distance between two concepts. The 8-dimensional conceptual features are extracted from different aspects of Clickture data set. The concept feature ED describes the difference of five conceptual relationships in textual queries. ED of CS is close to 0, such as hyena and hyaena, but ED of PR is close to 1, such as weasel and miniature poodle. These conceptual features EMD, JK, CK, JH, CH and ES derive from basic image features, such as SIFT and LLC. We use these conceptual features to capture the difference of concept relationship in visual domain. EMD and ES stem from basic image feature, so these two conceptual features can easily capture these differences of image content between two concepts. JK, CK, JH and CH are from visual word distribution. Since different latent topic concepts have different visual word distributions, these conceptual features can easily find the difference of latent topic of different concepts. The conceptual feature NGD is used to describe the concurrence of image file. The NGD is close to 0 between hyena and hyaena, because of a mass of the same image files in these two concepts. In according to the difference of 8dimensional conceptual features, we can recognise the type of concept relationship.</p><p>We sample a portion of the common concepts and construct the labeled data set as above. We apply the multi-class SVM to train the visual concept relationship models, which can be further used to classify the relationship of any two concepts.</p><p>Finally, we apply conceptual classifiers on all concept pairs within evaluated dataset. For practical use, we employ the first four specific relationships (i.e., CS, TS, HH and PR) and keep those concept pairs with high confidence (e.g., confidence ¿=0.8). Therefore, we have a conceptual relationship collection, denoted as R = {r 1 , r 2 , . . . , r N }, where r i = (c p , c q , R) is conceptual relationship of c p and c q , N is the number of all relationships and R ∈ {CS, T S, HH, P R}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Augmenting Image Tags with Visual Semantic Relationships</head><p>Given an image x with imperfect descriptive tag set T = {t 1 , t 2 , . . . , t l }, we may use the derived visual concepts relationships to augment it. Intuitively, if a concept c is not in the tag set of x, but c is of high relevance to the tags in T , then c should be probably added to T . We design a practical algorithm to perform the tag augmentation procedure as shown in Algorithm 1.</p><p>After we use the above procedure to pre-process all image tags in database, we may obtain an image dataset with complete and reliable tag feature. We denote the processed dataset as X = [x 1 , x2 , . . . , xn ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Encoding Image Tags into Binary Representation</head><p>Note that image tags may be of high dimensionality and contain unnecessary redundancy. In this part, we employ a sophisticated indexing approach to generate efficient binary representation of image data. for all t ∈ T do 3:</p><formula xml:id="formula_15">A = {}; 4:</formula><p>for all c ∈ C do 5:</p><formula xml:id="formula_16">if (t, c) ∈ R ∧ c / ∈ C then 6:</formula><p>A ← A ∪ {c}; Given any new image tag vector x, we first generate an embedding y. We may expect that y preserves local structural information within the entire training dataset. To this end, we can compute y by optimizing the following model:</p><formula xml:id="formula_17">min y m i=1 s(x, x i ) y -y i 2 (14)</formula><p>where s(x, x i ) is similarity measurement between x and x i . We may use any type of similarity metrics. In this work, we use the following</p><formula xml:id="formula_18">s(x, x i ) = exp(-x-xi 2 σ 2 ), if x i ∈ N k (x); 0, otherwise.<label>(15)</label></formula><p>It is notable that the solution of Eq.( <ref type="formula">14</ref>) guarantees that if the new datum is similar to some points in the original space, then the embedding y of the new data points is close to the embeddings of those neighbors.</p><p>Setting the derivative of the objective function in Eq.( <ref type="formula">14</ref>) w.r.t. y to zero, we arrive at</p><formula xml:id="formula_19">y * = m i=1 (s(x, x i )y i ) m i=1 s(x, x i )<label>(16)</label></formula><p>which is actually a weighted linear combination of the embeddings of the training data. Note that the above solution may be inefficient when the number of training samples is large. In order to solve this problem, we apply the conventionally used process in this area <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> to obtain the binary representation of x:</p><formula xml:id="formula_20">h(x) = sgn( o i=1 (s(x, x i )y i ) o i=1 s(x, x i ) ) (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>where sgn(•) is the sign function and</p><formula xml:id="formula_22">{y i | o i=1 } is the embeddings of certain base set B = {b 1 , b 2 , . . . , b o }.</formula><p>Here, B can be randomly sampled from X or use the cluster centers of k-means on X.</p><p>The aforementioned hash function can well generalize to new data with a constant computational cost. Now, we may embed all training data with the following transformation:</p><formula xml:id="formula_23">Y = W XB Y B , (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>where Y is the embedding matrix of all training data, Y B is the embedding of base set, and W XB is a matrix defined over X and B. The element corresponding to the ith row and j column is defined as</p><formula xml:id="formula_25">W ij XB = s(x i , b j ) m i=1 s(x i , b j )<label>(19)</label></formula><p>In fact, Eq.( <ref type="formula" target="#formula_23">18</ref>) defines a general framework of data embedding, where Y B can be generated by any proper embedding method. In this work, we choose to use the approach <ref type="bibr" target="#b40">[41]</ref> considering smoothness within B and smoothness between B and X:</p><formula xml:id="formula_26">min Y B bi,bj ∈B s(b i , b j ) y i -y j 2 +λ bi∈B,xj ∈X s(b i , x j ) y i -y j 2 (20)</formula><p>where λ is the trade-off parameter. While the first term in Eq.( <ref type="formula">20</ref>) guarantees smoothness of embeddings within B, the second term makes sure the embeddings are smooth between B and X.</p><p>Substituting Eq.( <ref type="formula" target="#formula_23">18</ref>) into Eq.( <ref type="formula">20</ref>), we have min</p><formula xml:id="formula_27">Y B T r(Y T B L B Y B ) + λT r(Y T B (D BX -W T XB S XB )Y B ),<label>(21)</label></formula><p>where T r(•) is the trace of a matrix. L B is the Laplacian matrix over B. W BX is the affinity matrix between B and X. D BX = diag(W BX 1).</p><p>Denoting M = L B +λ(D BX -W T XB S XB ) and adding an orthogonal constraint to avoid trivial solution, we transform the above problem to the following form:</p><formula xml:id="formula_28">min Y B T r(Y T B M Y B ), s.t. Y T B Y B = I.<label>(22)</label></formula><p>which can be easily solved by performing eigendecomposition on M and identifying the top r eigenvector of M . r is the number of bits. Finally, we summarize the procedure for encoding image tag vectors into binary representation in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed framework in terms of its ability in enhancing representative power of image tags as well as its efficacy in hashingbased image retrieval. Eq.( <ref type="formula">20</ref>); 4: Transform X into its corresponding embedding matrix using Eq.( <ref type="formula" target="#formula_23">18</ref>); 5: Binarize Y = sgn(Y ); 6: return Y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Experimental Settings</head><p>We used two large-scale real-world datasets for evaluation. The first one is Clickture, which is one of the largest click log datasets from commercial image search engine. As introduce in Section 3, Clickture records user search behaviors in forms of triad, namely, &lt; query, image, number of clicks &gt; .</p><p>Clickture contains 212.3 million triads, 40 million unique image keys and 73.6 million unique queries.</p><p>For image retrieval evaluation, we employ NUS-WIDE <ref type="bibr" target="#b41">[42]</ref>, which is one of the largest real-world image databases with manual labels. In consists of 269,648 Flickr images annotated with 81 concepts. We randomly selected 10% test samples from each concept as query examples to search the results in training samples. We extracted 4096D DeCAF <ref type="bibr" target="#b42">[43]</ref> generic feature, which is the activations of the 6-th layer of a deep CNN trained in a fully supervised fashion on ImageNet. It has shown that this feature is very effective for multimedia tasks on various benchmark data sets. All features are 2 -normalized.</p><p>For evaluation metrics, we chose commonly used mAP (mean Average Precision) over all concepts for visual annotation evaluation. For visual retrieval, we adopt Average Precision at top K retrieved results (AP@K), which is defined as</p><formula xml:id="formula_29">AP@K = 1 min(R, K) K j=1 R j j × I j ,<label>(23)</label></formula><p>where R is the number of relevant visual samples within the dataset and R j is the number of relevant visual samples among top j search results. I j is set to 1 if the j-th visual sample is relevant, and 0 otherwise. We averaged AP@K over all queries to obtain mAP@K as overall evaluation metric, where K ∈ {1, 2, . . . , 100}.</p><p>All comparison hashing methods use the number of hash bits in the range of {16, 32, 64, 128}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment I: Modelling Conceptual Relationships</head><p>In this part, we test the effects of 8 designed conceptual features, namely, ED, EMD, NGD, ES, CK, JK, CH and JH. We evaluate the classification performance (Accuracy) of each individual feature as well as that of their combination. We use Clickture-Lite to Filter out queries with no less than 50 clicked images and form a candidate concept list. We select 423 common concepts from the candidate list as concept set. Eventually, the concept database have 423 different concepts, 0.231 million unique images and 3.21 million repeatable images. We adopt this approach as aforementioned to construct a training data set. The data set is evenly split into nine folds, each of which contains the five specific relationships. We choose one fold to tune the parameters of multiple classifier SVM. Among the remaining eight folds, we select four folds to train the model and the rest to evaluate the performance. We switch the train folds and the test folds to compute the mean performance.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the performance of the different conceptual features and their combination. It shows that these reasons are not the major factor of distinguishing different concepts. Furthermore, we illustrate the effectiveness of conceptual-semantic model to classify the concept relationship in a real-world scenario. In Table <ref type="table" target="#tab_2">1</ref>, we illustrate some of the derived conceptual semantic relationships. In the subjective evaluation, for each specific relationship of 20 concepts, we select the top 10 concept pairs of SVM scores. We present them to 6 independent users. For each concept pair, the user is required to give a score 0 or 1.1 indicates that the concept pair has the specific relationship,otherwise 0. By averaging these scores form all the users, we get the final score for each of concept pairs. If the score is greater than 0.5, the concept pair has the specific relationship. In the end, we compute the precision@10 for each specific relationship, which is shown in Table <ref type="table" target="#tab_3">2</ref>. We can conclude that • Complete similarity relationship (CS) obtains 98.73% precision. For such high precision, a mass of complete similarity concept pairs exist in the concept set. • similarity relationship (TS) obtains only 28.73% precision. For such low precision, the of concept pairs with this relationship is far less than this of CS concept pairs. For example, Barack Obama does not have a concept with TS. For this reason, It has a bigger possible to find a wrong concept.</p><p>• We obtain a more consistent and robust performance in this limited concept set(about 7370 test concepts). It demonstrates that our approach is effective to find these relationships between two concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment II: Image Retrieval on NUS-WIDE</head><p>In this part, we evaluate the effectiveness of our proposed method. We compare to state-of-the-art hashing approaches as well as test the effects of image augmentation with visual semantic relationships. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison Algorithms</head><p>We compare our proposed framework with other state-of-the-art methods in terms of image retrieval performance. Specially, the following methods are employed:</p><p>• PCAH <ref type="bibr" target="#b19">[20]</ref>, which learns hash functions using principle projections.</p><p>• SH <ref type="bibr" target="#b17">[18]</ref>, which is based on the classic spectral method to find the best binary codes.</p><p>• AGH <ref type="bibr" target="#b20">[21]</ref>, which automatically discovers the neighborhood structure inherent in the data to learn appropriate compact codes.</p><p>• ITQ <ref type="bibr" target="#b43">[44]</ref>, which tries to find a rotation of zerocentered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Figure <ref type="figure" target="#fig_2">2</ref>.(a)-(d) illustrates MAP@K performance of PCAH, SH, ITQ, AGH and our method w.r.t. different bits of hash codes. As we can see, our proposed method can consistently achieve the best performance among all comparison methods with different #bits. This phenomenon indicates that the proposed visual semantic relationships component can work more smoothly with the employ hashing technique as compared to other hashing approaches. Figure <ref type="figure" target="#fig_5">3</ref> also shows the detailed performance of each individual concept (AP@20), we choose to use the best results of different numbers of bits for all comparison algorithms. As we can see, in most cases, our method can achieve better performance than other methods, which further proves that our proposed method is superior with the sophisticated hashing techniques as well as the augmented image tags.</p><p>Furthermore, in order to obtain more detailed insights of the effects of visual semantic relationships, we perform an additional experiment. We compare our proposed method to its variant without augmentation using visual semantic relationships. The results are shown in Figure <ref type="figure">4</ref>. As can be seen, for all different numbers of hash codes, if we use visual semantic relationships to enhance image tags, we can always obtain better performance that without using visual semantic relationships. This reveals that the relationships mined from the click log indeed helps to provide positive influence on the final performance. Besides, 2332-7790 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. we can see that as the number of hash codes increases from 16 to 128, the performance first increases then sees a dropping trend. One possible reason is that as the number increases to 64, the information can be better preserved; however, as the number continues to rise, more noise may be introduced in the binary representation, thereby downgrading the performance. All the above observations and analysis hint us that we should carefully decide the length of hash codes in order to obtain the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a new approach to establish the conceptual relationships based on the large scale real-world click data from image commercial engine. We handled the problem that the click data suffers from the noise such as typos, the same concept with different queries, etc. In order to automatically establish the concept relationship. We first defined five types of specific relationships between concepts. We then extracted some concept relationship features in textual and visual domain to train the concept relationship models. The relationship of each pair of concepts will thus be classified into one of the five special relationships. We further studied the efficacy of the conceptual relationships by applying them to augment imperfect image tags, i.e., improve representative power. By using a sophisticated hashing approach, we transformed augmented image tags into binary codes, which are subsequently used for content-based image retrieval task. Experimental results on NUS-WIDE dataset demonstrated the superiority of our proposed approach as compared to state-of-the-art methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm for Augmenting Image Tags with Visual Semantic Relationships Input: An image's tag set T , conceptual relationships R and concept set C; Output: Augmented tag set T ; 1: repeat 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>11 :</head><label>11</label><figDesc>until there is no change to T 12: return Y It has been shown that the binary codes can help to support large-scale image search with reasonable performance. Therefore, we propose to use inductive manifold hashing to achieve this goal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Learning Binary Codes for Image Database Input: Augmented image tag matrix X, code length r, the number of bases o; Output: Binary codes Y ; 1: Cluster X into o groups and use o centers to form bases B; 2: Compute M = L B + λ(D BX -W T XB S XB ); 3: Embed B into low-dimensional space by solving</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Performance of different features and their combination: (a) Accuracy, (b) Precision and (c) Recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :Fig. 4 :</head><label>24</label><figDesc>Fig.2: Performance (MAP@K) of all comparison algorithm. K = 1, 2, . . . , 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Detailed performance (mAP@20) of individual concepts of all comparison algorithms NUS-WIDE datasets. The best results over different numbers of bits are used for each individual method.</figDesc><graphic coords="10,302.10,221.21,67.84,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>•</head><label></label><figDesc>The overall conceptual features obtains about 92.78% mean precision and 43.44% mean recall relative improvements compared to the best in eight concept features. It shows that the com-</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Precision@10 of five specific relationships.</figDesc><table><row><cell>Relationships</cell><cell>CS</cell><cell>TS</cell><cell>HH</cell><cell>PR</cell><cell>UR</cell></row><row><cell>Precision@10</cell><cell>0.9873</cell><cell>0.2073</cell><cell>0.5478</cell><cell>0.3298</cell><cell>0.9765</cell></row><row><cell cols="6">prehensive conceptual features are necessary to</cell></row><row><cell cols="6">measure the complex conceptual-semantic rela-</cell></row><row><cell>tionships.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">• The complete similarity (CS) relationship obtains</cell></row><row><cell cols="6">99.22% accuracy, 91.14% precision and 92.27%</cell></row><row><cell cols="6">recall. It shows that CS can be captured very</cell></row><row><cell cols="6">easily, because these two concepts share a mass</cell></row><row><cell cols="3">of the same images.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">• The Earth Mover's distance obtains the lowest</cell></row><row><cell cols="6">performance. The EMD in computer science is to</cell></row><row><cell cols="6">compare two gray-scale images that may differ</cell></row><row><cell cols="6">due to dithering, blurring, or local deformations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Illustration of derived concept relationships.</figDesc><table><row><cell>Query</cell><cell>Relevant Concept #1</cell><cell>Relevant Concept #2</cell></row><row><cell>Alaskan Husky</cell><cell>Husky</cell><cell>Husky Puppies</cell></row><row><cell>Barack Obama</cell><cell>Pictures of Obama</cell><cell>Obama</cell></row><row><cell>Black Cat</cell><cell>Black Cats</cell><cell>Cat Drawings</cell></row><row><cell>Bulldog</cell><cell>Bull Dog</cell><cell>Bulldog Pictures</cell></row><row><cell>Coast</cell><cell>Pics of Beaches</cell><cell>Images of The Beach</cell></row><row><cell>Justin</cell><cell>Justinbieber</cell><cell>Justin Bieber</cell></row><row><cell>Gray Wolf</cell><cell>Wolf Pictures</cell><cell>Pictures of Wolfs</cell></row><row><cell>Laptop</cell><cell>Computers</cell><cell>Computer Pics</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Annotation of paintings with high-level semantic concepts using transductive inference and ontology-based concept disambiguation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale duplicate detection for web image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="353" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Routledge dictionary of language and linguistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bussmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flickr distance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view intact space learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2531" to="2544" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-margin multiviewinformation bottleneck</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">General tensor discriminant analysis and gabor features for gait recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1700" to="1715" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Manifold rankingbased matrix factorization for saliency detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal component 2-d long short-term memory for font recognition on single chinese characters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multifold concept relationships metrics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustering by compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vitanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TOIT</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1523" to="1545" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sowa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Cognitive Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 21</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large scale search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hash functions using column generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Intermedia hashing for large-scale retrieval from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse hashing for fast multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Localitysensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Computational Geometry</title>
		<meeting>International Symposium on Computational Geometry</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast similarity search for learned metrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2143" to="2157" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Isotropic hashing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LDAHash: Improved matching with smaller descriptors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning hash codes with listwise supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast supervised hashing with decision trees for high-dimensional data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random maximum margin hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Discrete graph hashing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inductive hashing on manifolds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hashing on nonlinear manifolds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1839" to="1851" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient nonparametric function induction in semi-supervised learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Artif. Intelli. Stat</title>
		<meeting>Int. Workshop Artif. Intelli. Stat</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<editor>CVIR</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
