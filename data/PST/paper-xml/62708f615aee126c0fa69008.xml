<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-30">30 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chin-Lun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zih-Ching</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun-Ru</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<email>hungyilee@ntu.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-30">30 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.00305v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pretrained models. We further find that Adapter-Bias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While large pre-trained language models (PLMs) reached state-of-the-art results on natural language processing (NLP) tasks, PLMs require updating all parameters and storing the fully fine-tuned model for each downstream task. These requirements have led to difficulties in real-world applications. Moreover, fine-tuning PLMs on lowresource datasets is subject to instabilities.</p><p>To tackle these shortcomings, Adapters <ref type="bibr" target="#b11">(Houlsby et al., 2019)</ref>, a more parameter-efficient alternative training strategy for the transformer architecture <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> have been proposed. Instead of full fine-tuning the whole model, Adapters introduce extra tunable weights and freeze the original parameters of PLM. Adapters demonstrated comparable performance with fully fine-tuning the Figure <ref type="figure">1</ref>: Overview of the main concept of our work compared to BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref>. Left: Bit-Fit tends to add the same representation shift to different tokens. Right: Our work applies different representation shifts to tokens considering their importance to the downstream task and their characteristics. The shifts of the input words that are more task-related is more significant than that of other tokens. For example, in SST-2 <ref type="bibr" target="#b28">(Socher et al., 2013)</ref>, which is a semantic task, the representation shifts of the semantic words, such as "kind" and "worse", are larger than that of other words. entire model. Although Adapters solve the problem of the PLM's massive parameters, researchers are curious about how many more parameters are required to reach state-of-the-art performance on standard NLP tasks. The results in <ref type="bibr" target="#b11">Houlsby et al. (2019)</ref> have shown that the performance on GLUE benchmark <ref type="bibr" target="#b33">(Wang et al., 2018)</ref> is almost the same when removing the Adapters in the lower layers, which indicates that not every adapter is useful. It raises the question of whether adapters can be even more parameter-efficient.</p><p>To develop practical and memory-efficient methods of utilizing PLMs, Diff pruning <ref type="bibr" target="#b10">(Guo et al., 2020)</ref> enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific "diff" vector that extends the original pre-trained parameters and encourages the sparsity of the vector through L 0 -norm regularization. Another approach is BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref>, which shows that with small-to-medium training data, fine-tuning only a subset of the bias terms of pre-trained BERT models <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> is competitive with fine-tuning the entire model. The central concept of these approaches is to add task-specific shifts to each output representation of the PLM layers so as to adapt to different tasks. In the previous works, Ben <ref type="bibr" target="#b3">Zaken et al. (2021)</ref>; <ref type="bibr" target="#b10">Guo et al. (2020)</ref> both add the same shifts to the output representation regardless of which token is more relevant to the task. However, considering some specific tokens might be more critical to a particular task, the representation can better adapt to the downstream task under a limited amount of parameters if these shifts are based on the input tokens.</p><p>Based on this concept, in this study, we add token-dependent biases to the shifts by proposing AdapterBias, which consists of a vector and a linear layer (L ? ). The vector represents the task-specific shift, and L ? produces the weights for input tokens. Thus, with the vector and the weights, AdapterBias can add a token-dependent shift to the transformer layer. Since the concept of BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref> is similar to AdapterBias by adding a shift to the representation, we demonstrate the difference between BitFit and AdapterBias in Figure <ref type="figure">1</ref>. Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the representations that are related to the task.</p><p>With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with <ref type="bibr" target="#b11">Houlsby et al. (2019)</ref>; <ref type="bibr">Pfeiffer et al. (2020a)</ref>; <ref type="bibr" target="#b10">Guo et al. (2020);</ref><ref type="bibr" target="#b3">Ben Zaken et al. (2021);</ref><ref type="bibr" target="#b12">Hu et al. (2021)</ref>. We further decrease the parameters of AdapterBias in different ways, including partial weight-sharing in AdapterBias and adding L 0 -norm regularization. Finally, Adapter-Bias has better interpretability due to its simplicity. We use different tools, including word cloud and PCA <ref type="bibr" target="#b15">(Jolliffe, 2002)</ref>, to visualize what Adapter-Bias has learned, and we found that the proposed approach automatically learns to assign larger representation shifts to the task-related tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For NLP tasks, adapters are introduced for the transformer architecture. A set of adapter parameters was added at each transformer layer, which is mostly bottleneck architectures <ref type="bibr" target="#b11">Houlsby et al. (2019)</ref>. By keeping the output dimension identical, they cause no change to the structure or parameters of the original model.</p><p>Adapters quickly gained popularity in NLP with various applications. For multi-task learning <ref type="bibr" target="#b5">(Caruana, 1997;</ref><ref type="bibr" target="#b37">Zhang and Yang, 2017;</ref><ref type="bibr">Liu et al., 2019b)</ref>, a projected self-attention layer is proposed by <ref type="bibr" target="#b29">Stickland and</ref><ref type="bibr">Murray (2019), while Bapna et al. (2019)</ref> proposed an additional layer norm suitable for machine translation.</p><p>Besides the applications of adapters, researchers are also dedicated to improving their performance. Based on the architecture introduced by <ref type="bibr" target="#b11">Houlsby et al. (2019)</ref>, AdapterFusion <ref type="bibr">(Pfeiffer et al., 2020a)</ref> leveraged knowledge from multiple tasks with a new two-stage learning algorithm. Despite the recent popularity of these methods, they still train a relatively large number of training parameters.</p><p>Recently, studies start to focus on improving the parameter-efficiency of adaptation to a new task. Diff-pruning <ref type="bibr" target="#b10">(Guo et al., 2020)</ref> achieves parameter efficiency by adding a sparse, task-specific difference-vector to the fixed original parameters. The vector is adaptively pruned during training with a differentiable approximation to the L 0 -norm penalty to encourage sparsity. <ref type="bibr" target="#b27">R?ckl? et al. (2020)</ref> introduced AdapterDrop, which has been recently integrated into AdapterHub <ref type="bibr">(Pfeiffer et al., 2020b)</ref>. It removes adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost. <ref type="bibr" target="#b23">Mahabadi et al. (2021)</ref> proposed Compacter, which improved the trade-off between performance and trainable parameters per task with low-rank optimization.</p><p>On the other hand, without modifying the architecture of the PLM, BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref> shows that fine-tuning only the bias terms of a large PLM is also competitive with fine-tuning the entire model. Fine-tuning only the bias terms can be considered as adding a task-specific shift to the token representation. BitFit is most similar to our work. While in BitFit, the shifts added to all the representations are exactly the same for all input tokens, in our work, the shifts are token-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present AdapterBias, an efficient way to adapt large-scale PLMs. In order to better adapt to different downstream tasks, the adapter module should be token-specific. AdapterBias produces a suitable weight for the bias based on the input token. Problem Formulation We consider the general problem of fine-tuning PLMs, where the training data D = (x i , y i ) N n=1 is given. Assume that given a PLM with parameters ? and AdapterBias with parameters ? . During the training stage, we freeze ? and tune ? only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AdapterBias</head><p>The architecture of AdapterBias is shown in the right part of Figure <ref type="figure" target="#fig_0">2</ref>. AdapterBias consists of two modules: a vector (v) and a linear layer (L ? ). v is a task-specific shift added to the output of each transformer layer. The tokens which are more related to the task should be assigned larger representation shifts than other tokens. The linear layer (L ? ) produces a token-dependent weight vector ? = [? 1 , ? 2 . . . ? m ] T , where ? i is the weight of the i th token's representation shift. By applying the token-specific weight to the task-specific representation shift (v), AdapterBias can focus on the tokens that are more important to the task and is able to adapt to different downstream tasks efficiently.</p><p>We define the output of AdapterBias as the bias (B), which is the outer product of v and the learned weights vector ?. When the dimension of the token's representation is r with m input tokens, the function can be defined as follows:</p><formula xml:id="formula_0">B = v ? ? T = ? 1 v ? 2 v . . . ? m v (1) where v ? R r , ? ? R m , and B ? R r?m .</formula><p>To further elaborate on the details of Adapter-Bias, we give an example of how AdapterBias produces B and how B adds to the transformer layer. In Figure <ref type="figure" target="#fig_1">3</ref>, we assume that there are three representation outputs (r 1 , r 2 , r 3 ) after the first layer normalization. The dimension of r 1 , r 2 and r 3 is the dimension of the 2 nd feedforward layer, while the input dimension of the linear layer (L ? ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs. The linear layer (L ? ) produces ?, where ? ? R 3 . The blocks in different colors represent the difference of the weights (? 1 , ? 2 , ? 3 ). Take BERT-base for example, after performing outer product with the weights vector ? and the vector (v), the dimension of B becomes 768 ? 3. For example, b 1 , the first column of B, is the shift for the first token representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Further improvement on parameter-efficiency of AdapterBias</head><p>In this section, we experiment on two different methods to make AdapterBias more parameter efficient. One is partial weight-sharing of AdapterBias among transformer layers, another is enforcing the weights of the linear layer (L ? ) to be sparse by utilizing L 0 -norm penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cross-layer parameters sharing in AdapterBias</head><p>Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important <ref type="bibr" target="#b11">(Houlsby et al., 2019)</ref>.</p><p>In addition, sharing parameters of the Adapter across layers leads to a comparatively small drop in performance in some tasks. In light of the above information, we further reduce the number of parameters required for each task by partially sharing the weights of the adapters across all transformer layers. The experimental results are discussed at Section 4.6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">L 0 regularization in AdapterBias</head><p>Sparsity has been utilized in various parameterefficient methods. For applications in NLP tasks, Diff-pruning <ref type="bibr" target="#b10">(Guo et al., 2020</ref>) learns a sparse vector added to the whole PLM with L 0 -norm penalty.</p><p>Inspired by their work, we further apply L 0 -norm regularization to L ? in the AdapterBias module, aiming to encourage the sparsity of L ? . We choose to drop L ? because it contributes most of the parameters in AdapterBias. Encouraging its sparsity can further increase the parameter efficiency. Note that we specifically apply L 0 regularization in Section 4.6.2.</p><p>In AdapterBias, we add L 0 -norm penalty to the linear layer (L ? ). The optimization problem can be expressed as,</p><formula xml:id="formula_1">min ? L(D; ?, ? ) + ? ? L? 0 ,<label>(2)</label></formula><p>where L(D; ?) represents the original loss with training data D. ? is the hyperparameter for L 0norm penalty. Note that ? represents trainable parameters and ? L? represents the parameters of L ? in AdapterBias. Following the work of Diffpruning, we utilize a relaxed mask vector <ref type="bibr" target="#b21">(Louizos et al., 2017)</ref> with a stretched Hard-Concrete distribution <ref type="bibr" target="#b13">(Jang et al., 2016;</ref><ref type="bibr" target="#b22">Maddison et al., 2016)</ref> to encourage L 0 sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness of our proposed adapter module in NLP training tasks, and provide the analysis of what AdapterBias has learned in different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>We base our experiments on HuggingFace PyTorch implementation <ref type="bibr" target="#b36">(Wolf et al., 2019)</ref> of BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr">(Liu et al., 2019c</ref>)  <ref type="figure" target="#fig_0">2</ref>. We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server. We report the test metrics provided on the submission website<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on GLUE</head><p>In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters <ref type="bibr" target="#b11">(Houlsby et al., 2019)</ref>, Diff-pruning <ref type="bibr" target="#b10">(Guo et al., 2020)</ref>, <ref type="bibr">BitFit (Ben Zaken et al., 2021), and</ref><ref type="bibr">LoRA (Hu et al., 2021)</ref>. In Table <ref type="table" target="#tab_0">1</ref>, we report the test scores on the GLUE benchmark and the required new parameters per task. Here we use BERTlarge as the PLM. AdapterBias reaches 81.2 average score in GLUE benchmark, with the smallest amount of parameters (0.17M) added per task. AdapterBias shows competitive performance as its Here we experiment with four models : BERTbase (BB), BERT-large (BL), RoBERTa-base (RoB), and RoBERTa-large (RoL). The settings are the same as in Table <ref type="table" target="#tab_0">1</ref>. The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.</p><p>parameters are 40? less than the works of <ref type="bibr" target="#b11">Houlsby et al. (2019)</ref>. Although Diff-pruning <ref type="bibr" target="#b10">(Guo et al., 2020)</ref> achieves the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM. Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.17M parameters. Furthermore, AdapterBias achieves comparable performance with BitFit and LoRA with fewer parameters needed per task. This shows that AdapterBias is a worthwhile targeted fine-tuning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Different base models</head><p>To analyze the generalization ability of this approach to different PLMs on different models of AdapterBias, as shown in  show that AdapterBias has the ability to outperform fine-tuning the whole PLM with small-to-medium data size, similarly to BitFit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Investigation on the effectiveness of token dependent representation shift</head><p>Different from BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref>, where the bias terms in all transformer layers are tuned, we claim that the bias added to the representation should be token-dependent, and proposed AdapterBias based on this concept. We conduct ablation studies to verify this claim. In this experiment, the linear layer (L ? ) in AdapterBias that produces the token-dependent weights vector (?) is removed; that is, only the v is trained. All shifts added to the representation outputs are identical within the same transformer layer. The experiments are conducted with BERT-base model. We report the test scores on the GLUE benchmark in Table <ref type="table" target="#tab_3">3</ref>. The performance of AdapterBias without the linear layer (L ? ) dramatically decreases. Without L ? , it is hard for the vector (v) to adapt to different downstream tasks. This result demonstrates the importance of L ? . In other words, assigning different shifts to different token representations improves the performance of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Improving the parameter efficiency of AdapterBias</head><p>We further apply two additional methods to AdapterBias to enhance its parameter efficiency.</p><p>Experiments are conducted to examine whether AdapterBias can be more parameter-efficient by sharing its components across all layers. Moreover, we experiment on adding L 0 -norm regularization during the training stage to encourage the sparsity of AdapterBias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Sharing components in AdapterBias</head><p>In this experiment, we conduct an ablation study of partial weight-sharing in the AdapterBias module. In Table <ref type="table">4</ref>, we share components of Adapter-Bias among different transformer layers. Share v represents sharing v across all transformer layers, while Share L ? means sharing the linear layer (L ? ). Share v+L ? denotes sharing one Adapter-Bias across all transformer layers. As can be seen in Table <ref type="table">4</ref>, the performance of Share L ? stands out among other partial weight-sharing methods, while Share v leads to a poor performance.</p><p>From the experiments above, we conclude that the linear layer (L ? ) captures general task information by learning the weights of the bias for different tokens. Thus, sharing L ? across all layers results in better performance compared to other components. The vector module (v) in AdapterBias aims to learn local information in each transformer layer. If v among different transformer layers are shared, the performance drops dramatically. This might be due to a failure of v to learn general information which can be adapted to each individual transformer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">L 0 -norm regularization in AdapterBias</head><p>We observed that many of the trained parameters in L ? have values that are extremely close to zero after tuning on downstream tasks, which might cause redundancy of the parameters. To further encourage the sparsity of AdapterBias, we add L 0norm regularization to L ? during the training stage.</p><p>Table <ref type="table">5</ref>: Performance of our AdapterBias with L 0 -norm regularization. Here we experiment with two models: BERT-base (BB), and BERT-large (BL). The settings are the same as in Table <ref type="table" target="#tab_0">1</ref>. The Full-FT represents finetuning the whole PLM without adding adapters.</p><p>In Table <ref type="table">5</ref>, we use BERT-base (BB) and BERTlarge (BL) as the PLMs. We compare the performance of fine-tuning, the original AdapterBias, and the one trained with L 0 -norm regularization. The experiment shows that adding L 0 -norm regularization during the training step improves the performance on 7 out of 9 tasks in BERT-base models. However, the performance did not improve when applied to BERT-large models. As for the parameter efficiency of applying L 0 -norm penalty, the linear layer (L ? ) with L 0 -norm penalty saves about 17% parameter on average compared to the original AdapterBias. The details of the reduced parameters of each task are shown in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">What AdapterBias learns</head><p>AdapterBias has good interpretability due to its simplicity. Compared to the similar work <ref type="bibr">Bit-Fit (Ben Zaken et al., 2021)</ref>, where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation. By observing these token-dependent shifts, we analyze what AdapterBias learns when adapting to downstream tasks.  sentation shifts to the transformer layers through task-specific fine-tuning.</p><p>In AdapterBias, the linear layer (L ? ) produces a weights vector ? for representation shifts, therefore, the average absolute value of vector ? can give us a look at the shifting amount in the transformer layers when adapting to downstream tasks. In Figure <ref type="figure">5</ref>, the layers are ordered from lower to upper. From the experimental result, we find that the weight in each layer is considerably different in different tasks in general.</p><p>CoLA <ref type="bibr" target="#b34">(Warstadt et al., 2019)</ref> is a syntactic task that consists of English acceptability judgments in the GLUE benchmark. As shown in Figure <ref type="figure">5</ref>, its average shift at the ninth layer is the highest among all layers, which is quite different from the others. We speculate that the ninth layer has the ability to the syntactic information, leading AdapterBias to add the largest shift in this layer. Our experiment has a similar observation with the work of <ref type="bibr" target="#b14">Jawahar et al. (2019)</ref>. They observe on a syntactic task with BShift <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref> that the ninth layer of BERT embeds a rich hierarchy of syntactic information. <ref type="bibr" target="#b14">(Jawahar et al., 2019)</ref> Moreover, we observe similar distributions between specific tasks. For instance, RTE <ref type="bibr" target="#b9">(Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b4">Bentivogli et al., 2009)</ref> and MNLI <ref type="bibr" target="#b35">(Williams et al., 2017)</ref>, where both recognize textual entailment, have higher values in the upper layers than the lower ones.</p><p>Based on these findings, we find that Adapter-Bias assigns suitable representation shifts in different tasks. For tasks with similar objectives, AdapterBias tends to add similar representation shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Which kind of word does L ? focus on</head><p>Since ? i represents the weight of the representation shift for i th token in a transformer layer, we can observe the significance of i th token from the summation of ? i in all the transformer layers. Special tokens, including [CLS], [SEP], and [PAD], are not included for analysis. We use the validation sets of CoLA and SST-2, and word cloud is used for visualizations.</p><p>In Figure <ref type="figure" target="#fig_4">6</ref>, we visualize all words in the validation data of CoLA. The result shows that Adapter-Bias focuses more on reflexive pronouns, such as yourself, himself, and myself. This is because there are many incorrect sentences with misused reflexive pronouns, such as "He washed yourself."</p><p>In Figure <ref type="figure" target="#fig_5">7</ref>, we visualize all words in the valida- The visualization approach is same as in Figure <ref type="figure" target="#fig_4">6</ref>.</p><p>tion of SST-2. The result shows that Adapter-Bias focuses more on adjectives, such as "bad", "awful", and "worst". SST-2 is a binary sentiment analysis dataset, which classifies movie reviews into positive and negative classes. AdapterBias learns that adjectives often constitute a crucial factor in sentiment analysis during tuning, and adds larger shifts to these adjective tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we present AdapterBias. By adding token-dependent representation shifts to the PLM, AdapterBias shows competitive results even though it uses far fewer parameters than the existing methods. Through extensive experiments, not only does AdapterBias reach competitive results on the GLUE benchmark, but also obtain good performance on small-to-medium datasets. A.2 L 0 -norm regularization in AdapterBias</p><p>In Table <ref type="table" target="#tab_6">B</ref>, we report the remaining parameters of utilizing L 0 -norm regularization compared with the original AdapterBias. BERT-base (BB) and BERT-large (BL) are used as PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 The direction of representation shifts in different tasks</head><p>Different from BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2021)</ref>, where all the representation shifts are identical within one task, AdapterBias produces different weights for the shift based on each token. In this section, we compare the transformed tokens in AdapterBias and BitFit. We utilize PCA <ref type="bibr" target="#b15">(Jolliffe, 2002)</ref> to reduce the dimension of the vectors. In Figure <ref type="figure" target="#fig_6">A</ref>, we input five sentences from the evaluation set of SST-2. We experiment on the last transformer layer since it has the most obvious shifts compared to the previous layers. '0' with lighter color indicates the representation before shifting, which is the output of the first layer normalization. '1' with darker color is the shifted representation, which is the output of the second layer normalization. The color red represents positive sentences, and blue are the negative ones.</p><p>The result shows that BitFit shifts all tokens towards the same direction regardless of the groundtruth label. On the other hand, AdapterBias discerns the label of the sentences and thus shifts the tokens of different sentences toward different directions.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architectures comparison of Houlsby et al. (2019), BitFit (Ben Zaken et al., 2021), and the proposed method AdapterBias. The orange blocks indicate the trainable parts, while the gray blocks indicate the frozen parameters during the training stage. Left: Houlsby et al. (2019) add their Adapters after the feed-forward layers, and their Adapter consists of two linear layers and an active function. Middle: BitFit tunes all biases from the original transformer layers. Right: AdapterBias, consisting of a linear layer (L ? ) and a vector (v), is added after the second feed-forward layer only in each transformer layer.</figDesc><graphic url="image-2.png" coords="3,93.55,70.87,408.19,213.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The detailed architecture of how AdapterBias produces the bias (B) and how B is added to the output of transformer layers.</figDesc><graphic url="image-3.png" coords="4,329.10,70.87,172.35,250.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of Finetune, BitFit (Ben Zaken et al., 2021), and AdapterBias with BERT-base on SQuAD validation set. The x-axis represents the total number of training examples while the y-axis represents the exact match score.</figDesc><graphic url="image-4.png" coords="6,70.87,165.04,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5: We analyze the average absolute value of weights vector ?, the output of the linear layer (L ? ), in each layer for different tasks. The y-axis represents the index of transformer layers, ordered from earlier to later (i.e. the embedding layer is shown at the top). The x-axis represents the average absolute value of ?.</figDesc><graphic url="image-6.png" coords="7,313.23,598.58,204.09,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Word cloud of CoLA, a corpus of linguistic acceptability. We utilize BERT-base model as the PLM and words come from validation data. The weights of the words are the summation of their weights produced by the linear layer (L ? ) in twelve transformer layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Word cloud of SST-2, a corpus of movie reviews categorized in two sentimental classes (i.e. positive, negative). The visualization approach is same as in Figure6.</figDesc><graphic url="image-7.png" coords="8,313.23,70.87,204.09,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A :</head><label>A</label><figDesc>Figure A: We utilize PCA (Jolliffe, 2002) to visualize the shifting difference between Bitfit (Ben Zaken et al., 2021) and AdapterBias on SST-2 validation set. '0' with light color means the embedding before shifting. '1' with dark color means the embedding after shifting. The color red represents positive sentences, and blue represents negative sentences.</figDesc><graphic url="image-8.png" coords="11,306.14,317.44,226.76,114.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure B :</head><label>B</label><figDesc>Figure B: Word cloud of SST-2 in layer 0 to layer 6.</figDesc><graphic url="image-9.png" coords="13,89.29,91.93,181.41,636.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure C :</head><label>C</label><figDesc>Figure C: Word cloud of SST-2 in layer 7 to layer 12.</figDesc><graphic url="image-10.png" coords="13,324.57,91.93,181.41,636.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure D :</head><label>D</label><figDesc>Figure D: Word cloud of CoLA in layer 0 to layer 6.</figDesc><graphic url="image-11.png" coords="14,89.29,91.93,181.41,636.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure E :</head><label>E</label><figDesc>Figure E: Word cloud of CoLA in layer 7 to layer 12.</figDesc><graphic url="image-12.png" coords="14,324.57,91.93,181.41,636.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of all methods on the GLUE testing sets scored by the GLUE evaluation server. For each method, we report the new adding parameters per task. For QQP, we report the F1 score. For STS-B<ref type="bibr" target="#b6">(Cer et al., 2017)</ref>, we report Spearman correlation coefficients. For CoLA<ref type="bibr" target="#b34">(Warstadt et al., 2019)</ref>, we report Matthews correlation. For all other tasks, we report accuracy. Bold fonts indicate the least trainable parameter per task. The first row (BERT LARGE ) represents fine-tuning the whole BERT-large model without adding new parameters. The results of baselines including<ref type="bibr" target="#b11">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b10">Guo et al., 2020;</ref><ref type="bibr" target="#b3">Ben Zaken et al., 2021)</ref> are their reported performance and Pfeiffer et al. (2020a); Hu et al. (2021) performance is reproduced on our setting. Due to instability during training, we restart experiments with 3 random seeds and report the best.</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell cols="9">Params CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Avg</cell></row><row><cell></cell><cell>BERT LARGE</cell><cell></cell><cell>340M</cell><cell>60.5</cell><cell>94.9</cell><cell>89.3</cell><cell cols="2">92.7 70.1 87.6</cell><cell>86.7</cell><cell>85.9</cell><cell>72.1 82.2</cell></row><row><cell cols="4">Adapters (Houlsby et al., 2019) 7.14M</cell><cell>56.9</cell><cell>94.2</cell><cell>89.6</cell><cell cols="2">91.4 68.8 87.3</cell><cell>85.3</cell><cell>84.6</cell><cell>71.8 81.1</cell></row><row><cell cols="4">Diff-Pruning (Guo et al., 2020) 1.7M</cell><cell>61.1</cell><cell>94.1</cell><cell>89.7</cell><cell cols="2">93.3 70.6 86.0</cell><cell>86.4</cell><cell>86.0</cell><cell>71.1 82.0</cell></row><row><cell cols="4">BitFit (Ben Zaken et al., 2021) 0.27M</cell><cell>59.7</cell><cell>94.1</cell><cell>88.9</cell><cell cols="2">92.0 72.0 85.5</cell><cell>84.5</cell><cell>84.8</cell><cell>70.5 81.3</cell></row><row><cell cols="3">LoRA (Hu et al., 2021)</cell><cell>0.39M</cell><cell>60.6</cell><cell>94.0</cell><cell>87.9</cell><cell cols="2">92.2 70.3 85.6</cell><cell>84.2</cell><cell>84.0</cell><cell>70.0 81.0</cell></row><row><cell></cell><cell>AdapterBias</cell><cell></cell><cell>0.17M</cell><cell>60.0</cell><cell>94.4</cell><cell>88.2</cell><cell cols="2">91.2 70.5 87.5</cell><cell>84.3</cell><cell>83.9</cell><cell>70.5 81.2</cell></row><row><cell></cell><cell>Method</cell><cell cols="10">Params CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Avg</cell></row><row><cell>BB</cell><cell>Full-FT</cell><cell>110M</cell><cell>52.1</cell><cell>93.5</cell><cell>88.9</cell><cell>90.5</cell><cell>66.4</cell><cell>85.8</cell><cell>84.6</cell><cell>83.4</cell><cell>71.2 79.6</cell></row><row><cell>BB</cell><cell>BitFit</cell><cell>0.10M</cell><cell>47.2</cell><cell>92.4</cell><cell>87.4</cell><cell>89.7</cell><cell>65.5</cell><cell>87.6</cell><cell>80.8</cell><cell>80.9</cell><cell>67.8 77.7</cell></row><row><cell cols="3">BB AdapterBias 0.06M</cell><cell>51.6</cell><cell>93.1</cell><cell>87.5</cell><cell>89.4</cell><cell>66.1</cell><cell>84.6</cell><cell>80.9</cell><cell>80.5</cell><cell>67.9 78.0</cell></row><row><cell>BL</cell><cell>Full-FT</cell><cell>340M</cell><cell>60.5</cell><cell>94.9</cell><cell>89.3</cell><cell>92.7</cell><cell>70.1</cell><cell>87.6</cell><cell>86.7</cell><cell>85.9</cell><cell>72.1 82.2</cell></row><row><cell>BL</cell><cell>BitFit</cell><cell>0.27M</cell><cell>62.0</cell><cell>93.1</cell><cell>86.8</cell><cell>89.8</cell><cell>66.6</cell><cell>87.2</cell><cell>84.1</cell><cell>84.3</cell><cell>67.2 80.1</cell></row><row><cell cols="3">BL AdapterBias 0.17M</cell><cell>60.0</cell><cell>94.4</cell><cell>88.2</cell><cell>91.2</cell><cell>70.5</cell><cell>87.5</cell><cell>84.3</cell><cell>83.9</cell><cell>70.5 81.2</cell></row><row><cell>RoB</cell><cell>Full-FT</cell><cell>125M</cell><cell>61.3</cell><cell>94.7</cell><cell>90.4</cell><cell>92.0</cell><cell>74.4</cell><cell>87.5</cell><cell>87.4</cell><cell>86.8</cell><cell>71.9 82.9</cell></row><row><cell>RoB</cell><cell>BitFit</cell><cell>0.10M</cell><cell>62.7</cell><cell>94.8</cell><cell>89.7</cell><cell>91.3</cell><cell>73.6</cell><cell>88.5</cell><cell>85.3</cell><cell>84.9</cell><cell>68.1 82.1</cell></row><row><cell cols="3">RoB AdapterBias 0.06M</cell><cell>61.9</cell><cell>94.5</cell><cell>90.2</cell><cell>91.1</cell><cell>74.1</cell><cell>88.7</cell><cell>85.3</cell><cell>85.1</cell><cell>70.5 82.4</cell></row><row><cell>RoL</cell><cell>Full-FT</cell><cell>355M</cell><cell>63.3</cell><cell>96.7</cell><cell>92.3</cell><cell>95.4</cell><cell>84.5</cell><cell>92.2</cell><cell>90.8</cell><cell>90.2</cell><cell>74.3 86.6</cell></row><row><cell>RoL</cell><cell>BitFit</cell><cell>0.26M</cell><cell>64.7</cell><cell>95.8</cell><cell>91.5</cell><cell>94.2</cell><cell>80.9</cell><cell>90.6</cell><cell>89</cell><cell>88.9</cell><cell>72.0 85.3</cell></row><row><cell cols="3">RoL AdapterBias 0.17M</cell><cell>63.9</cell><cell>96.4</cell><cell>90.4</cell><cell>94.7</cell><cell>83.6</cell><cell>91.3</cell><cell>89.8</cell><cell>89.4</cell><cell>72.3 85.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of AdapterBias adding in different PLMs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>4.4 Size of training data</cell></row><row><cell>In the previous experimental results, we observe</cell></row><row><cell>that AdapterBias tends to have higher performance</cell></row><row><cell>on tasks with a smaller amount of data (i.e. CoLA,</cell></row><row><cell>SST-2, and RTE). To further validate this obser-</cell></row><row><cell>vation, we follow the work of BitFit (Ben Zaken</cell></row><row><cell>et al., 2021) by training AdapterBias on subsets</cell></row><row><cell>of SQuAD v1.0 (Rajpurkar et al., 2016) of in-</cell></row><row><cell>creasing size. The experiments are conducted with</cell></row><row><cell>BERT-base. The results on the validation set of</cell></row></table><note><p><p><p><p><p><p><p>, we apply AdapterBias in different transformer-based PLMs, including BERT-base (BB), BERT-large (BL), RoBERTa-base (RoB), and RoBERTa-large (RoL), on the GLUE benchmark. All results are scored by the GLUE evaluation server. Compared with BitFit, In Table</p>2</p>, not only can AdapterBias perform well on BERT but also achieve competitive performance on larger PLMs such as RoBERTa. the SQuAD dataset are listed in Figure</p>4</p>, which shows the tendency of AdapterBias outperforming full fine-tuning when the size of the training dataset is smaller. However, with more training data available, the trend is reversed. The results</p>Method</p>Params CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Avg</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluating the importance of the linear layer (L ? ) in AdapterBias. The settings are the same as in Table1. The backbone model is BERT-base. w/o L ? means that there is only a vector (v) in AdapterBias.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>We train our model on Pytorch. The training details are shown in TableA. In addition, the bottleneck of Adapters<ref type="bibr" target="#b11">(Houlsby et al., 2019)</ref> and is 32.</figDesc><table><row><cell>A Appendix</cell></row><row><cell>A.1 Training Details</cell></row><row><cell>In addition,</cell></row><row><cell>we demonstrate the robustness of AdapterBias</cell></row><row><cell>to different PLMs. Finally, we provide analysis</cell></row><row><cell>on what AdapterBias learns by comparing ?, the</cell></row><row><cell>weights of representation shift for different tokens,</cell></row><row><cell>finding AdapterBias has the ability to identify task-</cell></row><row><cell>specific information. Our study is different from</cell></row><row><cell>the previous architectures of adapters by proposing</cell></row><row><cell>a simple adapter that can produce suitable repre-</cell></row><row><cell>sentation shifts for different tokens.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A :</head><label>A</label><figDesc>Our training details of GLUE benchmark<ref type="bibr" target="#b33">(Wang et al., 2018)</ref>.</figDesc><table><row><cell></cell><cell cols="4">CoLA SST-2 MRPC QNLI</cell><cell>RTE</cell><cell cols="4">STS-B MNLI-m MNLI-mm</cell><cell>QQP</cell></row><row><cell>Max_len</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>512</cell><cell>350</cell><cell>512</cell><cell></cell><cell>128</cell><cell>128</cell><cell>350</cell></row><row><cell>Batchsize</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell></cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell cols="2">Learning rate 10 -3</cell><cell>10 -3</cell><cell>10 -3</cell><cell>10 -4</cell><cell>4 ? 10 -4</cell><cell>10 -3</cell><cell cols="2">4 ? 10 -4</cell><cell>4 ? 10 -4</cell><cell>4 ? 10 -4</cell></row><row><cell>Epoch</cell><cell>20</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>20</cell><cell>20</cell><cell></cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell cols="2">Method</cell><cell cols="9">CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP</cell></row><row><cell cols="8">BB AdapterBias (L0) 26.2% 82.0% 83.1% 82.3% 81.0% 83.0%</cell><cell>83.2%</cell><cell>83.3%</cell><cell>83.4%</cell></row><row><cell cols="8">BL AdapterBias (L0) 83.2% 83.0% 83.3% 83.7% 83.2% 83.2%</cell><cell>83.4%</cell><cell>83.7%</cell><cell>83.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B :</head><label>B</label><figDesc>Percentage of remaining parameters compared with the original parameters of the linear layer (L ? ). Here we experiment with two models: BERT-base (BB) and BERT-large (BL). The setting follows by Table1.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The source code is available at: https://github. com/Allen0307/AdapterBias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://gluebenchmark.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Method Params CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Avg Share v 56</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of more parameter-efficiency methods in AdapterBias. The settings are the same as in Table 1. The backbone model is BERT-base. Share v, Share L ? , and Share v+L ? means that we share vector, linear layer, and both of them, respectively. Method CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm</title>
	</analytic>
	<monogr>
		<title level="j">QQP Avg BB Full-FT</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Ankur</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08478</idno>
		<title level="m">Simple, scalable adaptation for neural machine translation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2106</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01070</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07463</idno>
		<title level="m">Parameter-efficient transfer learning with diff pruning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019-57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nelson F Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08855</idno>
		<title level="m">Linguistic knowledge and transferability of contextual representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l_0 regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04647</idno>
		<title level="m">Compacter: Efficient lowrank hypercomplex adapter layers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<title level="m">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07779</idno>
		<title level="m">A framework for adapting transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nils Reimers, and Iryna Gurevych</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11918</idno>
	</analytic>
	<monogr>
		<title level="m">Adapterdrop: On the efficiency of adapters in transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bert and pals: Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName><forename type="first">Asa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5986" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<title level="m">Bert rediscovers the classical nlp pipeline</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A survey on multitask learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
