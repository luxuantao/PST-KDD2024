<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of total least-squares methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-04-14">14 April 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Markovsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics and Computer Science</orgName>
								<orgName type="institution">University of Southampton</orgName>
								<address>
									<postCode>SO17 1BJ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sabine</forename><surname>Van Huffel</surname></persName>
							<email>sabine.vanhuffel@esat.kuleuven.be</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">ESAT-SCD/SISTA</orgName>
								<address>
									<addrLine>Kasteelpark Arenberg 10</addrLine>
									<postCode>B-3001</postCode>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of total least-squares methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-04-14">14 April 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">7A360E012F91934038ABE0E6039C7A12</idno>
					<idno type="DOI">10.1016/j.sigpro.2007.04.004</idno>
					<note type="submission">Received 28 September 2006; received in revised form 30 March 2007; accepted 3 April 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Total least squares</term>
					<term>Orthogonal regression</term>
					<term>Errors-in-variables model</term>
					<term>Deconvolution</term>
					<term>Linear prediction</term>
					<term>System identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We review the development and extensions of the classical total least-squares method and describe algorithms for its generalization to weighted and structured approximation problems. In the generic case, the classical total least-squares problem has a unique solution, which is given in analytic form in terms of the singular value decomposition of the data matrix. The weighted and structured total least-squares problems have no such analytic solution and are currently solved numerically by local optimization methods. We explain how special structure of the weight matrix and the data matrix can be exploited for efficient cost function and first derivative computation. This allows to obtain computationally efficient solution methods. The total least-squares family of methods has a wide range of applications in system theory, signal processing, and computer algebra. We describe the applications for deconvolution, linear prediction, and errors-invariables system identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The total least-squares method was introduced by Golub and Van Loan <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> as a solution technique for an overdetermined system of equations AX % B, where A 2 R mÂn and B 2 R mÂd are the given data and X 2 R nÂd is unknown. With m4n, typically there is no exact solution for X, so that an approximate one is sought for. The total leastsquares method is a natural generalization of the least-squares approximation method when the data in both A and B is perturbed.</p><p>The least-squares approximation b X ls is obtained as a solution of the optimization problem f b X ls ; DB ls g:¼ arg min</p><formula xml:id="formula_0">X ;DB kDBk F subject to AX ¼ B þ DB.<label>ðLSÞ</label></formula><p>The rationale behind this approximation method is to correct the right-hand side B as little as possible in the Frobenius norm sense, so that the corrected system of equations AX ¼ b B, b B:¼B þ DB has an exact solution. Under the condition that vec A is full column rank, the unique solution b X ls ¼ ðA &gt; AÞ À1 A &gt; B of the optimally corrected system of equations AX ¼ b B ls , b B ls :¼B þ DB ls is by definition the least-squares approximate solution of the original incompatible system of equations.</p><p>The definition of the total least-squares method is motivated by the asymmetry of the least-squares method: B is corrected while A is not. Provided that both A and B are given data, it is reasonable to treat them symmetrically. The classical total least-squares problem looks for the minimal (in the Frobenius norm sense) corrections DA and DB on the given data A and B that make the corrected system of equations b </p><formula xml:id="formula_1">AX ¼ b B, b A:¼A þ DA, b B:¼B þ DB</formula><formula xml:id="formula_2">subject to ðA þ DAÞX ¼ B þ DB.<label>ðTLS1Þ</label></formula><p>The total least-squares approximate solution b X tls for X is a solution of the optimally corrected system of equations b</p><formula xml:id="formula_3">A tls X ¼ b B tls , b A tls :¼A þ DA tls , b B tls :¼B þ DB tls .</formula><p>The least-squares approximation is statistically motivated as a maximum likelihood estimator in a linear regression model under standard assumptions (zero mean, normally distributed residual with a covariance matrix that is a multiple of the identity). Similarly, the total least-squares approximation is a maximum likelihood estimator in the errors-invariables model</p><formula xml:id="formula_4">A ¼ Ā þ Ã; B ¼ B þ B there exists an X 2 R nÂd such that Ā X ¼ B<label>ðEIVÞ</label></formula><p>under the assumption that vec ð½ Ã BÞ is a zero mean, normally distributed random vector with a covariance matrix that is a multiple of the identity.</p><p>In the errors-in-variables (EIV) model, Ā, B are the ''true data'', X is the ''true'' value of the parameter X, and Ã, B consist of ''measurement noise''.</p><p>Our first aim is to review the development and generalizations of the total least-squares method. We start in Section 2 with an overview of the classical total least-squares method. Section 2.1 gives historical notes that relate the total leastsquares method to work on consistent estimation in the EIV model. Section 2.2 presents the solution of the total least-squares problem and the resulting basic computational algorithm. Some properties, generalizations, and applications of the total leastsquares method are stated in Sections 2.3-2.5.</p><p>Our second aim is to present an alternative formulation of the total least-squares problem as a matrix low rank approximation problem </p><p>which in some respects, described in detail later, has advantages over the classical one. With C ¼ ½A B, the classical total least-squares problem (TLS1) is generically equivalent to the matrix low rank approximation problem (TLS2), however, in certain exceptional cases, known in the literature as non-generic total least-squares problems, (TLS1) fails to have a solution, while (TLS2) always has a solution.</p><p>The following example illustrates the geometry behind the least-squares and total least-squares approximations.</p><p>Example 1 (Geometry of the least-squares and total least-squares methods). Consider a data matrix C ¼ ½a b with m ¼ 20 rows and n þ d ¼ 2 columns. B R nþd a static model in R nþd L linear static model class B 2 L n linear static model of dimension at most n, i.e., a subspace (in R nþd ) of dimension at most n X, R, P parameters of input/output, kernel, and image representations B i=o ðX Þ input/output representation, see (I/O repr) in Section 3.1.3 col spanðPÞ image representation, i.e., the space spanned by the columns of P kerðRÞ kernel representation, i.e., the right null space of R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>The data are visualized in the plane: the rows ½a i b i of C correspond to the circles in Fig. <ref type="figure" target="#fig_3">1</ref>. Finding an approximate solution b</p><p>x of the incompatible system of equations ax % b amounts to fitting the data points by a non-vertical line passing through the origin. (The vertical line cannot be represented by an x 2 R.) The cases when the best fitting line happens to be vertical correspond to non-generic problems.</p><p>Alternatively, finding a rank-1 approximation b C of the given matrix C (refer to problem (TLS2)) amounts to fitting the data points ½a i b i by points ½b a i b b i (corresponding to the rows of b C) that lie on a line passing through the origin. Note that now we do not exclude an approximation by the vertical line, because approximation points lying on a vertical line define a rank deficient matrix b C and problem (TLS2) does not impose further restrictions on the solution.</p><p>The least-squares and total least-squares methods assess the fitting accuracy in different ways: the least-squares method minimizes the sum of the squared vertical distances from the data points to the fitting line, while the total leastsquares method minimizes the sum of the squared orthogonal distances from the data points to the fitting line. Fig. <ref type="figure" target="#fig_3">1</ref> shows the least-squares and total least-squares fitting lines as well as the data approximation (the crosses lying on the lines). In the least-squares case, the data approximation b C ls ¼ ½a b þ Db ls is obtained by correcting the second coordinate only. In the total leastsquares case, the data approximation b</p><formula xml:id="formula_6">C tls ¼ ½a þ Da tls b þ Db tls is obtained by correcting both coordinates. In (TLS1) the constraint b AX ¼ b B represents the rank constraint rankð b</formula><p>CÞpn, via the implication there exists an</p><formula xml:id="formula_7">X 2 R nÂd such that b AX ¼ b B ) rankð b CÞpn; where b C :¼ ½ b A b B.</formula><p>Note, however, that the reverse implication does not hold in general. This lack of equivalence is the reason for the existence of non-generic total leastsquares problems. Problem (TLS1) is non-generic when the rank deficiency of b C tls (an optimal solution of (TLS2)) cannot be expressed as existence of linear relations b AX ¼ b B for some X 2 R nÂd . In Section 3. CÞpn. The representation-free total least-squares problem formulation (TLS2), described in Section 3, is inspired by the behavioral approach to system theory, put forward by Willems in the three part remarkable paper <ref type="bibr" target="#b2">[3]</ref>. We give an interpretation of the abstract rank condition as existence of a linear static model for the given data. Then: the total least squares method is viewed as a tool for deriving approximate linear static models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>This point of view is treated in more details for dynamic as well as static models in <ref type="bibr" target="#b3">[4]</ref>.</p><p>In Sections 3 and 5 we describe the extensions of the classical total least squares problem to weighted and structured total least-squares problems and classify the existing methods according to the representation of the rank constraint (input/output, kernel, or image) and the optimization method that is used for the solution of the resulting parameter optimization problem. We show that the block-Hankel structured total least-squares problem is a kernel problem for approximate modeling by a linear time-invariant dynamical model. Motivating examples are the deconvolution problem, the linear prediction problem, and the EIV system identification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The classical total least-squares method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">History</head><p>Although the name ''total least squares'' appeared only recently in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, this fitting method is not new and has a long history in the statistical literature where it is known as ''orthogonal regression'', ''errors-in-variables'', and ''measurement errors''. The univariate (n ¼ 1, d ¼ 1) problem is discussed already in 1877 by Adcock <ref type="bibr" target="#b4">[5]</ref>. Latter on contributions are made by Adcock <ref type="bibr" target="#b5">[6]</ref>, Pearson <ref type="bibr" target="#b6">[7]</ref>, Koopmans <ref type="bibr" target="#b7">[8]</ref>, Madansky <ref type="bibr" target="#b8">[9]</ref>, and York <ref type="bibr" target="#b9">[10]</ref>. The orthogonal regression method has been rediscovered many times, often independently. About 30 years ago, the technique was extended by Sprent <ref type="bibr" target="#b10">[11]</ref> and Gleser <ref type="bibr" target="#b11">[12]</ref> to multivariate (n41, d41) problems.</p><p>More recently, the total least-squares method also stimulated interest outside statistics. In the field of numerical analysis, this problem was first studied by Golub and Van Loan <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Their analysis, as well as their algorithm, is based on the singular value decomposition. Geometrical insight into the properties of the singular value decomposition brought Staar <ref type="bibr" target="#b12">[13]</ref> independently to the same concept. Van Huffel and Vandewalle <ref type="bibr" target="#b13">[14]</ref> generalized the algorithm of Golub and Van Loan to all cases in which their algorithm fails to produce a solution, described the properties of these so-called nongeneric total least-squares problems and proved that the proposed generalization still satisfies the total least-squares criteria if additional constraints are imposed on the solution space. This seemingly different linear algebraic approach is actually equivalent to the method of multivariate EIV regression analysis, studied by Gleser <ref type="bibr" target="#b11">[12]</ref>. Gleser's method is based on an eigenvalue-eigenvector analysis, while the total least-squares method uses the singular value decomposition which is numerically more robust in the sense of algorithmic implementation. Furthermore, the total leastsquares algorithm computes the minimum norm solution whenever the total least-squares solution is not unique. These extensions are not considered by <ref type="bibr">Gleser.</ref> In engineering fields, e.g., experimental modal analysis, the total least-squares technique (more commonly known as the H v technique), was also introduced about 20 years ago by Leuridan et al. <ref type="bibr" target="#b14">[15]</ref>. In the field of system identification, Levin <ref type="bibr" target="#b15">[16]</ref> first studied the problem. His method, called the eigenvector method or Koopmans-Levin method <ref type="bibr" target="#b16">[17]</ref>, computes the same estimate as the total leastsquares algorithm whenever the total least-squares problem has a unique solution. Compensated least squares was yet another name arising in this area: this method compensates for the bias in the estimator, due to measurement error, and is shown by Stoica and So¨derstro¨m <ref type="bibr" target="#b17">[18]</ref> to be asymptotically equivalent to total least squares. Furthermore, in the area of signal processing, the minimum norm method Kumaresan and Tufts <ref type="bibr" target="#b18">[19]</ref> was introduced and shown to be equivalent to minimum norm total least squares, see Dowling and Degroat <ref type="bibr" target="#b19">[20]</ref>. Finally, the total least-squares approach is tightly related to the maximum likelihood principal component analysis method introduced in chemometrics by Wentzell et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, see the discussion in Section 4.2.</p><p>The key role of least squares in regression analysis is the same as that of total least squares in EIV modeling. Nevertheless, a lot of confusion exists in the fields of numerical analysis and statistics about the principle of total least squares and its relation to EIV modeling. The computational advantages of total least squares are still largely unknown in the statistical community, while inversely the concept of EIV modeling did not penetrate sufficiently well in the field of computational mathematics and engineering.</p><p>A comprehensive description of the state of the art on total least squares from its conception up to the summer of 1990 and its use in parameter estimation has been presented in Van Huffel and Vandewalle <ref type="bibr" target="#b22">[23]</ref>. While the latter book is entirely devoted to total least squares, a second <ref type="bibr" target="#b23">[24]</ref> and third <ref type="bibr" target="#b24">[25]</ref> edited books present the progress in total least squares and in the field of EIV modeling, respectively, from 1990 till 1996 and from 1996 till 2001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Algorithm</head><p>The following theorem gives conditions for the existence and uniqueness of a total least-squares solution.</p><p>Theorem 2 (Solution of the classical total leastsquares problem). Let C:¼½A B ¼ USV &gt; where S ¼ diagðs 1 ; . . . ; s nþd Þ be a singular value decomposition of C, s 1 X Á Á Á Xs nþd be the singular values of C, and define the partitionings</p><formula xml:id="formula_8">V :¼ n d V 11 V 12 V 21 V 22 " # n d</formula><p>and S:¼</p><formula xml:id="formula_9">n d S 1 0 0 S 2 " # n d</formula><p>.</p><p>A total least-squares solution exists if and only if V 22 is non-singular. In addition, it is unique if and only if s n as nþ1 . In the case when the total least-squares solution exists and is unique, it is given by b X tls ¼ ÀV 12 V À1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>and the corresponding total least-squares correction matrix is</p><formula xml:id="formula_10">DC tls :¼½DA tls DB tls ¼ ÀU diagð0; S 2 ÞV &gt; .</formula><p>In the generic case when a unique total leastsquares solution b</p><p>X tls exists, it is computed from the d right singular vectors corresponding to the smallest singular values by normalization. This gives Algorithm 1 as a basic algorithm for solving the classical total least-squares problem (TLS1). Note that the total least-squares correction matrix DC tls is such that the total least-squares data approximation</p><formula xml:id="formula_11">b C tls :¼C þ DC tls ¼ U diagðS 1 ; 0ÞV &gt;</formula><p>is the best rank-n approximation of C. Most total least-squares problems which arise in practice can be solved by Algorithm 1. Extensions of the basic total least-squares algorithm to problems in which the total least-squares solution does not exist or is not unique are considered in detail in <ref type="bibr" target="#b22">[23]</ref>. In addition, it is shown how to speed up the total leastsquares computations directly by computing the singular value decomposition only partially or iteratively if a good starting vector is available. More recent advances, e.g., recursive total least-squares algorithms, neural based total least-squares algorithms, rank-revealing total least-squares algorithms, total least-squares algorithms for large scale problems, etc., are reviewed in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. A novel theoretical and computational framework for treating non-generic and non-unique total least-squares problems is presented by Paige and Strakos <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Properties</head><p>Consider the EIV model and assume that vecð½ Ã BÞ is a zero mean random vector with a multiple of the identity covariance matrix. In addition, assume that lim m!1 Ã&gt; Ã=m exists and is a positive definite matrix. Under these assumptions it is proven <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref> that the total least-squares solution b</p><p>X tls is a weakly consistent estimator of the true parameter values X , i.e., b X tls ! X in probability as m ! 1.</p><p>This total least-squares property does not depend on the distribution of the errors. The total least-squares correction ½DA tls DB tls , however, being a rank d matrix is not an appropriate estimator for the measurement error matrix ½ Ã B (which is a full rank matrix with probability one). Note that the leastsquares estimator b X ls is inconsistent in the EIV case.</p><p>In the special case of a single right-hand side (d ¼ 1) and A full rank, the total least-squares problem has an analytic expression that is similar to the one of the least-squares solution least squares: b</p><formula xml:id="formula_12">x ls ¼ ðA &gt; AÞ À1 A &gt; b, total least squares: b x tls ¼ ðA &gt; A À s 2 nþ1 IÞ À1 A &gt; b,<label>ðÃÞ</label></formula><p>where s nþ1 is the smallest singular value of ½A b.</p><p>From a numerical analyst's point of view, ( * ) tells that the total least-squares solution is more illconditioned than the least-squares solution since it has a higher condition number. The implication is that errors in the data are more likely to affect the total least-squares solution than the least-squares solution. This is particularly true for the worst case perturbations. In fact, total least-squares is a deregularizing procedure. However, from a statistician's point of view, ( * ) tells that the total leastsquares method asymptotically removes the bias by subtracting the error covariance matrix (estimated by s 2 nþ1 I) from the data covariance matrix A &gt; A. While least-squares minimizes a sum of squared residuals, total least-squares minimizes a sum of weighted squared residuals: least squares: min</p><formula xml:id="formula_13">x kAx À bk 2 ,</formula><p>total least squares: min</p><formula xml:id="formula_14">x kAx À bk 2 kxk 2 þ 1 .</formula><p>From a numerical analyst's point of view, total least-squares minimizes the Rayleigh quotient.</p><p>From a statistician's point of view, total leastsquares weights the residuals by multiplying them with the inverse of the corresponding error covariance matrix in order to derive a consistent estimate. Other properties of total least squares, which were studied in the field of numerical analysis, are its sensitivity in the presence of errors on all data <ref type="bibr" target="#b22">[23]</ref>. Differences between the least-squares and total least-squares solution are shown to increase when the ratio between the second smallest singular value of ½A b and the smallest singular value of A is growing. In particular, this is the case when the set of equations Ax % b becomes less compatible, the vector y is growing in length, or A tends to be rankdeficient. Assuming independent and identically distributed errors, the improved accuracy of the total least-squares solution compared to that of the least-squares solution is maximal when the orthogonal projection of b is parallel to the singular vector of A corresponding to the smallest singular value. Additional algebraic connections and sensitivity properties of the total least-squares and leastsquares problems, as well as other statistical properties have been described in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Extensions</head><p>The statistical model that corresponds to the basic total least-squares approach is the EIV model with the restrictive condition that the measurement errors are zero mean independent and identically distributed. In order to relax these restrictions, several extensions of the total least-squares problem have been investigated. The mixed least-squarestotal least-squares problem formulation allows to extend consistency of the total least-squares estimator in EIV models, where some of the variables are measured without error. The data least-squares problem <ref type="bibr" target="#b27">[28]</ref> refers to the special case in which the A matrix is noisy and the B matrix is exact. When the errors ½ Ã B are row-wise independent with equal row covariance matrix (which is known up to a scaling factor), the generalized total least-squares problem formulation <ref type="bibr" target="#b28">[29]</ref> allows to extend consistency of the total least-squares estimator.</p><p>More general problem formulations, such as restricted total least squares <ref type="bibr" target="#b29">[30]</ref>, which also allow the incorporation of equality constraints, have been proposed, as well as total least-squares problem formulations using ' p norms in the cost function. The latter problems, called total ' p approximations, proved to be useful in the presence of outliers. Robustness of the total least-squares solution is also improved by adding regularization, resulting in regularized total least-squares methods <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. In addition, various types of bounded uncertainties have been proposed in order to improve robustness of the estimators under various noise conditions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Similarly to the classical total least-squares estimator, the generalized total least-squares estimator is computed reliably using the singular value decomposition. This is no longer the case for more general weighted total least-squares problems where the measurement errors are differently sized and/or correlated from row to row. Consistency of the weighted total least-squares estimator is proven and an iterative procedure for its computation is proposed in <ref type="bibr" target="#b37">[38]</ref>. This problem is discussed in more detail in Section 3.</p><p>Furthermore, constrained total least-squares problems have been formulated. Arun <ref type="bibr" target="#b38">[39]</ref> addressed the unitarily constrained total least-squares problem, i.e., AX % B, subject to the constraint that the solution matrix X is unitary. He proved that this solution is the same as the solution to the orthogonal Procrustes problem <ref type="bibr">[40, p. 582</ref>]. Abatzoglou et al. <ref type="bibr" target="#b40">[41]</ref> considered yet another constrained total least-squares problem, which extends the classical total least-squares problem to the case where the errors ½ Ã B are algebraically related. In this case, the total least-squares solution is no longer statistically optimal (e.g., maximum likelihood in the case of normal distribution).</p><p>In the so-called structured total least-squares problems <ref type="bibr" target="#b41">[42]</ref>, the data matrix ½A B is structured. In order to preserve the maximum likelihood properties of the solution, the total least-squares problem formulation is extended <ref type="bibr" target="#b42">[43]</ref> with the additional constraint that the structure of the data matrix ½A B is preserved in the correction matrix ½DA DB. Similarly to the weighted total leastsquares problem, the structured total least-squares solution, in general, has no closed form expression in terms of the singular value decomposition. An important exception is the circulant structured total least squares, which can be solved using the fast Fourier transform, see <ref type="bibr" target="#b43">[44]</ref>. In the general case, a structured total least-squares solution is searched via numerical optimization methods. However, efficient algorithms are proposed in the literature that exploit the matrix structure on the level of the computations. This research direction is further described in Section 5.</p><p>Regularized structured total least-squares solution methods are proposed in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Regularization turns out to be important in the application of the structured total least-squares method for image deblurring <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>. In addition, solution methods for nonlinearly structured total least-squares methods are developed in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Applications</head><p>Since the publication of the singular value decomposition based total least-squares algorithm <ref type="bibr" target="#b1">[2]</ref>, many new total least-squares algorithms have been developed and, as a result, the number of applications in total least squares and EIV modeling has increased in the last decade. Total least squares is applied in computer vision <ref type="bibr" target="#b51">[52]</ref>, image reconstruction <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>, speech and audio processing <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, modal and spectral analysis <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>, linear system theory <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, system identification <ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref>, and astronomy <ref type="bibr" target="#b65">[66]</ref>. An overview of EIV methods in system identification is given by So¨derstro¨m in <ref type="bibr" target="#b66">[67]</ref>. In <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, the use of total least squares and EIV models in the application fields are surveyed and new algorithms that apply the total least-squares concept are described.</p><p>A lot of common problems in system identification and signal processing can be reduced to special types of block-Hankel and block-Toeplitz structured total least-squares problems. In the field of signal processing, in particular in vivo magnetic resonance spectroscopy, and audio coding, new state-space based methods have been derived by making use of the total least-squares approach for spectral estimation with extensions to decimation and multichannel data quantification <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. In addition, it has been shown how to extend the least mean squares algorithm to the EIV context for use in adaptive signal processing and various noise environments. Finally, total least-squares applications also emerge in other fields, including information retrieval <ref type="bibr" target="#b69">[70]</ref>, shape from moments <ref type="bibr" target="#b70">[71]</ref>, and computer algebra <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Representation-free total least-squares problem formulation</head><p>An insightful way of viewing the abstract rank constraint rankðCÞpn is as the existence of a linear static model for C: rankðCÞpn is equivalent to the existence of a subspace B &amp; R nþd of dimension at most n that contains the rows of C.</p><p>A subspace B R nþd is referred to as a linear static model. Its dimension n is a measure of the model complexity: the higher the dimension the more complex and therefore less useful is the model B.</p><p>The set of all linear static models of dimension at most n is denoted by L n . It is a non-convex set and has special properties that make it a Grassman manifold.</p><p>Let ½c 1 Á Á Á c m :¼C &gt; , i.e., c i is the transposed ith row of the matrix C and define the shorthand notation</p><formula xml:id="formula_15">C 2 B R nþd : 3 c i 2 B for i ¼ 1; . . . ; m.</formula><p>We have the following equivalence rankðCÞpn 3 C 2 B 2 L n , which relates the total least-squares problem (TLS2) to approximate linear static modeling. We restate problem (TLS2) with this new interpretation and notation.</p><p>Problem 3 (Total least squares). Given a data matrix C 2 R mÂðnþdÞ and a complexity specification n, solve the optimization problem</p><formula xml:id="formula_16">f b B tls ; b C tls g:¼ arg min B2L n min b C2B kC À b Ck F . (TLS)</formula><p>Note that (TLS) is a double minimization problem. On the inner level is the search for the best approximation of the given data C in a given model B. The optimum value of this minimization</p><formula xml:id="formula_17">M tls ðC; BÞ:¼ min b C2B kC À b Ck F (Mtls)</formula><p>is a measure of the lack of fit between the data and the model and is called misfit. On the outer level is the search for the optimal model in the model class L n of linear static models with bounded complexity. The optimality of the model is in terms of the total least-squares misfit function M tls .</p><p>The double minimization structure, described above, is characteristic for all total least-squares problems. Since the model B is linear and the cost function is convex quadratic, the inner minimization can be solved analytically yielding a closed form expression for the misfit function. The resulting outer minimization, however, is a non-convex optimization problem and needs numerical solution methods. In the case of the basic total least-squares problem and the generalized total least-squares problem, presented in Section 3.3, the outer minimization can be brought back to a singular value decomposition computation. In more general cases, however, one has to rely on non-convex optimization methods and the guarantee to compute a global solution quickly and efficiently is lost.</p><p>In order to solve numerically the abstract total least-squares problem (TLS), we need to parameterize the fitting model. This important issue is discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Kernel, image, and input/output representations</head><p>As argued in the Introduction, the representationfree formulation is conceptually useful. For analysis, however, often it is more convenient to consider concrete representations of the model, which turn the abstract problem (TLS) into concrete parameter optimization problems, such as (TLS1). In this section, we present three representations of a linear static model: kernel, image, and input/output. They give different parameterizations of the model and are important in setting up algorithms for the solution of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Kernel representation</head><p>Let B 2 L n , i.e., B is an n-dimensional subspace of R nþd . A kernel representation of B is given by a system of equations Rc ¼ 0, such that</p><formula xml:id="formula_18">B ¼ f c 2 R nþd j Rc ¼ 0g¼: kerðRÞ. The matrix R 2 R gÂðnþdÞ is a parameter of the model B.</formula><p>The parameter R is not unique. There are two sources for the non-uniqueness:</p><p>1. R might have redundant rows, and 2. for a full rank matrix U, kerðRÞ ¼ kerðURÞ.</p><p>The parameter R having redundant rows is related to the minimality of the representation. For a given linear static model B, the representation Rc ¼ 0 of B is minimal if R has the minimal number of rows among all parameters R that define a kernel representation of B. The kernel representation, defined by R, is minimal if and only if R is full row rank.</p><p>Because of item 2, a minimal kernel representation is still not unique. All minimal representations, however, are related to a given one via a premultiplication of the parameter R with a nonsingular matrix U. In a minimal kernel representation, the rows of R are a basis for B ? , the orthogonal complement of B, i.e., B ? ¼ row spanðRÞ.</p><p>The choice of R is non-unique due to the nonuniqueness in the choice of basis of B ? .</p><p>The minimal number of independent linear equations necessary to define a linear static model B is d, i.e., in a minimal representation B ¼ kerðRÞ with row dimðRÞ ¼ d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Image representation</head><p>The dual of the kernel representation B ¼ kerðRÞ is the image representation</p><formula xml:id="formula_19">B ¼ fc 2 R nþd j c ¼ Pl; l 2 R n g¼:col spanðPÞ.</formula><p>Again for a given B 2 L n an image representation B ¼ col spanðPÞ is not unique because of possible non-minimality of P and the choice of basis. The representation is minimal if and only if P is a full column rank matrix. In a minimal image representation, col dimðPÞ ¼ dimðBÞ and the columns of P form a basis for B. Clearly col spanðPÞ ¼ col spanðPUÞ, for any non-singular matrix U 2 R nÂn . Note that</p><formula xml:id="formula_20">kerðRÞ ¼ col spanðPÞ ¼ B 2 L n ) RP ¼ 0,</formula><p>which gives a link between the parameters P and R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Input/output representation</head><p>Both, the kernel and the image representations, treat all variables on an equal footing. In contrast, the more classical input/output representation</p><formula xml:id="formula_21">B i=o ðX Þ:¼fc¼:colða; bÞ 2 R nþd j X &gt; a ¼ bg (I/O repr)</formula><p>distinguishes free variables a 2 R n , called inputs, and dependent variables b 2 R d , called outputs. In an input/output representation, a can be chosen freely, while b is fixed by a and the model. Note that for repeated observations</p><formula xml:id="formula_22">C &gt; ¼ ½c 1 Á Á Á c m the statement C 2 B i=o ðX Þ is equivalent to the linear system of equations AX ¼ B, where ½A B:¼C with A 2 R mÂn and B 2 R mÂd .</formula><p>The partitioning c ¼ colða; bÞ gives an input/ output partitioning of the variables: the first n:¼ dimðaÞ variables are inputs and the remaining d:¼ dimðbÞ variables are outputs. An input/output partitioning is not unique. Given a kernel or image representation, finding an input/output partitioning is equivalent to selecting a d Â d full rank submatrix of R or an n Â n full rank submatrix of P. In fact, generically, any splitting of the variables into a group of d variables (outputs) and a group of remaining variables (inputs), defines a valid input/ output partitioning. In non-generic cases certain partitionings of the variables into inputs and outputs are not possible.</p><p>Note that in (I/O repr), the first n variables are fixed to be inputs, so that given X, the input/output represent B i=o ðX Þ is fixed and vice versa, given B 2 L n , the parameter X (if it exists) is unique. Thus, as opposed to the parameters R and P in the kernel and the image representations, the parameter X in the input/output representation (I/O repr) is unique.</p><p>Consider the input/output B i=o ðX Þ, kernel kerðRÞ, and image col spanðPÞ representations of B 2 L n and define the partitionings</p><formula xml:id="formula_23">R¼:½R i R o ; R o 2 R dÂd and P¼: P i P o " # ; P i 2 R nÂn .</formula><p>The links among the parameters X, R, and P are summarized in Fig. <ref type="figure" target="#fig_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Solution of the total least-squares problem</head><p>Approximation of the data matrix C with a model B in the model class L n is equivalent to finding a matrix b C 2 R mÂðnþdÞ with rank at most n. In the case when the approximation criterion is kC À b Ck F (total least-squares problem) or kC À b Ck 2 , the problem has a solution in terms of the singular value decomposition of C. The result is known as the Eckart-Young-Mirsky low-rank matrix approximation theorem <ref type="bibr" target="#b73">[74]</ref>. We state it in the next lemma.</p><p>Lemma 4 (Matrix approximation lemma). Let C ¼ USV &gt; be the singular value decomposition of C 2 R mÂðnþdÞ and partition the matrices U, S¼:diagðs 1 ; . . . ; s nþd Þ, and V as follows:</p><formula xml:id="formula_24">n d U¼: ½ U 1 U 2 m ; n d S¼: S 1 0 0 S 2 " # n d and n d V ¼: ½ V 1 V 2 n þ d: ðSVD PRTÞ Then the rank-n matrix b C Ã ¼ U 1 S 1 V &gt; 1 is such that kC À b C Ã k F ¼ min rankðb C Þpn kC À b Ck F ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi s 2 nþ1 þ Á Á Á þ s 2 nþd q .</formula><p>The solution b C Ã is unique if and only if s nþ1 as n .</p><p>The solution of the total least-squares problem (TLS) trivially follows from Lemma 4.</p><p>Theorem 5 (Solution of the total least-squares problem). Let C ¼ USV &gt; be the singular value decomposition of C and partition the matrices U, S, and V as in (SVD PRT). Then a total least-squares</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_25">approximation of C in L n is b C tls ¼ U 1 S 1 V &gt; 1 ; b B tls ¼ kerðV &gt; 2 Þ ¼ col spanðV 1 Þ,</formula><p>and the total least-squares misfit is</p><formula xml:id="formula_26">M tls ðC; BÞ ¼ kS 2 k F ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi s 2 nþ1 þ Á Á Á þ s 2 nþd</formula><p>q where S 2 ¼:diagðs nþ1 ; . . . ; s nþd Þ.</p><p>A total least-squares approximation always exists. It is unique if and only if s n as nþ1 .</p><p>Note 6 (Non-generic total least-squares problems). The optimal approximating model b B tls might have no input/output representation (I/O repr). In this case, the optimization problem (TLS1) has no solution. By suitable permutation of the variables, however, (TLS1) can be made solvable, so that b X tls exists and b</p><formula xml:id="formula_27">B tls ¼ B i=o ð b X tls Þ.</formula><p>The issue of whether the total least-squares problem is generic or not is not related to the approximation of the data per se but to the possibility of representing the optimal model b B tls in the form (I/O repr), i.e., to the possibility of imposing a given input/output partition on b B tls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalized total least-squares problem</head><p>Let W ' 2 R mÂm and W r 2 R ðnþdÞÂðnþdÞ be given positive definite matrices and define the following generalized total least-squares misfit function</p><formula xml:id="formula_28">M gtls ðC; BÞ ¼ min b C2B k ffiffiffiffiffiffiffi W ' p ðC À b CÞ ffiffiffiffiffiffiffi W r p k F .<label>(Mgtls)</label></formula><p>(W ' allows for a row weighting and W r for a column weighting in the cost function.) The resulting approximation problem is called generalized total least-squares problem. The solution of the generalized total least-squares problem can be obtained from the solution of a total least-squares problem for a modified data matrix.</p><p>Theorem 8 (Solution of the generalized total leastsquares problem). Define the modified data matrix</p><formula xml:id="formula_29">C m :¼ ffiffiffiffiffiffiffi W ' p C ffiffiffiffiffiffiffi W r p ,</formula><p>and let b C m;tls , b B m;tls ¼ kerðR m;tls Þ ¼ col spanðP m;tls Þ be a total least-squares approximation of C m in L n . Then a solution of the generalized total least-squares problem Robust algorithms for solving the generalized total least-squares problem without explicitly computing the inverses ð ffiffiffiffiffiffiffi <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b74">75]</ref>. These algorithms give better accuracy when the weight matrices are nearly rank deficient. In addition, they can treat the singular case, which implies that some rows and/or columns of C are considered exact and are not modified in the solution b C. If the matrices W ' and W r are diagonal, i.e., W ' ¼ diagðw ';1 ; :::; w ';m Þ, where w ' 2 R m þ and W r ¼ diagðw r;1 ; :::; w r;nþd Þ, where w l 2 R nþd þ the generalized total least-squares problem is called scaled total least-squares.</p><formula xml:id="formula_30">(GTLS) is b C gtls ¼ ð ffiffiffiffiffiffiffi W ' p Þ À1 b C m;tls ð ffiffiffiffiffiffiffi W r p Þ À1 , b B gtls ¼ kerðR m;tls ffiffiffiffiffiffiffi W r p Þ ¼ col spanðð ffiffiffiffiffiffiffi W r p Þ À1 P m;tls Þ</formula><formula xml:id="formula_31">W ' p Þ À1 and ð ffiffiffiffiffiffiffi W r p Þ À1 are proposed in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Weighted total least squares</head><p>For a given positive definite weight matrix W 2 R mðnþdÞÂmðnþdÞ define the weighted matrix norm</p><formula xml:id="formula_32">kCk W :¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi vec &gt; ðC &gt; ÞW vecðC &gt; Þ q</formula><p>and the weighted total least-squares misfit function</p><formula xml:id="formula_33">M wtls ðC; BÞ:¼ min b C2B kC À b Ck W . (Mwtls)</formula><p>The approximation problem with weighted total least-squares misfit function is called the weighted total least-squares problem. The motivation for considering the weighted total least-squares problem is that it defines the maximum likelihood estimator for the EIV model when the measurement noise C ¼ ½ Ã B is zero mean, normally distributed, with a covariance matrix covðvecð C&gt; ÞÞ ¼ s 2 W À1 , ( ** )</p><p>i.e., the weight matrix W is up to a scaling factor s 2 the inverse of the measurement noise covariance matrix.</p><p>Note 10 (Element-wise weighted total least-squares). Note 11 (Total least squares as an unweighted weighted total least squares). The extreme special case when W ¼ I is called unweighted. Then the weighted total least-squares problem reduces to the total least-squares problem. The total leastsquares misfit M tls weights equally all elements of the correction matrix DC. It is a natural choice when there is no prior knowledge about the data.</p><p>In addition, the unweighted case is computationally easier to solve than the general weighted case.</p><p>Special structure of the weight matrix W results in special weighted total least-squares problems. Fig. <ref type="figure">3</ref> shows a hierarchical classification of various problems considered in the literature. From top to bottom the generality of the problems decreases: on the top is a weighted total least-squares problem for a general positive semi-definite weight matrix and on the bottom is the classical total least-squares problem. In between are weighted total leastsquares problems with (using the stochastic terminology) uncorrelated errors among the rows, among the columns, and among all elements (element-wise weighted total least-squares case). Row-wise and column-wise uncorrelated weighted total leastsquares problems, in which the row or column weight matrices are equal are generalized total leastsquares problems with, respectively, W ' ¼ I and W r ¼ I. In order to express easily the structure of the weight matrix in the case of column-wise uncorrelated errors, we introduce the weight matrix W as follows: covðvecð CÞÞ ¼ s 2 W À1 ; compare with ( ** ), where C is transposed.</p><p>With W ¼ I, (WTLS) coincides with the total least-squares problem (TLS). Except for the special case of generalized total least squares, however, the weighted total least-squares problem has no closed form solution in terms of the singular value decomposition. As an optimization problem it is non-convex, so that the currently available solution methods do not guarantee convergence to a global optimum solution. In the rest of this section, we give an overview of solution methods for the weighted total least-squares problem, with emphasis on the row-wise weighted total least-squares case, i.e., when the weight matrix W is block diagonal</p><formula xml:id="formula_34">W ¼ diagðW 1 ; . . . ; W m Þ, W i 2 R ðnþdÞÂðnþdÞ , W i 40.</formula><p>In the EIV setting, this assumption implies that the measurement errors ci and cj are uncorrelated for all i; j ¼ 1; . . . ; m, iaj, which is a reasonable assumption for most applications.</p><p>Similarly to the total least-squares and generalized total least-squares problems, the weighted total least-squares problem is a double minimization problem. The inner minimization is the search for the best approximation of the data in a given model and an outer minimization is the search for the model. First, we solve the inner minimization problem-the misfit computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Best approximation of the data by a given model</head><p>Since the model is linear, (Mwtls) is a convex optimization problem with an analytic solution. In order to give explicit formulas for the optimal approximation b C wtls and misfit M wtls ðC; BÞ, however, we need to choose a particular parameterization of the given model B. We state the results for the kernel and the image representations. The results for the input/output representation follow from the given ones by the substitutions R7 !½X &gt; À I and P7 !½ I X &gt; . Theorem 12 (Weighted total least-squares misfit computation, kernel representation version). Let kerðRÞ be a minimal kernel representation of B 2 L n . The best weighted total least-squares approximation of C in B, i.e., the solution of (Mwtls), is</p><formula xml:id="formula_35">b c wtls;i ¼ ðI À W À1 i R &gt; ðRW À1 i R &gt; Þ À1 RÞc i for i ¼ 1; . . . ; m with the corresponding misfit M wtls ðC; kerðRÞÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X m i¼1 c &gt; i R &gt; ðRW À1 i R &gt; Þ À1 Rc i s . ðMwtls R Þ</formula><p>The image representation is dual to the kernel representation. Correspondingly, the misfit computation with kernel and with image representations of the model are dual problems. The kernel representation leads to a least norm problem and the image representation leads to a least-squares problem.</p><p>Theorem 13 (Weighted total least-squares misfit computation, image representation version). Let col spanðPÞ be a minimal image representation of B 2 L n . The best weighted total least-squares approximation of C in B is b c wtls;i ¼ PðP &gt; W i PÞ À1 P &gt; W i c i for i ¼ 1; . . . ; m with the corresponding misfit M wtls ðC; col spanðPÞÞ</p><formula xml:id="formula_36">¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X m i¼1 c &gt; i W i ðI À PðP &gt; W i PÞ À1 P &gt; W i Þc i s . ðMwtls P Þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization over the model parameters</head><p>The remaining problem-the minimization with respect to the model parameters is a non-convex optimization problem that in general has no closed form solution. For this reason numerical optimization methods are employed for its solution.</p><p>Special optimization methods for the weighted total least-squares problem are proposed in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>. The Riemannian singular value decomposition framework of De Moor <ref type="bibr" target="#b41">[42]</ref> is derived for the structured total least-squares problem but</p><formula xml:id="formula_37">ARTICLE IN PRESS WTLS W ≥ 0 Row-wise WTLS W = diag (W 1 ,...,W m ) Column-wise WTLS W = diag (W 1 ,..., W n+d ) ¯Row-wise GTLS W = diag (W r ,...,W r ) m EWTLS W = diag(w)</formula><p>Column-wise GTLS ¯n+d Row-wises caled TLS</p><formula xml:id="formula_38">W = diag (col (w r ,...,w r )) m Column-wises caled TLS ¯ n+d TLS W = 2 I m (n+d)</formula><p>TLS -total least squares GTLS -generalized total least squares WTLS -weighted total least squares EWTLS -element-wise weighted total least squares Fig. <ref type="figure">3</ref>. Hierarchy of weighted total least-squares problems according to the structure of the weight matrix W. On the left side are weighted total least-squares problems with row-wise uncorrelated measurement errors and on the right side are weighted total least-squares problems with column-wise uncorrelated measurement errors.</p><p>includes the weighted total least-squares problem with diagonal weight matrix and d ¼ 1 as a special case. The restriction to more general weighted total least-squares problems comes from the fact that the Riemannian singular value decomposition framework is derived for matrix approximation problems with rank reduction by one. De Moor proposed an algorithm resembling the inverse power iteration algorithm for computing the solution. The method, however, has no proven convergence properties.</p><p>The maximum likelihood principle component analysis method of Wentzell et al. <ref type="bibr" target="#b20">[21]</ref> is an alternating least-squares algorithm. It applies to the general weighted total least-squares problems and is globally convergent, with linear convergence rate. The method of Premoli and Rastello <ref type="bibr" target="#b75">[76]</ref> is a heuristic for solving the first order optimality condition of (WTLS). A solution of a nonlinear equation is sought instead of a minimum point of the original optimization problem. The method is locally convergent with superlinear convergence rate. The region of convergence around a minimum point could be rather small in practice. The weighted low rank approximation framework of Manton et al. <ref type="bibr" target="#b77">[78]</ref> proposes specialized optimization methods on a Grassman manifold. The leastsquares nature of the problem is not exploited by the algorithms proposed in <ref type="bibr" target="#b77">[78]</ref>.</p><p>The Riemannian singular value decomposition, maximum likelihood principle component analysis, Premoli-Rastello, and weighted low rank approximation methods differ in the parameterization of the model and the optimization algorithm used, see Table <ref type="table" target="#tab_4">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Structured total least squares</head><p>The total least-squares problem is a tool for approximate modeling by a static linear model. Similarly, the structured total least-squares problem with block-Hankel structured data matrix is a tool for approximate modeling by a linear time-invariant dynamic model. In order to show how the block-Hankel structure occurs, consider a difference equation represented by an linear time-invariant model</p><formula xml:id="formula_39">R 0 w t þ R 1 w tþ1 þ Á Á Á þ R l w tþl ¼ 0. (KER)</formula><p>Here R 0 ; . . . ; R l are the model parameters and the integer l is the lag of the equation. For t ¼ 1; . . . ; T À l, the difference equation (KER) is equivalent to the block-Hankel structured system of equations</p><formula xml:id="formula_40">½ R 0 R 1 Á Á Á R l w 1 w 2 Á Á Á w TÀl w 2 w 3 Á Á Á w TÀlþ1 . . . . . . . . . w lþ1 w lþ2 Á Á Á w T<label>2 6 6 6 6 6 4 3 7 7 7 7 7 5</label></formula><p>|fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl}</p><formula xml:id="formula_41">H l ðwÞ ¼ 0. (<label>Hank eqn)</label></formula><p>Thus the constraint that a time series w ¼ ðwð1Þ; . . . ; wðTÞÞ is a trajectory of the linear timeinvariant model implies rank deficiency of the block-Hankel matrix H l ðwÞ.</p><p>Next we show three typical examples that illustrate the occurrence of structured system of equations in approximate modeling problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Deconvolution</head><p>The convolution of the (scalar) sequences ð. . . ; a À1 ; a 0 ; a 1 ; . . .Þ and ð. . . ; x À1 ; x 0 ; x 1 ; . . .Þ is the sequence ð. . . ; b À1 ; b 0 ; b 1 ; . . .Þ defined as follows:</p><formula xml:id="formula_42">b i ¼ X 1 j¼À1 x j a iÀj . (CONV)</formula><p>Assume that x j ¼ 0 for all jo1 and for all j4n. Then (CONV) for i ¼ 1; . . . ; m can be written as the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel</head><p>Newton method following structured system of equations: Note that the matrix A is Toeplitz structured and is parameterized by the vector a ¼ colða 1Àn ; . . .</p><formula xml:id="formula_43">a 0 a À1 Á Á Á a 1Àn a 1 a 0 Á Á Á a 2Àn . . . . . . . . . a mÀ1 a mþnÀ2 Á Á Á a mÀn</formula><formula xml:id="formula_44">; a mÀ1 Þ 2 R mþnÀ1 .</formula><p>The aim of the deconvolution problem is to find x, given a and b. With exact data the problem boils down to solving the system of equations (CONV 0 ). By construction it has an exact solution. Moreover the solution is unique whenever A is of full column rank, which can be translated to a persistency of excitation condition on a, see <ref type="bibr" target="#b78">[79]</ref>.</p><p>The deconvolution problem is more realistic and more challenging when the data a; b are perturbed. We assume that m4n, so that the system of equations (CONV 0 ) is overdetermined. Because both a and b are perturbed and the A matrix is structured, the deconvolution problem is a total least-squares problem with structured data matrix C ¼ ½A b, A Toeplitz and b unstructured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Linear prediction</head><p>In , where m:¼T À l. Therefore, the Hankel matrix H lþ1 ðb yÞ with l þ 1 columns, constructed from b y is rank deficient. Conversely, if H lþ1 ðb yÞ has a onedimensional left kernel, then b y satisfies the linear recursion (LP). Therefore, the linear prediction problem is the problem of finding the smallest in some sense (e.g., 2-norm) correction Dy on the given sequence y d that makes a block-Hankel matrix H lþ1 ðb yÞ constructed from the corrected sequence b y:¼y d À Dy rank deficient. This is an structured total least-squares problem Ax % b with Hankel structured data matrix C ¼ ½A b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">EIV identification</head><p>Consider the linear time-invariant system described by the difference equation</p><formula xml:id="formula_45">b y t þ X l t¼1 a t b y tþt ¼ X l t¼0 b t b u tþt (DE)</formula><p>and define the parameter vector</p><p>x:¼colðb 0 ; . . . ; b l ; Àa 0 ; . . .</p><formula xml:id="formula_46">; Àa lÀ1 Þ 2 R 2lþ1 .</formula><p>Given a set of input/output data ðu d;1 ; y d;1 Þ; . . . ; ðu d;T ; y d;T Þ and an order specification l, we want to find the parameter x of a system that fits the data.</p><p>For a time horizon t ¼ 1; . . . ; T, (DE) can be written as the structured system of equations</p><formula xml:id="formula_47">(DE 0 )</formula><p>where m:¼T À l. We assume that the time horizon is large enough to ensure m42l þ 1. The system (DE 0 ) is satisfied for exact data and a solution is the true value of the parameter x. Moreover, under additional assumption on the input (persistency of excitation) the solution is unique.</p><p>For perturbed data an approximate solution is sought and the fact that the system of equation (DE 0 ) is structured suggests the use of the structured total least-squares method. Under appropriate conditions for the data generating mechanism an structured total least-squares solution provides a maximum likelihood estimator. The structure arising in the EIV identification problem is</p><formula xml:id="formula_48">C ¼ ½H &gt; l ðu d Þ H &gt; l ðy d Þ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">History of the structured total least-squares problem</head><p>The origin of the structured total least-squares problem dates back to the work of Aoki and Yue <ref type="bibr" target="#b79">[80]</ref>, although the name ''structured total leastsquares'' appeared only 23 years later in the literature <ref type="bibr" target="#b41">[42]</ref>. Aoki and Yue consider a single input single output system identification problem, where both the input and the output are noisy (EIV setting) and derive a maximum likelihood solution. Under the normality assumption for the measurement errors, a maximum likelihood estimate turns out to be a solution of the structured total leastsquares problem. Aoki and Yue approach the optimization problem in a similar way to the one presented in Section 5.3: they use classical nonlinear least-squares minimization methods for solving an equivalent unconstrained problem.</p><p>The structured total least-squares problem occurs frequently in signal processing applications. Cadzow <ref type="bibr" target="#b80">[81]</ref>, Bresler and Macovski <ref type="bibr" target="#b81">[82]</ref> propose heuristic solution methods that turn out to be suboptimal with respect to the ' 2 -optimality criterion, see Tufts and Shah <ref type="bibr" target="#b82">[83]</ref> and De Moor [61, Section V]. These methods, however, became popular because of their simplicity. For example, the method of Cadzow is an iterative method that alternates between unstructured low rank approximation and structure enforcement, thereby only requiring singular value decomposition computations and manipulation of the matrix entries.</p><p>Tufts and Shah propose in <ref type="bibr" target="#b82">[83]</ref>, a non-iterative method for Hankel structured total least-squares approximation that is based on perturbation analysis and provides nearly optimal solution for high signal-to-noise ratio (SNR). In a statistical setting, this method achieves the Cramer-Rao lower bound asymptotically as the SNR tends to infinity. Noniterative methods for solving the linear prediction problem (which, as shown in Section 5.1, is equivalent to Hankel structured total least-squares problem) are proposed by Tufts and Kumaresan in their seminal work <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b84">85]</ref>.</p><p>Abatzoglou et al. <ref type="bibr" target="#b40">[41]</ref> are considered to be the first who formulated a structured total least-squares problem. They called their approach constrained total least squares and motivate the problem as an extension of the total least-squares method to matrices with structure. The solution approach adopted by Abatzoglou et al. is closely related to the one of Aoki and Yue. Again an equivalent optimization problem is derived, but it is solved numerically using a Newton-type optimization method.</p><p>Shortly after the publication of the work on the constrained total least-squares problem, De Moor <ref type="bibr" target="#b41">[42]</ref> lists many applications of the structured total least-squares problem and outlines a new framework for deriving analytical properties and numerical methods. His approach is based on the Lagrange multipliers and the basic result is an equivalent problem, called Riemannian singular value decomposition, which can be considered as a ''nonlinear'' extension of the classical singular value decomposition. As an outcome of the new problem formulation, an iterative solution method based on the inverse power iteration is proposed.</p><p>Another algorithm for solving the structured total least-squares problem (even with ' 1 and ' 1 norm in the cost function), called structured total least norm, is proposed by Rosen et al. <ref type="bibr" target="#b85">[86]</ref>. In contrast to the approaches of Aoki The weighted low rank approximation framework of Manton et al. <ref type="bibr" target="#b77">[78]</ref> has been extended in <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b87">88]</ref> to structured low rank approximation problems. All problem formulations and solution methods cited above, except for the ones in the structured low rank approximation framework, aim at rank reduction of the data matrix C by one. A generalization of the algorithm of Rosen et al. to problems with rank reduction by more than one is proposed by Van Huffel et al. <ref type="bibr" target="#b88">[89]</ref>. It involves, however, Kronecker products that unnecessary inflate the dimension of the involved matrices.</p><p>When dealing with a general affine structure the constrained total least squares, Riemannian singular value decomposition, and structured total least norm methods have cubic computational complexity per iteration in the number of measurements. Fast algorithms with linear computational complexity are proposed by Mastronardi et al. <ref type="bibr" target="#b89">[90]</ref><ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref> for special structured total least-squares problems with data matrix C ¼ ½A b that is Hankel or composed of a Hankel block A and an unstructured column b. They use the structured total least norm approach but recognize that a matrix appearing in the kernel subproblem of the algorithm has low displacement rank. This structure is exploited using the Schur algorithm.</p><p>The structured total least-squares solution methods outlined above point out the following issues:</p><p>Structure: The structure specification for the data matrix C varies from general affine to specific affine, like Hankel/Toeplitz, or Hankel/ Toeplitz block augmented with an unstructured column. Rank reduction: All methods, except for <ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref>, reduce the rank of the data matrix by one. Computational efficiency: The efficiency varies from cubic for the methods that use a general affine structure to linear for the efficient methods of Lemmerling et al. <ref type="bibr" target="#b89">[90]</ref> and Mastronardi et al. <ref type="bibr" target="#b90">[91]</ref> that use a Hankel/Toeplitz type structure.</p><p>Efficient algorithms for problems with block-Hankel/Toeplitz structure and rank reduction with more than one are proposed by Markovsky et al. <ref type="bibr" target="#b92">[93]</ref><ref type="bibr" target="#b93">[94]</ref><ref type="bibr" target="#b94">[95]</ref>. In addition, a numerically reliable and robust software implementation is available <ref type="bibr" target="#b95">[96]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Structured total least-squares problem formulation and solution method</head><p>Let S : R n p ! R mÂðnþdÞ be an injective function. A matrix C 2 R mÂðnþdÞ is said to be S-structured if C 2 imageðSÞ. The vector p for which C ¼ SðpÞ is called the parameter vector of the structured matrix C. Respectively, R n p is called the parameter space of the structure S.</p><p>The aim of the structured total least-squares problem is to perturb as little as possible a given parameter vector p by a vector Dp, so that the perturbed structured matrix Sðp þ DpÞ becomes rank deficient with rank at most n.</p><p>Problem 14 (Structured total least squares). Given a data vector p 2 R n p , a structure specification S : R n p ! R mÂðnþdÞ , and a rank specification n, solve the optimization problem The structured total least-squares problem is said to be affine structured if the function S is affine, i.e., SðpÞ ¼ S 0 þ X n p i¼1 S i p i for all p 2 R n p and for some S i ; i ¼ 1; . . . ; n p . ðAFFÞ</p><p>In an affine structured total least-squares problem, the constraint Sðp À DpÞX ext ¼ 0 is bilinear in the decision variables X and Dp.</p><p>Lemma 15. Let S : R n p ! R mÂðnþdÞ be an affine function. Then</p><formula xml:id="formula_49">Sðp À DpÞX ext ¼ 0 3 GðX ÞDp ¼ rðX Þ, where GðX Þ:¼½vecððS 1 X ext Þ &gt; Þ ÁÁÁ vecððS n p X ext Þ &gt; Þ 2 R mdÂn p , (G) and rðX Þ:¼vecððSðpÞX ext Þ &gt; Þ 2 R md .</formula><p>Using Lemma 15, we rewrite the affine structured total least-squares problem as follows: The significance of Theorem 16 is that the constraint and the decision variable Dp in problem (STLS X ) are eliminated. Typically the number of elements nd in X is much smaller than the number of elements n p in the correction Dp. Thus the reduction in the complexity is significant.</p><p>The equivalent optimization problem (SRLS 00 X ) is a nonlinear least-squares problem, so that classical optimization methods can be used for its solution. The optimization methods require a cost function and first derivative evaluation. In order to evaluate the cost function for a given value of the argument X, we need to form the weight matrix GðX Þ and to solve the system of equations GðX ÞyðX Þ ¼ rðX Þ. This straightforward implementation requires Oðm 3 Þ floating point operation (flops). For large m (the applications that we aim at) this computational complexity becomes prohibitive.</p><p>It turns out, however, that for the special case of affine structures SðpÞ ¼ ½C 1 . . . C q for all p 2 R n p where C l ; for l ¼ 1; . . . ; q; is blockÀToeplitz; blockÀHankel, unstructured; or exact. ðAÞ the weight matrix GðX Þ has a block-Toeplitz and block-banded structure, which can be exploited for efficient cost function and first derivative evaluations. According to Assumption (A), SðpÞ is composed of blocks, each one of which is block-Toeplitz, block-Hankel, unstructured, or exact (an exact block C l is not modified in the solution b C:¼Sðp À DpÞ, i.e., b C l ¼ C l ).</p><p>Theorem 17 (Structure of the weight matrix G <ref type="bibr" target="#b92">[93]</ref>). Consider the equivalent optimization problem (STLS 0 X ). If in addition to the assumptions of Theorem 16, the structure S is such that (A) holds, then the weight matrix GðX Þ has the block-Toeplitz and block-banded structure: , where s ¼ max l¼1;...;q ðn l À 1Þ and n l is the number of block columns in the block C l .</p><formula xml:id="formula_50">GðX Þ ¼ C 0 C &gt; 1 Á Á Á C &gt; s 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We reviewed the development and extensions of the classical total least-squares problem and presented a new total least-squares problem formulation. The new formulation is a matrix low rank approximation problem and allows for different representations of the rank constraint. Once a representation is fixed the matrix low rank approximation problem becomes a parameter optimization problem. The classical total least-squares formulation results from the new one when an input/output representation is chosen. The input/output representation is a linear system of equations AX ¼ B, which is the classical way of addressing approximation problems. However, the input/output representation is not equivalent to the low rank constraint, which leads to non-generic total leastsquares problems. Using the representation-free formulation, we classified existing total leastsquares solution methods. The existing methods differ in the representation and the optimization method used.</p><p>The basic and generalized total least-squares problems have an analytic solution in terms of the singular value decomposition of the data matrix, which allows fast and reliable computation of the solution. Moreover, all globally optimal solutions can be classified in terms of the singular value decomposition. In contrast, more general total least-squares problems like the weighted and structured total least-squares problems require numerical optimization methods, which at best find a single locally optimal solution. The separation between the global total least-squares problem and general weighted and structured total least-squares problems is an important dividing line in the total least-squares hierarchy.</p><p>We emphasized the double minimization structure of the total least-squares problems and showed how it can be used for deriving efficient solution methods. The key step in our approach is the elimination of the correction by analytically minimizing over it. Then the structure of the data and weight matrices are exploited for efficient cost function and first derivative evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>b</head><label></label><figDesc>C tls :¼ arg min b C kC À b Ck F subject to rankð b CÞpn,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>NomenclatureR</head><label></label><figDesc>and R þ the set of real numbers and nonnegative real numbers :¼ and : 3 left-hand side is defined by the righthand side ¼: and 3: right-hand side is defined by the lefthand side vec column-wise vectorization of a matrix C, DC, b C data, correction, and approximation matrices C ¼ ½A B input/output partitioning of the data c 1 ; . . . ; c m observations, ½c 1 Á Á Á c m ¼ C &gt; c ¼ colða; bÞ the column vector c ¼ ½ a b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1, we give an interpretation of the linear system of equations b AX ¼ b B as an input/output representation of a linear static model. Apart from b AX ¼ b B with b C ¼ ½ b A b B, there are numerous other ways to represent the rank constraint rankð b CÞpn. For example, b AX ¼ b B with b CP ¼ ½ b A b B, where P is an arbitrary permutation matrix, i.e., in (TLS2) we can choose to express any d columns of b C as a linear combination of the remaining columns in order to ensure rank deficiency of b C. Any a priori fixed selection, however, leads to non-generic problems and therefore will be inadequate in certain cases. Of special importance are the kernel representation R b C &gt; ¼ 0, where RR &gt; ¼ I d , and the image representation b C &gt; ¼ PL, where P 2 R ðnþdÞÂn , L 2 R nÂm . In contrast to the input/output representations, the kernel and image representations are equivalent to rankð b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Least-squares and total least-squares fits of a set of m ¼ 20 data points in the plane. -data points ½a i b i , Â-approximations ½b a i b b i , solid line-fitting model b a b x ¼ b b, dashed lines-approximation errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Links among kernel, image, and input/output representations of B 2 L n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Problem 7 (</head><label>7</label><figDesc>Generalized total least squares). Given a data matrix C 2 R mÂðnþdÞ , positive definite weight matrices W ' and W r , and a complexity specification n, solve the optimization problem f b B gtls ; b C gtls g ¼ arg min b B2L n M gtls ðC; BÞ. (GTLS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Problem 9 (</head><label>9</label><figDesc>Weighted total least squares). Given a data matrix C 2 R mÂðnþdÞ , a positive definite weight matrix W, and a complexity specification n, solve the optimization problem f b B wtls ; b C wtls g:¼ arg min B2L n M wtls ðC; BÞ. (WTLS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, Yue and Abatzoglou et al., Rosen et al. solve the problem in its original formulation. The constraint is linearized around the current iteration point, which results in a linearly constrained least-squares problem. In the algorithm of Rosen et al., the constraint is incorporated in the cost function by adding a multiple of its residual norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Dp stls ¼ arg min Dp kDpk subject to rankðSðp À DpÞÞpn.In what follows, we will use the input/output representationSðp À DpÞX ext ¼ 0; X ext :¼ X ÀIof the rank constraint, so that the structured total least-squares problem becomes the following parameter optimization problem b X stls ¼ arg min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>inner minimization problem has an analytic solution, which allows to derive an equivalent optimization problem.Theorem 16 (Equivalent optimization problem for affine structured total least squares). Assuming that n p Xmd, the affine structured total least squares problem (STLS X ) is equivalent tomin X r &gt; ðX ÞG y ðX ÞrðX Þ where GðX Þ:¼GðX ÞG &gt; ðX Þ, (STLS 00 X )and G y is the pseudoinverse of G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1. Basic total least-squares algorithm.</figDesc><table><row><cell>Input: A 2 R mÂn and B 2 R mÂd .</cell></row><row><cell>1: Compute the singular value decomposition</cell></row><row><cell>½A B ¼ USV &gt; .</cell></row><row><cell>2: if V 22 is non-singular then</cell></row><row><cell>3: Set b X tls ¼ ÀV 12 V À1 22 .</cell></row><row><cell>4: else</cell></row><row><cell>5: Output a message that the problem (TLS1) has</cell></row><row><cell>no solution and stop.</cell></row><row><cell>end if</cell></row><row><cell>Output: b X tls -a total least-squares solution of</cell></row><row><cell>AX % B.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and the corresponding generalized total least-squares misfit is M gtls ðC; B gtls Þ ¼ M tls ðC m ; B m;tls Þ. A generalized total least-squares solution always exists. It is unique if and only if b B m;tls is unique.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The special case when the weight matrix W is diagonal is called element-wise weighted total least squares. It corresponds to an EIV problem with uncorrelated measurement errors. Let W ¼ diagðw 1 ; . . . ; w mðnþdÞ Þ and define the m Â ðn þ dÞ weight matrix S by S ij :¼w ðiÀ1ÞðnþdÞþj . Denote by the element-wise product A B ¼ ½a ij b ij . Then kDCk W ¼ kS DCk F .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>Model representations and optimization algorithms used in the methods of<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78]</ref> </figDesc><table><row><cell>Method</cell><cell>Representation</cell><cell>Algorithm</cell></row><row><cell>Riemannian</cell><cell>Kernel</cell><cell>Inverse power</cell></row><row><cell>singular value</cell><cell></cell><cell>iteration</cell></row><row><cell>decomposition</cell><cell></cell><cell></cell></row><row><cell>Maximum</cell><cell>Image</cell><cell>Alternating</cell></row><row><cell>likelihood</cell><cell></cell><cell>projections</cell></row><row><cell>principle</cell><cell></cell><cell></cell></row><row><cell>component</cell><cell></cell><cell></cell></row><row><cell>analysis</cell><cell></cell><cell></cell></row><row><cell>Premoli-Rastello</cell><cell>Input/output</cell><cell>Iteration based on</cell></row><row><cell></cell><cell></cell><cell>heuristic</cell></row><row><cell></cell><cell></cell><cell>linearization</cell></row><row><cell>Weighted low rank</cell><cell></cell><cell></cell></row><row><cell>approximation</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Àlþ1 and parameters fa i g l i¼1 of (LP) and the parameters fc i ; d i ; o i ; f i g l</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>difference equation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>b y t þ</cell><cell>X l t¼1</cell><cell>a t b y tþt ¼ 0.</cell><cell>(LP)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>y that satisfies (LP) Approximating y d by a signal b is a linear prediction problem, so modeling y d</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>as a sum of damped exponentials is equivalent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to the linear prediction problem. Of course, there</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>is a one-to-one relation between the initial</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>conditions b y 0 ; . . . ; b y i¼1</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(SDE).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>For a time horizon t ¼ 1; . . . ; T, with T4l þ 1,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(LP) can be written as the structured system of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>equations</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 6 6 6 6 6 4</cell><cell>b y 1 b y 2 . . . b y m b b y 2 b y 3 . . . y mþ1 Á Á Á b . . . . . . b b y l y lþ1 . . . y TÀ1</cell><cell>3 7 7 7 7 7 5</cell><cell>2 6 6 6 6 4</cell><cell>a 1 a 2 . . . a l</cell><cell>3 7 7 7 7 5</cell><cell>¼ À</cell><cell>2 6 6 6 6 6 4</cell><cell>b y lþ1 b y lþ2 . . . b y T</cell><cell>3 7 7 7 7 7 5</cell></row><row><cell></cell><cell cols="10">many signal processing applications the sum of</cell></row><row><cell cols="10">damped exponentials model</cell></row><row><cell>b y t ¼</cell><cell cols="2">X l</cell><cell cols="7">c i e d i t e iðo i tþf i Þ where i:¼</cell><cell>p ffiffiffiffiffiffi ffi À1</cell><cell>(SDE)</cell></row><row><cell></cell><cell cols="2">i¼1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">is considered. Given an observed sequence</cell></row><row><cell cols="11">ðy d;1 ; . . . ; y d;T Þ (''d'' stands for data), the aim is to find parameters fc i ; d i ; o i ; f i g l i¼1 of a sum of damped y given by exponentials model, such that the signal b (SDE) is close to the observed one, e.g.,</cell></row><row><cell>min</cell><cell>2 6 6 6 4</cell><cell cols="2">y d;1 . . . y d;T</cell><cell>3 7 7 7 5</cell><cell>À</cell><cell>2 6 6 4</cell><cell>b y 1 . . . y T b</cell><cell>3 7 7 5</cell><cell>.</cell></row><row><cell cols="11">Note that the sum of damped exponentials model</cell></row><row><cell cols="11">is just an autonomous linear time-invariant model,</cell></row><row><cell cols="11">y is a free response of an linear time-invariant i.e., b system. Therefore b y satisfies a homogeneous linear</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>I. Markovsky, S. Van Huffel / Signal Processing 87 (2007) 2283-2302</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I. Markovsky  is a lecturer at the University of Southampton, UK, and S. Van Huffel is a full professor at the Katholieke Universiteit Leuven, Belgium. Our research is supported by Research Council KUL: GOA-AMBioRICS, GOA-Mefisto 666, Center of Excellence EF/05/006 ''Optimization in engineering'', several PhD/postdoc and fellow grants; Flemish Government: FWO: PhD/postdoc grants, projects, G.0360.05 (EEG signal processing), G.0321.06 (numerical tensor techniques), research communities (ICCoS, ANMMM); IWT: PhD Grants; Belgian Federal Science Policy Office IUAP P5/22 ('Dynamical Systems and Control: Computation, Identification and Modelling'); EU: BIOPATTERN, ETUMOUR; HEALTHagents; HPC-EUROPA (RII3-CT-2003-506079), with the support of the European Community-Research Infrastructure Action under the FP6 ''Structuring the European Research Area'' Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some modified matrix eigenvalue problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="318" to="344" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of the total least squares problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="883" to="893" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From time series to linear system-Part I. Finite dimensional linear time invariant systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part II. Exact modelling, Part III. Approximate modelling</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="87" to="115" />
			<date type="published" when="1986">1986. 1986. 1987</date>
		</imprint>
	</monogr>
	<note>Automatica</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact and approximate modeling of linear systems: a behavioral approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monographs on Mathematical Modeling and Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2006">2006</date>
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Adcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Note on the method of least squares</title>
		<imprint>
			<date type="published" when="1877">1877</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="183" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A problem in least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analyst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="53" to="54" />
			<date type="published" when="1878">1878</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Mag</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linear Regression Analysis of Economic Time Series</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koopmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1937">1937</date>
			<publisher>De Erven F. Bohn</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The fitting of straight lines when both variables are subject to error</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="173" to="205" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Least squares fitting of a straight line</title>
		<author>
			<persName><forename type="first">D</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1079" to="1086" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Models in Regression and Related Topics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sprent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Methuen &amp; Co. Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation in a multivariate &apos;&apos;errors in variables&apos;&apos; regression model: large sample results</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gleser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="44" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Concepts for reliable modelling of linear systems with application to on-line identification of multivariable state space descriptions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Staar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<pubPlace>Leuven, Belgium</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department EE, K.U</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis and solution of the nongeneric total least squares problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="360" to="372" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparison of some frequency response function measurement techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leuridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Vis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Auweraer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lembregts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Modal Analysis Conference</title>
		<meeting>the Fourth International Modal Analysis Conference</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of a system pulse transfer function in the presence of noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="229" to="235" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identification of linear systems with input and output noise: the Koopmans-Levin method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nicholson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proc. D</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="30" to="36" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bias correction in least-squares identification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>So¨derstro¨m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="457" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the angles of arrival of multiple plane waves</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tufts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerospace Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="139" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The equivalence of the total leastsquares and minimum norm methods</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Degroat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1891" to="1892" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum likelihood principle component analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wentzell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemometrics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="339" to="366" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the equivalence between total least squares and maximum likelihood PCA</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wentzell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Chim. Acta</title>
		<imprint>
			<biblScope unit="volume">544</biblScope>
			<biblScope unit="page" from="254" to="267" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Total Least Squares Problem: Computational Aspects and Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<title level="m">Recent Advances in Total Least Squares Techniques and Errors-in-Variables Modeling</title>
		<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">Total Least Squares and Errors-in-variables Modeling: Analysis, Algorithms and Applications</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Core problems in linear algebraic systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Strakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="875" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Fuller</surname></persName>
		</author>
		<title level="m">Measurement error Models</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The data least squares problem and channel equalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Degroat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="407" to="411" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analysis and properties of the generalized total least squares problem AX % B when some or all columns in A are subject to error</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="294" to="315" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The restricted total least squares problem: formulation, algorithm and properties</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="309" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularization by truncated total least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1223" to="1241" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tikhonov regularization and total least squares</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized total least squares based on quadratic eigenvalue problem solvers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numer. Math</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="793" to="812" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Appropriate cross-validation for regularized errors-in-variables linear models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COMPSTAT 2004 Symposium</title>
		<meeting>the COMPSTAT 2004 Symposium<address><addrLine>Prague, Czech Republic; Wurzburg, Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-08">August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the solution of the Tikhonov regularization of the total least squares</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="118" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust solutions to least-squares problems with uncertain data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lebret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1035" to="1064" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parameter estimation in the presence of bounded data uncertainties</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="235" to="252" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Consistency of elementwiseweighted total least squares estimator in a multivariate errors-in-variables model AX ¼ B</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kukush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metrika</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A unitarily constrained total least-squares problem in signal-processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="729" to="745" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The constrained total least squares technique and its application to harmonic superresolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Abatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1070" to="1087" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Structured total least squares and L 2 approximation problems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="page" from="163" to="207" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Consistency of the structured total least squares estimator in a multivariate errors-in-variables model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kukush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Plann. Inference</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="358" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A global solution for the structured total least squares problem with block circulant matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="255" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Signal restoration via the regularized constrained total least squares</title>
		<author>
			<persName><forename type="first">N</forename><surname>Younan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast regularized structured total least squares algorithm for solving the basic deconvolution problem, Numer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Regularized constrained total least squares image restoration</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mesarovic´</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A new approach to constrained total least squares image restoration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pimentel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="237" to="258" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Constrained total least squares computations for high resolution image reconstruction with multisensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Systems Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structured total least norm for nonlinear problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="30" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The structured total least squares approach for nonlinearly structured matrices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="321" to="332" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The role of total least squares in motion analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mu¨hlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</editor>
		<meeting>the Fifth European Conference on Computer Vision<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a regularized structured total least norm algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pruessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1018" to="1037" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Implementation of the regularized structured total least squares algorithms for blind image deblurring</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="203" to="221" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A regularized structured total least squares algorithm for high-resolution image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient implementation of a structured total least squares based speech compression method</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="295" to="315" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Perceptual audio modeling with exponentially damped sinusoids</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hermus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Verhelst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wambacq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="163" to="176" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Frequency-domain generalized total leastsquares identification for modal analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verboven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cauberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanlanduit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sound Vib</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="21" to="38" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multiple delays estimation for chirp signals using structured total least squares</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yeredor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="261" to="286" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Total linear least squares and the algebraic Riccati equation</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="329" to="337" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Total least squares for affinely structured matrices and the noisy realization problem</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3104" to="3113" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Global total least squares modeling of multivariate time series</title>
		<author>
			<persName><forename type="first">B</forename><surname>Roorda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="63" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Misfit versus latency</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2057" to="2067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rolain, Analyses, development, and applications of TLS algorithms in frequency domain system identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pintelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vandersteen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="983" to="1004" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Application of structured total least squares for system identification and model reduction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pintelon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1490" to="1500" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multivariate orthogonal regression in astronomy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Branham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Celestial Mech. Dyn. Astron</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="251" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Errors-in-variables methods in system identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>So¨derstro¨m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IFAC Symposium on System Identification</title>
		<meeting><address><addrLine>Newcastle, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improved Lanczos algorithms for blackbox MRS data quantitation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Laudadio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanhamme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Hecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Res</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="292" to="297" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Subspace-based MRS data quantitation of multiplets using prior knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Laudadio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Selen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanhamme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Hecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Res</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Fierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lanczos and the Riemannian SVD in information retrieval applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The use of total least squares data fitting in the shape from moments problem, Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1109" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Computing approximate GCD of univariate polynomials by structure total least norm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Sin</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="375" to="387" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An algorithm for approximate common divisor computation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Symposium on Mathematical Theory of Networks and Systems</title>
		<meeting>the 17th Symposium on Mathematical Theory of Networks and Systems<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The total least squares technique: computation, properties and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SVD and Signal Processing: Algorithms, Applications and Architectures</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Deprettere</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The parametric quadratic form method for solving TLS problems with elementwise weighting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Premoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Rastello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Total Least Squares Techniques and Errors-in-Variables Modeling: Analysis, Algorithms and Applications</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The element-wise weighted total least squares problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Rastello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Premoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kukush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="209" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The geometry of weighted low-rank approximations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="500" to="514" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A note on persistency of excitation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rapisarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Lett</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="329" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">On a priori error estimates of some identification methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="541" to="548" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Signal enhancement-a composite property mapping algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cadzow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exact maximum likelihood parameter estimation of superimposed exponential signals in noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Macovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Estimation of a signal waveform from noisy data using low-rank approximation to a data matrix</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tufts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1716" to="1721" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Estimation of frequencies of multiple sinusoids: making linear prediction perform like maximum likelihood</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tufts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="975" to="989" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Estimating the parameters of exponentially damped sinusoids and pole-zero modeling in noise</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tufts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="833" to="840" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Total least norm formulation and solution of structured problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="110" to="126" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Structured weighted low rank approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="609" to="618" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Block-row hankel weighted low rank approximation, Numer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Formulation and solution of structured total least norm problems for parameter estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2474" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Fast algorithm for solving the Hankel/Toeplitz structured total least squares problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Algorithms</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="371" to="392" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fast structured total least squares algorithm for solving the basic deconvolution problem</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemmerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="533" to="553" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Fast and reliable algorithms for structured total least squares and related matrix problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mastronardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>ESAT/SISTA, K.U. Leuven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Block-Toeplitz/ Hankel structured total least squares</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pintelon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1083" to="1099" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">On the computation of the structured total least squares estimator</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kukush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="591" to="608" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On weighted structured total least squares</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-Scale Scientific Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Lirkov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Margenov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Was´niewski</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3743</biblScope>
			<biblScope unit="page" from="695" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">High-performance numerical algorithms and software for structured total least squares</title>
		<author>
			<persName><forename type="first">I</forename><surname>Markovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
