<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Network Anomaly Detection via Cross-network Meta-learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-22">22 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
							<email>kaize.ding@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
							<email>qinghai2@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
							<email>htong@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huan.liu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Network Anomaly Detection via Cross-network Meta-learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-22">22 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449922</idno>
					<idno type="arXiv">arXiv:2102.11165v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network anomaly detection aims to find network elements (e.g., nodes, edges, subgraphs) with significantly different behaviors from the vast majority. It has a profound impact in a variety of applications ranging from finance, healthcare to social network analysis. Due to the unbearable labeling cost, existing methods are predominately developed in an unsupervised manner. Nonetheless, the anomalies they identify may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies of interest. Hence, it is critical to investigate and develop few-shot learning for network anomaly detection. In real-world scenarios, few labeled anomalies are also easy to be accessed on similar networks from the same domain as of the target network, while most of the existing works omit to leverage them and merely focus on a single network. Taking advantage of this potential, in this work, we tackle the problem of few-shot network anomaly detection by (1) proposing a new family of graph neural networks -Graph Deviation Networks (GDN) that can leverage a small number of labeled anomalies for enforcing statistically significant deviations between abnormal and normal nodes on a network; (2) equipping the proposed GDN with a new cross-network meta-learning algorithm to realize few-shot network anomaly detection by transferring metaknowledge from multiple auxiliary networks. Extensive evaluations demonstrate the efficacy of the proposed approach on few-shot or even one-shot network anomaly detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Network-structured data, ranging from social networks <ref type="bibr" target="#b44">[45]</ref> to team collaboration networks <ref type="bibr" target="#b52">[53]</ref>, from citation networks <ref type="bibr" target="#b31">[32]</ref> to molecular graphs <ref type="bibr" target="#b43">[44]</ref>, has been widely used in modeling a myriad of real-world systems. Nonetheless, real-world networks are commonly contaminated with a small portion of nodes, namely,  existing methods may easily fail to distinguish them from normal nodes in the latent representation space with only few labeled anomalies, (b) while they can be well separated in an anomaly score space by enforcing statistically significant deviations between abnormal and normal nodes.</p><p>anomalies 1 , whose patterns significantly deviate from the vast majority of nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b49">50]</ref>. For instance, in a citation network that represents citation relations between papers, there are some research papers with a few spurious references (i.e., edges) which do not comply with the content of the papers <ref type="bibr" target="#b1">[2]</ref>; In a social network that represents friendship of users, there may exist camouflaged users who randomly follow different users, rendering properties like homophily not applicable to this type of relationships <ref type="bibr" target="#b8">[9]</ref>. As the existence of even few abnormal instances could cause extremely detrimental effects, the problem of network anomaly detection has received much attention in industry and academy alike.</p><p>Due to the fact that labeling anomalies is highly labor-intensive and takes specialized domain-knowledge, existing methods are predominately developed in an unsupervised manner. As a prevailing paradigm, people try to measure the abnormality of nodes with the reconstruction errors of autoencoder-based models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> or the residuals of matrix factorization-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>. However, the anomalies they identify may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies of interest. A potential solution to this problem is to leverage limited or few labeled anomalies as the prior knowledge to learn anomaly-informed models, since it is relatively low-cost in real-world scenarios -a small set of labeled anomalies could be either from a deployed detection system or be provided by user feedback. In the meantime, such valuable knowledge is usually scattered among other networks within the same domain of the target one, which could be further exploited for distilling supervised signal. For example, LinkedIn and Indeed have similar social networks that represent user friendship in the job-search domain; ACM and DBLP can be treated as citation networks that share similar citation relations in the computer science domain. According to previous studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, because of the similarity of topological structure and nodal attributes, it is feasible to transfer valuable knowledge from source network(s) to the target network so that the performance on the target one is elevated. As such, in this work we propose to investigate the novel problem of few-shot network anomaly detection under the cross-network setting.</p><p>Nonetheless, solving this under-explored problem remains nontrivial, mainly owing to the following reasons: (1) From the micro (intra-network) view, since we only have limited knowledge of anomalies, it is hard to precisely characterize the abnormal patterns. If we directly adopt existing semi-supervised <ref type="bibr" target="#b36">[37]</ref> or PU <ref type="bibr" target="#b39">[40]</ref> learning techniques, those methods often fall short in achieving satisfactory results as they might still require a relatively large percentage of positive examples <ref type="bibr" target="#b22">[23]</ref>. To handle such incomplete supervision challenge <ref type="bibr" target="#b46">[47]</ref> as illustrated in Figure <ref type="figure" target="#fig_1">1</ref>(a), instead of focusing on abnormal nodes, how to leverage labeled anomalies as few as possible to learn a high-level abstraction of normal patterns is necessary to be explored; (2) From the macro (inter-network) view, though networks in the same domain might share similar characteristics in general, anomalies exist in different networks may be from very different manifolds. Previous studies on cross-network learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> mostly focus on transferring the knowledge only from a single network, which may cause unstable results and the risk of negative transfer. As learning from multiple networks could provide more comprehensive knowledge about the characteristics of anomalies, a cross-network learning algorithm that is capable of adapting the knowledge is highly desirable.</p><p>To address the aforementioned challenges, in this work we first design a new GNN architecture, namely Graph Deviation Networks (GDN), to enable network anomaly detection with limited labeled data. Specifically, given an arbitrary network, GDN first uses a GNN-backboned anomaly score learner to assign each node with an anomaly score, and then defines the mean of the anomaly scores based on a prior probability to serve as a reference score for guiding the subsequent anomaly score learning. By leveraging a deviation loss <ref type="bibr" target="#b22">[23]</ref>, GDN is able to enforce statistically significant deviations of the anomaly scores of anomalies from that of normal nodes in the anomaly score space (as shown in Figure <ref type="figure" target="#fig_1">1</ref>(b)). To further transfer this ability from multiple networks to the target one, we propose a cross-network meta-learning algorithm to learn a well-generalized initialization of GDN from multiple few-shot network anomaly detection tasks. The seamlessly integrated framework Meta-GDN is capable of extracting comprehensive meta-knowledge for detecting anomalies across multiple networks, which largely alleviates the limitations of transferring from a single network. Subsequently, the initialization can be easily adapted to a target network via fine-tuning with few or even one labeled anomaly, improving the anomaly detection performance on the target network to a large extent. To summarize, our main contributions is three-fold:</p><p>â€¢ Problem: To the best of knowledge, we are the first to investigate the novel problem of few-shot network anomaly detection. Remarkably, we propose to solve this problem by transferring the knowledge across multiple networks.</p><p>â€¢ Algorithms: We propose a principled framework Meta-GDN, which integrates a new family of graph neural networks (i.e., GDN) and cross-network meta-learning to detect anomalies with few labeled instances. â€¢ Evaluations: We perform extensive experiments to corroborate the effectiveness of our approach. The experimental results demonstrate the superior performance of Meta-GNN over the state-of-the-art methods on network anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review the related work in terms of (1) network anomaly detection; and (2) graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Anomaly Detection</head><p>Network anomaly detection methods have a specific focus on the network structured data. Previous research mostly study the problem of anomaly detection on plain networks. As network structure is the only available information modality in a plain network, this category of anomaly detection methods try to exploit the network structure information to spot anomalies from different perspectives <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>. For instance, SCAN <ref type="bibr" target="#b42">[43]</ref> is one of the first methods that target to find structural anomalies in networks. In recent days, attributed networks have been widely used to model a wide range of complex systems due to their superior capacity for handling data heterogeneity. In addition to the observed node-to-node interactions, attributed networks also encode a rich set of features for each node. Therefore, anomaly detection on attributed networks has drawn increasing research attention in the community, and various methods have been proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. Among them, ConOut <ref type="bibr" target="#b21">[22]</ref> identifies the local context for each node and performs anomaly ranking within the local context. More recently, researchers also propose to solve the problem of network anomaly detection using graph neural networks due to its strong modeling power. DOMINANT <ref type="bibr" target="#b5">[6]</ref> achieves superior performance over other shallow methods by building a deep autoencoder architecture on top of the graph convolutional networks. Semi-GNN <ref type="bibr" target="#b36">[37]</ref> is a semisupervised graph neural model which adopts hierarchical attention to model the multi-view graph for fraud detection. GAS <ref type="bibr" target="#b15">[16]</ref> is a GCN-based large-scale anti-spam method for detecting spam advertisements. Zhao et al. propose a novel loss function to train GNNs for anomaly-detectable node representations <ref type="bibr" target="#b47">[48]</ref>. Apart from the aforementioned methods, our approach focus on detecting anomalies on a target network with few labels by learning from multiple auxiliary networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Graph neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref> have achieved groundbreaking success in transforming the information of a graph into lowdimensional latent representations. Originally inspired by graph spectral theory, spectral-based graph convolutional networks (GCNs) have emerged and demonstrated their efficacy by designing different graph convolutional layers. Among them, The model proposed by Kipf et al. <ref type="bibr" target="#b13">[14]</ref> has become the most prevailing one by using a linear filter. In addition to spectral-based graph convolution models, spatial-based graph neural networks that follow neighborhoods aggregation schemes also have been extensively investigated. Instead of training individual embeddings for each node, those methods learn a set of aggregator functions to aggregate features from a node's local neighborhood. GraphSAGE <ref type="bibr" target="#b11">[12]</ref> learns an embedding function that can be generalized to unseen nodes, which enables inductive representation learning on networkstructured data. Similarly, Graph Attention Networks (GATs) <ref type="bibr" target="#b34">[35]</ref> proposes to learn hidden representations by introducing a selfattention strategy when aggregating neighborhood information of a node. Furthermore, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b41">[42]</ref> extends the idea of parameterizing universal multiset functions with neural networks, and is proven to be as theoretically powerful as the Weisfeiler-Lehman (WL) graph isomorphism test. To go beyond a single graph and transfer the knowledge across multiple ones, more recently, researchers have explored to integrate GNNs with meta-learning techniques <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref>. For instance, PA-GNN <ref type="bibr" target="#b32">[33]</ref> transfers the robustness from cleaned graphs to the target graph via meta-optimization. Meta-NA <ref type="bibr" target="#b50">[51]</ref> is a graph alignment model that learns a unified metric space across multiple graphs, where one can easily link entities across different graphs. However, those efforts cannot be applied to our problem and we are the first to study the problem of few-shot cross-network anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In this section, we formally define the problem of few-shot crossnetwork anomaly detection. Throughout the paper, we use bold uppercase letters for matrices (e.g., A), bold lowercase letters for vectors (e.g., u), lowercase letters for scalars (e.g., ğ‘ ) and calligraphic fonts to denote sets (e.g., V). Notably, in this work we focus on attributed network for a more general purpose. Given an attributed network G = (V, E, X) where V is the set of nodes, i.e., {ğ‘£ 1 , ğ‘£ 2 , . . . , ğ‘£ ğ‘› }, E denotes the set of edges, i.e., {ğ‘’ 1 , ğ‘’ 2 , . . . , ğ‘’ ğ‘š }. The node attributes are represented by</p><formula xml:id="formula_0">X = [x T 1 , x T 2 , â€¢ â€¢ â€¢ , x T ğ‘› ] âˆˆ R ğ‘›Ã—ğ‘‘ and</formula><p>x ğ‘– is the attribute vector for node ğ‘£ ğ‘– . More concretely, we represent the attributed network as G = (A, X), where A = {0, 1} ğ‘›Ã—ğ‘› is an adjacency matrix representing the network structure. Specifically, A ğ‘–,ğ‘— = 1 indicates that there is an edge between node ğ‘£ ğ‘– and node ğ‘£ ğ‘— ; otherwise, A ğ‘–,ğ‘— = 0.</p><p>Generally speaking, few-shot cross-network anomaly detection aims to maximally improve the detection performance on the target network through transferring very limited supervised knowledge of ground-truth anomalies from the auxiliary network(s). In addition to the target network G ğ‘¡ , in this work we assume there exist ğ‘ƒ auxiliary networks</p><formula xml:id="formula_1">G ğ‘  = {G ğ‘  1 , G ğ‘  2 , .</formula><p>. . , G ğ‘  ğ‘ƒ } sharing the same or similar domain with G ğ‘¡ . For an attributed network, the set of labeled abnormal nodes is denoted as V ğ¿ and the set of unlabeled nodes is represented as V ğ‘ˆ . Note that V = {V ğ¿ , V ğ‘ˆ } and in our problem |V ğ¿ |â‰ª |V ğ‘ˆ | since only few-shot labeled data is given. As network anomaly detection is commonly formulated as a ranking problem <ref type="bibr" target="#b0">[1]</ref>, we formally define the few-shot cross-network anomaly detection problem as follows:</p><p>Problem 1. Few-shot Cross-network Anomaly Detection Given: ğ‘ƒ auxiliary networks, i.e.,</p><formula xml:id="formula_2">G ğ‘  = {G ğ‘  1 = (A ğ‘  1 , X ğ‘  1 ), G ğ‘  2 = (A ğ‘  2 , X ğ‘  2 ), . . . , G ğ‘  ğ‘ƒ = (A ğ‘  ğ‘ƒ , X ğ‘  ğ‘ƒ )</formula><p>} and a target network G ğ‘¡ = (A ğ‘¡ , X ğ‘¡ ), each of which contains a set of few-shot labeled anomalies (i.e., V ğ¿ 1 , V ğ¿ 2 , . . . , V ğ¿ ğ‘ƒ and V ğ¿ ğ‘¡ ). Goal: to learn an anomaly detection model, which is capable of leveraging the knowledge of ground-truth anomalies from the multiple auxiliary networks, i.e., {G ğ‘  1 , G ğ‘  2 , . . . , G ğ‘  ğ‘ƒ }, to detect abnormal nodes in the target network G ğ‘¡ . Ideally, anomalies that are detected should have higher ranking scores than that of the normal nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED APPROACH</head><p>In this section, we introduce the details of the proposed framework -Meta-GDN for few-shot network anomaly detection. Specifically, Meta-GDN addresses the discussed challenges with the following two key contributions: (1) Graph Deviation Networks (GDN), a new family of graph neural networks that enable anomaly detection on an arbitrary individual network with limited labeled data; and (2) a cross-network meta-learning algorithm, which empowers GDN to transfer meta-knowledge across multiple auxiliary networks to enable few-shot anomaly detection on the target network. An overview of the proposed Meta-GDN is provided in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Deviation Networks</head><p>To enable anomaly detection on an arbitrary network with few-shot labeled data, we first propose a new family of graph neural networks, called Graph Deviation Network (GDN). In essence, GDN is composed of three key building blocks, including (1) a network encoder for learning node representations; (2) an abnormality valuator for estimating the anomaly score for each node; and (3) a deviation loss for optimizing the model with few-shot labeled anomalies. The details are as follows: Network Encoder. In order to learn expressive nodes representations from an input network, we first build the network encoder module. Specifically, it is built with multiple GNN layers that encode each node to a low-dimensional latent representation. In general, GNNs follow the neighborhood message-passing mechanism, and compute the node representations by aggregating features from local neighborhoods in an iterative manner. Formally, a generic GNN layer computes the node representations using two key functions:</p><formula xml:id="formula_3">h ğ‘™ N ğ‘– = Aggregate ğ‘™ {h ğ‘™âˆ’1 ğ‘— |âˆ€ğ‘— âˆˆ N ğ‘– âˆª ğ‘£ ğ‘– } , h ğ‘™ ğ‘– = Transform ğ‘™ h ğ‘™âˆ’1 ğ‘– , h ğ‘™ N ğ‘– ,<label>(1)</label></formula><p>where h ğ‘™ ğ‘– is the latent representation of node ğ‘£ ğ‘– at the ğ‘™-th layer and N ğ‘– is the set of first-order neighboring nodes of node ğ‘£ ğ‘– . Notably, Aggregate(â€¢) is an aggregation function that aggregates messages from neighboring nodes and Transform(â€¢) computes the new representation of a node according to its previous-layer representation and the aggregated messages from neighbors.</p><p>To capture the long-range node dependencies in the network, we stack multiple GNN layers in the network encoder. Thus, the network encoder can be represented by:</p><formula xml:id="formula_4">H 1 = GNN 1 (A, X), . . . Z = GNN ğ¿ (A, H ğ¿âˆ’1 ), (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where Z is the learned node representations from the network encoder. For simplicity, we use a parameterized function ğ‘“ ğœ½ ğ‘’ (â€¢) to denote the network encoder with ğ¿ GNN layers throughout the paper. It is worth noting that the network encoder is compatible with arbitrary GNN-based architecture <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, and here we employ Simple Graph Convolution (SGC) <ref type="bibr" target="#b38">[39]</ref> in our implementation. Abnormality Valuator. Afterwards, the learned node representations from the network encoder will be passed to the abnormality valuator ğ‘“ ğœ½ ğ‘  (â€¢) for further estimating the abnormality of each node. Specifically, the abnormality valuator is built with two feed-forward layers that transform the intermediate node representations to scalar anomaly scores:</p><formula xml:id="formula_6">o ğ‘– = ReLU(W ğ‘  z ğ‘– + b ğ‘  ), ğ‘  ğ‘– = u T ğ‘  o ğ‘– + ğ‘ ğ‘  ,<label>(3)</label></formula><p>where ğ‘  ğ‘– is the anomaly score of node ğ‘£ ğ‘– and o ğ‘– is the intermediate output. W ğ‘  and u ğ‘  are the learnable weight matrix and weight vector, respectively. b ğ‘  and ğ‘ ğ‘  are corresponding bias terms.</p><p>To be more concrete, the whole GDN model ğ‘“ ğœ½ (â€¢) can be formally represented as:</p><formula xml:id="formula_7">ğ‘“ ğœ½ (A, X) = ğ‘“ ğœ½ ğ‘  (ğ‘“ ğœ½ ğ‘’ (A, X)),<label>(4</label></formula><p>) which directly maps the input network to scalar anomaly scores, and can be trained in an end-to-end fashion. Deviation Loss. In essence, the objective of GDN is to distinguish normal and abnormal nodes according to the computed anomaly scores with few-shot labels. Here we propose to adopt the deviation loss <ref type="bibr" target="#b22">[23]</ref> to enforce the model to assign large anomaly scores to those nodes whose characteristics significantly deviate from normal nodes. To guide the model learning, we first define a reference score (i.e., ğœ‡ ğ‘Ÿ ) as the mean value of the anomaly scores of a set of randomly selected normal nodes. It serves as the reference to quantify how much the scores of anomalies deviate from those of normal nodes.</p><p>According to previous studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, Gaussian distribution is commonly a robust choice to fit the abnormality scores for a wide range of datasets. Based on this assumption, we first sample a set of ğ‘˜ anomaly scores from the Gaussian prior distribution, i.e., R = {ğ‘Ÿ 1 , ğ‘Ÿ 2 , . . . , ğ‘Ÿ ğ‘˜ } âˆ¼ N (ğœ‡, ğœ 2 ), each of which denotes the abnormality of a random normal node. The reference score is computed as the mean value of all the sampled scores:</p><formula xml:id="formula_8">ğœ‡ ğ‘Ÿ = 1 ğ‘˜ ğ‘˜ âˆ‘ï¸ ğ‘–=1 ğ‘Ÿ ğ‘– .<label>(5)</label></formula><p>With the reference score ğœ‡ ğ‘Ÿ , the deviation between the anomaly score of node ğ‘£ ğ‘– and the reference score can be defined in the form of standard score: dev(ğ‘£ ğ‘– ) =</p><formula xml:id="formula_9">ğ‘  ğ‘– âˆ’ ğœ‡ ğ‘Ÿ ğœ ğ‘Ÿ ,<label>(6)</label></formula><p>where ğœ ğ‘Ÿ is the standard deviation of the set of sampled anomaly scores R = {ğ‘Ÿ 1 , . . . , ğ‘Ÿ ğ‘˜ }. Then the final objective function can be derived from the contrastive loss <ref type="bibr" target="#b10">[11]</ref> by replacing the distance function with the deviation in Eq. ( <ref type="formula" target="#formula_9">6</ref>):</p><formula xml:id="formula_10">L = (1 âˆ’ ğ‘¦ ğ‘– ) â€¢ |dev(ğ‘£ ğ‘– )|+ğ‘¦ ğ‘– â€¢ max(0, ğ‘š âˆ’ dev(ğ‘£ ğ‘– )),<label>(7)</label></formula><p>where ğ‘¦ ğ‘– is the ground-truth label of input node ğ‘£ ğ‘– . If node ğ‘£ ğ‘– is an abnormal node, ğ‘¦ ğ‘– = 1, otherwise, ğ‘¦ ğ‘– = 0. Note that ğ‘š is a confidence margin which defines a radius around the deviation. By minimizing the above loss function, GDN will push the anomaly scores of normal nodes as close as possible to ğœ‡ ğ‘Ÿ while enforcing a large positive deviation of at least ğ‘š between ğœ‡ ğ‘Ÿ and the anomaly scores of abnormal nodes. This way GDN is able to learn a high-level abstraction of normal patterns with substantially less labeled anomalies, and empowers the node representation learning to discriminate normal nodes from the rare anomalies. Accordingly, a large anomaly score will be assigned to a node if its pattern significantly deviates from the learned abstraction of normal patterns.</p><p>Our preliminary results show that GDN is not sensitive to the choices of ğœ‡ and ğœ as long as ğœ is not too large. Specifically, we set ğœ‡ = 0 and ğœ = 1 in our experiments, which helps GDN to achieve stable detection performance on different datasets. It is also worth mentioning that, as we cannot access the labels of normal nodes, we simply consider the unlabeled node in V ğ‘ˆ as normal. Note that this way the remaining unlabeled anomalies and all the normal nodes will be treated as normal, thus contamination is introduced to the training set (i.e., the ratio of unlabeled anomalies to the total unlabeled training data V ğ‘ˆ ). Remarkably, GDN performs very well by using this simple strategy and is robust to different contamination levels. The effect of different contamination levels to model performance is evaluated in Sec. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-network Meta-learning</head><p>Having the proposed Graph Deviation Networks (GDN), we are able to effectively detect anomalies on an arbitrary network with limited labeled data. When auxiliary networks from the same domain of the target network are available, how to transfer such valuable knowledge is the key to enable few-shot anomaly detection on the target network. Despite its feasibility, the performance would be rather limited if we directly borrow the idea of existing crossnetwork learning methods. The main reason is that those methods merely focus on transferring the knowledge from only a single network <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>, which may cause negative transfer due to the divergent characteristics of anomalies on different networks. To this end, we turn to exploit multiple auxiliary networks to distill comprehensive knowledge of anomalies.</p><p>As an effective paradigm for extracting and transferring knowledge, meta-learning has recently received increasing research attention because of the broad applications in a variety of high-impact domains <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. In essence, the goal of meta-learning is to train a model on a variety of learning tasks, such that the learned model is capable of effectively adapting to new tasks with very few or even one labeled data <ref type="bibr" target="#b12">[13]</ref>. In particular, Finn et al. <ref type="bibr" target="#b9">[10]</ref> propose a model-agnostic meta-learning algorithm to explicitly learn the model parameters such that the model can achieve good generalization to a new task through a small number of gradient steps with limited labeled data. Inspired by this work, we propose to learn a meta-learner (i.e., Meta-GDN) as the initialization of GDN from multiple auxiliary networks, which possesses the generalization ability to effectively identify anomalous nodes on a new target network. Specifically, Meta-GDN extracts meta-knowledge of ground-truth anomalies from different few-shot network anomaly detection tasks on auxiliary networks during the training phase, and will be further fine-tuned for the new task on the target network, such that the model can make fast and effective adaptation.</p><p>We define each learning task as performing few-shot anomaly detection on an individual network, whose objective is to enforce large anomaly scores to be assigned to anomalies as defined in Eq. ( <ref type="formula" target="#formula_10">7</ref>). Let T ğ‘– denote the few-shot network anomaly detection task constructed from network G ğ‘  ğ‘– , then we have ğ‘ƒ learning tasks in each epoch. We consider a GDN model represented by a parameterized function ğ‘“ ğœ½ with parameters ğœ½ . Given ğ‘ƒ tasks, the optimization algorithm first adapts the initial model parameters ğœ½ to ğœ½ â€² ğ‘– for each learning task T ğ‘– independently. Specifically, the updated parameter ğœ½ â€² ğ‘– is computed using L T ğ‘– on a batch of training data sampled from V ğ¿ ğ‘– and V ğ‘ˆ ğ‘– in G ğ‘  ğ‘– . Formally, the parameter update with one gradient step can be expressed as:</p><formula xml:id="formula_11">ğœ½ â€² ğ‘– = ğœ½ âˆ’ ğ›¼âˆ‡ ğœ½ L T ğ‘– (ğ‘“ ğœ½ ),<label>(8)</label></formula><p>where ğ›¼ controls the meta-learning rate. Note that Eq. ( <ref type="formula" target="#formula_11">8</ref>) only includes one-step gradient update, while it is straightforward to extend to multiple gradient updates <ref type="bibr" target="#b9">[10]</ref>.</p><p>The model parameters are trained by optimizing for the best performance of ğ‘“ ğœ½ with respect to ğœ½ across all learning tasks. More concretely, the meta-objective function is defined as follows:</p><formula xml:id="formula_12">min ğœ½ ğ‘ƒ âˆ‘ï¸ ğ‘–=1 L T ğ‘– (ğ‘“ ğœ½ â€² ğ‘– ) = min ğœ½ ğ‘ƒ âˆ‘ï¸ ğ‘–=1 L T ğ‘– (ğ‘“ ğœ½ âˆ’ğ›¼ âˆ‡ ğœ½ L T ğ‘– (ğ‘“ ğœ½ ) ).<label>(9)</label></formula><p>By optimizing the objective of GDN, the updated model parameter can preserve the capability of detecting anomalies on each network. Since the meta-optimization is performed over parameters ğœ½ with the objective computed using the updated parameters Algorithm 1 The learning algorithm of Meta-GDN Input: (1) ğ‘ƒ auxiliary networks, i.e., (i.e., ğœ½ â€² ğ‘– ) for all tasks, correspondingly, the model parameters are optimized such that one or a small number of gradient steps on the target task (network) will produce great effectiveness.</p><formula xml:id="formula_13">G ğ‘  = {G ğ‘  1 = (A ğ‘  1 , X ğ‘  1 ), G ğ‘  2 = (A ğ‘  2 , X ğ‘  2 ), . . . , G ğ‘  ğ‘ƒ = (A ğ‘  ğ‘ƒ , X ğ‘  ğ‘ƒ )}; (2) a target network G ğ‘¡ = (A ğ‘¡ , X ğ‘¡ ); (3)</formula><p>Formally, we leverage stochastic gradient descent (SGD) to update the model parameters ğœ½ across all tasks, such that the model parameters ğœ½ are updated as follows:</p><formula xml:id="formula_14">ğœ½ â† ğœ½ âˆ’ ğ›½âˆ‡ ğœ½ ğ‘ƒ âˆ‘ï¸ ğ‘–=1 L T ğ‘– (ğ‘“ ğœ½ â€² ğ‘– ), (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where ğ›½ is the meta step size. The full algorithm is summarized in Algorithm 1. Specifically, for each batch, we randomly sample the same number of nodes from unlabeled data (i.e., V ğ‘ˆ ) and labeled anomalies (i.e., V ğ¿ ) to represent normal and abnormal nodes, respectively (Step-4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we perform empirical evaluations to demonstrate the effectiveness of the proposed framework. Specifically, we aim to answer the following research questions:</p><p>â€¢ RQ1. How effective is the proposed approach Meta-GDN for detecting anomalies on the target network with few or even one labeled instance? â€¢ RQ2. How much will the performance of Meta-GDN change by providing different numbers of auxiliary networks or different anomaly contamination levels? â€¢ RQ3. How does each component of Meta-GDN (i.e., graph deviation networks or cross-network meta-learning) contribute to the final detection performance? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Evaluation Datasets. In the experiment, we adopt three realworld datasets, which are publicly available and have been widely used in previous research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. Table <ref type="table" target="#tab_1">1</ref> summarizes the statistics of each dataset. The detailed description is as follows:</p><p>â€¢ Yelp <ref type="bibr" target="#b23">[24]</ref> is collected from Yelp.com and contains reviews for restaurants in several states of the U.S., where the restaurants are organized by ZIP codes. The reviewers are classified into two classes, abnormal (reviewers with only filtered reviews) and normal (reviewers with no filtered reviews) according to the Yelp anti-fraud filtering algorithm. We select restaurants in the same location according to ZIP codes to construct each network, where nodes represent reviewers and there is a link between two reviewers if they have reviewed the same restaurant. We apply the bag-of-words model <ref type="bibr" target="#b45">[46]</ref> on top of the textual contents to obtain the attributes of each node.</p><p>â€¢ PubMed <ref type="bibr" target="#b27">[28]</ref> is a citation network where nodes represent scientific articles related to diabetes and edges are citations relations. Node attribute is represented by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. We randomly partition the large network into non-overlapping sub-networks of similar size. â€¢ Reddit <ref type="bibr" target="#b11">[12]</ref> is collected from an online discussion forum where nodes represent threads and an edge exits between two threads if they are commented by the same user. The node attributes are constructed using averaged word embedding vectors of the threads. Similarly, we extract non-overlapping sub-networks from the original large network for our experiments.</p><p>Note that except the Yelp dataset, we are not able to access ground-truth anomalies for PubMed and Reddit. Thus we refer to two anomaly injection methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> to inject a combined set of anomalies (i.e., structural anomalies and contextual anomalies) by perturbing the topological structure and node attributes of the original network, respectively. To inject structural anomalies, we adopt the approach used by <ref type="bibr" target="#b6">[7]</ref> to generate a set of small cliques since small clique is a typical abnormal substructure in which a small set of nodes are much more closely linked to each other than average <ref type="bibr" target="#b29">[30]</ref>. Accordingly, we randomly select ğ‘ nodes (i.e., clique size) in the network and then make these nodes fully linked to each other. By repeating this process ğ¾ times (i.e., ğ¾ cliques), we can obtain ğ¾ Ã— ğ‘ structural anomalies. In our experiment, we set the clique size ğ‘ to 15. In addition, we leverage the method introduced by <ref type="bibr" target="#b30">[31]</ref> to generate contextual anomalies. Specifically, we first randomly select a node ğ‘– and then randomly sample another 50 nodes from the network. We choose the node ğ‘— whose attributes have the largest Euclidean distance from node ğ‘– among the 50 nodes. The attributes of node ğ‘– (i.e., x ğ‘– ) will then be replaced with the attributes of node ğ‘— (i.e., x ğ‘— ). Note that we inject structural and contextual anomalies with the same quantity and the total number of injected anomalies is around 5% of the network size. Comparison Methods. We compare our proposed Meta-GDN framework and its base model GDN with two categories of anomaly detection methods, including (1) feature-based methods (i.e., LOF, Autoencoder and DeepSAD) where only the node attributes are considered, and (2) network-based methods (i.e., SCAN, ConOut, Radar, DOMINANT, and SemiGNN) where both topological information and node attributes are involved. Details of these compared baseline methods are as follows:</p><p>â€¢ LOF [3] is a feature-based approach which detects outliers at the contextual level. â€¢ Autoencoder <ref type="bibr" target="#b48">[49]</ref> is a feature-based unsupervised deep autoencoder model which introduces an anomaly regularizing penalty based upon L1 or L2 norms. â€¢ DeepSAD <ref type="bibr" target="#b24">[25]</ref> is a state-of-the-art deep learning approach for general semi-supervised anomaly detection. In our experiment, we leverage the node attribute as the input feature. â€¢ SCAN <ref type="bibr" target="#b42">[43]</ref> is an efficient algorithm for detecting network anomalies based on a structural similarity measure. â€¢ ConOut <ref type="bibr" target="#b25">[26]</ref> identifies network anomalies according to the corresponding subgraph and the relevant subset of attributes in the local context. â€¢ Radar <ref type="bibr" target="#b16">[17]</ref> is an unsupervised method that detects anomalies on attributed network by characterizing the residuals of attribute information and its coherence with network structure. â€¢ DOMINANT <ref type="bibr" target="#b5">[6]</ref> is a GCN-based autoencoder framework which computes anomaly scores using the reconstruction errors from both network structure and node attributes. â€¢ SemiGNN <ref type="bibr" target="#b36">[37]</ref> is a semi-supervised GNN model, which leverages the hierarchical attention mechanism to better correlate different neighbors and different views. Evaluation Metrics. In this paper, we use the following metrics to have a comprehensive evaluation of the performance of different anomaly detection methods:</p><p>â€¢ AUC-ROC is widely used in previous anomaly detection research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. Area under curve (AUC) is interpreted as the probability that a randomly chosen anomaly receives a higher score than a randomly chosen normal object. â€¢ AUC-PR is the area under the curve of precision against recall at different thresholds, and it only evaluates the performance on the positive class (i.e., abnormal objects). AUC-PR is computed as the average precision as defined in <ref type="bibr" target="#b20">[21]</ref> and is used as the evaluation metric in <ref type="bibr" target="#b22">[23]</ref>. â€¢ Precision@K is defined as the proportion of true anomalies in a ranked list of ğ¾ objects. We obtain the ranking list in descending order according to the anomaly scores that are computed from a specific anomaly detection algorithm. Implementation Details. Regarding the proposed GDN model, we use Simple Graph Convolution <ref type="bibr" target="#b38">[39]</ref> to build the network encoder with degree ğ¾ = 2 (two layers). As shown in Eq. ( <ref type="formula" target="#formula_6">3</ref>), the abnormality  valuator employs a two-layer neural network with one hidden layer of 512 units followed by an output layer of 1 unit. The confidence margin (i.e., ğ‘š) in Eq. ( <ref type="formula" target="#formula_10">7</ref>) is set as 5 and the reference score (i.e., ğœ‡ ğ‘Ÿ ) is computed using Eq. ( <ref type="formula" target="#formula_8">5</ref>) from ğ‘˜ = 5, 000 scores that are sampled from a Gaussian prior distribution, i.e., N (0, 1). Unless otherwise specified, we set the total number of networks as 5 (4 auxiliary networks and 1 target network), and for each one we have access to 10 labeled abnormal nodes that are randomly selected from the set of labeled anomalies (V ğ¿ ) in every run of the experiment. For model training, the proposed GDN and Meta-GDN are trained with 1000 epochs, with batch size 16 in each epoch, and a 5-step gradient update is leveraged to compute ğœ½ â€² in the meta-optimization process. The network-level learning rate ğ›¼ is 0.01 and the metalevel learning rate ğ›½ = 0.001. Fine-tuning is performed on the target network where the corresponding nodes are split into 40% for fine-tuning, 20% for validation, and 40% for testing. For all the comparison methods, we select the hyper-parameters with the best performance on the validation set and report the results on the test data of the target network for a fair comparison. Particularly, for all the network-based methods, the whole network structure and node attributes are accessible during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness Results (RQ1)</head><p>Overall Comparison. In the experiments, we evaluate the performance of the proposed framework Meta-GDN along with its base model GDN by comparing with the included baseline methods. We first present the evaluation results (10-shot) w.r.t. AUC-ROC and AUC-PR in Table <ref type="table" target="#tab_2">2</ref> and the results w.r.t. Precision@K are visualized in Figure <ref type="figure" target="#fig_3">3</ref>. Accordingly, we have the following observations, including: (1) in terms of AUC-ROC and AUC-PR, our approach Meta-GDN outperforms all the other compared methods by a significant margin. Meanwhile, the results w.r.t. Precision@K again demonstrate that Meta-GDN can better rank abnormal nodes on higher positions than other methods by estimating accurate anomaly scores; (2) unsupervised methods (e.g., DOMINANT, Radar) are not able to leverage supervised knowledge of labeled anomalies and therefore have limited performance. Semi-supervised methods (e.g., DeepSAD, SemiGNN) also fail to deliver satisfactory results. The possible explanation is that DeepSAD cannot model network information and SemiGNN requires a relatively large number of labeled data and multi-view data, which make them less effective in our evaluation; and (3) compared to the base model GDN, Meta-GDN is capable of extracting comprehensive meta-knowledge across multiple auxiliary networks by virtue of the cross-network meta-learning algorithm, which further enhances the detection performance on the target network.</p><p>Few-shot Evaluation. In order to verify the effectiveness of Meta-GDN in few-shot as well as one-shot network anomaly detection, we evaluate the performance of Meta-GDN with different numbers  <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref>, we can see that even with only one labeled anomaly on the target network (i.e., 1-shot), Meta-GDN can still achieve good performance and significantly outperforms all the baseline methods. In the meantime, we can clearly observe that the performance of Meta-GDN increases with the growth of the number of labeled anomalies, which demonstrates that Meta-GDN can be better fine-tuned on the target network with more labeled examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sensitivity &amp; Robustness Analysis (RQ2)</head><p>In this section, we further analyze the sensitivity and robustness of the proposed framework Meta-GDN. By providing different numbers of auxiliary networks during training, the model sensitivity results w.r.t. AUC-ROC are presented in Figure <ref type="figure" target="#fig_4">4</ref>(a). Specifically, we can clearly find that (1) as the number of auxiliary networks increases, Meta-GDN achieves constantly stronger performance on all the three datasets. It shows that more auxiliary networks can provide better meta-knowledge during the training process, which is consistent with our intuition; (2) Meta-GDN can still achieve relatively good performance when training with a small number of auxiliary networks (e.g., ğ‘ = 2), which demonstrates the strong capability of its base model GDN. For example, on Yelp dataset, the performance barely drops 0.033 if we change the number of auxiliary networks from ğ‘ = 6 to ğ‘ = 2. As discussed in Sec. 4.1, we treat all the sampled nodes from unlabeled data as normal for computing the deviation loss. This simple strategy introduces anomaly contamination in the unlabeled training data. Due to the fact that ğ‘Ÿ ğ‘ is a small number in practice, our approach can work very well in a wide range of real-world datasets. To further investigate the robustness of Meta-GDN w.r.t. different contamination levels ğ‘Ÿ ğ‘ (i.e., the proportion of anomalies in the unlabeled training data), we report the evaluation results of Meta-GDN, GDN and the semi-supervised baseline method SemiGNN in Figure <ref type="figure" target="#fig_4">4</ref>(b). As shown in the figure, though the performance of all the methods decreases with increasing contamination levels, both Meta-GDN and GDN are remarkably robust and can consistently outperform SemiGNN to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study (RQ3)</head><p>Moreover, we conduct an ablation study to better examine the contribution of each key component in the proposed framework. In addition to Meta-GDN and its base model GDN, we include another variant GDN âˆ’ that excludes the network encoder and crossnetwork meta-learning in Meta-GDN. We present the results of AUC-ROC and Precision@100 in Figure <ref type="figure" target="#fig_5">5</ref>(a) and Figure <ref type="figure" target="#fig_5">5</ref>(b), respectively. The corresponding observations are two-fold: (1) by incorporating GNN-based network encoder, GDN largely outperforms GDN âˆ’ in anomaly detection on the target network. For example, GDN achieves 8.1% performance improvement over GDN âˆ’ on PubMed in terms of precision@100. The main reason is that the GNN-based network encoder is able to extract topological information of nodes and to learn highly expressive node representations; and (2) the complete framework Meta-GDN performs consistently better than the base model GDN on all the three datasets. For instance, Meta-GDN improves AUC-ROC by 5.75% over GDN on Yelp dataset, which verifies the effectiveness of the proposed crossnetwork meta-learning algorithm for extracting and transferring meta-knowledge across multiple auxiliary networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we make the first investigation on the problem of few-shot cross-network anomaly detection. To tackle this problem, we first design a novel GNN architecture, GDN, which is capable of leveraging limited labeled anomalies to enforce statistically significant deviations between abnormal and normal nodes on an individual network. To further utilize the knowledge from auxiliary networks and enable few-shot anomaly detection on the target network, we propose a cross-network meta-learning approach, Meta-GDN, which is able to extract comprehensive meta-knowledge from multiple auxiliary networks in the same domain of the target network. Through extensive experimental evaluations, we demonstrate the superiority of Meta-GDN over the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Since anomalies usually have distinct patterns, (a) existing methods may easily fail to distinguish them from normal nodes in the latent representation space with only few labeled anomalies, (b) while they can be well separated in an anomaly score space by enforcing statistically significant deviations between abnormal and normal nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Left) The model architecture of Graph Deviation Networks (GDN) for network anomaly detection with limited labeled data. (Right) The illustration of the overall framework Meta-GDN. Meta-GDN is trained across multiple auxiliary networks and can be well adapted to the target network with few-shot labeled data. Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison results (10-shot) w.r.t. Precision@K on three datasets. Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Sensitivity analysis of Meta-GDN w.r.t. different number of auxiliary networks; (b) Model robustness study w.r.t. AUC-ROC with different contamination levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) AUC-ROC results of Meta-GDN and its variants; (b) Precision@100 results of Meta-GDN and its variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ğœ½ L T ğ‘– (ğ‘“ ğœ½ ) using ğµ ğ‘– and L(â€¢) in Eq. (7); ğœ½ â€² ğ‘– â† ğœ½ âˆ’ ğ›¼âˆ‡ ğœ½ L T ğ‘– (ğ‘“ ğœ½ ); Fine-tune ğœ½ on target network G ğ‘¡ with {V ğ¿ ğ‘¡ , V ğ‘ˆ ğ‘¡ }; 12: Compute anomaly scores for nodes in V ğ‘ˆ ğ‘¡ ;</figDesc><table><row><cell></cell><cell cols="2">sets of few-shot labeled anomalies and unlabeled</cell></row><row><cell></cell><cell cols="2">nodes for each network (i.e., {V ğ¿ 1 , V ğ‘ˆ 1 }, . . . , {V ğ¿ ğ‘ƒ , V ğ‘ˆ ğ‘ƒ } and</cell></row><row><cell></cell><cell cols="2">{V ğ¿ ğ‘¡ , V ğ‘ˆ ğ‘¡ }); (4) training epochs ğ¸, batch size ğ‘, and meta-</cell></row><row><cell></cell><cell cols="2">learning hyper-parameters ğ›¼, ğ›½.</cell></row><row><cell cols="3">Output: Anomaly scores of nodes in V ğ‘ˆ ğ‘¡ .</cell></row><row><cell cols="2">1: Initialize parameters ğœ½ ;</cell><cell></cell></row><row><cell cols="2">2: while ğ‘’ &lt; ğ¸ do</cell><cell></cell></row><row><cell>3:</cell><cell cols="2">for each network G ğ‘  ğ‘– (task T ğ‘– ) do</cell></row><row><cell>4:</cell><cell cols="2">Randomly sample ğ‘ 2 nodes from V ğ¿ ğ‘– and ğ‘ 2 from V ğ‘ˆ ğ‘–</cell></row><row><cell></cell><cell cols="2">to comprise the batch ğµ ğ‘– ;</cell></row><row><cell cols="3">5: Evaluate âˆ‡ 6: Compute adapted parameters ğœ½ â€² with gradient descent</cell></row><row><cell cols="3">using Eq. (8), 7: Sample a new batch ğµ â€² ğ‘– for the meta-update;</cell></row><row><cell>8:</cell><cell>end for</cell><cell></cell></row><row><cell>9:</cell><cell>Update ğœ½ â† ğœ½ âˆ’ ğ›½âˆ‡ ğœ½</cell><cell>ğ‘ ğ‘–=1 L T ğ‘– (ğ‘“ ğœ½ â€² ğ‘– ) using {ğµ â€² ğ‘– } and L(â€¢)</cell></row><row><cell></cell><cell>according to Eq. (7);</cell><cell></cell></row><row><cell cols="2">10: end while</cell><cell></cell></row><row><cell>11:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of evaluation datasets. ğ‘Ÿ 1 denotes the ratio of labeled anomalies to the total anomalies and ğ‘Ÿ 2 is the ratio of labeled anomalies to the total number of nodes.</figDesc><table><row><cell>Datasets</cell><cell>Yelp</cell><cell cols="2">PubMed Reddit</cell></row><row><cell># nodes (avg.)</cell><cell>4, 872</cell><cell>3, 675</cell><cell>15, 860</cell></row><row><cell># edges (avg.)</cell><cell>43, 728</cell><cell>8, 895</cell><cell>136, 781</cell></row><row><cell># features</cell><cell>10, 000</cell><cell>500</cell><cell>602</cell></row><row><cell># anomalies (avg.)</cell><cell>223</cell><cell>201</cell><cell>796</cell></row><row><cell>ğ‘Ÿ 1 (avg.)</cell><cell>4.48%</cell><cell>4.97%</cell><cell>1.26%</cell></row><row><cell>ğ‘Ÿ 2 (avg.)</cell><cell>0.21%</cell><cell>0.27%</cell><cell>0.063%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison results (10-shot) w.r.t. AUC-ROC and AUC-PR on three datasets.</figDesc><table><row><cell></cell><cell>Yelp</cell><cell></cell><cell cols="2">PubMed</cell><cell>Reddit</cell><cell></cell></row><row><cell>Methods</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell></row><row><cell>LOF</cell><cell cols="6">0.375 Â± 0.011 0.042 Â± 0.004 0.575 Â± 0.007 0.187 Â± 0.016 0.518 Â± 0.015 0.071 Â± 0.006</cell></row><row><cell>Autoencoder</cell><cell cols="6">0.365 Â± 0.013 0.041 Â± 0.008 0.584 Â± 0.018 0.236 Â± 0.005 0.722 Â± 0.012 0.347 Â± 0.007</cell></row><row><cell>DeepSAD</cell><cell cols="6">0.460 Â± 0.008 0.062 Â± 0.005 0.528 Â± 0.008 0.115 Â± 0.004 0.503 Â± 0.010 0.066 Â± 0.005</cell></row><row><cell>SCAN</cell><cell cols="6">0.397 Â± 0.011 0.046 Â± 0.005 0.421 Â± 0.016 0.048 Â± 0.005 0.298 Â± 0.009 0.048 Â± 0.002</cell></row><row><cell>ConOut</cell><cell cols="6">0.402 Â± 0.015 0.041 Â± 0.005 0.511 Â± 0.019 0.093 Â± 0.012 0.551 Â± 0.008 0.085 Â± 0.007</cell></row><row><cell>Radar</cell><cell cols="6">0.415 Â± 0.012 0.045 Â± 0.007 0.573 Â± 0.013 0.244 Â± 0.011 0.721 Â± 0.008 0.281 Â± 0.007</cell></row><row><cell>DOMINANT</cell><cell cols="6">0.578 Â± 0.018 0.109 Â± 0.003 0.636 Â± 0.021 0.337 Â± 0.013 0.735 Â± 0.013 0.357 Â± 0.009</cell></row><row><cell>SemiGNN</cell><cell cols="6">0.497 Â± 0.004 0.058 Â± 0.003 0.523 Â± 0.008 0.065 Â± 0.006 0.610 Â± 0.007 0.134 Â± 0.003</cell></row><row><cell>GDN (ours)</cell><cell cols="6">0.678 Â± 0.015 0.132 Â± 0.009 0.736 Â± 0.012 0.438 Â± 0.012 0.811 Â± 0.015 0.379 Â± 0.011</cell></row><row><cell cols="7">Meta-GDN (ours) 0.724 Â± 0.012 0.175 Â± 0.011 0.761 Â± 0.014 0.485 Â± 0.010 0.842 Â± 0.011 0.395 Â± 0.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Few-shot performance evaluation of Meta-GDN w.r.t. AUC-ROC and AUC-PR. Note that we respectively set the batch size ğ‘ to 2, 4, 8, and 16 to ensure that there is no duplication of labeled anomalies exist in a sampled training batch. Also, we keep the number of labeled anomalies on auxiliary networks as 10. Table3summarizes the AUC-ROC/AUC-PR performance of Meta-GDN under different few-shot settings. By comparing the results in Table</figDesc><table><row><cell></cell><cell>Yelp</cell><cell></cell><cell cols="2">PubMed</cell><cell>Reddit</cell><cell></cell></row><row><cell>Setting</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell><cell>AUC-ROC</cell><cell>AUC-PR</cell></row><row><cell>1-shot</cell><cell cols="6">0.702 Â± 0.008 0.159 Â± 0.015 0.742 Â± 0.012 0.462 Â± 0.013 0.821 Â± 0.013 0.380 Â± 0.011</cell></row><row><cell>3-shot</cell><cell cols="6">0.709 Â± 0.006 0.164 Â± 0.010 0.748 Â± 0.008 0.468 Â± 0.008 0.828 Â± 0.012 0.386 Â± 0.007</cell></row><row><cell>5-shot</cell><cell cols="6">0.717 Â± 0.013 0.169 Â± 0.007 0.753 Â± 0.011 0.474 Â± 0.005 0.834 Â± 0.009 0.389 Â± 0.008</cell></row><row><cell>10-shot</cell><cell cols="6">0.724 Â± 0.012 0.175 Â± 0.011 0.761 Â± 0.014 0.485 Â± 0.010 0.842 Â± 0.011 0.395 Â± 0.009</cell></row><row><cell cols="4">of labeled anomalies on the target network (i.e., 1-shot, 3-shot, 5-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shot and 10-shot).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work is partially supported by NSF (2029044, 1947135 and  1939725)  and ONR (N00014-21-1-4002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Outlier aware network embedding for attributed networks</title>
		<author>
			<persName><forename type="first">Sambaran</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Lokesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murty</forename><surname>Narasimha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LOF: identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¶rg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive anomaly detection on attributed networks</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep anomaly detection on attributed networks</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Bhanushali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive anomaly detection on attributed networks</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph prototypical networks for few-shot learning on attributed networks</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</title>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In ICANN</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpreting and unifying outlier scores</title>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spam review detection with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Radar: Residual Analysis for Anomaly Detection in Attributed Networks</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SpecAE: Spectral AutoEncoder for Anomaly Detection in Attributed Networks</title>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Isometric Propagation Network for Generalized Zero-shot Learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prototype propagation networks (PPN) for weakly-supervised few-shot learning on category graph</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<author>
			<persName><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ranking outlier nodes in subspaces of attributed graphs</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">Iglesias</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>MÃ¼lle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klemens</forename><surname>BÃ¶hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collective opinion spam detection: Bridging review networks and metadata</title>
		<author>
			<persName><forename type="first">Shebuti</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Semi-Supervised Anomaly Detection</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>GÃ¶rnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local context selection for outlier ranking in graphs with multiple numeric node attributes</title>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Iglesias SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oretta</forename><surname>Irmler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klemens</forename><surname>BÃ¶hm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial Deep Network Embedding for Cross-Network Node Classification</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Lai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kup-Sze</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting anomalies in graphs</title>
		<author>
			<persName><surname>David B Skillicorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISI</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional anomaly detection</title>
		<author>
			<persName><forename type="first">Xiuyao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Jermaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferring Robustness for Graph Neural Network Against Poisoning Attacks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-negative residual matrix factorization with application to graph anomaly detection</title>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Semi-supervised Graph Attentive Network for Financial Fraud Detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Few-shot Learning with Attribute Matching</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Longshort Distance Aggregation Networks for Positive Unlabeled Graph Learning</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptive Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scan: a structural clustering algorithm for networks</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurcan</forename><surname>Yuruk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhidan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Aj</forename><surname>Schweiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)</title>
				<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Social media mining: an introduction</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding bag-of-words model: a statistical framework</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJMLC</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from incomplete and inaccurate supervision</title>
		<author>
			<persName><forename type="first">Zhen-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Error-Bounded Graph Anomaly Loss for GNNs</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuchen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparc: Self-paced network representation for few-shot rare category characterization</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast Network Alignment via Graph Meta-Learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">AD-MIRING: Adversarial multi-network mining</title>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards Real Time Team Optimization</title>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Adversarial Attacks on Graph Neural Networks via Meta Learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>ZÃ¼gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
