<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Feng</surname></persName>
							<email>fengyue@software.ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<email>junxu@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<email>lanyanyan@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
							<email>guojiafeng@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<email>zengwei@software.ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3209978.3209979</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diverse ranking</term>
					<term>Markov decision process</term>
					<term>Monte Carlo tree search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of search result diversi cation is to select a subset of documents from the candidate set to satisfy as many di erent subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M 2 Div. In M 2 Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M 2 Div can signi cantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the e ectiveness of the exploratory decision-making mechanism in M 2 Div.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One important goal in many information retrieval tasks involves providing search results that covers a wide range of topics for a search query, called search result diversi cation <ref type="bibr" target="#b0">[1]</ref>. The goal of search result diversi cation can be formalized as selecting a minimal subset of documents from the candidate set to cover as many di erent subtopics as possible. Since the novelty of a document depends on the other selected documents, selecting an optimal subset of documents amounts to the problem subset selection and its complexity is in general NP-hard.</p><p>Typical approaches treat search result diversi cation as ranking the documents based on their relevance as well as the novelty. Greedy sequential document selection has been widely adopted to construct the diverse document ranking, that is, the document ranking is constructed step by step. At each step the ranking model selects one document from the candidate set for the current ranking position. Usually, the document with the maximal amount of additional utility, a.k.a. marginal relevance, is selected.</p><p>A number of diverse ranking algorithms have been developed under the greedy document selection framework. Di erent algorithms utilize di erent criteria to estimate the additional utility a candidate document can provide. For example, the representative approach of maximal marginal relevance (MMR) <ref type="bibr" target="#b2">[3]</ref> uses the sum of the query-document relevance and the maximal document distance (referred to as marginal relevance) as the utility. xQuAD <ref type="bibr" target="#b24">[25]</ref> de nes the utility so as to explicitly account for the relationship between documents retrieved for the original query and the possible subqueries. In recent years, machine learning based methods have been proposed for conducting diverse ranking <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. The relational learning to rank (R-LTR) <ref type="bibr" target="#b38">[39]</ref> and its variations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref> de ne the utilities based on the relevance features and the novelty features. MDP-DIV adapted the Markov decision process (MDP) to model the document ranking process. The utility of a document is estimated based on the MDP state, which consists of the query, the preceding documents, and the remaining candidates <ref type="bibr" target="#b32">[33]</ref>.</p><p>The greedy sequential document selection simpli es the ranking process and can accelerate the online ranking. However, the rankings produced by greedy document selection are inevitably suboptimal. At each ranking position, the greedy selection mechanism only considers the possibilities at the current ranking position (i.e., estimates the utility of each candidate document if it were selected). Thus, greedy document selection will select the locally optimal document at each ranking position. However, a sequence of the locally optimal documents cannot lead to the globally optimal diverse ranking, because the utilities of the documents are not independent. The selection of a document at one position will change the utilities of the remaining candidate documents and thereafter a ects the subsequent decisions. In general the ranking algorithm need to explore the whole ranking space if the optimal ranking is mandatory. However, this is usually infeasible in real applications because of the huge space size: there exist N ! di erent rankings for N documents.</p><p>Inspired by the success and methodology of the AlphaGo <ref type="bibr" target="#b26">[27]</ref> and AlphaGo Zero <ref type="bibr" target="#b27">[28]</ref> for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking <ref type="bibr" target="#b32">[33]</ref> with the Monte Carlo tree search (MCTS), for alleviating the suboptimal ranking problem. The new ranking model, referred to as M 2 Div (stands for MCTS enhanced MDP for Diverse ranking), makes use of an MDP to model the sequential document selection process of diverse ranking. At each time step (corresponding to a ranking position), based on the user query and the preceding document ranking, a recurrent neural network (RNN) is used to produce the policy (a distribution over the candidate documents) for guiding the document selection and the value for estimating the whole document ranking quality (e.g., in terms of α-NDCG@M). To alleviate the problem of suboptimal diverse ranking, in stead of greedily selecting a document with the predicted raw policy, M 2 Div conducts an exploratory decision making: an MCTS is conducted to explore the possible document rankings at the subsequent positions, resulting a strengthened search policy for conducting the real document selection at current position. Since it has explored more future possible document rankings, the search policy has higher probability to select a globally optimal document than the predicted raw policy. Moving to the next iteration, the above process is continued until the candidate set is empty.</p><p>Reinforcement learning is used to train the model parameters. In the training phase, at each training iteration and for each training query, an MCTS guided by the current policy function and value function is conducted at each ranking position. The MCTS produces a search policy for the document selection. Then the model parameters are adjusted to minimize the loss function. The loss function consists of two terms: 1) the squared error between the predicted value and the nal quality of the whole document ranking in terms of α-NDCG@M; and 2) the cross entropy of the predicted raw policy and the search policy for document selection. Stochastic gradient descent is utilized for conducting the optimization.</p><p>To evaluate the e ectiveness of M 2 Div, we conducted experiments on the basis of TREC benchmark datasets. The experimental results showed that M 2 Div can signi cantly outperform the stateof-the-art diverse ranking approaches that using greedy sequential decision making, including the heuristic based diverse ranking methods of MMR and xQuAD, and the machine learning based diverse ranking methods of PAMM and MDP-DIV. We analyzed the results and showed that the exploratory decision-making mechanism in M 2 Div does help to improve the ranking performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Search result diversi cation</head><p>It is a common practice to formalize the construction of a diverse ranking list in search as a process of greedy sequential decision making. Existing research focus on designing e ective criteria to estimate the utility a document can provide. Carbonell and Goldstein <ref type="bibr" target="#b2">[3]</ref> proposed the maximal marginal relevance criterion, which is a linear combination of the query-document relevance and the document novelty, to select the document. xQuAD <ref type="bibr" target="#b23">[24]</ref> directly models di erent aspects of a query and estimates the utility as the relevance of the retrieved documents to each identi ed aspects. Hu et al. <ref type="bibr" target="#b12">[13]</ref> proposed a utility function that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. See also <ref type="bibr">[2, 4, 7, 9-11, 22, 29]</ref> Machine learning techniques have been applied to construct diverse ranking, also adopting the greedy sequential decision making as the basic framework. The key problem becomes how to automatically learn the utility function on the basis of training queries. Some researchers de ne the utility as a linear combination of the handcrafted relevance features and novelty features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. The novelty term in the utility function can be modeled with the deep learning model of neural tensor networks <ref type="bibr" target="#b31">[32]</ref>. Jiang et al. used recurrent neural networks and max-pooling to model subtopic information explicitly with the attention mechanism <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Xia et al. <ref type="bibr" target="#b32">[33]</ref> proposed to model the dynamics of the document utility with MDP and learning the model parameters with policy gradient. Other learning approaches please refer to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement learning for IR</head><p>The reinforcement learning has been widely used in variant IR applications. For example, in <ref type="bibr" target="#b19">[20]</ref>, a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In <ref type="bibr" target="#b37">[38]</ref>, the log-based document re-ranking is modeled as a POMDP. <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b29">[30]</ref> propose to model the process of constructing a document ranking with MDP, for the ranking tasks of search result diversi cation and relevance ranking, respectively. Muti-armed bandit, another type of reinforcement learning model, is also widely applied to rank learning. For example, <ref type="bibr" target="#b22">[23]</ref> proposes two online learning bandit algorithms to learn a diverse ranking of documents based on users clicking behaviors. <ref type="bibr" target="#b36">[37]</ref> formalizes the interactively optimizing of information retrieval systems as a dueling bandit problem and <ref type="bibr" target="#b15">[16]</ref> proposes cascading bandits to identify K most attractive document for users. See also <ref type="bibr" target="#b11">[12]</ref> Reinforcement learning models are also used for building recommender systems. For example, <ref type="bibr" target="#b25">[26]</ref> designs an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. Lu and Yang <ref type="bibr" target="#b18">[19]</ref> proposes POMDP-Rec, a neuraloptimized POMDP algorithm, for building a collaborative ltering recommender system.</p><p>In this paper, we also adopt the reinforcement learning model of MDP to formalize the diverse ranking process in search result diversi cation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MDP AND MCTS</head><p>We employ Markov decision process and Monte Carlo tree search to model diverse ranking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Markov decision process</head><p>MDP provides a mathematical framework for modeling the sequential decision making process with the agent-environment interface.</p><p>The key components of an MDP include:</p><p>States S is a set of states. For instance, in this paper we de ne the state as a tuple consists of the query, the preceding document ranking, and the candidate documents.</p><p>Actions A is a discrete set of actions that an agent can take. The actions available may depend on the state s, denoted as A(s).</p><p>Policy p describes the behaviors of an agent, which is a probabilistic distribution over the possible actions. p is usually optimized to maximize the long term return.</p><p>Transition T is the state transition function s t +1 = T (s t , a t ) which speci es a function that maps a state s t into a new state s t +1 in response to the action selected a t .</p><p>Value: state value function V : S → R is a function that predicts the long term return of the whole episode, on the basis of the current state s under the policy p.</p><p>An MDP model is running as follows: the agent and environment interact at each of a sequence of discrete time steps, t = 0, 1, 2, • • • . At each time step t the agent receives some representation of the environment's state, s t ∈ S, and on that basis selects an action a t ∈ A(s t ). One time step later, in part as a consequence of its action, the agent nds itself in a new state s t +1 = T (s t , a t ). Usually the goal of reinforcement learning is to achieve maximum long term return, that is, to maximize the value V (s 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monte Carlo tree search</head><p>The decisions made by an MDP (e.g., selecting the most con dent actions according to the policy) may get suboptimal results. Theoretically the system has to explore the whole space of decision sequences to get a global optimal result. However, this is usually not feasible. MCTS is a tool to conduct heuristic search inside the whole space, more likely to produce a better decision sequence than that of produced by the greedy decisions.</p><p>Given the time step t, the policy function p, and the state value function V , MCTS aims at searching a strengthened policy for making better decisions. The MCTS consists of four phases: selection, expansion, simulation, and back-propagation:</p><p>Selection: Starting at the root node R, MCTS recursively selects the optimal child nodes until a leaf node L is reached.</p><p>Expansion: If L is not a terminal node (i.e. it does not end the episode) then create one or more child nodes for L (each corresponds a possible action) and select one child node C according to the predicted policy.</p><p>Simulation/Evaluation: MCTS runs a simulation from C until a result is achieved. In the AlphaGo Zero <ref type="bibr" target="#b27">[28]</ref> the simulation is replaced with the value function for accelerating the tree search. That is, the result of simulation is estimated by the value function.</p><p>Back-propagation: Update the statistics stored in the current move sequence with the simulation or estimated result.</p><p>The MCTS outputs a search policy π over the actions, which is utilized to choose the action at time step t. The iteration is continued until the whole episode is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTS</head><formula xml:id="formula_0">Environment raw policy ! value function " search policy # action $ % ~# ' %() state ' % Policy-Value network LSTM *($|' % ) "(' % )</formula><p>Figure <ref type="figure">1</ref>: The agent-environment interaction of M 2 Div.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIVERSE RANKING WITH POLICY-VALUE NETWORKS</head><p>In this section, we introduce the proposed M 2 Div model, which makes use of MDP and MCTS for modeling the diverse ranking process and for strengthening the policy for selecting documents at each of the MDP iteration, respectively. The agent-environment interaction of M 2 Div is illustrated in Figure <ref type="figure">1</ref>. Each of the MDP time step corresponds to a ranking position. At time step t (t = 0, 1, • • • ), the policy-value network receives the environment state s t and makes use of an LSTM to produce the representation of the state s t . After that, guided by the current policy function p and value function V , an MCTS search is executed. The output of MCTS is a strengthened new search policy π . The action a t is then selected according to the strengthened policy π , which chooses a document from the candidate set and places it to the ranking position t + 1.</p><p>Moving to the next time step t + 1, the system nds itself in a new state s t +1 and the process is repeated until all of the documents are ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MDP formulation of diverse ranking</head><p>Suppose we are given a query q, which is associated with a set of retrieved documents</p><formula xml:id="formula_1">X = {x 1 , • • • , x M } ⊆ X,</formula><p>where both the query q and the documents x i are represented with their preliminary representations, i.e., the vectors learned by the doc2vec model <ref type="bibr" target="#b16">[17]</ref>, and X is the set of all possible documents. The goal of diverse ranking is to construct a model that can rank the documents so that the top ranked documents cover a wide range of subtopics of a search query. The construction of a diverse ranking can be considered as a process of sequential decision making with an MDP in which each time step corresponds to a ranking position. The states, actions, transition function, value function, and policy function of the MDP are set as:</p><p>States S: We design the state at time step t as a triple s t = [q, Z t , X t ], where q is the preliminary representation of the user issued query; Z t = {x (n) } t n=1 is the sequence of t preceding documents, where x (n) is the document ranked at position n; X t is the set of candidate documents. At the beginning (t = 0), the state is initialized as s 0 = [q, ∅, X ], where ∅ is the empty sequence and X contains all of the M retrieved candidate documents.</p><p>Actions A: At each time step t, the A(s t ) is the set of actions the agent can choose, each corresponds to a document from X t . That is, the action a t ∈ A(s t ) at the time step t selects a document x m(a t ) ∈ X t for the ranking position t + 1, where m(a t ) is the index of the document selected by a t .</p><p>Transition T : The transition function T : S × A → S is de ned as follows:</p><formula xml:id="formula_2">s t +1 = T (s t , a t ) = T ([q, Z t , X t ], a t ) = q, Z t ⊕ {x m(a t ) }, X t \ {x m(a t ) } ,<label>(1)</label></formula><p>where ⊕ appends x m(a t ) to Z t and \ removes x m(a t ) from X t . At each time step t, based on state s t the system chooses an action a t . Then, the system moves to time step t + 1 and the system transits to a new state s t +1 : rst, the query q is kept unchanged; second, the selected document is appended to the end of Z t , generating a new document sequence; nally, the selected document at step t is removed from the candidate set: X t +1 = X t \ {x m(a t ) }. Thus, the number of actions the agent can choose at t + 1 is reduced by one.</p><p>Value function V : The state value function V : S → R is a scalar evaluation, estimating the quality of the whole document ranking (an episode) based on the input state. The value function is learned so as to approximate a prede ned evaluation measure (e.g., α-NDCG@M).</p><p>In this paper, we make use of the long short term memory (LSTM) to summarize the input state s as a real vector, and then de ne the value function as nonlinear transformation of the weighted sum of the LSTM outputs:</p><formula xml:id="formula_3">V (s) = σ ( w, LSTM(s) + b ) ,<label>(2)</label></formula><p>where w and b are the weight vector and the bias to be learned during training, and σ (x) = 1 1+e −x is the nonlinear sigmoid function. The deep neural network model LSTM: S → R L maps a state to a real vector where L is the number of dimensions. Given</p><formula xml:id="formula_4">s = [q, Z = {x 1 , x 2 , • • • , x t }, X t ], where x k (k = 1, • • • , t)</formula><p>is the document ranked at k-th position and represented with its doc2vec embedding. LSTM outputs a representation h k for position k as:</p><formula xml:id="formula_5">f k =σ (W f x k + U f h k −1 + b f ), i k =σ (W i x k + U i h k −1 + b i ), o k =σ (W o x k + U o h k−1 + b o ), c k =f k • c k −1 + i k • tanh(W c x k + U c h k −1 + b c ), h k =o k • tanh(c k ),</formula><p>where h and c are initialized with the query q:</p><formula xml:id="formula_6">[h 0 , c 0 ] = [σ (V h q), σ (V c q)],</formula><p>operator "•" denotes the element-wise product and "σ " is applied to each of the entries; the variables f k , i k , o k , c k and h k denote the forget gate's activation vector, input gate's activation vector, output gate's activation vector, cell state vector, and output vector of the LSTM block, respectively.</p><formula xml:id="formula_7">W f , W i , W o , U f , U i , U o , V h , V c , b f , b i , b o</formula><p>are weight matrices and bias vectors need to be learned during training. The output vector and cell state vector at the t-th cell are concatenated as the output of LSTM:</p><formula xml:id="formula_8">LSTM(s) = h t T , c t T T .<label>(3)</label></formula><p>Policy function p: The policy p(s) de nes a function that takes the state as input and output a distribution over all of the possible actions a ∈ A(s). Speci cally, each probability in the distribution is a normalized soft-max function whose input is the bilinear product of the LSTM de ned in Equation (3) and the selected document:</p><formula xml:id="formula_9">p(a|s) = exp x T m(a) U p LSTM(s) a ∈A(s) exp x T m(a ) U p LSTM(s)</formula><p>, where U p is the parameter in the bilinear product. Thus, the policy function p(s) is:</p><formula xml:id="formula_10">p(s) = p(a 1 |s), • • • , p(a | A(s) | |s) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Strengthening policy with MCTS</head><p>Selecting documents greedily with the predicted raw policy p in Equation ( <ref type="formula" target="#formula_10">4</ref>) may lead to suboptimal rankings because the policy only summarizes the historical information and has no idea on how the action a t will in uent the future decisions. Let's use an example to show the limitation of greedy selection. Suppose that the query q has 6 subtopics, and the 3 retrieved candidate documents d 1 , d 2 , and d 3 cover the subtopics of {1, 2, 3, 6}, {1, 2, 5}, and {1, 2, 6}, respectively. The greedy selection with raw policy prefers d 1 for the rst position, as it covers the most number of subtopics. Thus, the document ranking constructed with the greedy policy could be [d 1 , d 2 , d 3 ] and S-recall@2 = 5  6 . However, if the ranking algorithm could explore (part of) the whole ranking space, it will found a better document ranking in terms of S-recall@2: [d 2 , d 3 , d 1 ] with S-recall@2 = 1. The example clearly indicate that greedy selection could lead to suboptimal rankings.</p><p>To alleviate the issue, following the practices in AlphaGo <ref type="bibr" target="#b26">[27]</ref> and AlphaGo Zero <ref type="bibr" target="#b27">[28]</ref>, we propose to conduct lookahead searches in the ranking space with MCTS. Speci cally, at each ranking position t, an MCTS search is executed, guided by the current policy function p and value function V , and output a strengthened new search policy π . It is believed that the search policy π will select a much better action (document) for the ranking position than that of selected by the raw policy p in Equation ( <ref type="formula" target="#formula_10">4</ref>). This is because the lookahead MCTS tries to explore the whole ranking space and can partially alleviate the suboptimal ranking problem.</p><p>Figure <ref type="figure" target="#fig_0">2</ref> illustrates the MCTS process and Algorithm 1 shows the details. Each node of the tree corresponds to an MDP state. The tree search algorithm takes a root node s R , number of search times K, value function V , policy function p, human labels , and the evaluation measure R as inputs. Note that and R are only used at the training time. The algorithm iterate K times and outputs a strengthened search policy π for selecting a document for root node s R . Suppose that each edge e(s, a) (the edge from state s to the state T (s, a)) of the MCTS tree stores an action value Q(s, a), visit count N (s, a), and prior probability P(s, a). The raw policy p(s R ) at the root state s R is strengthened with the following steps:</p><p>Selection (line 3 to line 7 of Algorithm 1): Each of the K iterations starts from the root state s R and iteratively selects the documents that maximize an upper con dence bound. Speci cally, at each time step t of each simulation, an action a t is selected from  state s t so as to maximize action value plus a bonus:</p><formula xml:id="formula_11">!, # = &amp; ' = {) * , ) + , ) , … } !, # = {) * } ' = {) + , ) , … } !, # = {) + } ' = {) * , ) , , … } !, #, ' !, #, ' !, #, ' !, #, ' / + 1 2 + 3 max max / + 1 2 + 3 / + 1 / + 1 !, # = &amp; ' = {) * , ) + , ) , … } !, # = {) * } ' = {) + , ) , … } !, # = {) + } ' = {) * , ) , , … } !, #, ' !, #, ' !, #, ' !, #, ' 4 = 5(7) !, # = &amp; ' = {) * , ) + , ) , … } !, # = {) * } ' = {) + , ) , … } !, # = {) + } ' = {) * , ) , , … } !, #, ' !, #, ' !, #, ' !, #, ' !, #, ' !, #, ' 2 = 9 : = 9 ; = &lt;(= &gt; |7) 2 = 9 : = 9 ; = &lt;(= @ |7) !, # = &amp; ' = {) * , ) + , ) , … } !, # = {) * } ' = {) + , ) , … } !, # = {) + } ' = {) * , ) , , … } !, #, ' !, #, ' !, #, ' !, #, ' !, #, ' !, #, '</formula><formula xml:id="formula_12">a t = arg max a (Q(s t , a) + λU (s t , a)),<label>(5)</label></formula><p>where λ ≥ 0 is the tradeo coe cient, and the bonus U (s t , a) is de ned as</p><formula xml:id="formula_13">U (s t , a) = p(a|s t ) a ∈A(s t ) N (s t , a ) 1 + N (s t , a) ,</formula><p>where p(a|s t ) is the predicted probability by the policy function p(s t ), A(s t ) is the set of available actions (documents) at state s t . U (s t , a) is proportional to the prior probability but decays with repeated visits to encourage exploration.</p><p>Evaluation and expansion (line 8 to line 19 of Algorithm 1): When the traversal reaches a leaf node s L , the node is evaluated either with the value function V (s L ) or with the prede ned performance measure if the node is the end of an episode and the human labels are available. Speci cally, at the test phase or online ranking phase, there is no human label information available. s L will be evaluated with the value function V (s L ). At the training phase, if s L is not the end of the episode, V (s L ) is used to conduct the evaluation. If s L is the last node of the episode (cannot be expanded), it is evaluated with the true performance measure (e.g., α-NDCG@M) at the position. Line 10 and line 18 of Algorithm 1 shows the evaluation details. Please note that following the practice in AlphaGo Zero program, we use the value function instead of rollouts for evaluating a node.</p><p>Then, the leaf node s L may be expanded (line 11 to line 16 of Algorithm 1). Each edge from the leaf position s L (corresponds to each action a ∈ A(s L )) is initialized as: P(s L , a) = p(a|s L ) (Equation (4)), Q(s L , a) = 0, and N (s L , a) = 0. In this paper all of the available actions of s L are expanded.</p><p>Back-propagation and update (line 20 to line 28 of Algorithm 1): At the end of evaluation, the action values and visit counts of all traversed edges are updated. For each edge e(s, a), the prior probability P(s, a) is kept unchanged, and Q(s, a) and N (s, a) are updated:</p><formula xml:id="formula_14">Q(s, a) ← Q (s,a)×N (s,a)+V (s L ) N (s,a)+1 , N (s, a) ← N (s, a) + 1.<label>(6)</label></formula><p>Calculate the strengthened search policy (line 29 to line 32 of Algorithm 1): After iterating K times, the strengthened search policy π for the root node s R can be calculated according to the visit counts N (s R , a) of the edges starting from s R :</p><formula xml:id="formula_15">π (a|s R ) = N (s R , a) a ∈A(s R ) N (s R , a ) ,<label>(7)</label></formula><p>for all a ∈ A(s R ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training with reinforcement learning</head><p>The model has parameters</p><formula xml:id="formula_16">Θ = {W f , W i , W o , U f , U i , U o , U p , b f , b i , b o , V h , V c</formula><p>, w} to learn. In the training phase, suppose we are given N labeled training queries {(q (n) , X (n) , (n) )} N n=1 , where (n) denotes the human labels on the documents, in the form of a binary matrix. (n) </p><formula xml:id="formula_17">(i, j) = 1 if document x (n)</formula><p>i contains the j-th subtopic of q (n) and 0 otherwise. Algorithm (2) shows the training procedure. First, the parameters Θ is initialized to random weights in [−1, 1]. At each subsequent iteration, for each query, a ranking of documents is generated with current parameter setting. At each ranking position t, an MCTS search is executed, using previous iteration of value function and policy function, and a document x m(a t ) is selected according to the search probabilities π t .</p><p>The ranking terminates when the candidate is empty or the ranking exceeds the maximum length de ned by the prede ned evaluation measure R. These sampled documents consist a permutation of documents τ , which is then evaluated with the evaluation measure to give a ground-truth return:  </p><formula xml:id="formula_18">r = R(τ , ).</formula><formula xml:id="formula_19">← V (s L ) R = NULL or = ∅ R(s L .Z, )</formula><formula xml:id="formula_20">π (a|s R ) ← e(s R ,a).N a ∈A(s R ) e(s R ,a ).N {e(s, a)</formula><p>is the edge from s to the state by taking action a} 32: end for 33: return π Here R can be any diverse ranking evaluation measure such as α-NDCG@M etc. The data generated at each time step</p><formula xml:id="formula_21">E = {(s t , π t )} T t =1</formula><p>and the nal evaluation r are utilized as the ground-truth signals in training for adjusting the value function. The model parameters are adjusted to minimize the error between the predicted value V (s t ) and the evaluation of the ranking in terms of the chosen evaluation measure, and to maximize the similarity of the raw policy p(s t ) to the search policy π t . Speci cally, the parameters Θ are adjusted by gradient descent on a loss function that sums over Algorithm 2 Train diverse ranking model for all (q, X , ) ∈ D do 4:</p><formula xml:id="formula_22">Input: Labeled training set D = {(q (n) , X (n) , (n) )} N n=1 ,</formula><p>s ← [q, ∅, X ] π ← TreeSearch(s, V , p, K, λ, , R) {Algorithm ( <ref type="formula" target="#formula_2">1</ref>): tree search using s as root, with current Θ} 9:</p><p>a = arg max a ∈A(s) π (a|s) {select the best document} 10:</p><formula xml:id="formula_23">τ (t + 1) ← m(a){document x m(a) is ranked at t + 1} 11: E ← E ⊕ {(s, π )} 12: s ← [q, s.Z ⊕ {x m(a) }, s.X \ {x m(a) }] 13:</formula><p>end for 14:</p><p>r ← R(τ , ){evaluating the generalized ranking} 15: end for 17: until converge 18: return Θ the mean-squared error and cross-entropy losses, respectively:</p><formula xml:id="formula_24">Θ ← Θ − η ∂ (E,</formula><formula xml:id="formula_25">(E, r ) = |E | t =1 (V (s t ) − r ) 2 + a ∈A(s t ) π t (a|s t ) log 1 p(a|s t ) .<label>(8)</label></formula><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates the construction of the loss given a training query. The model parameters are trained by back propagation and stochastic gradient descent. Speci cally, we use AdaGrad <ref type="bibr" target="#b7">[8]</ref> on all parameters in the training process.</p><p>Please note that following the practices in <ref type="bibr" target="#b27">[28]</ref>, the search tree constructed at the t-th iteration (line 8 to line 12 of Algorithm 2) is reused at subsequent steps: the child node corresponding to the selected document (chosen action) becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Online ranking</head><p>The construction of a diverse ranking for an online query is shown in Algorithm 3. Given a user query q, a set of M retrieved documents X , the system state is initialized as s 0 = [q, Z 0 = ∅, X 0 = X ]. Then, at each of the time steps t = 0, • • • , M − 1, the agent receives the state s t = [q, Z t , X t ] and searches the policy π with MCTS, on the basis of the value function V and policy function p. Then, it chooses an action a according π , selects the document x m(a) from the candidate set, and places it to the rank t + 1. Moving to the next step t + 1, the state becomes s t +1 = [q, Z t +1 , X t +1 ]. The process is repeated until the candidate set becomes empty.</p><p>Considering that the MCTS is time consuming and may be infeasible in some online ranking tasks, the online ranking algorithm can also skip the tree search and directly use the raw policy for Session 1D: Learning to Rank I SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA </p><formula xml:id="formula_26">! " ! # ! $ LSTM %('|)*) V()*) % " -" ≈ ≈ ≈ ≈ ≈ ≈ /, 1 * = 3 /, 1 " = {5 " } /, 1 # = {5 " , 5 # } LSTM %('|) " ) V() " ) % # -# LSTM %('|) # ) V() # ) % $ -$ ! " ! # ! $ 7 = 9 NDCG 5 " ~!" 5 # ~!# 5 ? ~!? /, 1 * = 3 @ * = {5 " , 5 # , 5 $ … } /, 1" = {5"} @" = {5#, 5$, 5B … } /, 1 # = {5 " , 5 # } @ # = {5 $ , 5 B , 5 C … } /, 1? = {5", 5# … } @? = {… } 7 7 7</formula><formula xml:id="formula_27">1: s ← [q, Φ, X ] 2: M ← |X | 3: for t = 0 to M − 1 do 4: π ← TreeSearch(s, V , p, K, λ, ∅, NULL) with MCTS, p<label>(s)</label></formula><p>without MCTS {Case 1: ranking with search policy. No human labels and evaluation function are available at the test/online ranking phase; Case 2: ranking with raw policy for accelerating online ranking speeds.} 5:</p><p>a ← arg max a ∈A(s) π (a|s) 6:</p><formula xml:id="formula_28">τ (t + 1) ← m(a){document x m(a) is ranked at t + 1} 7: [q, Z, X ] ← s 8:</formula><p>s ← [q, Z ⊕ {x m(a) }, X \ {x m(a) }] 9: end for 10: return τ ranking, denoted with "without MCTS" in the line 4 of Algorithm 3.</p><p>In the experiments, we observed that the M<ref type="foot" target="#foot_2">2</ref> Div without MCTS can still outperform the baselines. This is because the training process of M 2 Div can generate high quality episodes to train the model parameters with the help of MCTS, which leads to more accurate policy function p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Di erence with AlphaGo Zero</head><p>M 2 Div is inspired by the AlphaGo Zero program. It enjoys a number of merits from AlphaGo Zero, including the shared neural network for estimating policies and values, lookahead MCTS for strengthening the raw policy, and the compounded loss that simultaneously optimizes the value function and the policy function etc. However, M 2 Div has made a number of fundamental modi cations for search result diversi cation.</p><p>First, the formalizations of the tasks are di erent. AlphaGo Zero formalizes the playing of Go as an alternating Markov game where each action corresponds a move, the states are the raw board positions, the value function approximates the probability of winning, and the next state depends not only on the chosen action but also on the move by the opponent. M 2 Div, on the other hand, formalizes the ranking of documents as planning with an MDP where each action corresponds to a document selection. The MDP states consists of the query, the preceding document ranking, and the remaining candidates. The value function approximates a diverse ranking evaluation measure (e.g., α-NDCG@M). The state transition is fully determined by the current state and the chosen action.</p><p>Second, the supervision signals for learning the model parameters are di erent. AlphaGo Zero uses the results of self-play (-1 for loss, 0 for draw, and 1 for win) as the supervision signals and the value function is tted to the results. The task of diverse ranking, however, is not a Markov game. It is di cult (also unnecessary) to execute the self-play. In the training phase, M 2 Div resorts to the human labels and the prede ned evaluation measure to generate supervision information. Speci cally, the α-NDCG@M of each generated episode is calculated and used as the ground-truth to t the value function (the rst part of Equation ( <ref type="formula" target="#formula_25">8</ref>)). In this way, M 2 Div drives the training process so as to directly optimize the evaluation measure of α-NDCG@M. Note that α-NDCG@M can be replaced with any other diverse ranking evaluation measures.</p><p>Third, the shared deep neural networks for calculating the policies and values are di erent. AlphaGo Zero makes use of a residual network which takes the raw board position as its inputs and outputs the a probability distribution over moves, and a probability of the current player winning in position. The raw boards can be considered as some xed sized images. In M 2 Div, the MDP states consist of the queries and sequences of documents etc. Residual network cannot handle the state data as the queries/documents are raw texts, and the lengths of the document sequences vary in di erent time steps. To address the issue, M 2 Div rst represents the query and document sequence as the state vector of an LSTM (Equation <ref type="formula" target="#formula_8">3</ref>). The policy and the value are then calculated on the basis of the representations outputted by the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conducted experiments to test the performances of M 2 Div using a combination of four TREC benchmark datasets: TREC 2009 ∼ 2012 Web Track datasets (WT2009, WT2010, WT2011, and WT2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>In our experiments, for e ective training of the model parameters and following the practices in <ref type="bibr" target="#b32">[33]</ref>, we combined four TREC datasets and constructed a new dataset with 200 queries and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC assessors. The document relevance labels are made at the subtopic level and the labels are binary <ref type="foot" target="#foot_1">1</ref> .</p><p>All the experiments were carried out on the ClueWeb09 Category B data collection 2 , which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. We conducted 5-fold cross-validation experiments.</p><p>We randomly split the queries into ve even subsets. At each fold, three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the ve trials.</p><p>The TREC o cial diversity evaluation metrics of α-NDCG [6] and ERR-IA <ref type="bibr" target="#b4">[5]</ref> were used in the experiments, including. They measure the diversity of a result list by explicitly rewarding diversity and penalizing redundancy observed at every rank. Following the default settings in o cial TREC evaluation program, the parameter α in these evaluation measures are set to 0.5.</p><p>We compared M 2 Div with several state-of-the-art baselines in search result diversi cation:</p><p>MMR <ref type="bibr" target="#b2">[3]</ref>: a heuristic approach in which the document is selected according to maximal marginal relevance.</p><p>xQuAD <ref type="bibr" target="#b23">[24]</ref>: a representative method which explicitly models di erent aspects underlying original query in form of subqueries.</p><p>PM-2 <ref type="bibr" target="#b6">[7]</ref>: a method of optimizing proportionality for search result diversi cation.</p><p>We also compared MDP-DIV with the learning methods: SVM-DIV <ref type="bibr" target="#b35">[36]</ref>: a learning approach which utilizes structural SVMs to optimize the subtopic coverage. R-LTR <ref type="bibr" target="#b38">[39]</ref>: a learning approach developed in the relational learning to rank framework.</p><p>PAMM <ref type="bibr" target="#b30">[31]</ref>: a learning approach that directly optimizes diversity evaluation measure using structured Perceptron.</p><p>NTN-DIV <ref type="bibr" target="#b31">[32]</ref>: a learning approach which automatically learns novelty features based on neural tensor networks.</p><p>MDP-DIV <ref type="bibr" target="#b32">[33]</ref>: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process. Following the practice in <ref type="bibr" target="#b32">[33]</ref>, we con gured the reward function in MDP-DIV as α-DCG and the discounting parameter γ = 1.</p><p>M 2 Div, and the baselines of MDP-DIV and NTN-DIV need preliminary representations of the queries and the documents as their inputs. Following the practices in <ref type="bibr" target="#b32">[33]</ref>, in the experiments we used the query vector and document vector generated by the doc2vec <ref type="bibr" target="#b16">[17]</ref> to represent the document. Doc2vec model was trained on all of the documents in Web Track dataset and the number of vector dimensions were set to 100. For training the model, we used the distributed bag of words model 3 . The learning rate was set to 0.025 and the window size was set to 8.</p><p>The M 2 Div also has some parameters to tune: the training evaluation measure function R was set to α-NDCG@5, making the value function V to approximate α-NDCG@5. In the training phrase, the MCTS also was set to control the maximal length of the search depth less than 5. That is, the condition at line 9 of Algorithm 1 is true if the length of the tree depth is less than 5 and the candidate set is not empty. In all of the experiments, the learning rate η, the number of search times K, the tree search trade-o parameter λ, and the number of LSTM hidden units h were tuned based on the validation set and set to η = 0.01, K = 5000, λ = 3.0, and h = 5.</p><p>In online ranking phase, M 2 Div has two versions: selecting the documents with the raw policy p or with the MCTS strengthened policy π . These two versions are respectively denoted as "M 2 Div(without MCTS)" and "M 2 Div(with MCTS)". The source code of M 2 Div is available at https://github.com/sweetalyssum/M2DIV. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental results</head><p>Table <ref type="table" target="#tab_5">1</ref> reports the performances of our approach and all of the baseline methods in terms of diversity performance metrics including α-NDCG@5, α-NDCG@10, ERR-IA@5, and ERR-IA@10. Boldface indicates the highest scores among all runs. From the results we can see that, in terms of the four diversity evaluation metrics, "M 2 Div (with MCTS)" and "M 2 Div (without MCTS)" outperformed all of the baseline methods, including the heuristic method of MMR, xQuAD, PM-2 and learning methods of SVM-DIV, R-LTR, PAMM(α-NDCG), NTN-DIV(α-NDCG), and MDP-DIV(α-DCG). We conducted significant testing (t-test) on the improvements of our approaches over the best baseline MDP-DIV(α-DCG). The results indicate that most of the improvements are signi cant (p-value &lt; 0.05 and denoted with '*' in Table <ref type="table" target="#tab_5">1</ref>).</p><p>From the results we can see that "M 2 Div (without MCTS)", which did not conduct MCTS at the online time, still outperformed all of the baselines including "MDP-DIV(α-DCG)", indicating that the MCTS conducted at the training time can generate better episodes to estimate the model parameters, achieving better raw policy p for ranking. Note that "M 2 Div(with MCTS)", which conducted MCTS at the online time, further improved the ranking accuracies and performed the best among all of the methods. The results indicate that the MCTS can improve the raw policies p at both the training and the online ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>In this section, we conducted experiments to investigate how M 2 Div works and why it can outperform the baselines, using the experimental results on the rst fold of the data as example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">E ects of Monte Carlo tree search.</head><p>One key step in M 2 Div is the MCTS which outputs the search policy π . It is very likely that the search policy π (s) are better than raw policy p(s) in terms of choosing optimal documents.</p><p>We conducted experiments to show the e ectiveness of the MCTS in online ranking. Speci cally, based on the trained M 2 Div model, we tracked the online ranking process for query number 148 ("martha stewart and imclone"). The red real curve in Figure <ref type="figure" target="#fig_3">4</ref> shows the α-NDCG values of the document ranking generated by π . The blue dashed lines in Figure <ref type="figure" target="#fig_3">4</ref> show the α-NDCG values at these 5 positions if the corresponding document was chosen by the raw policy p (note the preceding documents are still selected by π ).</p><p>From the results, we can see that π improved p at the positions of 1, 3, and 5, showing the e ectiveness of MCTS at the online ranking. We also conducted experiments to show the e ectiveness of the MCTS in o ine training. Speci cally, at the end of each training iteration, based on current ranking model, we tested the performances of the document ranking constructed with the raw policy p and with the MCTS search policy π , in term of α-NDCG@5 on the training queries. Figure <ref type="figure" target="#fig_4">5</ref> shows the averaged α-NDCG@5 scores among all of the training queries at each training iterations. It is not surprise that the document rankings generated by the search policy π are superior to that of generated by the raw policy p, at all of the training iterations. The results indicate that the lookahead MCTS can generate better diverse document rankings for training the model parameters, at all of the training iterations. The results also partially explained why "M 2 Div(without MCTS)", which did not conduct MCTS at the online time, can still outperform the baselines. The MCTS conducted at the training time produced better diverse document rankings for training the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ranking with policy or with value?</head><p>In "M 2 Div(without MCTS)", we rely on the raw policy p for ranking the documents. In principle, the value function V can also be used for ranking, that is, at each state s a document (action) â is selected if â = arg max a ∈A(s) V (T (s, a)). We conducted experiments to show the performances of these two approaches and Figure <ref type="figure" target="#fig_5">6</ref> shows the test performance curves of the document rankings with the policy function and with the value function. From the results, we can see that the document rankings generated by the raw policy p are more stable and better than that of generated by the value function V . One possible reason for the phenomenon is that the ranking performance measure α-NDCG@5 are not easy to estimate accurately, especially at the early stages of the ranking procedure. This is why p in stead of V is adopted in "M 2 Div(without MCTS)".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper we have proposed a novel approach to learning diverse ranking model for search result diversi cation, referred to as M 2 Div. In contrast to existing methods that greedily select a locally optimal document at each of the ranking positions, M 2 Div conducts an exploratory decision making with the lookahead MCTS and thus can select a better document. MDP is used to model the ranking process and reinforcement learning is utilized to train the model parameters. M 2 Div o ers several advantages: ranking with both the shared policy function and the value function, high accuracy in ranking, and exibility of trading-o between accuracy and online ranking speed. Experimental results based on the TREC benchmark datasets show that M 2 Div can signi cantly outperform the state-ofthe-art baselines including the heuristic methods of MMR, xQuAD and learning methods of R-LTR, PAMM, and MDP-DIV.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The MCTS process for calculating the strengthened search policy π .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for t = 0 to M − 1 do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Construction of loss function for a training query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: α-NDCG at the top 5 positions for the ranking generated by the search policy π (red real curve) for query number 148. Blue dashed lines show the α-NDCG values if the documents were selected by the raw policy p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance curves of the document rankings generated with the raw policy p and with the search policy π , w.r.t. the training iterations. The curves illustrate the averaged performances over all of the training queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance curves of the document rankings generated with the policy function and with the value function, w.r.t. training iterations. The curves illustrates the averaged performances over all of the test queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Session 1D: Learning to Rank I SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</figDesc><table><row><cell></cell><cell>Repeat K times</cell><cell></cell><cell></cell></row><row><cell>Selection</cell><cell>Evaluation</cell><cell>Expansion</cell><cell>Back-propagation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TreeSearchInput: root state s R , value function V , and policy function p, number of search times K, trade-o parameter λ, human labels , and performance evaluation function R Output: Tree search probabilities π 1: for k = 0 to K − 1 do</figDesc><table><row><cell cols="2">Session 1D: Learning to Rank I</cell><cell>SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</cell></row><row><cell cols="2">Algorithm 1 2: s L ← s R</cell></row><row><cell>3:</cell><cell>{Selection}</cell></row><row><cell>4:</cell><cell>while s L is not a leaf node do</cell></row><row><cell>5:</cell><cell>a ← arg max a ∈A(s L ) Q(s L , a) + λ • U (s L , a){Equation (5), using the P, Q, and N stored in the corresponding edges}</cell></row><row><cell>6:</cell><cell>s L ← child node pointed by edge (s L , a)</cell></row><row><cell>7:</cell><cell>end while</cell></row><row><cell>8:</cell><cell>{Evaluation and expansion}</cell></row><row><cell>9:</cell><cell>if s L can be expanded then</cell></row><row><cell>10:</cell><cell>← V (s L ) {simulate with value function V }</cell></row><row><cell>11:</cell><cell></cell></row><row><cell>14:</cell><cell>e.Q ← 0</cell></row><row><cell>15:</cell><cell>e.N ← 0</cell></row><row><cell>16:</cell><cell>end for</cell></row><row><cell>17:</cell><cell>else</cell></row><row><cell>18:</cell><cell></cell></row></table><note>for all a ∈ A(s L ) do 12: Expand a new edge e which connects to a new node s = [q, s L .Z ⊕ {x m(a) }, s L .X \ {x m(a) }] 13: e.P ← p(a|s L ) {initialize the prior probability}</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>learning rate η, number of search steps K, MCTS trade-o parameter λ, and evaluation measure R</figDesc><table><row><cell>Output: Θ</cell></row><row><cell>1: Initialize Θ ← random values in [−1, 1]</cell></row><row><cell>2: repeat</cell></row><row><cell>3:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of all methods on TREC web track datasets.</figDesc><table><row><cell>Method</cell><cell cols="4">α-NDCG@5 α-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>MMR</cell><cell>0.2753</cell><cell>0.2979</cell><cell>0.2005</cell><cell>0.2309</cell></row><row><cell>xQuAD</cell><cell>0.3165</cell><cell>0.3941</cell><cell>0.2314</cell><cell>0.2890</cell></row><row><cell>PM-2</cell><cell>0.3047</cell><cell>0.3730</cell><cell>0.2298</cell><cell>0.2814</cell></row><row><cell>SVM-DIV</cell><cell>0.3030</cell><cell>0.3699</cell><cell>0.2268</cell><cell>0.2726</cell></row><row><cell>R-LTR</cell><cell>0.3498</cell><cell>0.4132</cell><cell>0.2521</cell><cell>0.3011</cell></row><row><cell>PAMM(α-NDCG)</cell><cell>0.3712</cell><cell>0.4327</cell><cell>0.2619</cell><cell>0.3029</cell></row><row><cell cols="2">NTN-DIV(α-NDCG) 0.3962</cell><cell>0.4577</cell><cell>0.2773</cell><cell>0.3285</cell></row><row><cell>MDP-DIV(α-DCG)</cell><cell>0.4189</cell><cell>0.4762</cell><cell>0.2988</cell><cell>0.3494</cell></row><row><cell>M 2 Div(without</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MCTS)</cell><cell>0.4386  *</cell><cell>0.4835</cell><cell>0.3435  *</cell><cell>0.3668  *</cell></row><row><cell cols="2">M 2 Div(with MCTS) 0.4424  *</cell><cell>0.4852</cell><cell>0.3459  *</cell><cell>0.3686  *</cell></row></table><note>3 http://radimrehurek.com/gensim/tutorial.html</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Session 1D: Learning to Rank I SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">WT2011 has graded judgements and we treat them as binary in the experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">http://boston.lti.cs.cmu.edu/data/clueweb09 Session 1D: Learning to Rank I SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Key R&amp;D Program of China under Grants No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61773362, 61425016, 61472401, 61722211, and 20180290, and the Youth Innovation Promotion Association CAS under Grants No. 20144310, and 2016102.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diversifying Search Results</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM &apos;09</title>
				<meeting>the Second ACM International Conference on Web Search and Data Mining (WSDM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multidimensional Search Result Diversi cation: Diverse Search Results for Diverse Users</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;11)</title>
				<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1331" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)</title>
				<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09</title>
				<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1287" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Expected Reciprocal Rank for Graded Relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09</title>
				<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Novelty and Diversity in Information Retrieval Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)</title>
				<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)</title>
				<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Axiomatic Approach for Result Diversi cation</title>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web (WWW &apos;09)</title>
				<meeting>the 18th International Conference on World Wide Web (WWW &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Maximal Marginal Relevance</title>
		<author>
			<persName><forename type="first">Shengbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;10)</title>
				<meeting>the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;10)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="833" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining Implicit and Explicit Topic Representations for Result Diversi cation</title>
		<author>
			<persName><forename type="first">Jiyin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjen</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)</title>
				<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Balancing Exploration and Exploitation in Listwise and Pairwise Online Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="63" to="90" />
			<date type="published" when="2013-02">2013. Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Search Result Diversi cation Based on Hierarchical Intents</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM &apos;15)</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management (CIKM &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised Search Result Diversi cation via Subtopic Attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Diversify Search Results via Subtopic Attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cascading Bandits: Learning to Rank in the Cascade Model</title>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Branislav Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Ashkan</surname></persName>
		</author>
		<idno>CoRR abs/1502.02763</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
				<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. 2014. June 2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing Diversity, Coverage and Balance for Summarization Through Structure Learning</title>
		<author>
			<persName><forename type="first">Liangda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web (WWW &apos;09)</title>
				<meeting>the 18th International Conference on World Wide Web (WWW &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Partially Observable Markov Decision Process for Recommender Systems</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR abs/1608.07793</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Win-win Search: Dual-agent Stochastic Game in Session Search</title>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Disambiguate Search Queries from Short Sessions</title>
		<author>
			<persName><forename type="first">Lilyana</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5782</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving Personalized Web Search Using Result Diversi cation</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;06)</title>
				<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;06)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="691" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Diverse Rankings with Multi-armed Bandits</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="784" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting Query Reformulations for Web Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web</title>
				<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Explicit Search Result Diversi cation through Sub-queries</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An MDP-Based Recommender System</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-12">2005. Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating Search Result Diversity Using Intent Hierarchies</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforcement Learning to Rank with Markov Decision Process</title>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adapting Markov Decision Process for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2017-01">2017. Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A concise integer linear programming formulation for implicit search result diversi cation</title>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predicting Diverse Subsets Using Structural SVMs</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML &apos;09)</title>
				<meeting>the 26th Annual International Conference on Machine Learning (ICML &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A POMDP Model for Contentfree Document Re-ranking</title>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1139" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
