<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Atomic Visibility with RAMP Transactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Fekete</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Atomic Visibility with RAMP Transactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">65C338FA62B8F8B6AB177A6D2068B4C6</idno>
					<idno type="DOI">10.1145/2588555.2588562</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Databases can provide scalability by partitioning data across several servers. However, multi-partition, multi-operation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many realworld systems avoid mechanisms that provide useful semantics for multi-partition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation model-Read Atomic (RA) isolation-that matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transaction's updates are observed by other transactions. We present algorithms for Read Atomic Multi-Partition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via synchronization independence), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multi-versioning and by allowing clients to independently resolve non-atomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overhead-even under high contention-and scale linearly to 100 servers.</p><p>1 Our use of "atomic" (specifically, Read Atomic isolation) concerns all-or-nothing visibility of updates (i.e., the ACID isolation effects of ACID atomicity; Section 3). This differs from uses of "atomicity" to denote serializability [8]  or linearizability <ref type="bibr" target="#b4">[4]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Faced with growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b43">43]</ref>. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. However, operations that access multiple partitions must communicate across servers-often synchronouslyin order to provide correct behavior. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>In this work, we address a largely underserved class of applications requiring multi-partition, atomically visible 1 transactional access: cases where all or none of each transaction's effects should be visible. The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure. Many of the largest modern, real-world systems opt for protocols that guarantee fast and scalable operation but provide few-if any-transactional semantics for operations on arbitrary sets of data items <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44]</ref>. This results in incorrect behavior for use cases that require atomic visibility, including secondary indexing, foreign key constraint enforcement, and materialized view maintenance (Section 2). In contrast, many traditional transactional mechanisms correctly ensure atomicity of updates <ref type="bibr">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b43">43]</ref>. However, these algorithms-such as two-phase locking and variants of optimistic concurrency control-are often coordination-intensive, slow, and, under failure, unavailable in a distributed environment <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35]</ref>. This dichotomy between scalability and atomic visibility has been described as "a fact of life in the big cruel world of huge systems" <ref type="bibr" target="#b25">[25]</ref>. The proliferation of non-transactional multi-item operations is symptomatic of a widespread "fear of synchronization" at scale <ref type="bibr" target="#b9">[9]</ref>.</p><p>Our contribution in this paper is to demonstrate that atomically visible transactions on partitioned databases are not at odds with scalability. Specifically, we provide high-performance implementations of a new, non-serializable isolation model called Read Atomic (RA) isolation. RA ensures that all or none of each transaction's updates are visible to others and that each transaction reads from an atomic snapshot of database state (Section 3)-this is useful in the applications we target. We subsequently develop three new, scalable algorithms for achieving RA isolation that we collectively title Read Atomic Multi-Partition (RAMP) transactions (Section 4). RAMP transactions guarantee scalability and outperform existing atomic algorithms because they satisfy two key scalability constraints. First, RAMP transactions guarantee synchronization independence: one client's transactions cannot cause another client's transactions to stall or fail. Second, RAMP transactions guarantee partition independence: clients never need to contact partitions that their transactions do not directly reference. Together, these properties ensure guaranteed completion, limited coordination across partitions, and horizontal scalability for multi-partition access.</p><p>RAMP transactions are scalable because they appropriately control the visibility of updates without inhibiting concurrency. Rather than force concurrent reads and writes to stall, RAMP transactions allow reads to "race" writes: RAMP transactions can autonomously detect the presence of non-atomic (partial) reads and, if necessary, repair them via a second round of communication with servers. To accomplish this, RAMP writers attach metadata to each write and use limited multi-versioning to prevent readers from stalling. The three algorithms we present offer a trade-off between the size of this metadata and performance. RAMP-Small transactions require constant space (a timestamp per write) and two round trip time delays (RTTs) for reads and writes. RAMP-Fast transactions require metadata size that is linear in the number of writes in the transaction but only require one RTT for reads in the common case and two in the worst case. RAMP-Hybrid transactions employ Bloom filters <ref type="bibr" target="#b10">[10]</ref> to provide an intermediate solution. Traditional techniques like locking couple atomic visibility and mutual exclusion; RAMP transactions provide the benefits of the former without incurring the scalability, availability, or latency penalties of the latter.</p><p>In addition to providing a theoretical analysis and proofs of correctness, we demonstrate that RAMP transactions deliver in practice. Our RAMP implementation achieves linear scalability to over 7 million operations per second on a 100 server cluster (at overhead below 5% for a workload of 95% reads). Moreover, across a range of workload configurations, RAMP transactions incur limited overhead compared to other techniques and achieve higher performance than existing approaches to atomic visibility (Section 5).</p><p>While the literature contains an abundance of isolation models <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5]</ref>, we believe that the large number of modern applications requiring RA isolation and the excellent scalability of RAMP transactions justify the addition of yet another model. RA isolation is too weak for some applications, but, for the many that it can serve, RAMP transactions offer substantial benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW AND MOTIVATION</head><p>In this paper, we consider the problem of making transactional updates atomically visible to readers-a requirement that, as we outline in this section, is found in several prominent use cases today. The basic property we provide is fairly simple: either all or none of each transaction's updates should be visible to other transactions. For example, if a transaction T 1 writes x = 1 and y = 1, then another transaction T 2 should not read x = 1 and y = null. Instead, T 2 should either read x = 1 and y = 1 or, possibly, x = null and y = null. Informally, each transaction reads from an unchanging snapshot of database state that is aligned along transactional boundaries. We call this property atomic visibility and formalize it via the Read Atomic isolation guarantee in Section 3.</p><p>The classic strategy for providing atomic visibility is to ensure mutual exclusion between readers and writers. For example, if a transaction like T 1 above wants to update data items x and y, it can acquire exclusive locks for each of x and y, update both items, then release the locks. No other transactions will observe partial updates to x and y, ensuring atomic visibility. However, this solution has a drawback: while one transaction holds exclusive locks on x and y, no other transactions can access x and y for either reads or writes. By using mutual exclusion to enforce the atomic visibility of updates, we have also limited concurrency. In our example, if x and y are located on different servers, concurrent readers and writers will be unable to perform useful work during communication delays. These communication delays form an upper bound on throughput: effectively, 1 message delay operations per second. To avoid this upper bound, we separate the problem of providing atomic visibility from the problem of maintaining mutual exclusion. By achieving the former but avoiding the latter, the algorithms we develop in this paper are not subject to the scalability penalties of many prior approaches. To ensure that all servers successfully execute a transaction (or that none do), our algorithms employ an atomic commitment protocol (ACP). When coupled with a blocking concurrency control mechanism like locking, ACPs are harmful to scalability and availability: arbitrary failures can (provably) cause any ACP implementation to stall <ref type="bibr">[8]</ref>. (Optimistic concurrency control mechanisms can similarly block during validation.) We instead use ACPs with non-blocking concurrency control mechanisms; this means that individual transactions can stall due to failures or communication delays without forcing other transactions to stall. In a departure from traditional concurrency control, we allow multiple ACP rounds to proceed in parallel over the same data.</p><p>The end result-our RAMP transactions-provide excellent scalability and performance under contention (e.g., in the event of write hotspots) and are robust to partial failure. RAMP transactions' nonblocking behavior means that they cannot provide certain guarantees like preventing concurrent updates. However, applications that can use Read Atomic isolation will benefit from our algorithms. The remainder of this section identifies several relevant use cases from industry that require atomic visibility for correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Read Atomic Isolation in the Wild</head><p>As a simple example, consider a social networking application: if two users, Sam and Mary, become "friends" (a bi-directional relationship), other users should never see that Sam is a friend of Mary but Mary is not a friend of Sam: either both relationships should be visible, or neither should be. A transaction under Read Atomic isolation would correctly enforce this behavior, and we can further classify three general use cases for Read Atomic isolation: 1.) Foreign key constraints. Many database schemas contain information about relationships between records in the form of foreign key constraints. For example, Facebook's TAO <ref type="bibr" target="#b11">[11]</ref>, LinkedIn's Espresso <ref type="bibr" target="#b38">[38]</ref>, and Yahoo! PNUTS <ref type="bibr" target="#b15">[15]</ref> store information about business entities such as users, photos, and status updates as well as relationships between them (e.g., the friend relationships above). Their data models often represent bi-directional edges as two distinct uni-directional relationships. For example, in TAO, a user performing a "like" action on a Facebook page produces updates to both the LIKES and LIKED_BY associations <ref type="bibr" target="#b11">[11]</ref>. PNUTS's authors describe an identical scenario <ref type="bibr" target="#b15">[15]</ref>. These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access. Violations of atomic visibility surface as broken bi-directional relationships (as with Sam and Mary above) and dangling or incorrect references (e.g., Frank is an employee of department.id=5, but no such department exists in the department table ).</p><p>With RAMP transactions, when inserting new entities, applications can bundle relevant entities from each side of a foreign key constraint into a transaction. When deleting associations, users can "tombstone" the opposite end of the association (i.e., delete any entries with associations via a special record that signifies deletion) <ref type="bibr" target="#b45">[45]</ref> to avoid dangling pointers. 2.) Secondary indexing. Data is typically partitioned across servers according to a primary key (e.g., user ID). This allows fast location and retrieval of data via primary key lookups but makes access by secondary attributes (e.g., birth date) challenging. There are two dominant strategies for distributed secondary indexing. First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b38">38]</ref>. This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b38">38]</ref>. Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15]</ref>. This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two).</p><p>Real-world services employ either local secondary indexing (e.g., Espresso <ref type="bibr" target="#b38">[38]</ref>, Cassandra, and Google Megastore's local indexes <ref type="bibr" target="#b7">[7]</ref>) or non-atomic (incorrect) global secondary indexing (e.g., Espresso and Megastore's global indexes, Yahoo! PNUTS's proposed secondary indexes <ref type="bibr" target="#b15">[15]</ref>). The former is non-scalable but correct, while the latter is scalable but incorrect. For example, in a database partitioned by id with an incorrectly-maintained global secondary index on salary, the query 'SELECT id, salary WHERE salary &gt; 6 , ' might return records with salary less than $60,000 and omit some records with salary greater than $60,000.</p><p>With RAMP transactions, the secondary index entry for a given attribute can be updated atomically with base data. For example, if a secondary index is stored as a mapping from secondary attribute values to sets of item-versions matching the secondary attribute (e.g., the secondary index entry for users with blue hair would contain a list of user IDs and last-modified timestamps corresponding to all of the users with attribute hair-color=blue), then insertions of new primary data require additions to the corresponding index entry, deletions require removals, and updates require a "tombstone" deletion from one entry and an insertion into another. 3.) Materialized view maintenance. Many applications precompute (i.e., materialize) queries over data, as in Twitter's Rainbird service <ref type="bibr" target="#b44">[44]</ref>, Google's Percolator <ref type="bibr" target="#b36">[36]</ref>, and LinkedIn's Espresso systems <ref type="bibr" target="#b38">[38]</ref>. As a simple example, Espresso stores a mailbox of messages for each user along with statistics about the mailbox messages: for Espresso's read-mostly workload, it is more efficient to maintain (i.e., pre-materialize) a count of unread messages rather than scan all messages every time a user accesses her mailbox <ref type="bibr" target="#b38">[38]</ref>. In this case, any unread message indicators should remain in sync with the messages in the mailbox. However, atomicity violations will allow materialized views to diverge from the base data (e.g., Susan's mailbox displays a notification that she has unread messages but all 63, 201 messages in her inbox are marked as read).</p><p>With RAMP transactions, base data and views can be updated atomically. The physical maintenance of a view depends on its specification <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b27">27]</ref>, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition. For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance <ref type="bibr" target="#b46">[46]</ref>). For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter <ref type="bibr" target="#b40">[40]</ref>.</p><p>In brief: Status Quo. Despite application requirements for Read Atomic isolation, few large-scale production systems provide it. For example, the authors of Tao, Espresso, and PNUTS describe several classes of atomicity anomalies exposed by their systems, ranging from dangling pointers to the exposure of intermediate states and incorrect secondary index lookups, often highlighting these cases as areas for future research and design <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b38">38]</ref>. These systems are not exceptions: data stores like Bigtable <ref type="bibr" target="#b13">[13]</ref>, Dynamo <ref type="bibr" target="#b22">[22]</ref>, and many popular "NoSQL" <ref type="bibr" target="#b34">[34]</ref> and even some "NewSQL" <ref type="bibr" target="#b5">[5]</ref> stores do not provide transactional guarantees for multi-item operations.</p><p>The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics. Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEMANTICS AND SYSTEM MODEL</head><p>In this section, we formalize Read Atomic isolation and, to capture scalability, formulate a pair of strict scalability criteria: synchronization and partition independence. Readers more interested in RAMP algorithms may wish to proceed to Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RA Isolation: Formal Specification</head><p>To formalize RA isolation, as is standard <ref type="bibr" target="#b2">[2]</ref>, we consider ordered sequences of reads and writes to arbitrary sets of items, or transactions. We call the set of items a transaction reads from and writes to its read set and write set. Each write creates a version of an item and we identify versions of items by a unique timestamp taken from a totally ordered set (e.g., rational numbers). Timestamps induce a total order on versions of each item (and a partial order across versions of different items). We denote version i of item x as x i .</p><p>A transaction T j exhibits fractured reads if transaction T i writes versions x m and y n (in any order, with x possibly but not necessarily equal to y), T j reads version x m and version y k , and k &lt; n.</p><p>A system provides Read Atomic isolation (RA) if it prevents fractured reads anomalies and also prevents transactions from reading uncommitted, aborted, or intermediate data. Thus, RA provides transactions with a "snapshot" view of the database that respects transaction boundaries (see the Appendix for more details, including a discussion of transitivity). RA is simply a restriction on write visibility-if the ACID "Atomicity" property requires that all or none of a transaction's updates are performed, RA requires that all or none of a transaction's updates are made visible to other transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RA Implications and Limitations</head><p>As outlined in Section 2.1, RA isolation matches many of our use cases. However, RA is not sufficient for all applications. RA does not prevent concurrent updates or provide serial access to data items. For example, RA is an incorrect choice for an application that wishes to maintain positive bank account balances in the event of withdrawals. RA is a better fit for our "friend" operation because the operation is write-only and correct execution (i.e., inserting both records) is not conditional on concurrent updates.</p><p>From a programmer's perspective, we have found RA isolation to be most easily understandable (at least initially) with read-only and write-only transactions; after all, because RA allows concurrent writes, any values that are read might be changed at any time. However, read-write transactions are indeed well defined under RA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System Model and Scalability</head><p>We consider databases that are partitioned, with the set of items in the database spread over multiple servers. Each item has a single logical copy, stored on a server-called the item's partition-whose identity can be calculated using the item. Clients forward operations on each item to the item's partition, where they are executed. Transaction execution terminates in commit, signaling success, or abort, signaling failure. In our examples, all data items have the null value (?) at database initialization. We do not model replication of data items within a partition; this can happen at a lower level of the system than our discussion (see Section 4.6) as long as operations on each item are linearizable <ref type="bibr" target="#b4">[4]</ref>.</p><p>Scalability criteria. As we hinted in Section 1, large-scale deployments often eschew transactional functionality on the premise that it would be too expensive or unstable in the presence of failure and degraded operating modes <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44]</ref>. Our goal in this paper is to provide robust and scalable transactional functionality, and, so we first define criteria for "scalability": Synchronization independence ensures that one client's transactions cannot cause another client's to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition). This prevents one transaction from causing another to abort-which is particularly important in the presence of partial failures-and guarantees that each client is able to make useful progress. In the absence of failures, this maximizes useful concurrency. In the distributed systems literature, synchronization independence for replicated transactions is called transactional availability <ref type="bibr" target="#b5">[5]</ref>. Note that "strong" isolation models like serializability and Snapshot Isolation violate synchronization independence and limit scalability.</p><p>While many applications can limit their data accesses to a single partition via explicit data modeling <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b38">38]</ref> or planning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35]</ref>, this is not always possible. In the case of secondary indexing, there is a tangible cost associated with requiring single-partition updates (scatter-gather reads), while, in social networks like Facebook and large-scale hierarchical access patterns as in Rainbird, perfect partitioning of data accesses is near-impossible. Accordingly: Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access. Thus, a partition failure only affects transactions that access items contained on the partition. This also reduces load on servers not directly involved in a transaction's execution. In the distributed systems literature, partition independence for replicated data is called replica availability <ref type="bibr" target="#b5">[5]</ref> or genuine partial replication <ref type="bibr" target="#b39">[39]</ref>.</p><p>In addition to the above requirements, we limit the metadata overhead of algorithms. There are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state. As a straw-man solution, each transaction could send copies of all of its writes to every partition it accesses so that readers observe all of its writes by reading a single item. This provides RA isolation but requires considerable storage. Other solutions may require extra data storage proportional to the number of servers in the cluster or, worse, the database size (Section 6). We will attempt to minimize this metadata-that is, data that the transaction did not itself write but which is required for correct execution. In our algorithms, we will specifically provide constant-factor metadata overheads (RAMP-S, RAMP-H) or else overhead linear in transaction size (but independent of data size; RAMP-F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RAMP TRANSACTION ALGORITHMS</head><p>Given specifications for RA isolation and scalability, we present algorithms for achieving both. For ease of understanding, we first focus on providing read-only and write-only transactions with a "last writer wins" overwrite policy, then subsequently discuss how to perform read/write transactions. Our focus in this section is on intuition and understanding; we defer all correctness and scalability proofs to the Appendix, providing salient details inline.</p><p>At a high level, RAMP transactions allow reads and writes to proceed concurrently. This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction's writes, violating RA (i.e., fractured reads might occur). Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions. To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions.</p><p>In this section, we present three algorithms that provide a trade-off between the amount of metadata required and the expected number of extra reads to fetch missing writes. As discussed in Section 2, if techniques like distributed locking couple mutual exclusion with atomic visibility of writes, RAMP transactions correctly control visibility but allow concurrent and scalable execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RAMP-Fast</head><p>To begin, we present a RAMP algorithm that, in the race-free case, requires one RTT for reads and two RTTs for writes, called RAMP-Fast (abbreviated RAMP-F; Algorithm 1). RAMP-F stores metadata in the form of write sets (overhead linear in transaction size). Overview. Each write in RAMP-F (lines 14-21) contains a timestamp (line 15) that uniquely identifies the writing transaction as well as a set of items written in the transaction (line 16). For now, combining a unique client ID and client-local sequence number is sufficient for timestamp generation (see also Section 4.5). RAMP-F write transactions proceed in two phases: a first round of communication places each timestamped write on its respective partition. In this PREPARE phase, each partition adds the write to its local database (versions, lines 1, <ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref>. A second round of communication marks versions as committed. In this COMMIT phase, each partition updates an index containing the highest-timestamped committed version of each item (lastCommit, lines 2, 20-21).</p><formula xml:id="formula_0">C 1 PREPARE PREPARE lastCommit[x]= ∅ versions={x 1 } x 1 , md={y} y 1 , md={x} lastCommit[y]= ∅ versions={y 1 } COMMIT ts c =1 COMMIT ts c =1 GET i=x, ts req = ∅ GET lastCommit[x]=1 versions={x 1 } GET x 1 , md={y} y ∅ , md={} y 1 , md={x} lastCommit[y]=1 versions={y 1 } v latest ←{x:1, y:1} prepared RESPONSE prepared i=y, ts req = ∅ i=y, ts req =1 lastCommit[x]= ∅ versions={} lastCommit[y]= ∅ versions={} BEGIN T 1 committed committed BEGIN T 2 COMMIT T 1 COMMIT T 2 resp={x 1 ,y 1 } RESPONSE RESPONSE RESPONSE RESPONSE RESPONSE RESPONSE C 2 P x P y [w(x 1 ), w(y 1 )] [r(x), r(y)]</formula><p>RAMP-F read transactions begin by first fetching the last (highesttimestamped) committed version for each item from its respective partition (lines <ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref>. Using the results from this first round of reads, each reader can calculate whether it is "missing" any versions (that is, versions that were prepared but not yet committed on their partitions). Combining the timestamp and set of items from each version read (i.e., its metadata) produces a mapping from items to timestamps that represent the highest-timestamped write for each transaction that appears in this first-round read set (lines 26-29). If the reader has read a version of an item that has a lower timestamp than indicated in the mapping for that item, the reader issues a second read to fetch the missing version (by timestamp) from its partition (lines <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref>. Once all missing versions are fetched (which can be done in parallel), the client can return the resulting set of versions-the first-round reads, with any missing versions replaced by the optional, second round of reads.</p><p>By example. Consider the RAMP-F execution depicted in Figure <ref type="figure" target="#fig_0">1</ref>. T 1 writes to both x and y, performing the two-round write protocol on two partitions, P x and P y . However, T 2 reads from x and y while T 1 is concurrently writing. Specifically, T 2 reads from P x after P x has committed T 1 's write to x, but T 2 reads from P y before P y has committed T 1 's write to y. Therefore, T 2 's first-round reads return x = x 1 and y = ?, and returning this set of reads would violate RA. Using the metadata attached to its first-round reads, T 2 determines that it is missing y 1 (since v latest [y] = 1 and 1 &gt; ?) and so T 2 subsequently issues a second read from P y to fetch y 1 by version. After completing its second-round read, T 2 can safely return its result set. T 1 's progress is unaffected by T 2 , and T 1 subsequently completes by committing y 1 on P y .</p><p>Why it works. RAMP-F writers use metadata as a record of intent: a reader can detect if it has raced with an in-progress commit round and use the metadata stored by the writer to fetch the missing data. Accordingly, RAMP-F readers only issue a second round of reads in the event that they read from a partially-committed write transaction (where some but not all partitions have committed a write). In this event, readers will fetch the appropriate writes from the not-yetcommitted partitions. Most importantly, RAMP-F readers never have to stall waiting for a write that has not yet arrived at a partition: the two-round RAMP-F write protocol guarantees that, if a partition commits a write, all of the corresponding writes in the transaction are present on their respective partitions (though possibly not committed locally). As long as a reader can identify the corresponding version by timestamp, the reader can fetch the version from the respective partition's set of pending writes without waiting. To enable this, RAMP-F writes contain metadata linear in the size of the writing transaction's write set (plus a timestamp per write).</p><p>RAMP-F requires 2 RTTs for writes: one for PREPARE and one for COMMIT. For reads, RAMP-F requires one RTT in the absence of concurrent writes and two RTTs otherwise.</p><p>RAMP timestamps are only used to identify specific versions and in ordering concurrent writes to the same item; RAMP-F transactions do not require a "global" timestamp authority. For example, if lastCommit[k] = 2, there is no requirement that a transaction with timestamp 1 has committed or even that such a transaction exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RAMP-Small: Trading Metadata for RTTs</head><p>While RAMP-F requires linearly-sized metadata but provides bestcase one RTT for reads, RAMP-Small (RAMP-S) uses constant-size metadata but always requires two RTT for reads (Algorithm 2). RAMP-S and RAMP-F writes are identical, but, instead of attaching the entire write set to each write, RAMP-S writers only store the transaction timestamp (line 7). Unlike RAMP-F, RAMP-S readers issue a first round of reads to fetch the highest committed timestamp for each item from its respective partition (lines 3, 9-11). Once RAMP-S readers have recieved the highest committed timestamp for each item, the readers send the entire set of timestamps they received to the partitions in a second round of communication (lines <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref>. For each item in the read request, RAMP-S servers return the highesttimestamped version of the item that also appears in the supplied set of timestamps (lines 5-6). Readers subsequently return the results from the mandatory second round of requests. By example. In Figure <ref type="figure" target="#fig_0">1</ref>, under RAMP-S, P x and P y would respectively return the sets {1} and {?} in response to T 2 's first round of reads. T 2 would subsequently send the set {1, ?} to both P x and P y , which would return x 1 and y 1 . (Including ? in the second-round request is unnecessary, but we leave it in for ease of understanding.)</p><p>Why it works. In RAMP-S, if a transaction has committed on some but not all partitions, the transaction timestamp will be returned in the first round of any concurrent read transaction accessing the committed partitions' items. In the (required) second round of read requests, any prepared-but-not-committed partitions will find the committed timestamp in the reader-provided set and return the appropriate version. In contrast with RAMP-F, where readers explicitly provide partitions with a specific version to return in the (optional) second round, RAMP-S readers defer the decision of which version to return to the partition, which uses the reader-provided set to decide. This saves metadata but increases RTTs, and the size of the parameters of each second-round GET request is (worst-case) linear in the read set size. Unlike RAMP-F, there is no requirement to return the value of the last committed version in the first round (returning the version, lastCommit[k], suffices in line 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RAMP-Hybrid: An Intermediate Solution</head><p>RAMP-Hybrid (RAMP-H; Algorithm 3) strikes a compromise between RAMP-F and RAMP-S. RAMP-H and RAMP-S write protocols are identical, but, instead of storing the entire write set (as in RAMP-F), RAMP-H writers store a Bloom filter <ref type="bibr" target="#b10">[10]</ref> representing the transaction write set (line 1). RAMP-H readers proceed as in RAMP-F, with a first round of communication to fetch the last-committed version of each item from its partition (lines 3-5). Given this set of versions, RAMP-H readers subsequently compute a list of potentially highertimestamped writes for each item (lines 7-10). Any potentially missing versions are fetched in a second round of reads (lines 12).</p><p>By example. In Figure <ref type="figure" target="#fig_0">1</ref>, under RAMP-H, x 1 would contain a Bloom filter with positives for x and y and y ? would contain an empty Bloom filter. T 2 would check for the presence of y in x 1 's Bloom filter (since x 1 's version is 1 and 1 &gt; ?) and, finding a match, conclude that it is potentially missing a write (y 1 ). T 2 would subsequently fetch y 1 from P y .</p><p>Why it works. RAMP-H is effectively a hybrid between RAMP-F and RAMP-S. If the Bloom filter has no false positives, RAMP-H reads behave like RAMP-F reads. If the Bloom filter has all false positives, RAMP-H reads behave like RAMP-S reads. Accordingly, the number of (unnecessary) second-round reads (i.e., which would not be performed by RAMP-F) is controlled by the Bloom filter false positive rate, which is in turn (in expectation) proportional to the size of the Bloom filter. Any second-round GET requests are accompanied by a set of timestamps that is also proportional in size to the false positive rate. Therefore, RAMP-H exposes a trade-off between metadata size and expected performance. To understand why RAMP-H is safe, we simply have to show that any false positives (second-round reads) will not compromise the integrity of the result set; with unique timestamps, any reads due to false positives will return null.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary of Basic Algorithms</head><p>The RAMP algorithms allow readers to safely race writers without requiring either to stall. The metadata attached to each write allows readers in all three algorithms to safely handle concurrent and/or partial writes and in turn allows a trade-off between metadata size and performance (Table <ref type="table" target="#tab_3">1</ref>): RAMP-F is optimized for fast reads, RAMP-S is optimized for small metadata, and RAMP-H is, as the name suggests, a middle ground. RAMP-F requires metadata linear in transaction size, while RAMP-S and RAMP-H require constant metadata. However, RAMP-S and RAMP-H require more RTTs for reads compared to RAMP-F when there is no race between readers and writers. parallel-for i 2 I 5:</p><formula xml:id="formula_1">ret[i] GET(i, / 0) 6: v f etch {} 7:</formula><p>for version v 2 ret do 8:</p><p>for version v 0 2 ret : When reads and writes race, in the worst case, all algorithms require two RTTs for reads. Writes always require two RTTs to prevent readers from stalling due to missing, unprepared writes. RAMP algorithms are scalable because clients only contact partitions relative to their transactions (partition independence), and clients cannot stall one another (synchronization independence). More specifically, readers do not interfere with other readers, writers do not interfere with other writers, and readers and writers can proceed concurrently. When a reader races a writer to the same items, the writer's new versions will only become visible to the reader (i.e., be committed) once it is guaranteed that the reader will be able to fetch all of them (possibly via a second round of communication). A reader will never have to stall waiting for writes to arrive at a partition (for details, see Invariant 1 in the Appendix).</p><formula xml:id="formula_2">v 0 6 = v do 9: if v.ts v &gt; v 0 .ts v ^v.md.lookup(v 0 .item) ! True then 10: v f etch [v 0 .item].add(v.ts v ) 11: parallel-for item i 2 v f etch 12: ret[i] GET(k, v f etch [i]) if GET(k, v f etch [i])<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Additional Details</head><p>In this section, we discuss relevant implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-versioning and garbage collection. RAMP transactions rely on multi-versioning to allow readers to access versions that</head><p>have not yet committed and/or have been overwritten. In our initial presentation, we have used a completely multi-versioned storage engine; in practice, multi-versioning can be implemented by using a single-versioned storage engine for retaining the last committed version of each item and using a "look-aside" store for access to both prepared-but-not-yet-committed writes and (temporarily) any overwritten versions. The look-aside store should make prepared versions durable but can-at the risk of aborting transactions in the event of a server failure-simply store any overwritten versions in memory. Thus, with some work, RAMP algorithms are portable to legacy, non-multi-versioned storage systems.</p><p>In both architectures, each partition's data will grow without bound if old versions are not removed. If a committed version of an item is not the highest-timestamped committed version (i.e., a committed version v of item k where v &lt; lastCommit[k]), it can be safely discarded (i.e., garbage collected, or GCed) as long as no readers will attempt to access it in the future (via second-round GET requests). It is easiest to simply limit the running time of read transactions and GC overwritten versions after a fixed amount of real time has elapsed. Any read transactions that take longer than this GC window can be restarted <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>. Therefore, the maximum number of versions retained for each item is bounded by the item's update rate, and servers can reject any client GET requests for versions that have been GCed (and the read transaction can be restarted). As a more principled solution, partitions can also gossip the timestamps of items that have been overwritten and have not been returned in the first round of any ongoing read transactions.</p><p>Read-write transactions. Until now, we have focused on readonly and write-only transactions. However, we can extend our algorithms to provide read-write transactions. If transactions predeclare the data items they wish to read, then the client can execute a GET_ALL transaction at the start of transaction execution to prefetch all items; subsequent accesses to those items can be served from this pre-fetched set. Clients can buffer any writes and, upon transaction commit, send all new versions to servers (in parallel) via a PUT_ALL request. As in Section 3, this may result in anomalies due to concurrent update but does not violate RA isolation. Given the benefits of pre-declared read/write sets <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b43">43]</ref> and write buffering <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b41">41]</ref>, we believe this is a reasonable strategy. For secondary index lookups, clients can first look up secondary index entries then subsequently (within the same transaction) read primary data (specifying versions from index entries as appropriate).</p><p>Timestamps. Timestamps should be unique across transactions, and, for "session" consistency (Appendix), increase on a per-client basis. Given unique client IDs, a client ID and sequence number form unique transaction timestamps without coordination. Without unique client IDs, servers can assign unique timestamps with high probability using UUIDs and by hashing transaction contents.</p><p>Overwrites. In our algorithms, we have depicted a policy in which versions are overwritten according to a highest-timestamp-wins policy. In practice, and, for commutative updates, users may wish to employ a different policy upon COMMIT: for example, perform set union. In this case, lastCommit[k] contains an abstract data type (e.g., set of versions) that can be updated with a merge operation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b42">42]</ref> (instead of updateI f Greater) upon commit. This treats each committed record as a set of versions, requiring additional metadata (that can be GCed as in Section 4.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Distribution and Fault Tolerance</head><p>RAMP transactions operate in a distributed setting, which poses challenges due to latency, partial failure, and network partitions. Synchronization independence ensures that failed clients do not cause other clients to fail, while partition independence ensures that clients only have to contact partitions for items in their transactions. This provides fault tolerance and availability as long as clients can access relevant partitions, but here we further elucidate RAMP interactions with replication and stalled operations.</p><p>Replication. A variety of mechanisms including traditional database master-slave replication with failover, quorum-based protocols, and state machine replication and can ensure availability of individual partitions in the event of individual server failure <ref type="bibr">[8]</ref>. To control durability, clients can wait until the effects of their operations (e.g., modifications to versions and lastCommit) are persisted locally on their respective partitions and/or to multiple physical servers before returning from PUT_ALL calls (either via master-to-slave replication or via quorum replication and by performing two-phase commit across multiple active servers). Notably, because RAMP transactions can safely overlap in time, replicas can process different transactions' PREPARE and COMMIT requests in parallel.</p><p>Stalled Operations. RAMP writes use a two-phase atomic commitment protocol that ensures readers never block waiting for writes to arrive. As discussed in Section 2, every ACP may block during failures <ref type="bibr">[8]</ref>. However, due to synchronization independence, a blocked transaction (due to failed clients, failed servers, or network partitions) cannot cause other transactions to block. Blocked writes instead act as "resource leaks" on partitions: partitions will retain prepared versions indefinitely unless action is taken.</p><p>To "free" these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP) described in <ref type="bibr">[8]</ref>. CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT. In CTP, if a server S p has performed PREPARE for transaction T but times out waiting for a COMMIT, S p can check the status of T on any other partitions for items in T 's write set. If another server S c has received COMMIT for T , then S p can COMMIT T . If S a , a server responsible for an item in T , has not received PREPARE for T , S a and S p can promise never to PREPARE or COMMIT T in the future and S p can safely discard its versions. A client recovering from a failure can read from the servers to determine if they unblocked its write. Writes that block mid-COMMIT will also become visible on all partitions. CTP (evaluated in Section 5) only runs when writes block (or time-outs fire) and runs asynchronously with respect to other operations. CTP requires that PREPARE messages contain a list of servers involved in the transaction (a subset of RAMP-F metadata but a superset of RAMP-H and RAMP-S) and that servers remember when they COMMIT and "abort" writes (e.g., in a log file). Compared to alternatives (e.g., replicating clients <ref type="bibr" target="#b24">[24]</ref>), we have found CTP to be both lightweight and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Further Optimizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAMP algorithms also allow several possible optimizations:</head><p>Faster commit detection. If a server returns a version in response to a GET request and the version's timestamp is greater than the highest committed version of that item (i.e., lastCommit), then transaction writing the version has committed on at least one partition. In this case, the server can mark the version as committed. This scenario will occur when all partitions have performed PREPARE and at least one server but not all partitions have performed COMMIT (as in CTP). This allows faster updates to lastCommit (and therefore fewer expected RAMP-F and RAMP-H RTTs).</p><p>Metadata garbage collection. Once all of transaction T 's writes are committed on each respective partition (i.e., are reflected in lastCommit), readers are guaranteed to read T 's writes (or later writes). Therefore, non-timestamp metadata for T 's writes stored in RAMP-F and RAMP-H (write sets and Bloom filters) can therefore be discarded. Detecting that all servers have performed COMMIT can be performed asynchronously via a third round of communication performed by either clients or servers.</p><p>One-phase writes. We have considered two-phase writes, but, if a user does not wish to read her writes (thereby sacrificing session guarantees outlined in the Appendix), the client can return after issuing its PREPARE round (without sacrificing durability). The client can subsequently execute the COMMIT phase asynchronously, or, similar to optimizations presented in Paxos Commit <ref type="bibr" target="#b24">[24]</ref>, the servers can exchange PREPARE acknowledgements with one another and decide to COMMIT autonomously. This optimization is safe because multiple PREPARE phases can safely overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL EVALUATION</head><p>We proceed to experimentally demonstrate RAMP transaction scalability as compared to existing transactional and non-transactional mechanisms. RAMP-F, RAMP-H, and often RAMP-S outperform existing solutions across a range of workload conditions while exhibiting overheads typically within 8% and no more than 48% of peak throughput. As expected from our theoretical analysis, the performance of our RAMP algorithms does not degrade substantially under contention and scales linearly to over 7.1 million operations per second on 100 servers. These outcomes validate our choice to pursue synchronization-and partition-independent algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>To demonstrate the effect of concurrency control on performance and scalability, we implemented several concurrency control algorithms in a partitioned, multi-versioned, main-memory database prototype. Our prototype is in Java and employs a custom RPC system with Kryo 2.20 for serialization. Servers are arranged as a distributed hash table with partition placement determined by random hashing. As in stores like Dynamo <ref type="bibr" target="#b22">[22]</ref>, clients can connect to any server to execute operations, which the server will perform on their behalf (i.e., each server acts as a client in our RAMP pseudocode). We implemented RAMP-F, RAMP-S, and RAMP-H and configure a wall-clock GC window of 5 seconds as described in Section 4.5. RAMP-H uses a 256-bit Bloom filter based on an implementation of MurmurHash2.0, with four hashes per entry; to demonstrate the effects of filter saturation, we do not modify these parameters in our experiments. Our prototype utilizes the "Faster commit detection" optimization from Section 4.5 but we chose not to employ the latter two optimizations in order to preserve session guarantees and because metadata overheads were generally minor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms for comparison.</head><p>As a baseline, we do not employ any concurrency control (denoted NWNR, for no write and no read locks); reads and writes take one RTT and are executed in parallel.</p><p>We also consider three lock-based mechanisms: long write locks and long read locks, providing Repeatable Read isolation (PL-2.99; denoted LWLR), long write locks with short read locks, providing Read Committed isolation (PL-2L; denoted LWSR; does not provide RA), and long write locks with no read locks, providing Read Uncommitted isolation <ref type="bibr" target="#b2">[2]</ref> (LWNR; also does not provide RA). While only LWLR provides RA, LWSR and LWNR provide a useful basis for comparison, particularly in measuring concurrency-related locking overheads. To avoid deadlocks, the system lexicographically orders lock requests by item and performs them sequentially. When locks are not used (as for reads in LWNR and reads and writes for NWNR), the system parallelizes operations.</p><p>We also consider an algorithm where, for each transaction, designated "coordinator" servers enforce RA isolation-effectively, the Eiger system's 2PC-PCI mechanism <ref type="bibr" target="#b33">[33]</ref> (denoted E-PCI; Section 6). Writes proceed via prepare and commit rounds, but any reads that arrive at a partition and overlap with a concurrent write to the same item must contact a (randomly chosen, per-write-transaction) "coordinator" partition to determine whether the coordinator's prepared writes have been committed. Writes require two RTTs, while reads require one RTT during quiescence and two RTTs in the presence of concurrent updates (to a variable number of coordinator partitions-linear in the number of concurrent writes to the item).</p><p>Using a coordinator violates partition independence but not synchronization independence. We optimize 2PC-PCI reads by having clients determine a read timestamp for each transaction (eliminating an RTT) and do not include happens-before metadata.</p><p>This range of lock-based strategies (LWNR, LWSR, LWNR), recent comparable approach (E-PCI), and best-case (NWNR; no concurrency control) baseline provides a spectrum of strategies for comparison.</p><p>Environment and benchmark. We evaluate each algorithm using the YCSB benchmark <ref type="bibr" target="#b16">[16]</ref> and deploy variably-sized sets of servers on public cloud infrastructure. We employ cr1.8xlarge instances on Amazon EC2 and, by default, deploy five partitions on five servers. We group sets of reads and sets of writes into read-only and write-only transactions (default size: 4 operations), and use the default YCSB workload (workloada, with Zipfian distributed item accesses) but with a 95% read and 5% write proportion, reflecting read-heavy applications (Section 2, <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b44">44]</ref>; e.g., Tao's 500 to 1 reads-to-writes <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b33">33]</ref>, Espresso's 1000 to 1 Mailbox application <ref type="bibr" target="#b38">[38]</ref>, and Spanner's 3396 to 1 advertising application <ref type="bibr" target="#b17">[17]</ref>).</p><p>By default, we use 5000 concurrent clients split across 5 separate EC2 instances and, to fully expose our metadata overheads, use a value size of 1 byte per write. We found that lock-based algorithms were highly inefficient for YCSB's default 1K item database, so we increased the database size to 1M items by default. Each version contains a timestamp (64 bits), and, with YCSB keys (i.e., item IDs) of size 11 bytes and a transaction length L, RAMP-F requires 11L bytes of metadata per version, while RAMP-H requires 32 bytes. We successively vary several parameters, including number of clients, read proportion, transaction length, value size, database size, and number of servers and report the average of three sixty-second trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results: Comparison</head><p>Our first set of experiments focuses on two metrics: performance compared to baseline and performance compared to existing techniques. The overhead of RAMP algorithms is typically less than 8% compared to baseline (NWNR) throughput, is sometimes zero, and is never greater than 50%. RAMP-F and RAMP-H always outperform the lock-based and E-PCI techniques, while RAMP-S outperforms lock-based techniques and often outperforms E-PCI. We proceed to demonstrate this behavior over a variety of conditions: Number of clients. RAMP performance scales well with increased load and incurs little overhead (Figure <ref type="figure" target="#fig_2">2</ref>). With few concurrent clients, there are few concurrent updates and therefore few secondround reads; performance for RAMP-F and RAMP-H is close to or even matches that of NWNR. At peak throughput (at 10,000 clients), RAMP-F and RAMP-H pay a throughput overhead of 4.2% compared to NWNR. RAMP-F and RAMP-H exhibit near-identical performance; the RAMP-H Bloom filter triggers few false positives (and therefore few extra RTTs compared to RAMP-F). RAMP-S incurs greater overhead and peaks at almost 60% of the throughput of NWNR. Its guaranteed two-round trip reads are expensive and it acts as an effective lower bound on RAMP-F and RAMP-H performance. In all configurations, the algorithms achieve low latency (RAMP-F, RAMP-H, NWNR less than 35ms on average and less than 10 ms at 5,000 clients; RAMP-S less than 53ms, 14.3 ms at 5,000 clients).</p><p>In comparison, the remaining algorithms perform less favorably. In contrast with the RAMP algorithms, E-PCI servers must check a coordinator server for each in-flight write transaction to determine whether to reveal writes to clients. For modest load, the overhead of these commit checks places E-PCI performance between that of RAMP-S and RAMP-H. However, the number of in-flight writes increases with load (and is worsened due to YCSB's Zipfian distributed accesses), increasing the number of E-PCI commit checks.  We omit latencies for LWLR, which peaked at over 1.5s. This in turn decreases throughput, and, with 10,000 concurrent clients, E-PCI performs so many commit checks per read (over 20% of reads trigger a commit check, and, on servers with hot items, each commit check requires indirected coordinator checks for an average of 9.84 transactions) that it underperforms the LWNR lockbased scheme. Meanwhile, multi-partition locking is expensive <ref type="bibr" target="#b35">[35]</ref>: with 10,000 clients, the most efficient algorithm, LWNR, attains only 28.6% of the throughput of NWNR, while the least efficient, LWLR, attains only 1.6% (peaking at 3,412 transactions per second).</p><p>We subsequently varied several other workload parameters, which we briefly discuss below and plot in Figure <ref type="figure" target="#fig_4">3</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read proportion.</head><p>Increased write activity leads to a greater number of races between reads and writes and therefore additional second-round RTTs for RAMP-F and RAMP-H reads. With all write transactions, all RAMP algorithms are equivalent (two RTT) and achieve approximately 65% of the throughput of NWNR. With all reads, RAMP-F, RAMP-S, NWNR, and E-PCI are identical, with a single RTT. Between these extremes, RAMP-F and RAMP-S scale nearlinearly with the write proportion. In contrast, lock-based protocols fare poorly as contention increases, while E-PCI again incurs penalties due to commit checks.</p><p>Transaction length. Increased transaction lengths have variable impact on the relative performance of RAMP algorithms. Synchronization independence does not penalize long-running transactions, but, with longer transactions, metadata overheads increase. RAMP-F relative throughput decreases due to additional metadata (linear in transaction length) and RAMP-H relative performance also decreases as its Bloom filters saturate. (However, YCSB's Zipfian-distributed access patterns result in a non-linear relationship between length and throughput.) As discussed above, we explicitly decided not to tune RAMP-H Bloom filter size but believe a logarithmic increase in filter size could improve RAMP-H performance for large transaction lengths (e.g., 1024 bit filters should lower the false positive rate for transactions of length 256 from over 92% to slightly over 2%).</p><p>Value size. Value size similarly does not seriously impact relative throughput. At a value size of 1B, RAMP-F is within 2.3% of NWNR. However, at a value size of 100KB, RAMP-F performance nearly matches that of NWNR: the overhead due to metadata decreases, and write request rates slow, decreasing concurrent writes (and subse-  quently second-round RTTs). Nonetheless, absolute throughput drops by a factor of 24 as value sizes moves from 1B to 100KB.</p><p>Database size. RAMP algorithms are robust to high contention for a small set of items: with only 1000 items in the database, RAMP-F achieves throughput within 3.1% of NWNR. RAMP algorithms are largely agnostic to read/write contention, although, with fewer items in the database, the probability of races between readers and inprogress writers increases, resulting in additional second-round reads for RAMP-F and RAMP-H. In contrast, lock-based algorithms fare poorly under high contention, while E-PCI indirected commit checks again incurred additional overhead. By relying on clients (rather than additional partitions) to repair fractured writes, RAMP-F, RAMP-H, and RAMP-S performance is less affected by hot items.</p><p>Overall, RAMP-F and RAMP-H exhibit performance close to that of no concurrency control due to their independence properties and guaranteed worst-case performance. As the proportion of writes increases, an increasing proportion of RAMP-F and RAMP-H operations take two RTTs and performance trends towards that of RAMP-S, which provides a constant two RTT overhead. In contrast, lockbased protocols perform poorly under contention while E-PCI triggers more commit checks than RAMP-F and RAMP-H trigger second round reads (but still performs well without contention and for particularly read-heavy workloads). The ability to allow clients to independently verify read sets enables good performance despite a range of (sometimes adverse) conditions (e.g., high contention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results: CTP Overhead</head><p>We also evaluated the overhead of blocked writes in our implementation of the Cooperative Termination Protocol discussed in Section 4.6. To simulate blocked writes, we artificially dropped a percentage of COMMIT commands in PUT_ALL calls such that clients returned from writes early and partitions were forced to complete the commit via CTP. This behavior is worse than expected because "blocked" clients continue to issue new operations. The table below reports the throughput reduction as the proportion of blocked writes increases (compared to no blocked writes) for a workload of 100% RAMP-F write transactions: Blocked % 0.01% 0.1% 25% 50% Throughput No change 99.86% 77.53% 67.92% As these results demonstrate, CTP can reduce throughput because each commit check consumes resources (here, network and CPU capacity). However, CTP only performs commit checks in the event of blocked writes (or time-outs; set to 5s in our experiments), so a modest failure rate of 1 in 1000 writes has a limited effect. The higher failure rates produce a near-linear throughput reduction but, in practice, a blocking rate of even a few percent is likely indicative of larger systemic failures. As Figure <ref type="figure" target="#fig_4">3</ref> hints, the effect of additional metadata for the participant list in RAMP-H and RAMP-S is limited, and, for our default workload of 5% writes, we observe similar trends but with throughput degradation of 10% or less across the above configurations. This validates our initial motivation behind the choice of CTP: average-case overheads are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Results: Scalability</head><p>We finally validate our chosen scalability criteria by demonstrating linear scalability of RAMP transactions to 100 servers. We deployed an increasing number of servers within the us-west-2 EC2 region and, to mitigate the effects of hot items during scaling, configured uniform random access to items. We were unable to include more than 20 instances in an EC2 "placement group," which guarantees 10 GbE connections between instances, so, past 20 servers, servers communicated over a degraded network. Around 40 servers, we exhausted the us-west-2b "availability zone" (datacenter) capacity and had to allocate our instances across the remaining zones, further degrading network performance. However, as shown in Figure <ref type="figure" target="#fig_5">4</ref>, each RAMP algorithm scales linearly, even though in expectation, at 100 servers, all but one in 100M transactions is a multi-partition operation. In particular, RAMP-F achieves slightly under 7.1 million operations per second, or 1.79 million transactions per second on a set of 100 servers (71, 635 operations per partition per second). At all scales, RAMP-F throughput was always within 10% of NWNR. With 100 servers, RAMP-F was within 2.6%, RAMP-S within 3.4%, and RAMP-S was within 45% of NWNR. In light of our scalability criteria, this behavior is unsurprising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Replicated databases offer a broad spectrum of isolation guarantees at varying costs to performance and availability <ref type="bibr">[8]</ref>:</p><p>Serializability. At the strong end of the isolation spectrum is serializability, which provides transactions with the equivalent of a serial execution (and therefore also provides RA). A range of techniques can enforce serializability in distributed databases <ref type="bibr" target="#b3">[3,</ref><ref type="bibr">8]</ref>, multi-version concurrency control (e.g. <ref type="bibr" target="#b37">[37]</ref>) locking (e.g. <ref type="bibr" target="#b31">[31]</ref>), and optimistic concurrency control <ref type="bibr" target="#b41">[41]</ref>. These useful semantics come with costs in the form of decreased concurrency (e.g., contention and/or failed optimistic operations) and limited availability during partial failure <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b21">21]</ref>. Many designs <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b29">29]</ref> exploit cheap serializability within a single partition but face scalability challenges for distributed operations. Recent industrial efforts like F1 <ref type="bibr" target="#b41">[41]</ref> and Spanner <ref type="bibr" target="#b17">[17]</ref> have improved performance via aggressive hardware advances but, their reported throughput is still limited to 20 and 250 writes per item per second. Multi-partition serializable transactions are expensive and, especially under adverse conditions, are likely to remain expensive <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35]</ref>.</p><p>Weak isolation. The remainder of the isolation spectrum is more varied. Most real-world databases offer (and often default to) nonserializable isolation models <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b34">34]</ref>. These "weak isolation" levels allow greater concurrency and fewer system-induced aborts compared to serializable execution but provide weaker semantic guarantees. For example, the popular choice of Snapshot Isolation prevents Lost Update anomalies but not Write Skew anomalies <ref type="bibr" target="#b2">[2]</ref>; by preventing Lost Update, concurrency control mechanisms providing Snapshot Isolation violate synchronization independence <ref type="bibr" target="#b5">[5]</ref>. In recent years, many "NoSQL" designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation in many industrial databases such PNUTS <ref type="bibr" target="#b15">[15]</ref>, Dynamo <ref type="bibr" target="#b22">[22]</ref>, TAO <ref type="bibr" target="#b11">[11]</ref>, Espresso <ref type="bibr" target="#b38">[38]</ref>, Rainbird <ref type="bibr" target="#b44">[44]</ref>, and BigTable <ref type="bibr" target="#b13">[13]</ref>. These systems avoid penalties associated with stronger isolation but in turn sacrifice transactional guarantees (and therefore do not offer RA).</p><p>Related mechanisms. There are several algorithms that are closely related to our choice of RA and RAMP algorithm design.</p><p>COPS-GT's two-round read-only transaction protocol <ref type="bibr" target="#b32">[32]</ref> is similar to RAMP-F reads-client read transactions identify causally inconsistent versions by timestamp and fetch them from servers. While COPS-GT provides causal consistency (requiring additional metadata), it does not support RA isolation for multi-item writes.</p><p>Eiger provides its write-only transactions <ref type="bibr" target="#b33">[33]</ref> by electing a coordinator server for each write. As discussed in Section 5 (E-PCI), the number of "commit checks" performed during its read-only transactions is proportional to the number of concurrent writes. Using a coordinator violates partition independence but in turn provides causal consistency. This coordinator election is analogous to G-Store's dynamic key grouping <ref type="bibr" target="#b19">[19]</ref> but with weaker isolation guarantees; each coordinator effectively contains a partitioned completed transaction list from <ref type="bibr" target="#b12">[12]</ref>. Instead of relying on indirection, RAMP transaction clients autonomously assemble reads and only require constant factor (or, for RAMP-F, linear in transaction size) metadata size compared to Eiger's PL-2L (worst-case linear in database size).</p><p>RAMP transactions are inspired by our earlier proposal for Monotonic Atomic View (MAV) isolation: transactions read from a monotonically advancing view of database state <ref type="bibr" target="#b5">[5]</ref>. MAV is strictly weaker than RA and does not prevent fractured reads, as required for our applications (i.e., reads are not guaranteed to be transactionally aligned). The prior MAV algorithm we briefly sketched in <ref type="bibr" target="#b5">[5]</ref> is similar to RAMP-F but, as a consequence of its weaker semantics, allows one-round read transactions. The RAMP algorithms described here are portable to the highly available (i.e., nonlinearizable, "AP/EL" <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b23">23]</ref>) replicated setting of <ref type="bibr" target="#b5">[5]</ref>, albeit with necessary penalties to latency between updates and their visibility.</p><p>Overall, we are not aware of a concurrency control mechanism for partitioned databases that provides synchronization independence, partition independence, and at least RA isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>This paper described how to achieve atomically visible multipartition transactions without incurring the performance and availability penalties of traditional algorithms. We first identified a new isolation level-Read Atomic isolation-that provides atomic visibility and matches the requirements of a large class of real-world applications. We subsequently achieved RA isolation via scalable, contention-agnostic RAMP transactions. In contrast with techniques that use inconsistent but fast updates, RAMP transactions provide correct semantics for applications requiring secondary indexing, foreign key constraints, and materialized view maintenance while maintaining scalability and performance. By leveraging multi-versioning with a variable but small (and, in two of three algorithms, constant) amount of metadata per write, RAMP transactions allow clients to detect and assemble atomic sets of versions in one to two rounds of communication with servers (depending on the RAMP implementation). The choice of synchronization and partition independent algorithms allowed us to achieve near-baseline performance across a variety of workload configurations and scale linearly to 100 servers. While RAMP transactions are not appropriate for all applications, the many for which they are well suited will benefit measurably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: Proofs and Isolation Details</head><p>RAMP-F Correctness. To prove RAMP-F provides RA isolation, we show that the two-round read protocol returns a transactionally atomic set of versions. To do so, we formalize criteria for atomic (read) sets of versions in the form of companion sets. We will call the set of versions produced by a transaction sibling versions and call two items from the same write set sibling items.</p><p>Given two versions x i and y j , we say that x i is a companion to y j if x i is a transactional sibling of y j or x is a sibling item of y j and i &gt; j. We say that a set of versions V is a companion set if, for every pair (x i , y j ) of versions in V where x is a sibling item of y j , x i is a companion to y j . In Figure <ref type="figure" target="#fig_0">1</ref>, the versions returned by T 2 's first round of reads ({x 1 , y ? }) do not comprise a companion set because y ? has a lower timestamp than x 1 's sibling version of y (that is, x 1 has sibling version y 1 and but ? &lt; 1 so y ? has too low of a timestamp). Subsets of companion sets are also companion sets and companion sets also have a useful property for RA isolation: Claim 1 (Companion sets are atomic). Companion sets do not contain fractured reads. Proof. Claim 1 follows from the definitions of companion sets and fractured reads. If V is a companion set, then every version x i 2 V is also a companion to every other version y j 2 V where v j contains x in its sibling items. If V contained fractured reads, V would contain two versions x i , y j such that the transaction that wrote y j also wrote a version x k , i &lt; k. However, in this case, x i would not be a companion to y j , a contradiction. Therefore, V cannot contain fractured reads.</p><p>To provide RA, RAMP-F clients assemble a companion set for the requested items (in v latest ), which we prove below: Claim 2. RAMP-F provides Read Atomic isolation. Proof. Each write in RAMP-F contains information regarding its siblings, which can be identified by item and timestamp. Given a set of RAMP-F versions, recording the highest timestamped version of each item (as recorded either in the version itself or via sibling metadata) yields a companion set of item-timestamp pairs: if a client reads two versions x i and y j such that x is in y j 's sibling items but i &lt; j, then v latest [x] will contain j and not i. Accordingly, given the versions returned by the first round of RAMP-F reads, clients calculate a companion set containing versions of the requested items. Given this companion set, clients check the first-round versions against this set by timestamp and issue a second round of reads to fetch any companions that were not returned in the first round. The resulting set of versions will be a subset of the computed companion set and will therefore also be a companion set. This ensures that the returned results do not contain fractured reads. RAMP-F first-round reads access lastCommit, so each transaction corresponding to a first-round version is committed, and, therefore, any siblings requested in the (optional) second round of reads are also committed. Accordingly, RAMP-F never reads aborted or non-final (intermediate) writes. This establishes that RAMP-F provides RA.</p><p>RAMP-F Scalability and Independence. RAMP-F also provides the independence guarantees from Section 3.3. The following invariant over lastCommit is core to RAMP-F GET request completion: Invariant 1 (Companions present). If a version x i is referenced by lastCommit (that is, lastCommit[x] = i), then each of x i 's sibling versions are present in versions on their respective partitions. Invariant 1 is maintained by RAMP-F's two-phase write protocol. lastCommit is only updated once a transaction's writes have been placed into versions by a first round of PREPARE messages. Siblings will be present in versions (but not necessarily lastCommit). Claim 3. RAMP-F provides synchronization independence. Proof. Clients in RAMP-F do not communicate or coordinate with one another and only contact servers. Accordingly, to show that RAMP-F provides synchronization independence, it suffices to show that server-side operations always terminate. PREPARE and COMMIT methods only access data stored on the local partition and do not block due to external coordination or other method invocations; therefore, they complete. GET requests issued in the first round of reads have ts req = ? and therefore will return the version corresponding to lastCommit[k], which was placed into versions in a previously completed PREPARE round. GET requests issued in the second round of client reads have ts req set to the client's calculated v latest <ref type="bibr">[k]</ref>. v latest [k] is a sibling of a version returned from lastCommit in the first round, so, due to Invariant 1, the requested version will be present in versions. Therefore, GET invocations are guaranteed access to their requested version and can return without waiting. The success of RAMP-F operations do not depend on the success or failure of other clients' RAMP-F operations. Claim 4. RAMP-F provides partition independence. Proof. RAMP-F transactions do not access partitions that are unrelated to each transaction's specified data items and servers do not contact other servers in order to provide a safe response for operations.</p><p>RAMP-S Correctness. RAMP-S writes and first-round reads proceed identically to RAMP-F writes, but the metadata written and returned is different. Therefore, the proof is similar to RAMP-F, with a slight modification for the second round of reads. Claim 5. RAMP-S provides Read Atomic isolation. Proof. To show that RAMP-S provides RA, it suffices to show that RAMP-S second-round reads (resp) are a companion set. Given two versions x i , y j 2 resp such that x 6 = y, if x is a sibling item of y j , then x i must be a companion to y j . If x i were not a companion to y j , then it would imply that x is not a sibling item of y j (so we are done) or that j &gt; i. If j &gt; i, then, due to Invariant 1 (which also holds for RAMP-S writes due to identical write protocols), y j 's sibling is present in versions on the partition for x and would have been returned by the server (line 6), a contradiction. Each second-round GET request returns only one version, so we are done.</p><p>RAMP-S Scalability and Independence. RAMP-S provides synchronization independence and partition independence. For brevity, we again omit full proofs, which closely resemble those of RAMP-F.</p><p>RAMP-H Correctness. The probabilistic behavior of the RAMP-H Bloom filter admits false positives. However, given unique transaction timestamps (Section 4.5), requesting false siblings by timestamp and item does not affect correctness: Claim 6. RAMP-H provides Read Atomic isolation. Proof. To show that RAMP-H provides Read Atomic isolation, it suffices to show that any versions requested by RAMP-H second-round reads that would not have been requested by RAMP-F second-round reads (call this set v f alse ) do not compromise the validity of RAMP-H's returned companion set. Any versions in v f alse do not exist: timestamps are unique, so, for each version x i , there are no versions x j of non-sibling items with the same timestamp as x i (i.e., where i = j). Therefore, requesting versions in v f alse do not change the set of results collected in the second round.</p><p>RAMP-H Scalability and Independence. RAMP-H provides synchronization independence and partition independence. We omit full proofs, which closely resemble those of RAMP-F. The only significant difference from RAMP-F is that second-round GET requests may return ?, but, as we showed above, these empty responses correspond to false positives in the Bloom filter and therefore do not affect correctness.</p><p>Comparison to other isolation levels. The fractured reads anomaly is similar to Adya's "Missing Transaction Updates" definition, only applied to immediate read dependencies (rather than all transitive dependencies). RA is stronger than PL-2 (Read Committed), but weaker than PL-SI, PL-CS, and PL-2.99 (notably, RA does not prevent anti-dependency cycles, or Adya's G2 or G-SIa-informally, it allows concurrent updates) <ref type="bibr" target="#b2">[2]</ref>.</p><p>RA does not (by itself) provide ordering guarantees across transactions. Our RAMP implementations provide a variant of PRAM consistency, where, for each item, each user's writes are serialized <ref type="bibr" target="#b30">[30]</ref> (i.e., "session" ordering <ref type="bibr" target="#b20">[20]</ref>), and, once a user's operation completes, all other users will observe its effects (regular register semantics, applied at the transaction level). This provides transitivity with respect to each user's operations. For example, if a user updates her privacy settings and subsequently posts a new photo, the photo cannot be read without the privacy setting change <ref type="bibr" target="#b15">[15]</ref>. However, PRAM does not respect the happens-before relation <ref type="bibr" target="#b4">[4]</ref> across users. If Sam reads Mary's comment and replies to it, other users may read Sam's comment without Mary's comment. In this case, RAMP transactions can leverage explicit causality <ref type="bibr" target="#b6">[6]</ref> via foreign key dependencies, but happensbefore is not provided by default. If required, we believe it is possible to enforce happens-before but, due to scalability concerns regarding metadata and partition independence (e.g., <ref type="bibr" target="#b6">[6]</ref> and Section 5), do not further explore this possibility. An "active-active" replicated implementation can provide available <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b23">23]</ref> operation at the cost of these recency guarantees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Space-time diagram for RAMP-F execution for two transactions T 1 and T 2 performed by clients C 1 and C 2 on partitions P x and P y . Because T 1 overlaps with T 2 , T 2 must perform a second round of reads to repair the fractured read between x and y. T 1 's writes are assigned timestamp 1. Lightlyshaded boxes represent current partition state (lastCommit and versions), while the single darkly-shaded box encapsulates all messages exchanged during C 2 's execution of transaction T 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Throughput and latency under varying client load.We omit latencies for LWLR, which peaked at over 1.5s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Algorithm performance across varying workload conditions. RAMP-F and RAMP-H exhibit similar performance to NWNR baseline, while RAMP-S's 2 RTT reads incur a greater performance penalty across almost all configurations. RAMP transactions consistently outperform RA isolated alternatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: RAMP transactions scale linearly to over 7 million operations/s with comparable performance to NWNR baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 RAMP-FastServer-side Data Structures 1: versions: set of versions hitem, value, timestamp ts v , metadata mdi 2: latestCommit[i]: last committed timestamp for item i</figDesc><table><row><cell cols="2">Server-side Methods</cell></row><row><cell cols="2">3: procedure PREPARE(v : version) 4: versions.add(v)</cell></row><row><cell>5:</cell><cell>return</cell></row><row><cell cols="2">6: procedure COMMIT(ts c : timestamp) 7: I ts {w.item | w 2 versions ^w.ts v = ts c } 8: 8i 2 I ts , latestCommit[i] max(latestCommit[i],ts c ) 9: procedure GET(i : item, ts req : timestamp) 10: if ts req = / 0 then 11: return v 2 versions : v.item = i ^v.ts v = latestCommit[item] 12: else 13: return v 2 versions : v.item = i ^v.ts v = ts req</cell></row><row><cell cols="2">Client-side Methods</cell></row><row><cell cols="2">14: procedure PUT_ALL(W : set of hitem, valuei) 15: ts tx generate new timestamp 16: I tx set of items in W 17: parallel-for hi, vi 2 W 18:</cell></row><row><cell>21:</cell><cell>invoke COMMIT(ts tx ) on s</cell></row><row><cell cols="2">22: procedure GET_ALL(I : set of items) 23: ret {} 24: parallel-for i 2 I 25: ret[i] GET(i, / 0)</cell></row><row><cell>26:</cell><cell>v</cell></row></table><note><p><p><p><p><p><p><p>v hitem = i, value = v,ts v = ts tx , md = (I tx {i})i 19:</p>invoke PREPARE(v) on respective server (i.e., partition) 20:</p>parallel-for server s : s contains an item in W latest {} (default value: 1) 27:</p>for response r 2 ret do 28:</p>for i tx 2 r.md do 29:</p>v latest [i tx ] max(v latest [i tx ],</p>r.ts v ) 30: parallel-for item i 2 I 31: if v latest [i] &gt; ret[i].ts v then 32: ret[i] GET(i, v latest [i]) 33: return ret</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 RAMP-SmallServer-side Data Structures same as in RAMP-F (Algorithm 1)Server-side Methods PREPARE, COMMIT same as in RAMP-F 1: procedure GET(i : item, ts set : set of timestamps) 2:if ts set = / 0 then 3:return v 2 versions : v.item = i ^v.ts v = latestCommit[k] 4: else 5: ts match = {t | t 2 ts set ^9v 2 versions : v.item = i ^v.t v = t} 6: return v 2 versions : v.item = i ^v.ts v = max(ts match )Client-side Methods 7: procedure PUT_ALL(W : set of hitem, valuei) same as RAMP-F PUT_ALL but do not instantiate md on line 18</figDesc><table><row><cell cols="2">8: procedure GET_ALL(I : set of items) 9: ts set {} 10: parallel-for i 2 I 11: ts set .add(GET(i, / 0).ts v )</cell></row><row><cell>12: 13: 14:</cell><cell>ret {} parallel-for item i 2 I ret[i] GET(i, ts set )</cell></row><row><cell>15:</cell><cell>return ret</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 3 RAMP-Hybrid Server-side Data Structures Same as in RAMP-F (Algorithm 1) Server-side Methods PREPARE, COMMIT same as in RAMP-F GET same as in RAMP-S Client-side Methods 1: procedure PUT_ALL(W : set of hitem, valuei) same as RAMP-F PUT_ALL but instantiate md on line 18 with Bloom filter containing I tx</figDesc><table><row><cell>2: procedure GET_ALL(I : set of items) 3: ret {} 4:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>13:</cell><cell cols="2">return ret</cell><cell></cell><cell></cell><cell>= ?</cell></row><row><cell cols="2">Algorithm</cell><cell cols="3">RTTs/transaction W R (stable) R (O)</cell><cell cols="2">Metadata (+stamp) Stored Per-Request</cell></row><row><cell></cell><cell>RAMP-F</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>txn items</cell><cell>-</cell></row><row><cell></cell><cell>RAMP-S RAMP-H</cell><cell>2 2</cell><cell>2 1 + e</cell><cell>2 2</cell><cell cols="2">-Bloom filter stamp/item stamp/item</cell></row></table><note><p>Comparison of basic algorithms: RTTs required for writes (W), reads (R) without concurrent writes and in the worst case (O), stored metadata and metadata attached to read requests (in addition to a timestamp for each).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors would like to thank Peter Alvaro, Giselle Cheung, Neil Conway, Aaron Davidson, Mike Franklin, Aurojit Panda, Nuno Preguiça, Edward Ribeiro, Shivaram Venkataraman, and the SIGMOD reviewers for their insightful feedback. This research is supported by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, the National Science Foundation Graduate Research Fellowship (grant DGE-1106400), and gifts from Amazon Web Services, Google, SAP, Apple, Inc., Cisco, Clearstory Data, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, General Electric, Hortonworks, Huawei, Intel, Microsoft, NetApp, NTT Multimedia Communications Laboratories, Oracle, Samsung, Splunk, VMware, WANdisco and Yahoo!.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Consistency tradeoffs in modern distributed database system design: CAP is only part of the story</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weak consistency: a generalized theory and optimistic implementations for distributed transactions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using multiversion data for non-interfering execution of write-only transactions</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 1991</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Attiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welch</surname></persName>
		</author>
		<title level="m">Distributed Computing: Fundamentals, Simulations and Advanced Topics</title>
		<imprint>
			<publisher>John Wiley Interscience</publisher>
			<date type="published" when="2004-03">March 2004</date>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<title level="m">Highly Available Transactions: Virtues and Limitations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The potential dangers of causal consistency and an explicit solution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Megastore: Providing scalable, highly available storage for interactive services</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Furman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR 2011</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Concurrency control and recovery in database systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hadzilacos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Addison-wesley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward a cloud computing research agenda</title>
		<author>
			<persName><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space/time trade-offs in hash coding with allowable errors</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TAO: Facebook&apos;s distributed data store for the social graph</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Amsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implementing distributed read-only transactions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="212" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Materialized views</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chirkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Databases</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="405" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PNUTS: Yahoo!&apos;s hosted data serving platform</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bohannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<idno>ACM SOCC 2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spanner: Google&apos;s globally-distributed database</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Furman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Schism: a workload-driven approach to database replication and partitioning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">G-store: a scalable data store for transactional multi key access in the cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Abbadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOCC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lazy database replication with ordering guarantees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Daudjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE 2004</title>
		<imprint>
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Consistency in partitioned networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="370" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brewer&apos;s conjecture and the feasibility of consistent, available, partition-tolerant web services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consensus on transaction commit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TODS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="160" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Life beyond distributed transactions: an apostate&apos;s opinion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Helland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">20 obstacles to scalability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="54" to="59" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maintaining global integrity constraints in distributed databases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Huyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constraints</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low overhead concurrency control for partitioned main memory databases</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">H-Store: a high-performance, distributed main memory transaction processing system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kallman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Natkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">PRAM: a scalable shared memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sandberg</surname></persName>
		</author>
		<idno>TR-180-88</idno>
		<imprint>
			<date type="published" when="1988-09">September 1988</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using versions in update transactions: Application to integrity checking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Llirbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tombroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Don&apos;t settle for eventual: scalable causal consistency for wide-area storage with COPS</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP 2011</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stronger semantics for low-latency geo-replicated storage</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">History repeats itself: Sensible and NonsenSQL aspects of the NoSQL hoopla</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Skew-aware automatic database partitioning in shared-nothing, parallel OLTP systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale incremental processing using distributed transactions and notifications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiversion reconciliation for mobile databases</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Phatak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Badrinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On brewing fresh Espresso: LinkedIn&apos;s distributed data serving platform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Surlaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Quiggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">P-store: Genuine partial replication in wide area networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schiper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedone</surname></persName>
		</author>
		<idno>IEEE SRDS 2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comprehensive study of convergent and commutative replicated data types</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
		<idno>7506</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">F1: A distributed SQL database that scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Session guarantees for weakly consistent replicated data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PDIS</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Calvin: Fast distributed transactions for partitioned database systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Weil</surname></persName>
		</author>
		<ptr target="http://slidesha.re/hjMOui" />
		<title level="m">Rainbird: Real-time analytics at Twitter. Strata</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object-oriented type evolution</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DBPL</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lazy maintenance of materialized views</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
