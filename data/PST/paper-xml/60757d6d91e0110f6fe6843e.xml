<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-12">12 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Delian</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vision Intelligence Center</orgName>
								<address>
									<settlement>Meituan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vision Intelligence Center</orgName>
								<address>
									<settlement>Meituan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Chai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Vision Intelligence Center</orgName>
								<address>
									<settlement>Meituan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanzi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Fe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Su</surname></persName>
						</author>
						<title level="a" type="main">Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-12">12 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.05160v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel Feature Decomposition and Reconstruction Learning (FDRL) method for effective facial expression recognition. We view the expression information as the combination of the shared information (expression similarities) across different expressions and the unique information (expression-specific variations) for each expression. More specifically, FDRL mainly consists of two crucial networks: a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN). In particular, FDN first decomposes the basic features extracted from a backbone network into a set of facial action-aware latent features to model expression similarities. Then, FRN captures the intra-feature and inter-feature relationships for latent features to characterize expression-specific variations, and reconstructs the expression feature. To this end, two modules including an intra-feature relation modeling module and an inter-feature relation modeling module are developed in FRN. Experimental results on both the in-thelab databases (including CK+, MMI, and Oulu-CASIA) and the in-the-wild databases (including RAF-DB and SFEW) show that the proposed FDRL method consistently achieves higher recognition accuracy than several state-of-the-art methods. This clearly highlights the benefit of feature decomposition and reconstruction for classifying expressions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expression is one of the most natural and universal signals for human beings to express their inner states and intentions <ref type="bibr" target="#b3">[4]</ref>. Over the past few decades, Facial Expression Recognition (FER) has received much attention in computer vision, due to its various applications including virtual reality, intelligent tutoring systems, health-care, etc. <ref type="bibr" target="#b28">[29]</ref>. According to psychological studies <ref type="bibr" target="#b8">[9]</ref>, the FER task is to classify an input facial image into one of the following seven categories: angry (AN), disgust (DI), fear (FE), happy (HA), sad (SA), surprise (SU), and neutral (NE).</p><p>A variety of FER methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> have been proposed to learn holistic expression features by disentangling the disturbance caused by various disturbing factors, such as pose, identity, illumination, and so on. However, these methods neglect the fact that the extracted expression features corresponding to some expressions may still not be easily distinguishable, mainly because of high similarities across different expressions.</p><p>An example is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We can observe that some facial images corresponding to the NE, SA, HA, and DI expressions exhibit closing eyes. The facial images corresponding to the SU, FE, AN, and HA expressions all show opening mouths, while those corresponding to the AN, DI, SA, and FE expressions show frowning brows. The images from different facial expressions in each group give a similar facial action, where the distinctions between some expressions are subtle. Therefore, how to learn effective finegrained expression features to identify subtle differences in expressions by considering similar facial actions is of great importance.</p><p>The expression information is composed of the shared information (expression similarities) across different expressions and the unique information (expression-specific variations) for each expression. The expression similari-ties can be characterized by shared latent features between different expressions, while the expression-specific variations can be reflected by importance weights for latent features. Therefore, the expression features can be represented by combining a set of latent features associated with their corresponding importance weights. Traditional FER methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref> adopt Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) to extract eigenvectors (corresponding to latent features) and eigenvalues (corresponding to importance weights). However, these eigenvectors only capture holistic structural information rather than fine-grained semantic information of facial images, which is critical for FER.</p><p>Motivated by the success of deep learning in various vision tasks, here we propose a novel Feature Decomposition and Reconstruction Learning (FDRL) method for effective FER. FDRL is mainly comprised of two crucial networks, including a Feature Decomposition Network (FDN) and a Feature Reconstruction Network (FRN). The two networks are tightly combined and jointly trained in an end-to-end manner.</p><p>Specifically, a backbone convolutional neural network is first used to extract basic features. Then, FDN decomposes the basic feature into a set of facial action-aware latent features, which effectively encode expression similarities across different expressions. In particular, a compactness loss is developed to obtain compact latent feature representations. Next, FRN, which includes an Intra-feature Relation Modeling module (Intra-RM) and an Inter-feature Relation Modeling module (Inter-RM), models expressionspecific variations and reconstructs the expression feature. Finally, an expression prediction network is employed for expression classification.</p><p>In summary, our main contributions are summarized as follows.</p><p>• A novel FDRL method is proposed to perform FER.</p><p>In FDRL, FDN and FRN are respectively developed to explicitly model expression similarities and expression-specific variations, enabling the extraction of fine-grained expression features. Thus, the subtle differences between facial expressions can be accurately identified. • Intra-RM and Inter-RM are elaborately designed to learn an intra-feature relation weight and an interfeature relation weight for each latent feature, respectively. Therefore, the intra-feature and inter-feature relationships between latent features are effectively captured to obtain discriminative expression features. • Our FDRL method is extensively evaluated on both the in-the-lab and the in-the-wild FER databases. Experimental results show that our method consistently outperforms several state-of-the-art FER methods. In particular, FDRL achieves 89.47% and 62.16% recognition accuracy on the RAF-DB and SFEW databases, respectively. This convincingly shows the great potentials of feature decomposition and reconstruction for FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>With the rapid development of deep learning, extensive efforts have been made to perform FER. State-of-the-art deep learning-based FER methods mainly focus on two aspects: 1) disturbance disentangling, and 2) expression feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Disturbance Disentangling</head><p>Many FER methods have been proposed to predict expressions by disentangling the disturbance caused by various disturbing factors, such as pose, identity, illumination, and so on. Wang et al. <ref type="bibr" target="#b21">[22]</ref> propose an adversarial feature learning method to tackle the disturbance caused by facial identity and pose variations. Ruan et al. <ref type="bibr" target="#b19">[20]</ref> propose a novel Disturbance-Disentangled Learning (DDL) method to simultaneously disentangle multiple disturbing factors. Note that the above methods depend largely on the label information of disturbing factors. A few methods address the occlusion problem of FER. Wang and Peng <ref type="bibr" target="#b23">[24]</ref> propose a novel Region Attention Network (RAN) to adaptively adjust the importance of facial regions to mitigate the problems of occlusion and variant poses for FER.</p><p>Recently, some methods are concerned with the noisy label problem in the FER databases. Zeng et al. <ref type="bibr" target="#b27">[28]</ref> propose an Inconsistent Pseudo Annotations to Latent Truth (IPA2LT) method to deal with the problem of inconsistency in different FER databases. Wang et al. <ref type="bibr" target="#b22">[23]</ref> introduce a Self-Cure Network (SCN) to prevent the trained model from over-fitting uncertain facial images.</p><p>The above methods perform FER by alleviating the influence caused by disturbing factors or noisy labels. However, they do not take into account subtle differences between different facial expressions. In this paper, we formulate the FER problem from the perspective of feature decomposition and reconstruction, which successfully models expression similarities and expression-specific variations. Therefore, high-level semantic information can be effectively encoded to classify facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Expression Feature Extraction</head><p>Some FER methods design effective network architectures and loss functions to reduce inter-class similarities and enhance intra-class compactness for expression feature extraction. Li et al. <ref type="bibr" target="#b12">[13]</ref> propose a deep locality-preserving loss based method, which extracts discriminative expression features by preserving the locality closeness. Cai et  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>al. [3] design a novel island loss to simultaneously increase inter-class separability and intra-class compactness.</head><p>A few FER methods employ attention mechanisms <ref type="bibr" target="#b25">[26]</ref> to improve the discriminative ability of expression features. Xie et al. <ref type="bibr" target="#b25">[26]</ref> design an attention layer to focus on salient regions of a facial expression. Wang et al. <ref type="bibr" target="#b23">[24]</ref> determine the importance of different facial regions by leveraging an attention network.</p><p>The above methods enhance the discriminative capability of expression features by designing different loss functions or attention mechanisms. These methods consider the expression features as holistic features. In contrast, we decompose the basic features into a set of facial action-aware latent features and then model the intra-feature and interfeature relationships for latent features. Compared with holistic features used in traditional methods, the latent feature representations developed in our method are more finegrained and facial action-aware. Such a manner is beneficial to learn expression features for identifying subtle differences between facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Overview The proposed FDRL method consists of a backbone network, a Feature Decomposition Network (FDN), a Feature Reconstruction Network (FRN), and an Expression Prediction Network (EPN). An overview of the proposed method is shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Given a batch of facial images, we first feed them into a backbone network (in this paper, we use ResNet-18 <ref type="bibr" target="#b10">[11]</ref> as the backbone) to extract basic CNN features. Then, FDN decomposes the basic features into a set of facial action-aware latent features, where a compactness loss is designed to extract compact feature representations. Next, FRN learns an intra-feature relation weight and an interfeature relation weight for each latent feature, and reconstructs the expression feature. Finally, EPN (a simple linear fully-connected layer) predicts a facial expression label.</p><p>In particular, FRN consists of two modules: an Intrafeature Relation Modeling module (Intra-RM) and an Interfeature Relation Modeling module (Inter-RM). To be specific, Intra-RM is first introduced to assign an intra-feature relation weight to each latent feature according to the importance of the feature, and thus an intra-aware feature is obtained. To ensure similar distributions of intra-feature relation weights for facial images from the same expression category, a distribution loss and a balance loss are employed in Intra-RM. Then, Inter-RM computes an interfeature relation weight by investigating the relationship between intra-aware features, and thus an inter-aware feature is extracted. At last, the expression feature is represented by a combination of the intra-aware feature and the interaware feature. FRN exploits both the contribution of each latent feature and the correlations between intra-aware features, enabling the extraction of discriminative expression features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Decomposition Network (FDN)</head><p>Given the i-th facial image, the basic feature extracted by the backbone network is denoted as x i ∈ R P , where P is the dimension of the basic feature. As men-tioned previously, FDN decomposes the basic feature into a set of facial action-aware latent features. Let</p><formula xml:id="formula_0">L i = [l i,1 , l i,2 , • • • , l i,M ] ∈ R D×M</formula><p>denote a facial action-aware latent feature matrix, where l i,j ∈ R D represents the j-th latent feature for the i-th facial image. D and M represent the dimension of each latent feature and the number of latent features, respectively. Specifically, to extract the j-th latent feature, we employ a linear Fully-Connected (FC) layer and a ReLU activation function, which can be formulated as:</p><formula xml:id="formula_1">l i,j = σ 1 (W T dj x i ) for j = 1, 2, • • • , M,<label>(1)</label></formula><p>where W dj denotes the parameters of the FC layer used for extracting the j-th latent feature and σ 1 represents the ReLU function.</p><p>Compactness Loss. Since different facial expressions share the same set of latent features, it is expected that a set of compact latent feature representations are extracted. In other words, the j-th latent feature extracted from one basic feature should be similar to that extracted from another basic feature. To achieve this, inspired by the center loss <ref type="bibr" target="#b24">[25]</ref>, we develop a compactness loss. The compactness loss L C learns a center for the same latent features and penalizes the distances between the latent features and their corresponding centers, which can be formulated as:</p><formula xml:id="formula_2">L C = 1 N N i=1 M j=1 l i,j − c j 2 2 ,<label>(2)</label></formula><p>where N denotes the number of images in a mini-batch.</p><p>• 2 indicates the L 2 norm. c j ∈ R D denotes the center of the j-th latent features, and is updated based on a minibatch. Thus, the intra-latent variations are minimized and a set of compact latent features are effectively learned.</p><p>To visually demonstrate the interpretation of latent features, we collect a group of images that corresponds to the highest intra-feature relation weight (see Section 3.2) of the same latent feature and then visualize them. In Figure <ref type="figure" target="#fig_3">3</ref>, we can observe that the images from each group show a specific facial motion. The images from the nine groups show the facial motions of "Neutral", "Lip Corner Puller", "Staring", "Opening Mouths", "Lips Part", "Closing Eyes", "Grinning", "Frowning Brows", and "Lip Corner Depressor", respectively. Therefore, the latent features obtained by FDN are fine-grained and facial action-aware features, which can be useful for subsequent expression feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Reconstruction Network (FRN)</head><p>In this section, FRN, which models expression-specific variations, is carefully designed to obtain discriminative expression features. FRN contains two modules: Intra-RM and Inter-RM. Intra-feature Relation Modeling Module (Intra-RM).</p><p>Intra-RM consists of multiple intra-feature relation modeling blocks, where each block is designed to model the intrafeature relationship between feature elements. To be specific, each block is composed of an FC layer and a sigmoid activation function, that is:</p><formula xml:id="formula_3">α i,j = σ 2 (W T sj l i,j ) for j = 1, 2, • • • , M,<label>(3)</label></formula><p>where α i,j ∈ R D denotes the importance weights for the jth latent feature corresponding to the i-th facial image, W sj represents the parameters of the FC layer, and σ 2 indicates the sigmoid function. With Eq. ( <ref type="formula" target="#formula_3">3</ref>), we compute the L 1 norm of α i,j as the Intra-feature relation Weight (Intra-W) to determine the importance of the j-th latent feature, that is:</p><formula xml:id="formula_4">α i,j = α i,j 1 for j = 1, 2, • • • , M,<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">• 1 denotes the L 1 norm.</formula><p>It is desirable that the distributions of Intra-Ws corresponding to different images from the same expression category are as close as possible. Therefore, similarly to the compactness loss, a distribution loss is used to learn a center for each expression category and penalize the distances between the Intra-Ws from one class and the corresponding center. Hence, the variations caused by different disturbing factors are alleviated.</p><p>Suppose that the i-th facial image belongs to the k i -th expression category. Mathematically, the distribution loss L D is formulated as:</p><formula xml:id="formula_6">L D = 1 N N i=1 w i − w ki 2 2 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">w i = [α i,1 , α i,2 , • • • , α i,M ] T ∈ R M</formula><p>represents the Intra-W vector for the i-th facial image. w ki ∈ R M denotes the class center corresponding to the k i -th expression category.</p><p>By optimizing the distribution loss, the Intra-W vectors corresponding to different images from the same expression category are closely distributed. Thus, they are able to focus on expression-specific variations.</p><p>In practice, as shown in Figure <ref type="figure" target="#fig_2">2</ref>, some Intra-Ws (corresponding to one or two latent features) usually show much higher values than the other Intra-Ws in the Intra-W vector for each image, since these Intra-Ws are individually computed. To alleviate this problem, we further design a balance loss to balance the distributions of elements in each Intra-W vector as:</p><formula xml:id="formula_8">L B = w − w u 1 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">w = [α 1 , α 2 , • • • , α M ] T ∈ R M</formula><p>represents the mean Intra-W vector for a batch of samples (i.e., w =</p><formula xml:id="formula_10">1 N N i=1 w i ). w u = [ 1 M , 1 M , • • • , 1 M ] T ∈ R M</formula><p>denotes a uniformly-distributed weight vector.</p><p>After computing an Intra-W for each latent feature, we assign this weight to the corresponding feature and obtain an intra-aware feature for the i-th facial image as:</p><formula xml:id="formula_11">f i,j = α i,j l i,j for j = 1, 2, • • • , M,<label>(7)</label></formula><p>where f i,j ∈ R D represents the j-th intra-aware feature for the i-th facial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-feature Relation Modeling Module (Inter-RM).</head><p>Intra-RM learns an Intra-W for each individual latent feature. However, these Intra-Ws are independently extracted.</p><p>Although the distribution loss imposes consistency regularization on the Intra-W, it does not fully consider the interrelationship between latent features. In fact, for each facial expression, different kinds of facial motions usually simultaneously appear. For example, the FE expression often involves the facial motions of frowning brows and opening mouths. The HA expression contains the facial motions of stretching brows, closing eyes, and opening mouths. Therefore, it is critical to exploit the correlations between different facial motion-aware latent features. To achieve this, we further introduce Inter-RM to learn an Inter-feature Relation Weight (Inter-W) between intra-aware features based on Graph Neural Network (GNN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Inter-RM learns a set of relation messages and estimates the Inter-Ws between these messages. Specifically, for each f i,j , it is first fed into a message network for feature encoding. In this paper, the message network is composed of an FC layer and a ReLU activation function, which is:</p><formula xml:id="formula_12">g i,j = σ 1 (W T ej f i,j ) for j = 1, 2, • • • , M,<label>(8)</label></formula><p>where W ej denotes the parameters of the FC layer used for feature encoding and σ 1 represents the ReLU function. g i,j ∈ R D denotes the j-th relation message for the i-th facial image. Then, a relation message matrix</p><formula xml:id="formula_13">G i = [g i,1 , g i,2 , • • • , g i,M</formula><p>] ∈ R D×M is represented as nodes in the graph G(G i , E). In our formulation, G is an undirected complete graph and E represents the set of relationships between different relation messages. ω i (j, m) is the Inter-W which denotes the relation importance between the node g i,j and the node g i,m . It can be calculated as:</p><formula xml:id="formula_14">ω i (j, m) = σ 3 (S(g i,j , g i,m )) j = m 0 j = m ,<label>(9)</label></formula><p>where g i,j and g i,m are the j-th and the m-th relation messages for the i-th facial image, respectively. S is a distance function, which estimates the similarity score between g i,j and g i,m . In our paper, we use the Euclidean distance function. Since the results of S(•) are all positive, we further adopt the tanh activation function σ 3 to normalize the positive distance value to [0,1). The purpose of setting ω i (j, j) to 0 is to avoid self-enhancing. According to Eq. ( <ref type="formula" target="#formula_14">9</ref>), an Inter-W matrix W i = {ω i (j, m)} ∈ R M ×M can be obtained.</p><p>Hence, the j-th inter-aware feature fi,j ∈ R D for the i-th facial image can be formulated as:</p><formula xml:id="formula_15">fi,j = M m=1 ω i (j, m)g i,m for j = 1, 2, • • • , M. (10)</formula><p>By combining the j-th intra-aware feature and the jth inter-aware feature, the j-th importance-aware feature y i,j ∈ R D for the i-th facial image is calculated as: <ref type="bibr" target="#b10">(11)</ref> where δ represents the regularization parameter that balances the intra-aware feature and the inter-aware feature.</p><formula xml:id="formula_16">y i,j = δf i,j + (1 − δ) fi,j for j = 1, 2, • • • , M,</formula><p>Finally, a set of importance-aware features are added to obtain the final expression feature, that is,</p><formula xml:id="formula_17">y i = M j=1 y i,j ,<label>(12)</label></formula><p>where y i ∈ R D represents the expression feature for the i-th facial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Loss Function</head><p>In the proposed FDRL, the backbone network, FDN, FRN, and EPN are jointly trained in an end-to-end manner. The whole network minimizes the following joint loss function:</p><formula xml:id="formula_18">L = L cls + λ 1 L C + λ 2 L B + λ 3 L D ,<label>(13)</label></formula><p>where L cls , L C , L B , and L D represent the classification loss, the compactness loss, the balance loss, and the distribution loss, respectively. In this paper, we use the crossentropy loss as the classification loss. λ 1 , λ 2 , and λ 3 denote the regularization parameters. By optimizing the joint loss, FDRL is able to extract discriminative fine-grained expression features for FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first briefly introduce five public FER databases. Then, we describe the implementation details, and perform ablation studies with qualitative and quantitative results to show the importance of each component in FDRL. Finally, we compare FDRL with state-of-the-art FER methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Databases</head><p>CK+ <ref type="bibr" target="#b13">[14]</ref> contains 327 video sequences, which are captured in controlled lab environments. We choose the three peak expression frames from each expression sequence to construct the training set and the test set, thus resulting in a total of 981 images. MMI <ref type="bibr" target="#b18">[19]</ref> is also a lab-controlled database, containing 205 video sequences with six basic expressions. We choose the three peak frames from each sequence to construct the training set and the test set, thus resulting in a total of 615 images. Oulu-CASIA <ref type="bibr" target="#b29">[30]</ref> contains videos captured in controlled lab conditions. We select the last three frames in each sequence captured with the visible light and strong illumination to construct the training set and the test set (consisting of 1,440 images in total). Similarly to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, we employ the subject-independent ten-fold cross-validation protocol for evaluation on all the three in-the-lab databases. RAF-DB <ref type="bibr" target="#b12">[13]</ref> is a real-world FER database, which contains 30,000 images labeled with basic or compound expressions by 40 trained human labelers. The images with six basic expressions and one neutral expression are used in our experiment. RAF-DB involves 12,271 images for training and 3,068 images for testing. SFEW <ref type="bibr" target="#b5">[6]</ref> is created by selecting static frames from Acted Facial Expressions in the Wild (AFEW) <ref type="bibr" target="#b6">[7]</ref>. The images in SFEW are labeled with six basic expressions and one neutral expression. We use 958 images for training and 436 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For each database, all the facial images are detected and cropped according to eye positions, and the cropped images are further resized to the size of 256 × 256. During the training process, the facial images are randomly cropped to the size of 224 × 224, and then a random horizontal flip is applied for data augmentation. During the test process, the input image is center cropped to the size of 224 × 224 and then fed into the trained model. The FDRL method is implemented with the Pytorch toolbox and the backbone network is a lightweight ResNet-18 model <ref type="bibr" target="#b10">[11]</ref>. Similarly to <ref type="bibr" target="#b22">[23]</ref>, the ResNet-18 is pre-trained on the MS-Celeb-1M face recognition database <ref type="bibr" target="#b9">[10]</ref>.</p><p>The dimension of the basic feature is 512. The dimensions of both the latent feature and the expression feature are 128. The value of δ in Eq. ( <ref type="formula">11</ref>) is empirically set to 0.5. We train our FDRL in an end-to-end manner with a single TITAN X GPU for 40 epochs, and the batch size for all the databases is set to 64. The model is trained using the Adam algorithm <ref type="bibr" target="#b11">[12]</ref> with the initial learning rate of 0.0001, β 1 = 0.500, and β 2 = 0.999. The learning rate is further divided by 10 after 10, 18, 25, and 32 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To show the effectiveness of our method, we perform ablation studies to evaluate the influence of key parameters and components on the final performance. For all the experiments, we use one in-the-lab database (MMI) and one inthe-wild database (RAF-DB) to evaluate the performance. Influence of the number of latent features. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, we can see that the proposed method achieves the best recognition accuracy when the number of latent features is set to 9. On one hand, when a small number of latent features are used, the expression similarities cannot be effectively modeled. On the other hand, when a large number of latent features are used, there exist redundancy and noise among latent features, thus leading to a performance decrease. In the following experiments, we set the number of latent features to 9. Influence of the parameters. We evaluate the recognition performance of the proposed method with the different values of λ 1 , λ 2 , and λ 3 in Eq. ( <ref type="formula" target="#formula_18">13</ref>), as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Specifically, we first fix λ 2 = 1.0 and λ 3 = 0.0001, and set the value of λ 1 from 0 to 0.01. Experimental results are given in Table <ref type="table" target="#tab_0">1</ref> (a). We can observe that our method achieves the best performance when the value of λ 1 is set to 0.0001. When λ 1 = 0, our method is trained without using the compactness loss, and the performance decreases. Table <ref type="table" target="#tab_0">1</ref> (b) shows the recognition performance obtained by our method, when the values of λ 1 and λ 3 are both set to 0.0001, and the value of λ 2 varies from 0 to 2.0. When the value of λ 2 is set to 1.0, our method achieves the top performance. Then, we fix λ 1 = 0.0001 and λ 2 = 1.0, and set the value   of λ 3 from 0 to 0.01. Experimental results are given in Table <ref type="table" target="#tab_0">1</ref> (c). Our method obtains the top performance when λ 3 = 0.0001. In the following, we set the values of both λ 1 and λ 3 to 0.0001, and set the value of λ 2 to 1.0. Influence of the key modules. To evaluate the effectiveness of the key modules in FDRL, we perform ablation studies for FDN, Intra-RM, and Inter-RM on the MMI and RAF-DB databases, respectively. Experimental results are reported in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We can see that incorporating FDN into the backbone network improves the performance, which shows the importance of FDN. Moreover, by employing Intra-RM or Inter-RM in FRN, we are able to achieve better recognition accuracy than the method combining FDN and the backbone network. This is because the features extracted by FDN are not distinguishable enough to classify different expressions, FDN does not take expression-specific variations into account. In contrast, Intra-RM and Inter-RM effectively model the intra-feature relationship of each latent feature and the inter-feature relationship between intra-aware features, respectively, leading to performance improvements. Our proposed FDRL method, which combines the backbone network, FDN, and FRN in an integrated network, achieves the best results among all the variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>2D feature visualization. We use t-SNE <ref type="bibr" target="#b15">[16]</ref> to visualize the expression features extracted by the baseline method (which only adopts ResNet-18) and the proposed FDRL method on the 2D space, respectively, as shown in Figure <ref type="figure">5</ref>. We can observe that the expression features extracted from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) CK+ MMI Oulu-CASIA PPDN <ref type="bibr" target="#b31">[32]</ref> 97.30 † -72.40 IACNN <ref type="bibr" target="#b16">[17]</ref> 95.37 ‡ 71.55 -DLP-CNN <ref type="bibr" target="#b12">[13]</ref> 95.78 † 78.46 -IPA2LT <ref type="bibr" target="#b27">[28]</ref> 92.45 ‡ 65.61 61.49 DeRL <ref type="bibr" target="#b26">[27]</ref> 97.37 ‡ 73.23 88.00 FN2EN <ref type="bibr" target="#b7">[8]</ref> 98.60 † -87.71 DDL <ref type="bibr" target="#b19">[20]</ref> 99. <ref type="bibr" target="#b15">16</ref>   <ref type="bibr" target="#b12">[13]</ref> 84.13 51.05 IPA2LT <ref type="bibr" target="#b27">[28]</ref> 86.77 58.29 SPDNet <ref type="bibr" target="#b0">[1]</ref> 87.00 58.14 RAN <ref type="bibr" target="#b23">[24]</ref> 86.90 56.40 SCN <ref type="bibr" target="#b22">[23]</ref> 87.01 -DDL <ref type="bibr" target="#b19">[20]</ref> 87 the baseline method are not easily distinguishable for different facial expressions. In contrast, the features extracted from our proposed method can effectively reduce intra-class differences and enhance inter-class separability for different expressions. Especially, compared with baseline, the differences between fear and surprise, disgust and sadness are more distinct for FDRL. Distribution of mean Intra-W vectors. We visualize the distribution of mean Intra-W vectors (corresponding to nine latent features) for seven basic expression categories on the RAF-DB database, as shown in Figure <ref type="figure">6</ref>. Generally, each expression shows relatively high weights on the latent features associated with facial actions (as shown in Figure <ref type="figure" target="#fig_3">3</ref>) closely related to this expression. Nevertheless, we can observe that some latent features (such as 2nd and 6th, 1st and 4th) have similar weights for different expressions. Hence, we further develop Inter-RM to exploit the inter-feature relationship between different intra-aware features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Art Methods</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the comparison results between our method and several state-of-the-art FER methods on the inthe-lab databases and the in-the-wild databases.</p><p>Among all the competing methods, IACNN, DDL, and RAN aim to disentangle the disturbing factors in facial expression images. SCN and IPA2LT are proposed to solve the noise label problem. FN2EN, DTAGN, and SPDNet improve the model performance by designing new network architectures. DLP-CNN alleviates intra-class variations by using a novel loss function. The above methods improve the FER performance by suppressing the influence of different disturbing factors or noise labels, but they ignore large expression similarities among different expressions. In contrast, our method explicitly models expression similarities and expression-specific variations with FDN and FRN, respectively, leading to performance improvements.</p><p>PPDN is developed to focus on the differences between expression images. DeRL claims that a facial expression is composed of the expression component and the neutral component. These two methods extract coarse-grained expression features. On the contrary, our proposed FDRL extracts more fine-grained features based on feature decomposition and reconstruction. Such a manner is beneficial to discriminate subtle differences between facial expressions, especially similar expression categories (such as fear and surprise). The above experimental results show the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel FDRL method for effective FER. FDRL consists of two main networks: FDN and FRN. FDN effectively models the shared information across different expressions based on a compactness loss. FRN accurately characterizes the unique information for each expression by taking advantage of Intra-RM and Inter-RM, and reconstructs the expression feature. In particular, Intra-RM encodes the intra-feature relationship of each latent feature and obtains an intra-aware feature. Inter-RM exploits the inter-feature relationship between different intra-aware features and extracts an inter-aware feature. The expression feature is represented by combining the intraaware feature and the inter-aware feature. Experimental results on both the in-the-lab and the in-the-wild databases have shown the superiority of our method to perform FER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -The images in each group show a similar facial action, but they are from different expressions. Images are from the RAF-DB database [13].</figDesc><graphic url="image-1.png" coords="1,310.28,281.21,118.66,70.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Overview of our proposed FDRL method. (a) The backbone network (ResNet-18) that extracts basic CNN features; (b) A Feature Decomposition Network (FDN) that decomposes the basic feature into a set of facial action-aware latent features; (c) A Feature Reconstruction Network (FRN) that learns an intra-feature relation weight and an inter-feature relation weight for each latent feature, and reconstructs the expression feature. FRN contains two modules: an Intra-feature Relation Modeling module (Intra-RM) and an Inter-feature Relation Modeling module (Inter-RM); (d) An Expression Prediction Network (EPN) that predicts an expression label.</figDesc><graphic url="image-30.png" coords="3,163.65,139.88,72.82,113.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 -</head><label>3</label><figDesc>Figure 3 -Visualization of the image groups from the RAF-DB database when M is set to 9. Each group corresponds to the highest intra-feature relation weight of the same latent feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Ablation studies for the different numbers of latent features on the MMI and RAF-DB databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 -Figure 6 -</head><label>56</label><figDesc>Figure 5 -Visualization of the expression features using t-SNE. Features are extracted from the RAF-DB database.</figDesc><graphic url="image-246.png" coords="7,322.57,209.24,103.22,77.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Comparisons on the in-the-lab databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>Ablation studies for the different values of λ1, λ2, and λ3 (represent the balance parameters for compactness loss, balance loss, and distribution loss, respectively) on MMI and RAF-DB. The recognition accuracy (%) is used for performance evaluation. Influence of λ 1 . Influence of λ 2 . Influence of λ 3 .</figDesc><table><row><cell></cell><cell>λ1</cell><cell></cell><cell></cell><cell cols="5">MMI RAF-DB</cell><cell>(b) λ2</cell><cell>MMI</cell><cell>RAF-DB</cell><cell>(c) λ3</cell><cell>MMI RAF-DB</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>84.64</cell><cell></cell><cell></cell><cell>88.75</cell><cell></cell><cell>0</cell><cell>82.66</cell><cell>88.23</cell><cell>0</cell><cell>84.96</cell><cell>89.15</cell></row><row><cell cols="5">0.00001 85.02</cell><cell></cell><cell></cell><cell>89.02</cell><cell></cell><cell>0.5</cell><cell>83.68</cell><cell>88.89</cell><cell>0.00001 85.07</cell><cell>88.89</cell></row><row><cell cols="3">0.0001</cell><cell></cell><cell>85.23</cell><cell></cell><cell></cell><cell>89.47</cell><cell></cell><cell>1.0</cell><cell>85.23</cell><cell>89.47</cell><cell>0.0001</cell><cell>85.23</cell><cell>89.47</cell></row><row><cell cols="3">0.001</cell><cell></cell><cell>82.67</cell><cell></cell><cell></cell><cell>88.82</cell><cell></cell><cell>1.5</cell><cell>84.94</cell><cell>88.92</cell><cell>0.001</cell><cell>82.66</cell><cell>88.95</cell></row><row><cell cols="3">0.01</cell><cell></cell><cell>82.24</cell><cell></cell><cell></cell><cell>88.63</cell><cell></cell><cell>2.0</cell><cell>83.23</cell><cell>88.63</cell><cell>0.01</cell><cell>81.64</cell><cell>88.49</cell></row><row><cell>89.00 90.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">89.47%</cell><cell>RAF-DB MMI</cell></row><row><cell>85.00 86.00 87.00 88.00 Accuracy(%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">85.23%</cell><cell></cell></row><row><cell>84.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>83.00</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10 11 12 13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Number of latent features</cell></row></table><note>(a)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 -</head><label>2</label><figDesc>Ablation studies for three key modules of our FDRL on the MMI and RAF-DB databases. The recognition accuracy (%) is used for performance evaluation.</figDesc><table><row><cell>FDN</cell><cell cols="2">FRN Intra-RM Inter-RM</cell><cell>MMI</cell><cell>RAF-DB</cell></row><row><cell>× √ √ √ √</cell><cell>× × × √ √</cell><cell>× × √ × √</cell><cell>79.69 81.23 83.44 84.74 85.23</cell><cell>86.93 87.71 88.76 89.34 89.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 -</head><label>3</label><figDesc>Performance comparisons among different methods on several public FER databases. The best results are boldfaced. ‡ and † respectively denote that seven expression categories and six expression categories are used in CK+.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparisons on the in-the-wild databases.</figDesc><table><row><cell></cell><cell></cell><cell>(b) Methods</cell><cell>Accuracy (%) RAF-DB SFEW</cell></row><row><cell></cell><cell></cell><cell>IACNN [17]</cell><cell>-</cell><cell>50.98</cell></row><row><cell></cell><cell></cell><cell>DLP-CNN</cell></row><row><cell></cell><cell>‡ 83.67</cell><cell>88.26</cell></row><row><cell>Baseline</cell><cell>97.15 ‡ 79.69</cell><cell>86.18</cell></row><row><cell cols="2">FDRL (proposed) 99.54  ‡ 85.23</cell><cell>88.26</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was in part supported by the National Natural Science Foundation of China under Grants 62071404 and 61872307, by the Natural Science Foundation of Fujian Province under Grant 2020J01001, by the Youth Innovation Foundation of Xiamen City under Grant 3502Z20206046, and by the Beijing Science and Technology Project under Grant Z181100008918018.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Covariance pooling for facial expression recognition</title>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Island loss for learning discriminative features in facial expression recognition</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Shehab Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<meeting>eeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Prodger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new facial expression recognition method based on local gabor filter bank and PCA plus LDA</title>
		<author>
			<persName><forename type="first">Hong-Bo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lian-Wen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Xin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Cheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE International Conference on Computer Vision Workshops</title>
				<meeting>eeding of IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FaceNet2ExpNet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Shaohua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<meeting>eeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wallace</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on fusion feature of PCA and LBP with SVM</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cai-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2767" to="2770" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity-aware convolutional neural network for facial expression recognition</title>
		<author>
			<persName><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition)</title>
				<meeting>eeding of IEEE International Conference on Automatic Face &amp; Gesture Recognition)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PCA-based dictionary building for accurate facial expression recognition via sparse representation</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Fatemizadeh</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1082" to="1092" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludo</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Conference on Multimedia and Expo</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep disturbance-disentangled learning for facial expression recognition</title>
		<author>
			<persName><forename type="first">Delian</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2833" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity-and pose-robust facial expression recognition through adversarial feature learning</title>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="238" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6897" to="6906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition</title>
		<author>
			<persName><forename type="first">Siyue</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="177" to="191" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial expression recognition by de-expression residue learning</title>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2168" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName><forename type="first">Jiabei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="222" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint pose and expression modeling for facial expression recognition</title>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3359" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facial expression recognition from nearinfrared videos</title>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on PCA and NMF</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhe</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 7th World Congress on Intelligent Control and Automation</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="6826" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yugang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
