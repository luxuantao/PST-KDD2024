<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotion-Preserving Representation Learning via Generative Adversarial Network for Multi-view Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ying-Hsiu</forename><surname>Lai</surname></persName>
							<email>lai@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shang-Hong</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Emotion-Preserving Representation Learning via Generative Adversarial Network for Multi-view Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/FG.2018.00046</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>face frontalization</term>
					<term>facial expression recognition</term>
					<term>pose variation</term>
					<term>263</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face frontalization is one way to overcome the pose variation problem, which simplifies multi-view recognition into one canonical-view recognition. This paper presents a multi-task learning approach based on the generative adversarial network (GAN) that learns the emotion-preserving representations in the face frontalization framework. Taking advantage of adversarial relationship between the generator and the discriminator in GAN, the generator can frontalize input non-frontal face images into frontal face images while preserving the identity and expression characteristics; in the meantime, it can employ the learnt emotion-preserving representations to predict the expression class label from the input face. The proposed network is optimized by combining both synthesis and classification objective functions to make the learnt representations generative and discriminative simultaneously. Experimental results demonstrate that the proposed face frontalization system is very effective for expression recognition with large head pose variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, due to the emergence of deep learning, significant progress has been made in face-related tasks. Mollahossein et al. <ref type="bibr" target="#b11">[12]</ref> proposed a deep neural network architecture inspired by GoogLeNet <ref type="bibr" target="#b18">[19]</ref> and AlexNet <ref type="bibr" target="#b7">[8]</ref> for developing a facial expression recognition system, which outperforms traditional methods based on handcrafted features. Jung et al. <ref type="bibr" target="#b6">[7]</ref> joint fine-tuned two small deep network models, temporal appearance network and temporal geometry network, to obtain more discriminative features and achieved higher performance on two public facial expression recognition databases, i.e., CK+ database <ref type="bibr" target="#b9">[10]</ref>, Oulu-CASIA database <ref type="bibr" target="#b24">[25]</ref>. However, most of the facial expression recognition methods focus on analysis of expressions from frontal faces. Even applying deep learning technologies has made significant improvements, pose variation is still a challenging problem for many realistic face-related applications.</p><p>To deal with the problem of head pose variations, increasing massive face images with arbitrary views for training is a common and easy way to learn pose-robust representations. However, collecting and labeling a large number of face images is quite a huge work, and the improvement of recognition is limited. Another intuitive approach is to simplify the problem of face-related recognition under large pose Figure <ref type="figure">1</ref>. The flow chart of the proposed method for multi-task learning. Given a non-frontal face image, the proposed model would predict its expression and synthesize its frontal view at the same time.</p><p>variations by reducing it to the canonical view recognition, i.e., automatic synthesis of the corresponding frontal face image from a non-frontal face image.</p><p>Frontalization is to synthesize a frontal face image from a non-frontal face image. By using deep learning algorithms, Zhu et al. <ref type="bibr" target="#b27">[28]</ref> first developed a simple neural network to learn identity-preserving pose-invariant features from a frontalization process, and they obtained great improvement for non-frontal face recognition. After that, some other face recognition researches <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref> also presented more complicated deep models, e.g., convolutional neural network (CNN), generative adversarial network <ref type="bibr" target="#b1">[2]</ref>, and obtained better synthesis results and better handled large pose variations. However, the existing frontalization approaches can only preserve face identity with neural expression, but they cannot preserve facial expressions after the frontalization. Therefore, we aim to develop a face frontalization system that can preserve the facial expression in this paper. This paper presents a multi-task learning via generative adversarial networks for multi-view facial expression recognition. As the example shown in Fig. <ref type="figure">1</ref>, given a profile face image at an arbitrary head pose and with an arbitrary expression, the proposed model would generate two kinds of outputs: the expression class label and synthesized frontal face image. We design different kinds of objective functions for learning the emotion-preserving representations during the frontalization process, which can not only facilitate the synthesized frontal face image maintaining more expression characteristics, but also obtain more discriminative poseinvariant features for the expression recognition under large poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Facial Expression Recognition</head><p>For deep learning based approaches, the facial expression recognition systems have been developed very well for frontal face expressions. Mollahossein et al. <ref type="bibr" target="#b11">[12]</ref> exploited deep neural network architecture to simplify traditional hand-crafted feature extraction and feature selection methods, but these methods still outperformed the traditional handcraft-based methods. In addition, Jung et al. <ref type="bibr" target="#b6">[7]</ref> developed jointly fine-tuned small deep network models, temporal appearance model and temporal geometric model, which provide better recognition results than the single deeper network model. However, the existing methods still suffer from the accuracy degradation under large head pose variations.</p><p>To overcome the head pose variation problem, facial expression recognition methods can be grouped into two categories: 3D-based and 2D-based approaches. 3D-based methods typically exploit 3D features or map the 3D data onto a representation <ref type="bibr" target="#b17">[18]</ref>. Since 3D face data is informationrich by nature, 3D-based methods make them more robust to pose variation. However, 2D-based studies are used more often in practice, because 2D data can be easily obtained and processed with different ways. For 2D-based methods, the researchers usually focused on developing discriminative pose-invariant features or handling facial expression recognition separately on different face views. Zhang et al. <ref type="bibr" target="#b23">[24]</ref> proposed a deep neural network (DNN) model to learn the relationship between extracted low-level SIFT features and high-level information. Jampour et al. <ref type="bibr" target="#b5">[6]</ref> introduced a mapping algorithm that maps the features extracted from non-frontal view to an approximately frontal view feature space according to the head pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Frontalization</head><p>Face frontalization means automatically synthesizing a frontal face image from a face image at an arbitrary head pose. There are two ways to accomplish face frontalization: 3D transformation solutions and 2D deep learning-based solutions.</p><p>In 3D transformation solutions, Hassner et al. <ref type="bibr" target="#b3">[4]</ref> aligned 2D non-frontal face to 3D reference model points by utilizing facial landmarks, and then computed projection matrix to transform the non-frontal face into frontal view. Zhu et al. <ref type="bibr" target="#b26">[27]</ref> not only used facial landmark to align with 3D points, but also meshed 2D face into 3D object and normalized facial expression during the frontalization process.</p><p>In contrast, 2D deep learning-based solutions do not need to design algorithms in each step manually; they just designed a deep network architecture to learn the whole process of frontalization directly. The first approach <ref type="bibr" target="#b27">[28]</ref> presented the possibility of utilizing deep models to learn identity-preserving representation during the frontalization. Yim et al. <ref type="bibr" target="#b20">[21]</ref> went on to develop a novel network that can not only generate frontal face views, but also generate arbitrary face views of the desired poses. At the same time, they designed a new multi-task learning strategy that can recover the generated face back to original face views, in order to improve the identity preserving ability.</p><p>Furthermore, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref> showed that the frontalization results can be greatly improved when generative adversarial networks (GAN) <ref type="bibr" target="#b1">[2]</ref> replaced the simple deep neural networks, and the adversarial loss replaced the simple L 2 loss. Especially the TP-GAN network proposed by Huang et al. <ref type="bibr" target="#b4">[5]</ref> can even deal with 90-degree large pose frontalization. However, no deep learning-based approaches have put their interests on preserving facial expression characteristics during the frontalization, since they only focused on improving the performance in face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>This paper proposes a novel facial expression representation learning method based on GAN network, which can not only recognize expressions but also synthesize the corresponding frontal face images at the same time. We aim to synthesize the frontal face images from profile face images and learn the emotion-preserving representation under the face frontalization process. Our goal is to train a multi-task learning network that can generate the emotion-preserving frontal views to achieve canonical-view facial expression recognition, and simultaneously recognize expressions in our network model. Therefore, the proposed GANmodel as depicted in Figure <ref type="figure" target="#fig_0">2</ref> is not only for improving the face synthesis quality, but also for better representation learning on both the generator and the discriminator due to the adversarial loss in GAN. Inspired by the existing GAN-based face frontalization approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref> for face recognition fields, our generator G is designed with an encoder-decoder structure model, consisting of encoder G enc and decoder G dec , where G enc was taken to learn the emotion-preserving representation from I P , and G dec was taken to recover the frontal view of I P as similar as I F from the extracted features. Moreover, the bottleneck of G enc , the extracted emotion-preserving features, can be used to recognize expressions directly. Therefore, we apply an additional fully-connection layer at the end of the bottleneck layer in G enc to enforce G be trained for multi-task learning.</p><p>For the discriminator D, its main goal is to distinguish the real images I F from the fake generated images G(I P ). The minimax two-player game lets the generated frontal faces G(I P ) move towards the same distribution as real images I F , and makes it really difficult to separate the generated images from the real images. Furthermore, D as a discriminator, of course it also can be trained to recognize expression at the same time. Not only the generative model benefits from the adversarial relationship between G and D, but also the representation learning ability, so the design of multitask learning for D can make G learn more discriminative emotion-preserving representations and improve the performance for facial expression recognition.</p><p>The detailed structures of D and G enc are provided in Table <ref type="table" target="#tab_0">I</ref>. Take Conv1 layer for example, Conv1 is a convolutional layer with filter size 33, stride-1, and its outputs are 32 128128 feature maps, and so on. In addition, FC means fully connected layer. In D and G enc , we replace common pooling layers with 2-strided convolutions. Particularly, there are two branch layers on top of the Conv5 layer. Branch Conv6 is for facial expression recognition, and branch Conv9 is for distinguishing fake generated images from real images. Therefore, both D and G enc consist of branch Conv6 for expression recognition, but only D includes branch Conv9 to judge real/fake. Specifically, Conv8 layer represents the to-be-learned emotion-preserving features from G enc .</p><p>In the proposed GAN model, we use a random vector to represent some other face variations, except expressions, for the face synthesis. However, in our experiments without adding the noise vector, the expression recognition result is very similar to that with adding the noise vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Objective Function</head><p>This paper adopts different loss functions to optimize the proposed model, the optimization losses can be grouped into two categories: synthesis loss and classification loss. The following sections will describe each individual loss function included in the total loss function in detail.</p><p>1) Adversarial Loss: The basic network structure of GAN <ref type="bibr" target="#b1">[2]</ref> contains a generator G and a discriminator D that com- </p><formula xml:id="formula_0">L ad D = E[log (D(I F ))] + E[1 − log (D(G(I P )))] L ad G = −E[log (D(G(I P )))]<label>(1)</label></formula><p>As the minimax game formulation introduced in <ref type="bibr" target="#b1">[2]</ref>, originally G was optimized by minimizing (1 − log?D(G(I P ))). However, since D converges early in learning that G cannot obtain sufficient gradients to learn well. So it is better for G to alternatively maximize log?D(G(I P )) (same as minimizing − log D(G(I P )) ) in practice. Adversarial loss makes the synthesis images look like the real frontal face images. It can prevent blurred effects and synthesize highfidelity images.</p><p>2) Pixel-wise Loss: To speed up the convergence of G and facilitate the image content consistency, we adopt pixelwise L 1 loss between the synthesized frontal faces G(I P ) and ground truth frontal faces I F , given by:</p><formula xml:id="formula_1">L pixel = 1 W × H W ∑ x=1 H ∑ y=1 |G(I P x,y ) − I F x,y | (2)</formula><p>where W, H represent the width and height of the image, and x, y means the position in the image space. Instead of using L 2 loss but L 1 loss is because L 1 loss is more robust, and L 2 loss is too sensitive to the training samples and easily influenced by "outlier". Although pixelwise loss would cause blurred effects, it is still an important part for accelerating the optimization speeds and improving synthesis performance.</p><p>3) Symmetry Loss: Due to the self-occlusion on profile faces, it is quite hard to recover the occluded facial parts back to frontal face views. Generally, human faces have symmetrical characteristics that the left and right sides of face are bilateral symmetry. Therefore, we exploit the symmetry traits of human face as a prior to solve the selfocclusion problem slightly on large pose cases and thus may improve the frontal face synthesis results. The equation of symmetry loss is given as follows:</p><formula xml:id="formula_2">L sym = 1 (W /2) × H W /2 ∑ x=1 H ∑ y=1 |G(I P x,y ) − G(I P W −(x−1),y )| (3)</formula><p>However, human faces, especially with expressions, are not symmetric all the time, so we adjust the weighting for symmetry loss to reduce the symmetry constraint for face image synthesis. 4) Feature Loss: In the Improved-GAN <ref type="bibr" target="#b16">[17]</ref>, they introduced many kinds of improved techniques for training the GAN model; the feature loss L f eat actually is one of the improved techniques called feature matching. Feature matching facilitates the generator to generate the images that match the probability distribution of real frontal face images, which is a way to prevent the generator G from overtraining on the current discriminator. The feature loss function is given by</p><formula xml:id="formula_3">L f eat = 1 N N ∑ i=1 |F(I P i ) − F(I F i )|<label>(4)</label></formula><p>where F represents the features for matching, and N is the total number of the features. Originally, improved-GAN trained the generator by L f eat on an intermediate layer of the discriminator, whereas we optimize L f eat on emotion-preserving feature vector in G enc . We compute the feature loss by L 1 loss between the features of profile images I P and the features of truth frontal images I F , in order to obtain more discriminative features, in addition to match the probability distribution of real frontal face images.</p><p>5) Classification Loss: Besides the synthesis optimization functions, we adopt the classification loss to optimize the performance of facial expression recognition on both G and D. According to auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b13">[14]</ref>, every training sample has a class label y, and the discriminator D estimates both the real frontal face probability distribution and the class label probability distribution. In other words, D is optimized by the log-likelihood of the real images L ad D (Section III-B1) and the log-likelihood of the correct class L class (Equation <ref type="formula">5</ref>). L class includes the loglikelihood of correct class on both real frontal faces D(I F ) and synthesis frontal faces D(G(I P )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L class = E[log(D(I F ) = y] + E[log(D(G(I</head><formula xml:id="formula_4">P )) = y)] (5) L class enc = E[log(G enc (I P ) = y)] (6)</formula><p>For the same situation, G is also trained to optimize L class to match real frontal faces probability distribution and at once learn more discriminative emotion-preserving representation. Moreover, the proposed G is trained with an additional classification loss L class enc (Equation <ref type="formula">6</ref>) to minimize the log-likelihood of the correct class from the emotion-preserving features in G enc directly, which can make G be able to deal with multi-task learning.</p><p>6) Overall Objection Function: To sum up, the overall objective function for D is denoted by L θ D , and the overall objective function for G is denoted by L θ G , and they are given as follows:</p><formula xml:id="formula_5">L θ D = μ 1 L ad D + μ 2 L class , (<label>7</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">L θ G = L synthesis + L classi f ication , (<label>8</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">L synthesis = λ 1 L ad G + λ 2 L pixel + λ 3 L sym + λ 4 L f eat , (<label>9</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">L classi f ication = λ 5 L class enc + λ 6 L class , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where μ's and λ 's are parameters for adjusting the weights for individual loss functions. The synthesis loss in L θ G includes adversarial loss, pixelwise loss, symmetry loss, and feature loss. The classification loss in L θ G consists of L class enc and L class .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>The proposed method aims at both representation learning and frontal face synthesis. We quantitatively demonstrate the representation learning capability of our method for multiview facial expression recognition in Sec IV-B, and illustrate the qualitative frontal face synthesis results in Sec IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setting 1) Implementation Details:</head><p>In pre-processing, we apply MTCNN <ref type="bibr" target="#b22">[23]</ref> to detect human face. According to the predicted bounding box and five facial landmark points, we crop the detected face and resize it into a 128×128 grayscale image, with a setting that the nose would be the center in x-axis coordinates (left-right). Our network is implemented on Tensorflow <ref type="bibr" target="#b0">[1]</ref>. We use Adam optimizer with learning rate of 10 −4 and momentum of 0.5. We empirically set the weighting parameters μ 1 = 0.5, μ 2 = 0.5, λ 1 = 10 −3 , λ 2 = 1, λ 3 = 0.3, λ 4 = 0.03, λ 5 = 0.1, λ 6 = 0.05 for all experiments. Batch size is set to 78 in Multi-PIE database, 60 in BU-3DFE database. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Feature Subjects Pose Expression number Accuracy Moore <ref type="bibr" target="#b12">[13]</ref> LBP ms 100, 10-fold 7 6 73.98% Moore <ref type="bibr" target="#b12">[13]</ref> LGBP 100, 10-fold 7 6 80.17% GSRRR <ref type="bibr" target="#b25">[26]</ref> LBP u2 100, 5-fold 7 6 81.7% 2D JFDNN <ref type="bibr" target="#b6">[7]</ref> Image+landmarks(DNN) 100, 5-fold 7 6 82.9% Zhang <ref type="bibr" target="#b23">[24]</ref> SIFT(DNN) 100, 5-fold 7 6 82.0% KPSNM <ref type="bibr" target="#b5">[6]</ref> HoG+LBP 145, X 13 6 82.55% KPSNM <ref type="bibr" target="#b5">[6]</ref> HoG+LBP  2) Databases: The Multi-PIE database <ref type="bibr" target="#b2">[3]</ref> contains more than 750000 images of 337 subjects with four recording sessions, in which 235 are male, 107 are female. Subjects were recorded with 15 cameras at different viewpoints and 19 different illumination conditions. In addition, subjects were asked to perform different expressions in each session. For each recording session, the participants and recorded expressions are a bit different. In total, there are six kinds of expressions recorded, which consist of neutral, smile, squint, surprise, disgust, and scream. Although there are 337 subjects in the Multi-PIE database, only 100 subjects presented in all four recording sessions are selected, in order to balance the training data class labels. 13 face views are chosen in the experiments, i.e., 0</p><formula xml:id="formula_13">• , ±15 • , ±30 • , ±45 • , ±60 • , ±75 • , ±90 • face views are considered.</formula><p>The BU-3DFE database <ref type="bibr" target="#b21">[22]</ref> contains 100 subjects (56% female, 44% male) with a variety of ethnics, ages. Every subject was recorded with six standard facial expressions, i.e., anger (AN), disgust (DI), fear (FE), happiness (HA), sadness (SA), and surprise (SU), of four levels of intensities. Therefore, there are 24 instant 3D expression models for each subject. To study on the multi-view facial expression recognition fields, we render these 3D expression models and adjust viewpoints to project them into 2D face images with specified head poses, i.e., with 0 • , ±30 • , ±45 • , ±60 • , ±90 • yaw angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Representation Learning 1) Results on Multi-PIE Database:</head><p>We compare our method with the state-of-the-art methods for multi-view facial expression recognition in Multi-PIE database. We use 5-fold cross validation for the experiments, i.e., we randomly divide 100 subjects into 80 subjects for training and 20 subjects for testing, and there are no overlap between the training subjects and the testing subjects.</p><p>Table <ref type="table" target="#tab_1">II</ref> shows the comparison between our work and some previous methods. Among these methods, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b23">[24]</ref> are deep learning based approaches and the others are traditional methods using hand-crafted features. Instead of taking 7 poses (0 • , 15 • , 30 • , 45 • , 60 • , 75 • , 90 • yaw angles) which were used in most of the previous works, we use 13 poses (0 • , ±15 • , ±30 • , ±45 • , ±60 • , ±75 • , ±90 • yaw angles) to conduct the experiment, which lead to worse performance according to the results of <ref type="bibr" target="#b5">[6]</ref>. However, our method outperforms the state-of-the-art methods in this experiment. G enc (I P ) represents the performance of expression recognition based on the encoder G enc , and D(G(I P )) represents the result that D uses the synthesis image G(I P ) to recognize expressions. As the result, our generated frontal face images have preserved the expression characteristics that are effective for the recognition task.</p><p>Table <ref type="table" target="#tab_1">III</ref> shows the overall accuracies of both G enc (I P ) and D(G(I P )) under different head poses, and Table <ref type="table" target="#tab_4">IV</ref> presents the corresponding accuracies on every expression under different head poses predicted by G enc . Consequently, the performance of D(G(I P )) is very similar to G enc (I P ). From Table <ref type="table" target="#tab_4">IV</ref>, most of the expressions are easier to recognize under smaller pose angles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Feature Subjects Pose Expression number Accuracy Moore <ref type="bibr" target="#b12">[13]</ref> LBP ms 100, 10-fold 5 6(4 levels) 65.0% Moore <ref type="bibr" target="#b12">[13]</ref> LGBP 100, 10-fold 5 6(4 levels) 68.0% GSRRR <ref type="bibr" target="#b25">[26]</ref> LBP u2 100, 5-fold 5 6(4 levels) 66.0% 2D JFDNN <ref type="bibr" target="#b6">[7]</ref> Image+landmarks(DNN) 100, 5-fold 5 6(4 levels) 72.5% Zhang <ref type="bibr" target="#b23">[24]</ref> SIFT(DNN) 100, 5-fold 5 6(4 levels) 80.1% Ours G enc (I P ) Image(GAN) 100, 5-fold 5 6(4 levels) 73.13%</p><p>2) Ablation Study: To demonstrate the contribution of the loss function proposed in this paper to the final expression recognition accuracy, we perform an ablation study to evaluate the model accuracies by incrementally adding the loss terms. Here, we denote the adversarial loss, pixel-wise loss, symmetry loss, feature loss, and classification loss by L ad , L pixel , L sym , L f eat , and L classi f ication , respectively. The detailed definitions of these loss terms are defined in the previous section. The expression classification accuracies by using the proposed deep network model trained with different combinations of the loss terms under the same experimental setting for MULTI-PIE dataset are listed in Table <ref type="table" target="#tab_5">VI</ref>. It is clear to see the incremental improvement of recognition accuracies by adding the loss terms. 3) Results on BU-3DFE Database: Similar to the setting in Multi-PIE database, we apply 5-fold cross validation, and take the 5 head poses (0 • , 30 • , 45 • , 60 • , 90 • ) for the experiment in BU-3DFE database. We also use the images with four levels of expression intensities to conduct the experiment. It is challenging for the generator to preserve much detail of face characteristics (e.g. identity, expression) and learn discriminative features for expression recognition.</p><p>Table <ref type="table" target="#tab_5">V</ref> shows the comparison with other previous methods on BU-3DFE database. Our method outperforms most of the previous works except <ref type="bibr" target="#b23">[24]</ref>, which is one of the deep learning based approach that utilized SIFT features as their designed deep neural network (DNN) inputs to learn higher-level representations. This is probably because the proposed method did not explicitly use detailed local expression representation, thus making the proposed model unable to achieve the best expression recognition accuracy for the BU-3DFE database. However, the proposed model still can compete with another deep learning based approach <ref type="bibr" target="#b6">[7]</ref> that utilized image feature and landmarks to jointly train a DNN discriminator. In Comparison with the results on the Multi-PIE database, the expressions in BU-3DFE are more difficult to recognize, even by humans. For example, the difference between Fear, Sadness, and Anger are very subtle and these expressions are easily confused with each other. For the experiments on both BU-3DFE database and Multi-PIE database, the competitive recognition performance by using D(G(I P )) indicates that the synthesized frontal face images by using our proposed network are effective for the expression recognition task. <ref type="table" target="#tab_9">IX</ref> show the frontal face synthesis results on Multi-PIE database and BU-3DFE database (more synthesis results are shown in Supplement). Consequently, the learnt representations facilitate the synthesized frontal face images to preserve a certain expression, identity characteristics. In addition, the advantage of GAN makes the synthesized faces similar to the corresponding real frontal face I F and achieves high-quality image synthesis results, even in the cases with large head poses. Specifically, the details of facial characteristics like wrinkles and beards are difficult for our model to reconstruct perfectly, but the learnt emotionpreserving representations make the generator reconstruct the corresponding frontal faces with expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Synthesis 1) Synthesis Results on Databases: Table VIII and Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparison of Frontalization Results:</head><p>To further demonstrate the image synthesis ability of our work, we conduct an experiment to compare the synthesis results with other existing face frontalization methods. For the consideration that some methods can only handle the pose smaller than 45 angle, we present the frontalization results that are under small head poses (Table <ref type="table" target="#tab_10">X</ref>). In fact, our work can synthesize realistic-looking frontal faces under very large poses.    From Table <ref type="table" target="#tab_10">X</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref> are 3D-based frontalization solutions that their results would depend on the precision of the facial landmarks detector. <ref type="bibr" target="#b3">[4]</ref> did not handle well to fill in the invisible region caused by self-occlusion. Thus, their results may generate ghosting shadows that may influences the performance of expression recognition. The results from <ref type="bibr" target="#b26">[27]</ref> are seriously distorted that the facial characteristics, including expression and identity characteristics, are quite different from the ground truth frontal faces. In Table X, the L 2 distance between the synthesized image and the target image is also shown under each synthesized image. It is clear from the table that the proposed model can provide visually appealing face frontalization results, though the L 2 distance to the ground-truth image is not the smallest among all methods under comparison. V. CONCLUSION In conclusion, this paper a multi-task GANnetwork model that emotion-preserving representation during face frontalization process. The discriminator is trained to distinguish real/fake and recognize class labels. The encoder in the generator learns representative features not only for recognition, but also makes the decoder capable of reconstructing emotion-preserving and realisticlooking frontal faces. By combining several different loss functions, the learnt representations are discriminative for facial expression recognition under large head pose variations, and the synthesized frontal face images maintain the expression characteristics that are effective for recognition task. Experimental results demonstrate that the proposed method outperforms the state-of-the-art facial expression recognition methods on Multi-PIE database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The proposed GAN model</figDesc><graphic url="image-3.png" coords="2,330.99,438.34,154.38,136.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fear</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I NETWORK</head><label>I</label><figDesc>ARCHITECTURE FOR D AND G enc .</figDesc><table><row><cell>Layer</cell><cell>Filter Size</cell><cell>Output Size</cell></row><row><cell>Conv1</cell><cell>3 × 3/1</cell><cell>128 × 128 × 32</cell></row><row><cell>Conv21</cell><cell>3 × 3/2</cell><cell>64 × 64 × 64</cell></row><row><cell>Conv22</cell><cell>3 × 3/1</cell><cell>64 × 64 × 64</cell></row><row><cell>Conv23</cell><cell>3 × 3/1</cell><cell>64 × 64 × 64</cell></row><row><cell>Conv31</cell><cell>3 × 3/2</cell><cell>32 × 32 × 128</cell></row><row><cell>Conv32</cell><cell>3 × 3/1</cell><cell>32 × 32 × 128</cell></row><row><cell>Conv33</cell><cell>3 × 3/1</cell><cell>32 × 32 × 128</cell></row><row><cell>Conv41</cell><cell>3 × 3/2</cell><cell>16 × 16 × 256</cell></row><row><cell>Conv42</cell><cell>3 × 3/1</cell><cell>16 × 16 × 256</cell></row><row><cell>Conv43</cell><cell>3 × 3/1</cell><cell>16 × 16 × 256</cell></row><row><cell>Conv5</cell><cell>3 × 3/2</cell><cell>8 × 8 × 512</cell></row><row><cell>Conv6</cell><cell>3 × 3/2</cell><cell>4 × 4 × 512</cell></row><row><cell>Conv7</cell><cell>3 × 3/2</cell><cell>2 × 2 × 512</cell></row><row><cell>Conv8</cell><cell>3 × 3/2</cell><cell>1 × 1 × 256</cell></row><row><cell>FC1</cell><cell>−</cell><cell>class number</cell></row><row><cell></cell><cell></cell><cell>for D and Genc</cell></row><row><cell>Conv9</cell><cell>3 × 3/4</cell><cell>2 × 2 × 256</cell></row><row><cell></cell><cell></cell><cell>for D only</cell></row><row><cell>FC2</cell><cell>−</cell><cell>1(real/fake)</cell></row><row><cell></cell><cell></cell><cell>for D only</cell></row></table><note>pete with two-player minimax game. D tries to distinguish the real frontal face images I F from fake generated frontal faces G(I P ), and G tries to generate realistic-like frontal faces to fool D. The corresponding adversarial losses are listed below, D is trained to maximize L ad D , and G is trained to minimize L ad G :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II COMPARISON</head><label>II</label><figDesc>WITH EXISTING METHODS IN MULTI-PIE DATABASE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV CORRESPONDING</head><label>IV</label><figDesc>ACCURACIES ON DIFFERENT EXPRESSIONS AT DIFFERENT HEAD POSE ANGLES IN MULTI-PIE DATABASE.</figDesc><table><row><cell>Acc. (%)</cell><cell>0 •</cell><cell>15 •</cell><cell>30 •</cell><cell>45 •</cell><cell>60 •</cell><cell>75 •</cell><cell>90 •</cell><cell>Avg.</cell></row><row><cell>Neutral</cell><cell>92</cell><cell>93</cell><cell>92.5</cell><cell>89.5</cell><cell>90.5</cell><cell>88</cell><cell>70.5</cell><cell>87.69</cell></row><row><cell>Smile</cell><cell>95</cell><cell>98.5</cell><cell>99</cell><cell>98</cell><cell>95.5</cell><cell>94.5</cell><cell>83.5</cell><cell>94.85</cell></row><row><cell>Squint</cell><cell>80</cell><cell>78.5</cell><cell>78</cell><cell>77.5</cell><cell>74.5</cell><cell>80</cell><cell>74.5</cell><cell>77.38</cell></row><row><cell>Surprise</cell><cell>98</cell><cell>96</cell><cell>96.5</cell><cell>97</cell><cell>96.5</cell><cell>90</cell><cell>81.5</cell><cell>93.31</cell></row><row><cell>Disgust</cell><cell>78</cell><cell>76</cell><cell>78</cell><cell>74.5</cell><cell>73</cell><cell>67.5</cell><cell>69.5</cell><cell>73.46</cell></row><row><cell>Scream</cell><cell>97</cell><cell>96.5</cell><cell>96.5</cell><cell>95.5</cell><cell>95</cell><cell>92</cell><cell>86</cell><cell>93.85</cell></row><row><cell>Overall</cell><cell>90</cell><cell>89.75</cell><cell>90.08</cell><cell>88.67</cell><cell>87.5</cell><cell>85.33</cell><cell>77.58</cell><cell>86.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V COMPARISON</head><label>V</label><figDesc>OF DIFFERENT EXPRESSION RECOGNITION METHODS ON BU-3DFE DATABASE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VIII SYNTHESIS</head><label>VIII</label><figDesc>RESULTS ON MULTI-PIE DATABASE: ROW (A) SHOWS THE INPUT PROFILE IMAGES I P , AND ROW (B) SHOWS THE SYNTHESIS RESULTS G(I P ) FOR DIFFERENT EXPRESSIONS. THE INPUT IMAGES UNDER 0 • ANGLE CAN BE CONSIDERED AS THE REAL FRONTAL FACE IMAGES I F . EACH COLUMN REPRESENTS THE CORRESPONDING VIEWPOINTS: 0 • , 15 • , 30 • , 45 • , 60 • , 75 • , 90 • YAW ANGLES.</figDesc><table><row><cell>0 •</cell><cell>15 •</cell><cell>30 •</cell><cell>45 •</cell><cell>60 •</cell><cell>75 •</cell><cell>90 •</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IX SYNTHESIS</head><label>IX</label><figDesc>RESULTS ON 3D-BUFE DATABASE: ROW (A) SHOWS THE INPUT PROFILE IMAGES I P , AND ROW (B) SHOWS THE SYNTHESIS RESULTS G(I P ) FOR DIFFERENT EXPRESSIONS. THE INPUT IMAGES UNDER 0 • ANGLE CAN BE CONSIDERED AS THE REAL FRONTAL FACE IMAGES I F . EACH COLUMN REPRESENTS THE CORRESPONDING VIEWPOINTS: 0 • , 30 • , 45 • , 60 • , 90 • .</figDesc><table><row><cell>0 •</cell><cell>90 •</cell><cell>60 •</cell><cell>45 •</cell><cell>30 •</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table X THE</head><label>X</label><figDesc>FIRST ROW GIVES AN EXAMPLE OF A SMILING FACE UNDER 45 • HEAD POSE, AND THE SECOND ROW DEPICTS AN EXAMPLE OF A SCREAMING FACE UNDER 30 • HEAD POSE. THE FIRST COLUMN IS THE INPUT IMAGE I P , THE LAST COLUMN IS THE GROUND TRUTH FRONTAL IMAGE I F , AND THE MIDDLE COLUMNS ARE THE FACE FRONTALIZATION RESULTS BY USING DIFFERENT FACE FRONTALIZATION APPROACHES. THE L 2 DISTANCE BETWEEN THE SYNTHESIZED IMAGE AND THE TARGET IMAGE IS GIVEN UNDER EACH SYNTHESIZED IMAGE.</figDesc><table><row><cell>Profile I P</cell><cell>Ours</cell><cell>[28]</cell><cell>[4]</cell><cell>[27]</cell><cell>Frontal I F</cell></row><row><cell></cell><cell>711.39</cell><cell>827.21</cell><cell>631.56</cell><cell>920.01</cell><cell>0</cell></row><row><cell></cell><cell>594.94</cell><cell>802.04</cell><cell>552.98</cell><cell>886.09</cell><cell>0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by Ministry of Science and Technology, Taiwan, under the project 105-2218-E-007-030.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04086</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose-specific non-linear mappings in feature space towards multiview facial expression recognition</title>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Jampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The extended Cohn-Kanade (CK+): A complete for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local binary patterns for multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="558" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Static and dynamic 3d facial expression recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="683" to="697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangled representation learning GAN for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rotating your face using multitask deep neural network</title>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byungin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic face and gesture recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep neural network-driven feature learning method for multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2528" to="2536" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression recognition from nearinfrared videos</title>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view facial expression recognition based on group sparse reduced-rank regression</title>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
