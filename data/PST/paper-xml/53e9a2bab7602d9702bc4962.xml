<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lamarckian Learning in Multi-agent Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
							<email>gref@aic.nrl.navy.mil</email>
							<affiliation key="aff0">
								<orgName type="department">Navy Center for Applied Research in Artificial Intelligence Code</orgName>
								<orgName type="laboratory">Naval Research Laboratory Washington</orgName>
								<address>
									<postCode>5514, 20375-5000</postCode>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lamarckian Learning in Multi-agent Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DAF2973010F10B2120BDCEB7F150F6DE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Genetic algorithms gain much of their power from mechanisms derived from the field of population genetics. However, it is possible, and in some cases desirable, to augment the standard mechanisms with additional features not available in biological systems. In this paper, we examine the use of Lamarckian learning operators in the SAMUEL architecture. The use of the operators is illustrated on three tasks in multi-agent environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of this work is to explore the application of machine learning techniques to reactive control problems arising in competitive, multi-agent domains. In such domains, traditional AI planning approaches are usually infeasible, because of the complexity of the multi-agent interactions and the inherent uncertainty about the future actions of other agents. On the other hand, genetic algorithms <ref type="bibr" target="#b10">[11]</ref> appear to be a promising approach to developing high performance control strategies. SAMUEL is our platform for exploring the use of genetic algorithms to learn control strategies, expressed as sets of condition/action rules, for sequential decision problems. Several new features have recently been added to SAMUEL that improve significantly both the quality of the rules that are learned and the computational cost of learning those rules. These improvements have been achieved by complementing the Darwinian principles embodied in SAMUEL with a number of mechanisms that are more Lamarckian in flavor. While such mechanisms may not be appropriate for systems intended to accurately model biological adaptive systems, they are appropriate for systems like SAMUEL whose primary motivation is the practical demonstration of genetic learning in interesting domains. This report will emphasize the new mechanisms in SAMUEL, and illustrate their utility in learning some interesting behaviors in multi-agent environments.</p><p>The reactive systems we consider here may be characterized by the following general scenario: The decision making agent interacts with a discrete-time dynamical system in an iterative fashion. At the beginning of each time step, the agent observes a representation of the current state and selects one of a finite set of actions, based on the agent's decision rules. As a result, the dynamical system enters a new state (perhaps based on the actions of other agents in the environment) and returns a (perhaps null) payoff. This cycle repeats indefinitely. The objective is to find a set of decision rules that maximizes the expected total payoff. <ref type="foot" target="#foot_0">1</ref> Several tasks for which reactive systems are appropriate have been investigated in the machine learning literature, including pole balancing <ref type="bibr" target="#b14">[15]</ref>, gas pipeline control <ref type="bibr" target="#b2">[3]</ref>, and the animat problem <ref type="bibr" target="#b17">[18]</ref>. For many interesting problems, including those considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps. SAMUEL is a genetic learning system designed for sequential decision problems. The design of SAMUEL builds on De Jong and Smith's LS-1 approach <ref type="bibr" target="#b16">[17]</ref> as well as our own previous system called RUDI <ref type="bibr" target="#b6">[7]</ref>. Some of the key features of SAMUEL are:</p><p>• A flexible and natural language for expressing rules.</p><p>• Incremental rule-level credit assignment.</p><p>• Competition both at the rule level and at the strategy level. • A genetic algorithm for search the space of strategies.</p><p>• A set of heuristic rule learning operators that are integrated with the genetic operators. Initial studies on an evasive maneuvers task have demonstrated that • SAMUEL can learn general strategies for evasion that are effective against adversaries with a broad range of maneuverability characteristics, and under a variety __________________ of initial conditions (e.g., initial speed and range) <ref type="bibr" target="#b8">[9]</ref>. • SAMUEL can learn high-performance strategies even with with noisy sensors <ref type="bibr" target="#b12">[13]</ref>.</p><p>• SAMUEL can effectively use existing knowledge to speed up learning <ref type="bibr" target="#b15">[16]</ref>. Rather than detail a particular application, this report will try to illustrate how SAMUEL can be applied to learn several different behaviors, with a focus on tasks for an autonomous agent in a hostile environment. New results with three such environments are presented. We believe that these illustrations convey something of the generality of the approach. The remainder of the paper is organized as follows: Section 2 offers a brief overview of the SAMUEL architecture. The next section presents the newest enhancements to SAMUEL, including a more strongly biased conflict resolution algorithm, an extension of the rule language to include symbolic attributes organized into a generalization hierarchy, and heuristic rule learning operators, including SPECIALIZE, GENERALIZE, MERGE, and DELETE. Section 4 presents a test suite of environments that each requires the system to learn a different kind of behavior. This is followed by some empirical studies of SAMUEL's performance on these environments. The last section contains a few final comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW OF SAMUEL</head><p>SAMUEL adopts a number of assumptions that make the system potentially applicable to real-world problems. First, the learning agent's perception facilities are limited to a fixed set of discrete, possibly noisy, sensors. There is also a fixed set of control variables that may be set by the decision making agent. Reflecting our primary interest in rapidly changing and uncertain environments, the agent's decision rules are limited to simple condition/action rules of the form where each condition c i specifies a set of values for one of the sensors and each action a j specifies a setting for one of the control variables. We call a set of such decision rules is called a reactive control strategy.</p><p>One of the key features of SAMUEL is that, unlike many previous genetic learning systems, the knowledge representation consists of symbolic condition-action rules, rather than low-level binary pattern matching primitives. <ref type="foot" target="#foot_1">2</ref> The use of a symbolic language offers several advantages. First, it is easier to transfer the knowledge learned to human operators. Second, it makes it easier to combine genetic algorithms with analytic learning methods that explain the success of the empirically derived rules <ref type="bibr" target="#b4">[5]</ref>. Finally, it makes it easier __________________ to incorporate existing knowledge. A recent study <ref type="bibr" target="#b15">[16]</ref>  For further details on the operation of SAMUEL, see <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LAMARCKIAN ASPECTS OF SAMUEL</head><p>This section describes some of the more recently developed features of SAMUEL, especially those which modify the internal knowledge structures of a strategy as a direct result of the strategy's experience with the task domain. These changes are subsequently passed along to the strategy's offspring, reflecting an evolutionary theory most often connected with the work of Jean Baptiste Lamarck <ref type="bibr" target="#b5">[6]</ref>. Lamarck developed a theory that stressed the inheritance of acquired characteristics, in particular acquired characteristics that are well adapted to the surrounding environment. Of course, Lamarck's theory was superseded by Darwin's emphasis on two-stage adaptation: undirected variation followed by selection.</p><p>Research has generally failed to substantiate any Lamarckian mechanisms in biological systems. Fortunately, in artificial systems, we can easily provide that which nature cannot. <ref type="foot" target="#foot_2">3</ref> The primary Lamarckian feature SAMUEL is the association of strengths with individual rules, and the use of this information for conflict resolution. In addition, many of the rule modification operators are Lamarckian in that they are triggered either directly by a strategy's interaction with the environment or indirectly by the strength of the rules within a strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>__________________</head><p>The next two sections describe these mechanisms in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CREDIT ASSIGNMENT AND CONFLICT RESOLUTION</head><p>Each rule in SAMUEL has an associated strength that estimates the rule's utility for the learning task <ref type="bibr" target="#b6">[7]</ref>. <ref type="foot" target="#foot_3">4</ref>When a rule is inherited by a newly formed strategy, the rule's strength is passed along as well. The primary way that rule strength is used in SAMUEL is in conflict resolution, which runs as follows:</p><p>1. Find the match set, consisting of all rules that most nearly match the current sensor readings.</p><p>2. For each possible action, define the action's bid as the maximum strength of any rule in the match set that specifies that action.</p><p>3. Raise each (non-null) bid to a power specified by a parameter called the bid bias.</p><p>4. Select an action by sampling from the probability distribution defined by the modified bids. The bid bias serves as kind of a Lamarckian control knob. <ref type="foot" target="#foot_4">5</ref> For example, if the bid bias = 0, all non-null bids are considered equal, and the impact of the inherited strength information on conflict resolution is nullified. Any non-zero value for the bid bias results in a Lamarckian system, with a varying degree of greediness. For example, if the bid bias = 1, we get a roulette wheel selection based on the maximum strength associated with each action. <ref type="foot" target="#foot_5">6</ref> If the bid bias &gt; 1, we get a bias toward the higher bids. (Any value of bid bias ≥ 10 is treated as infinite --all bids less than the maximum bid are deleted.) Initial experience indicates that the best performance is obtained with the maximum value for the bid bias. This is the default value used in the experiments described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING OPERATORS</head><p>SAMUEL has one binary recombination operator, CROSS-OVER. CROSSOVER exchanges rules between two strategies. <ref type="foot" target="#foot_6">7</ref> In its default mode, CROSSOVER first clusters rules so that rules that fire in sequence within a highpayoff environment tend to be assigned to the same offspring. The idea is to promote the inheritance of __________________ behavior associated with high payoff. This form of CROSSOVER is Lamarckian since the clustering depends directly on the strategy's past experience with the environment. SAMUEL currently includes six unary operators that modify the rules within a single strategy: MUTATION, CREEP, SPECIALIZE, GENERALIZE, MERGE, and DELETE. Unlike previous versions of the system, SAMUEL has now adopted the policy, common in classifier systems <ref type="bibr" target="#b11">[12]</ref>, that all of these operators (except DELETE, of course) are creative, i.e., any modifications are made on a new copy of the original rule. Once created, a rule survives intact unless it is explicitly deleted or lost when its strategy is not selected for reproduction. We have found that this policy allows a much more aggressive application of rule modification operators with little damage if the changes are maladaptive. A little more detail on the rule language is necessary before we discuss the new rule creation operators. Each sensor and control variable has an attribute type declared by the user. There are four type of attributes: linear, cyclic, structured, pattern. Linear and cyclic attributes take on values from a linearly or cyclicly ordered numeric range, respectively. For example, the sensor time might be a linear attribute with values between 0 and 20. A condition for this attribute might be</p><formula xml:id="formula_0">(time is [5 .. 10])</formula><p>which would be matched if 5 ≤ time ≤ 10. The cyclic attribute direction might take on value between 0 and 360, and a condition for this attribute might be</p><formula xml:id="formula_1">(direction is [270 .. 90])</formula><p>which would be satisfied if (270 ≤ direction ≤ 360 or 0 ≤ direction ≤ 90). A pattern attribute is associated with a fixed-length string over the alphabet { 0, 1, # }, like classifiers in classifier systems <ref type="bibr" target="#b11">[12]</ref>. For example, the pattern attribute visual-field might be defined as a six-bit string, and a condition for this attribute might be</p><formula xml:id="formula_2">(visual-field is 0####1)</formula><p>which would be matched if the first bit of visual-field is 0 and the last bit is 1. A structured attribute can assume values from a hierarchy of symbolic values specified by the user. For example, an attribute called distance might be defined as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Conditions for structured sensors specify a list of values, and the condition matches if the sensor's current value occurs in a subtree labeled by one of the values in the list. A condition for the distance sensor might be</p><formula xml:id="formula_3">(distance is [close, 400])</formula><p>This would match if the sensor distance had the value close, medium-close, very-close, 100, 200, 300, or 400. The user also specifies an ordering for the structured hierarchy that indicates the order relationship among the nodes at each level of the hierarchy. The order may be linear, cyclic, or none. For example, the distance attribute above has a linear order among the leaves, as well as among the nodes at each higher level. On the other hand, a hierarchy having to do with an object's color may have no inherent order among the nodes at a given level of generality. An example of a hierarchy with cyclic order would be one based on compass direction, with leaf values such as due-north, north-east, due-east, north-west. The structured type allows the user to bias the learning operators to reflect the semantics of the sensor. We will now discuss the new rule creation operators, emphasizing their action on the structured type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPECIALIZE</head><p>The SPECIALIZE operator can be applied when a general rule fires in a high payoff episode. (The generality threshold and the payoff threshold for the operator are runtime parameters). The operator creates a new rule whose left-hand-side more closely matches the current sensor values and whose right-hand-side more closely matches the current action value. For numeric conditions (i.e., linear or cyclic), the operator creates a new condition with roughly half the generality of the previous condition by moving each endpoint half way toward the sensor reading. For example, if the original condition is</p><formula xml:id="formula_4">(speed is [100 .. 1500])</formula><p>and the sensor reading is speed = 500 then the new condition would be</p><formula xml:id="formula_5">(speed is [300 .. 1000])</formula><p>For structured conditions, SPECIALIZE replaces each value in the disjunct by each of its children that covers the current sensor reading. For example, if the original condition is</p><formula xml:id="formula_6">(distance is [close, far])</formula><p>and the sensor reading is distance = 300 then the new condition would be</p><formula xml:id="formula_7">(distance is [medium-close, medium-far])</formula><p>If the sensor reading is distance = 400 then the new condition would be</p><formula xml:id="formula_8">(distance is [medium-far])</formula><p>since there is no specialization of close that covers the sensor reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENERALIZE</head><p>The GENERALIZE operator can be applied when a rule fires due to a partial match, during a high payoff episode. (The payoff threshold for the operator is a run-time parameter). A partial match occurs when there is no rule that completely matches all the current sensor readings. The operator creates a new rule whose left-hand-side is generalized enough to match the current sensor values. For numeric conditions, the operator creates a new condition with one of the end points set to the sensor reading. For example, if the original condition is</p><formula xml:id="formula_9">(speed is [700 .. 1500])</formula><p>and the sensor reading is speed = 500 then the new condition would be</p><formula xml:id="formula_10">(speed is [500 .. 1500])</formula><p>For structured conditions, GENERALIZE adds the current sensor value to the disjunct and generalizes up the hierarchy if all the children of a given node are present in the disjunct. For example, if the original condition is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MERGE</head><p>The MERGE operator creates a new rule from two existing high-strength rules that have identical right-hand-sides. The new rule will match any sensor value matched by either of the original rules. The right-hand-side of the new rule is the same as both of the original rules. For example, the result of MERGE of two rules: The MERGE operator, in combination with the DELETE operator below, helps to eliminate overspecialized rules from the strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DELETE</head><p>The DELETE operator is the only mechanism for removing rules from a strategy. A rule may be deleted if it meets one or more of the following criteria: (1) the rule has low activity level (hasn't fired recently); (2) the rule has low strength; or (3) the rule is subsumed by another rule with higher strength. All of these criteria are controlled by run-time parameters. The operators SPECIALIZE and GENERALIZE are clearly Lamarckian in the sense that they are triggered only by successful experiences and they change a strategy to more closely reflect this experience. MERGE and DELETE are indirectly Lamarckian in the sense that they are sensitive to the strength or activity level of the rules, and these statistics directly reflect the rule's past experience with the environment. It should be noted, however, that with the proper selection of run-time parameters, the degree of Lamarckism in all these operators can be reduced or eliminated. Future studies will explored the effects of Lamarckism in SAMUEL in more depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A TEST SUITE OF COMPETITIVE ENVIRONMENTS</head><p>This section describes three rather challenging testing environments we have designed for SAMUEL. In each environment there is one learning agent and another adversary agent. This adversary may behave unpredictably (within certain bounds). The learning agent is the same for all three environments, but the adversaries and the performance tasks differ. This arrangement provides an interesting test of SAMUEL's ability to learn various tasks with an agent with only general purpose sensors and effectors. 8  We first describe the learning agent. The agent has a fixed set of sensors, namely: time (since the beginning of the episode), last-turn (by the agent), bearing (direction to adversary's position), heading (relative direction of adversary's motion), speed (of adversary), and range (to adversary). Each sensor has a fixed granularity that is __________________ 8 These environments can be made available to other researchers who wish to experiment with classifier systems or other learning architectures. We would be happy to participate in comparative studies. fairly large. <ref type="foot" target="#foot_7">9</ref> That is, the mapping from the true world state to observed world state is many-to-one. The sensors are also noisy, and may report incorrect values. The agent has two actions: it can change its own direction and speed. (In two cases, the agent only learns to directly control its turning rate, and its speed is determined by its turning rate.) Finally, the agent's own actions are noisy. That is, the agent may select a 90 degree turn, but in fact, it may turn a little less or a little more than it had indicated. Unlike an agent in a typical AI planning program, our agent generally cannot accurately predict the next state on the basis of the current observed state and the action it selects. These assumptions, which are intended to capture some of the flavor of robotic interactions with the real world, preclude the use of traditional AI planning techniques, and argue in favor of SAMUEL's more reactive approach. We now describe the three test environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVASION</head><p>The first environment is a model of predator-prey situation in which the learning agent plays the role of prey. <ref type="foot" target="#foot_8">10</ref>The adversary, or predator, can track the motion of the prey and steer toward the prey's anticipated position. In this environment, the agent learns only its turning rate; its speed is determined by the turning rate. The process is divided into episodes that begin with the predator approaching the prey from a randomly chosen direction. The predator initially travels at a far greater speed but is less maneuverable than the prey (i.e., the predator has a greater turning radius than the prey) and gradually loses energy (i.e., speed) as it maneuvers. The episode ends when either the predator captures the prey or the predator's energy drops below a threshold and it gives up. This requires between 2 and 20 decision steps, depending on how many turns the predator performs while tracking the prey. At the end of each episode, the critic provides full payoff if the agent evades the adversary, and partial payoff otherwise, proportional to the amount of time before the agent's capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRACKING</head><p>The second environment is a slightly different predatorprey model in which the learning agent plays the predator. In this model, the goal is to stalk the prey at a distance. The adversary (the prey) follows a random course and speed. The tracker must learn to control both its speed and its direction. It is assumed that the tracker has sensors that operate at a greater distance than the prey's sensors. The object is to keep the prey within range of __________________ the tracker's sensors, without being detected by the prey. If the tracker enters the range of the prey's sensors, it will be detected and captured with a probability that depends on the tracker's distance and speed. At the end of each episode, the critic provides full payoff if the tracker keeps within a certain average range of the prey, proportionately less payoff if the average range exceeds the threshold, and 0 payoff if the tracker is captured by the prey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DOGFIGHT</head><p>The final environment pits the learning agent against a rule-based adversary with identical sensor and action capabilities. Like the learner in the Evasion environment, each agent controls its own turning rate, but its speed is a deterministic function of its turning rate. Each agent has a weapon that allows it to destroy the opponent if the agent is heading toward the opponent and is within the weapon's range. The object, therefore, is both to evade the opponent's fire while getting in position to make an attack. The learner receives full payoff for an episode in which the adversary is destroyed, partial payoff for a draw, and 0 payoff if the learner is destroyed. The adversary operates according to a fixed set of rules, and does not learn during these experiments. 11   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE OF SAMUEL ON TEST ENVIRONMENTS</head><p>This section presents some initial empirical studies of the performance of SAMUEL on the test environments. At intervals of five generations, a single strategy is extracted by running extended tests on the top 20% of the current population. The performance of the extracted strategy is shown in the graph. All graphs represent the mean performance over 10 independent runs of the system, each run using a different seed for the random number generator. The error bars indicate one standard deviation across the runs. All experiments used a common set of parameters. 12  In Figure <ref type="figure" target="#fig_3">2</ref> the solid line shows the performance of the current version of SAMUEL on the Evasion environment.</p><p>The initial strategy (a random walk) evades the adversary about 31% of the time. After 50 generations, the final strategy evades the adversary about 82% of the time. Due to differences in the rule representation language, a direct comparison with the previous version of SAMUEL could not be performed. However, a good approximation of the previous behavior of SAMUEL can be obtained by lowering the bid bias to 1, disabling the GENERALIZE, MERGE, and CREEP operators, and restricting SPECIALIZE __________________ 11 We plan to address adaptive adversaries in future experiments. 12 Population size = 100; crossover rate = 0.6; maximum number of rules per strategy = 64; noisy sensors and actions for the learning agent, 50 generations per run. After each evaluation, the remaining space in each strategy was allocated equally to the rule creation operators: MUTATION, CREEP, SPECIALIZE, GENERALIZE, and MERGE. Investigation of optimal parameter settings awaits future studies. The mechanisms in the current version appear to yield significantly better performance, particularly in the early stages of learning. Note again that this environment is much more challenging than our earlier studies of the EM problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Figure <ref type="figure" target="#fig_5">3</ref> shows a typical learning curve for the Tracking environment.  This environment is more difficult than the Evasion environment in the sense that a random walk has very little chance of producing acceptable behavior. An initial plausible strategy, shown in Figure <ref type="figure">4</ref>, provides an overgeneral but plausible initial starting point. Since the probability of detection depends in part on the tracker's own speed, it can easily be surprised and trapped by the adversary. Future studies will shed more light on the ultimate level of performance that can be obtained in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUCCESS</head><p>Figure <ref type="figure" target="#fig_7">5</ref> shows a typical learning curve for the Dogfight environment.  The initial strategy (a random walk) defeats the adversary approximately 40% of the time. After 50 generations, the final strategy evades the adversary about 83% of the time. Again, it is not currently known whether there exists a completely successful strategy for this environment. SAMUEL appears to perform well in these initial studies on the new environments. Although the current version represents a significant improvement in learning speed over previous versions, some limitations of the system remain. There seems to be a window of environmental complexity in which SAMUEL performs best. If the environment is too simple, other methods such as traditional control theory or explanation-based learning may be much more efficient ways to develop high performance control rules. If the environment is too complex, SAMUEL flounders badly. As an example, the Tracking environment requires some initial knowledge in order to provide a minimum level of successful experience upon which SAMUEL can build better strategies. The user should not expect SAMUEL to develop strategies for a difficult environment on its own. Nonetheless, we believe that SAMUEL can be part of a methodology that combines knowledge engineering and machine learning in a way that significantly reduces the overall development effort for systems that exhibit expert performance in complex environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUCCESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY</head><p>This paper has presented a number of recent enhancements to SAMUEL, emphasizing the enhanced rule representation language and learning operators that take advantage of this new representation. It is expected that the inclusion of these operators will present new opportunities to merge to power of genetic algorithms with traditional machine learning approaches. The performance of the system has been illustrated on three competitive environments. We encourage others in the GA community to explore learning in environments of at least this complexity. Complex, uncertain environments offer a promising niche for genetic learning approaches, a niche that has not been addressed adequately by traditional learning methods. Finally, SAMUEL represents an integration of the two major genetic approaches to machine learning, the Michigan approach (i.e., Holland's classifier systems <ref type="bibr" target="#b11">[12]</ref>) and the Pittsburgh approach (i.e., De Jong and Smith's LS-1 approach <ref type="bibr" target="#b16">[17]</ref>). It is interesting to note that the more Lamarckian features of SAMUEL -using rule strengths for conflict resolution, and the triggered rule creation operators -were inspired by mechanisms in classifier systems. This suggests a fascinating question: Is John Holland a Lamarckian?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Structured Attribute</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>distance is [very-close, very-far]) and the sensor reading is distance = 300 then the new condition would be (distance is [close, very-far])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SAMUEL on Evasion Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SAMUEL on Tracking Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SAMUEL on Dogfight Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>range is [close low medium]) ) then (and (turn is [straight]) (speed is [low, medium]) )</head><label></label><figDesc>The initial strategy successfully tracks the adversary approximately 22% of the time. After 50 generations, the final strategy evades the adversary over 72% of the time. It is not currently known whether there exists a completely successful strategy for this environment. Since the adver-</figDesc><table><row><cell>if</cell><cell>(and (bearing is [directly-ahead])</cell></row><row><cell></cell><cell>(range is [high]) )</cell></row><row><cell cols="2">then (and (turn is [straight])</cell></row><row><cell></cell><cell>(speed is [medium high]) )</cell></row><row><cell>if</cell><cell>(and (bearing is [hard-right, behind-right])</cell></row><row><cell></cell><cell>(range is [high]) )</cell></row><row><cell cols="2">then (and (turn is [soft-right])</cell></row><row><cell></cell><cell>(speed is [medium, high]) )</cell></row><row><cell>if</cell><cell>(and (bearing is [directly-behind])</cell></row><row><cell></cell><cell>(range is [high]) )</cell></row><row><cell cols="2">then (and (turn is [hard-right])</cell></row><row><cell></cell><cell>(speed is [medium, high]) )</cell></row><row><cell>if</cell><cell>(and (bearing is [hard-left behind-left)</cell></row><row><cell></cell><cell>(range is [high]) )</cell></row><row><cell cols="2">then (and (turn is [soft-left])</cell></row><row><cell></cell><cell>(speed is [medium, high]) )</cell></row><row><cell>if</cell><cell>(and (</cell></row></table><note><p>Figure 4: Initial Strategy for Tracking Environment sary follows a random route, it can, and often does, turn directly toward the tracker and approach at high speed.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Barto, Sutton and Watkins<ref type="bibr" target="#b0">[1]</ref> give a good discussion of broad applicability of this general model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Booker [2]  shows how more expressive encodings could be implemented in a classifier system, but experimental results are not yet available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>It might be mentioned that human cultural evolution is highly Lamarckian, and subsequently much more rapid than biological evolution<ref type="bibr" target="#b5">[6]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>A rule's strength increases as a function of the mean of the expected payoff and decreases with the variance of the expected payoff, so that high strength indicates both high utility and high confidence in the rule<ref type="bibr" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The bid bias was inspired by a similar mechanism in Riolo's classifier system CFS-C<ref type="bibr" target="#b13">[14]</ref>, and is similar in effect to the notion of using bidding noise based on a classifier's variance<ref type="bibr" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>In all previously reported results with SAMUEL, the bid bias was set to 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We prefer to rely on explicit mutation operators, rather than overload CROSSOVER with the additional task of introducing new rules by crossing with rule boundaries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>In the experiments described here, all sensors are structured attributes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>This environment differs from the EM problem in previous papers<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. In this paper, we introduce noise into both the agent's sensors and actions, and we vary both the initial state and the maneuverability characteristics of the adversary. As a result, the task is more realistic and more challenging.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I want to acknowledge the contributions toward the development of SAMUEL by the members of the Machine Learning Group at NRL, especially Alan Schultz, Connie Ramsey, Diana Gordon, Helen Cobb, and Ken De Jong. This work is supported in part by ONR under Work Request N00014-91-WX24011.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning and sequential decision making</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">COINS Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The classifier system concept description language</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Booker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 Foundations of Genetic Algorithms Workshop</title>
		<meeting>the 1990 Foundations of Genetic Algorithms Workshop<address><addrLine>Bloomington, IN</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computer-aided gas pipeline operation using genetic algorithms and machine learning, Doctoral dissertation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department Civil Engineering, University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probability matching, the magnitude of reinforcement, and classifier system bidding</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>No. 88002</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Tuscaloosa</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alabama, Department of Engineering Mechanics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">TCGA Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explanations of empirically derived reactive plans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning</title>
		<meeting>the Seventh International Conference on Machine Learning<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="198" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gould</surname></persName>
		</author>
		<title level="m">The Panda&apos;s Thumb</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Norton &amp; Co</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Credit assignment in rule discovery system based on genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A system for learning control strategies with genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Genetic Algorithms</title>
		<meeting>the Third International Conference on Genetic Algorithms<address><addrLine>Fairfax, VA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning sequential decision rules using simulation models and competition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="381" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">User&apos;s guide for SAMUEL</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Cobb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NRL Report</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Naval Research Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: An artificial intelligence approach</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Michalski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simulation-assisted learning by competition: Effects of noise differences between training model and target environment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning</title>
		<meeting>the Seventh International Conference on Machine Learning<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="211" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bucket brigade performance II: Default hierarchies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Riolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Genetic Algorithms</title>
		<meeting>the Second International Conference on Genetic Algorithms<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Assoc</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training and tracking in robotics</title>
		<author>
			<persName><forename type="first">O</forename><surname>Selfridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Intelligence</title>
		<meeting>the Ninth International Conference on Artificial Intelligence<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985-08">1985. August, 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving tactical plans with genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Conference on Tools for AI 90</title>
		<meeting>eeding of IEEE Conference on Tools for AI 90<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="328" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A learning system based on genetic adaptive algorithms, Doctoral dissertation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifier systems and the animat problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="199" to="228" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
