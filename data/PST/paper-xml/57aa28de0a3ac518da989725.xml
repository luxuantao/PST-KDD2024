<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Visual-Semantic Hashing for Cross-Modal Retrieval *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>caoyue10@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<email>mingsheng@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">Tsinghua National Laboratory (TNList)</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">KDD &apos;16</orgName>
								<address>
									<addrLine>August 13-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Visual-Semantic Hashing for Cross-Modal Retrieval *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC1DAFE3C562F0A57F87AE870976721B</idno>
					<idno type="DOI">10.1145/2939672.2939812</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep hashing</term>
					<term>cross-modal retrieval</term>
					<term>multimodal embedding Hash Function Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visualsemantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While multimedia big data of massive volumes and high dimensions are pervasive in search engines and social net-works, it has attracted increasing attention to approximate nearest neighbors search across different media modalities that brings both computation efficiency and search quality. Since correspondence data from different modalities may endow semantic correlations, it is imperative to support crossmodal retrieval that returns relevant results of one modality in response to query of another modality, e.g. retrieval of images with text query. An advantageous solution to crossmodal retrieval is hashing methods, which compress highdimensional data into compact binary codes with similar binary codes for similar objects <ref type="bibr" target="#b36">[36]</ref>. This paper focuses on cross-modal hashing that builds isomorphic hash codes for efficient cross-media retrieval. To date, effective and efficient cross-modal hashing remains a challenge, due to the heterogeneity across different modalities <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b38">38]</ref>, and the semantic gap between low-level features and high-level semantics <ref type="bibr" target="#b32">[32]</ref>.</p><p>Many cross-modal hashing methods have been proposed to exploit shared structures across different modalities in the process of hash function learning and compress cross-modal data in an isomorphic Hamming space <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29]</ref>. These cross-modal hashing methods based on shallow architectures cannot exploit heterogeneous correlation structure effectively to bridge different modalities. Several recent deep models for multimodal embedding <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b1">1]</ref> show that deep learning can capture heterogeneous cross-modal correlations more effectively than shallow learning methods. While these deep models have been successfully applied to image captioning and retrieval, they cannot generate compact hash codes for efficient crossmodal retrieval. Meanwhile, latest deep hashing methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b5">5]</ref> yielded state of art results on many datasets, but these methods are limited to single-modal retrieval.</p><p>In this work, we strive for the goal of efficient cross-modal retrieval of images in response to natural sentence queries or vice versa, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. This new hashing scenario, different from previous work that uses unordered keyword queries, is more desirable for practical applications, since it is usually easier for users to describe the images by free-style text sentences instead of a couple of keywords. The primary challenge towards this goal is in the design of a model that is rich enough to simultaneously reason about contents of images and their representation in the domain of natural language. Additionally, the model should be able to generate compact hash codes that capture rich features of images and sentences as well as the cross-modal correlation structures to enable efficient cross-modal retrieval. To our knowledge, this work is the first end-to-end learning approach to cross- modal hashing that enables efficient cross-modal retrieval of images in response to sentence queries and vice versa.</p><p>This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a seamless combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable the learning of similarity-preserving and high-quality hash codes. Comprehensive empirical results show that our DVSH model yields state of the art results in cross-modal retrieval experiments on popular image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>This work is related to cross-modal hashing, which has been an increasingly popular research topic in machine learning, data mining, and multimedia retrieval communities <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b25">25]</ref>. We refer the readers to <ref type="bibr" target="#b36">[36]</ref> for a comprehensive survey.</p><p>Prior cross-modal hashing methods can be roughly organized into unsupervised methods and supervised methods. Unsupervised hashing methods learn hash functions that can encode input data points to binary codes only using the unlabeled training data. Typical learning criteria include reconstruction error minimization <ref type="bibr">[8,</ref><ref type="bibr" target="#b37">37]</ref>, similarity preservation as graph-based hashing <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b33">33]</ref>, and quantization error minimization as correlation quantization <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b29">29]</ref>. Supervised hashing methods explore the supervised information (e.g., relative similarity or relevance feedback) to learn compact hash coding. Typical learning methods include metric learning <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b25">25]</ref>, neural network <ref type="bibr" target="#b30">[30]</ref>, and correlation analysis <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39]</ref>. As supervised hashing methods can explore semantic information to enhance the cross-modal correlation and reduce the semantic gap <ref type="bibr" target="#b32">[32]</ref>, they can achieve superior accuracy than unsupervised methods for cross-modal retrieval.</p><p>Most of previous cross-modal hashing methods based on shallow architectures cannot effectively exploit the heterogeneous correlation structure across different modalities. Latest deep models for multimodal embedding <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15]</ref> have shown that deep learning can capture heterogeneous cross-modal correlations more effectively for image captioning and cross-modal reasoning, but it remains unclear how to extend these deep models to cross-modal hashing. Recent deep hashing methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b46">46]</ref> have given state of the art results on many datasets, but these methods can only be applied to single-modal retrieval. To our knowledge, this work is the first end-to-end learning approach to cross-modal deep hashing that enables efficient cross-modal retrieval of images in response to text-sentences queries and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRELIMINARY ON DEEP NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutional Neural Network (CNN)</head><p>To learn deep representation of visual data, we start with AlexNet <ref type="bibr" target="#b21">[21]</ref>, the deep convolutional network (CNN) architecture which won the ImageNet ILSVRC 2012 challenge. AlexNet comprises five convolutional layers (conv1-conv5) and three fully connected layers (f c6-f c8), as in Figure <ref type="figure" target="#fig_2">3</ref>. Each fully-connected layer learns a nonlinear mapping h = a W h -1 + b , where h is the -th layer activation of image x, W and b are the weight and bias parameters of the -th layer, and a is the activation function, taken as rectifier linear units (ReLU) a (x) = max(0, x) for layers conv1-f c7. Different from fully connected layers, each convolutional layer is a three-dimensional array of size h×w×d, where h and w are spatial dimensions, and d is the feature or channel dimension. The first layer is input image, with pixel size h × w and d color channels. Locations in higher convolutional layers correspond to the locations in the image they are connected to, which are called the receptive fields.</p><p>CNNs are built on translation invariance <ref type="bibr" target="#b6">[6]</ref>. Their basic components (convolution, pooling, and activation functions) operate on local input regions, and depend only on relative spatial coordinates. Writing xij for the image vector at location (i, j) in a particular layer, and hij for the following layer, these functions in convolutional layers compute hij by</p><formula xml:id="formula_0">hij = f ks {x si+δi,sj+δj } 0≤δi,δj≤k ,<label>(1)</label></formula><p>where k is called the kernel size, s is the stride or subsampling factor, and f ks determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for max pooling, or an elementwise nonlinearity for an activation function, and so on for other types of layers. This functional form is maintained under composition, with kernel size and stride obeying the following transformation rule</p><formula xml:id="formula_1">f ks • g k s = (f • g) k +(k-1)s ,ss .<label>(2)</label></formula><p>While a general deep network computes a general nonlinear function, a network with only layers of this form computes a nonlinear filter, which we call a deep filter or feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long Short-Term Memory (LSTM)</head><p>To learn deep representation of sequential data, we adopt Long Short-Term Memory (LSTM) recurrent neural network <ref type="bibr" target="#b14">[14]</ref>. Though recurrent neural networks (RNNs) have proven successful on tasks such as speech recognition and text generation, it can be difficult to train them to learn long-term dynamics, likely due in part to the vanishing and exploding gradients problem that can result from propagating the gradients down through the many layers of the recurrent network, each corresponding to a particular timestep. LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information.  In this paper, we adopt the LSTM unit as described in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b6">6]</ref>, which is a slight simplification of the one described in <ref type="bibr" target="#b11">[11]</ref>, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Let σ(x) = 1 1+exp -x be the sigmoid function that maps real-valued inputs to [0, 1], and let φ(x) = e x -e -x e x +e -x = 2σ(2x) -1 be the hyperbolic tangent (tanh) function, similarly mapping its inputs to [-1, 1], the LSTM updates for timestep t given inputs xt, ht-1 and ct-1:</p><formula xml:id="formula_2">it = σ (Wxixt + W hi ht-1 + bi) ft = σ (W xf xt + W hf ht-1 + b f ) ot = σ (Wxoxt + W ho ht-1 + bo) gt = φ (Wxcxt + W hc ht-1 + bc) ct = ft ct-1 + it gt ht = ot φ (ct) ,<label>(3)</label></formula><p>where it, ft, ot, gt, ct, ht are respectively input gate, forget gate, output gate, input modulation gate, memory cell and hidden unit for timestep t. The weight matrix has the obvious meaning that W xf is the input-forget gate matrix and W hi is the hidden-input gate matrix, etc. Because the activation function of ft and it is sigmoid function, their values are in [0, 1], and they are learned to control how much of the memory cell to forget its previous memory or consider their current inputs. Similarly, the output gate ot learns that how much the memory cell transfers to hidden unit. Considering the memory cell, which is a summation of two parts: the previous memory cell ct-1 which is modulated by the forget gate ft, and gt which is modulated by the input gate it. These additional gates enable LSTM to learn more complex and long-term temporal dynamics which cannot gain from RNN. Additional depth can be added to LSTMs by stacking them on top of each other, using the hidden state of the LSTM in layer ( -1) as the input to the LSTM in layer .</p><p>The advantages of LSTMs for modeling sequential data in vision and natural language problems are: (1) when integrated with current vision systems, LSTMs are straightforward to fine-tune end-to-end; (2) LSTMs are not confined to fixed length inputs or outputs, which allow simple modeling for sequential data of varying lengths, such as text or video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DEEP VISUAL-SEMANTIC HASHING</head><p>In cross-modal retrieval systems, the database consists of objects from one modality and the query consists of objects from another modality. In this paper, we study a novel cross-modal hashing scheme, where we are given image-sentence pairs each corresponding to an image and a text sentence that correctly describes the image. We uncover the correlation structure between images and texts by learning from a training set of N bimodal objects {oi = (xi, yi)} N i=1 , where xi ∈ R Dx denotes the Dx-dimensional feature vector of the image modality, and yi =&lt;yi1, yi2, ..., yiT &gt; ∈ R Dy ×T denotes sentence i consisting of word sequences, where yit ∈ R Dy is a one-hot vector that denotes a word of time t in sentence i (the nonzero element of yit denotes the index of the word in the vocabulary of size Dy). Some pairs of the bimodal objects are associated with similarity labels sij, where sij = 1 implies oi and oj are similar and sij = -1 indicates oi and oj are dissimilar. In supervised cross-modal hashing, S = {sij} is constructed from the semantic labels of data points or the relevance feedback from click-through data.</p><p>We propose a novel Deep Visual-Semantic Hashing (DVSH) approach to cross-modal retrieval, which learns end-to-end (1) a bimodal fusion function f (x, y) : R Dx , R Dy ×T → {-1, 1} K , which maps images and texts into a K-dimensional joint Hamming embedding space H so that the embeddings of each image-sentence pair are tightly fused to bridge different modalities whilst the similarity information conveyed in given bimodal object pairs S is preserved; and (2) two modality-specific hashing functions fx (x) : R Dx → {-1, 1} K and fy (y) : R Dy ×T → {-1, 1} K , which encode each image x and sentence y from database and query to compact binary hash codes u ∈ {-1, 1} K and v ∈ {-1, 1} K in the joint embedding space H to enable efficient cross-modal retrieval.</p><p>The proposed cross-modal deep hashing approach (DVSH) in Figure <ref type="figure" target="#fig_2">3</ref> is an end-to-end deep architecture for crossmodal hashing, which comprises both convolutional neural network (AlexNet) for learning image representations and recurrent neural network (LSTM) for learning text representations. The architecture accepts pairwise input (oi, oj, sij) and processes them in an end-to-end deep representation learning and hash coding pipeline: (1) a deep visual-semantic fusion network for learning isomorphic hash codes in the joint embedding space such that the representations of each image-sentence pair is tightly fused and correlated; (2) an image hashing network and a sentence hashing network for learning nonlinear modality-specific hash functions that encode each unseen image and sentence to compact hash codes in the joint embedding space; (3) a new cosine max-margin loss to preserve the pairwise similarity information and enhance the robustness to outliers; (4) a novel bitwise maxmargin loss to control the quality of the binary hash codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual-Semantic Fusion Network</head><p>The challenge of cross-modal retrieval arises in that crossmodal data (images and texts) have significantly different statistical properties (heterogeneous), which makes it very difficult to capture the correlation across modalities based on hand-crafted features. Recently, it has been witnessed that deep learning methods <ref type="bibr" target="#b3">[3]</ref>, such as deep convolutional networks (CNNs) <ref type="bibr" target="#b21">[21]</ref> and deep recurrent networks (RNNs) <ref type="bibr" target="#b35">[35]</ref>, have made performance breakthroughs on many real-world perception problems. Deep architectures are very powerful for extracting the multimodal embedding shared by different modalities since they can extract nonlinear feature representations to bridge different modalities effectively <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b18">18]</ref>. We thus leverage deep networks for cross-modal joint embedding by designing a deep visual-semantic fusion network as illustrated in the left part of Figure <ref type="figure" target="#fig_2">3</ref>, which maps the deep feature representations of images and texts into the shared visual-semantic embedding space such that the correspondence relations conveyed in the image-sentence pair can be maximized whilst the pairwise similarity information conveyed in the similarity labels can be preserved. The proposed deep visual-semantic fusion network works by passing each visual input xi (an image in our case) through the deep convolutional neural network (CNN) to produce a fixed-length vector representation h x i . Note that, we replace the softmax classifier in the f c8 layer of the original AlexNet <ref type="bibr" target="#b21">[21]</ref> with a feature map, which maps the image features from the f c7 layer to new features of K-dimension. We adopt the LSTM as our sequence model, which maps an input yit of each sequence (a sentence in our case) at timestep t and a hidden state h y i(t-1) of previous timestep (t-1) to an output z y it and updates hidden state h y it . Therefore, inference must be run sequentially (i.e. from top to bottom in Figure <ref type="figure" target="#fig_2">3</ref>), by computing the activations in order using Equation <ref type="bibr" target="#b3">(3)</ref>, that is, updating the t-th state based on the (t -1)-th state.</p><p>To integrate CNN and LSTM into a unified deep visualsemantic embedding model, the computed feature-space representation h x i of the visual input xi is fused into the second layer of the LSTM model over each state, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, the activation hi of the fusion layer (the LSTMs with green color) for the t-th state (a word) in the sequence (text sentence) can be calculated as follows:</p><formula xml:id="formula_3">hit = f (h x i + h y it ) ,<label>(4)</label></formula><p>where f (•) denotes the updates made to the timestep t of the second-layer LSTM by substituting xt h x i +h y it into Equation (3). Note that, to reduce the gap between the activation hit of the fusion layer and the final binary hash codes ui and vi, we first squash the activations hit to [-1, 1] using the hyperbolic tangent (tanh) activation function φ(x) = tanh(x) in Equation ( <ref type="formula" target="#formula_2">3</ref>). This fusion operation is very important to embody the multimodal visual-semantic embedding space.</p><p>The aforementioned timestep-wise fusion tights the visual and textual embeddings h x i and h y it to a unified embedding. However, each timestep t produces a joint embedding hit, while we would expect that each image-text pair produces only one fusion code to make cross-modal retrieval efficient. To this end, we are motivated by the technique of mean embeddings of distributions <ref type="bibr" target="#b12">[12]</ref> and generate pair-level fusion code hi for each image-sentence pair by weighted averaging:</p><formula xml:id="formula_4">hi = T t=1 πithit T t=1 πit = T t=1 πitf (h x i + h y it ) T t=1 πit ,<label>(5)</label></formula><p>where πit ∈ {1, 0} is the indicator variable, πit = 1 if word t is present in timestep t, and πit = 0 otherwise. We handle these cases because the text sentences are of variable-length and some sentences are shorter than the number T of states in the LSTMs. It is important to note that, the derived joint visual-semantic embedding hi not only captures the spatial dependencies over images and temporal dynamics over sentences using CNN and LSTM respectively, but also captures the cross-modal relationship in a multimodal Hamming embedding space. To achieve an optimal joint embedding space for binary coding, the joint embeddings should be made to preserve the pairwise similarity information in training data S and to be separated well by bitwise hyperplane h ik = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Cosine Max-Margin Loss</head><p>In order to make the learned joint visual-semantic embeddings maximally preserve the similarity information across different modalities, we propose the following criterion: for each pair of objects (oi, oj, sij), if sij = 1, indicating that oi and oj are similar, then their hash codes ui and vj must be similar across different modalities (image and sentence), which is equivalent to requiring that their joint visual-semantic embeddings hi and hj should be similar. Correspondingly, if sij = -1, indicating that oi and oj are dissimilar, then their joint visual-semantic embeddings hi and hj should be dissimilar. We use the cosine similarity cos(hi, hj) =</p><formula xml:id="formula_5">h i •h j h i h j</formula><p>for measuring the closeness between hi and hj, where hi • hj is the inner-product of hi and hj, and • denotes the Euclidean norm of a vector. For similarity-preserving learning, we propose to minimize the following cosine max-margin loss</p><formula xml:id="formula_6">L = s ij ∈S max 0, µc -sij hi • hj hi hj ,<label>(6)</label></formula><p>where µc &gt; 0 is the margin parameter, which is fixed to µc = 0.5. This objective encourages similar image-sentences pairs to have a higher cosine similarity than dissimilar pairs, by a margin. Similar to the support vector machines (SVMs), the max-margin loss enhances the robustness to outliers. The cosine max-margin loss is particularly powerful for crossmodal correlation analysis, since the vector lengths are very diverse in different modalities and may make many distance metrics (e.g. Euclidean distance) as well as loss functions (e.g. squared loss) misspecified. To date this problem has not been studied in cross-modal deep hashing methods <ref type="bibr" target="#b36">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Bitwise Max-Margin Loss</head><p>For each image-sentence pair oi = (xi, yi), to reduce the gap between its joint embedding hi and its modality-specific binary codes ui and vi, we require that the joint embedding hi to be close to its signed code sgn(hi) ∈ {-1, 1} K , which is equivalent to minimizing |hi|-1 2 . However, as a common knowledge, such squared loss is not robust to outliers. Thus we propose to minimize a novel bitwise max-margin loss as</p><formula xml:id="formula_7">Q = N i=1 K k=1 max (0, µ b -|h ik |),<label>(7)</label></formula><p>where µ b &gt; 0 is the bitwise margin parameter, which is fixed to µ b = 0.5. This objective encourages the joint embedding to separate apart from the hyperplane h ik = 0 corresponding to the k-th bit, by a margin, hence we call it bitwise maxmargin. Note that, minimizing the bitwise max-margin loss will lead to lower quantization error when binarizing the continuous embeddings ui ∈ R K and vi ∈ R K to binary hash codes, which allows us to learn high-fidelity binary codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modality-Specific Hashing Network</head><p>The proposed deep visual-semantic fusion network will produce isomorphic joint embeddings that are sharable as the bridge to correlate different modalities, which effectively mitigates the cross-modal heterogeneity by deep representations of images and texts and the deep fusion between them. However, two major problems remain: (1) the fusion network cannot extend the embedding model to out-of-sample images and texts; (2) the fusion network require bimodal objects (both image and text modalities should be available) to predict the joint embeddings. In other words, the fusion network cannot be directly applied to cross-modal retrieval, where only one modality is available for the database or the query. Most importantly, it does not provide a mechanism to map each unimodal input to the joint embedding space. This thus motivates us to craft two more hashing networks for directly learning the modality-specific hashing functions.</p><p>The key difference between the hashing network and the fusion network is: in the fusion network, we map each input to its modality-specific representation and then unify all modalities by elementwise summation in Equation ( <ref type="formula" target="#formula_3">4</ref>); in the hashing network, however, we directly map each input to the joint embedding space learned by the fusion network. Hence the hashing network can address the above two problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Image Hashing Network</head><p>The image hashing network is crafted to learn the hashing function for the image modality. It is similar to the CNN module of the fusion network: we directly copy the conv1f c7 layers from AlexNet <ref type="bibr" target="#b21">[21]</ref>, and replace the softmax classifier in f c8 layer with a hash function that transforms the feature representation of input image xi to hash code ui. To guarantee that the hash code ui produced by the hashing network lie in the joint embedding space, we require the hash code ui and the joint embedding hi corresponding to the same training image xi to be close with the squared loss:</p><formula xml:id="formula_8">L x = 1 2N N i=1     ui - T t=1 πithit T t=1 πit     2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Sentence Hashing Network</head><p>The sentence hashing network is crafted to learn the hashing function for the text modality. It is similar to the LSTM module of the fusion network, but by removing the visual input branch. We replace the softmax classifier in the output layer of the LSTM with a hash function that transforms the feature representation of input sentence yi to hash code vi. Again, to guarantee that the hash code vi lie in the joint embedding space, we require the hash code vi and the joint embedding hi corresponding to the same training sentence yi to be similar in each timestep t under the squared loss:</p><formula xml:id="formula_9">L y = 1 2N N i=1 T t=1 πit(vit -hit) 2 T t=1 πit .<label>(9)</label></formula><p>Note that for both hashing networks, bimodal objects are only required in the training phase. After the hash functions are learned, we can directly encode any out-of-sample input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep Visual-Semantic Hashing</head><p>In this paper, we enable joint representation learning and hash coding in an end-to-end deep architecture. Specifically, (1) we guarantee robust similarity-preserving representation learning by minimizing the cosine max-margin loss (6); (2) we guarantee the high-quality of compact binary hash codes by minimizing the bitwise max-margin loss (7); (3) we enable effective and efficient out-of-sample code generation by minimizing the squared losses ( <ref type="formula" target="#formula_8">8</ref>)- <ref type="bibr" target="#b9">(9)</ref>. Integrating these loss functions in a joint optimization problem that is taken over the deep visual-semantic hashing (DVSH) network, it yields min</p><formula xml:id="formula_10">Θ O = L + λQ + β (L x + L y ) ,<label>(10)</label></formula><p>where Θ W * , b * * ∈{x,y,u,v} denotes the set of network parameters, λ and β are the penalty parameters for trading off the relative importance of the bitwise max-margin loss and modality-specific squared loss. Through joint optimization <ref type="bibr" target="#b10">(10)</ref> over the deep visual-semantic hashing network, we can jointly learn an isomorphic joint embedding space that effectively bridges the image and text modalities, and two modality-specific hashing functions that respectively map the image and text inputs to compact binary codes in the joint embedding space, which enables effective and efficient cross-modal retrieval. With the trained fusion network and hashing networks, we can obtain K-bit binary hash codes by simple sigh thresholding sgn(u) and sgn(v) for each modality, where sgn(•) is the element-wise sign function that for i = 1, . . . , K, sgn(zi) = 1 if zi &gt; 0, otherwise sgn(zi) = -1. It is worth noting that, since we have minimized the bitwise max-margin loss in Equation <ref type="bibr" target="#b10">(10)</ref> during training, this final binarization step will incur relatively small loss of retrieval quality, which will also be validated in the empirical study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithms and Training Details</head><p>The CNN module is pre-trained on the ImageNet classification task <ref type="bibr" target="#b21">[21]</ref>. The LSTM module is pre-trained on the MS COCO dataset <ref type="bibr" target="#b24">[24]</ref> using the neural language model <ref type="bibr" target="#b35">[35]</ref>. These two components are fined-tuned during the training of the proposed DVSH model. We jointly train the new layers (colored modules in Figure <ref type="figure" target="#fig_2">3</ref>) of visual-semantic fusion network and modality-specific hashing network with mini-batch stochastic gradient descent (SGD) method. And the hyperparameters of the model are selected by cross-validation.</p><p>We derive the learning algorithms for the DVSH model in Equation <ref type="bibr" target="#b10">(10)</ref>, and show rigorously that both cosine maxmargin loss and bitwise max-margin quantization loss can be optimized efficiently through the standard back-propagation (BP). For notation brevity, we define the point-wise loss as</p><formula xml:id="formula_11">Oi j:s ij ∈S Lij + λ K k=1 Q ik + β (L x i + L y i ) .<label>(11)</label></formula><p>To improve the convergence stableness, we let the loss of hashing network make no effect to the updates of the fusion network during the training of DVSH. We derive the gradient of point-wise loss Oi w.r.t. W x,k , the parameter of the k-th unit of -th layer of the CNN part of the fusion network:</p><formula xml:id="formula_12">∂Oi ∂W x,k = j:s ij ∈S ∂Lij ∂W x,k + λ ∂Q ik ∂W x,k =   j:s ij ∈S ∂Lij ∂ ĥ ik + λ ∂Q ik ∂ ĥ ik   ∂ ĥ ik ∂W x,k = δ x,ik h -1 i ,<label>(12)</label></formula><p>where ĥ</p><formula xml:id="formula_13">i = W x h -1 i + b x is the output of the -th layer before activation a (•), δ x,ik j:s ij ∈S ∂L ij ∂ ĥ ik + λ ∂Q ik</formula><p>∂ ĥ ik is the point-wise residual term that measures how much the k-th unit in the -th layer is responsible for the error of point xi in the network output. For an output unit k, we can measure the difference between the network's activation and the true target value, and use that to define the residual δ l</p><p>x,ik as</p><formula xml:id="formula_14">δ l x,ik = j =i:s ij ∈S I µc -sij h l i • h l j h l i h l j &gt; 0 • -sij h l jk h l i h l j - h l ik h l i , h l j h l i 3 h l j ȧl ( ĥl ik ) + λI (h ik &lt; 0) I (µ b -|h ik | &gt; 0) ȧl ( ĥl ik ), (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where l is the output layer of LSTMs, ȧl (•) is the derivative of the l-th layer activation function, and I(A) is an indicator function, I(A) = 1 if A is true and I(A) = 0 otherwise. For a hidden unit k in the ( -1)-th layer, we compute the residual δ -1</p><p>x,ik based on a weighted average of the errors of all the units k = 1, . . . , n -1 in the ( -1)-th layer that use h -1 i as an input, which is consistent with standard back-propagation,</p><formula xml:id="formula_16">δ -1 x,ik = n -1 k =1 δ x,ik W -1 x,kk ȧ -1 x ĥ -1 ik ,<label>(14)</label></formula><p>where n -1 is number of units in the ( -1)-th layer. The residuals in all layers can be computed by back-propagation.</p><p>For the hashing networks, we derive the gradient of pointwise loss Oi w.r.t. W u,k and W v,k , the network parameter of the k-th unit of -th layer in the hashing networks for image and sentence, respectively. The derivatives are as follows,</p><formula xml:id="formula_17">∂Oi ∂W u u,k = β ∂L x i ∂W u u,k = βδ u u,ik û u-1 i , ∂Oi ∂W v v,k = β ∂L y i ∂W v v,k = βδ v v,ik v v -1 i ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_18">û i = W u u -1 i + b u is the -th layer output before activation a (•), δ u,ik ∂L x i ∂ û ik</formula><p>is the point-wise residual term that measures how much the k-th unit in the -th layer is responsible for the error of point ui in the network output (similar definitions apply to the sentence hashing network):</p><formula xml:id="formula_19">δ lu u,ik = u lu i -h l it =     u lu i - T t=1 πith l it T t=1 πit     , δ lv v,ik = T t=1 πit v lv it -h l it T t=1 πit , (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>where lu is the output layer of the image hashing network, and ȧlu (•) is the derivative of the lu-th layer activation function. For a hidden unit k in the ( u -1)-th layer, we compute the residual δ u -1 u,ik based on a weighted average of the errors of all the units k = 1, . . . , n u -1 in the ( u -1)-th layer that use u u-1 i as an input, which is consistent with standard BP. As the only differences between standard back-propagation (BP) and our algorithm are the residual terms defined in Equations ( <ref type="formula" target="#formula_14">13</ref>) <ref type="bibr" target="#b16">(16)</ref>, we analyze the computational complexity for ( <ref type="formula" target="#formula_14">13</ref>) and <ref type="bibr" target="#b16">(16)</ref>. Denote the number of similarity pairs S available for training as |S| and the number of bimodal objects available for training as N , then it is easy to verify that the overall computational complexity is O(|S| + N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We conduct extensive experiments to evaluate the efficacy of the proposed DVSH model with several state of the art hashing methods on two widely-used benchmark datasets. Datasets, codes and configurations will be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Setup</head><p>The evaluation is conducted on two benchmark cross-modal datasets: Microsoft COCO <ref type="bibr" target="#b24">[24]</ref> and IAPR TC-12 <ref type="bibr" target="#b13">[13]</ref>.</p><p>Microsoft COCO 1 The current release of this recently proposed dataset contains 82,783 training images and 5000   testing images. For each image, it provides five sentences annotations, belonging to 90 most frequent categories as ground truth labels. After pruning images with no category information, we get 82,120 training images and 4,960 testing images, from which we generate 410,600 training imagesentence pairs and 24,800 testing image-sentence pairs. IAPR TC-12 2 This dataset consists of 20,000 images collected from a wide variety of domains, such as sports and actions, people, animals, cities, landscapes, and so forth. For each image, it provides at least one sentence annotation. On average there are about 1.7 sentence annotations for each image. Besides, it provides category annotations generated from segmentation tasks 3 with 275 concepts. For evaluation, we prune the original IAPR TC-12 to form a new dataset, which consists of 18715 images belonging to 22 most frequent concepts, and then generate 33447 image-sentence pairs.</p><p>For the propose deep-hashing approach DVSH, we directly use the raw pixels as the image input and word sequences as the sentence input, which consists of one-hot vectors each representing a word of the sentence. As a common practice for fair comparison, for traditional shallow-hashing methods, we use AlexNet <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b7">7]</ref> to extract deep fc7 features for each image in two benchmark dataset by a 4096-dimensional vector, and represent each sentence by a bag-of-word vector.</p><p>All image and text features are available at the datasets' website. For Microsoft COCO, we randomly select 25,000 image-sentence pairs as training set, 5000 pairs as validation set and 5000 pairs as query set. For IAPR TC-12 dataset, we randomly select 5000 pairs as the training set, 1000 pairs as the validation set and 100 pairs per class as the test query set. The pairwise similarity labels for training are randomly constructed using semantic labels or concepts, and each pair 2 http://imageclef.org/photodata 3 http://imageclef.org/SIAPRdata is considered similar (dissimilar) if they share at least one (none) semantic label, a common protocol used by <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>We compare the cross-modal retrieval performance of our approach with eight state of the art cross-modal hashing methods, including three unsupervised methods IMH <ref type="foot" target="#foot_0">4</ref> [33], CVH<ref type="foot" target="#foot_1">5</ref>  <ref type="bibr" target="#b22">[22]</ref> and CorrAE<ref type="foot" target="#foot_2">6</ref>  <ref type="bibr">[8]</ref>, and five supervised methods CMSSH 5 <ref type="bibr" target="#b4">[4]</ref>, CM-NN <ref type="foot" target="#foot_3">7</ref> [30], SCM 7 [43], QCH 7 <ref type="bibr" target="#b39">[39]</ref> and SePH<ref type="foot" target="#foot_4">8</ref>  <ref type="bibr" target="#b25">[25]</ref>, where CorrAE and CM-NN are deep methods and the rest are shallow methods. To our best knowledge, there is no cross-modal deep hashing method based either on CNNs or RNNs, hence we extend the state of the art deep network hashing (DNH) method for image retrieval <ref type="bibr" target="#b23">[23]</ref> to cross-modal retrieval as a strong baseline, denoted as DNH-C. This baseline is modified by applying multi-layer perceptrons to the sentence modality with the same triplet hinge loss as image modality, and adding a least square loss to reduce the gap between the codes of different modalities.</p><p>We follow <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b23">23]</ref> to evaluate the retrieval performance based on three metrics: Mean Average Precision(MAP), precision-recall curves, and precision@top-R curves. We adopt MAP@R = 500 following the baseline methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>We implement the DVSH model in the open-source Caffe framework <ref type="bibr" target="#b17">[17]</ref>. For training network, we employ the AlexNet architecture <ref type="bibr" target="#b21">[21]</ref> and a factored-2-layer LSTM <ref type="bibr" target="#b20">[20]</ref>, fine-tune convolutional layers conv1-conv5 and fully-connected layers f c6-f c7 that were copied from the pre-trained model, and train LSTM layers and feature-map layer f c8, all via backpropagation. As the f c8 layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. For hashing networks, we employ AlexNet for image network and a 2-layer LSTM for sentence network, with the featuremap layers (f c8 of AlexNet and the output layer of LSTM) trained from scratch. We use the mini-batch stochastic gradient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe, cross-validate learning rate from 10 -5 to 1 with a multiplicative step-size 10, and fix mini-batch size as 50. Following <ref type="bibr" target="#b6">[6]</ref>, we adopt 20 and 25 as the maximum number of words in each sentence for Microsoft COCO and IAPR-TC12 datasets, respectively. The DVSH approach involves two penalty parameters λ and β for trading off the relative importance of bitwise maxmargin loss <ref type="bibr" target="#b7">(7)</ref> and squared losses (8) and ( <ref type="formula" target="#formula_9">9</ref>), which can be automatically selected using cross-validation. And we can always achieve good empirical results with λ = 0.1 and β = 1. For comparison methods, we use cross-validation to carefully tune their parameters for best results. Each experiment repeats five runs and the average results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussions</head><p>We compare our approach DVSH with the nine state of the art methods on the two datasets in terms of MAP, precisionrecall curves and precision@top-R curves of two cross-modal retrieval tasks: image query on sentence database (I → T ), and sentence query on image database (T → I).</p><p>We evaluate all methods with different lengths of hash codes, i.e. 16, 32, 64 and 128 bits, and report their MAP results in Table <ref type="table" target="#tab_1">1</ref>. From the experimental results, we can observe that DVSH substantially outperforms all state of the art methods for most cross-modal tasks on the benchmark datasets which well demonstrates its effectiveness. Specifically, compared to the best shallow baseline SCM with deep AlexNet-f c7 features as input, DVSH achieves absolute increases of 8.6%/8.3% and 3.9%/4.0% in average MAP for two cross-modal tasks I → T and T → I on Microsoft COCO and IAPR TC-12 datasets. SePH does not perform well in comparison to SCM, due to its assumption of t-distribution in the learning procedure, which does not hold on our datasets. Compared to the cross-modal deep hashing methods, DVSH outperform CM-NN by large margins 12.5%/10.3% and 9.7%/10.9%. As we expected, DVSH also outperforms the cross-modal extension of the state of the art deep hashing method DNH-C. But DNH-C cannot outperform the shallow methods with deep features as input (SCM, QCH and SePH), which implies that different architectures and loss functions should be crafted together to achieve optimal performance. This motivates us to craft an end-to-end deep hashing architecture for cross-modal retrieval.</p><p>The precision-recall curves with 32 bits for the two crossmodal tasks I → T and T → I on two datasets Microsoft COCO and IAPR TC-12 are shown in Figure <ref type="figure" target="#fig_4">4</ref>, respectively. DVSH shows the best cross-modal retrieval performance at all recall levels. Figure <ref type="figure" target="#fig_5">5</ref> shows the precision@top-R curves of all comparison methods with 32 bits on the two datasets, which shows how the precision changes with the number R of top-retrieved results. Again, we can observe that DVSH significantly outperforms all state of the art methods, which shows that DVSH is also for applications that prefer higher precision while tolerating fewer top-retrieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Empirical Analysis</head><p>To extensively evaluate the effectiveness of the components newly-crafted in this paper, including the cosine maxmargin loss for similarity-preserving learning (6), the bitwise max-margin loss for controlling the quality of binary codes <ref type="bibr" target="#b7">(7)</ref>, and the modality-specific hashing networks for generating out-of-sample hash codes (8)-( <ref type="formula" target="#formula_9">9</ref>), we design four variants of the DVSH approach: (a) DVSH-B is the DVSH variant without binarization (sgn(h) is not performed), which may serve as the upper bound of performance. (b) DVSH-Q is the DVSH variant without bitwise max-margin loss <ref type="bibr" target="#b7">(7)</ref>; (c) DVSH-I is the DVSH variant by replacing the cosine maxmargin loss <ref type="bibr" target="#b6">(6)</ref> with the widely-used inner-product squared loss L = s ij ∈S sij -   <ref type="table" target="#tab_3">2</ref>.</p><p>From Table <ref type="table" target="#tab_3">2</ref>, we may have the following observations: (a) By using cosine max-margin loss, DVSH outperforms DVSH-I by large margins of 11.2%/15.1% and 12.3%/9.2% in average MAP on the two benchmark datasets. The squared inner-product loss has been widely adopted in previous work <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b40">40]</ref>. However, this loss does not link well the pairwise distances between points (taking values in (-∞, +∞) when using continuous relaxation) to the pairwise similarity labels (taking binary values {-1,1}). In contrast, the proposed cosine max-margin loss ( <ref type="formula" target="#formula_6">6</ref>) is inherently consistent with the training pairs. Besides, the margin µc in (6) can also control the robustness of similarity-preserving learning to outliers.</p><p>(b) By optimizing bitwise max-margin loss <ref type="bibr" target="#b7">(7)</ref>, DVSH incurs small decreases 3.3%/8.7% and 4.3%/1.8% in average MAP when quantizing continuous embeddings of DVSH-B into binary codes. In contrast, without optimizing bitwise max-margin loss, DVSH-Q incurs larger decreases 4.6%/10.7% and 6.2%/3.9% in average MAP. Especially for shorter length of hash codes (16 bits), DVSH-Q suffers from huge decreases of 9.1%/20.8% and 8.8%/6.0% while DVSH incurs smaller MAP decreases 7.9%/17.0% and 5.6%/2.5%. This validates that the bitwise max-margin loss <ref type="bibr" target="#b7">(7)</ref> can effectively reduce the quantization error and achieve higher-quality hash codes.</p><p>(c) As we have expected, the performance of DVSH-H drops by huge decreases 16.3%/16.3% and 13.7%/15.5% in average MAP w.r.t. the carefully-crafted DVSH approach. This validates that the visual-semantic fusion network cannot perform well if it is used to generate out-of-sample hash codes which may have only single-modal inputs. This result motivates us to integrate the modality-specific hashing networks into DVSH, our end-to-end deep hashing architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Sensitivity</head><p>In this section, we further discuss the performance of DVSH w.r.t the two model parameters λ and β to validate the robustness of our approach. Here we compute the MAP score @ 64 bits on both the cross-modal retrieval tasks by varying λ between 0.005 and 5 and β between 0.02 and 20. The sensitivity performance of DVSH with respect to two parameters is illustrated in Figure <ref type="figure" target="#fig_7">6</ref>(a) and 6(b). From the figure, we see that DVSH can consistently outperform all the baseline methods by large margins when varying λ between 0.005 and 1, and β between 0.1 and 5. When λ → 0, DVSH deprecates to DVSH-Q which learns hash codes without bitwise max-  margin loss <ref type="bibr" target="#b7">(7)</ref>. We observer the retrieval performance of DVSH first increases and then decreases as λ and β vary and demonstrates a desirable bell-shaped curve. This justifies our motivation of jointly learning deep features whilst minimizing the bitwise max-margin loss <ref type="bibr" target="#b7">(7)</ref> and squared losses (8) and ( <ref type="formula" target="#formula_9">9</ref>), since a good trade-off between them can enable effective learning of high-quality hash codes. The results also validate that DVSH is robust against parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper presented a novel deep visual-semantic hashing (DVSH) model to enable efficient cross-modal retrieval of images in response to text sentences and vice versa. Our DVSH model generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which effectively unifies joint multimodal embedding with crossmodal hashing. In particular, by embedding convolutional neural networks over images into recurrent neural networks over sentences, we jointly capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. Comprehensive empirical evidence shows that our DVSH model yields state of the art performance in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO. In the future, we plan to extend DVSH to data from social media and mobile computing, and to heterogeneous scenarios where inter-modal relationship information is not available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep visual-semantic hashing (DVSH) for cross-modal retrieval of images and text sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A diagram of an LSTM memory cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of Deep Visual-Semantic Hashing (DVSH), an end-to-end deep hashing approach to image-sentence cross-modal retrieval. The architecture comprises four key components: (1) a deep visualsemantic fusion network (unifying CNN and LSTM) for learning isomorphic hash codes in the joint embedding space; (2) an image hashing network (CNN) and a sentence hashing network (LSTM) for learning nonlinear modality-specific hash functions that map inputs to the joint embedding space; (3) a new cosine max-margin loss to preserve the pairwise similarity information; (4) a novel bitwise max-margin loss to control the quality of binary hash codes. Colored ones are modules modified or newly-crafted in this paper. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label></label><figDesc>http://mscoco.org T → I on IAPR TC-12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision-recall curves of cross-modal retrieval on Microsoft COCO and IAPR TC-12 @ 32 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Precision@top-R curves of cross-modal retrieval on Microsoft COCO and IAPR TC-12 @ 32 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>COCO) I→ T(IAPR) T→ I(COCO) T→ I(IAPR) (a) MAP w.r.t. λ @ 64 bits 0.02 0.05 0.1 0.2 0.5 COCO) I→ T(IAPR) T→ I(COCO) T→ I(IAPR) (b) MAP w.r.t. β @ 64 bits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The MAP of DVSH @ 64 bits versus the parameter λ ∈ [0.005, 5] and β ∈ [0.02, 20] for the two cross-modal retrieval tasks (I → T and T → I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean Average Precision (MAP) Comparison of Cross-Modal Retrieval Tasks on Two Datasets</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>16 bits</cell><cell cols="2">Microsoft COCO [24] 32 bits 64 bits</cell><cell>128 bits</cell><cell>16 bits</cell><cell cols="2">IAPR TC-12 [13] 32 bits 64 bits</cell><cell>128 bits</cell></row><row><cell></cell><cell>CMSSH [4]</cell><cell>0.4047</cell><cell>0.4886</cell><cell>0.4405</cell><cell>0.4480</cell><cell>0.3445</cell><cell>0.3371</cell><cell>0.3478</cell><cell>0.3738</cell></row><row><cell></cell><cell>CVH [22]</cell><cell>0.3731</cell><cell>0.3677</cell><cell>0.3657</cell><cell>0.3570</cell><cell>0.3788</cell><cell>0.3686</cell><cell>0.3620</cell><cell>0.3540</cell></row><row><cell></cell><cell>IMH [33]</cell><cell>0.6154</cell><cell>0.6505</cell><cell>0.6573</cell><cell>0.6770</cell><cell>0.4632</cell><cell>0.4901</cell><cell>0.5104</cell><cell>0.5212</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell>0.5498</cell><cell>0.5559</cell><cell>0.5695</cell><cell>0.5809</cell><cell>0.4951</cell><cell>0.5252</cell><cell>0.5578</cell><cell>0.5890</cell></row><row><cell>I → T</cell><cell>CM-NN [30] SCM [43]</cell><cell>0.5557 0.5699</cell><cell>0.5602 0.6002</cell><cell>0.5847 0.6307</cell><cell>0.5938 0.6487</cell><cell>0.5159 0.5880</cell><cell>0.5419 0.6110</cell><cell>0.5766 0.6282</cell><cell>0.6003 0.6370</cell></row><row><cell></cell><cell>QCH [39]</cell><cell>0.5723</cell><cell>0.5954</cell><cell>0.6132</cell><cell>0.6345</cell><cell>0.5259</cell><cell>0.5546</cell><cell>0.5785</cell><cell>0.6054</cell></row><row><cell></cell><cell>SePH [25]</cell><cell>0.5813</cell><cell>0.6134</cell><cell>0.6253</cell><cell>0.6339</cell><cell>0.5070</cell><cell>0.5130</cell><cell>0.5151</cell><cell>0.5309</cell></row><row><cell></cell><cell>DNH-C [23]</cell><cell>0.5353</cell><cell>0.5560</cell><cell>0.5693</cell><cell>0.5824</cell><cell>0.4801</cell><cell>0.5093</cell><cell>0.5259</cell><cell>0.5349</cell></row><row><cell></cell><cell>DVSH</cell><cell>0.5870</cell><cell>0.7132</cell><cell>0.7386</cell><cell>0.7552</cell><cell>0.5696</cell><cell>0.6321</cell><cell>0.6964</cell><cell>0.7236</cell></row><row><cell></cell><cell>CMSSH [4]</cell><cell>0.3747</cell><cell>0.3838</cell><cell>0.3400</cell><cell>0.3601</cell><cell>0.3633</cell><cell>0.3770</cell><cell>0.3645</cell><cell>0.3482</cell></row><row><cell></cell><cell>CVH [22]</cell><cell>0.3734</cell><cell>0.3686</cell><cell>0.3645</cell><cell>0.3711</cell><cell>0.3790</cell><cell>0.3674</cell><cell>0.3636</cell><cell>0.3560</cell></row><row><cell></cell><cell>IMH [33]</cell><cell>0.6068</cell><cell>0.6793</cell><cell>0.7280</cell><cell>0.7403</cell><cell>0.5157</cell><cell>0.5259</cell><cell>0.5337</cell><cell>0.5274</cell></row><row><cell></cell><cell>CorrAE [8]</cell><cell>0.5593</cell><cell>0.5807</cell><cell>0.6109</cell><cell>0.6262</cell><cell>0.4975</cell><cell>0.5195</cell><cell>0.5329</cell><cell>0.5495</cell></row><row><cell>T → I</cell><cell>CM-NN [30] SCM [43]</cell><cell>0.5793 0.5581</cell><cell>0.5984 0.6188</cell><cell>0.6195 0.6583</cell><cell>0.6448 0.6858</cell><cell>0.5119 0.5876</cell><cell>0.5394 0.6045</cell><cell>0.5487 0.6200</cell><cell>0.5649 0.6262</cell></row><row><cell></cell><cell>QCH [39]</cell><cell>0.5742</cell><cell>0.6057</cell><cell>0.6375</cell><cell>0.6669</cell><cell>0.4997</cell><cell>0.5364</cell><cell>0.5652</cell><cell>0.5885</cell></row><row><cell></cell><cell>SePH [25]</cell><cell>0.6127</cell><cell>0.6496</cell><cell>0.6723</cell><cell>0.6929</cell><cell>0.4712</cell><cell>0.4801</cell><cell>0.4812</cell><cell>0.4955</cell></row><row><cell></cell><cell>DNH-C [23]</cell><cell>0.5250</cell><cell>0.5592</cell><cell>0.5902</cell><cell>0.6339</cell><cell>0.4692</cell><cell>0.4838</cell><cell>0.4905</cell><cell>0.5053</cell></row><row><cell></cell><cell>DVSH</cell><cell>0.5906</cell><cell>0.7365</cell><cell>0.7583</cell><cell>0.7673</cell><cell>0.6037</cell><cell>0.6395</cell><cell>0.6806</cell><cell>0.6751</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean Average Precision (MAP) of DVSH Variants for Cross-Modal Retrieval Tasks on Two Datasets</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell>16 bits</cell><cell cols="2">Microsoft COCO [24] 32 bits 64 bits</cell><cell>128 bits</cell><cell>16 bits</cell><cell cols="2">IAPR TC-12 [13] 32 bits 64 bits</cell><cell>128 bits</cell></row><row><cell></cell><cell>DVSH-B</cell><cell>0.6658</cell><cell>0.7408</cell><cell>0.7532</cell><cell>0.7645</cell><cell>0.6260</cell><cell>0.6761</cell><cell>0.7359</cell><cell>0.7554</cell></row><row><cell></cell><cell>DVSH</cell><cell>0.5870</cell><cell>0.7132</cell><cell>0.7386</cell><cell>0.7552</cell><cell>0.5696</cell><cell>0.6321</cell><cell>0.6964</cell><cell>0.7236</cell></row><row><cell>I → T</cell><cell>DVSH-Q</cell><cell>0.5746</cell><cell>0.7019</cell><cell>0.7145</cell><cell>0.7505</cell><cell>0.5385</cell><cell>0.6113</cell><cell>0.6869</cell><cell>0.7097</cell></row><row><cell></cell><cell>DVSH-I</cell><cell>0.5264</cell><cell>0.5745</cell><cell>0.6056</cell><cell>0.6391</cell><cell>0.4792</cell><cell>0.5035</cell><cell>0.5583</cell><cell>0.5890</cell></row><row><cell></cell><cell>DVSH-H</cell><cell>0.4856</cell><cell>0.5244</cell><cell>0.5545</cell><cell>0.5786</cell><cell>0.4575</cell><cell>0.4975</cell><cell>0.5493</cell><cell>0.5690</cell></row><row><cell></cell><cell>DVSH-B</cell><cell>0.7605</cell><cell>0.8192</cell><cell>0.8034</cell><cell>0.8194</cell><cell>0.6285</cell><cell>0.6728</cell><cell>0.6922</cell><cell>0.6756</cell></row><row><cell></cell><cell>DVSH</cell><cell>0.5906</cell><cell>0.7365</cell><cell>0.7583</cell><cell>0.7673</cell><cell>0.6037</cell><cell>0.6395</cell><cell>0.6806</cell><cell>0.6751</cell></row><row><cell>T → I</cell><cell>DVSH-Q</cell><cell>0.5530</cell><cell>0.7105</cell><cell>0.7541</cell><cell>0.7569</cell><cell>0.5684</cell><cell>0.6153</cell><cell>0.6618</cell><cell>0.6693</cell></row><row><cell></cell><cell>DVSH-I</cell><cell>0.5185</cell><cell>0.5353</cell><cell>0.5805</cell><cell>0.6136</cell><cell>0.4903</cell><cell>0.5496</cell><cell>0.5890</cell><cell>0.6012</cell></row><row><cell></cell><cell>DVSH-H</cell><cell>0.5025</cell><cell>0.5368</cell><cell>0.5688</cell><cell>0.5939</cell><cell>0.4396</cell><cell>0.4853</cell><cell>0.5185</cell><cell>0.5337</cell></row><row><cell cols="5">and (9), which means that we use the fusion network with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">single-modal features (image or sentence) to generate hash</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">codes. MAP results of all variants are shown in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>http://staff.itee.uq.edu.au/shenht/UQ IMH</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>http://www.cse.ust.hk/˜dyyeung/code/mlbe.zip</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/fangxiangfeng/deepnet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>Since code is not publicly available, we implement it by ourselves.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>We thank the authors for kindly providing the codes.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (61325008, 61502265), China Postdoctoral Science Foundation (2015T80088), NSF through grant III-1526499, and Tsinghua National Laboratory Special Fund for Big Data Science and Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data fusion through cross-modality metric learning using similarity-sensitive hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep quantization network for efficient image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop OntoImage</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterative multi-view hashing for cross media indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hash functions for cross-view similarity search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantics-preserving hashing for cross-view retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collaborative hashing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composite correlation quantization for efficient multimodal search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal similarity-preserving hashing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparing apples to oranges: a scalable solution with heterogeneous hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inter-media hashing for large-scale retrieval from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective multi-modal retrieval based on stacked auto-encoders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable heterogeneous translated hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantized correlation hashing for fast cross-modal search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discriminative coupled dictionary hashing for fast cross-media retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1410.4615</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale supervised multimodal hashing with semantic correlation maximization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Co-regularized hashing for multimodal data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A probabilistic model for multimodal hash function learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
