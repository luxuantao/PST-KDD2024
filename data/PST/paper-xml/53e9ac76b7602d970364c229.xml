<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiences on Processing Spatial Data with MapReduce *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ariel</forename><surname>Cary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 SW 8 th St</addrLine>
									<postCode>33199</postCode>
									<settlement>Miami</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengguo</forename><surname>Sun</surname></persName>
							<email>sunz@cis.fiu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 SW 8 th St</addrLine>
									<postCode>33199</postCode>
									<settlement>Miami</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vagelis</forename><surname>Hristidis</surname></persName>
							<email>vagelis@cis.fiu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 SW 8 th St</addrLine>
									<postCode>33199</postCode>
									<settlement>Miami</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naphtali</forename><surname>Rishe</surname></persName>
							<email>rishen@cis.fiu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 SW 8 th St</addrLine>
									<postCode>33199</postCode>
									<settlement>Miami</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Experiences on Processing Spatial Data with MapReduce *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C7C26DE1DABF379D6AD9F80C553BAB1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The amount of information in spatial databases is growing as more data is made available. Spatial databases mainly store two types of data: raster data (satellite/aerial digital images), and vector data (points, lines, polygons). The complexity and nature of spatial databases makes them ideal for applying parallel processing. MapReduce is an emerging massively parallel computing model, proposed by Google. In this work, we present our experiences in applying the MapReduce model to solve two important spatial problems: (a) bulk-construction of R-Trees and (b) aerial image quality computation, which involve vector and raster data, respectively. We present our results on the scalability of MapReduce, and the effect of parallelism on the quality of the results. Our algorithms were executed on a Google&amp;IBM cluster, which became available to us through an NSF-supported program. The cluster supports the Hadoop framework -an open source implementation of MapReduce. Our results confirm the excellent scalability of the MapReduce framework in processing parallelizable problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Geographic Information Systems (GIS) deal with complex and large amounts of spatial data of mainly two categories: raster data (satellite/aerial digital images), and vector data (points, lines, polygons). This type of data is periodically generated via specialized sensors, satellites or aircraft-mounted cameras (sampling geographical regions into digital images), or GPS devices (generating geo-location information). GIS systems have to efficiently manage repositories of spatial data for various purposes, such as spatial searches, and imagery processing. Due to the large size of spatial repositories and the complexity of the applications to process them, traditional sequential computing models may take excessive time to complete. Emerging parallel computing models, such as MapReduce, provide a potential for scaling data processing in spatial applications.</p><p>The goal of this paper is to present to the research community our experiences from using the MapReduce model to tackle two typical and representative spatial data processing problems. The first problem involves vector spatial data and the second involves raster data.</p><p>The first problem is the bulk-construction of R-Trees <ref type="bibr" target="#b0">[1]</ref>, a popular indexing mechanism for spatial search query processing. We show how previous ideas, like the ordering of multi-dimensional objects via space-filling curves, can be used to create a MapReduce algorithm for this problem. We also discuss how our solution is different from previous approaches on parallelizing the construction of an R-Tree.</p><p>The second problem processes aerial digital imagery, and computes and stores image quality characteristics as metadata. Original images may contain inaccurate, distorted, or incomplete data introduced at some phase of imagery generation; for example, a portion of an image may be completely blank. Pre-computed metadata is important in dynamic imagery consistency checking, and allows the appropriate mosaicing with better sources to improve the imagery display. This problem is naturally parallelizable since each tile can be potentially processed independently. In practice, the amount of data processed by each cluster processor depends on the file system characteristics like the minimum processing unit.</p><p>Both problems were solved and evaluated on a Google&amp;IBM cluster supplied by the NSF Cluster Exploratory (CluE) program <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. We present our experiences on using such a cluster in practice and deploying MapReduce jobs.</p><p>The key contribution of this work is as follow:</p><p>• We present techniques for bulk building R-trees using the MapReduce framework.</p><p>• We present how MapReduce can be applied to massively parallel processing of raster data.</p><p>• We experimentally evaluated our algorithms in terms of execution time, scalability and quality of the output. We provide various metrics to measure the quality of the resulting R-Tree. This paper is organized as follows. Section 2 describes the steps in deploying MapReduce applications on the Google&amp;IBM's cluster, as well as some physical configurations. Sections 3 and 4 present the detailed MapReduce algorithms for our two target problems. Section 5 presents experimental results of our algorithm implementations for different settings. Section 6 discusses related works. Last, Section 7 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Using MapReduce in Practice</head><p>The cluster used in this paper is provided by the Google and IBM Academic Cluster Computing Initiative <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. The cluster contains around 480 computers (nodes) running open source software including the Linux operating system, XEN hypervisor and Apache's Hadoop <ref type="bibr" target="#b3">[4]</ref>, which is an open source implementation of the MapReduce programming model. Each node has half terabytes storage capacity summing up to about 240 Terabytes in total. Access to the cluster is provided through the Internet by a SOCKS proxy server. SOCKS is an Internet protocol that secures client-server communications over a non-secure network.</p><p>There are three main steps in interacting with the cluster, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. (1) Input data is uploaded into the cluster. The user uses file system shell scripts provided MapReduce programming model requires expressing the solutions with two functions: map and reduce. A map function takes a key/value pair, executes some computation, and emits a set of intermediate key/value pairs as output. A reduce function merges all intermediate values associated with the same intermediate key, executes some computation on them, and emits the final output. More complex interactions can be achieved by pipelining several MapReduce compounds in a workflow fashion. A data set is stored as a set of files in HDFS, which are in turn stored as a sequence of blocks (typically of 64MB in size) that are replicated on multiple nodes to provide fault-tolerance. An interested reader may refer to MapReduce Google's work <ref type="bibr" target="#b5">[6]</ref> and open source Hadoop documentation <ref type="bibr" target="#b3">[4]</ref> for a detailed description of MapReduce and Hadoop concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building R-Tree with MapReduce</head><p>This section discusses a MapReduce-based algorithm for building an R-Tree index structure <ref type="bibr" target="#b0">[1]</ref> on a spatial data set in parallel fashion. Let us start our description by defining the problem. Let D be a spatial data set composed of objects o i , i=1, .., |D|. Each object o has two attributes &lt;o.id, o.P&gt;, where o.id is the object's unique identifier and o.P is the object's location in some spatial domain; other attributes are possible, but we concentrate on these only for our R-Tree construction purpose. The R-Tree minimum bounding rectangles (MBRs) are created based on the objects' spatial attribute o.P. Identifiers o.id are used as references to objects stored in the R-Tree leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Phases involved in building an R-Tree index for a data set D in MapReduce</head><p>The proposed method consists of three phases executed in sequence, as can be seen in Figure <ref type="figure">2</ref>. First, the spatial objects are partitioned into groups. Then, each group is processed to create a small R-Tree. Finally, the small R-Trees are merged into the final R-Tree. The first two phases are executed in MapReduce, while the last phase does not require high computational power, thus it is executed sequentially outside of the cluster.</p><p>The three main phases of the algorithm are:</p><p>1 Computation of partitioning function f. The inputs for this phase are the data set D and a positive number R, which represents the number of partitions. The purpose of f is to assign any object of D into one of the R possible partitions. The function is computed in such a way that applying f on D yields R (ideally) equally-sized partitions. In practice, minimal variance in sizes is acceptable. At the same time, f attempts to put objects that are close in the spatial domain in the same partition. The output of this phase is a function f which takes as input an object location o.P and outputs a partition number. Note that no actual partitioning or data moving happens at this point. The next phase utilizes f for such purpose. More details of this step are presented in Section 3.1.</p><p>2 R-Tree construction. During this phase, the function f calculated in the first phase is used by Mappers to divide D into R partitions. Then, R Reducers build R independent "small" R-Tree indices simultaneously on their input partitions. The output of this phase is a set of R independent R-Trees. Details of this step are presented in Section 3.2.</p><p>3 R-Tree consolidation. This phase combines the R individual R-Trees, built in the second phase, under a single root node to form the final R-Tree index of D. This phase can be as simple as making the R R-Trees children of a single root node, or it may require adding a few extra levels (at most one in practice) if R exceeds the capacity of a single node. Since this phase is not computationally intensive for R under a few hundreds or thousands, it is executed by a single process outside the cluster. The logic to run this phase is fairly simple, so no further elaboration will be done on this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partitioning Function</head><p>The purpose of the partitioning function f is to provide a means for assigning objects of D to a pre-defined number of R partitions. We use the idea of mapping multidimensional spaces into an ordered sequence of single-dimensional values via spacefilling curves for this purpose. This idea has been studied in the literature as a way to numbering objects in multi-dimensional spaces <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In our present problem, we map objects' location attribute o.P into such curves. We use the Z-order curve <ref type="bibr" target="#b8">[9]</ref> in our experiments in Section 5.1. The partition number of an object o is determined by f(o.P), which evaluates to a value from the set {1, 2, .., R}. By using a space-filling curve, the partitioning function f achieves two goals:</p><p>• Generate R (almost) uniformly-sized partitions, and • Preserve spatial locality. If two distinct objects o1 and o2 are close to each other in the spatial domain, then they are likely to be assigned to the same partition, i.e. f(o1.P) = f(o2.P). Next, we propose a MapReduce algorithm to define f.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MapReduce Algorithm</head><p>The general idea is inspired by the TeraSort Hadoop application <ref type="bibr" target="#b9">[10]</ref>, which partitions an input data set via data sampling. Given a data set D and target number of partitions R, the MapReduce algorithm runs M Mappers that collectively take L sample objects from D (that is, each Mapper samples objects) and emit their single-dimensional values S={U(o i .P), i=1, .., L} given a space filling curve U. Then, a single Reducer sorts S, and determines a list S´ of R-1 splitting points that split the ordered sequence of samples into R equal-sized partitions. Then, in general, an object o belongs to partition j if S´[j-1] &lt; U(o.P) ≤ S´ <ref type="bibr">[j]</ref>. Thus, f utilizes the splitting points in S´ to assign objects to partitions.</p><p>The specific MapReduce key/value input pairs as well as outputs are presented in Table <ref type="table" target="#tab_0">1</ref>. Mappers read in total L samples at random offsets of their input D, and compute their single dimensional value with the space-filling curve U. The intermmediate key equals to C which is a constant, whose value is irrelevant, that helps in sending Mappers' outputs to a single Reducer. The Reducer receives the L single-dimensional values generated by Mappers, and sorts them into an auxiliary list u 1 , ..,u L , from which R-1 elements are taken starting at the -th element and subsequently at fixedlength offsets to form a list S´ of splitting points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, L)) S´</head><p>An important observation in the sampling process is that Mappers read input data from the distributed storage at block-sized amounts, which is a Hadoop distributed file system parameter specifically tuned for load balancing large files across storage nodes. Thus, all Mappers, except perhaps for the last one, will read the same amount of data, equal to the file system's block size.</p><p>The rationale of the splitting points in S´ is that they provide good enough boundaries to sub-divide D into R partitions since they come from randomly sampled objects. Experiments in section 5.1 show very low standard deviation (under 1%) on the number of objects per partition. Formally, the function f is defined as follows:</p><p>. 1,</p><p>.</p><p>´</p><p>´ ,</p><p>This computation is characterized by running multiple Mappers (samplig data) and one Reducer (sorting samples), which may become a limiting factor in scalability. If the size of S becomes sufficiently large, then the TeraSort <ref type="bibr" target="#b9">[10]</ref> approach can be used to sort its items in parallel, which makes the algorithm more scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">R-Tree Construction</head><p>In this phase, R individual R-Tree indices are built concurrently. Mappers partition the input data set D into R groups using the partitioning function f. Then, every partition is passed to a different Reducer, which independently builds an R-Tree on its input. Next, every Reducer outputs a root node of their constructed R-Trees, so R subtrees are written to the file system at the end of this phase.</p><p>Input and output key/value pairs are shown in Table <ref type="table" target="#tab_1">2</ref>. Mappers read their input data in its entirety and compute objects assigned partitions via f(o.P). Then, every Reducer receives a number of input objects A for which an R-Tree is built and its root emitted as output. Since f balances partitions, it is expected that all Reducers will receive a similar number of objects ( ~| | ) , thus executing similar amount of work in constructing their R-Trees. However, good balancing depends on the underlying space-filling curve U used by f, and the number of sampled objects L. More samples help in tuning the splitting points, but incur in larger sorting time of L elements. Another concern is the quality of the produced R-Trees in relation to the parameter R. In Section 5.1, we provide some initial insight into this direction by measuring R-Tree parameters such as area and overlap in a simplified way, and plotting their MBRs for visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tile Quality Computation Using MapReduce</head><p>This section discusses a MapReduce algorithm to compute the quality information of aerial/satellite imagery. Such information is useful for fast identification of defective image portions, e.g. blank regions inside a tile or a group of tiles, and subsequent dynamic image patching using better imagery available at rendering time. For a given tile, we define a pixel as "bad" if all the values of its samples are below or above some predefined value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Tile quality computation algorithm overview</head><p>Image tiles are stored in customized DOQQ files <ref type="bibr" target="#b10">[11]</ref>, augmented with a descriptive header. Let d be a DOQQ file and t be a tile inside d, d.name is d's file name and t.q is the quality information of tile t. More details of our data set are presented in Section 5.2. Figure <ref type="figure">3</ref> depicts the execution overview of our MapReduce algorithm. The algorithm runs on a tile by tile basis within the boundaries of a given DOQQ file, computing a bitmap per tile where a tile pixel is associated to a bit that is set to 1 if the pixel is deemed "bad", and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MapReduce Algorithm</head><p>Each DOQQ file is first partitioned into several splits, each of which is then processed by a separate Mapper. Splits are carefully generated by parsing tiles out of the input file until the size of all the tiles is close (little smaller) to the block size of the underlying distributed file system or end of file is reached. In doing so, tile boundaries are preserved between different splits. Then, each Mapper will have to read at most two blocks of a file. This helps reduce data transfer time between nodes because different blocks of a file are usually stored on separate nodes. Tiles (values) inside one split are identified by d.name and t.id (keys) and combined as key/value input for Mappers. The input and output key/value pairs for Mappers and Reducers are described in Table <ref type="table" target="#tab_2">3</ref>. The Mapper decompresses the JPEG tile t, iterates through each pixel of t to obtain quality information t.q (a bitmap, one bit per pixel) and compresses it using Run-length encoding (RLE) algorithm. After that, it emits the intermediate key/value pair with d.name as the key and t.q as the value. The Reducer merges all the t.q bitmaps that belongs to a file d and writes them to an output file, containing image quality for d, as shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents and discusses the experimental results we obtained by running the algorithms described in Sections 3 and 4 as Hadoop applications on the Google&amp;IBM cluster presented in Section 2. All the data sets used in this section are real spatial data sets supplied by the High Performance Database Research Center at Florida International University <ref type="bibr">[12]</ref>. At the time of experimentation, there were jobs running in the cluster from other researchers that share this resource, thus some fluctuation in the results is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">R-Tree Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets and Setup</head><p>Experiments are executed on two real spatial data sets. Data set descriptions are shown in Table <ref type="table" target="#tab_3">4</ref>. The points in the data sets are angular coordinates in (latitude, longitude) format. In the following experiments, we use the Z-order space-filling curve <ref type="bibr" target="#b8">[9]</ref> as U function to map the two-dimensional points into a single dimension. We used 3% of each data set as sampling size L (see first phase of the algorithm in Section 3). Data sets are in tabular structured format (CSV), where each line represents an object. We used Hadoop supplied functions to read objects (text lines) from the data sets. During the second phase, Reducers build their individual R-Trees inmemory (to avoid high disk latencies in maintaining the tree along object insertions), then the trees are peristed on Hadoop distributed file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Performance</head><p>This experiment consists of building R-Tree indices on the Google&amp;IBM cluster changing the parameter R in phase-2, that is, the number of concurrently-built R-Trees, from 2 up to 64. As R varies, job completion times are measured for Mappers and Reducers as well as quality statistics on the resulting R-Trees. As a reference, we also ran a single-process R-Tree construction on a dedicated local machine with Intel Xeon E7340 2.4GHz processor and 8GB of RAM running Windows OS; we could not run the single process in the cluster since we do not have login access to individual nodes. Thus, cluster and single process times are not comparable due to dissimilar hardware. Table <ref type="table" target="#tab_4">5</ref> shows MapReduce job completion times for R-Tree construction phases 1 and 2 on both spatial data sets as well as for a single-process build (SP); for YPD we start at R=4 due to memory limitations in cluster nodes for building in-memory trees with less number of Reducers. We do not include phase-3 processing times since it is of little significance compared to the other phases. Phase-1 (partitioning function computation) takes very little time, which is expected since sorting L=3% of objects from a data set can be quickly done in memory by the single reducer in this phase; for our largest data set YPD, about 1 million elements are sampled. Our Z-order values are 8-byte sized elements, so around 8MB of RAM is needed to execute the sort, which is much less than the memory of each cluster node. Likewise, Mappers in phase-2 read data sequentially and execute inexpensive Z-order value computations on their inputs. The most computationally intensive part is performed by Reducers in phase-2, where the actual R-Tree constrution occurs. The fewer the number of Reducers, the longer the R-Tree construction takes, since each task receives larger number of objects. Figure <ref type="figure" target="#fig_1">4</ref> shows job completion times as stacked bars of the map and reduce execution times. In this figure, almost linear scalability is observed as more parallelism is induced by increasing R in phase-2. As expected, the improvement rate is high for few Reducers but drops as the number of Reducers increases since partitioning overheads in phase-1 (MR1) start becoming significant compared to R-Tree build time in phase-2 (MR2). In fact, for larger values of R, the dominating time component is given by MR1 which, as can be seen in Table <ref type="table" target="#tab_4">5</ref>, is almost constant for a given data set. Thus, much less improvements are expected as R is increased beyond 64.</p><p>Although we cannot compare our MapReduce and single process (SP) times due to mismatch in hardware, the MapReduce parallelization certainly yields performance benefits for large-scale data sets. For example, it takes more than an hour to sequentially build the YPD R-Tree, while in parallel the task can be achieved in less than 5 minutes with 64 Reducers. However, the resulting R-Trees are different due to differences in object insertion sequences. Later in this section we measure and discuss R-Tree quality parameters for both cases.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> presents percentages of performance gains in job completion times in relation to subsequent increases in the number of Reducers in the second phase of the algorithm. For example, in the YPD dataset, going from 4 to 8 Reducers we observe 50% decrease in job completion time, which represents linear scalability. On the other hand, going from 8 to 16 Reducers shows super-linear gains (62%). We pressume this may be due to heterogeneous nodes in the cluster (eventually the job with R=16 was executed on faster nodes), or it may be the cluster resources were idler during that period. As discussed, as we increase the number of Reducers, performance gains are less significant because the execution time for the first phase, which has a sequential component (Reduce), stays almost constant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of Generated R-Trees</head><p>We use equations ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_3">3</ref>) below to compute the area and overlap metrics respectively for a given consolidated R-Tree with root T:</p><p>.</p><p>. .</p><formula xml:id="formula_3">)<label>(3</label></formula><p>where n is the number of children (small R-Trees generated by Reducers) of T, and T i is the i-th child node of T. Note that other metrics of R-Tree quality could be considered as well, e.g., consider all the nodes of the R-Tree instead of just the top level. Minimal area and overlap are known to improve search performance <ref type="bibr" target="#b11">[13]</ref> since they increase path pruning abilities of R-Tree navigation algorithms.</p><p>Table <ref type="table" target="#tab_5">6</ref> shows quality metrics on the consolidated R-Trees built for various number of Reducers and single process (SP); for reference, the U.S. Census Bureau reports Florida state land area roughly as 54,000 square miles as of 2000 <ref type="bibr" target="#b12">[14]</ref>. As expected, we see the total MBR area and the overlap increase as the parallelism (R) increases because the construction of each small R-Tree is unaware of the rest of the data set, lowering the chance of co-locating neighbor objects within the same Rtree. This means that we degrade the R-Tree quality without gaining in execution time. The latter can adversely effect performance of search algorithms, such as nearest neighbor type of queries, due to extra I/Os incurred in traversing multiple sub-trees. Fig. <ref type="figure">6</ref>. MBR plotting for FLD data set on an R-Tree built by a single process For a sequential construction (SP), we observe these metrics are much worse, especially the overlap factor, since objects are not spatially shuffled but rather inserted in the data set original sequence. Thus, higher performance penalties are expected in SP constructed R-Trees. On the other hand, the tree height slightly decreases for FLD for R beyond 32 because more small trees means that each one of them may be shorter, while for YPD the height increases by one level for the SP case. In general, small variations in tree height is less significant from a performance standpoint. To visually study the effect of increasing R over the MBR distribution, we have plotted the MBRs of the resulting R-Trees for the case of 4 and 8 Reducers in Figures <ref type="figure">7</ref> and<ref type="figure">8</ref> respectively for the Florida state data set (FLD). Also the same type of graph is shown in Figure <ref type="figure">6</ref> for the SP R-Tree. In neither case is the root MBR plotted since it is common for all trees.</p><p>A few observations can be made from the MBR plottings. First, the partitioning mechanism employed in our algorithms seems to be effective in preserving spatial locality. This results in individual Reducers indexing highly localized objects; their boundaries, however, result in multiple overlappings, which are inevitable. Second, as the number of Reducers is increased from 4 to 8, the plotting shape resembles more the actual shape of the Florida state; that is, R=8 reduces wasted areas (where no actual objects are located) as the Area statistic confirms in Table <ref type="table" target="#tab_5">6</ref>. In fact, Table <ref type="table" target="#tab_5">6</ref> shows steady decrease in area from 2 to 16 Reducers; after that the area keeps on increasing. Third, when the R-Tree is built on the original sequence of objects (no object shuffling) in SP mode, large wasted areas are generated as can be observed in Figure <ref type="figure">6</ref>. From a performance optimization perspective, MapReduce generated R-Trees seem to be better tuned than their single-process counterpart. Therefore, we see promising performance improvements in MapReduce generated R-Trees, which deserve closer verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tile Quality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data set and Setup</head><p>The data set used in the experiments is a 3-inch resolution aerial imagery of Miami Dade County of Florida. The size of the data set is about 52GB after compression. Imagery data is stored as compressed DOQQ file format. There are 482 compressed DOQQ files, each of which contains 4096 tiles. Each tile is 400 by 400 pixels and has 3 bytes for each pixel as the Red, Green and Blue channel. The size for each tile is 480,000 bytes uncompressed and compressed tile is about 50 KB each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Two experiments are carried out for this data set. The first experiment uses a subset of the data set that is a re-sampled version of the original one. It is about 20GB and has 482 files with 1024 tiles each. The size of the files ranges from several megabytes to around 80 megabytes, and the number of Reducers is varied from 4 to 512. The second experiment uses different sized subsets of the original data set. The size of the files ranges from 2GB to 16GB, and the number of Reducers is fixed at 256.</p><p>In the first experiment, the number of Mappers is also fixed, determined by the data set size. Thus, the execution time of the map phase is similar through different runs, as can be seen in Figure <ref type="figure" target="#fig_4">9</ref> (a). The execution time slightly fluctuates because there were other concurrent jobs running in the cluster at the same time. As the number of Reducers increases, the execution time of the reduce phase largely decreases for smaller number of reducers, and less improvements are obtained for larger number of reducers. This is because the same amount of work is now shared by more Reducers. When the number of Reducers is larger than 64, the execution time of the reduce phase stabilizes to around 2.5 minutes. This could be explained by the launching time of Reducers dominating the whole time at this point. With 64 Reducers, each of them will be writing around 482/64 ≈ 8 files. The time taken to write 8, 4 (128 Reducers) or even less files is negligible compared with the launching time of that many Reducers.</p><p>In the second experiment, Figure <ref type="figure" target="#fig_4">9</ref> (b), as the size of the data set increases with constant number of reducers (256), the execution time of the map phase hardly changes, which is consistent with the data parallelization provided by the MapReduce </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Space-filling Curves</head><p>The idea of using space-filling curves to map multi-dimensional spaces into a single dimension has been studied for the case of spatial databases <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b7">8]</ref>; popular spacefilling curves, such as Peano and Hilbert, have been studied in great level of detail. We used the Z-order curve in our experiments. This curve showed high spatial locality preservation for our experimented real data sets. Other curves can certainly be evaluated, which goes beyond our focus on the parallelization of two concrete spatial problems with MapReduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel R-Tree Constructions</head><p>Previous works on R-Tree parallel construction have faced several intrinsic distributed computing problems such as data load balancing, process scheduling, fault tolerance, etc., for which they elaborated special-purpose algorithms. Schnitzer and Leutenegger <ref type="bibr" target="#b14">[16]</ref> propose a Master-Client R-Tree, where the data set is first partitioned using Hilbert packing sort algorithm, then the partitions are declustered into a number of processors (via an specialized declustering algorithm), where individual trees are built. At the end, a master process combines the individual trees into the final R-Tree. Another work by Papadopoulos and Manolopoulos <ref type="bibr" target="#b15">[17]</ref> proposed a methodology for sampling-based space partitionining, load balancing, and partition assignment into a set of processors in parallely building R-Trees. They also discuss some alternatives when the global (consolidated) index has imperfections such as different heights across individual R-Trees.</p><p>In MapReduce, these parallel computing concerns are abstracted out from the application logic, and managed transparently as part of the MapReduce framework. Further, all nodes in the cluster access a common distributed file system, with automatic fault-tolerance and load balancing support, where data locality is employed as base criterion to assign Mappers and Reducers (preferably) to nodes already containing the input data. In contrast, traditional parallel processing works assume every node has its own storage, in a shared-nothing type of architecture, where data transfer among nodes becomes an important optimization goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MapReduce on Spatial Data</head><p>MapReduce framework was used to solve another spatial data problem by Google <ref type="bibr" target="#b16">[18]</ref>, where they study the problem of road alignment by combining satellite and vector data. This work concentrates on the complexities of the problem, which are more challenging than the MapReduce algorithms.</p><p>Schlosser et al. <ref type="bibr" target="#b17">[19]</ref> worked on building octrees in Hadoop for later use in earthquake simulations at large-scale. Their approach builds a tree in a bottom up fashion. The map function in the first iteration generates leaf nodes, then the reduce function coalesces homogeneous leaf nodes into a subtree. Subsequent iterations have identity functions in mappers, and successively use reduce functions to construct the final tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship to MPI</head><p>Message Passing Interface (MPI) <ref type="bibr" target="#b18">[20]</ref> is a specification of a language-independent communication model targeted at writing parallel programs, and it is widely used in a variety of computer cluster platforms. MPI libraries provide primitives and functionality for communication control among a set of processes. Typically, developers need to add explicit calls to synchronize processes and move data around. The key differences between MPI and MapReduce is that MapReduce exploits its simplified model to automatically parallelize tasks (Mappers and Reducers), hiding from programmers the need to worry about process communication, fault-tolerance, and scalability, which are transparently managed by key components, such as cluster management system and distributed file system, that the MapReduce framework is built-upon <ref type="bibr" target="#b5">[6]</ref>. For example, for the R-Tree case study, the Java implementation of the Map and Reduce functions of the first phase, and Map of the second phase have each less than 40 lines of code. The Reduce function in the second phase has about 70 lines of code since it includes extra code for persisting the tree on the distributed file system and collecting build statistics. These numbers do not include applicationspecific routines, which are needed regardless of the parallel model.</p><p>In MapReduce, the underlying assumption is that the solution can be expressed in terms of the Map and Reduce functions working on key/value pairs. In some cases this may not be natural, such as relational joins or multi-stage processes, and can lead to inefficiencies. Then, MPI-like parallel implementations have more opportunities to address application-specific optimizations, due to its finer process control. However, high-level languages have been proposed to address this problem in MapReduce architectures by providing efficient primitives for massive data analysis combining SQL-like declarative style and MapReduce procedural programming <ref type="bibr" target="#b19">[21]</ref> <ref type="bibr" target="#b20">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we used the MapReduce programming model to solve two important spatial problems on a Google&amp;IBM cluster: (a) bulk-construction of R-Trees and (b) aerial image quality computation, which involve vector and raster data, respectively. The experimental results we obtained indicate that the appropriate application of MapReduce could dramatically improve task completion times. Our experiments show close to linear scalability. However, performance is not the only concern for R-Tree construction, which is sensitive to the ordering of objects in its input, but also the quality of the result. MapReduce generated R-Trees have improved quality in terms of MBR area and overlap measurements compared to the single-process construction counterpart. No such quality problem arises in the aerial image quality computation. Our experience in this work shows MapReduce has the potential to be applicable to more complex spatial problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Google, IBM Academic Cluster Overview by the Hadoop Distributed File System (HDFS), which is an integral part of the Apache Hadoop project; HDFS is a clone project of Google's files system GFS [5].(2) A user develops a Hadoop application and submits it to the cluster via Hadoop command. Hadoop applications are usually developed in Java, but other languages are supported, like C++ and Python. (3) After application execution is completed, the output is downloaded to the user's local site with Hadoop file system shell scripts.MapReduce programming model requires expressing the solutions with two functions: map and reduce. A map function takes a key/value pair, executes some computation, and emits a set of intermediate key/value pairs as output. A reduce function merges all intermediate values associated with the same intermediate key, executes some computation on them, and emits the final output. More complex interactions can be achieved by pipelining several MapReduce compounds in a workflow fashion. A data set is stored as a set of files in HDFS, which are in turn stored as a sequence of blocks (typically of 64MB in size) that are replicated on multiple nodes to provide fault-tolerance. An interested reader may refer to MapReduce Google's work<ref type="bibr" target="#b5">[6]</ref> and open source Hadoop documentation<ref type="bibr" target="#b3">[4]</ref> for a detailed description of MapReduce and Hadoop concepts.</figDesc><graphic coords="3,214.81,152.55,69.21,60.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. MapReduce job completion times for various number of reducers in phase-2 (MR2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. MapReduce job percentage of performance gains as the number of reducers is increased</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. MBR plotting for FLD data set for R-Tree built by MapReduce with R=4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. MapReduce job completion time for tile quality computation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Map and Reduce inputs/outputs in computing partitioning function f</figDesc><table><row><cell cols="2">Function Input: (Key, Value)</cell><cell>Output: (Key, Value)</cell></row><row><cell>Map</cell><cell>(o.id, o.P)</cell><cell>(C, U(o.P))</cell></row><row><cell>Reduce</cell><cell>(C, list(u</cell><cell></cell></row></table><note><p>i , i=1, ..</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>MapReduce functions in constructing R-Trees</figDesc><table><row><cell>Function</cell><cell>Input: (Key, Value)</cell><cell>Output: (Key, Value)</cell></row><row><cell>Map</cell><cell>(o.id, o.P)</cell><cell>(f(o.P), o)</cell></row><row><cell>Reduce</cell><cell>(f(o.P), list(o i, i=1, .., A ))</cell><cell>tree.root</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Input and output of map and reduce functions</figDesc><table><row><cell>Function</cell><cell>Input: (Key, Value)</cell><cell>Output: (Key, Value)</cell></row><row><cell>Map</cell><cell>(d.name+t.id, t)</cell><cell>(d.name, t.q)</cell></row><row><cell>Reduce</cell><cell>(d.name, list( .q))</cell><cell>Quality-bitmap of d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Spatial data sets used in experiments</figDesc><table><row><cell>Data set</cell><cell>Objects (millions)</cell><cell>Data size (GB)</cell><cell>Description</cell></row><row><cell>FLD</cell><cell>11.4</cell><cell>5</cell><cell>Points of properties in the state of Florida.</cell></row><row><cell>YPD</cell><cell>37</cell><cell>5.3</cell><cell>Yellow pages directory of points of businesses mostly in the United States but also in other countries.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>MapReduce</figDesc><table><row><cell></cell><cell cols="6">job completion times (in minutes) for the Phase 1 (MR1), and various</cell></row><row><cell cols="7">Reducers (R) in Phase 2 (MR2) of building an R-Tree. Also, completion times for single-</cell></row><row><cell cols="6">process (SP) constructions ran on a local machine are shown.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MR1: Partitioning</cell><cell cols="2">MR2: R-Tree</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Function</cell><cell cols="2">Construction</cell><cell></cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Total</cell></row><row><cell>set</cell><cell>R</cell><cell>Map</cell><cell>Reduce</cell><cell>Map</cell><cell>Reduce</cell><cell>MR1+MR2</cell></row><row><cell>FLD</cell><cell>2</cell><cell>0.35</cell><cell>0.28</cell><cell>0.40</cell><cell>24.12</cell><cell>25.15</cell></row><row><cell></cell><cell>4</cell><cell>0.28</cell><cell>0.23</cell><cell>0.40</cell><cell>11.07</cell><cell>11.98</cell></row><row><cell></cell><cell>8</cell><cell>0.47</cell><cell>0.22</cell><cell>1.73</cell><cell>5.62</cell><cell>8.03</cell></row><row><cell></cell><cell>16</cell><cell>0.30</cell><cell>0.22</cell><cell>0.40</cell><cell>3.05</cell><cell>3.97</cell></row><row><cell></cell><cell>32</cell><cell>0.48</cell><cell>0.23</cell><cell>0.40</cell><cell>1.95</cell><cell>3.07</cell></row><row><cell></cell><cell>64</cell><cell>0.28</cell><cell>0.33</cell><cell>0.45</cell><cell>1.60</cell><cell>2.67</cell></row><row><cell></cell><cell>SP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.34</cell></row><row><cell>YPD</cell><cell>4</cell><cell>0.47</cell><cell>0.38</cell><cell>0.47</cell><cell>52.57</cell><cell>53.88</cell></row><row><cell></cell><cell>8</cell><cell>0.22</cell><cell>0.45</cell><cell>0.72</cell><cell>25.42</cell><cell>26.80</cell></row><row><cell></cell><cell>16</cell><cell>0.40</cell><cell>0.43</cell><cell>0.38</cell><cell>8.93</cell><cell>10.15</cell></row><row><cell></cell><cell>32</cell><cell>0.40</cell><cell>0.43</cell><cell>0.42</cell><cell>4.65</cell><cell>5.90</cell></row><row><cell></cell><cell>64</cell><cell>0.40</cell><cell>0.42</cell><cell>0.88</cell><cell>2.55</cell><cell>4.25</cell></row><row><cell></cell><cell>SP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Statistics on consolidated R-Trees built by various number of Reducers (R), and single process (SP) construction</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Objects per Reducer</cell><cell></cell><cell cols="2">Consolidated R-Tree</cell><cell></cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>set</cell><cell>R</cell><cell>Average</cell><cell>Stdev</cell><cell>Nodes</cell><cell>Height</cell><cell>Area (sq.mi)</cell><cell>Overlap (sq.mi)</cell></row><row><cell>FLD</cell><cell>2</cell><cell>5,690,419</cell><cell cols="2">12,183 172,776</cell><cell>4</cell><cell>132,333.9</cell><cell>304.4</cell></row><row><cell></cell><cell>4</cell><cell>2,845,210</cell><cell cols="2">6,347 172,624</cell><cell>4</cell><cell>106,230.4</cell><cell>4,307.9</cell></row><row><cell></cell><cell>8</cell><cell>1,422,605</cell><cell cols="2">2,235 173,141</cell><cell>4</cell><cell>103,885.8</cell><cell>17,261.9</cell></row><row><cell></cell><cell>16</cell><cell>711,379</cell><cell cols="2">2,533 162,518</cell><cell>4</cell><cell>96,443.1</cell><cell>21,586.3</cell></row><row><cell></cell><cell>32</cell><cell>355,651</cell><cell cols="2">2,379 173,273</cell><cell>3</cell><cell>140,028.7</cell><cell>80,389.1</cell></row><row><cell></cell><cell>64</cell><cell>177,826</cell><cell cols="2">1,816 173,445</cell><cell>3</cell><cell>152,664.2</cell><cell>96,857.7</cell></row><row><cell></cell><cell cols="2">SP 11,382,185</cell><cell cols="2">0 172,681</cell><cell>4</cell><cell>746,145.0</cell><cell>1,344,836.8</cell></row><row><cell>YPD</cell><cell>4</cell><cell>9,257,188</cell><cell cols="2">22,137 568,854</cell><cell>4</cell><cell>26,510,946.3</cell><cell>21,574,857.8</cell></row><row><cell></cell><cell>8</cell><cell>4,628,594</cell><cell cols="2">9,413 568,716</cell><cell>4</cell><cell>23,160,080.0</cell><cell>20,480,729.6</cell></row><row><cell></cell><cell>16</cell><cell>2,314,297</cell><cell cols="2">7,634 568,232</cell><cell>4</cell><cell>67,260,270.0</cell><cell>54,582,299.8</cell></row><row><cell></cell><cell>32</cell><cell>1,157,149</cell><cell cols="2">6,043 567,550</cell><cell>4</cell><cell>68,626,854.9</cell><cell>54,008,538.5</cell></row><row><cell></cell><cell>64</cell><cell>578,574</cell><cell cols="2">2,982 566,199</cell><cell>4</cell><cell>69,791,363.8</cell><cell>55,064,139.4</cell></row><row><cell></cell><cell cols="2">SP 37,034,126</cell><cell cols="2">0 587,353</cell><cell>5</cell><cell>164,966,688.5</cell><cell>658,583,322.6</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This research was supported in part by NSF grants IIS-0837716, CNS-0821345, HRD-0833093, EIA-0220562, IIS-0811922, IIP-0829576 and IIS-0534530, and equipment support by Google and IBM.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">R-Trees: A Dynamic Index Structure for Spatial Searching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guttman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 1984</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">NSF Cluster Exploratory Program</orgName>
		</author>
		<ptr target="http://www.nsf.gov/pubs/2008/nsf08560/nsf08560.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.google.com/intl/en/press/pressrel/20071008_ibm_univ.html" />
		<title level="m">Google&amp;IBM Academic Cluster Computing Initiative</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://hadoop.apache.org" />
		<title level="m">Apache Hadoop project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Google file system. SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation</title>
		<meeting>the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2004-12">December 2004</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Space-filling curves and their use in the design of geometric data structures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asanoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ranjanb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roosc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Welzld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Widmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Space-Filling Curves for Multi-dimensional Indexing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lawder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J H</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BNCOD 2000</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Jeffery</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Lings</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1832</biblScope>
			<biblScope unit="page" from="20" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A computer Oriented Geodetic Data Base; and a New Technique in File Sequencing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ottawa. IBM Ltd</title>
		<imprint>
			<date type="published" when="1966">1966</date>
			<pubPlace>Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TeraByte Sort on Apache Hadoop, Yahoo!</title>
		<author>
			<persName><forename type="first">O</forename><surname>O'malley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="http://egsc.usgs.gov/isb/pubs/factsheets/fs05701.html" />
		<title level="m">Doqq file format</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
		<title level="m">The R*-tree: an efficient and robust access method for points and rectangles</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="http://quickfacts.census.gov/qfd/states/12000.html" />
		<title level="m">Florida State and County QuickFacts</title>
		<imprint>
			<publisher>U.S. Census Bureau</publisher>
			<date type="published" when="2008-07-25">July 25, 2008</date>
		</imprint>
	</monogr>
	<note>last revised</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative analysis of some two-dimensional orderings</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Master-client R-trees: a new parallel R-tree architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schnitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Scientific and Statistical Database Management</title>
		<meeting>the 11th International Conference on Scientific and Statistical Database Management</meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel bulk-loading of spatial data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1419" to="1444" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic alignment of largescale aerial rasters to road-maps, Geographic Information Systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carceroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zelinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirmse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual ACM international symposium on Advances in geographic information systems</title>
		<meeting>the 15th annual ACM international symposium on Advances in geographic information systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Materialized community ground models for large-scale earthquake simulation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taborda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>O'hallaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bielak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM/IEEE Conference on Supercomputing, Conference on High Performance Networking and Computing</title>
		<meeting>the 2008 ACM/IEEE Conference on Supercomputing, Conference on High Performance Networking and Computing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Using MPI: Portable Parallel Programming with the Message-Passing Interface</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Map-reduce-merge: simplified relational data processing on large clusters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>-C</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-L</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2007 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1029" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pig latin: a not-so-foreign language for data processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
