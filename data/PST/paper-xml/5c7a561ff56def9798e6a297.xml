<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language-adversarial Transfer Learning for Low-resource Speech Recognition</title>
				<funder ref="#_bsssyYH">
					<orgName type="full">National Key Research &amp; Development Plan of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ye</forename><surname>Bai</surname></persName>
						</author>
						<title level="a" type="main">Language-adversarial Transfer Learning for Low-resource Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TASLP.2018.2889606</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2018.2889606, IEEE/ACM Transactions on Audio, Speech, and Language Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2018.2889606, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial training</term>
					<term>transfer learning</term>
					<term>crosslingual</term>
					<term>low-resource</term>
					<term>speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The acoustic model trained using the knowledge from the shared hidden layer model (SHL-Model) outperforms the model trained only using the target language, especially under low-resource conditions. However, the shared features may contain some unnecessary language dependent information. It will degrade the performance of the target model. Therefore, this paper proposes language-adversarial transfer learning to alleviate this problem. Adversarial learning is used to ensure that the shared layers of the SHL-Model can learn more language invariant features. Experiments are conducted on IARPA Babel datasets. The results show that the target model trained using the knowledge transferred from the adversarial SHL-Model achieves up to 10.1% relative word error rate reduction when compared with the target model trained using the knowledge transferred from the SHL-Model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EEP neural networks (DNN) based acoustic models have obtained significant improvement for automatic speech recognition (ASR) systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However it is still challenging to rapidly build an ASR system for a novel language with significantly less labeled training data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. This was also the goal of IARPA Babel program. Without any question, data collection and annotation are very timeconsuming and expensive. Therefore, how to effectively use an available larger set of languages to improve the performance of the novel language is very important.</p><p>It is easy for human beings to transfer knowledge from other languages when learning a new language <ref type="bibr" target="#b7">[8]</ref>. Human beings not only share the same vocal tract architecture, but also use the universal phonetic systems of different languages. Similarly, acoustic models are able to share language invariant low-level components across various languages <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. An acoustic model trained using other source languages is referred to as a source model. An acoustic model trained using a novel target language is called a target model. Multilingual training is an effective technique to train the source model Jiangyan Yi is currently an Assistant Professor in NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China. e-mail: jiangyan.yi@nlpr.ia.ac.cn.</p><p>Jianhua Tao is currently a Professor in NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China. e-mail: jhtao@nlpr.ia.ac.cn (Corresponding author: Jianhua Tao).</p><p>Zhengqi Wen is currently an Associate Professor in NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China. e-mail: zqwen@nlpr.ia.ac.cn.</p><p>Ye Bai is currently a PhD student at University of Chinese Academy of Sciences (UCAS), Beijing, China. e-mail: baiye2016@ia.ac.cn. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr">[15]</ref>, <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. This approach benefits from multi-task learning <ref type="bibr" target="#b16">[18]</ref>. The source model is trained jointly on several languages <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b18">[20]</ref>. In addition, language identification based multilingual training is proposed to extract multilingual bottleneck features for low resource speech recognition <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>. The knowledge from the source model can be transferred to the target model via transfer learning. The goal of transfer learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> is to improve the performance of the target model via using knowledge from the source model. The transfer learning methods can be roughly classified into two categories: transferring bottleneck features <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref> and transferring model parameters <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[27]</ref>. This paper focuses on the latter.</p><p>The basic idea of transferring model parameters is that the source model is trained on the source languages, and the trained parameters are used to initialize the target model for the novel language. Previously, shared feature representations for low-resource languages have been studied by Thomas <ref type="bibr" target="#b26">[28]</ref>. Scanzio et al. <ref type="bibr" target="#b27">[29]</ref> present a front-end consisting of an artificial neural network architecture trained with multilingual data. The proposed network is called a shared hidden layer model (SHL-Model). <ref type="bibr">Huang et al.</ref> propose to use DNN based SHL-Model <ref type="bibr" target="#b28">[30]</ref> to transfer model parameters for unseen languages. All the hidden layers of the SHL-Model are shared across multiple languages. The softmax layers of the SHL-Model are language dependent. Recently, Xu et al. <ref type="bibr" target="#b29">[31]</ref> combine this method and semi-supervised learning to transfer cross-lingual knowledge to a target model. More recently in <ref type="bibr" target="#b5">[6]</ref>, Karafiat et al. use bi-directional long-short term memory (BLSTM) based source model to transfer shared parameters. The abovementioned SHL-Models only use softmax layers to learn language specific features. However, a lot of variants of SHL-Models are proposed to use hidden layers and softmax layers to learn language dependent information <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. These models have language specific hidden layers prior to softmax layers.</p><p>The results show that the target model trained using the knowledge transferred from the SHL-Model as shown in Fig. <ref type="figure" target="#fig_0">1</ref> performs better than the model trained only using the target training data, especially when the amount of labeled data of the target language is limited. However, the shared hidden layers may learn some unnecessary language specific information. It will degrade the performance of the target model.</p><p>Therefore, this paper proposes language-adversarial transfer learning to alleviate this problem. Inspired by the success of adversarial learning on domain adaptation <ref type="bibr" target="#b31">[33]</ref>, adversarial learning <ref type="bibr" target="#b32">[34]</ref> is used to ensure that the shared layers of the source model can learn more language invariant features as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Recently, adversarial learning of various neural networks has attracted attention in many tasks. Ganin et al. <ref type="bibr" target="#b31">[33]</ref> and Tzeng et al. <ref type="bibr" target="#b33">[35]</ref> use adversarial strategy for domain adaptation in image classification tasks. More recently, Chen et al. <ref type="bibr" target="#b34">[36]</ref> utilize adversarial learning for Chinese word segmentation on various heterogeneous annotation data. Zhang et al. <ref type="bibr" target="#b35">[37]</ref> use adversarial strategy to obtain bilingual lexicon without cross-lingual knowledge. Shinohara <ref type="bibr" target="#b36">[38]</ref> utilizes adversarial training to perform environment adaptation for robust speech recognition. Saon et al. <ref type="bibr" target="#b2">[3]</ref> use adversarial learning for speaker adaptation in speech recognition systems. The results show that all these methods achieve state-of-the-art performance. However, this paper uses adversarial learning to train multilingual acoustic models which are treated as source models. The shared parameters of the source model are used to initialize the target model. There has been no work, to the best of our knowledge, that uses adversarial strategy and transfer learning to improve the performance of low-resource speech recognition systems.</p><p>The main contribution of this paper is that language adversarial training is used to force the shared layers of the SHL-Model to learn language invariant features. Experiments are conducted on IARPA Babel datasets. The results show that the target model trained using the knowledge transferred from the adversarial SHL-Model obtains improvement by up to 10.1% relative word error rate (WER) reduction over the target model trained using the knowledge transferred from the SHL-Model.</p><p>The rest of this paper is organized as follows. Section II briefly reviews conventional cross-lingual knowledge transfer learning method. The proposed language-adversarial transfer learning method is described in Section III. Experiments are presented in Section IV. Section V discusses the results. This paper is concluded in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REVIEW OF CONVENTIONAL CROSS-LINGUAL KNOWLEDGE TRANSFER LEARNING METHOD</head><p>This section briefly introduces the conventional shared hidden layer model (SHL-Model) and the transfer learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shared Hidden Layer Model</head><p>The SHL-Model is widely used for multilingual tasks <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b29">[31]</ref>. A lot of variants of SHL-Models are proposed to train multilingual models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. The SHL-Model is composed of shared hidden layers and language dependent layers. The shared hidden layers and the language dependent layers are jointly optimized using a multilingual training set. The shared layers can be treated as a universal feature transformation that works well for other novel languages. One kind of the SHL-Models only uses the softmax layer to learn language specific features <ref type="bibr" target="#b28">[30]</ref>. The other kind of SHL-Models is proposed to use hidden layers to learn more language dependent information <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b30">[32]</ref>. These models have language specific hidden layers prior to softmax layers. The latter SHL-Model is used to train multilingual models in this The labels of the softmax layer are language specific senones (tied triphone states).</p><p>paper. The architecture of the SHL-Model is depicted in the left of Fig. <ref type="figure" target="#fig_0">1</ref>. The SHL-Model consists of 2 shared BLSTM hidden layers and 2 language specific BLSTM hidden layers prior to softmax layers. The labels of the softmax layer are language specific senones (tied triphone states).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multilingual Training</head><p>Multilingual training is an instance of multi-task learning <ref type="bibr" target="#b16">[18]</ref>. The source model is trained simultaneously on the training data of multiple languages. Each language has its own softmax layer to estimate the posterior probabilities of language specific senones.</p><p>For the m-th language, given a dataset with N m training samples {x is the total number of senones. The multilingual model is trained to minimize the cross-entropy for all the languages. So the loss function of the SHL-Model is defined as:</p><formula xml:id="formula_0">(m) i , y (m) i } Nm i=1 , where {x (m) i , y (m) i } is the i- th training sample (frame-level), x (m) i ? R d is</formula><formula xml:id="formula_1">L M ul (? m , ? s ) = - M m=1 Nm i=1 logP (y (m) i |x (m) i ; ? m , ? s )<label>(1)</label></formula><p>where m denotes the index of the m-th language, ? m denotes the parameters of the language specific layers for the m-th language, ? s denotes the parameters of the shared layers for all the languages, M is the total number of the source languages. P (y</p><formula xml:id="formula_2">(m) i |x (m) i</formula><p>; ? m , ? s ) is computed with a parametric classifier, such as a BLSTM based model with a set of trainable weights and biases. Stochastic gradient descent (SGD) is commonly used to optimize the parameters. Specifically, its gradient w.r.t the parameters are calculated using backpropagation through time (BPTT) and the parameters are updated as:</p><formula xml:id="formula_3">? m ? ? m -? ?L M ul ?? m (2) ? s ? ? s -? ?L M ul ?? s<label>(3)</label></formula><p>where ? ? R is the learning rate. The update procedure is repeated util convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transferring Model parameters</head><p>The shared layers of the SHL-Model are transferable to an unseen target language <ref type="bibr" target="#b28">[30]</ref>. An acoustic model trained for the unseen target language is called a target model as shown in the right of Fig. <ref type="figure" target="#fig_0">1</ref>. The target model consists of shared layers of the SHL-Model and target language specific layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED LANGUAGE-ADVERSARIAL TRANSFER LEARNING METHOD</head><p>The SHL-Model divides the feature space into shared and private spaces. However, the shared spaces may contain some unnecessary language dependent features. A good representation for cross-lingual knowledge transfer is one for which an algorithm can not learn to identify the language origin of the input observation <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b31">[33]</ref>. Thus, we jointly optimize the shared layers of the SHL-Model model via adversarial training. An adversarial loss is used to prevent the shared space from containing language specific features. This training strategy is called language-adversarial training. It is to find a representation of the samples where all the languages are as indistinguishable as possible. Therefore, the adversarial SHL-Model is proposed to realize this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adversarial Shared Hidden Layer Model</head><p>The adversarial SHL-Model is the SHL-Model which has an additional adversarial language discriminator with the gradient reversal layer (GRL) <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b37">[39]</ref>. The adversarial SHL-Model is depicted in the left of Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The language discriminator is used to recognize the language label of each frame using the shared features. The outputs of the shared layers are the inputs of the language discriminator through GRL. The language discriminator is implemented as a fully connected (FC) neural network with a single hidden layer. The activation function of the hidden layer is rectified linear units (ReLU) <ref type="bibr" target="#b38">[40]</ref>.</p><p>The GRL is introduced to ensure that the feature distributions over all the languages are as indistinguishable as possible for the language discriminator. So the adversarial SHL-Model is to learn a representation that can generalize well from one language to another. They ensure that the internal representation of the shared layers contains no discriminative information about the origin of the input. Thus the shared layers can learn more language invariant features. The language invariant features will be helpful for the target language. discriminator. FC denotes the fully connected layer. The gradient reversal layer (GRL) is introduced to ensure the feature distributions over all the languages are as indistinguishable as possible for the language discriminator. The output labels of the language discriminator are language labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial Training</head><p>In adversarial training procedure, a language discriminator is used to recognize the language label. Since the GRL is below the language classifier, the gradients minimizing language classification errors are passed back with an opposite sign to the shared hidden layers. Thus, it ensures the feature distributions over all the languages are as indistinguishable as possible for the language discriminator.</p><p>Given an additional language label for each training sample {x</p><formula xml:id="formula_4">(m) i , y (m) i</formula><p>, m}, where m ? {1, ..., M } denotes the language label for each frame, and M is the total number of language labels. The loss function of the language discriminator is formulated as:</p><formula xml:id="formula_5">L Adv (? a , ? s ) = - M m=1 Nm i=1 logP (m|x (m) i ; ? a , ? s )<label>(4)</label></formula><p>where ? a denotes the parameters of the FC and softmax layer of the language discriminator, ? s denotes the parameters of the shared layers.</p><p>Although the language classifier is optimized to minimize the language classification error, the gradient of the language classifier is negative so that the bottom shared layers are trained to be language independent. Therefore, the parameters of the language classifier are updated as:</p><formula xml:id="formula_6">? a ? ? a -? ?L Adv ?? a<label>(5)</label></formula><formula xml:id="formula_7">? s ? ? s + ? ?L Adv ?? s<label>(6)</label></formula><p>where ? ? R is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Language-adversarial Training</head><p>The language-adversarial training is to jointly optimize the two loss functions L M ul (? m , ? s ) and L Adv (? a , ? s ). Unlike the standard multilingual training where the shared representation is trained to maximize the classification accuracies of the primary and other languages, the parameters of the shared layers are optimized in order to minimize the loss of the senone classifiers and maximize the loss of the language discriminator. However, the latter works adversarially to the language discriminator by GRL. Thus it encourages language invariant features to emerge in the course of the optimization. So the shared features become senone discriminative and language invariant. The improved language invariance leads to the improved performance of the target language. So the loss function of the adversarial SHL-Model is defined as:</p><formula xml:id="formula_8">L(? m , ? a , ? s ) = L M ul (? m , ? s ) + ?L Adv (? a , ? s ) (7)</formula><p>where ? ? R is the loss weight.</p><p>The GRL has no parameters associated with it. At the feedforward stage, the GRL acts as an identity transformation. During the back-propagation, however, the GRL takes the gradient from the subsequent level and changes its sign, i.e., multiplying by -1, before passing it to the preceding layer. In other words, the GRL reverses the gradient (multiplies -?). So the parameters are updated as:</p><formula xml:id="formula_9">? m ? ? m -? ?L M ul ?? m (8) ? a ? ? a -?? ?L Adv ?? a<label>(9)</label></formula><formula xml:id="formula_10">? s ? ? s -?( ?L M ul ?? s -? ?L Adv ?? s ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where ? is gradually increased from 0 to 1 as epoch increases so that the model is stably trained <ref type="bibr" target="#b31">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-lingual Knowledge Transfer Learning</head><p>Cross-lingual knowledge transfer learning is a special case of transfer learning. The knowledge is referred to as shared model parameters in this paper. With the help of adversarial learning, the shared layers can learn language invariant features easily. The language invariant parameters can be viewed as off-the-shelf knowledge used for the unseen new languages.</p><p>There are several methods proposed to transfer shared layers to the target model. One kind of the methods is widely used in previous work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b28">[30]</ref>. This method is to initialize all the hidden layers of the target model using the shared layers. The softmax layer of the target model is randomly initialized. The other kind of methods is to initialize part of the hidden layers of the target model using the shared layers. Another part of the hidden layers and softmax layer are target language dependent. The share hidden layers can be sequence or parallel with target language specific hidden layers <ref type="bibr" target="#b39">[41]</ref>, <ref type="bibr" target="#b45">[47]</ref>. In this paper, the target model consists of share hidden layers and target language specific hidden layers as shown in the right of Fig. <ref type="figure" target="#fig_1">2</ref>. The output of the shared layers is the input of the target language specific hidden layers. The target model is fine-tuned using the standard BPTT algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A series of experiments are conducted on IARPA Babel datasets to evaluate the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Our experiments are conducted on the datasets of IARPA Babel program. The IARPA Babel datasets consist of conversational telephone speech for 28 languages collected across a variety of environments. The speech is collected in reallife scenarios and recorded under different conditions, such as mobile phone conversation made on the street. Most of the languages contain a small amount of data collected using a distant microphone. The total amount of transcribed audio data varies depending on the language and condition.</p><p>Only 15 languages from the Babel datasets are available for us. Therefore, we select 12 languages as the source languages. All the source languages are the full language pack (FLP), which are only used to train the source models. We also select 3 languages as the target languages: Pashto, Turkish, Vietnamese. The FLP and the limited language pack (LLP) of the target language are both used to train the target models, respectively. Table <ref type="table" target="#tab_1">I</ref> describes experimental data statistics.</p><p>Each language has a training set and dev set. The training sets of in-languages are available for the target language. The parameters of all the models are updated on the training set. The dev set is used to adjust hyper-parameters and select models. All the results of the target models are reported in terms of WER on 10-hours dev set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>Our experiments are conducted using the Kaldi speech recognition toolkit <ref type="bibr" target="#b30">[32]</ref> and the open source deep learning framework called TensorFlow <ref type="bibr" target="#b40">[42]</ref>. The Gaussian mixture model hidden Markov models (GMM-HMM) are trained using the Kaldi toolkit. The decoding of the ASR systems is also performed using Kaldi toolkit. The BLSTM based models are trained using TensorFlow.</p><p>The features are extracted with a 25-ms sliding window with a 10-ms shift. Input features for the GMM-HMM based models consist of 3-dimensional pitch features and 13-dimensional MFCC and their delta and delta-delta. We follow the officially released Kaldi recipe to build GMM-HMM based models for each language. The GMM-HMM based models are used to generate frame-level state alignments for BLSTM based models. The tied triphone states are called senones. The last column of Table I reports the number of senones for each language.</p><p>Language classification usually requires long-term context compared to the ASR task. Various ASR efforts in the last couple of years have shown improved performance with ivector features in addition to the acoustic features in the ASR modeling <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The i-vector approach is also successfully applied to language recognition <ref type="bibr" target="#b41">[43]</ref>. The approach provides an elegant way of reducing high-dimensional sequential input data to a low-dimensional fixed-length feature vector while retaining most of the relevant information. Given that the ivector features carry language information, we use i-vector features to capture long-term context for language identification tasks. There are 15 languages used to train i-vector extractors. We use 19-dimensional MFCC coefficients with energy and their delta and double delta coefficients which results in 60-dimensional feature vectors. More details on ivector extraction can be found in <ref type="bibr" target="#b41">[43]</ref>. The results are reported with 100-dimensional i-vectors in this paper.</p><p>All the BLSTM models use a single frame as the input, with no frame stacking. The BLSTM acoustic models are based on the work in <ref type="bibr" target="#b42">[44]</ref>, where each BLSTM layer consists of peephole connections and a recurrent projection layer. Each BLSTM layer has two directions: the forward direction and the backward direction. Each direction is a regular LSTM layer. The LSTM layer has 320 memory cells and the recurrent projection layer would project the output to 160 dimensions.</p><p>The BLSTM layers are initialized to the range (-0.02, 0.02) with a uniform distribution. We use the BPTT learning algorithm to compute parameter gradients. Each update is based on 20 time-steps of recurrent forward-propagations and backpropagations. Apart from clipping the activations of memory cells to range [-50, 50], we do not limit the activations of other units, the weights or the estimated gradients. The training terminates, if only a little improvement between two epochs has been observed.</p><p>The 3-gram language models are trained using the transcriptions of the training data for each language. The vocabulary of the language model is the officially released vocabulary from IARPA Babel datasets as listed in Table <ref type="table" target="#tab_1">I</ref>. At the test stage, decoding is performed using fully composed 3-gram weighted finite state transducers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Target Models Trained only using One Target language</head><p>In this section, the target model is trained only using the training data of the target language. The cross-lingual knowledge transferred from the source model is not used to train the target model.</p><p>The target model is BLSTM based monolingual model. The BLSTM model consists of 4 hidden layers, which is called BLSM-4L model. Each BLSTM layer consists of peephole connections and a recurrent projection layer. Each direction has 320 memory cells and the recurrent projection layer would project the output to 160 dimensions. The output labels are language specific senones. The number of the senones for each language is listed in Table <ref type="table" target="#tab_1">I</ref>. The BLSTM models are trained using SGD with a momentum term to minimize the crossentropy criterion. The initial learning rate and momentum are set to 0.003 and 0.9, respectively. The learning rate is exponentially decayed during training. The dropout method is applied to regularize the target model. The dropout rate is fixed at 0.5. All the models are trained on the LLP and FLP datasets, respectively.</p><p>In the first group of experiments, we only use 3-dimensional pitch and 40-dimensional log mel-filter bank (Fbank) features plus their delta and delta-delta parameters as input features to train the BLSTM models. The results of the three target monolingual models on the dev data set are listed in Table <ref type="table" target="#tab_2">II</ref>.</p><p>In the second group of experiments, we use 100-dimensional i-vectors features in addition to the above-mentioned acoustic features to train the three target models. The results of the target models on the dev data set are listed in Table <ref type="table" target="#tab_3">IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Target Models Trained with Knowledge from SHL-Model</head><p>In this section, we conduct a series of experiments to evaluate the performance of the target models trained with cross-lingual knowledge from the SHL-Model. We select 4 languages from the Babel datasets as the source languages, which are composed of Assamese, Bengali, Kurmanji and Lithuanian. The source languages belong to the same language family (Indo-European). All the source languages are the FLP datasets, which are only used to train the source models.</p><p>The architecture of the SHL-Model is shown in the left of Fig. <ref type="figure" target="#fig_0">1</ref>. It has language specific hidden layers. It consists of 2 shared BLSTM hidden layers and 2 language specific BLSTM hidden layers prior to softmax layers. Each BLSTM layer consists of peephole connections and a recurrent projection layer. Each direction has 320 memory cells and the recurrent projection layer would project the output to 160 dimensions. The output labels are language specific senones. The number of the senones for each source languages is listed in Table <ref type="table" target="#tab_1">I</ref>.</p><p>The SHL-Model is trained using SGD with a momentum term to minimize the cross-entropy criterion. The initial learn- The shared hidden layers of SHL-Model are transferred to the target models. The target model consists of 2 transferred layers and 2 language specific BLSTM hidden layers. The configuration of each BLSTM layer is identical to the BLSTM layer of monolingual BLSTM-4L model. The output units of the softmax layer are listed in Table <ref type="table" target="#tab_1">I</ref>. There are two finetuning methods for the target model: Private and Overall.</p><p>Private: At first, the BLSTM layers are initialized to the range (-0.02, 0.02) with a uniform distribution. The softmax layers are randomly initialized. The parameters of the transferred layers are fixed. Then, only the parameters of the private layers are fine-tuned using the training data of the target language. The private layers consist of the language specific BLSTM layers and one softmax layer.</p><p>Overall: At first, the BLSTM layers are initialized to the range (-0.02, 0.02) with a uniform distribution. The softmax layers are randomly initialized.Then, all the layers are finetuned using the training data of the target language.</p><p>The target model is fine-tuned using the standard BPTT algorithm on target training data. The dropout rate is fixed at 0.5. When the target model is trained using the LLP dataset, the initial learning rate is set to 0.0005. The initial learning rate is set to 0.001 when the target model is trained using the FLP dataset. The learning rate is exponentially decayed during training.</p><p>Experimental results show that the performance of the target model trained using the Private fine-tuning method outperforms the target model trained using the Overall finetuning method on both LLP and FLP datasets. Therefore, we only report the results using Private fine-tuning method. The results of the target models are reported in Table <ref type="table" target="#tab_2">II</ref>. The results show that all the target models trained with the knowledge transferred from the SHL-Model outperform the monolingual BLSTM-4L models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Target Models Trained with Knowledge from Adversarial SHL-Model</head><p>In this section, a series of experiments are performed to evaluate the performance of the target models trained with cross-lingual knowledge from the adversarial SHL-Model. We also select 4 languages from the Babel datasets as the source languages, which are composed of Assamese, Bengali, Kurmanji and Lithuanian. The source languages belong to the same language family (Indo-European). All the source languages are the FLP datasets, which are only used to train the source models.</p><p>The architecture of the adversarial SHL-Models is shown in the left of Fig. <ref type="figure" target="#fig_1">2</ref>. The network configuration of the adversarial SHL-Model is similar to the SHL-Model. The only difference is that the adversarial model has an additional language discriminator with GRL.</p><p>We also train another source model to compare with the adversarial SHL-Model. This source model is the SHL-Model having an additional language identification without GRL, which is called the LID-SHL-Model. The LID-SHL-Model is trained using the conventional SGD without adversarial loss.</p><p>The language discriminator has one FC layer and a softmax layer. The activation function of the FC layer is ReLU. The FC layer has 2048 nodes. The softmax layer has 4 language labels. The GRL has no parameters.</p><p>The loss weight ? is initiated at 0 and is gradually changed to 1 using the following formula <ref type="bibr" target="#b31">[33]</ref>:</p><formula xml:id="formula_12">? = 2 1 + exp(-? ? p) -1 (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where p is the training progress linearly changing from 0 to 1, ? is set to 10 in all experiments. This strategy allows the language classifier to be less sensitive to noisy signal at the early stages of the training procedure. Note that the ? is used only for updating the shared layers of the source model. However, for updating the language classification component, we use a fixed ? = 1, to ensure that the latter trains as fast as the senone classifiers <ref type="bibr" target="#b31">[33]</ref>. The results of the source languages on dev data sets are reported in Table <ref type="table" target="#tab_2">III</ref>. The results show that the WERs of the source languages on the LID-SHL-Model are lower than the source languages on the SHL-Model. However, the source languages on the adversarial SHL-Model obtains the best performance.</p><p>For the LID-SHL-Model and the adversarial SHL-Model, the configurations of the target models are shown in the right of Fig. <ref type="figure" target="#fig_1">2</ref>, which are identical to the target model for the SHL-Model. The target models are trained using the shared parameters transferred from the LID-SHL-Model and the adversarial SHL-Model, respectively. The results of the target models are reported in Table <ref type="table" target="#tab_2">II</ref>.</p><p>In Table <ref type="table" target="#tab_2">II</ref>, the results show that the target models trained using shared parameters from the adversarial SHL-Model outperform the target models trained using shared parameters from the SHL-Model. The results also show that the target models trained using shared parameters from the adversarial SHL-Model outperform the target models trained using shared parameters from the LID-SHL-Model without adversarial loss.</p><p>The equal error rates (EERs) of the language classifier in source models are listed in the third column of Table <ref type="table" target="#tab_1">VI</ref>. The results show that the performance of the adversarial language classifier is worse than the language classifier significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Target Models Trained with Knowledge from Source Models using I-Vector Features</head><p>In this group of experiments, we use i-vector features with the above acoustic features to train source models and target models. We also select 4 languages from the Babel datasets as the source languages, which are composed of Assamese, Bengali, Kurmanji and Lithuanian.</p><p>Language classification usually requires long-term context compared to the ASR task. The i-vector features carry language information. So we use i-vector features to capture this context.</p><p>There are 15 languages used to train i-vector extractors. The dimension of i-vectors is 100. Therefore, we use 100dimensional i-vectors in addition to 3-dimensional pitch and 40-dimensional Fbank features plus their delta and delta-delta parameters as input features to train source models and target models. The configurations of the source models and target models are similar to the above-mentioned source models and target models, respectively. We only change the input features.</p><p>The EERs of the language classifier in source models with ivector features are listed in the fourth column of Table <ref type="table" target="#tab_1">VI</ref>. The results show that the performance of the adversarial language classifier is worse than the language classifier significantly.</p><p>The results of the source languages on source models are reported in Table <ref type="table">V</ref>. The results show that the WERs of the source languages on LID-SHL-Model are lower than the source languages on SHL-Model. However, the source languages on adversarial SHL-Model outperform the source languages on LID-SHL-Model. From Table V and III, we can find that all source models trained using i-vector features obtain more performance gains compared to the source models trained without i-vector features.</p><p>The results of the target models are listed in Table <ref type="table" target="#tab_3">IV</ref>. The results in Table <ref type="table" target="#tab_3">IV</ref> show that the target models trained using shared parameters from the adversarial SHL-Model outperform the target models trained using shared parameters from the SHL-Model and the LID-SHL-Model without adversarial loss. From Table <ref type="table" target="#tab_3">IV</ref> and II, we can find that all target models trained using i-vector features obtain more performance gains compared to the target models trained without i-vector features.</p><p>Pashto target model trained using knowledge from the SHL-Model achieves up to 10.0 % relative WER reduction over monolingual target model with i-vector features. Pashto target </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. The Effect of Different Number of Source Languages</head><p>In this section, our main concern is to evaluate the performance of adversarial training when the source models are trained on more languages. The number of the source languages in multilingual training was one of the important factors in Babel Project. Therefore, we try to use more source languages to train source models.</p><p>Although previous studies show that the Babel datasets consist of 28 languages, we can only obtain 15 languages from the linguistic data consortium (LDC). So we use 12 languages as source languages and 3 languages as target languages. The source languages are divided into 3 sets. The first set of source languages has 4 languages: Assamese, Bengali, Kurmanji and Lithuanian. The second set of source languages has 8 languages: Assamese, Bengali, Kurmanji, Lithuanian, Tamil, Telugu, Haitian and Tok Pisin. The third set of source languages has 12 languages, which are all the source languages in Table <ref type="table" target="#tab_1">I</ref>.</p><p>The above-mentioned experiments show that all target models trained using i-vector features obtain more improvements compared to the target models without i-vector features. So the input features of the source and target models are 100dimensional i-vectors in addition to 3-dimensional pitch and 40-dimensional Fbank features plus their delta and delta-delta parameters in this section.</p><p>The configurations of the source models and target models are similar to the above-mentioned source and target models, respectively. The only difference is that the source models are trained using more source languages. The results of the target models are reported in Table <ref type="table" target="#tab_4">VII</ref>.</p><p>The results show that the target model trained using the shared layers from the SHL-Model obtains WER reduction when the number of source languages increases. The results also show that the target model trained using the shared layers from the adversarial SHL-Model achieves further performance improvement when the number of source languages increases.</p><p>When the source models are trained using 4 source languages, the target model trained using the knowledge transferred from the adversarial SHL-Model achieves up to 10.1% relative WER reduction compared to the target model trained using the knowledge transferred from the SHL-Model.</p><p>When the source models are trained using 8 source languages, the target model trained using the knowledge transferred from the adversarial SHL-Model achieves up to 9.1% relative WER reduction compared to the target model trained using the knowledge transferred from the SHL-Model.</p><p>When the source models are trained using 12 source languages, the target model trained using the knowledge transferred from the adversarial SHL-Model achieves up to 8.8% relative WER reduction compared to the target model trained using the knowledge transferred from the SHL-Model.</p><p>Previous work <ref type="bibr" target="#b22">[24]</ref> on Pashto FLP condition using the Babel data reported WER as low as 45.7%. But many competitive teams <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS 2013 reported WER of 47.1% or higher. In addition, lots of competitive systems <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref> in NIST OpenKWS 2013 reported WER of 48.1% or higher on Turkish FLP condition. Past work <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref> on Vietnamese FLP condition using the Babel data reported WER as low as 45% on combined systems. But many promising systems in NIST OpenKWS 2013 reported WER of 50% or higher, among them the Babel team SWORDFISH (led by ICSI) reported 55.9% on a single system.</p><p>In our experiments, the best model achieves up to 41.3%, 41.2%, 45.7% WER on Pashto, Turkish and Vietnamese FLP condition, respectively. Our best model also achieves up to 44.4%, 45.8%, 52.8% WER on Pashto, Turkish and Vietnamese LLP condition, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSIONS</head><p>The above experimental results show that the proposed language-adversarial transfer learning is effective. Some interesting observations are made as follows.</p><p>All the target models benefit from both better feature coverage and better initialization via cross-lingual knowledge transfer learning. Cross-lingual knowledge transfer learning is a special case of transfer learning. The parameters of the shared layers transferred from the source model are used to initialize the target model. This is helpful for at least two reasons. One reason is that the target model will have parameters for feature types observed in the source languages as well as the target language. Thus it has better feature coverage. The other reason is that the training objective is non-convex. So this initialization can be helpful in avoiding bad local optima.</p><p>The target model trained utilizing the shared knowledge transferred from the adversarial SHL-Model outperforms the target models trained using the shared parameters transferred from the SHL-Model. Moreover, the target model trained utilizing the shared knowledge transferred from the adversarial SHL-Model also outperforms the target models trained using the shared parameters transferred from the LID-SHL-Model without adversarial loss. This is because the shared hidden layers of the SHL-Model and LID-SHL-Model learn some unnecessary language specific features. The adversarial training makes the shared layers to prevent from learning the language dependent features. So the shared layers of the the adversarial SHL-Model can learn more language invariant features. The language invariant features are helpful for improving the performance of the target model.</p><p>The target model trained using the shared parameters from the SHL-Model obtains WER reduction when the number of source languages increases. Moreover, the target model trained using the shared parameters from the adversarial SHL-Model achieves further performance improvement when the number of source languages increases.</p><p>In summary, all the target models benefit from both better feature coverage and better initialization via transfer learning. Furthermore, the adversarial learning forces the shared hidden layers of the shared-private model to learn more language invariant features. The target models trained using the shared parameters from the adversarial SHL-Model obtain performance gains when the number of source languages increases. Finally, the target model benefits from the language invariant features by language-adversarial transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper proposes language-adversarial transfer learning to improve the performance of low-resource speech recognition tasks. Adversarial learning is used to ensure that the shared layers can learn language invariant features. Experiments are conducted on IARPA Babel datasets. The results show that the target model trained using the knowledge from the adversarial SHL-Model obtains performance improvement by up to 10.1% relative WER reduction over the target model trained using the knowledge transferred from the SHL-Model. The results also show that the target model trained using the shared parameters from the adversarial SHL-Model achieves WER reduction when the number of source languages increases. The current study randomly chooses the source languages. Further work will study how to select source languages effectively. Moreover, it is well known that adversarial learning is difficult to get its best performance. More dedicated algorithm will be studied. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of the conventional cross-lingual knowledge transfer learning method. The left model is the shared hidden layer model (SHL-Model), which is referred to as the source model. The right model is the target model. The shared parameters of the SHL-Model are transferred to the target model. Each language has its own hidden layers and softmax layer. The labels of the softmax layer are language specific senones (tied triphone states).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The architecture of the proposed language-adversarial transfer learning method. The left model is the adversarial SHL-Model, which is referred to as the source model. The right model is the target model. The adversarial SHL-Model denotes that the SHL-Model having an additional language discriminator. FC denotes the fully connected layer. The gradient reversal layer (GRL) is introduced to ensure the feature distributions over all the languages are as indistinguishable as possible for the language discriminator. The output labels of the language discriminator are language labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Zhengqi</head><label></label><figDesc>Wen received his B.S. degree from University Of Science and Technology of China (USTC) in 2008, and got his Doctor degree from Chinese Academy of Sciences (CAS) in 2013. He is currently an Associate Professor in National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences. He current research interests include speech processing, speech recognition and speech synthesis. Ye Bai received his B.S. degree from China Agricultural University in 2016. He is currently a PhD student at University of Chinese Academy of Sciences (UCAS). He current research interests include keywords spotting, language modeling and decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I OVERALL</head><label>I</label><figDesc>EXPERIMENTAL DATA DISTRIBUTIONS.THERE ARE 12 SOURCE LANGUAGES AND 3 TARGET LANGUAGES.</figDesc><table><row><cell>Language Set</cell><cell>Language (Id)</cell><cell cols="2">Language Family Dataset</cell><cell>Training (hours)</cell><cell cols="2">Dev (hours) #Phones</cell><cell>Lexicon Size</cell><cell>#Senones</cell></row><row><cell></cell><cell>Assamese (102)</cell><cell>Indo-European</cell><cell>FLP</cell><cell>61</cell><cell>10</cell><cell>50</cell><cell>23904</cell><cell>4362</cell></row><row><cell></cell><cell>Bengali (103)</cell><cell>Indo-European</cell><cell>FLP</cell><cell>62</cell><cell>10</cell><cell>53</cell><cell>26508</cell><cell>4560</cell></row><row><cell></cell><cell>Kurmanji (205)</cell><cell>Indo-European</cell><cell>FLP</cell><cell>41</cell><cell>10</cell><cell>37</cell><cell>14411</cell><cell>4306</cell></row><row><cell></cell><cell>Lithuanian (304)</cell><cell>Indo-European</cell><cell>FLP</cell><cell>42</cell><cell>10</cell><cell>89</cell><cell>32713</cell><cell>4489</cell></row><row><cell>Source Languages</cell><cell>Tamil (204) Telugu (303)</cell><cell>Dravidian Dravidian</cell><cell>FLP FLP</cell><cell>69 41</cell><cell>10 10</cell><cell>34 50</cell><cell>58470 6306</cell><cell>4810 4281</cell></row><row><cell></cell><cell>Haitian (201)</cell><cell>Creole</cell><cell>FLP</cell><cell>32</cell><cell>10</cell><cell>67</cell><cell>14017</cell><cell>4157</cell></row><row><cell></cell><cell>Tok Pisin (207)</cell><cell>Creole</cell><cell>FLP</cell><cell>39</cell><cell>10</cell><cell>37</cell><cell>2103</cell><cell>4201</cell></row><row><cell></cell><cell>Zulu (206)</cell><cell>Niger-Congo</cell><cell>FLP</cell><cell>62</cell><cell>10</cell><cell>47</cell><cell>60608</cell><cell>4758</cell></row><row><cell></cell><cell>Kazakh (302)</cell><cell>Turkic</cell><cell>FLP</cell><cell>39</cell><cell>10</cell><cell>61</cell><cell>6062</cell><cell>4258</cell></row><row><cell></cell><cell>Georgian (404)</cell><cell>Kartvelian</cell><cell>FLP</cell><cell>50</cell><cell>10</cell><cell>33</cell><cell>35244</cell><cell>4657</cell></row><row><cell></cell><cell>Lao (203)</cell><cell>Tai-Kadai</cell><cell>FLP</cell><cell>66</cell><cell>10</cell><cell>43</cell><cell>6340</cell><cell>4587</cell></row><row><cell></cell><cell>Pashto (104)</cell><cell>Indo-European</cell><cell>FLP LLP</cell><cell>78 10</cell><cell>10 10</cell><cell>44 44</cell><cell>18745 6186</cell><cell>4787 3225</cell></row><row><cell>Target Languages</cell><cell>Turkish (105)</cell><cell>Turkic</cell><cell>FLP LLP</cell><cell>77 10</cell><cell>10 10</cell><cell>42 42</cell><cell>41320 10110</cell><cell>4761 3176</cell></row><row><cell></cell><cell>Vietnamese (107)</cell><cell>Austroasiatic</cell><cell>FLP LLP</cell><cell>88 11</cell><cell>10 10</cell><cell>68 68</cell><cell>6422 3205</cell><cell>4969 3126</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II WERS</head><label>II</label><figDesc>(%) OF THE TARGET MODELS TRAINED USING SHARED PARAMETERS FROM VARIOUS SOURCE MODELS TRAINED USING 4 SOURCE LANGUAGES (ASSAMESE, BENGALI, KURMANJI AND LITHUANIAN).</figDesc><table><row><cell>Source Model</cell><cell></cell><cell>Source Model Setting</cell><cell cols="3">Target Languages (LLP) Pashto Turkish Vietnamese</cell><cell cols="3">Target Languages (FLP) Pashto Turkish Vietnamese</cell></row><row><cell>None</cell><cell></cell><cell>only target language</cell><cell>56.8</cell><cell>54.2</cell><cell>57.4</cell><cell>47.8</cell><cell>45.7</cell><cell>48.6</cell></row><row><cell>SHL-Model</cell><cell cols="2">with language specific hidden layers</cell><cell>52.9</cell><cell>51.9</cell><cell>56.4</cell><cell>45.2</cell><cell>44.1</cell><cell>48.2</cell></row><row><cell>LID-SHL-Model</cell><cell cols="2">+ language identification (LID)</cell><cell>52.3</cell><cell>51.5</cell><cell>56.1</cell><cell>44.9</cell><cell>43.2</cell><cell>48.0</cell></row><row><cell>Adversarial SHL-Model</cell><cell cols="2">+ gradient reversal layer (GRL)</cell><cell>49.1</cell><cell>49.5</cell><cell>55.2</cell><cell>43.5</cell><cell>42.6</cell><cell>47.5</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">WERS (%) OF SOURCE LANGUAGES ON SOURCE MODELS TRAINED USING 4 SOURCE LANGUAGES (ASSAMESE, BENGALI, KURMANJI AND</cell></row><row><cell></cell><cell></cell><cell cols="2">LITHUANIAN).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Source Model</cell><cell>Source Model Setting</cell><cell></cell><cell>Assamese</cell><cell cols="2">Source Languages (FLP) Bengali Kurmanji</cell><cell>Lithuanian</cell></row><row><cell cols="2">SHL-Model</cell><cell cols="2">with language specific hidden layers</cell><cell>48.4</cell><cell>51.5</cell><cell>64.5</cell><cell>64.9</cell></row><row><cell cols="2">LID-SHL-Model</cell><cell cols="2">+ language identification (LID)</cell><cell>48.1</cell><cell>51.3</cell><cell>64.2</cell><cell>64.6</cell></row><row><cell cols="2">Adversarial SHL-Model</cell><cell cols="2">+ gradient reversal layer (GRL)</cell><cell>47.3</cell><cell>50.3</cell><cell>63.2</cell><cell>63.7</cell></row><row><cell cols="3">ing rate and momentum are set to 0.002 and 0.9, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">The learning rate is exponentially decayed during training. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">dropout rate is fixed at 0.1. We use the BPTT learning algo-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">rithm to compute parameter gradients. The training terminates,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">if only a little improvement between two epochs has been</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>observed.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV WERS</head><label>IV</label><figDesc>(%) OF THE TARGET MODELS TRAINED USING SHARED PARAMETERS FROM VARIOUS SOURCE MODELS TRAINED USING 4 SOURCE LANGUAGES (ASSAMESE, BENGALI, KURMANJI AND LITHUANIAN). NOTE: INPUT FEATURES HAS I-VECTORS.</figDesc><table><row><cell>Source Model</cell><cell></cell><cell cols="2">Source Model Setting</cell><cell cols="4">Target Languages (LLP) Pashto Turkish Vietnamese</cell><cell>Target Languages (FLP) Pashto Turkish Vietnamese</cell></row><row><cell>None</cell><cell cols="4">only target language + i-vector</cell><cell>55.9</cell><cell>53.5</cell><cell>56.7</cell><cell>46.9</cell><cell>44.9</cell><cell>47.8</cell></row><row><cell>SHL-Model</cell><cell cols="4">with language specific hidden layers + i-vector</cell><cell>50.3</cell><cell>50.1</cell><cell>55.5</cell><cell>44.1</cell><cell>42.9</cell><cell>47.6</cell></row><row><cell>LID-SHL-Model</cell><cell cols="4">+ language identification (LID)</cell><cell>49.7</cell><cell>49.6</cell><cell>54.9</cell><cell>43.7</cell><cell>42.2</cell><cell>47.2</cell></row><row><cell>Adversarial SHL-Model</cell><cell cols="4">+ gradient reversal layer (GRL)</cell><cell>45.2</cell><cell>47.1</cell><cell>53.7</cell><cell>42.1</cell><cell>41.1</cell><cell>46.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">WERS (%) OF SOURCE LANGUAGES ON SOURCE MODELS TRAINED USING 4 SOURCE LANGUAGES (ASSAMESE, BENGALI, KURMANJI AND</cell></row><row><cell></cell><cell></cell><cell cols="6">LITHUANIAN). NOTE: INPUT FEATURES HAS I-VECTORS.</cell></row><row><cell cols="2">Source Model</cell><cell></cell><cell cols="2">Source Model Setting</cell><cell></cell><cell>Assamese</cell><cell cols="2">Source Languages (FLP) Bengali Kurmanji</cell><cell>Lithuanian</cell></row><row><cell cols="2">SHL-Model</cell><cell cols="4">with language specific hidden layers + i-vector</cell><cell>47.3</cell><cell>50.4</cell><cell>63.4</cell><cell>63.7</cell></row><row><cell cols="2">LID-SHL-Model</cell><cell cols="3">+ language identification (LID)</cell><cell></cell><cell>47.1</cell><cell>50.2</cell><cell>63.1</cell><cell>63.5</cell></row><row><cell cols="2">Adversarial SHL-Model</cell><cell cols="3">+ gradient reversal layer (GRL)</cell><cell></cell><cell>46.2</cell><cell>49.1</cell><cell>62.2</cell><cell>62.6</cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">EER (%) OF LANGUAGE IDENTIFICATION (LID) IN SOURCE MODELS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">TRAINED WITHOUT OR WITH I-VECTOR FEATURES. NOTE: SOURCE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">MODELS ARE TRAINED USING 4 SOURCE LANGUAGES (ASSAMESE,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BENGALI, KURMANJI AND LITHUANIAN)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Model</cell><cell cols="2">Source Model Setting</cell><cell>no i-vector</cell><cell>i-vector</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LID-SHL-Model</cell><cell>LID</cell><cell></cell><cell>24.81</cell><cell>20.54</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adversarial SHL-Model</cell><cell cols="2">+ GRL</cell><cell>81.05</cell><cell>85.81</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII WERS</head><label>VII</label><figDesc>(%) OF THE TARGET MODELS TRAINED USING SHARED PARAMETERS FROM VARIOUS SOURCE MODELS TRAINED USING DIFFERENT NUMBER OF Model also achieves up to 5.3 % relative WER reduction over monolingual target model with i-vector features. The possible reason is that the 4 source languages belong to Indo-European language family. Meanwhile, Pashto is also Indo-European language. So the cross-lingual knowledge transferred from the source model trained on the 4 source languages can be more helpful for Pashto. However, Vietnamese is Austroasiatic languages which is different from Indo-European languages. So the Vietnamese language gets less benefit from the source model trained on the 4 source languages.</figDesc><table><row><cell></cell><cell cols="2">SOURCE LANGUAGES.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Source Languages</cell><cell>Source Model Setting</cell><cell cols="3">Target Languages (LLP) Pashto Turkish Vietnamese</cell><cell cols="3">Target Languages (FLP) Pashto Turkish Vietnamese</cell></row><row><cell>0</cell><cell>only target language + i-vector</cell><cell>55.9</cell><cell>53.5</cell><cell>56.7</cell><cell>46.9</cell><cell>44.9</cell><cell>47.8</cell></row><row><cell></cell><cell>with language specific hidden layers + i-vector</cell><cell>50.3</cell><cell>50.1</cell><cell>55.5</cell><cell>44.1</cell><cell>43.2</cell><cell>47.6</cell></row><row><cell>4</cell><cell>+ language identification (LID)</cell><cell>49.7</cell><cell>49.6</cell><cell>54.9</cell><cell>43.7</cell><cell>42.9</cell><cell>47.2</cell></row><row><cell></cell><cell>+ gradient reversal layer (GRL)</cell><cell>45.2</cell><cell>47.1</cell><cell>53.7</cell><cell>42.1</cell><cell>42.1</cell><cell>46.5</cell></row><row><cell></cell><cell>with language specific hidden layers + i-vector</cell><cell>49.2</cell><cell>48.8</cell><cell>55.1</cell><cell>43.5</cell><cell>42.7</cell><cell>47.3</cell></row><row><cell>8</cell><cell>+ language identification (LID)</cell><cell>48.7</cell><cell>48.3</cell><cell>54.6</cell><cell>43.2</cell><cell>42.4</cell><cell>47.0</cell></row><row><cell></cell><cell>+ gradient reversal layer (GRL)</cell><cell>44.7</cell><cell>46.4</cell><cell>53.2</cell><cell>41.6</cell><cell>41.7</cell><cell>46.2</cell></row><row><cell></cell><cell>with language specific hidden layers + i-vector</cell><cell>48.7</cell><cell>48.1</cell><cell>54.8</cell><cell>43.1</cell><cell>42.1</cell><cell>46.9</cell></row><row><cell>12</cell><cell>+ language identification (LID)</cell><cell>48.2</cell><cell>47.5</cell><cell>54.3</cell><cell>42.8</cell><cell>41.8</cell><cell>46.6</cell></row><row><cell></cell><cell>+ gradient reversal layer (GRL)</cell><cell>44.4</cell><cell>45.8</cell><cell>52.8</cell><cell>41.3</cell><cell>41.2</cell><cell>45.7</cell></row><row><cell cols="2">model trained using knowledge from the adversarial SHL-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Model also achieves up to 19.1 % relative WER reduction over</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">monolingual target model with i-vector features. However,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Vietnamese target model trained using knowledge from the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SHL-Model achieves up to 2.1 % relative WER reduction over</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">monolingual target model with i-vector features. Vietnamese</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">target model trained using knowledge from the adversarial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SHL-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, NOVEMBER 2017</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by the <rs type="funder">National Key Research &amp; Development Plan of China</rs> (No. <rs type="grantNumber">2017YFC0820602</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bsssyYH">
					<idno type="grant-number">2017YFC0820602</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5255" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Acoustic modeling for Google home</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deliverable speech-to-text systems with multilingual knowledge transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="127" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2016 BUT Babel system: Multilingual BLSTM acoustic model with i-vector based adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="719" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Network architectures for multilingual speech representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5295" to="5299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language transfer in language learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Selinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>John Benjamins Publishing Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Spoken Language Technology</title>
		<meeting>Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Spoken Language Technology Workshop</title>
		<meeting>Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual representations for low resource speech recognition and keyword search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kislal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nussbaum-Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding</title>
		<meeting>Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2015</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
	<note>Improving low-resource CD-DNN-HMM using dropout and multilingual DNN training</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved multilingual training of stacked neural network acoustic models for low resource languages</title>
		<author>
			<persName><forename type="first">T</forename><surname>Alumae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3883" to="3887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Kaldi OpenKWS system: Improving low resource keyword search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3597" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual training of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7319" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual MRASTA features for low-resource keyword search and speech recognition systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tuske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nolden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7854" to="7858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language ID-based training of multilingual stacked bottleneck features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chuangsuwanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual data selection for low resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3853" to="3857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alternative networks for monolingual bottleneck features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5290" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia-Pacific Signal and Information Processing Association Summit and Conference</title>
		<meeting>Asia-Pacific Signal and Information Processing Association Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1322" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitask learning for phone recognition of underresourced languages using mismatched transcription</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative study of bnf and DNN multilingual training on cross-lingual low-resource speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2132" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Data-driven neural network based feature front-ends for automatic speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Dissertations of The Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the use of a multilingual neural network front-end</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scanzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2711" to="2714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semisupervised and cross-lingual knowledge transfer learnings for DNN hybrid acoustic models under low-resource conditions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1315" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motliaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding</title>
		<meeting>Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial multi-criteria learning for chinese word segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1193" to="1203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning of deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2369" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on International Conference on Machine Learning</title>
		<meeting>International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on International Conference on Machine Learning</title>
		<meeting>International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilingual Features Based Keyword Search for Very Low-Resource Languages</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1260" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language recognition in ivectors space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Strategies for vietnamese keyword search</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Models of tone for tonal and nontonal languages</title>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A W</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding</title>
		<meeting>Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Investigation of multilingual deep neural networks for spoken term detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding</title>
		<meeting>Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">She is currently an Assistant Professor in National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences. She current research interests include speech recognition, distributed computing, deep learning and transfer learning. Jianhua Tao received his PhD from Tsinghua University in 2001, and got his M.S. from Nanjing University in 1996. He is currently a Professor in NLPR, Institute of Automation, Chinese Academy of Sciences. His current research interests include speech synthesis and coding methods, human computer interaction, multimedia information processing and pattern recognition. He has published more than eighty papers on major journals and proceedings including IEEE</title>
	</analytic>
	<monogr>
		<title level="s">UCAS) in 2018, and got her M.A. degree from Graduate School of Chinese Academy of Social Sciences</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Jiangyan Yi received her PhD from University of Chinese Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note>She was a senior R&amp;D engineer at Alibaba Group during 2011 to 2014. got several awards from the important conferences. such as</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">He serves as the chair or program committee member for several major conferences</title>
		<author>
			<persName><forename type="first">Ncmmsc</forename><surname>Eurospeech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acii</forename><surname>Icpr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Icmi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Isc-Slp</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ncmmsc</forename><surname>Etc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He also serves as the steering committee member for IEEE Transactions on Affective Computing</title>
		<imprint/>
		<respStmt>
			<orgName>Chinese Journal of Phonetics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
