<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN-Based Chinese NER with Lexicon Rethinking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
							<email>ljzhao16@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Jilian Technology Group (Video++)</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chimelong</forename><surname>Park</surname></persName>
						</author>
						<title level="a" type="main">CNN-Based Chinese NER with Lexicon Rethinking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Character-level Chinese named entity recognition (NER) that applies long short-term memory (LSTM) to incorporate lexicons has achieved great success. However, this method fails to fully exploit GPU parallelism and candidate lexicons can conflict. In this work, we propose a faster alternative to Chinese NER: a convolutional neural network (CNN)-based method that incorporates lexicons using a rethinking mechanism. The proposed method can model all the characters and potential words that match the sentence in parallel. In addition, the rethinking mechanism can address the word conflict by feeding back the high-level features to refine the networks. Experimental results on four datasets show that the proposed method can achieve better performance than both word-level and character-level baseline methods.</p><p>In addition, the proposed method performs up to 3.21 times faster than state-of-the-art methods, while realizing better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of named entity recognition (NER) involves the determination of entity boundaries and the recognition of categories of named entities, which is a fundamental task in the field of natural language processing (NLP). NER plays an important role in many downstream NLP tasks, including information retrieval <ref type="bibr">[Chen et al., 2015]</ref>, relation extraction <ref type="bibr" target="#b0">[Bunescu and Mooney, 2005]</ref>, question answering systems <ref type="bibr" target="#b2">[Diefenbach et al., 2018]</ref>, and other applications. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex compositions, and NE definitions within the nest <ref type="bibr" target="#b3">[Duan and Zheng, 2011]</ref>. Hence, one intuitive way to perform Chinese NER is to first perform word segmentation and then apply word sequence labeling <ref type="bibr" target="#b7">[Yang et al., 2016;</ref><ref type="bibr">He and Sun, 2017]</ref>.</p><p>However, gold-standard segmentation is rarely available in NER datasets, and word segmentation errors negatively impact the identification of named entities [Peng and Dredze,</p><formula xml:id="formula_0">⼴广州 ⼴广州市 市⻓长 ⻓长隆隆 公园 Guangzhou Major Guangzhou City E-GPE M-GPE O B-GPE ？ ？</formula><p>Figure <ref type="figure">1</ref>: Example of word character lattice. RNN-based methods must recurrently encode each character with potential words in the lexicon, which reduces their speed. In addition, they may suffer from conflicts between potential words.</p><p>2015; <ref type="bibr">He and Sun, 2016]</ref>. The development of ways to use lexicon features to better leverage word information for NER has attracted research attention <ref type="bibr">[Passos et al., 2014;</ref><ref type="bibr" target="#b7">Zhang and Yang, 2018]</ref>. In particular, to exploit explicit word information, Zhang and Yang <ref type="bibr">[2018]</ref> introduced a variant of LSTM (lattice-structured LSTM) that encodes all potential words that match a sentence. Because of its rich lexicon information, the lattice LSTM model has achieved state-ofthe-art results on various datasets. Although previous work using RNNs to incorporate lexicons has achieved great success, these methods continue to have difficulty with two issues. First, RNN-based methods fail to fully exploit GPU parallelism because of the recurrent structure, which limits their computational efficiency <ref type="bibr">[Strubell et al., 2017]</ref>.</p><p>Specifically, lattice LSTM <ref type="bibr" target="#b7">[Zhang and Yang, 2018]</ref> employs double recurrent transition computation across the length of the input, one for each character in a sentence and the other for matched potential words in the lexicon. As such, their speeds are limited. Second, they have difficulty dealing with conflicts between potential words being incorporated in the lexicon: one character may correspond to couples of potential words in the lexicon, and this conflict can misguide the model such that it predicts different labels, as shown in Figure <ref type="figure">1</ref>. Because of the nature of the sequential processing in RNNs, it is difficult to determine which word is correct based only on the previous inputs. For example, given part of the sentence "广 州市长隆(Guangzhou Chimelong)," an RNN-based model would be uncertain whether the character "长" is part of the word "市长(Major)" or "长隆(Chimelong)." The matching of the words "市长(Major)" and "长隆(Chimelong)" would guide the character "长" to be identified as 'O' and 'B-GPE,'  respectively. More seriously, this conflict among words is ubiquitous in Chinese NER, and these ambiguities cannot be settled without reference to the whole-sentence context and high-level information <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this paper, we present a novel convolutional neural network with a rethinking mechanism. The first issue is handled using a CNN to process the whole sentence as well as all the potential words in parallel. The intuition is that when the window size of the convolution operation is set to 2, then all the potential words can easily fuse into corresponding positions <ref type="bibr" target="#b4">[Kim, 2014]</ref>. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, words with certain lengths correspond to distinct positions in certain layers. In this manner, characters coupled with potential words can be processed in parallel. The second issue is addressed by the use of a rethinking mechanism <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. Most existing Chinese NER models learn features using only a feedforward structure. They have no chance to modify conflicting lexicon information after seeing the whole sentence. By adding a feedback layer and feeding back the high-level features <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>, this rethinking mechanism can leverage the high-level semantic to refine the weights of embedded words and address the conflicts between potential words. Experimental results on four NER datasets show that the proposed method can achieve better performance than other baseline methods.</p><p>In summary, our contributions are three-fold: 1) We propose a novel CNN structure for incorporating lexicons into Chinese NER, which effectively accelerates the model training by its use of parallelism; 2) we apply a rethinking mechanism to tackle the conflict between potential words in the lexicon, and the proposed model can leverage the highlevel semantic to identify the correct words; 3) experimental results on four datasets demonstrate that the proposed model can achieve better performance, and performs up to 3.21 times faster than state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is inspired by three lines of research: enhancing character-level Chinese NER through lexicon information, improving computational efficiency by parallelism, and refining networks using a rethinking mechanism. Chinese NER with lexicon. To date, using neural networks for NER has been the dominant approach <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref>. For Chinese NER, there have been explicit discussions comparing word-based and character-based methods, which have shown that due to the limited performance of current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts <ref type="bibr" target="#b7">[Zhang and Yang, 2018]</ref>. <ref type="bibr" target="#b7">Zhang and Yang [2018]</ref> exploit an RNNbased lattice structure to simultaneously model characters and corresponding words in a lexicon, which prevents segmentation errors. However, lattice LSTM suffers from inefficiency and word conflict. Our approach extends these ideas from the parallel modeling of characters with potential words and a rethinking mechanism. Hence, our method is more efficient and effective. Improving NER computational efficiency. In general, end-to-end CNNs in NLP have mainly been used for text classification <ref type="bibr" target="#b4">[Kim, 2014]</ref>. For sequence labeling tasks, CNNs have been mainly used for low-level feature extraction <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref> as input for alternative architectures. <ref type="bibr">Recently, Strubell et al. [2017]</ref> proposed iterated dilated convolutional neural networks (ID-CNNs) for sequence labeling with parallel computation. However, ID-CNNs applied a dilated window that skips over every dilation, which makes adding lexicon information impractical. In this work, we propose a novel CNN architecture that can process all the characters and lexicon information in parallel.</p><p>Refining networks with a rethinking mechanism. Previous attempts to use a rethinking mechanism in neural networks have been made in image classification to tackle issues of occlusion and noise <ref type="bibr" target="#b5">[Li et al., 2018]</ref>. <ref type="bibr" target="#b5">Li et al. [2018]</ref> used the output posterior possibilities of a CNN to refine its intermediate feature maps. We extend these concepts from refining networks to tackle conflict among words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexicon Rethinking CNNs</head><p>This paper presents a novel lexicon rethinking convolutional neural network (LR-CNN). The proposed model can process all of the characters in a sentence, as well as all of the potential words that match the sentence in parallel. By adding a feedback layer and feeding back the top-layer features, the rethinking mechanism can leverage the high-level semantic to refine the weights of embedded words and tackle the conflicts between potential words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lexicon-Based CNNs</head><p>CNNs have been shown to be effective for Chinese NER <ref type="bibr">[Strubell et al., 2017]</ref>. We propose the use of CNNs to encode the sentences and lexicons in parallel. We denote the input sentence as</p><formula xml:id="formula_1">C = {c 1 , c 2 , • • • , c M }, with character vocabulary V, where c m ∈ R d is the m-th character embedding. Then C ∈ R d×M denotes the sentence matrix.</formula><p>The potential words in the lexicon that match a sequence of characters can be formulated as w l m = {c m , • • • , c m+l−1 }; as such, the first letter of the word is c m and the length of the word is l.</p><p>Take the sentence in Figure <ref type="figure" target="#fig_0">2</ref> as an example. The sentence consists of five potential words (e.g.,</p><formula xml:id="formula_2">w 2 1 =广州, w 3 1 =广州 市, w 2 3 =市长).</formula><p>Given the sentence C and potential matching words w, we adopt character-level CNNs to encode the character features and attention modules to incorporate the lexicon features. The CNNs apply multiple filters H ∈ R d×2 with a window size of 2 to obtain the bigram information and stack the multiple layers to obtain the multigram information, as follows:</p><formula xml:id="formula_3">C 2 m = tanh( C 1 [ * , m : m + 1], H 1 + b 1 ) C l m = tanh( C l−1 [ * , m : m + 1], H l−1 + b l−1 ),<label>(1)</label></formula><p>where C[ * , m : m + 1] is the m-to-(m + 1)-th column in C and A, B = Tr(AB T ) is the Frobenius inner product. We regard the character embedding layer as the first layer C 1 .</p><p>Similarly, the feature in the l-th layer C l m represents the l gram feature.</p><p>From the perspective of the combination of characters, the l gram feature C l m corresponds to the word w l m (Figure <ref type="figure" target="#fig_0">2</ref>). To incorporate the lexicon feature effectively, we use the vectorbased attention <ref type="bibr" target="#b2">[Chen et al., 2018]</ref> to combine the l gram feature with the word feature, as follows:</p><formula xml:id="formula_4">i 1 = σ(W i C l m + U i w l m + b i ) f 1 = σ(W f C l m + U f w l m + b f ) u 1 = tanh(W u C l m + U u w l m + b u ) i 1 , f 1 = softmax(i 1 , f 1 ) X l m = i 1 u 1 + f 1 C l m ,<label>(2)</label></formula><p>where W , U and b are the parameters of the attention module. refers to the element wise production, and σ represents a sigmoid operation. In this way, the model can process the characters and potential words in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Refining Networks with Lexicon Rethinking</head><p>Although using an attention module is effective to incorporate lexicons, it still has difficulty dealing with the conflicts between the potential words. The model incorporating the potential words in the lower layers cannot refer to the words in the higher layers, due to the hierarchical structure of CNN. Hence, the effective use of high-level features to tackle the ambiguity among the potential words is still a key issue <ref type="bibr" target="#b5">[Ma et al., 2014]</ref>.</p><p>In this work, we treat the features at the top layer of CNN, X L m , as the high-level features. We then use these features to readjust the weights of the lexicon attention module by adding a feedback layer to each CNN layer, as follows:</p><formula xml:id="formula_5">i 2 = σ(W * i C l m + U * i w l m + V i X L m + b * i ) f 2 = σ(W * f C l m + U * f w l m + V f X L m + b * f ) r 2 = σ(W r C l m + U r w l m + V r X L m + b r ) u 2 = tanh(W * u C l m + U * u w l m + b * u ) i 2 , f 2 , r 2 = softmax(i 2 , f 2 , r 2 ) X l m = i 2 u 2 + f 2 C l m + r 2 X L m ,<label>(3)</label></formula><p>where W , U , V and b are the parameters of the rethinking module. To avoid overfitting, W * , U * and b * are the reused parameters of the attention module included to reduce the number of parameters. With high-level features participating in the reweighting of the word vectors and CNN features, the model can learn to address the conflicts between the potential words.</p><p>Consider the case in Figure <ref type="figure" target="#fig_0">2</ref>, where the word vector "市 长(Major)" is a misleading input that can give rise to an incorrect label decision. Without the features of "广州 市(Guangzhou City)" and "长隆(Chimelong)" in the higher layer, the model may focus more on the misleading word vector. However, the rethink vector from the upstream layer can decrease the weight of the word vector "市长(Major)" in the output features, so as to correct the label decision.</p><p>Through the CNN encoder and lexicon attention module, the model obtains the respective l gram features X l m within each layer of the model. These feature values correspond to the multi-scale l-grams, of which some are redundant (i.e., unigram X 1 m , bigram X 2 m , . . . , and L-gram X L m ). In this work, we propose the use of the multi-scale feature attention <ref type="bibr" target="#b6">[Wang et al., 2018]</ref> to adaptively selects the features of different scales in each position of a sentence, as follows:</p><formula xml:id="formula_6">s l m = D d=1 X l m [d], α l m = exp(s l m ) L l=1 exp(s l m ) X att m = L l=1 α l m X l m .<label>(4)</label></formula><p>After being processed by the multi-scale attention module, the final representation</p><formula xml:id="formula_7">X att = [X att 1 , X att 2 , • • • , X att M ]</formula><p>would be fed into the CRF layer for the prediction. During the training, we optimize the parameters of the model to maximize the following conditional likelihood:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predicting NER with CRF</head><formula xml:id="formula_8">L(W, b) = log p(y|C; W, b), (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where y is the true labels of sentence C, and W, b are the parameters of the model. During the decoding process, we search for the label sequence y * with the highest conditional probability:</p><formula xml:id="formula_10">y * = argmax y∈Y(C) p(y|C; W, b)<label>(7)</label></formula><p>4 Experimental Setup</p><p>In this section, we describe the datasets that include the newswire and social media domains. We then detail the baseline methods applied, including the word-and characterbased neural Chinese NER under different settings. Finally, we detail the configuration of the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed method on four datasets, including OntoNotes <ref type="bibr">[Weischedel et al., 2011]</ref>, MSRA <ref type="bibr" target="#b5">[Levow, 2006]</ref>  <ref type="table" target="#tab_1">1</ref>.</p><p>We use the pretrained character embeddings and lexicon embeddings<ref type="foot" target="#foot_1">1</ref> trained using word2vec <ref type="bibr" target="#b5">[Mikolov et al., 2013]</ref>, over the automatically segmented Chinese Giga-Word<ref type="foot" target="#foot_2">2</ref> . The lexicon consists of 704.4k words, containing 5.7k single-character, 291.5k two-character, 278.1k threecharacter words, and 129.1k other words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>As baselines for comparison, we applied several classic and state-of-the-art methods on the four datasets. In addition, to verify the effectiveness of the proposed method, we compare the word-and character-level methods that utilize the bichar, softword, and lexicon features.</p><p>For the dataset without gold segmentation, we first train the open source segmentation toolkit<ref type="foot" target="#foot_3">3</ref> on the training data. We then use it to automatically segment the datasets. Finally, we apply the word-level NER methods on these segmented datasets to evaluate their performances, which are denoted as Auto seg. For the dataset with gold segmentation, we directly apply the word-level NER methods, which are denoted as Gold seg. We also evaluate the character-level methods without using segmentation information, which are denoted as No seg. All the methods evaluated are as follows:</p><p>LSTM. A bi-directional LSTM <ref type="bibr">[Hochreiter and Schmidhuber, 1997</ref>] is used to obtain a left-to-right hidden state − → h w i and a right-to-left hidden state ← − h w i , which are concatenated for the NER prediction.</p><p>CNN. We apply a standard CNN <ref type="bibr" target="#b4">[Kim, 2014]</ref> structure on the character or word sequence to obtain its multiple gram representation for the NER prediction. Word-level model + char + bichar. To extract the morphological and combination information from the words, we first concatenate the character embedding and bigram embedding to represent the character x c m . We then use a bi-directional LSTM on the character sequence to obtain the characterlevel features. Finally, we augment the word-level model by combining word embedding with the character-level features as:</p><formula xml:id="formula_11">x w m = [w m ⊕ x c m ]</formula><p>. Character-level model + bichar + softword. We use the BMES scheme for representing the segmentation. In addition, we use the same bigram feature b m as a word-level model. Then, the character feature can be represented as: <ref type="bibr">CNN [Strubell et al., 2017]</ref> stacks several of the CNN layers that applied a dilated window skips over every dilation. This method achieves great performance in the English NER task. Lattice LSTM. Lattice LSTM <ref type="bibr" target="#b7">[Zhang and Yang, 2018]</ref> can model the characters in sequence and explicitly leverages word information through Gated recurrent cells, which can avoid segmentation errors. The lattice LSTM achieved stateof-the-art performance on the four datasets.</p><formula xml:id="formula_12">x c m = [c m ⊕ b m ⊕ seg(c m )]. Dilated CNN. Dilated</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-Parameter Settings</head><p>For all four of the datasets, we used the Adamax <ref type="bibr" target="#b4">[Kingma and Ba, 2014]</ref> optimization to train our networks. The initial learning rate was set at 0.0015, with a decay rate of 0.05. To avoid overfitting, we employed the dropout technique (50% dropout rate) on the character embeddings, lexicon embeddings and each layer of the CNNs. The character embeddings and lexicon embeddings were initialized by a pretrained embedding and then fine-tuned during the training. The character embedding size and lexicon embedding size were set to 50. For the biggest dataset, MSRA, we used five layers of CNNs with an output channel size of 300. For the other datasets, we used four layers of CNNs with an output channel size of 128. We used early stopping, based on the performance on the development set. Our code are released at https://github.com/guitaowufeng/LR-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>In this section, we detail the performance of the proposed baseline models. We present the results of a series of experiments to demonstrate the effectiveness of the lexicon and rethinking mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method Comparison</head><p>For OntoNotes, gold-standard segmentation is available in the entire dataset. No segmentation is available for the MSRA test sections, nor is the Weibo and Resume datasets. As a result, we study the oracle situations where gold segmentation is given on OntoNotes and evaluate the automatic/no segmentation situations on the other three datasets.  Table <ref type="table" target="#tab_3">2</ref> <ref type="foot" target="#foot_4">4</ref> illustrates a variety of settings for the wordand character-based Chinese NER. In the gold or automatic segmentation settings, the char and bichar features can enrich the word representations in the word-level models to obtain a better performance than those without char and bichar features. In particular, these models can achieve results that are competitive to the state-of-the-art <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr" target="#b6">Wang et al., 2013]</ref>. However, due to the influence of the word segmentation errors, the models on the automatic segmentation dataset would perform worse than those on the gold segmentation dataset. In addition, the gold segmentation is not available in most datasets. Hence, the previous work explores the character-based methods to avoid the need for word segmentation. The LR-CNN outperforms characterlevel lattice LSTM by 0.57% in F1 score. It also results in an almost 3 percent improvement over the LSTM with bichar and softword features. In addition, because the processing of the characters and lexicons are conducted in parallel, LR-CNN has an efficiency advantage (Table <ref type="table" target="#tab_8">6</ref>).</p><p>Tables <ref type="table" target="#tab_4">3, 4</ref>, and 5 present the comparisons between the classic and state-of-the-art methods on the MSRA, Weibo,   <ref type="bibr">et al., 2016]</ref>. He and Sun 2017 leverage the cross-domain and semi-supervised data for the Chinese NER. Our LR-CNN model significantly outperforms these models. In addition, with a better manner to incorporate the lexicons, LR-CNN also outperforms the strong baseline lattice LSTM on all of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficiency Advantage</head><p>Our LR-CNN not only achieves better F1 score results than the baseline models, but it is also faster. Table <ref type="table" target="#tab_8">6</ref> lists the relative decoding time on all four of the development sets, as compared to the lattice LSTM. We report the decoding time using the same batch size for each method. Without the CRF, the LR-CNN decodes up to 3.21 times faster than the lattice LSTM. With Viterbi decoding, the gap closes somewhat, but the LR-CNN still has better efficiency (i.e., about an average of 1.91 times faster than the lattice LSTM). The LR-CNN with CRF is even faster than the lattice LSTM without CRF. In addition, we study the degree of the decline in model performance when the models get rid of the CRF layer. We find that the F1 score of the LR-CNN decreases, on average, by 5.75% on the four datasets. In contrast, the lattice LSTM decreases by 6.10%, showing that the LR-CNN is a better structure for the token encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Influence of Sentence Length</head><p>To investigate the influence of the different sentence lengths, we analyze the performance of the LR-CNN and lattice LSTM on the OntoNotes dataset. The dataset is split into  five parts according to the sentence length. To rule out the impact of the CRF, we evaluate the models by removing the CRF layer. Figure <ref type="figure">3</ref> demonstrates the F1 score and processing speed on the different sentence lengths.</p><p>The results reveal that the LR-CNN outperforms the lattice LSTM on all of the datasets with different sentence lengths. In particular, the F1 score for LR-CNN is nearly 12% greater than that of lattice LSTM when the sentence length is less than 20 characters. Hence, LR-CNN has a much greater advantage than the lattice LSTM in dealing with short sentences. In addition, we evaluate the processing speed of the sentences with different lengths. We find that with the sentence length increases, the speed of the LR-CNN is relatively stable, while the speed of the lattice LSTM decreases substantially. In particular, when processing the sentence of which the length is greater than 100, LR-CNN is 5 times faster than the lattice LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To study the contribution of the main components in the LR-CNN, we ran an ablation study on all four datasets. The results are reported in Table <ref type="table" target="#tab_9">7</ref>. Overall, we find that:</p><p>(1) Lexicons play an important role in character-level Chinese NER. Compared with models incorporating lexicons, the performance of the model without lexicons is seriously degraded, and the F1 score decreases by 9% on the Weibo dataset.</p><p>(2) Rethinking mechanisms can further improve the performance of the models with lexicons, because the rethinking mechanisms can tackle the conflicts between the potential  words that match the same characters. Table <ref type="table" target="#tab_10">8</ref> illustrates two examples where the rethinking mechanism successfully tackles the conflict words and makes correct predictions.</p><p>(3) The rethinking mechanism may benefit models without lexicons. Although the lexicons are not available, the LR-CNN -Lexicon is 5% better than itself without the rethinking mechanism in the F1 score on OntoNotes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a novel convolutional neural network to incorporate lexicons with a rethinking mechanism. This network can model all the characters coupled with the potential words that match the sentence in parallel. To solve the conflicts between the potential words, the proposed model can refine the networks by adding a feedback layer to feed back the high-level features. We evaluate the proposed model on four Chinese NER datasets. The experimental results illustrate that the proposed method can significantly improve the performance when compared with that of the other baseline approaches. In addition, the proposed method is up to 3.21 times faster than the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic of CNN model incorporating lexicons with a rethinking mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Suppose that y = {y 1 , • • • , y M } represents a sequence of labels for sentence C, and Y(C) denotes the set of possible label sequences for C. The probability of the label sequence y is: p(y|X att ; W, b) = M m=1 ψ m (y m−1 , y m , X att ) y ∈Y(C) M m=1 ψ m (y m−1 , y m , X att ) , (5) where ψ i (y m−1 , y m , X att ) = exp(W m X att + b m ) is the potential function and W m and b m are the weight vector and bias, corresponding to label pair (y m−1 , y m ), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Sentence会议八月九日在汕头大学举行The meeting was held at Shantou University on August 9th.Gold seg 会议 八月 九日 在 汕头大学 举行 The meeting, August, 9th, was, Shantou University, held w/o rethinkB E O B M E B E (GPE) 中国 内 陆大省 四川 LR-CNN B E O O O O B E (GPE) 中国 内陆 大省 四川</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell></cell><cell>Type</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>OntoNotes</cell><cell>Sentence Char</cell><cell cols="3">15.7k 491.9k 200.5k 208.1k 4.3k 4.3k</cell></row><row><cell>MSRA</cell><cell>Sentence Char</cell><cell>46.4k 2169.9k</cell><cell>--</cell><cell>4.4k 172.6k</cell></row><row><cell>Weibo</cell><cell>Sentence Char</cell><cell>1.4k 73.8k</cell><cell>0.27k 14.5</cell><cell>0.27k 14.8k</cell></row><row><cell>Resume</cell><cell>Sentence Char</cell><cell>3.8k 124.1k</cell><cell>0.46 13.9k</cell><cell>0.48k 15.1k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, Weibo NER[Peng and Dredze, 2015; He and Sun, 2016], and Resume NER<ref type="bibr" target="#b7">[Zhang and Yang, 2018]</ref>. The splitting methods follow those in<ref type="bibr" target="#b7">[Zhang and Yang, 2018]</ref>.The OntoNotes and MSRA are the newswire domain, where gold-standard segmentation is available in the training data. For OntoNotes, gold segmentation is also available for the development and test data; however, no segmentation is available for the MSRA test data. The Weibo and Resume NER come from social media. There is no segmentation in the Weibo and Resume datasets. A description of the datasets is presented in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results on OntoNotes.</figDesc><table><row><cell>input</cell><cell>Models</cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>Yang et al. 2016</cell><cell></cell><cell cols="3">65.59 71.84 68.57</cell></row><row><cell></cell><cell cols="2">Yang et al. 2016  *   †</cell><cell cols="3">72.98 80.15 76.40</cell></row><row><cell></cell><cell>Che et al. 2013  *</cell><cell></cell><cell cols="3">77.71 72.51 75.02</cell></row><row><cell></cell><cell>Wang et al. 2013  *</cell><cell></cell><cell cols="3">76.43 72.32 74.32</cell></row><row><cell>Gold seg</cell><cell>Word-level LSTM</cell><cell></cell><cell cols="3">76.66 63.60 69.52</cell></row><row><cell></cell><cell>+ char+bichar</cell><cell></cell><cell cols="3">78.62 73.13 75.77</cell></row><row><cell></cell><cell>Word-level CNN</cell><cell></cell><cell cols="3">66.84 62.99 64.86</cell></row><row><cell></cell><cell>+ char+bichar</cell><cell></cell><cell cols="3">68.22 72.37 70.24</cell></row><row><cell></cell><cell cols="5">Word-level Dilated CNN 76.90 63.42 69.51</cell></row><row><cell></cell><cell>Word-level LSTM</cell><cell></cell><cell cols="3">72.84 59.72 65.63</cell></row><row><cell></cell><cell>+ char+bichar</cell><cell></cell><cell cols="3">73.36 70.12 71.70</cell></row><row><cell>Auto seg</cell><cell>Word-level CNN</cell><cell></cell><cell cols="3">54.62 55.20 54.91</cell></row><row><cell></cell><cell>+ char+bichar</cell><cell></cell><cell cols="3">64.69 65.09 64.89</cell></row><row><cell></cell><cell cols="5">Word-level Dilated CNN 74.60 56.31 64.18</cell></row><row><cell></cell><cell>Char-level LSTM</cell><cell></cell><cell cols="3">68.79 60.35 64.30</cell></row><row><cell></cell><cell cols="2">+ bichar+ softword</cell><cell cols="3">74.36 69.43 71.89</cell></row><row><cell></cell><cell>Char-level CNN</cell><cell></cell><cell cols="3">56.78 60.99 58.81</cell></row><row><cell>No seg</cell><cell cols="2">+ bichar + softword</cell><cell cols="3">59.60 65.14 62.25</cell></row><row><cell></cell><cell cols="2">Char-level Dilated CNN</cell><cell cols="3">59.42 57.43 58.41</cell></row><row><cell></cell><cell>Lattice LSTM</cell><cell></cell><cell cols="3">76.35 71.56 73.88</cell></row><row><cell></cell><cell>LR-CNN</cell><cell></cell><cell cols="3">76.40 72.60 74.45</cell></row><row><cell cols="2">Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="2">Chen et al. 2006</cell><cell cols="3">91.22 81.71 86.20</cell></row><row><cell cols="5">Zhang et al. 2006  *  92.20 90.18 91.18</cell></row><row><cell cols="2">Zhou et al. 2013</cell><cell cols="3">91.86 88.75 90.28</cell></row><row><cell cols="2">Lu et al. 2016</cell><cell>-</cell><cell>-</cell><cell>87.94</cell></row><row><cell cols="2">Dong et al. 2016</cell><cell cols="3">91.28 90.62 90.95</cell></row><row><cell cols="2">Lattice LSTM</cell><cell cols="3">93.57 92.79 93.18</cell></row><row><cell cols="2">LR-CNN</cell><cell cols="3">94.50 92.93 93.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Main results on MSRA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Main results on Resume NER. and Resume datasets. The existing classic methods explore the rich handcrafted features [Chen et al., 2006; Zhang et al., 2006; Zhou et al., 2013], character embedding features [Lu et al., 2016], segmentation embedding features [Peng and Dredze, 2016], and radical embedding features [Dong</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Relative test-time speed of different models with CRF (w/ CRF) and without CRF (w/o CRF).</figDesc><table><row><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bat/s</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell cols="2">LR-CNN</cell><cell></cell><cell></cell><cell>10</cell><cell cols="2">LR-CNN</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Lattice LSTM</cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">Lattice LSTM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="2">Sentence length</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sentence length</cell><cell></cell></row><row><cell cols="11">Figure 3: F1 and speed against sentence length. Bat/s refers to the</cell></row><row><cell cols="8">number of batches can be processed per second.</cell><cell></cell><cell></cell></row><row><cell cols="2">Models</cell><cell></cell><cell></cell><cell></cell><cell cols="6">OntoNotes MSRA Weibo Resume</cell></row><row><cell cols="2">LR-CNN</cell><cell></cell><cell></cell><cell></cell><cell>74.45</cell><cell>93.71</cell><cell cols="2">59.92</cell><cell></cell><cell>95.11</cell></row><row><cell cols="3">-Rethink</cell><cell></cell><cell></cell><cell>73.09</cell><cell>93.58</cell><cell cols="2">57.86</cell><cell></cell><cell>95.04</cell></row><row><cell cols="3">-Lexicon</cell><cell></cell><cell></cell><cell>65.31</cell><cell>86.81</cell><cell cols="2">49.58</cell><cell></cell><cell>94.27</cell></row><row><cell cols="4">-Lexicon -Rethink</cell><cell></cell><cell>59.86</cell><cell>87.81</cell><cell cols="2">50.75</cell><cell></cell><cell>94.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>An ablation study of the proposed model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Examples of OntoNotes dataset. Characters with blue and red highlights represent incorrect and correct entities, respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https:// github.com/jiesutd/LatticeLSTM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://catalog.ldc.upenn.edu/ LDC2011T13</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://github.com/lancopku/PKUSeg-python</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">In Table2, 3 and 4, the results with * refer to the model with external labeled data for semi-supervised learning. those with † mean that the model also uses discrete features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&amp;D Program (No. 2018YFC0831105, 2017YFB1002104, 2018YFB1005104), National Natural Science Foundation of China (No. 61751201, 61532011), Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), STCSM (No.16JC1420401, 17JC1420200), ZJLab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mooney ; Razvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP</title>
				<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Che</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
				<imprint>
			<date type="published" when="2006">2013. 2013. 2006. 2006. 2015. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>ACL-IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing sentence embedding with generalized pooling</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<meeting><address><addrLine>Dennis Diefenbach, Vanessa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="page" from="1815" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Maret</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04234</idno>
	</analytic>
	<monogr>
		<title level="m">Hangfeng He and Xu Sun. F-score driven max margin neural network for named entity recognition in chinese social media</title>
				<imprint>
			<publisher>Sepp Hochreiter and Jürgen Schmidhuber</publisher>
			<date type="published" when="1997">2018. 2016. 2016. 2011. 2011. 2016. 2016. 2017. 1997. 1997</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KAIS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Adam: A method for stochastic optimization</title>
				<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014. 2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nanyun Peng and Mark Dredze. Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
				<editor>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</editor>
		<imprint>
			<publisher>Nanyun Peng and Mark Dredze</publisher>
			<date type="published" when="2006">2006. 2006. 2018. 2018. 2016. 2016. 2016. 2016. 2014. 2014. 2013. 2013. 2014. 2015. 2016. 2016. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective bilingual constraints for semi-supervised learning of named entity recognizers</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
				<meeting><address><addrLine>Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin; Philadelphia, Penn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2013. 2013. 2018. 2018. 2011</date>
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition via joint identification and categorization</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN Workshop on Chinese Language Processing</title>
				<imprint>
			<publisher>Zhang and Yang</publisher>
			<date type="published" when="2006">2016. 2016. 2018. 2018. 2006. 2006. 2013. 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
	<note>Word segmentation and named entity recognition for sighan bakeoff3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
