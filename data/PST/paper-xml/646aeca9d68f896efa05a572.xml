<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning Implicit Sentiment with Chain-of-Thought Prompting</title>
				<funder ref="#_TQEehNw">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-18">18 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
							<email>haofei37@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sea-NExT Joint Lab</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bobo</forename><surname>Li</surname></persName>
							<email>boboli@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<email>liuqian@sea.com</email>
							<affiliation key="aff2">
								<orgName type="department">Sea AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sea-NExT Joint Lab</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning Implicit Sentiment with Chain-of-Thought Prompting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-18">18 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.11255v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multihop reasoning ability to infer the latent intent of opinion. Inspired by the recent chainof-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is at https://github.com/scofield7419/ THOR-ISA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis (SA) aims to detect the sentiment polarity towards a given target based on the input text. SA can be classified into explicit SA (ESA) and implicit SA (ISA), where the former type is the current mainstream task, in which the emotional expressions explicitly occur in texts <ref type="bibr" target="#b21">(Pontiki et al., 2014)</ref>. Different from ESA, ISA is much more challenging, because in ISA the inputs contain only factual descriptions with no explicit opinion expression directly given <ref type="bibr" target="#b24">(Russo et al., 2015)</ref>. For example, given a text 'Try the tandoori salmon!', having no salient cue word, almost all existing sentiment classifier 1 predicts a neutral polarity towards 'the tandoori salmon'. Human can easily determine the sentiment states accurately, because we always grasp the real intent or opinion</p><p>The environment of the hotel is so great !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Explicit Sentiment positive</head><p>Try the tandoori salmon !</p><p>Case#1:</p><p>Case#2:</p><p>? Implicit Sentiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning the underlying intent/context</head><p>Tandoori salmon is a dish made with salmon. By saying this, the speaker is recommending the tandoori salmon, mostly because he or she believes the taste of tandoori salmon is good and worth trying. Thus the polarity of tandoori salmon is positive.</p><p>positive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-hop reasoning</head><p>Commonsense reasoning Detecting the explicit and implicit sentiment polarities towards targets.</p><p>Explicit opinion expression helps direct inference, while detecting implicit sentiment requires common-sense and multi-hop reasoning.</p><p>behind the texts. Thus, without truly understanding how the sentiment is aroused, traditional SA methods are ineffective to ISA.</p><p>In fact, it is critical to first discover the hidden opinion contexts to achieve accurate ISA. For the explicit case#1 in Fig. <ref type="figure">1</ref>, it is effortless to capture the overall sentiment picture (e.g., 'environment' is the aspect, 'great' is the opinion), and thus can precisely infer the positive polarity towards the given target hotel. Inspired by such fine-grained sentiment spirit <ref type="bibr" target="#b33">(Xue and Li, 2018)</ref>, we consider mining the implicit aspect and opinion states. For the implicit case#2 in Fig. <ref type="figure">1</ref>, if a model can first infer the key sentiment components, e.g., the latent aspect 'taste', latent opinion 'good and worth trying', the inference of final polarity can be greatly eased. To reach the goal, the capabilities of common-sense reasoning (i.e., infer what is 'tandoori salmon') and multi-hop reasoning (i.e., infer the aspect and then the opinion) are indispensable.</p><p>Fortunately, the recent great triumph of pretrained large-scale language models (LLMs) offers a promising solution. On the one hand, LLMs have been found to carry very rich world knowledge, showing extraordinary ability on commonsense understanding <ref type="bibr" target="#b19">(Paranjape et al., 2021;</ref><ref type="bibr" target="#b16">Liu et al., 2022)</ref>. On the other hand, the latest chain-of- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head><p>The new mobile phone can be just put in my pocket.</p><p>The sentiment polarity of the given sentence is neutral. The sentence simply states that the new mobile phone can be put in the speaker's pocket and does not express any positive or negative sentiment towards the phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Three-hop Reasoning with CoT Prompting</head><p>Given the sentence "The new mobile phone can be just put in my pocket.", which specific aspect of the new mobile phone is possibly mentioned?</p><p>Given the sentence, "The new mobile phone can be just put in my pocket.", the specific aspect of the new mobile phone mentioned in the sentence is the size or portability.</p><p>The specific aspect of the new mobile phone mentioned in the sentence is the size or portability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polarity positive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect Opinion</head><p>Based on the mentioned aspect of size and portability, the sentence implies that the phone is small enough to fit in the speaker's pocket. According to common sense, the implicit opinion of speaker towards the portability is good, because the speaker is able to easily carry the phone with them by placing it in their pocket, and find the phone to be convenient and easy to use.</p><p>The sentiment polarity towards the new mobile phone based on the given sentence is positive. The speaker finds the phone to be convenient and easy to use, implying having a favorable view of the phone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><p>Given the sentence, "The new mobile phone can be just put in my pocket.", Based on the common sense, what is the implicit opinion towards the mentioned aspect of the new mobile phone, and why? pocket. According to common sense, the implicit opinion of speaker towards the portability is good, because the speaker can easily carry the phone with them by placing it in their pocket, and find the phone to be convenient and easy to use. Based on such opinion, what is the sentiment polarity</p><p>Given the sentence, "The new mobile phone can be just put in my pocket.", the specific aspect of the new mobile phone mentioned in the sentence is the size or portability.</p><p>Given the sentence, "The new mobile phone can be just put in my pocket.", Based on the mentioned aspect of size and portability, the sentence implies that the phone is small enough to fit in thespeaker's polarity towards the new mobile phone?</p><p>Reasoning question in prompt :</p><p>Aspect answer (A) Opinion answer (O)</p><p>Figure <ref type="figure">2</ref>: An illustration of our THOR framework for three-hop reasoning of implicit sentiment.</p><p>thought (CoT) idea has revealed the great potential of LMs' multi-hop reasoning <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr" target="#b35">Zhou et al., 2022;</ref><ref type="bibr" target="#b34">Zhang et al., 2023)</ref>, where an LLM with some prompts can do chain-style reasoning impressively. Built on top of all these successes, in this work we implement a Three-hop Reasoning CoT framework (namely THOR) for ISA. Based on an LLM, we design three prompts for three steps of reasoning, each of which respectively infers 1) the fine-grained aspect of the given target, 2) the underlying opinion towards the aspect, and 3) the final polarity. With such easy-to-hard incremental reasoning, the hidden contexts of the overall sentiment picture can be elicited step by step to achieve an easier prediction of final polarity, which effectively alleviates the difficulties of the task prediction.</p><p>To ensure the correctness of each reasoning step, we consider a self-consistency mechanism for CoT inspired by <ref type="bibr">Wang et al. (2022b)</ref>, which is to select the candidate answers (at each step) with high voting consistency of inferred aspect and opinion. For supervised fine-tuning setup, we further propose a reasoning revising method. We use the intermediate reasoning answers as model inputs to predict the final labels, where the supervision from gold labels will teach LLM to generate more correct reasoning. On supervised fine-tuning setup, our Flan-T5 based THOR improves the current best-performing baseline by more than 6% in F1 score, and such margins are further magnified on zero-shot setup. Most strikingly, our GPT3-based THOR with 175B parameters boosts the baseline to a high-to 51.10% increase of F1 score.</p><p>To sum up, this work contributes a multi-hop reasoning solution for implicit sentiment detection, which helps to achieve impressive improvement over the traditional non-reasoning methods. To our knowledge, this is the first attempt to successfully extend the CoT idea to the sentiment analysis community. Our method is simple yet effective, and can be broadly applied to other similar NLP problems without much effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Three-hop Reasoning Framework</head><p>The task of SA (either ESA or ISA) is defined as: given a sentence X with a target term t ? X, a model determines the sentiment polarity y towards t, i.e., positive, neutral or negative. We solve the task using an off-the-shelf LLM with prompt. For the standard prompt-based method, we can construct the following prompt template as LLM's input: Given the sentence X, what is the sentiment polarity towards t?</p><p>The LLM will return the answer ?=argmaxp(y|X, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Chain-of-Thought Prompting</head><p>Now we consider the CoT-style prompt <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr" target="#b12">Fu et al., 2022)</ref> method for multi-step reasoning. Instead of directly asking LLM the final result of y, in our THOR (cf. Fig. <ref type="figure">2</ref>) we hope the LLM infer the latent aspect and opinion information before answering the finale y. We here define the intermediate aspect term a and latent opinion expression o. We construct the three-hop prompts as follows.</p><p>Step 1. We first ask LLM what aspect a is mentioned with the following template:</p><p>C 1 [Given sentence X], which specific aspect of t is possibly mentioned?</p><p>C 1 is the first-hop prompt context. This step can be formulated as A=argmaxp(a|X, t), where A is the output text which explicitly mentions the aspect a.</p><p>Step 2. Now based on X, t and a, we ask LLM to answer in detail what would be the underlying opinion o towards the mentioned aspect a:</p><p>C C 3 is the third-hop prompt context. We note this step as ?=argmaxp(y|X, t, a, o).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Enhancing Reasoning via Self-consistency</head><p>We further leverage the self-consistency mechanism <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b14">Li et al., 2022)</ref> to consolidate the reasoning correctness. Specifically, for each of three reasoning steps, we set the LLM decoder to generate multiple answers, each of which will likely to give varied predictions of aspect a, opinion o as well as the polarity y. At each step, those answers with high voting consistency of inferred a, o or y are kept. We select the one with highest confidence as the context in next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reasoning Revising with Supervision</head><p>We can also fine-tune our THOR when the ondemand training set is available, i.e., supervised fine-tuning setup. We devise a reasoning revising method. Technically, at each step we construct a prompt by concatenating 1) initial context, 2) this step's reasoning answer text and 3) final question, and feed it into LLM to predict the sentiment label instead of going to the next step reasoning. For example, at end of step-1, we can assemble a prompt: [C 1 ,A, 'what is the sentiment polarity towards t?'].</p><p>In the supervision of gold labels, the LLM will be taught to generate more correct intermediate reasoning that is helpful to the final prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Setups We experiment on the benchmark Se-mEval14 Laptop and Restaurant datasets <ref type="bibr" target="#b21">(Pontiki et al., 2014)</ref>, where all the instances are split into explicit and implicit sentiment by <ref type="bibr" target="#b15">Li et al. (2021)</ref>. Since the encoder-style BERT cannot generate texts to support CoT, we use encoder-decoder style Flan-T5<ref type="foot" target="#foot_0">2</ref> as our backbone LLM. We also test with <ref type="bibr">GPT3 (Brown et al., 2020)</ref> and Chat-GPT. <ref type="foot" target="#foot_1">3</ref> We used four versions of Flan-T5: 250M (base), 780M (large), 3B (xl) and 11B (xxl), and four versions of GPT3: 220M, 770M, 3B and 11B. Note that GPT3 does not release the model parameters, so we use it in the prompting manner via the API<ref type="foot" target="#foot_2">4</ref> . This also means that we cannot perform supervised fine-tuning with GPT3. We compare with the current best-performing baselines, including: BERT+SPC <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, BERT+ADA <ref type="bibr" target="#b23">(Rietzler et al., 2020)</ref>, BERT+RGAT <ref type="bibr" target="#b26">(Wang et al., 2020)</ref>, BERT Asp +CEPT <ref type="bibr" target="#b15">(Li et al., 2021)</ref>, BERT+ISAIV <ref type="bibr">(Wang et al., 2022a)</ref> and BERT Asp +SCAPT <ref type="bibr" target="#b15">(Li et al., 2021)</ref>. We consider both the supervised fine-tuning and zero-shot setups. We adopt the F1 as the evaluation metric.</p><p>On the few-shot setup, we re-implement the baselines via their source codes. Our experiments are conducted with 4 NVIDIA A100 GPUs. Results on Zero-shot Reasoning In Table <ref type="table" target="#tab_3">2</ref> we compare the zero-shot performances. We can find that the improvement of both prompt-based and CoT-based methods over the current SoTA baseline increases dramatically. But overall, the CoTbased methods with our THOR show much more significant improvement on ISA. For example, our Flan-T5-11B THOR system gives over 30% F1 average improvement over the best-performing base- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Different Model Sizes of LLMs</head><p>In Table <ref type="table" target="#tab_2">1</ref> and 2 we have witnessed the power by using (very) large LLMs. In Fig. <ref type="figure">3</ref> we study the influence of different LLM scales. We see that with the increasing model scale, the efficacy of our multihop reasoning prompting is exponentially amplified. This coincides much with the existing findings of CoT prompting methods <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr" target="#b35">Zhou et al., 2022;</ref><ref type="bibr" target="#b12">Fu et al., 2022)</ref>, i.e., the larger the LMs, the more significant improvement by CoT. Because when the LLM is sufficiently large, the capabilities on common-sense and multi-hop reasoning are greatly developed and strengthened.</p><p>Improving ChatGPT with THOR The latest birth of ChatGPT has brought revolutionary advancement in NLP and AI community. Here we compare the improvement of our THOR on GPT3 (175B) and ChatGPT, respectively. In Fig. <ref type="figure">4</ref> we show the testing results on 100 testing instances.  has improved them on ISA very considerably.</p><p>Failure Analysis In Fig. <ref type="figure" target="#fig_2">5</ref> we show the error rates of failure cases when using THOR, where we summarize three error types. The Flan-T5-11B LLM gives 48.27% error rate on zero-shot setup, while it goes down to 12.79% when fine-tuned with supervision. Unsupervised-GPT3 (175B) gives similarity low error rate as with Supervised-T5, while the latter fails much frequently on incapability of reasoning. In contrast to Supervised-T5, the majority of failures in Unsupervised-GPT3 comes from the problematic data annotation. Since Supervised-T5 is fine-tuned with supervision of 'false' labels, it may actually learn the spurious correlations but with higher testing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Sentiment analysis has long been a hot research topic in NLP community <ref type="bibr" target="#b18">(Pang and Lee, 2007;</ref><ref type="bibr" target="#b2">Dong et al., 2014;</ref><ref type="bibr">Fei et al., 2020b;</ref><ref type="bibr" target="#b32">Wu et al., 2021;</ref><ref type="bibr">Fei et al., 2022d)</ref>. While the explicit SA models can make predictions based on the opinion expressions effortlessly <ref type="bibr" target="#b31">(Wu et al., 2022;</ref><ref type="bibr">Fei et al., 2022a)</ref>, the implicit SA can be much more tricky due to the hidden opinion characteristics <ref type="bibr" target="#b15">(Li et al., 2021;</ref><ref type="bibr">Wang et al., 2022a)</ref>. And ISA is often more ubiquitous in realistic scenarios. Although efforts have been made to ISA <ref type="bibr" target="#b15">(Li et al., 2021;</ref><ref type="bibr">Wang et al., 2022a)</ref>, existing work can still be limited to the traditional paradigm of inference. As aforementioned, ISA should be addressed via reasoning, i.e., common-sense and multi-hop reasoning. Thus, this work follows such intuition, targeting solving ISA with a multi-hop reasoning mechanism. As a key branch of SA, the fine-grained SA has been well explored <ref type="bibr" target="#b28">(Wang et al., 2017;</ref><ref type="bibr" target="#b13">Li et al., 2018;</ref><ref type="bibr" target="#b20">Peng et al., 2020)</ref>. The idea of fine-grained SA is to break down the SA into several key sentiment elements, including target, aspect, opinion and sentiment polarity, all of which together form a complete sentiment picture in detail <ref type="bibr" target="#b25">(Shi et al., 2022;</ref><ref type="bibr">Fei et al., 2022b</ref><ref type="bibr">Fei et al., , 2021a</ref><ref type="bibr" target="#b3">Fei et al., , 2023))</ref>. This work draws the same spirit of fine-grained SA. We believe the reasoning of implicit sentiment should be an incremental process, inferring the sentiment elements step by step and finally understand the sentiment polarity in an easy-to-hard manner.</p><p>Language model pre-training has received increasing research attention for enhancing the utility of downstream applications <ref type="bibr" target="#b22">(Raffel et al., 2020;</ref><ref type="bibr">Fei et al., 2020a</ref><ref type="bibr">Fei et al., , 2021b</ref><ref type="bibr">Fei et al., , 2022c) )</ref> Most recently, the large-scale language models (LLMs) have shown great potential to the human-level intelligence, e.g., ChatGPT <ref type="bibr" target="#b17">(Ouyang et al., 2022)</ref>. LLMs have extensively demonstrated to exhibit extraordinary abilities on common-sense understanding <ref type="bibr" target="#b19">(Paranjape et al., 2021;</ref><ref type="bibr" target="#b16">Liu et al., 2022)</ref> and multi-hop reasoning <ref type="bibr" target="#b30">(Wei et al., 2022;</ref><ref type="bibr" target="#b35">Zhou et al., 2022)</ref>. This work implements the implicit sentiment reasoning built upon LMs, based on the latest proposed chain-of-thought (CoT) idea. CoT prompting is a gradient-free technique that induces large LMs to produce intermediate reasoning steps leading to the final answer. <ref type="bibr" target="#b30">Wei et al. (2022)</ref> formally study the CoT prompting in language models, in which they elicit LMs to generate coherent series of intermediate reasoning steps that direct to the final answer to the original question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a Three-hop Reasoning prompting framework to achieve the chain-ofthought reasoning process for implicit sentiment analysis. Based on the existing LLM, we design three prompts for three steps of reasoning, each of which respectively infers the fine-grained aspect, the underlying opinion and the final polarity. On the ISA datasets, different LLMs equipped with our THOR show impressive performances over the existing best-performing baselines on both the supervised and zero-shot setups. We show that the larger the LLMs, the more significant improvement by our THOR method.  The gold sentiment label is positive towards the metro station.</p><p>In Fig. <ref type="figure" target="#fig_3">6</ref>, 7 and 8, we show that our THOR successfully induces the ChatGPT to finally give a correct decision on sentiment polarity, where the other two methods fail.</p><p>? Case-II Input text:</p><p>Lunch came with pickels and slaw, no extra charge.</p><p>The gold sentiment label is positive towards Lunch.</p><p>Fig. <ref type="figure" target="#fig_6">9</ref>, 10 and 11 shows the results and the LLM's response, respectively. Our THOR induces the ChatGPT to draw a correct decision on sentiment polarity, but the other two methods still fail.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1:Detecting the explicit and implicit sentiment polarities towards targets.Explicit opinion expression helps direct inference, while detecting implicit sentiment requires common-sense and multi-hop reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Error analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Vanilla prompt-based result for testing case-I.</figDesc><graphic url="image-1.png" coords="8,93.55,70.86,408.19,163.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Result by zero-shot CoT method for testing case-I.</figDesc><graphic url="image-2.png" coords="8,91.28,268.01,412.73,145.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Result by our THOR method for testing case-I.</figDesc><graphic url="image-3.png" coords="9,91.28,73.83,412.72,535.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Vanilla prompt-based result for testing case-II.</figDesc><graphic url="image-4.png" coords="9,93.55,645.35,408.19,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Result by zero-shot CoT method for testing case-II.</figDesc><graphic url="image-5.png" coords="10,91.28,79.32,412.74,181.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Result by our THOR method for testing case-II.</figDesc><graphic url="image-6.png" coords="10,91.28,307.94,412.73,432.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Given the sentence "The new mobile phone can be just put in my pocket.", what is the sentiment polarity towards the new mobile phone?</figDesc><table><row><cell>? Traditional Prompting</cell></row></table><note><p>:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 [C 1 ,A]. Based on the common sense, what is the implicit opinion towards the mentioned aspect of t, and why?C 2 is the second-hop prompt context which concatenates C 1 and A. This step can be written as O=argmaxp(o|X, t, a), where O is the answer text containing the possible opinion expression o.Step 3. With the complete sentiment skeleton (X, t, a and o) as context, we finally ask LLM to infer the final answer of polarity t:C 3 [C 2 ,O]. Based on the opinion, what is the sentiment polarity towards t?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>F1 results on supervised fine-tuning setup. Best results are marked in bold. Scores by model with ? are copied from Li et al. (2021).</figDesc><table><row><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell></row><row><cell>All</cell><cell>ISA</cell><cell>All</cell><cell>ISA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model results on Zero-shot setting. We reimplement the state-of-the-art baselines for the zeroshot performance. 'ZeroCoT' means prompting LLM with the zero-shot CoT, 'let's think step by step'<ref type="bibr" target="#b0">(Brown et al., 2020)</ref>.</figDesc><table><row><cell></cell><cell>Restaurant</cell><cell>Laptop</cell></row><row><cell></cell><cell cols="2">All ISA All ISA</cell></row><row><cell>? State-of-the-art baselines</cell><cell></cell></row><row><cell>BERT+SPC (110M)</cell><cell cols="2">21.76 19.48 25.34 17.71</cell></row><row><cell>BERT+RGAT (110M)</cell><cell cols="2">27.48 22.04 25.68 18.26</cell></row><row><cell cols="3">BERT Asp +SCAPT (110M) 30.02 25.49 25.77 13.70</cell></row><row><cell>? Prompt-based methods</cell><cell></cell></row><row><cell>BERT+Prompt (110M)</cell><cell cols="2">33.62 31.46 35.17 22.86</cell></row><row><cell cols="3">Flan-T5+Prompt (250M) 54.38 41.57 52.06 31.43</cell></row><row><cell>Flan-T5+Prompt (11B)</cell><cell cols="2">57.12 45.31 54.14 33.71</cell></row><row><cell>? CoT-based methods</cell><cell></cell></row><row><cell>Flan-T5+THOR (250M)</cell><cell cols="2">55.86 42.84 52.52 32.40</cell></row><row><cell>Flan-T5+THOR (3B)</cell><cell cols="2">57.33 50.04 56.36 36.16</cell></row><row><cell>Flan-T5+THOR (11B)</cell><cell cols="2">61.87 52.76 58.27 40.75</cell></row><row><cell cols="3">Flan-T5+ZeroCoT (11B) 56.58 47.41 55.53 35.67</cell></row><row><cell>GPT3+THOR (175B)</cell><cell cols="2">81.96 76.55 76.04 73.12</cell></row><row><cell cols="3">Results on Supervised Fine-tuning The com-</cell></row><row><cell cols="3">parisons are shown in Table 1. It is interesting</cell></row><row><cell cols="3">to see that the BERT with prompt learning under-</cell></row><row><cell cols="3">performs the SoTA baseline BERT Asp +SCAPT.</cell></row><row><cell cols="3">Even the Flan-T5-base (250M) with double-size pa-</cell></row><row><cell cols="3">rameters fails to beat the SoTA. BERT Asp +SCAPT</cell></row><row><cell cols="3">is pre-trained on the large-scale sentiment aspect-</cell></row><row><cell cols="3">aware annotation data, thus showing strong capa-</cell></row><row><cell cols="3">bility on SA. But with our THOR CoT prompting,</cell></row><row><cell cols="3">Flan-T5-base clearly outperforms SoTA. Further,</cell></row><row><cell cols="3">when using the larger LLM, i.e., with 11B param-</cell></row><row><cell cols="3">eters, we can find the vanilla prompt-based Flan-</cell></row><row><cell cols="3">T5 surpasses the best baseline. More prominently,</cell></row><row><cell cols="3">Flan-T5-11B with THOR shows significant boosts</cell></row><row><cell cols="3">for ISA, i.e., 7.45%(=79.73-72.28) on Restaurant</cell></row><row><cell cols="3">and 5.84%(=82.43-77.59) on Laptop, with average</cell></row><row><cell cols="3">improvement of 6.65%(7.45+5.84)/2 F1. Also the</cell></row><row><cell cols="3">ablations of the self-consistency and reasoning re-</cell></row><row><cell cols="3">vising mechanisms indicate their importances in</cell></row><row><cell>our THOR method.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>We can see that both LMs shows very high performances on ESA, and the enhancements by THOR are very limited. But prompting-based GPT3 and ChatGPT still fail much on ISA, where our THOR</figDesc><table><row><cell>Sup. T5 (11B)</cell><cell></cell><cell>12.79</cell><cell cols="3">Annotation error</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Reasoning failure</cell></row><row><cell>Unsup. GPT3 (175B)</cell><cell></cell><cell>14.68</cell><cell cols="3">Unsolvable instance</cell></row><row><cell>Unsup. T5 (11B)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.27</cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell cols="2">Error rate (%)</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://huggingface.co/docs/transformers/ model_doc/flan-t5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://chat.openai.com/, Dec. 15, 2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://beta.openai.com/docs/models/gpt-3</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No. <rs type="grantNumber">2022YFB3103602</rs>). The work is supported by the <rs type="institution">Sea-NExT Joint Lab</rs>, and <rs type="person">Alibaba Group</rs> through the <rs type="institution">Alibaba Innovative Research (AIR) Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TQEehNw">
					<idno type="grant-number">2022YFB3103602</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>THOR helps unleash the full power of LLMs only when being integrated into the large enough models, while on the middle or lower size LLMs, the improvement by THOR will be limited to certain extent, due to the emergence nature of LLMs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the robustness of aspect-based sentiment analysis: Rethinking model, data, and training</title>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2022a. Mutual disentanglement learning for joint fine-grained sentiment classification and controllable text generation</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2022b. Inheriting the wisdom of predecessors: A multiplex cascade framework for unified aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<biblScope unit="page" from="4096" to="4103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrofitting structure-aware transformer language model for end tasks</title>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2151" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2021a. Nonautoregressive encoder-decoder neural framework for end-to-end aspect-based sentiment triplet extraction</title>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2021b. Enriching contextualized language model from knowledge graph for biomedical information extraction</title>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lasuie: Unifying information extraction with latent adaptive structure-aware generative language model</title>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="15460" to="15475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matching structure for dual learning</title>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
		<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6373" to="6391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent emotion memory for multi-label emotion classification</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7692" to="7699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Complexity-based prompting for multi-step reasoning</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.00720</idno>
		<idno>CoRR, abs/2210.00720</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On the advance of making language models better reasoners</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.02336</idno>
		<idno>CoRR, abs/2206.02336</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning implicit sentiment in aspect-based sentiment analysis with supervised contrastive pre-training</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="246" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generated knowledge prompting for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3154" to="3169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prompting contrastive explanations for commonsense reasoning tasks</title>
		<author>
			<persName><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4179" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowing what, how and why: A near complete solution for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8600" to="8607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014">2014. SemEval 2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adapt or get left behind: Domain adaptation through BERT language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Engl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4933" to="4941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Irene</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<title level="m">SemEval-2015 task 9: CLIPEval implicit polarity of events. In Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2015">2015. SemEval 2015</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective token graph modeling using a novel labeling strategy for structured sentiment analysis</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4232" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2022a. Causal intervention improves implicit sentiment analysis</title>
		<author>
			<persName><forename type="first">Siyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="6966" to="6977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selfconsistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.11171</idno>
		<idno>CoRR, abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mastering the explicit opinion-role interaction: Syntax-aided neural transition system for unified opinion role labeling</title>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Sixth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11513" to="11521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learn from syntax: Improving pair-wise aspect and opinion terms extraction with rich syntactic knowledge</title>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3957" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Sch?rli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.10625</idno>
		<idno>CoRR, abs/2205.10625</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Qualitative Results Here we present several pieces of real testing examples. We compare THOR with the vanilla prompting method, and the zero-shot CoT method (Prompt + &apos;Lets think step by step&apos;)</title>
		<imprint/>
	</monogr>
	<note>We perform the comparisons based on the ChatGPT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
