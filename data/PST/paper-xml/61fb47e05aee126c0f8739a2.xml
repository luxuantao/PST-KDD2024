<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compiler-Driven Simulation of Reconfigurable Hardware Accelerators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-01">1 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhijing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Neuendorffer</surname></persName>
							<email>stephenn@xilinx.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Xilinx Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Sampson</surname></persName>
							<email>asampson@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>Kernel0 Memory0 Buffer0 Memory1 Kernel0 Memory0 Memory1 Buffer1 Buffer0 Buffer1 Memory</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>Memory0 Buffer0 Memory1 Kernel Memory0 Memory1 Buffer1 Buffer0 Buffer1 Memory</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>Memory0 Buffer0 Memory1 Kernel Memory0 Memory1 Buffer1 Buffer0 Buffer1 Memory</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compiler-Driven Simulation of Reconfigurable Hardware Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-01">1 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.00739v1[cs.PL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Programming Language</term>
					<term>MLIR</term>
					<term>Multi-level Abstractions</term>
					<term>Simulation</term>
					<term>Accelerators</term>
					<term>Reconfigurable Hardware</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As customized accelerator design has become increasingly popular to keep up with the demand for high performance computing, it poses challenges for modern simulator design to adapt to such a large variety of accelerators. Existing simulators tend to two extremes: low-level and general approaches, such as RTL simulation, that can model any hardware but require substantial effort and long execution times; and higher-level application-specific models that can be much faster and easier to use but require one-off engineering effort.</p><p>This work proposes a compiler-driven simulation workflow that can model configurable hardware accelerator. The key idea is to separate structure representation from simulation by developing an intermediate language that can flexibly represent a wide variety of hardware constructs. We design the Event Queue (EQueue) dialect of MLIR, a dialect that can model arbitrary hardware accelerators with explicit data movement and distributed event-based control; we also implement a generic simulation engine to model EQueue programs with hybrid MLIR dialects representing different abstraction levels. We demonstrate two case studies of EQueue-implemented accelerators: the systolic array of convolution and SIMD processors in a modern FPGA. In the former we show EQueue simulation is as accurate as a state-of-the-art simulator, while offering higher extensibility and lower iteration cost via compiler passes. In the latter we demonstrate our simulation flow can guide designer efficiently improve their design using visualizable simulation outputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Hardware accelerators are a central tool for improving efficiency in the post-Moore era. Successful accelerators cannot be designed in a vacuum: realizing their full potential requires simultaneous advances in algorithms and compilers. Co-design between hardware and its accompanying software stack requires a way to rapidly simulate a proposed hardware accelerator before finalizing its design.</p><p>However, standard approaches to hardware simulation, can impede this kind of rapid iteration. For instance, although RTL simulation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b32">[33]</ref> is valuable when the hardware is being finalized at the end of the design process, it tends to be too detailed and too slow to be practical for earlier hardware-software co-design phases. Designers often build custom high-level simulators <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref> for specific applications and architectures that sacrifice accuracy for greater flexibility and faster simulation times. However, these custom simulators specialize for a specific architecture model-changing the modeled hardware, such as introducing a new level in a memory hierarchy, requires rewriting substantial parts of the simulator. Generalpurpose simulation frameworks, in contrast, tend to focus on processor-centric architectures like CPUs and GPUs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>, meaning that modeling a custom accelerator architecture still requires a custom specialized implementation of the simulation logic. Finally, traditional simulators do not expose an intermediate representation, so it can be challenging to integrate them with a compiler stack to measure performance on real software.</p><p>This paper presents a general framework for rapidly implementing high-level simulators for arbitrary hardware accelerators. This framework shares basic discrete-event semantics with many existing simulation systems <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref> and focuses on implementation in a multi-level compiler infrastructure, MLIR <ref type="bibr" target="#b19">[20]</ref>. This implementation enables rapid iteration and efficient, low-effort simulation of generated architectures and is intended to exist as part of an end-to-end toolchain, rather than as a standalone simulation framework.</p><p>The system has two main components: an MLIR dialect for representing hardware accelerators, and a generic simulation engine that interprets those representations. Our core contribution is an event queue (EQueue) dialect in MLIR: an intermediate language that represents accelerators at many levels of detail, from simple first-order models to detailed, multi-component simulations. The dialect focuses on expressing memory allocation, data movement, and parallel execution with event-based control. The dialect is directly executable by a generic timed discrete-event simulation engine, enabling arbitrary architectures to be executed. The simulation can provide estimates of overall execution time taking into account data dependencies and resource limitations. The simulation engine also produces visualizable operation-level traces for detailed performance analysis.</p><p>By building on MLIR, the EQueue dialect leverages a broad ecosystem of transformations, analyses, and other dialects. Designers can quickly prototype compilers from highlevel languages to lower-level accelerator configurations. As a software compiler infrastructure, MLIR also enables analysis and transformation tools that are not possible with one-off simulators. For example, Fig. <ref type="figure" target="#fig_0">1</ref> shows a lowering pipeline for progressive optimization of tensor computations using existing MILR dialects: the high-level Linalg dialect to represent tensor operations, the Affine dialect to express explicit loop tiling, and finally our new EQueue dialect to model explicit data movement among hardware components. Critically, such a lowering pipeline enables simulation at multiple levels of detail: users can get quick-and-dirty performance estimates at the Linalg level on tensor behavior, or they can lower gradually to more detailed EQueue hardware simulations for more accurate but costly estimation.</p><p>Compared to traditional one-off accelerator simulators, we see several advantages in the compiler-based approach:</p><p>1) We can simulate at different points in a compilation flow, representing a hardware at multiple abstraction levels. 2) Changes to the architecture are decoupled from changes to the simulation logic, so design iteration can be easier. 3) By reusing compiler passes with different parameters to transform the architecture, designers can easily switch among different architectures for the same computation. This paper presents the EQueue dialect and its open-source 1 implementation using MLIR. We explain the core constructs via a running example (Section II) and then detail the dialect (Section III) and its simulation engine (Section IV). We show the flexibility of programming with two case studies: a systolic array for deep learning computations , and a model of the AI Engine cores in Xilinx's Versal ACAP fabric. In the first case, we compare our EQueue-based simulator against a traditional, special-purpose systolic array simulator, SCALE-Sim <ref type="bibr" target="#b31">[32]</ref>. The EQueue approach matches its accuracy while offering better flexibility to rapidly change the modeled data flow (Section VI). In the second case, we demonstrate how EQueue's flexibility can guide designers to improve their designs on a real-world reconfigurable 1 https://github.com/cucapra/EventQueue architecture (Section VII).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW BY EXAMPLE</head><p>This section summarizes our simulation flow. We write an EQueue program (Fig. <ref type="figure" target="#fig_3">2a</ref>) and show how our simulation engine executes it. Fig. <ref type="figure" target="#fig_3">2b</ref> depicts the modeled architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure Specification</head><p>The first main part of an EQueue program is a set of structural declarations, which define the hardware resources that make up an accelerator. Fig. <ref type="figure" target="#fig_3">2a</ref> lists an EQueue program describing a toy accelerator in Fig. <ref type="figure" target="#fig_3">2b</ref>. First, create_ * operations instantiate components like processing elements (PEs) and memories. Then, launch operations map work onto this structure to specify the computation.</p><p>We start with structure specification 1 . The program uses several create_ * operations to declare components including processors, memories, and direct memory access (DMA) units. These create_ * operations select from a range of primitive component types (ARMr6, SRAM, Register , etc.). These tags correspond to performance models in the simulation engine: for example, the simulation model for SRAM components has slower warm-up time, slower reads, and higher power usage than the Register model. Programs can assemble these components into hierarchies using create_comp to create a new component and add_comp to add one to an existing component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Control Flow</head><p>The second part of a EQueue program is its control flow. The core operation is launch. The launch operation takes a dependency, a processor component, and a block of code. The simulation engine implements launch by issuing code blocks on processors, which execute sequentially.</p><p>launch or memcpy are event operations executed out of order. Every processor in the simulation has an event queue; launching a code block enqueues it for the given processor. At simulation, the engine checks the dependency and executes code blocks when their dependencies are ready. Processors communicate by spawning events with launch or memcpy.</p><p>For example, the control flow 2 illustrates how the Kernel processor distributes its work to DMA, PE0, and PE1. Fig. <ref type="figure" target="#fig_3">2b</ref> indicates the communication using arrows. DMA, PE0, and PE1 are all independent processors but can communicate through their event queues. As the launch of PE0 and PE1 both depend on mempcy operation of DMA, PE0 and PE1 start simultaneously.    MLIR dialects for core accelerator logic: namely, the addi operation is from MLIR's standard dialect. The MLIR ecosystem offers operations from many abstraction levels, from linear algebra to machine code. These help to express architectures at different levels, from abstract black boxes to low-level implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits. This example shows how</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EQUEUE DIALECT</head><p>We illustrate the EQueue dialect with four parts: modeling hardware, expressing data movement, launching computation and concurrency between controllers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling Structure</head><p>The EQueue dialect lets programs declare structural components to model hardware. For example, this code creates the structure from Fig. <ref type="figure">3</ref>: kernel = equeue.create_proc(ARMr5) mem = equeue.create_mem([4096], 32, 4, SRAM) dma = equeue.create_dma() accel = equeue.create_comp("Memory Kernel DMA", mem, kernel, dma)</p><p>There are three kinds of components: processors, memories, and DMA engines. A processor is a component that can execute commands, via the launch operation described in Section III-D. A DMA component is a specialized processor that is only used for data movement. A memory stores data; its speed is affected by its type, size, and ports. Each create_ * operation encodes component properties in its arguments. For instance, create_mem([4096], 32, 4, SRAM) declares a memory component of SRAM type with 4 banks and 4096 data elements of 32 bits each.</p><p>The create_comp composes smaller components into a hierarchy of larger components. The example code declares a component accel with three subcomponents with the names "Memory", "Kernel" and "DMA". Later, code can look up components in the hierarchy using a get_comp operation: dma = equeue.get_comp(accel, "DMA") Finally, connections model bandwidth constraints:</p><formula xml:id="formula_0">connection = equeue.create_connection(Streaming, 32)</formula><p>The create_connection operation has two arguments: their type and their bandwidth in bytes per cycle. The two types are Streaming, which allows simultaneous reads and writes, and Window, which models a buffer that requires locking for exclusive access. Streaming interfaces typically offer lower latency while windowed interfaces offer higher bandwidth. The simulation engine outputs profiling statistics for each connection's bandwidth utilization over time. The bandwidth limit is optional; the simulation engine can also model infinite-bandwidth connections and still collect statistics, as we show in Section VII-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Explicit Data Movement</head><p>Given the hardware structure, we can specify data movement using allocation, deallocation, and read/write operations on memories. For example, consider two memories: We will model the data movement shown in Fig. <ref type="figure">4</ref>. To associate a buffer with a memory, we use alloc: buffer0 = equeue.alloc(mem0, [64], 32) buffer1 = equeue.alloc(mem1, [64], 32)</p><p>The alloc operation specifies the memory, buffer size in elements, and element size in bits.</p><p>To model data movement, we use read and write operations to transfer data into a buffer, optionally through a connection: data = equeue.read(buffer0, conn) equeue.write(data, buffer1, conn)</p><p>Both operations take a buffer and, optionally, a connection; write also takes the value to write. Here, we use connection  whose bandwidth is 32 bytes per cycle. Finally, programs use dealloc to free buffers:</p><formula xml:id="formula_1">equeue.dealloc(buffer0) equeue.dealloc(buffer1)</formula><p>So far, these operations specify how data moves, but not the processors executing the operations. The next subsection shows how to assign this code to processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Launching Computations</head><p>The launch operation schedules blocks of code onto processors. This code runs a block on the kernel processor: equeue.launch(buffer0, buffer1 = b0, b1) in (kernel){ data = equeue.read(buffer0) equeue.write(data, buffer1) equeue.return }</p><p>The arguments to launch pass resources that the code block, represented by an MLIR region, may access. The code in the region will be dispatched to the particular processor for execution. When the region is executed, the resources will be available, enabling the region to run to atomically run to completion. Although most The code runs sequentially. Fig. <ref type="figure">4a</ref> illustrates the above data movement.</p><p>The memcpy operation is syntactic sugar for a launch that reads and then writes data. memcpy is mostly used with DMA units. Fig. <ref type="figure">4b</ref> shows the data movement in this code: equeue.memcpy (mem0, mem1, dma) Launching a code block enqueues it for later execution; the next section describes how this queueing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Concurrent Event Scheduling</head><p>Concurrency in the EQueue dialect occurs at the granularity of events. While the code within a launch block executes sequentially, it can use event operations to spawn asynchronous, concurrent work. Event operations include launch and memcpy described above, and also logical operations on events: control_start, control_and and control_or.</p><p>Event operations can have dependencies, indicating other events they depend on that must finish before the event can start. launch and memcpy each have one dependency, and control_start has none: it is a special operation for beginning a chain of events. control_or and control_and are ready when any or all of their dependencies finish, respectively.</p><p>During simulation, launching an event pushes it onto a given processor's event queue. Different processors can execute events from their queues in parallel, but each processor only executes one event at a time. Events can launch other events, so simulations can nest launch operations in arbitrary ways to reflect their control hierarchy.</p><p>The EQueue dialect includes an await operation that blocks execution until a different event completes. Finally, a launch block can pass values out with the return operation. Example. Fig. <ref type="figure" target="#fig_7">6</ref> shows an example accelerator that uses concurrent tasks, and Fig. <ref type="figure" target="#fig_6">5</ref> illustrates the timeline of its execution. One ARMr5 processor uses a DMA unit for data transfer and a small MAC kernel using launch operations. The control_ * operations encode the execution order while allowing parallel execution on the DMA and MAC kernel. on start event to issue launch. Fig. <ref type="figure" target="#fig_6">5b</ref> shows when start is generated, the launch is removed from the event queue of ARMr5. Then, control_start and control_or are pushed to the queue of ARMr5, launch is pushed to the queue of kernel , and memcpy is pushed to the queue of dma. This way, the event operations run concurrently since they do not block the execution of other operations in ARMr5's launch block. Finally, since memcpy of dma and launch of kernel both depend on start_event, once it finishes, both memcpy and launch can be issued from their corresponding event queues, as indicated by Fig. <ref type="figure" target="#fig_6">5c</ref>. Fig. <ref type="figure" target="#fig_6">5c</ref> also shows that launch of MAC, after its dependency finishes, is pushed to event queue of MAC. The return operation passes done_compute back to the top level as the result value, ret. Notice that done, the first return value of launch, is the dependency generated by launch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Introducing External Operations</head><p>Sometimes there are special cases where existing MLIR dialects cannot express a specific hardware operation. We introduce op to address this situation: res0, ... = equeue.op("mac", {arg0, arg1, ...}) op takes in a signature specifying the operation name and an arbitrary number of inputs and outputs. Here the signature is "mac", which can be modeled as multiplication and addition in the one cycle in the simulator library. The simulation engine checks the signature to jump to the operation's implementation specifying cycle counts and the simulated behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION</head><p>This section introduces the EQueue simulation engine. Fig. <ref type="figure" target="#fig_8">7</ref> shows an overview of the simulation workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inputs</head><p>The simulation engine takes in an EQueue program. As Fig. <ref type="figure" target="#fig_8">7</ref> shows, an EQueue program is composed of a structure definition and a control flow. Designers can produce EQueue programs by writing simple generators in C++, as we demonstrate in Section VI-A. Alternatively, compilers can translate to EQueue from high-level dialects such as Linalg, as we show in Section VI-D. The infrastructure includes many reusable passes (Section V) to enable these lowering pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Outputs</head><p>The simulation engine outputs a profiling summary and a visualizable tracing file. The profiling summary includes the simulation execution time, the simulated runtime in cycles, read and write bandwidth for each connection, maximum bandwidth, and the total bytes read or written for each memory. We also report a max bandwidth portion for both read and write bandwidth, which is the fraction of the total simulated runtime spent at a channel's maximum bandwidth. The designer can use this statistic to adjust bandwidth accordingly to avoid waste or increase computation utilization.</p><p>The trace is a JSON file with operation-wise records in event trace format <ref type="bibr" target="#b8">[9]</ref>. The Chrome browser can visualize this event trace format <ref type="bibr" target="#b9">[10]</ref>. We show in Section VII on how to use this visualization to address a performance bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Simulation Engine</head><p>The simulation engine loops over four stages: set up entry, check event queue, schedule operation, and finish operation. The first stage sets up an operation entry for each processor's current and next operation. The second stage checks the head of each processor's event queue to decide whether to issue it. The third stage models operations' execution time by updating the time logs in the operation entry. To estimate execution time, the simulation engine uses a state object for each component. For instance, a memory component uses banks, cycles per access, and read/write ports to calculate the time for a read or write operation. Each component uses a schedule queue to track operations and to model delay when contention happens, such as when two concurrent writes contend for the same memory. The final stage models the effect of each operation by resetting its operation entry when it finishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extending the Simulator Library</head><p>The simulation engine implements the primitives for EQueue programs using an extensible set of operation functions and a component library. The interface for operation functions consists of a cycle count and a stall signal. In the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Finish Operation</head><p>[{ "name": "alloc", "cat": "operation" "ph": "B", "ts": 1, "pid": "Processor", "tid": "ARMr5", }, ...]  simulator's third stage, where it schedules operations (see previous section), it queries each operation function to obtain timing information. At this point, the operation function may invoke a component object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The EQueue infrastructure provides a standard library of components, such as SRAM memories and processors. Designers can extend the library with custom components to introduce custom simulation logic. To introduce a cache component, for example, a user would add a new Cache class to the component library and define an operation function to support create_mem("Cache", ...) in EQueue programs. The operation function simply instantiates the cache component object. The Cache class can inherit from a base Memory component class; the user only needs to override a method called getReadOrWriteCycles to determine whether the access is a hit or a miss and report a latency accordingly. The Memory class inherits from a more general Device class that manages one or more scheduling queues to avoid conflicts. In the case when there is conflict, the operation function returns a stall signal instead of a cycle count. By extending these base classes, users can specify arbitrary behavior for components in EQueue programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LOWERING PASSES</head><p>The EQueue dialect provides a set of reusable compiler passes. Designers can combine these passes to build accelerators for simulation. We will show how to use these passes with the case study of systolic array in Section VI-D.</p><p>1) EQueue Read Write Pass: This pass translates load and store in MLIR's Affine dialect to EQueue's read and write.</p><p>2) Allocate Memory Pass: This pass allocates buffers on a specified memory component.</p><p>3) Launch Pass: This pass adds launch operations by taking in a specified processor component and a code block.</p><p>4) Memcpy Pass: This pass adds memcpy operations given specified source and destination buffer and a DMA component.</p><p>5) Memcpy to Launch Pass: This pass changes a memcpy operation to launch with a block containing reads and write s.</p><p>6) Split Launch Pass: This pass splits the specified launch block at the specified place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7) Merge Memcpy Launch Pass:</head><p>This pass merges memcpy to the specified launch operation. It avoids repetition if the launch block accesses the same buffer as the memcpy.</p><p>8) Reassign Buffer Pass: This pass replaces the uses of a buffer to another buffer. For instance, a SRAM read can be replaced with a register read. 9) Parallel to EQueue Pass: This pass converts Affine dialect's parallel to EQueue's launch with event dependencies.</p><p>10) Lower Extraction Pass: This pass unrolls components denotation in vector form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CASE STUDY: SYSTOLIC ARRAY</head><p>Systolic array is a widely-used mapping strategy to implement efficient multiplications and additions among matrices <ref type="bibr" target="#b16">[17]</ref>. As the communication is limited to neighbor processing elements (PEs), there is no cycle wasted on global communication and address matching. Because of their extremely broad design space of application-specific mapping strategies, memory systems, and PE designs, rapid simulation is critical to effectively exploiting systolic array designs.</p><p>In this section, we build and study an EQueue model of a systolic array. We aim to answer these questions about the EQueue dialect for this case study:</p><p>1) Does embedding a simulator into a compiler framework help facilitate exploration of algorithmic mapping options? (Sections VI-B and VI-D) 2) Can the simulation accurately estimate performance?</p><p>(Section VI-C) 3) Is the simulation useful to help designers find the best design and does it scale? (Section VI-E) To answer question 1, we first show how to model a systolic array accelerator for convolutions using a generator that emits a variety of configurations as EQueue programs. We then also demonstrate a lowering pipeline that translates from a high-level MLIR dialect into an EQueue model via a series of reusable compiler passes. For question 2, we compare the EQueue simulation to a state-of-the-art custom simulator. To address question 3, we measure our model to explore a design space of convolution accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background: Dataflows</head><p>A key design decision in a systolic accelerator implementation is the dataflow, which determines how loops in the algorithm are mapped spatially onto processing elements (PEs) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In this case study, we consider three widelyused dataflows: Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS) <ref type="bibr" target="#b35">[36]</ref>. The difference is which tensor remains in each PE's register file: the weights, input feature map (ifmap), or output feature map (ofmap).</p><p>Fig. <ref type="figure" target="#fig_10">8</ref> illustrates the data movement for each dataflow. On each cycle, each PE computes a part of final result and passes the partial result to its neighbor <ref type="bibr" target="#b21">[22]</ref>. We use E h , E w for  the ofmap height and width, F h , F w for the filter height and width, N for number of weights, and C for channels. Fig. <ref type="figure" target="#fig_10">8a</ref> shows that for WS, on each cycle, ifmaps and ofmaps are passed to the neighbor PEs, while each weight is stationary until E h ? E w ifmaps convolve with it:</p><formula xml:id="formula_2">pe[i+1][j].ofmap = pe[i][j].ofmap + pe[i][j]. ifmap * pe[i][j].weight pe[i][j+1].ifmap = pe[i][j].ifmap</formula><p>Fig. <ref type="figure" target="#fig_10">8b</ref> shows IS. On each cycle, weights and ofmaps are passed to the neighbor PEs, while every ifmap is stationary until N weights convolve with it: </p><formula xml:id="formula_3">pe[i+1][j].ofmap = pe[i][j].ofmap + pe[i][j]. ifmap * pe[i][j].weight pe[i][j+1].weight = pe[i][j].weight</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Systolic Array Generator</head><p>This section demonstrates a generator that emits EQueue code to model systolic array architectures. We start with simple parallelism and build up to the full generator to illustrate the simplicity relative to a traditional, custom simulator.</p><p>1) Parallelization: We first show how to construct parallelism using EQueue dialect. We use MLIR's builder API, which lets C++ code construct MLIR programs. This pseudo code shows a generator for a simple parallel architecture: The for loop iterates over the dimension of the processing element (PE) array (arr_height by arr_width). Each PE runs in parallel since they are all "launched" by the same control_start event. On each loop, control_and collects the launch events of the current and previous PE. An await barrier ensures that the current processor waits for all launch events to finish. We later denote this pattern as par_for.</p><p>2) Systolic passing: We next extend the generator to pass values systolically between PEs. We use two stages: one reads values from a buffer and compute results, and a second stage passes values to neighboring PEs. This generator code shows WS dataflow and omits boundary conditions for simplicity: In the read stage, each PE reads ifmap, weight and ofmap values from corresponding buffers and computes an ofmap.</p><formula xml:id="formula_4">//</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In the write stage, the PEs in each column (pe[h][w]) pass weights to the next column (pe[h][w+1]). PEs in a given row write ofmaps to buffers in the next row (pe[h+1][w]).</head><p>3) Model SRAM Bandwidth: So far, we have a complete and cycle-accurate model of the core PE array logic. The next step is to model the PE array's interaction with associated SRAMs to measure read and write bandwidth.</p><p>Extending our EQueue generator, we can change the launch input in our read stage to make the first column of PEs read from an SRAM: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with SCALE-Sim</head><p>To check the accuracy of our systolic array EQueue model, we compare to a validated simulator SCALE-Sim <ref type="bibr" target="#b31">[32]</ref> specific to WS, IS, and OS convolutions on systolic array.</p><p>Fig. <ref type="figure">9</ref> compares the simulated cycles and average bandwidth for our model and SCALE-Sim, both modeling a 4?4 WS systolic array with various ifmap and weight sizes. Our EQueue-based simulation matches SCALE-Sim's results. Benefits and Costs. When exploring design alternatives, an EQueue-based simulator has a lower programming cost than a custom one-off simulator. SCALE-Sim <ref type="bibr" target="#b31">[32]</ref>'s WS and IS implementation have little code overlap: WS is implemented in Python in 569 lines of code (LOC), but switching from WS to IS requires changing 410 LOC. In contrast, our EQueue program for WS is implemented in C++ in 281 LOC only needs 11 LOC to switch from WS to IS.</p><p>In exchange, the one-off simulator has a performance advantage: for experiments in Fig. <ref type="figure">9</ref>, SCALE-Sim takes at most 1.1 second, while the EQueue simulator takes at most 7.2 seconds. The speed comes at the cost of complex modifications while exploring the architectures and algorithm mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Lowering Pipeline</head><p>Rationale. The benefit of a compiler-driven approach is not limited to lowering the bar of programming: more importantly, it makes it possible to program the simulator using compiler passes. Integrating with a compiler stack's shared passes avoids the need for tedious, manual modification to explore different program mappings.</p><p>This section constructs a lowering pipeline that compiles from high-level algorithmic specifications to EQueue hardware models. Critically, the pipeline can produce different dataflows for the same input program by applying different sequences of compiler passes. Implementation. IS, WS, and OS all share a core systolic design: on each cycle, each PE reads a value, modify it, and writes to a neighbor PE. Fig. <ref type="figure" target="#fig_0">10</ref> shows how the systolic dataflows share stages along a lowering pipeline. The first 3 stages (Linalg, Affine, and Reassign) are the same. The final stage (Systolic) diverges, but lowering from Reassign stage allows different dataflows to share lowering passes with different orders and parameters. This way, hardware designers can only implement the highest level abstraction and then explore design spaces with no programming overhead.</p><p>1) Linalg to Affine: We start with a convolution in Linalg dialect, an MLIR dialect that can express arbitrary linear algebra. The Linalg dialect can be first lowered to the Affine dialect with the standard --convert-linalg-to-affineloops, which lowers the convolution to explicit nested loops. We then apply --equeue-read-write to change load and store operations in Affine dialect to read and write in EQueue dialect to model data movement.</p><p>2) Affine to Buffer Reassign: Next, we apply the -allocate-buffer and --reassign-buffer passes to replace direct SRAM reads and writes with PE local register accesses. At this stage, we also flatten the 6 convolutional dimensions (E h , E w , N , F h , F w , C) into 3: E h ? E w , N , F h ?F w * C. This flattening reflects the stationary dimension on PEs for each dataflow: for WS, each weight is stationary on a PE until computed with E h ? E w ifmaps; for IS, each ifmap is stationary for N weights; for OS, each ofmap is stationary until accumulated with F h ? F w ? C ifmaps and weights.</p><p>3) Buffer Reassign to Systolic Array: After flattening, for WS and IS, we first need to copy weights or ifmaps from the SRAM into the PE array registers. We generate the necessary memcpy operations with a --mem-copy pass and merge them with launch operations using --merge-memcpy-launch . Then, we implement systolic communication. For WS, we need to pass the ifmaps and ofmaps to the right and down on every cycle. Similarly, for IS, N weights and ofmaps are passed, while for OS, F h ? F w * C ifmaps and weights are passed. The --split-launch and --reassign-buffer passes implement this systolic communication. Finally, we apply --parallel-to-equeue and --lower-extraction passes to complete lower operations to EQueue dialect.</p><p>The key advantage of the lowering pipeline approach is the reduced effort for implementing different dataflows. In a traditional simulator, changing the mapping strategy requires extensive rewriting of the simulation engine. In the compilerdriven approach, designers can apply different combinations     of reusable passes to try out different dataflows.</p><p>Results. Fig. <ref type="figure" target="#fig_18">11</ref> plots the simulator execution time, simulated runtime, and read and write bandwidth on the four convolution settings at the four stages (Linalg, Affine, Reassign, and Systolic). This compiler pipeline does not take a significant amount of time (it typically finishes in microseconds). The first three lowering stages are identical for different dataflows, so they have the same bandwidth and runtime. This sharing reflects the framework's reusability for common parts of different accelerator implementations. At the final stage, the runtime differs from the simpler generator-based approach from Section VI-B by 1.2% on average, up to 2%. The difference lies in warm-up and cool-down phases that the passes do not model. Register and SRAM bandwidth differs for the same reason.</p><p>Fig. <ref type="figure" target="#fig_18">11</ref> also reflects the transition of hardware at each stage. From Linalg stage to Affine stage, the execution time grows, the runtime reduces, and the SRAM bandwidths grow, since affine stage models explicit nested loops and data movements. At the Reassign stage, we model reads and writes on registers rather than SRAM, so the register bandwidth changes from 0 to 1 byte per cycle and the execution time grows. At systolic stage, we introduce a grid of PEs running concurrently, resulting in higher execution time, lower runtime in cycles and differentiated bandwidth. Benefits. The availability of reusable lowering passes lets designers rapidly switch between program-accelerator mappings and enables efficient design space exploration. In contrast, one-off simulators would require custom modifications to support these transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Scalability Evaluation</head><p>For hardware designers, information on different dataflow performance patterns is essential when designing new mapping strategies. To test our simulator's generality and scalability, we measure runtime and bandwidth for 4,050 combinations of array configuration (A h = 2, 4, 8, 16, 32, A w = 64/A h ) and convolutions (H/W = 2, 4, 8, 16, 32, F h /F w /C = 1, 2, 4, N = 1, 2, 4, 8, 16, 32) on the three dataflows. Simulator scalability. Fig. <ref type="figure" target="#fig_21">12a</ref> plots the simulator execution time versus the cycle counts for each simulation. The execution time is roughly proportional to the cycle count, since      </p><formula xml:id="formula_5">D 1 = F H ?F w ?C, D2 = N for WS, D 1 = F H ?F w ?C, D 2 = E h * E w for IS and D1 = N, D 2 = F H ?F w ?C for OS.</formula><p>With this general rule, we can always get the minimal execution time by choosing the array structure that minimizes loop iterations.</p><p>Benefits. The evaluation on 4050 data points shows that our simulator scales to various convolutions. Algorithm designers can use it to choose the best dataflows and array configuration for a convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CASE STUDY: ACAP AI ENGINE</head><p>A common approach to hardware-software co-design is to start simple and, guided by bottlenecks, build up a more sophisticated architecture. This section uses the EQueue dialect to simulate a real-world architecture: Xilinx's AI Engine in Versal ACAP <ref type="bibr" target="#b34">[35]</ref>. We show our simulation result matches the AI Engine simulator <ref type="bibr" target="#b37">[38]</ref>, while the high-level simulator allows architectures ignore real-word constraints like bandwidth and gradually introduce them with low programming cost. During this process, the EQueue visualized tracing can guide designers to improve their designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Versal ACAP</head><p>Xilinx's Versal adaptive compute acceleration platform (ACAP) is a reconfigurable platform that includes programmable logic, ARM cores, and AI Engines, which are specialized vector units <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>. The AI Engine is a fixed array of interconnected VLIW SIMD processors optimized for signal processing and machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FIR</head><p>A finite impulse response (FIR) filter is a common signal processing primitive that responds to inputs of finite duration. An FIR operation filters and accumulates on a slidingwindow. Given a series of discrete input samples x and N coefficients c, the output samples y are calculated as:</p><formula xml:id="formula_6">y n = N -1 k=0 c k ? x n+k</formula><p>Xilinx's AI Engine programming tutorial <ref type="bibr" target="#b37">[38]</ref> uses a FIR filter as an example to demonstrate the hardware's flexibility and capabilities. In this case study, we implement the same FIR example using the EQueue dialect dialect to demonstrate how the language and simulation engine can easily model an existing programmable architecture. We compare our simulator's reports to those from Xilinx's own, hand-written, closed-source simulator to ground the results.</p><p>The Xilinx FIR tutorial uses a filter with 32 complex, asymmetric coefficients and a digital series of length 512. Each value occupies 32 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Case 1: Unlimited Resources</head><p>We start with a basic 1-processor implementation and use empirical measurements to improve the design. We can use the AI Engine's intrinsics: mul4 and mac4. On each cycle, mul4 computes on 4 parallel lanes to perform 8 multiplications where each lane performs 2 <ref type="bibr" target="#b38">[39]</ref>. mac4 works in the same way. Analytically, therefore, it should take 16 cycles to compute 4 outputs for a filter length of 32.</p><p>We follow Section III-E to self-define mul4 with equeue .op("mul4", {ofmap, ifmap, filter}). In the simulator library, an operation with the "mul4" signature reads from a buffer, computes 4 lanes with 2 computation at each lane per cycle, and writes to the buffer. We define the mac4 operation the same way. This pseudocode shows the MLIR generator for a single-core implementation, where ifmap, ofmap and filter are buffers: start = equeue.control_start() equeue.launch(...) in (start, ai_engine){ equeue.op("mul4", {ofmap, ifmap, filter}) for 0 to 11: equeue.op("mac4", {ofmap, ifmap, filter}) ifmap_tensor = equeue.read(sin) equeue.write(ifmap_tensor, ifmap) for 0 to 4: equeue.op("mac4", {ofmap, ifmap, filter}) ofmap_tensor = equeue.read(ofmap); equeue.write(ofmap_tensor, sout) } Our EQueue simulation reports 2048 cycles to generate 512 outputs, close to Xilinx AI Engine simulator's result of 2276 cycles <ref type="bibr" target="#b37">[38]</ref>. The Xilinx simulator also models other factors in performance, including loop control costs, synchronization overhead, etc. The EQueue simulation engine's throughput is slightly higher because it does not model these overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Case 2: Optimizing Case 1</head><p>The next step for a hardware designer is to incrementally increase the design's complexity to attain higher throughput. In an ideal world, since mul4/mac4 computes 4 lanes, each with 2 operation per cycle, we could pipeline 32/2 = 16 processors to maximize throughput. Due to bandwidth constraints, Xilinx's FIR tutorial simulates 4 processors rather than 16. Using the EQueue model, we can first model the full 16-processor pipelined system and then introduce more realistic constraints to measure their effect on performance.</p><p>The modification to our EQueue program is straightforward. Instead of one processor executing 16 sequential operations, we now create 16 processors, where each processor completes one mul4/mac4 operation. We show the simplified control flow: The simulation engine reports 143 cycles to produce outputs for 512 inputs. This matches the expected performance because pipelining 16 processors requires 15 cycles to warm up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Case 3: Limited Bandwidth</head><p>The AI Engine is constrained by the 32-bit bandwidth of its AXI4-Stream I/O interfaces <ref type="bibr" target="#b38">[39]</ref>. To add bandwidth constraints, we need only add a connection (Section III-A) and update the reads and writes accordingly: conn_in = connection("Streaming", 32); conn_out = connection("Streaming", 32); ... ifmap_tensor = equeue.read(sin, conn_out) equeue.write(ofmap_tensor, sout, conn_out)</p><p>Adding this bandwidth constraint entails only simple, local changes to the EQueue program; extending a custom simulator, in contrast, could require invasive modifications. According to our simulation engine, it takes 588 cycles to generate 512 outputs, including 79 cycles to warm up.</p><p>To understand the reason for reduced throughput, Fig. <ref type="figure" target="#fig_22">13</ref> shows the operation-wise tracing with visualized via the Chrome browser, where green slots are mul operations, red slots are mac operations, blue slots indicate installing and the x-axis shows cycle counts where 1?s stands for one cycle. For every 4 cycles, each processor operation stalls for 3 cycles. The stalls are the result of the 32-bit bandwidth constraint: it takes 4 cycles to transmit 4 inputs, but computation only takes 1 cycle to consume these values. For each AI Engine's attempt to start computation, it waits for its preceding core compute (1 cycle) and pass values to it (4 cycles), so the warm-up stage takes 5 ? 16 -1 = 79 cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Case 4: Optimizing Case 3</head><p>Our bandwidth-constrained model shows that 75% of the hardware's computation power is wasted, i.e., we stall on 3 Our EQueue simulation engine reports that generating 512 outputs requires 538 cycles, which matches Xilinx's simulator result of 539 cycles. Warm-up takes 26 cycles, which is much faster than the previous case. Fig. <ref type="figure" target="#fig_23">14</ref> visualizes the operation trace for the balanced 4-processor system: there is no stalling once the processors have warmed up. Benefits. Typical simulation tools can make it challenging for software designers to identify hardware bottlenecks. This case study advocates an opposite approach: designers can start with a simple design and gradually add real-world constraints to examine their effects on performance. Our EQueue approach requires modest modification at each step, but effectively guides users to improve their design.</p><p>Our EQueue-based approach matches the results of Xilinx's existing AI Engine simulator tool. Thanks to its highlevel abstraction, the EQueue-based simulator is much faster: the 4 processor implementation takes 0.07 seconds, while the AIE simulator first requires 5 minutes for compilation and then 3 minutes for simulation. Also, due to its focus on low-level details, the AI Engine implementation is spread across six separate files. Any updates to the interface or mapping strategy requires substantial work to implement and recompile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>Because simulation is a critical part of a hardware design workflow, it is an old and well-studied research topic. Space precludes a complete census of all approaches to simulation, but we discuss the most closely related techniques here.</p><p>The EQueue simulation flow takes inspiration from hardware modeling languages <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>. It differs from RTL simulation with its higher-level representation and focus on an intermediate representation that can be transformed by compiler passes. We designed the EQueue dialect because existing languages and MLIR dialects cannot represent the core concepts required for flexible, high-level simulation: fine-grained concurrency, contention for shared heterogeneous hardware resources, and data movement constraints.</p><p>RTL simulators: Most RTL development tools have accompanying simulators <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. RTL simulation can faithfully model a complete hardware design, but implementing a design in RTL requires specialized hardware expertise and carries a high engineering burden. An alternative is integrating a more abstract simulation with RTL using a multi-level tool such as PyMTL <ref type="bibr" target="#b22">[23]</ref>. We view the EQueue dialect as a complement to these frameworks that makes it easier to generate and transform higher-level models before completing a more detailed implementation.</p><p>Application-specific simulators: Many efforts have constructed architecture-specific simulators, for domains including sparse linear algebra <ref type="bibr" target="#b3">[4]</ref>, stencils <ref type="bibr" target="#b5">[6]</ref>, and DNN inference <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>. While these simulators are fast and accurate, they are challenging to construct from scratch. The EQueue dialect dialect provides a faster way to build simulators.</p><p>MLIR methodology: It is appealing to use existing MLIR dialects that already offers various transformations and ways to express computations and hardware. For example, MLIR's Async dialect <ref type="bibr" target="#b24">[25]</ref> models asynchronous execution. It cannot, however, associate code with specific processing units in a hardware structure. CIRCT <ref type="bibr" target="#b6">[7]</ref> is an ongoing project to apply MLIR's methodology to hardware design tools. It encompasses many dialects, including a Handshake dialect representing asynchronous processes that can compile to a FIRRTL dialect for circuit-level transformations and then to LLHD dialect to describe RTL. The HIR dialect <ref type="bibr" target="#b23">[24]</ref> describes hardware with explicit scheduling and binding, which serves as a better IR than pure LLVM for HLS-like compilation from software to hardware. Both HIR and the CIRCT dialects are abstractions for generating concrete hardware implementations, not high-level abstractions for modeling concurrency and data movement for efficient simulation. The EQueue dialect differs by explicitly representing execution units and mapping event-triggered computations onto them.</p><p>Compiler-driven DSE: Compilation is an efficient way to perform design space exploration (DSE), especially in the specific domain of tensor computations. Interstellar <ref type="bibr" target="#b42">[43]</ref> uses Halide <ref type="bibr" target="#b29">[30]</ref> to explore DNN accelerator designs. Union <ref type="bibr" target="#b13">[14]</ref> uses MLIR programs as inputs to optimize spatial DNN accelerators by analyzing tensor operations expressed in Linalg or Affine dialect with MAESTRO <ref type="bibr" target="#b17">[18]</ref> and Timeloop <ref type="bibr" target="#b25">[26]</ref> as cost models. Similar to the EQueue methodology, these frameworks benefit from separating modeling from representation for rapid iteration. However, all of these approaches target a specific category of computation and hardware: they map high-level DNN dataflow mappings to synchronous PE arrays of regular structures. The EQueue dialect aims to address general hardware simulation, including programmable architectures that do not resemble systolic arrays, such as the AI Engine (Section VII). EQueue may also be a good fit for extending Union with support for explicit representations of hardware components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>Hardware simulation frameworks need a separation of simulation from representation. The simulation flow for EQueue programs lowers the bar for designers with abstract representations, exposes intermediate optimization stages, and makes it easy to apply changes with reusable compiler passes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of EQueue simulation on different levels of hardware abstraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>EQueue describes accelerator structure and control flow separately from the simulation logic. Designers can change the architecture without needing to modify the simulation engine, which reduces the cost of exploring alternative designs. It also shows how EQueue programs can intermix code from other .control_start() done = equeue.launch(...) in (start, kernel){ copy_dep = equeue.control_start() launch_dep = equeue.memcpy(copy_dep, DMA, ?) pe0_dep = equeue.launch(...) in (launch_dep, pe0){ ... ofmap = addi(ifmap, 4) ... } pe1_dep = equeue.launch(...) in (launch_dep, pe1) { ? } equeue.await(pe0_dep, pe1_dep) equeue.return() } kernel = equeue.create_proc(ARMr6) sram = equeue.create_mem(SRAM, [64], 4) dma = create_dma() accel = create_comp("Kernel SRAM DMA", host, sram, dma) pe0 = equeue.create_proc(MAC) reg0 = equeue.create_mem(Register,<ref type="bibr" target="#b3">[4]</ref>, 4) pe1 = equeue.create_proc(MAC) reg1 = equeue.create_mem(Register,<ref type="bibr" target="#b3">[4]</ref>, 4) add_comp(accel, "PE0 Reg0 PE1 Reg1, pe0, reg0, pe1, reg1) An EQueue program as input to our generic simulator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Modeling an accelerator with an EQueue program and the model created by the simulation engine. Code listings omit types and the % prefix for legibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>mem0 = equeue.create_mem([4096], 32, 4, SRAM) mem1 = equeue.create_mem([4096], 32, 4, SRAM) conn = equeue.create_connection(Streaming, 32)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Event queue of MAC when launch wait on its dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Three stages of the timeline of execution of the accelerator in Fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example showing concurrent execution of an ARMr5 control processor, a DMA engine, and a MAC unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The simulation workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Dataflows mapping on systolic array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8c</head><label></label><figDesc>Fig.8cshows OS. On each cycle, ifmaps and weights are passed to the neighbor PEs, while every ofmap is stationary until F h ? F w ? C ifmaps and are convolved: pe[i][j].ofmap += pe[i][j].ifmap * pe[i][j]. weight pe[i+1][j].ifmap = pe[i][j].ifmap pe[i][j+1].weight = pe[i][j].weight</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Comparing our simulation with SCALE-Sim on 4?4 systolic array, by convolving various ifmaps with fixed 2 ? 2 ? 3 weights (a-b) and by convolving various weights convolved with fixed 32 ? 32 weights (c-d).</figDesc><graphic url="image-1.png" coords="9,448.29,230.06,90.45,53.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Figure showing various metrics along the four stages of lowering pipeline on an accelerator with a 4?4 PE array and convolution settings H = W = 4, 8, 16, 32, F h = F w = 3, C = 3, N = 4. Compilation time is negligible and omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Loop iteration for OS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Given various convolution and array configuration, plotting different parameters versus cycles for three dataflows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Visualizing operation-wise tracing of FIR implemented with 16 processors with limited bandwidth.</figDesc><graphic url="image-7.png" coords="11,323.10,72.00,226.77,57.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Visualizing operation-wise tracing of FIR implemented with 4 processors with limited bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2 Control Flow 2 Control Flow 1 Structure Specification 1</head><label></label><figDesc></figDesc><table><row><cell>Structure</cell><cell></cell></row><row><cell>Specification</cell><cell></cell></row><row><cell>kernel = equeue.create_proc(ARMr6) sram = equeue.create_mem(SRAM, [64], 4) dma = create_dma() accel = create_comp("Kernel SRAM DMA", host, sram, dma) pe0 = equeue.create_proc(MAC) reg0 = equeue.create_mem(Register, [4], 4) pe1 = equeue.create_proc(MAC) reg1 = equeue.create_mem(Register, [4], 4) add_comp(accel, "PE0 Reg0 PE1 Reg1, pe0, reg0, pe1, reg1)</cell><cell>start = equeue.control_start() done = equeue.launch(...) in (start, kernel){ copy_dep = equeue.control_start() launch_dep = equeue.memcpy(copy_dep, ?) pe0_dep = equeue.launch(...) in (launch_dep, pe0){ ... ofmap = addi(ifmap, 4) ... } pe1_dep = equeue.launch(...) in (launch_dep, pe1) { ? } equeue.await(pe0_dep, pe1_dep) equeue.return()</cell></row><row><cell></cell><cell>}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Event Queue End Time Ready Time 3. Schedule Operation 4. Finish Operation</head><label></label><figDesc></figDesc><table><row><cell>%start = control_start() %done = launch(...) in (%start, kernel) { ... } await(%done)</cell><cell>launch launch memcpy</cell><cell>...</cell><cell>R ea Upd ate Sign als, crea te laun cher tabl e dy to la un ch ?</cell><cell>"cat": "operation" "ph": "B", "ts": 1, "pid": "Processor", }, ...] "tid": "ARMr5",</cell></row><row><cell>systolic.mlir</cell><cell></cell><cell></cell><cell></cell><cell>tracing.json</cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Setup Entry 2. Check Event Queue 3. Schedule Operation</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>reflects behavior of each processor. At most, our simulator may require over 10 minutes for simulation. Future work could reduce execution time by building a lookup table to skip duplicated behavior or by adding parallelism to the simulation engine.</figDesc><table><row><cell>Dataflows. Fig. 12b plots the SRAM read bandwidth at peak</cell></row><row><cell>(the maximum bandwidth times the duration) versus cycle</cell></row><row><cell>time. OS has the highest read bandwidth overhead while</cell></row><row><cell>WS requires the least. Though Fig. 12a highlights OS can</cell></row><row><cell>achieve the shortest runtime in cycles, designers can choose</cell></row><row><cell>the dataflow according to hardware requirements.</cell></row><row><cell>Array configuration. Our simulator can help designers ob-</cell></row><row><cell>serve general "rules" about performance. Figures 12c to 12e</cell></row><row><cell>plot the relationship between cycles and array structures.</cell></row><row><cell>The loop iteration count is proportional to cycle count. We</cell></row><row><cell>can calculate loop iterations as D1/A h ? D 2 /A w , where</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lime: a Java-compatible and synthesizable language for heterogeneous architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Auerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rabbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Object oriented programming systems languages and applications</title>
		<meeting>the ACM international conference on Object oriented programming systems languages and applications</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="89" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-driven autotuning of sparse matrix-vector multiply on GPUs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Vuduc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles and Practice of Parallel Programming (PPoPP)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CNN inference simulator for accurate and efficient accelerator design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International SoC Design Conference (ISOCC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="283" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization and performance modeling of stencil computations on modern microprocessors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Eldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chapyzhenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leontiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schuiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sunder</surname></persName>
		</author>
		<title level="m">Workshop on Open-Source EDA Technology (WOSET)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>MLIR as hardware compiler infrastructure</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xilinx adaptive compute acceleration platform: Versal architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gaitonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Trace event format</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/catapult-project/catapult/blob/master/tracing/docs/getting-started.md" />
		<imprint>
			<biblScope unit="page" from="4" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The trace event profiling tool</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="http://dev.chromium.org/developers/how-tos/trace-event-profiling-tool" />
		<imprint>
			<biblScope unit="page" from="5" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Darkroom: compiling high-level image processing code into hardware pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hegarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brunhaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="144" to="145" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rigel: Flexible multi-rate image processing hardware</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hegarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intel software development emulator</title>
		<author>
			<persName><forename type="first">Intel</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/intel-software-development-emulator.html" />
		<imprint>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Union: A unified HW-SW co-design ecosystem in MLIR for evaluating tensor operations on spatial accelerators</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kestor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Evaluation of RISC-V RTL with FPGA-accelerated simulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>First Workshop on Computer Architecture Research with RISC-V</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strober: Fast and accurate sample-based energy simulation for arbitrary RTL</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovicc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="128" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why systolic architectures?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Annals of the History of Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of DNN dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HeteroCL: A multi-paradigm programming infrastructure for software-defined reconfigurable computing</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11054</idno>
		<title level="m">MLIR: A compiler infrastructure for the end of Moore&apos;s law</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">HeteroHalide: From image processing DSL to efficient FPGA acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A systolic array for rapid string comparison</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Chapel Hill Conference on VLSI. Chapel Hill NC</title>
		<meeting>the Chapel Hill Conference on VLSI. Chapel Hill NC</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="363" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyMTL: A unified framework for vertically integrated computer architecture research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zibrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Batten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="280" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">HIR: An mlir-based intermediate representation for hardware accelerator description</title>
		<author>
			<persName><forename type="first">K</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00194</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Async dialect</title>
		<author>
			<persName><surname>Mlir Project</surname></persName>
		</author>
		<ptr target="https://mlir.llvm.org/docs/Dialects/AsyncDialect/" />
		<imprint>
			<biblScope unit="page" from="5" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to DNN accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international symposium on performance analysis of systems and software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">gem5-gpu: A heterogeneous CPU-GPU simulator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="36" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">System design, modeling, and simulation: using Ptolemy II</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ptolemaeus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ptolemy.org</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Programming heterogeneous systems from an image processing DSL</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Programmatic control of a compiler for generating high-performance spatial hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07606</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SCALE-Sim: Systolic cnn accelerator simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02883</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LLHD: A multi-level intermediate representation for hardware description languages</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schuiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="258" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-designing accelerators and SoC interfaces using gem5-Aladdin</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Network-on-chip programmable platform in Versal ACAP architecture</title>
		<author>
			<persName><forename type="first">I</forename><surname>Swarbrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gaitonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete event simulation system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Simulation Multiconference (ESM&apos;2001)</title>
		<meeting>of the European Simulation Multiconference (ESM&apos;2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Super sampling rate FIR filters implementation on the AI Engine</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://github.com/Xilinx/Vitis-Tutorials/tree/master/AIEngineDevelopment/DesignTutorials/02" />
	</analytic>
	<monogr>
		<title level="m">super sampling rate fir</title>
		<imprint>
			<biblScope unit="page" from="7" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Versal ACAP AI Engine programming environment -user guide</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/support/documentation/swmanuals/xilinx20202/ug1076-ai-engine-environment.pdf" />
		<imprint>
			<biblScope unit="page" from="4" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Versal: The first adaptive compute acceleration platform (ACAP)</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/support/documentation/whitepapers/wp505-versal-acap.pdf" />
		<imprint>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Vivado high-level synthesis</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/design-tools/vivado/integration/esl-design.html" />
		<imprint>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vivado simulator</title>
		<author>
			<persName><forename type="first">Xilinx</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/design-tools/vivado/simulator.html" />
		<imprint>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interstellar: using Halide&apos;s scheduling language to analyze DNN accelerators</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="369" to="383" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
