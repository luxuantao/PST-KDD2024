<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
							<email>mcdaniel@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs. Adversarial samples can be used to, for example, subvert fraud detection, bypass content filters or malware detection, or to mislead autonomous navigation systems <ref type="bibr" target="#b19">[20]</ref>. These attacks on input integrity exploit imperfections and approximations made by learning algorithms during training to control machine learning models outputs (see Figure <ref type="figure" target="#fig_0">1</ref>). Adversarial sample transferability <ref type="foot" target="#foot_0">1</ref> is the property that some adversarial samples produced to mislead a specific model f can mislead other models f -even if their architectures greatly differ <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. A practical impact of this property is that it leads to oracle-based black box attacks. In one such attack, Papernot et al. trained a local deep neural network (DNN) using crafted inputs and output labels generated by the target "victim" DNN <ref type="bibr" target="#b18">[19]</ref>. Thereafter, the local network was used to generate adversarial samples that were highly effective on the original victim DNN. The key here was that the adversary has very limited informationthey knew nothing about the architecture or parameters but only knew that the victim was a DNN-and had only oracle access that allowed it to obtain outputs for chosen inputs.</p><p>In this paper, we develop and validate a generalized algorithm for black box attacks that exploit adversarial sample transferability on broad classes of machine learning. In investigating these attacks, we explore transferability within and between different classes of machine learning classifier algorithms. We explore neural networks (DNNs), logistic regression (LR), support vector machines (SVM), decision trees (DT), nearest neighbors (kNN), and ensembles (Ens.).</p><p>In this, we demonstrate that black-box attacks are generally applicable to machine learning and can effectively target classifiers not built using deep neural networks. The generalization is two-fold: we show that (1) the substitute model can be trained with other techniques than deep learning, and (2) transferability-based black box attacks are not restricted to deep learning targets and is in fact successful with targeted models of many machine learning types. Our contributions are summarized as follows:</p><p>• We introduce adversarial sample crafting techniques for support vector machine as well as decision treeswhich are non-differentiable machine learning models.</p><p>• We study adversarial sample transferability across the machine learning space and find that samples largely transfer well across models trained with the same machine learning technique, and across models trained with different techniques or ensembles taking collective decisions. For example, a support vector machine and decision tree respectively misclassify 91.43% and 87.42% of adversarial samples crafted for a logistic regression model. Previous work on adversarial example transferability has primarily studied the case where at least one of the models involved in the transfer is a neural network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, while we aim to more generally characterize the transferability between a diverse set of models chosen to capture most of the space of popular machine learning algorithms.</p><p>• We generalize the learning of substitute models from deep learning to logistic regression and support vector machines. Furthermore, we show that it is possible to learn substitutes matching labels produced by many machine learning models (DNN, LR, SVM, kNN) at rates superior to 80%. We improve the accuracy and computational cost of a previously proposed substitute learning technique by introducing a new hyperparameter and the use of reservoir sampling.</p><p>• We conduct black-box attacks against classifiers hosted by Amazon and Google. We show that despite our lack of knowledge of the classifier internals, we can force them to respectively misclassify 96.19% and 88.94% of their inputs using a logistic regression substitute model trained by making only 800 queries to the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACH OVERVIEW</head><p>In this section, we describe our approach, which is structured around the evaluation of two hypotheses relevant to the design of black-box attacks against machine learning classifiers.</p><p>Let us precisely define adversarial sample transferability. Consider an adversary interested in producing an adversarial sample x * misclassified in any class different from the class assigned by model f to legitimate input x. This can be done by solving<ref type="foot" target="#foot_1">2</ref> the following optimization problem <ref type="bibr" target="#b21">[22]</ref>:</p><p>x * = x + δ x where δ x = arg min</p><formula xml:id="formula_0">z f ( x + z) = f ( x)<label>(1)</label></formula><p>Table <ref type="table">1</ref>: Machine Learning Techniques studied in Section 3 Samples x * solving Equation 1 are specifically computed to mislead model f . However, as stated previously, such adversarial samples are in practice also frequently misclassified by models f different from f . To facilitate our discussion, we formalize this adversarial sample transferability notion as:</p><formula xml:id="formula_1">ΩX (f, f ) = f ( x) = f ( x + δ x ) : x ∈ X (2)</formula><p>where set X is representative of the expected input distribution for the task solved by models f and f . We partition adversarial sample transferability in two variants characterizing the pair of models (f, f ). The first, intra-technique transferability, is defined across models trained with the same machine learning technique but different parameter initializations or datasets (e.g., f and f are both neural networks or both decision trees). The second, cross-technique transferability, considers models trained using two techniques (e.g., f is a neural network and f a decision tree).</p><p>Hypothesis 1: Both intra-technique and cross-technique adversarial sample transferabilities are consistently strong phenomena across the space of machine learning techniques.</p><p>In this first hypothesis, we explore how well both variants of transferability hold across classes of machine learning algorithms. The motivation behind this investigation is that adversarial sample transferability constitutes a threat vector against machine learning classifiers in adversarial settings.</p><p>To identify the most vulnerable classes of models, we need to generate an accurate comparison of the attack surface of each class in constrained experimental settings.</p><p>To validate this hypothesis, we perform a large-scale study in Section 3. Each of the study's two folds investigates one of the adversarial sample transferability variants: intratechnique and cross-technique. For completeness, we consider a collection of models representatively spanning the machine learning space, as demonstrated by Table <ref type="table">1</ref>. Models are trained on MNIST data <ref type="bibr" target="#b15">[16]</ref> to solve the hand-written digit recognition task. In the first fold of the study, we measure intra-technique adversarial sample transferability rates ΩX (f, f ), for each machine learning technique, across models trained on different subsets of the data. In the second fold of the study, we measure inter-technique adversarial sample transferability rates ΩX (f, f ) across models corresponding to all possible pairs of machine learning techniques.</p><p>Hypothesis 2: Black-box attacks are possible in practical settings against any unknown machine learning classifier.</p><p>Our motivation is to demonstrate that deployment of machine learning in settings where there are incentives for ad-versaries to have models misbehave must take into account the practical threat vector of adversarial samples. Indeed, if black-box attacks are realistic in practical settings, machine learning algorithm inputs must be validated as being part of the expected distribution of inputs. As is the case for SQL injections, the existence of adversarial samples calls for input validation in production systems using machine learning.</p><p>The verification of this second hypothesis is two-fold as well.</p><p>In Section 4, we show how to transfer the generalization knowledge of any machine learning classifiers into a substitute model by querying the classifier for labels on carefully selected inputs. In Section 5, we perform black-box attacks against commercial machine learning classifiers hosted by Amazon and Google. As we validate the hypothesis throughout Sections 4 and 5, we operate under the specific threat model of an oracle, described in <ref type="bibr" target="#b19">[20]</ref>, which characterizes realistic adversarial settings. Instead of having full knowledge of the model's architecture f and its parameters θ, as was the case for the first hypothesis validation in Section 3, we now assume the adversary's only capability is to observe the label predicted by the model f on inputs of its choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRANSFERABILITY OF ADVERSARIAL SAMPLES IN MACHINE LEARNING</head><p>In this section, our working hypothesis is that intra-technique and cross-technique adversarial sample transferability are strong phenomena across the machine learning space. Thus, we empirically study these two phenomena across a range of machine learning techniques: deep neural networks (DNNs), logistic regression (LR), support vector machines (SVM), decision trees (DT), nearest neighbors (kNN), and ensembles (Ens.). All models are found vulnerable to intra-technique adversarial sample transferability-misclassification of samples by different models trained using the same machine learning technique, the phenomenon is stronger for differentiable models like DNNs and LR than for non-differentiable models like SVMs, DTs and kNNs. Then, we observe that DNNs and kNNs boast resilience to cross-technique transferability, misclassifications of adversarial samples by models trained with distinct machine learning techniques. We find that all other models, including LR, SVMs, DTs, and an ensemble of models collectively making predictions, are considerably more vulnerable to cross-technique transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We describe here the dataset and machine learning models used in this section to study both types of transferability.</p><p>Dataset -We use the seminal MNIST dataset of handwritten digits <ref type="bibr" target="#b15">[16]</ref>. This dataset has been well-studied in both the machine learning and security communities. We chose it because its dimensionality is suitable to the range of machine learning techniques included in our study, which all perform at least reasonably well on this dataset. The task associated with the dataset is classification of images in one of the 10 classes corresponding to each possible digit ranging from 0 to 9. The dataset includes 50, 000 training samples, 10, 000 validation samples, and 10, 000 test samples. Each 28x28 gray-scale pixel image is encoded as a vector of intensities whose real values range from 0 (black) to 1 (white).</p><p>Machine learning models -We selected five machine learning techniques: DNNs, LR, SVMs, DTs, and kNNs. All of these machine learning techniques, as well as the algorithms used to craft adversarial samples, are presented in Section 6 of this paper. As outlined in Table <ref type="table">1</ref>, DNNs were chosen for their state-of-the-art performance, LR for its simplicity, SVMs for their potential robustness stemming from the margin constraints when choosing decision boundaries at training, DTs for their non-differentiability, and kNNs for being lazy-classification<ref type="foot" target="#foot_2">3</ref> models. To train DNN, LR, and kNN models, we use Theano <ref type="bibr" target="#b2">[3]</ref> and Lasagne <ref type="bibr" target="#b1">[2]</ref>. The DNN is made up of a hierarchy of 2 convolutional layers of 32 3x3 kernels, 2 convolutional layers of 64 3x3 kernels, 2 rectified linear layers of 100 units, and a softmax layer of 10 units. It is trained during 10 epochs with learning, momentum, and dropout rates of respectively 10 −2 , 0.9, and 0.5 decayed by 0.5 after 5 epochs. The LR is performed using a softmax regression on the inputs. It is trained during 15 epochs at a learning rate of 10 −2 with a momentum rate of 0.9 both decayed by 0.5 after 10 epochs. The linear SVM and DT are trained with scikit-Learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-technique Transferability</head><p>We show that differentiable models like DNNs and LR are more vulnerable to intra-technique transferability than nondifferentiable models like SVMs, DTs, and kNNs. We measure intra-technique transferability between models i and j, both learned using the same machine learning technique, as the proportion of adversarial samples produced to be misclassified by model i that are misclassified by model j.</p><p>To train different models using the same machine learning technique, we split the training set in disjoint subsets A,B,C,D,E of 10, 000 samples each, in order of increasing indices. For each of the machine learning techniques (DNN, LR, SVM, DT, kNN), we thus learn five different models referred to as A,B,C,D,E. Model accuracies, i.e. the proportion of labels correctly predicted by the model for the testing data, are reported in Figure <ref type="figure">2a</ref>. For each of the 25 models, we apply the suitable adversarial sample algorithm described in Section 3.2 and craft 10, 000 samples from the test set, which was unused during training. For adversarial sample algorithms with parameters, we fine-tune them to achieve a quasi-complete misclassification of the 10, 000 adversarial samples by the model on which they are crafted. Upon empirically exploring the input variation parameter space, we set it to ε = 0.3 for the fast gradient sign method algorithm, and ε = 1.5 for the SVM algorithm. important transferability, with rates of at least 49%. On the SVM, DT, and kNN matrices, the diagonals stand out more, indicating that these techniques are to some extent more robust to the phenomenon. In the case of SVMs, this could be explained by the explicit constraint during training on the choice of hyperplane decision boundaries that maximize the margins (i.e. support vectors). The robustness of both DTs and kNNs could simply stem from their non-differentiability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-technique Transferability</head><p>We define cross-technique transferability between models i and j, trained using different machine learning techniques, as the proportion of adversarial samples produced to be mis-classified by model i that are also misclassified by model j. Hence, this is a more complex phenomenon than intratechnique transferability because it involves models learned using possibly very different techniques like DNNs and DTs. Yet, cross-technique transferability is surprisingly a strong phenomenon to which techniques like LR, SVM, DT, and ensembles are vulnerable, making it easy for adversaries to craft adversarial samples misclassified by models trained using diverse machine learning techniques.</p><p>We study the cross-technique transferability phenomenon across models trained using the five machine learning techniques already used in Section 3.2 and described in Section 3.1 and 6. To these, we add a 6th model: an ensemble f ( x). The ensemble f is implemented using a collection of 5 experts, which are the 5 previously described models: the DNN denoted f1, LR denoted f2, SVM denoted f3, DT denoted f4, and kNN denoted f5. Each expert makes a decision and the ensemble outputs the most frequent choice (or the class with the lowest index if they all disagree):</p><formula xml:id="formula_2">f ( x) = arg max i∈0..N −1 j∈1..5 fj,i( x)<label>(3)</label></formula><p>where fj,i( x) = 1 f j ( x)==i indicates whether classifier fj assigned class i to input x. Note that in this section, we only train one model per machine learning technique on the full MNIST training set of 50, 000 samples, unlike in Section 3.2.</p><p>In this experiment, we are interested in transferability across machine learning techniques. As such, to ensure our results are comparable, we fine-tune the parameterizable crafting algorithms to produce adversarial samples with similar perturbation magnitudes. To compare magnitudes across perturbation styles, we use the L1 norm: the sum of each perturbation component's absolute value. Perturbation added to craft adversarial samples using the DNN, LR, and SVM have an average L1 norm δ x 1 of 11.5%. To achieve this, we use an input variation parameter of ε = 0.25 with the fast gradient sign method on the DNN, LR, and kNN. To craft adversarial samples on the SVM, we use an input variation parameter of ε = 5 with the crafting method introduced in Section 6. Unfortunately, the attack on DT cannot be parameterized to match the L1 norm of DNN, LR, kNN and SVM attacks. Hence, perturbations selected have much lower average L1 norms of respectively 1.05%.</p><p>We build a cross-technique transferability matrix where each cell (i, j) holds the percentage of adversarial samples produced for classifier i that are misclassified by classifier j. In other words, rows indicate the machine learning technique that trained the model against which adversarial samples were crafted. The row that would correspond to the ensemble is not included because there is no crafting algorithm designed to produce adversarial samples specifically for an ensemble, although we address this limitation in Section 4 using insight gained in this experiment. Columns indicate the underlying technique of the classifier making predictions on adversarial samples. This matrix, plotted in Figure <ref type="figure">3</ref>, shows that cross-technique transferability is a strong but heterogeneous phenomenon. The most vulnerable model is the decision tree (DT) with misclassification rates ranging from 47.20% to 89.29% while the most resilient is the deep neural network (DNN) with misclassification rates between Figure <ref type="figure">3</ref>: cross-technique Transferability matrix: cell (i, j) is the percentage of adversarial samples crafted to mislead a classifier learned using machine learning technique i that are misclassified by a classifier trained with technique j.</p><p>0.82% and 38.27%. Interestingly, the ensemble is not resilient to cross-technique transferability of adversarial samples with rates reaching 44.14% for samples crafted using the LR model. This is most likely due to the vulnerability of each underlying expert to adversarial samples.</p><p>We showed that all machine learning techniques we studied are vulnerable to two types of adversarial sample transferability. This most surprisingly results in adversarial samples being misclassified across multiple models learned with different machine learning techniques. This cross-technique transferability greatly reduces the minimum knowledge that adversaries must possess of a machine learning classifier in order to force it to misclassify inputs that they crafted. We leverage this observation, along with findings from Section 4, to justify design choices in the attack described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LEARNING CLASSIFIER SUBSTITUTES BY KNOWLEDGE TRANSFER</head><p>In the previous section, we identified machine learning techniques (e.g., DNNs and LR) yielding models adequate for crafting samples misclassified across models trained with different techniques, i.e adversarial samples with strong crosstechnique transferability. Thus, in order to craft adversarial samples misclassified by a classifier whose underlying model is unknown, adversaries can instead use a substitute model if it solves the same classification problem and its parameters are known. Therefore, efficiently learning substitutes is key to designing black-box attacks where adversaries target remote classifiers whose model, parameters, and training data are unknown to them. This is precisely the attack scenario evaluated against commercial machine learning platforms in Section 5, while we focus in this section on the prerequisite learning of substitutes for machine learning classifiers.</p><p>We enhance an algorithm introduced in <ref type="bibr" target="#b19">[20]</ref> to learn a substitute model for a given classifier simply by querying it for labels on carefully chosen inputs. More precisely, we introduce two refinements to the algorithm: one improves its accuracy and the second reduces its computational complexity. We generalize the learning of substitutes to oracles using a range of machine learning techniques: DNNs, LR, SVMs, DTs, and kNNs. Furthermore, we show that both DNNs and LR can be used as substitute models for all machine learning techniques studied to the exception of decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Augmentation for Substitutes</head><p>The targeted classifier is designated as an oracle because adversaries have the minimal capability of querying it for predictions on inputs of their choice. </p><formula xml:id="formula_3">Sρ+1 = { x + λρ • sgn(J f [ Õ( x)] : x ∈ Sρ)} ∪ Sρ<label>(4)</label></formula><p>where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Periodical</head><p>Step Size -When introducing the technique, Papernot et al. used a fixed step size parameter λρ throughout the substitute learning iterations ρ. In this section, we show that by having a step size periodically alternating between positive and negative values, one can improve the quality of the oracle approximation made by the substitute, which we measure in terms of the number of labels matched with the original classifier oracle. More precisely, we introduce an iteration period τ after which the step size is multiplied by −1. Thus, the step size λρ is defined as:</p><formula xml:id="formula_4">λρ = λ • (−1) ρ τ (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where τ is set to be the number of epochs after which the Jacobian-based dataset augmentation does not lead any substantial improvement in the substitute. A grid search can also be performed to find an optimal value for the period τ . We also experimented with a decreasing grid step amplitude λ, but did not find that it yielded substantial improvements.</p><p>Reservoir Sampling -We also introduce the use of reservoir sampling <ref type="bibr" target="#b22">[23]</ref> as a mean to reduce the number of queries made to the oracle. This is useful when learning substitutes in realistic environments where the number of label queries an adversary can make without exceeding a quota or being Algorithm 1 Jacobian-based augmentation with Reservoir Sampling: sets are considered as arrays for ease of notation.</p><formula xml:id="formula_6">Input: Sρ−1, κ, J f , λρ 1: N ← |Sρ−1| 2: Initialize Sρ as array of N + κ items 3: Sρ[0 : N − 1] ← Sρ−1 4: for i ∈ 0..κ − 1 do 5: Sρ[N + i] ← Sρ−1[i] + λρ • sgn(J f [ Õ(Sρ−1[i])]) 6: end for 7: for i ∈ κ..N − 1 do 8:</formula><p>r ← random integer between 0 and i 9:</p><p>if r &lt; κ then 10:</p><formula xml:id="formula_7">Sρ[N + r] ← Sρ−1[i] + λρ • sgn(J f [ Õ(Sρ−1[i])]) 11:</formula><p>end if 12: end for 13: return Sρ detected by a defender is constrained. Reservoir sampling is a class of algorithms that randomly select κ samples from a list of samples. The total number of samples in the list can be both very large and unknown. In our case, we use reservoir sampling to select a limited number of new inputs κ when performing a Jacobian-based dataset augmentation. This prevents the exponential growth of queries made to the oracle at each augmentation iteration. At iterations ρ &gt; σ (the first σ iterations are performed normally), when considering the previous set Sρ−1 of substitute training inputs, we select κ inputs from Sρ−1 to be augmented in Sρ. These κ inputs are selected using reservoir sampling, as described in Algorithm 1. This technique ensures that each input in Sρ−1 has an equal probability </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Neural Network Substitutes</head><p>In <ref type="bibr" target="#b19">[20]</ref>, the oracle classifier approximated was always a DNN. However, the authors concluded with preliminary results suggesting applicability to a nearest neighbors classifier. We here show that in fact the technique is generalizable and applicable to many machine learning techniques by evaluating its performance on 5 types of ML classifiers: a DNN, LR, SVM, DT, and kNN. This spectrum is representative of machine learning (cf. Section 3.1). Our experiments suggest that one can accurately transfer the knowledge from many machine learning classifiers to a DNN and obtain a DNN mimicking the decision boundaries of the original classifier.</p><p>Using the Jacobian-based augmentation technique, we train 5 different substitute DNNs to match the labels produced by 5 different oracles, one for each of the ML techniques mentioned. These classifiers serving as oracles are all trained on the 50, 000 sample MNIST training set using the models described previously in Section 3. and Reservoir Sampling (RS), on the percentage of label predictions matched between the substitutes and their target classifiers on test data after ρ = 9 substitute iterations.</p><p>with τ = 3 periodic step size, (3) with both τ = 3 periodic step size and reservoir sampling with parameters σ = 3 and κ = 400. The substitute architecture is identical to the DNN architecture from Section 3.1. We allow experiments to train substitutes for 10 augmentation iterations, i.e. ρ ≤ 9.</p><p>Figure <ref type="figure" target="#fig_4">4a</ref> plots at each iteration ρ the share of samples on which the substitute DNNs agree with predictions made by the classifier oracle they are approximating. This proportion is estimated by comparing the labels assigned to the MNIST test set by the substitutes and oracles before each iteration ρ of the Jacobian-based dataset augmentation. The substitutes used in this figure were all trained with both a periodic step size and reservoir sampling, as described previously.</p><p>Generally speaking, all substitutes are able to successfully approximate the corresponding oracle, after ρ = 10 augmentation iterations, the labels assigned match for about 77% to 83% of the MNIST test set, except for the case of the DT oracle, which is only matched for 48% of the samples. This difference could be explained by the non-differentiability of decisions trees. On the contrary, substitute DNNs are able to approximate the nearest neighbors oracle although it uses lazy classification: no model is learned at training time and predictions are made by finding close training sample(s).</p><p>The first three rows of Table <ref type="table" target="#tab_2">2</ref> quantify the impact of the two refinements introduced above on the proportion of test set labels produced by the oracle that were matched by DNN substitutes. The first refinement, the periodic step size, allows substitutes to approximate more accurately their target oracle. For instance at ρ = 9 iterations, the substitute DNN trained with a periodic ste size for the DNN oracle matches 89.28% of the labels whereas the vanilla substitute DNN only matched 78.01%. Similarly, the substitute DNN trained with a periodic ste size for the SVM oracle matches 83.79% of the labels whereas the vanilla substitute only matched 79.68%. The second refinement, reservoir sampling allows us to train substitutes for more augmentation iterations without making too many queries to the oracle. For instance, 10 iterations with reservoir sampling (using σ = 3 and κ = 400) make 100 • 2 3 + 400(10 − 3) = 3, 600 queries to the oracle instead of 102, 400 queries with the vanilla technique. The reduced number of queries has an impact on the substitute quality compared to the periodic step size substitutes but it is still superior to the vanilla substitutes. For instance, when approximating a DNN oracle, the vanilla substitute matched 7, 801 labels, the periodic step size one 8, 928, and the periodic step size with reservoir sampling one 8, 290.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Logistic Regression Substitutes</head><p>Having generalized substitute learning with a demonstration of the capacity of DNNs to approximate any machine learning model, we now consider replacing the substitute itself by another machine learning technique. Experiments in Section 3.3 led us to conclude that cross-technique transferability is not specific to adversarial samples crafted on DNNs, but instead applies to many learning techniques. Looking at Figure <ref type="figure">3</ref> again, a natural candidate is logistic regression, as it displays large cross-technique transferability rates superior to DNNs except when targeting DNNs themselves.</p><p>The Jacobian-based dataset augmentation's implementation for DNNs is easily adapted to multi-class logistic regression. Indeed, multi-class logistic regression is analog to the softmax layer frequently used by deep neural networks to produce class probability vectors. We can easily compute the (i, j) component of the Jacobian of a multi-class LR model:</p><formula xml:id="formula_8">J f ( x)[i, j] = wje w j [i]• x − N l=1 w l [i] • e w l x N l=1 e w l [i]• x 2<label>(6)</label></formula><p>where notations are the ones used in Equation <ref type="formula" target="#formula_12">9</ref>.</p><p>Hence, we repeat the experiment from Section 4.2 but we now train multi-class logistic regression substitute models (instead of the DNN substitutes) to match the labels produced by the classifier oracles. Everything else is unchanged in the experimental setup. As illustrated in Figure <ref type="figure" target="#fig_4">4b</ref>, the change of model type for the substitute generally speaking degrades the approximation quality: the proportion of labels matched is reduced. Performances of LR substitutes are competitive with those of DNN substitutes for LR and SVM oracles. Here again, the substitutes perform poorly on the decision tree oracle, with match rates barely above 40%.</p><p>The last three rows of Table <ref type="table" target="#tab_2">2</ref> quantify the impact of the two refinements introduced above on the proportion of test set labels produced by the oracle that were matched by LR substitutes. The first refinement, the periodic step size, allows LR substitutes to approximate more accurately their target oracle, as was also the case for DNN substitutes. For instance at ρ = 9 iterations, the LRsubstitute trained with a periodic ste size for the LR oracle matches 84.01% of the labels whereas the vanilla LR substitute only matched 72.00%. Similarly, the LR substitute trained with a periodic ste size for the SVM oracle matches 82.19% of the labels whereas the vanilla substitute only matched 71.56%. The second refinement, reservoir sampling allows us to reduce the number of queries with a limited impact on the substitute quality: less labels are match than the periodic step size substitutes but more than the vanilla substitutes. For instance, when approximating a SVM oracle, the vanilla substitute matched 71.56% of the labels, the periodic step size one 82.19%, and the periodic step size with reservoir sampling one 79.20%.</p><p>The benefit of vanilla LR substitutes compared to DNN substitutes is that they achieve their asymptotic match rate faster, after only ρ = 4 augmentation iterations, corresponding to 1, 600 oracle queries. Furthermore, LR models are much lighter in terms of computational cost. These two factors could justify the use of LR (instead of DNN)substitutes in some contexts. The reservoir sampling technique gives good performances, especially on LR and SVM oracles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Support Vector Machines Substitutes</head><p>Having observed that deep learning and logistic regression were both relevant when approximating classifier oracles, we now turn to SVMs for substitute learning. This is motivated by the strong cross-technique transferability of adversarial sample crafted using an SVM observed in Section 3, making SVMs good candidates for substitutes in a black-box attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM-based dataset augmentation -</head><p>To train SVMs to approximate oracles in a manner analogous to the Jacobianbased dataset augmentation, we introduce a new augmentation technique. We replace the heuristic in Equation 4 by the following, which is adapted to the specificities of SVMs:</p><formula xml:id="formula_9">Sρ+1 = { x − λ • w[ Õ( x)] w[ Õ( x)] x : x ∈ Sρ)} ∪ Sρ<label>(7)</label></formula><p>where w[k] is the weight indicating the hyperplane direction of subclassifier k used to implement a multi-class SVM with the one-vs-the-rest scheme as detailed in Equation <ref type="formula" target="#formula_15">12</ref>. This heuristic selects new points in the direction orthogonal to the hyperplane acting as the decision boundary for the bi-nary SVM subclassifier k corresponding to the input's label. This is precisely the direction used in Equation 13 to find adversarial samples but parameter λ is here generally set to lower values so as to find samples near the decision boundary instead of on the other side of the decision boundary.</p><p>Experimental In this section, we evaluated the capacity of DNN, LR, and SVM substitutes to approximate a classifier oracle by querying it for labels on inputs selected using a heuristic relying on the substitute's Jacobian. We observed that predictions made by DNN and LR substitutes more accurately matched the targeted oracles than SVM substitute predictions. We emphasize that all experiments only required knowledge of 100 samples from the MNIST test set. In other words, learning substitutes does not require knowledge of the targeted classifier's type, parameters, or training data, and can thus be performed under realistic adversarial threat models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BLACK-BOX ATTACKS OF REMOTE MACHINE LEARNING CLASSIFIERS</head><p>Intra-technique and cross-technique transferability of adversarial samples, together with the learning of substitutes for classifier oracles, enable a range of attacks targeting remote machine learning based systems whose internals are unknown to adversaries. To illustrate the feasibility of blackbox attacks on such remote systems, we target in an experiment two machine learning classifiers respectively trained and hosted by Amazon and Google. We find it is possible to craft samples misclassified by these commerical oracles at respective rates of 96.19% and 88.94% after making 800 queries to learn substitute models approximating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Oracle Attack Method</head><p>This section's adversarial threat model is identical to the one used when learning substitutes in Section 4: adversaries have an oracle access to the remote classifier. Its type, parameters, or training set are all unknown to the adversary. The attack method leverages Sections 3 and 4 of this paper, and is a generalization of the approach introduced in <ref type="bibr" target="#b19">[20]</ref>.</p><p>The adversary first locally trains a substitute model to approximate the remotely hosted classifier, using queries to the oracle as described in Section 4. We consider the use of deep learning and logistic regression to learn substitutes for classifiers. We apply the two refinements introduced in this paper: a periodic step size and reservoir sampling. Since substitute models are locally trained, the adversary has full knowledge of their model parameters. Thus, one of the adversarial sample crafting algorithms introduced in Section 6 corresponding to the machine learning technique used to learn the substitute are employed to craft adversarial samples misclassified by the substitute model. The adversary than leverages either intra-technique or cross-technique transferability of adversarial samples-depending on the techniques with which the substitute and oracle were learned: the inputs misleading the locally trained substitute model are very likely to also deceive the targeted remotely hosted oracle.</p><p>Previous work conducted such an attack using a substitute and targeted classifier both trained using deep learning, demonstrating that the attack was realistic using the Meta-Mind API providing Deep Learning as a Service <ref type="bibr" target="#b19">[20]</ref>. We generalize these results by performing the attack on Machine Learning as a Service platforms that employ techniques that are unknown to us: Amazon Web Services and Google Cloud Prediction. Both platforms automate the process of learning classifiers using a labeled dataset uploaded by the user. Unlike MetaMind, neither of these platforms claim to exclusively use deep learning to build classifiers. When analyzing our results, we found that Amazon uses logistic regression (cf. below) but to the best of our knowledge Google has never disclosed the technique they use to train classifiers, ensuring that our experiment is properly blind-folded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Amazon Web Services Oracle</head><p>Amazon offers a machine learning service, Amazon Machine Learning, <ref type="foot" target="#foot_3">4</ref> as part of their Amazon Web Services platform. We used this service to train and host a ML classifier oracle. First, we uploaded a CSV encoded version of the MNIST training set to an S3 bucket on Amazon Web Services. We truncated the pixel values in the CSV file to 8 decimal places.</p><p>We then started the ML model training process on the Machine Learning service: we loaded the CSV training data from our S3 bucket, selected the multi-class model type, provided the target column in the CSV file, and kept the default configuration settings. Note that Amazon offers limited customization options: the settings allow one to customize the recipe (data transformations), specify a maximum model size and number of training epochs, disable training data shuffle, and change the regularization type between L1 and L2 or simply disable regularization. The training process takes a few minutes and outputs a classifier model achieving a 92.17% accuracy on the MNIST test set. We have no way to improve that performance beyond the limited customizing options as the intent of the service is to automate model training. Finally, we activate real-time predictions to be able to query the model for labels from our local machine.</p><p>We then use the Python API provided with the Amazon Machine Learning service to submit prediction queries to our trained oracle model and retrieve the output label. Although confidence values are available for predictions, we only consider the label to ensure our threat model for adversarial capabilities remains realistic. We incorporate this oracle in our experimental setup and train two substitute models to approximate the labels produced by this oracle, Substitute type DNN LR ρ = 3 (800 queries) 87.44% 96.19% ρ = 6 (6,400 queries) 96.78 % 96.43% ρ = 6 (PSS + RS) (2,000 queries) 95.68% 95.83%</p><p>Table <ref type="table">3</ref>: Misclassification rates of the Amazon oracle on adversarial samples (ε = 0.3) produced with DNN and LR substitutes after ρ = {3, 6} augmentation iterations. Substitutes are trained without and with refinements from Section 4: periodic step size (PSS) and reservoir sampling (RS).</p><p>a DNN and LR, as SVM substitutes were dismissed by the conclusions of Section 4. We train two variants of the DNN and LR substitutes. The first variant is trained with the vanilla dataset augmentation and the second variant with the enhanced dataset augmentation introduced in this paper, which uses both a periodic step size and reservoir sampling. Learning is initialized with a substitute training set of 100 samples from the MNIST test set. For all substitutes, we measure the attack success as the proportion among the 10, 000 adversarial samples, produced using the fast gradient sign method with parameter ε = 0.3 (cf. Section 6) and the MNIST test set, misclassified by the Amazon oracle.</p><p>Misclassification rates of the Amazon Machine Learning oracle on adversarial samples crafted using both the DNN and LR substitutes after ρ ∈ {3, 6} dataset augmentation iterations are reported in Table <ref type="table">3</ref>. Results are given for models learned without and with the two refinements-periodic step size (PSS) and reservoir sampling (RS)-introduced in Section 4. With a misclassification rate of 96.19% for an adversarial perturbation ε = 0.3 using a LR substitute trained with 800 queries (ρ = 3) to the oracle, the model trained by Amazon is easily misled. To understand why, we carefully read the online documentation and eventually found one page indicating that the type of model trained by the Amazon Machine Learning service is an "industry-standard" multinomial logistic regression. <ref type="foot" target="#foot_4">5</ref> As seen in Section 3, LR is extremely vulnerable to intra-technique and to a lesser extend vulnerable to cross-technique transferability. In fact, as pointed out by Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref>, shallow models like logistic regression are unable to cope with adversarial samples and learn a classifier resistant to them. This explains why (1) the attack is very successful and (2) the LR substitute performs better than the DNN substitute.</p><p>Additionally, Table <ref type="table">3</ref> shows how the use of a periodic step size (PSS) together with reservoir sampling (RS) allows us to reduce the number of queries made to the Amazon oracle while learning a DNN substitute producing adversarial samples with higher transferability to the targeted classifier. Indeed, we reduce by a factor of more than 3 the number of queries made from 6, 400 to 2, 000, while only degrading the misclassification rate from 96.78% to 95.68%-still larger than the rate of 87.44% achieved after 800 queries by the substitute learned without PSS and RS. For the LR substitutes, we do not see any positive impact from the use of PSS and RS, which is most likely to the fast convergence of LR substitute learning, as observed in Section 4.</p><p>Substitute type DNN LR ρ = 3 (800 queries) 84.50% 88.94% ρ = 6 (6,400 queries) 97.17% 92.05% ρ = 6 (PSS + RS) (2,000 queries) 91.57% 97.72% Table <ref type="table">4</ref>: Misclassification rates of the Google oracle on adversarial samples (ε = 0.3) produced with DNN and LR substitutes after ρ = {3, 6} augmentation iterations.. Substitutes are trained without and with refinements from Section 4: periodic step size (PSS) and reservoir sampling (RS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Google Cloud Prediction Oracle</head><p>To test whether this poor performance is limited to the Amazon Web Services platform, we now target the Google Cloud Prediction API service <ref type="foot" target="#foot_5">6</ref> . The procedure to train a classifier on Google's platform is similar to Amazon's. We first upload to Google's Cloud Storage service the CSV encoded file of the MNIST training data identical to the one used to train the oracle on Amazon Machine Learning. We then activate the Prediction API on Google's Cloud Platform and train a model using the API's method named prediction.trainedmodels.insert. The only property we are able to specify is the expected multi-class nature of our classifier model as well as the column in the CSV indicating target labels. We then evaluate the resulting model using the API method prediction.trainedmodels.predict and an uploaded CSV file of the MNIST test set. The API reports an accuracy of 92% on this test set for the model trained.</p><p>We now use the Google Cloud Python API to connect our experimental setup to the Prediction API, thus allowing our algorithms to make queries to the Google classifier oracle. As we did for Amazon, we train two substitute models (DNN and LR) using an initial substitute training set of 100 samples from the MNIST test set. For each substitute type, we train two model variants: the first one without periodic step size (PSS) or reservoir sampling (RS), the second one with both PSS and RS. Table <ref type="table">4</ref> reports the rate of adversarial samples produced by each of the four resulting substitutes and misclassified by the Google Prediction API oracle.</p><p>The model trained using Google's machine learning service is a little more robust to adversarial samples than the one trained using Amazon's service, but is still vulnerable to a large proportion of samples: 88.94% of adversarial samples produced with a perturbation ε = 0.3 using a LR substitute trained with 800 queries to the oracle are misclassified. This confirms the above demonstration of the feasibility of blackbox attacks against the classifier hosted by Amazon. Furthermore, if we use PSS and RS, the misclassification rate is 91.57% for the DNN substitute and 97.72% for the LR substitute, which again demonstrates that combining PSS and RS increases misclassification compared to the original method for ρ = 3, and reduces by a factor of 3 the number of queries (2, 000) compared to the original method for ρ = 6.</p><p>A brief discussion of defenses -In an effort to evaluate possible defenses against such attacks, we now add these adversarial samples to the MNIST training dataset and train a new instance of the classifier oracle with the same procedure. The new oracle has an accuracy of 91.25% on the MNIST test set. Adversarial samples crafted by training a new DNN substitute, even without PSS and RS, are still misclassified at a rate of 94.2% after ρ = 3 iterations and 100% after ρ = 6. This defense is thus not effective to protect the oracle from adversaries manipulating inputs. This is most likely due to the fact that the Google Prediction API uses shallow techniques to train its machine learning models, but we have no means to verify this. One could also try to deploy other defense mechanisms like defensive distillation <ref type="bibr" target="#b20">[21]</ref>. Unfortunately, as we do not have any control on the training procedure used by Google Cloud, we cannot do so. To the best of our knowledge, Google has not disclosed the machine learning technique they use to train models served by their Google Cloud Prediction API service. As such, we cannot make any further recommendations on how to better secure models trained using this service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ADVERSARIAL SAMPLE CRAFTING</head><p>This section describes machine learning techniques used in this paper, along with methods used to craft adversarial samples against classifiers learned using these techniques. Building on previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> describing how adversaries can efficiently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Deep Neural Networks</head><p>Deep Neural Networks (DNNs) learn hierarchical representations of high dimensional inputs used to solve ML tasks <ref type="bibr" target="#b10">[11]</ref>, including classification. Each representation is modeled by a layer of neurons-elementary parameterized computing units-behaving like a multi-dimensional function. The input of each layer fi is the output of the previous layer fi−1 multiplied by a set of weights, which are part of the layer's parameter θi. Thus, a DNN f can be viewed as a composition of parameterized functions f : x → fn(θn, ...f2(θ2, f1(θ1, x))...) whose parameters θ = {θi}i are learned during training. For instance, in the case of classification, the network is given a large collection of known input-label pairs ( x, y) and adjusts its parameters θ to reduce the label prediction error f ( x)−y on these inputs. At test time, the model extrapolates from its training data to make predictions f ( x) on unseen inputs.</p><p>To craft adversarial samples misclassified by DNNs, an adversary with knowledge of the model f and its parameters θ can use the fast gradient sign method introduced in <ref type="bibr" target="#b11">[12]</ref> or the Jacobian-based iterative approach proposed in <ref type="bibr" target="#b18">[19]</ref>. We only provide here a brief description of the fast gradient sign method, which is the one we use in this work. To find an adversarial sample x * approximatively solving the optimization problem stated in Equation 1, Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> proposed to compute the following perturbation:</p><formula xml:id="formula_10">δ x = ε sgn(∇ x c(f, x, y)) (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where f is the targeted DNN, c its associated cost, and y the correct label of input x. In other words, perturbations are evaluated as the sign of the model's cost function gradient with respect to inputs. An adversarial sample x * = x + δ x is successfully crafted when misclassified by model f -it satisfies f ( x * ) = f ( x)-while its perturbation δ x remains in-distinguishable to humans. The input variation ε sets the perturbation magnitude: higher input variations yield samples more likely to be misclassified by the DNN model but introduce more perturbation, which can be easier to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multi-class Logistic Regression</head><p>Multi-class logistic regression is the generalization of logistic regression to classification problems with N &gt; 2 classes <ref type="bibr" target="#b17">[18]</ref>. Logistic regression seeks to find the hypothesis best matching the data among the class of hypothesis that are a composition of a sigmoid function over the class of linear functions.</p><p>A multi-class logistic regression model f can be written as:</p><formula xml:id="formula_12">f : x → e w j • x N l=1 e w l • x j∈1..N<label>(9)</label></formula><p>where θ = {w1, ..., wN } is the set of parameters learned during training, e.g., by gradient descent or Newton's method.</p><p>Adversaries can also craft adversarial samples misclassified by multi-class logistic regression models using the fast gradient sign method <ref type="bibr" target="#b11">[12]</ref>. In the case of logistic regression, the method finds the most damaging perturbation δ x (according to the max norm) by evaluating Equation <ref type="formula" target="#formula_10">8</ref>, unlike the case of deep neural networks where it found an approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Nearest Neighbors</head><p>The k nearest neighbor (kNN) algorithm is a lazy-learning non-parametric classifier <ref type="bibr" target="#b17">[18]</ref>: it does not require a training phase. Predictions are made on unseen inputs by considering the k points in the training sets that are closest according to some distance. The estimated class of the input is the one most frequently observed among these k points. When k is set to 1, as is the case in this paper, the classifier is:</p><formula xml:id="formula_13">f : x → Y arg min z∈X z − x 2 2<label>(10)</label></formula><p>which outputs one row of Y , the matrix of indicator vectors encoding labels for the training data X.</p><p>Although the kNN algorithm is non-parametric, it is still vulnerable to adversarial samples as pointed out in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper, we used the fast gradient sign method to craft adversarial samples misclassified by nearest neighbors. To be able to differentiate the models, we use a smoothed variant of the nearest neighbor classifiers, which replaces the argmin operation in Equation 11 by a soft-min, as follows:</p><formula xml:id="formula_14">f : x → e − z− x 2 2 z∈X z∈X e − z− x 2 2 • Y<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Multi-class Support Vector Machines</head><p>One possible implementation of a multiclass linear Support Vector Machine classifier f is the one-vs-the-rest scheme.</p><p>For each class k of the machine learning task, a binary Support Vector Machine classifier f k is trained with samples of class k labeled as positive and samples from other classes labeled as negative <ref type="bibr" target="#b7">[8]</ref>. To classify a sample, each binary linear SVM classifier f k makes a prediction and the overall multiclass classifier f outputs the class assigned the strongest confidence. Each of these underlying linear SVMs is a model f k classifying unseen samples x using the following:  We now introduce an algorithm to find adversarial samples misclassified by a multi-class linear SVM f . To the best of our knowledge, this method is more computationally efficient than previous <ref type="bibr" target="#b3">[4]</ref>: it does not require any optimization. To craft adversarial samples, we perturb a given input in a direction orthogonal to the decision boundary hyperplane. More precisely, we perturb legitimate samples correctly classified by model f in the direction orthogonal to the weight vector w[k] corresponding to the binary SVM classifier f k that assigned the correct class k output by the multiclass model f . The intuition, illustrated in Figure <ref type="figure" target="#fig_6">5</ref> with a binary SVM classifier, can be formalized as follows: for a sample x belonging to class k, an adversarial sample misclassified by the multiclass SVM model f can be computed by evaluating:</p><formula xml:id="formula_15">f k : x → sgn( w[k] • x + b k )<label>(12)</label></formula><formula xml:id="formula_16">x * = x − ε • w[k] w k (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where • is the Frobenius norm, w[k] the weight vector of binary SVM k, and ε the input variation parameter. The input variation parameter controls the amount of distortion introduced as is the case in the fast gradient sign method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Decision Trees</head><p>Decision trees are defined by recursively partitioning the input domain <ref type="bibr" target="#b17">[18]</ref>. Partitioning is performed by selecting a feature and a corresponding condition threshold that best minimize some cost function over the training data. Each node is a if-else statement with a threshold condition corresponding to one of the sample's features. A sample is classified by traversing the decision tree from its root to one of its leaves accordingly to conditions specified in intermediate tree nodes. The leaf reached indicates the class assigned.</p><p>Adversaries can also craft adversarial inputs misclassified by decision trees. To the best of our knowledge, this is the first adversarial sample crafting algorithm proposed for decision trees. The intuition exploits the underlying tree structure of the classifier model. To find an adversarial sample, given a sample and a tree, we simply search for leaves with different classes in the neighborhood of the leaf corresponding to the decision tree's original prediction for the sample. We then find the path from the original leaf to the adversarial leaf and modify the sample accordingly to the conditions on this path so as to force the decision tree to misclassify the sample in the adversarial class specified by the newly identified leaf.  This intuition, depicted in Figure <ref type="figure">6</ref>, is formalized by Algorithm 2. The algorithm takes a decision tree T , a sample x, the legitimate_class for sample x, and outputs an adversarial sample x * misclassified by decision tree T . The algorithm does not explicitly minimize the amount of perturbation introduced to craft adversarial samples, but as shown in Section 3.3, we found in practice that perturbations found involve a minuscule proportion of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION AND RELATED WORK</head><p>Upon completion of their training on collections of known input-label pairs ( x, y), classifiers f make label predictions f (x) on unseen inputs x <ref type="bibr" target="#b17">[18]</ref>. Models extrapolate from knowledge extracted by processing input-label pairs during training to make label predictions. Several factors, including (1) imperfections in the training algorithms, (2) the linearity of many underlying components used to built machine learning models, and (3) the limited amount of training points not always representative of the entire plausible input domain, leave numerous machine learning models exposed to adversarial manipulations of their inputs despite having excellent performances on legitimate-expected-inputs.</p><p>Our work builds on a practical method for attacks against black-box deep learning classifiers <ref type="bibr" target="#b19">[20]</ref>. Learning substitute models approximating the decision boundaries of targeted classifiers alleviates the need of previous attacks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> for knowledge of the target architecture and parameters. We generalized this method and showed that it can target any machine learning classifier. We also reduced its computational cost by (1) introducing substitute models trained using logistic regression instead of deep learning and (2) decreasing the number of queries made with reservoir sampling. Learning substitutes is an instance of knowledge transfer, a set of techniques to transfer the generalization knowledge learned by a model into another model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>This paper demonstrates that adversaries can reliably target classifiers whose characteristics are unknown, deployed remotely, e.g., by machine learning as a service platforms. The existence of such a threat vector calls for the design of defensive mechanisms <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, we found that defenses proposed in the literature-such as training with adversarial samples <ref type="bibr" target="#b11">[12]</ref>-were noneffective, or we were unable to deploy them because of our lack of access to the machine learning model targeted-for instance distillation <ref type="bibr" target="#b20">[21]</ref>. This failure is most likely due to the shallowness of models like logistic regression, which support the services offered by Amazon and Google, although we are unable to confirm that statement in Google's case using available documentation. This work is part of a series of security evaluations of machine learning algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Unlike us, previous work in this field assumed knowledge of the model architecture and parameters <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. Our threat model considered adversaries interested in misclassification at test time, once the model has been deployed. Other largely unexplored threat models exist. For instance poisoning the training data used to learn models was only considered in the context of binary SVMs whose training data is known <ref type="bibr" target="#b6">[7]</ref> or anomaly detection systems whose underlying model is known <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>Our work first exposed the strong phenomenon of adversarial sample transferability across the machine learning space. Not only do we find that adversarial samples are misclassified across models trained using the same machine learning technique, but also across models trained by different techniques. We then improved the accuracy and reduced the computational complexity of an existing algorithm for learning models substitutes of machine learning classifiers. We showed that DNNs and LR could both effectively be used to learn a substitute model for many classifiers trained with a deep neural network, logistic regression, support vector machine, decision tree, and nearest neighbors. In a final experiment, we demonstrated how all of these findings could be used to target online classifiers trained and hosted by Amazon and Google, without any knowledge of the model design or parameters, but instead simply by making label queries for 800 inputs. The attack successfully forces these classifiers to misclassify 96.19% and 88.94% of their inputs. These findings call for some validation of inputs used by machine learning algorithms. This remains an open problem. Future work should continue to improve the learning of substitutes to maximize their accuracy and the transferability of adversarial samples crafted to targeted models. Further-more, poisoning attacks at training time remain largely to be investigated, leaving room for contributions to the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An adversarial sample (bottom row) is produced by slightly altering a legitimate sample (top row) in a way that forces the model to make a wrong prediction whereas a human would still correctly classify the sample [19].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Sρ and Sρ+1 are the previous and new training sets, λρ a parameter fine-tuning the augmentation step size, J f the Jacobian matrix of substitute f , and Õ( x) the oracle's label for sample x. We train a new instance f of the substitute with the augmented training set Sρ+1, which we can label simply by querying oracle Õ. By alternatively augmenting the training set and training a new instance of the substitute model for multiple iterations ρ, Papernot et al. showed that substitute DNNs can approximate another DNNs [20].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The number of queries made to the oracle is reduced from n • 2 ρ for the vanilla Jacobian-based augmentation to n • 2 σ + κ • (ρ − σ) for the Jacobian-based augmentation with reservoir sampling. Our experiments show that the reduced number of training points in the reservoir sampling variant does not significantly degrade the quality of the substitute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Label predictions matched between the DNN and LR substitutes and their target classifier oracles on test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SVM Adversarial Samples: to move a sample x away from its legitimate class in a binary SVM classifier f k , we perturb it by ε along the direction orthogonal to w[k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The oracle returns the label (not the probabilities) assigned to the sample. No other knowledge of the classifier (e.g., model type, parameters, training data) is available. To circumvent this, we build on a technique introduced in<ref type="bibr" target="#b19">[20]</ref>, which leverages a dataset augmentation technique to train the substitute model.Jacobian-based dataset augmentation -We use this augmentation technique introduced in<ref type="bibr" target="#b19">[20]</ref> to learn DNN and LR substitutes for oracles. First, one collects an initial substitute training set of limited size (representative of the task solved by the oracle) and labels it by querying the oracle. Using this labeled data, we train a first substitute model f likely to perform poorly as a source of adversarial samples due to the small numbers of samples used for training. To select additional training points, we use the following:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Impact of our refinements, Periodic Step Size (PSS)</figDesc><table><row><cell>Substitute</cell><cell>DNN</cell><cell>LR</cell><cell>SVM</cell><cell>DT</cell><cell>kNN</cell></row><row><cell>DNN</cell><cell cols="5">78.01 82.17 79.68 62.75 81.83</cell></row><row><cell>DNN+PSS</cell><cell cols="5">89.28 89.16 83.79 61.10 85.67</cell></row><row><cell cols="6">DNN+PSS+RS 82.90 83.33 77.22 48.62 82.46</cell></row><row><cell>LR</cell><cell cols="5">64.93 72.00 71.56 38.44 70.74</cell></row><row><cell>LR+PSS</cell><cell cols="5">69.20 84.01 82.19 34.14 71.02</cell></row><row><cell>LR+PSS+RS</cell><cell cols="5">67.85 78.94 79.20 41.93 70.92</cell></row></table><note>1.  To approximate them, we use the first 100 samples from the MNIST test set (unseen during training) as the initial substitute training set and follow three variants of the procedure detailed in Section 4.1 with λ = 0.1: (1) vanilla Jacobian-based augmentation,<ref type="bibr" target="#b1">(2)</ref> </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 6: Decision Tree Adversarial Samples: leaves indicate output classes (here the problem has3 output classes) whereas intermediate nodes with letters indicate binary conditions (if condition do else do). To misclassify the sample from class 3 denoted by the green leaf, the adversary modifies it such that conditions g and i evaluate accordingly for the sample to be classified in class 2 denoted by the red leaf. legit_leaf ← find leaf in T corresponding to x 3: ancestor ← legitimate_leaf 4: components ← [] 5: while predict(T, x * ) == legitimate_class do 6: if ancestor == ancestor.parent.left then 7: advers_leaf ← find leaf under ancestor.right 8: elseancestor == ancestor.parent.right 9: advers_leaf ← find leaf under ancestor.left 10: end if 11: components ← nodes from legit_leaf to advers_leaf 12: ancestor ← ancestor.parent 13: end while 14: for i ∈ components do 15: perturb x * [i] to change node's condition output 16: end for 17: return x *</figDesc><table><row><cell>Algorithm 2 Crafting Decision Tree Adversarial Samples</cell></row><row><cell>Input: T , x, legitimate_class</cell></row><row><cell>1: x</cell></row></table><note>* ← x 2:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that this is distinct from knowledge transfer, which refers to techniques designed to transfer the generalization knowledge learned by a model f during training-and encoded in its parameters-to another model f<ref type="bibr" target="#b12">[13]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Finding a closed form solution to this problem is not always possible, as some machine learning models f preclude the optimization problem from being linear or convex. Nevertheless, several approaches have been proposed to find approximative solutions to Equation1. They yield adversarial samples effectively misleading non-linear and non-convex models like neural networks<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. In addition, we introduce new techniques to craft adversarial samples against support vector machines and decision trees in Section 6.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">No model is learned during training. Predictions are made by finding k points closest to the sample in the training data, and extrapolating its class from the class of these k points.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://aws.amazon.com/machine-learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://docs.aws.amazon.com/machine-learning/ latest/dg/types-of-ml-models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://cloud.google.com/prediction/</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ML Differentiable Linear Lazy Technique Model Model Prediction DNN Yes No No LR Yes Log-linear No SVM No No No DT No No No kNN No No Yes Ens No No No</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Can machine learning be secure?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM Symposium on Information, computer and communications security</title>
				<meeting>the 2006 ACM Symposium on Information, computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lasagne: Lightweight library to build and train neural networks in theano</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference (SciPy)</title>
				<meeting>the Python for scientific computing conference (SciPy)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack. Knowledge and Data Engineering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
				<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern recognition. Machine Learning</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Learning Representations. Computational and Biological Learning Society</title>
				<meeting>the 2016 International Conference on Learning Representations. Computational and Biological Learning Society</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
				<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>Computational and Biological Learning Society</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop at NIPS 2014</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM workshop on Security and artificial intelligence</title>
				<meeting>the 4th ACM workshop on Security and artificial intelligence</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online anomaly detection under adversarial impact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="405" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine Learning in Adversarial Settings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy Magazine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016-06">May/June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE European Symposium on Security and Privacy</title>
				<meeting>the 1st IEEE European Symposium on Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th IEEE Symposium on Security and Privacy</title>
				<meeting>the 37th IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society</title>
				<meeting>the 2014 International Conference on Learning Representations. Computational and Biological Learning Society</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random sampling with a reservoir</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial perturbations of deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Warde-Farley</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Structured Prediction</title>
				<editor>
			<persName><forename type="first">T</forename><surname>Hazan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
