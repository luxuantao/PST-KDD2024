<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention</title>
				<funder>
					<orgName type="full">Hong Kong Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
				<funder ref="#_BJQ7vj4">
					<orgName type="full">Innovation and Technology Commission-Innovation and Technology Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-11">11 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ningxin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-11">11 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.07027v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision transformers named EfficientViT. We find that the speed of existing transformer models is commonly bounded by memory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we design a new building block with a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN layers, which improves memory efficiency while enhancing channel communication. Moreover, we discover that the attention maps share high similarities across heads, leading to computational redundancy. To address this, we present a cascaded group attention module feeding attention heads with different splits of the full feature, which not only saves computation cost but also improves attention diversity. Comprehensive experiments demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while running 5.8?/3.7? faster on the GPU/CPU, and 7.4? faster when converted to ONNX format. Code and models are available at here. * Work done when Xinyu was an intern of Microsoft Research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision Transformers (ViTs) have taken computer vision domain by storm due to their high model capabilities and superior performance <ref type="bibr">[18,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b67">69]</ref>. However, the constantly improved accuracy comes at the cost of increasing model sizes and computation overhead. For example, SwinV2 <ref type="bibr" target="#b41">[43]</ref> uses 3.0B parameters, while V-MoE <ref type="bibr" target="#b60">[62]</ref> taking 14.7B parameters, to achieve state-of-the-art performance on Ima-Figure <ref type="figure">1</ref>. Speed and accuracy comparisons between EfficientViT (Ours) and other efficient CNN and ViT models tested on an Nvidia V100 GPU with ImageNet-1K dataset <ref type="bibr" target="#b16">[17]</ref>.</p><p>geNet <ref type="bibr" target="#b16">[17]</ref>. Such large model sizes and the accompanying heavy computational costs make these models unsuitable for applications with real-time requirements <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b84">86]</ref>.</p><p>There are several recent works designing light and efficient vision transformer models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b77">79,</ref><ref type="bibr" target="#b79">81]</ref>. Unfortunately, most of these methods aim to reduce model parameters or Flops, which are indirect metrics for speed and do not reflect the actual inference throughput of models. For example, MobileViT-XS <ref type="bibr" target="#b48">[50]</ref> using 700M Flops runs much slower than DeiT-T <ref type="bibr" target="#b67">[69]</ref> with 1,220M Flops on an Nvidia V100 GPU. Although these methods have achieved good performance with fewer Flops or parameters, many of them do not show significant wall-clock speedup against standard isomorphic or hierarchical transformers, e.g., DeiT <ref type="bibr" target="#b67">[69]</ref> and Swin <ref type="bibr" target="#b42">[44]</ref>, and have not gained wide adoption.</p><p>To address this issue, in this paper, we explore how to go faster with vision transformers, seeking to find principles for designing efficient transformer architectures. Based on the prevailing vision transformers DeiT <ref type="bibr" target="#b67">[69]</ref> and Swin <ref type="bibr" target="#b42">[44]</ref>, we systematically analyze three main factors that affect model inference speed, including memory access, computation redundancy, and parameter usage. In particular, we find that the speed of transformer models is commonly memory-bound. In other words, memory accessing delay prohibits the full utilization of the computing power in GPU/CPUs <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b70">72]</ref>, leading to a critically negative impact on the runtime speed of transformers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">31]</ref>. The most memory-inefficient operations are the frequent tensor reshaping and element-wise functions in multi-head selfattention (MHSA). We observe that through an appropriate adjustment of the ratio between MHSA and FFN (feedforward network) layers, the memory access time can be reduced significantly without compromising the performance. Moreover, we find that some attention heads tend to learn similar linear projections, resulting in redundancy in attention maps. The analysis shows that explicitly decomposing the computation of each head by feeding them with diverse features can mitigate this issue while improving computation efficiency. In addition, the parameter allocation in different modules is often overlooked by existing lightweight models, as they mainly follow the configurations in standard transformer models <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b67">69]</ref>. To improve parameter efficiency, we use structured pruning <ref type="bibr" target="#b43">[45]</ref> to identify the most important network components, and summarize empirical guidance of parameter reallocation for model acceleration.</p><p>Based upon the analysis and findings, we propose a new family of memory efficient transformer models named Effi-cientViT. Specifically, we design a new block with a sandwich layout to build up the model. The sandwich layout block applies a single memory-bound MHSA layer between FFN layers. It reduces the time cost caused by memorybound operations in MHSA, and applies more FFN layers to allow communication between different channels, which is more memory efficient. Then, we propose a new cascaded group attention (CGA) module to improve computation efficiency. The core idea is to enhance the diversity of the features fed into the attention heads. In contrast to prior selfattention using the same feature for all heads, CGA feeds each head with different input splits and cascades the output features across heads. This module not only reduces the computation redundancy in multi-head attention, but also elevates model capacity by increasing network depth. Last but not least, we redistribute parameters through expanding the channel width of critical network components such as value projections, while shrinking the ones with lower importance like hidden dimensions in FFNs. This reallocation finally promotes model parameter efficiency.</p><p>Experiments demonstrate that our models achieve clear improvements over existing efficient CNN and ViT models in terms of both speed and accuracy, as shown in Fig. <ref type="figure">1</ref>. For instance, our EfficientViT-M5 gets 77.1% top-1 accuracy on ImageNet with throughput of 10,621 images/s on an Nvidia V100 GPU and 56.8 images/s on an Intel Xeon E5-2690 v4 CPU @ 2.60GHz, outperforming MobileNetV3-Large <ref type="bibr" target="#b24">[26]</ref> by 1.9% in accuracy, 40.4% in GPU inference speed, and 45.2% in CPU speed. Moreover, EfficientViT-M2 gets 70.8% accuracy, surpassing MobileViT-XXS <ref type="bibr" target="#b48">[50]</ref> by 1.8%, while running 5.8?/3.7? faster on the GPU/CPU, and 7.4? faster when converted to ONNX <ref type="bibr" target="#b2">[3]</ref> format. When deployed on the mobile chipset, i.e., Apple A13 Bionic chip in iPhone 11, EfficientViT-M2 model runs 2.3? faster than MobileViT-XXS <ref type="bibr" target="#b48">[50]</ref> using the CoreML <ref type="bibr" target="#b0">[1]</ref>.</p><p>In summary, the contributions of this work are two-fold:</p><p>? We present a systematic analysis on the factors that affect the inference speed of vision transformers, deriving a set of guidelines for efficient model design. ? We design a new family of vision transformer models, which strike a good trade-off between efficiency and accuracy. The models also demonstrate good transfer ability on a variety of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Going Faster with Vision Transformers</head><p>In this section, we explore how to improve the efficiency of vision transformers from three perspectives: memory access, computation redundancy, and parameter usage. We seek to identify the underlying speed bottlenecks through empirical studies, and summarize useful design guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Memory Efficiency</head><p>Memory access overhead is a critical factor affecting model speed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b63">65]</ref>. Many operators in transformer <ref type="bibr" target="#b69">[71]</ref>, such as frequent reshaping, element-wise addition, and normalization are memory inefficient, requiring timeconsuming access across different memory units, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. Although there are some methods proposed to address this issue by simplifying the computation of standard softmax self-attention, e.g., sparse attention <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b73">75]</ref> and low-rank approximation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b72">74]</ref>, they often come at the cost of accuracy degradation and limited acceleration.</p><p>In this work, we turn to save memory access cost by reducing memory-inefficient layers. Recent studies reveal that memory-inefficient operations are mainly located in MHSA rather than FFN layers <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b31">33]</ref>. However, most existing ViTs <ref type="bibr">[18,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b67">69]</ref> use an equivalent number of these two layers, which may not achieve the optimal efficiency. We thereby explore the optimal allocation of MHSA and FFN layers in small models with fast inference. Specifically, we scale down Swin-T <ref type="bibr" target="#b42">[44]</ref> and DeiT-T <ref type="bibr" target="#b67">[69]</ref> to several small subnetworks with 1.25? and 1.5? higher inference throughput, and compare the performance of subnetworks with different proportions of MHSA layers. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, subnetworks with 20%-40% MHSA layers tend to get better accuracy. Such ratios are much smaller than the typical ViTs that adopt 50% MHSA layers. Furthermore, we measure the time consumption on memory-bound operations to compare memory access efficiency, including reshaping, element-wise addition, copying, and normalization. Memory-bound operations is reduced to 44.26% of the total runtime in Swin-T-1.25? that has 20% MHSA layers. The observation also generalizes to DeiT and smaller models with 1.5? speed-up. It is demonstrated that reducing MHSA layer utilization ratio appropriately can enhance memory efficiency while improving model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Computation Efficiency</head><p>MHSA embeds the input sequence into multiple subspaces (heads) and computes attention maps separately, which has been proven effective in improving performance <ref type="bibr">[18,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b69">71]</ref>. However, attention maps are computationally expensive, and studies have shown that a number of them are not of vital importance <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b71">73]</ref>. To save computation cost, we explore how to reduce redundant attention in small ViT models. We train width downscaled Swin-T <ref type="bibr" target="#b42">[44]</ref> and DeiT-T [69] models with 1.25? inference speed-up, and measure the maximum cosine similarity of each head and the remaining heads within each block. From Fig. <ref type="figure" target="#fig_2">4</ref>, we observe there exists high similarities between attention heads, especially in the last blocks. The phenomenon suggests that many heads learn similar projections of the same full feature and incur computation redundancy. To explicitly encourage the heads to learn different patterns, we apply an intuitive solution by feeding each head with only a split of the full feature, which is similar to the idea of group convolution in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b85">87]</ref>. We train the variants of downscaled models with the modified MHSA, and also compute the attention similarities in Fig. <ref type="figure" target="#fig_2">4</ref>. It is shown that using different channel-wise splits of the feature in different heads, instead of using the same full feature for all heads as MHSA, could effectively mitigate attention computation redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Parameter Efficiency</head><p>Typical ViTs mainly inherit the design strategies from NLP transformer <ref type="bibr" target="#b69">[71]</ref>, e.g., using an equivalent width for Q,K,V projections, increasing heads over stages, and setting the expansion ratio to 4 in FFN. For lightweight mod-  els, the configurations of these components need to be carefully re-designed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">39]</ref>. Inspired by <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b80">82]</ref>, we adopt Taylor structured pruning <ref type="bibr" target="#b51">[53]</ref> to automatically find the important components in Swin-T and DeiT-T, and explore the underlying principles of parameter allocation. The pruning method removes unimportant channels under a certain resource constraint and keeps the most critical ones to best preserve the accuracy. It uses the multiplication of gradient and weight as channel importance, which approximates the loss fluctuation when removing channels <ref type="bibr" target="#b36">[38]</ref>.</p><p>The ratio between the remaining output channels to the input channels is plotted in Fig. <ref type="figure" target="#fig_3">5</ref>, and the original ratios in the unpruned model are also given for reference. It is observed that: 1) The first two stages preserve more dimensions, while the last stage keeps much less; 2) The Q,K and FFN dimensions are largely trimmed, whereas the dimension of V is almost preserved and diminishes only at the last few blocks. These phenomena show that 1 ) the typical channel configuration, that doubles the channel after each stage <ref type="bibr" target="#b42">[44]</ref> or use equivalent channels for all blocks <ref type="bibr" target="#b67">[69]</ref>, may produce substantial redundancy in last few blocks; 2) The redundancy in Q,K is much larger than V when they have the same dimensions. V prefers a relative large channels, being close to the input embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Efficient Vision Transformer</head><p>Based upon the above analysis, in this section, we propose a new hierarchical model with fast inference named EfficientViT. The architecture overview is shown in Fig. <ref type="figure" target="#fig_5">6</ref>.  </p><formula xml:id="formula_0">Stage 1 ? L 1 EfficientViT Block Stage 2 Overlap PatchEmbed H ? W ? 3 H 16 ? W 16 ? C 1 EfficientViT Subsample H 32 ? W 32 ? C 2 Stage 3 H 64 ? W 64 ? C 3 AvgPool + Classifier Input ? Token Interaction Split Q K V Q K V Q K V + + Concat &amp; Projection</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">EfficientViT Building Blocks</head><p>We propose a new efficient building block for vision transformer, as shown in Fig. <ref type="figure" target="#fig_5">6 (b)</ref>. It is composed of a memory-efficient sandwich layout, a cascaded group attention module, and a parameter reallocation strategy, which focus on improving model efficiency in terms of memory, computation, and parameter, respectively.</p><p>Sandwich Layout. To build up a memory-efficient block, we propose a sandwich layout that employs less memorybound self-attention layers and more memory-efficient FFN layers for channel communication. Specifically, it applies a single self-attention layer ? A i for spatial mixing, which is sandwiched between FFN layers ? F i . The computation can be formulated as:</p><formula xml:id="formula_1">X i+1 = N ? F i (? A i ( N ? F i (X i ))),<label>(1)</label></formula><p>where X i is the full input feature for the i-th block. The block transforms X i into X i+1 with N FFNs before and after the single self-attention layer. This design reduces the memory time consumption caused by self-attention layers in the model, and applies more FFN layers to allow communication between different feature channels efficiently. We also apply an extra token interaction layer before each FFN using a depthwise convolution (DWConv) <ref type="bibr" target="#b25">[27]</ref>. It introduces inductive bias of the local structural information to enhance model capability <ref type="bibr" target="#b13">[14]</ref>. Cascaded Group Attention. Attention head redundancy is a severe issue in MHSA, which causes computation inefficiency. Inspired by group convolutions in efficient CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b85">87]</ref>, we propose a new attention module named cascaded group attention (CGA) for vision transformers. It feeds each head with different splits of the full features, thus explicitly decomposing the attention computation across heads. Formally, this attention can be formulated as:</p><formula xml:id="formula_2">X ij = Attn(X ij W Q ij , X ij W K ij , X ij W V ij ), X i+1 = Concat[ X ij ] j=1:h W P i ,<label>(2)</label></formula><p>where the j-th head computes the self-attention over X ij , which is the j-th split of the input feature X i , i.e., X i = [X i1 , X i2 , . . . , X ih ] and 1 ? j ? h. h is the total number of heads, W Q ij , W K ij , and W V ij are projection layers mapping the input feature split into different subspaces, and W P i is a linear layer that projects the concatenated output features back to the dimension consistent with the input.</p><p>Although using feature splits instead of the full features for each head is more efficient and saves computation overhead, we continue to improve its capacity, by encouraging the Q, K, V layers to learn projections on features with richer information. We compute the attention map of each head in a cascaded manner, as illustrated in Fig. <ref type="figure" target="#fig_5">6 (c)</ref>, which adds the output of each head to the subsequent head to refine the feature representations progressively:</p><formula xml:id="formula_3">X ij = X ij + X i(j-1) , 1 &lt; j ? h,<label>(3)</label></formula><p>where X ij is the addition of the j-th input split X ij and the (j-1)-th head output X i(j-1) calculated by Eq. ( <ref type="formula" target="#formula_2">2</ref>). It replaces X ij to serve as the new input feature for the j-th head when calculating the self-attention. Besides, another token interaction layer is applied after the Q projection, which enables the self-attention to jointly capture local and global relations and further enhances the feature representation. Such a cascaded design enjoys two advantages. First, feeding each head with different feature splits could improve the diversity of attention maps, as validated in Sec. 2.2. Similar to group convolutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b85">87]</ref>, the cascaded group attention could save the Flops and parameters by h?, since the input and output channels in the QKV layers are reduced by h?. Second, cascading the attention heads allows for an increase of network depth, thus further elevating the model capacity without introducing any extra parameters. It only incurs minor latency overhead since the attention map computation in each head uses smaller QK channel dimensions.</p><p>Parameter Reallocation. To improve parameter efficiency, we reallocate the parameters in the network by expanding the channel width of critical modules while shrinking the unimportant ones. Specifically, based on the Taylor importance analysis in Sec. 2.3, we set small channel dimensions for Q and K projections in each head for all stages. For the V projection, we allow it to have the same dimension as the input embedding. The expansion ratio in FFN is also reduced from 4 to 2 due to its parameter redundancy. With the proposed reallocation strategy, the important modules have larger number of channels to learn representations in a high dimensional space, which prevent the loss of feature information. Meanwhile, the redundant parameters in unimportant modules are removed to speed up inference and enhance the model efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">EfficientViT Network Architectures</head><p>The overall architecture of our EfficientViT is presented in Fig. <ref type="figure" target="#fig_5">6 (a)</ref>. Concretely, we introduce overlapping patch embedding <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b78">80]</ref> to embed 16?16 patches into tokens with C 1 dimension, which enhances the model capacity in low-level visual representation learning. The architecture contains three stages. Each stage stacks the proposed Ef-ficientViT building blocks and the number of tokens is reduced by 4? at each subsampling layer (2? subsampling of the resolution). To achieve efficient subsampling, we propose an EfficientViT subsample block which also has the sandwich layout, except that the self-attention layer is replaced by an inverted residual block to reduce the information loss during subsampling <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b61">63]</ref>. It is worth noting that we adopt BatchNorm (BN) <ref type="bibr" target="#b28">[30]</ref> instead of Layer-Norm (LN) <ref type="bibr" target="#b1">[2]</ref> throughout the model, as BN can be folded into the preceding convolution or linear layers, which is a runtime advantage over LN. We also use ReLU <ref type="bibr" target="#b52">[54]</ref> as the activation function, as the commonly used GELU <ref type="bibr" target="#b23">[25]</ref> or HardSwish <ref type="bibr" target="#b24">[26]</ref> are much slower, and sometimes not wellsupported by certain inference deployment platforms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>We build our model family with six different width and depth scales, and set different number of heads for each stage. We use fewer blocks in early stages than late stages similar to MobileNetV3 <ref type="bibr" target="#b24">[26]</ref> and LeViT <ref type="bibr" target="#b18">[20]</ref>, since that the processing on early stages with larger resolutions is more time consuming. We increase the width over stages with a small factor (? 2) to alleviate redundancy in later stages, as analyzed in Sec. 2.3. The architecture details of our model family are presented in Tab. 1. C i , L i , and H i refer to the width, depth, and number of heads in the i-th stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We conduct image classification experiments on ImageNet-1K <ref type="bibr" target="#b16">[17]</ref>. The models are built with PyTorch 1.11.0 <ref type="bibr" target="#b57">[59]</ref> and Timm 0.5.4 <ref type="bibr" target="#b75">[77]</ref>, and trained from scratch for 300 epochs on 8 Nvidia V100 GPUs using AdamW <ref type="bibr" target="#b44">[46]</ref> optimizer and cosine learning rate scheduler. We set the total batchsize as 2,048. The input images are resized and randomly cropped into 224?224. The initial learning rate is 1?10 -3 with weight decay of 2.5?10 -2 . We use the same data augmentation as <ref type="bibr" target="#b67">[69]</ref>, including Mixup <ref type="bibr" target="#b83">[85]</ref>, auto-augmentation <ref type="bibr" target="#b12">[13]</ref>, and random erasing <ref type="bibr" target="#b86">[88]</ref>. In addition, we provide throughput evaluation on different hardware. For GPU, we measure the throughput on an Nvidia V100, with the maximum power-of-two batchsize that fits in memory following <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b67">69]</ref>. For CPU and ONNX, we measure the runtime on an Intel Xeon E5-2690 v4 @ 2.60 GHz processor, with batchsize 16 and run the model in a single thread following <ref type="bibr" target="#b18">[20]</ref>. We also test the transferability of EfficientViT on downstream tasks. For the experiments on downstream image classification, we finetune the models for 300 epochs following <ref type="bibr" target="#b84">[86]</ref>, using AdamW <ref type="bibr" target="#b44">[46]</ref> with batchsize 256, learning rate 1?10 -3 and weight-decay 1?10 -8 . We use RetinaNet <ref type="bibr" target="#b39">[41]</ref> for object detection on COCO <ref type="bibr" target="#b40">[42]</ref>, and train the models for 12 epochs (1? schedule) with the same settings as <ref type="bibr" target="#b42">[44]</ref> on mmdetection <ref type="bibr" target="#b5">[6]</ref>. For instance segmentation, please refer to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on ImageNet</head><p>We compare EfficientViT with prevailing efficient CNN and ViT models on ImageNet <ref type="bibr" target="#b16">[17]</ref>, and report the results in Tab. 2 and Fig. <ref type="figure">1</ref>. The results show that, in most cases, our EfficientViT achieves the best accuracy and speed trade-off across different evaluation settings.</p><p>Comparisons with efficient CNNs. We first compare Ef-ficientViT with vanilla CNN models, such as MobileNets Table <ref type="table">2</ref>. EfficientViT image classification performance on ImageNet-1K <ref type="bibr" target="#b16">[17]</ref> with comparisons to state-of-the-art efficient CNN and ViT models trained without extra data. Throughput is tested on Nvidia V100 for GPU and Intel Xeon E5-2690 v4 @ 2.60 GHz processor for CPU and ONNX, where larger throughput means faster inference speed. ?: finetune with higher resolution.  <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b61">63]</ref> and EfficientNet <ref type="bibr" target="#b65">[67]</ref>. Specifically, compared to MobileNetV2 1.0? <ref type="bibr" target="#b61">[63]</ref>, EfficientViT-M3 obtains 1.4% better top-1 accuracy, while running at 2.5? and 3.0? faster speed on V100 GPU and Intel CPU, respectively. Compared to the state-of-the-art MobileNetV3-Large <ref type="bibr" target="#b24">[26]</ref>, EfficientViT-M5 achieves 1.9% higher accuracy yet runs much faster, e.g., 40.5% faster on the V100 GPU and 45.2% faster on the Intel CPU but is 11.5% slower as ONNX models. This may because reshaping is slower in ONNX implementation, which is inevitable in computing self-attention. Moreover, EfficientViT-M5 achieves comparable accuracy with the searched model EfficientNet-B0 <ref type="bibr" target="#b65">[67]</ref>, while runs 2.3?/1.9? faster on the V100 GPU/Intel CPU, and 2.1? faster as ONNX models. Although our model uses more parameters, it reduces memory-inefficient operations that affect the inference speed and achieves higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Comparisons with efficient ViTs. We also compare our models with recent efficient vision transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b54">56]</ref> in Tab. 2. In particular, when getting similar performance on ImageNet-1K <ref type="bibr" target="#b16">[17]</ref>, our EfficientViT-M4 runs 4.4? and 3.0? faster than the recent EdgeViT-XXS <ref type="bibr" target="#b54">[56]</ref> on the tested CPU and GPU devices. Even converted to ONNX runtime format, our model still gets 3.7? higher speed. Compared to the state-of-the-art MobileViTV2-0.5 <ref type="bibr" target="#b49">[51]</ref>, our EfficientViT-M2 achieves slightly better performance with higher throughput, e.g., 3.4? and 3.5? higher throughput tested on the GPU and CPU devices, respectively. Furthermore, we compare with tiny variants of state-of-the-art large ViTs in Tab. 3. PoolFormer-12S <ref type="bibr" target="#b81">[83]</ref> has comparable accuracy with EfficientViT-M5 yet runs 3.0? slower on the V100 GPU. Compared to Swin-T <ref type="bibr" target="#b42">[44]</ref>, EfficientViT-M5 is 4.1% inferior in accuracy yet is 12.3? faster on the Intel CPU, demonstrating the efficiency of the proposed design. In addition, we present the speed evaluation and comparison on mobile chipsets in the supplementary material.</p><p>Finetune with higher resolutions. Recent works on ViTs have demonstrated that finetuning with higher resolutions can further improve the capacity of the models. We also finetune our largest model EfficientViT-M5 to higher resolutions. EfficientViT-M5?384 reaches 79.8% top-1 accuracy with throughput of 3,986 images/s on the V100 GPU, and EfficientViT-M5?512 further improves the top-1 accuracy to 80.8%, demonstrating the efficiency on processing images with larger resolutions and the good model capacity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning Results</head><p>To further evaluate the transfer ability, we apply Effi-cientViT on various downstream tasks. Downstream Image Classification. We transfer Effi-cientViT to downstream image classification datasets to test its generalization ability: 1) CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b34">[36]</ref>; 2) fine-grained classification: Flowers <ref type="bibr" target="#b53">[55]</ref>, Stanford Cars <ref type="bibr" target="#b33">[35]</ref>, and Oxford-IIIT Pets <ref type="bibr" target="#b56">[58]</ref>. We report the results in Tab. 4. Compared to existing efficient models <ref type="bibr">[18,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b87">89]</ref>, our EfficientViT-M5 achieves comparable or slightly better accuracy across all datasets with much higher throughput. An exception lies in Cars, where our model is slightly inferior in accuracy. This may because the subtle differences between classes lie more in local details thus is more feasible to be captured with convolution.</p><p>Object Detection. We compare EfficientViT-M4 with efficient models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b66">68]</ref> on the COCO <ref type="bibr" target="#b40">[42]</ref> object detection task, and present the results in Tab. 5. Specifically, EfficientViT-M4 surpasses MobileNetV2 <ref type="bibr" target="#b61">[63]</ref> by 4.4% AP with comparable Flops. Compared to the searched method SPOS <ref type="bibr" target="#b20">[22]</ref>, our EfficientViT-M4 uses 18.1% fewer Flops while achieving 2.0% higher AP, demonstrating its capacity and generalization ability in different vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we ablate important design elements in the proposed EfficientViT on ImageNet-1K <ref type="bibr" target="#b16">[17]</ref>. All models are trained for 100 epochs to magnify the differences and reduce training time <ref type="bibr" target="#b18">[20]</ref>. Tab. 6 reports the results.</p><p>Impact of the sandwich layout block. We first present an ablation study to verify the efficiency of the proposed sandwich layout design, by replacing the sandwich layout block with the original Swin block <ref type="bibr" target="#b42">[44]</ref>. The depth is adjusted to {2, 2, 3} to guarantee similar throughput with EfficientViT-M4 for a fair comparison. The top-1 accuracy degrades by 3.0% at a similar speed, verifying that applying more FFNs instead of memory-bound MHSA is more effective for small models. Furthermore, to analyze the impact of the number of FFNs N before and after self-attention , we change the number from 1 to 2 and 3. The number of blocks is reduced accordingly to maintain similar throughput. As presented in Tab. 6 (#3 and #4), further increasing the number of FFNs is not effective due to the lack of long-range spatial relation and N =1 achieves the best efficiency. Impact of the cascaded group attention. We have proposed CGA to improve the computation efficiency of MHSA. As shown in Tab. 6 (#5 and #6), replacing CGA with MHSA decreases the accuracy by 1.1% and ONNX speed by 5.9%, suggesting that addressing head redundancy improves the model efficiency. For the model without the cascade operation, its performance is comparable with MHSA but worse than CGA, demonstrating the efficacy of enhancing the feature representations of each head.</p><p>Impact of the parameter reallocation. Our EfficientViT-M4 yields 1.4%/1.5% higher top-1 accuracy, 4.9%/3.8% higher GPU throughput than the models without QKV channel dimension reallocation or FFN ratio reduction, respectively, indicating the effectiveness of parameter reallocation (#1 vs. #7, #8). Moreover, we study the choices of QK dimension in each head and the ratio of V dimension to the input embedding in Fig. <ref type="figure" target="#fig_6">7</ref>. It is shown that the performance is improved gradually as QK dimension increases from 4 to 16, while further increasing it gives inferior performance. Besides, the performance improves from 70.3% to 71.3% when increasing the ratio between V dimension and input embedding from 0.4 to 1.0. When further enlarging the ratio to 1.2, it only gets 0.1% improvements. Therefore, setting the channels of V close to the input embedding achieves the best parameter efficiency, which meets our analysis in Sec. 2.3 and design strategy.</p><p>Impact of other components. We ablate the impact of using DWConv for token interaction, the normalization layer, and the activation function, as presented in Tab. 6 (#9, #10, and #11). With DWConv, the accuracy improves by 1.4% with a minor latency overhead, demonstrating the effectiveness of introducing local structural information. Replacing BN with LN decreases accuracy by 0.9% and GPU speed by 2.9%. Using HardSwish instead of ReLU improves accuracy by 0.9% but leads to a large drop of 20.0% ONNX speed. The activation functions are element-wise operations that occupy a considerable amount of processing time on GPU/CPU <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b70">72]</ref>, thus utilizing ReLU instead of more complicated activation functions is of better efficiency.</p><p>Results of 1,000 training epochs and distillation. Tab. 7 shows the results with 1,000 training epochs and knowledge distillation using RegNetY-16GF <ref type="bibr" target="#b58">[60]</ref> as the teacher model following <ref type="bibr" target="#b18">[20]</ref> on ImageNet-1K <ref type="bibr" target="#b16">[17]</ref> and ImageNet-ReaL <ref type="bibr" target="#b3">[4]</ref>. Compared to LeViT-128S <ref type="bibr" target="#b18">[20]</ref>, EfficientViT-M4 surpasses it by 0.5% on ImageNet-1K and 1.0% on ImageNet-ReaL, respectively. For the inference speed, our model has 34.2% higher throughput on ONNX and also shows superiority on other settings. The results demonstrate that the strong capability and generalization ability of EfficientViT can be further explored with longer training schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Efficient CNNs. With the demand of deploying CNNs on resource-constrained scenarios, efficient CNNs have been intensively studied in literature <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b85">87]</ref>. Xception <ref type="bibr" target="#b9">[10]</ref> proposes an architecture built with depthwise separable convolutions. MobileNetV2 <ref type="bibr" target="#b61">[63]</ref> builds an inverted residual structure which expands the input to a higher dimension. MobileNetV3 <ref type="bibr" target="#b24">[26]</ref> and EfficientNet <ref type="bibr" target="#b65">[67]</ref> resort to neural architecture search techniques to design compact models. To boost the actual speed on hardware, ShuffleNetV2 <ref type="bibr" target="#b46">[48]</ref> introduces channel split and shuffle operations to improve the information communication among channel groups. However, the spatial locality of convolutional kernels hampers CNN models from capturing long- range dependencies, thus limiting their model capacity.</p><p>Efficient ViTs. ViT and its variants <ref type="bibr">[18,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b74">76]</ref> have achieved success on various vision tasks. Despite the superior performance, most of them are inferior to typical CNNs in inference speed. Some efficient transformers have been proposed recently and they fall into two camps: 1) efficient self-attention; and 2) efficient architecture design. Efficient self-attention methods reduce the cost of softmax attention via sparse attention <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b73">75]</ref> or low-rank approximation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b72">74]</ref>. However, they suffer from performance degradation with negligible or moderate inference acceleration over softmax attention <ref type="bibr" target="#b69">[71]</ref>. Another line of work combines ViTs with lightweight CNNs to build efficient architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b79">81]</ref>. LVT <ref type="bibr" target="#b79">[81]</ref> proposes enhanced attention mechanisms with dilated convolution to improve the model performance and efficiency. Mobile-Former <ref type="bibr" target="#b8">[9]</ref> designs a parallel CNN-transformer block to encode both local features and global interaction. However, most of them target at minimizing Flops and parameters <ref type="bibr" target="#b15">[16]</ref>, which could have low correlations with actual inference latency <ref type="bibr" target="#b68">[70]</ref> and still inferior to efficient CNNs in speed. Different from them, we explore models with fast inference by directly optimizing their throughput on different hardware and deployment settings, and design a family of hierarchical models with a good trade-off between speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a systematic analysis on the factors that affect the inference speed of vision transformers, and proposed a new family of fast vision transformers with memory-efficient operations and cascaded group attention, named EfficientViT. Extensive experiments have demonstrated the efficacy and high speed of Effi-cientViT, and also show its superiority on various downstream benchmarks.</p><p>Limitations. One limitation of EfficientViT is that, despite its high inference speed, the model size is slightly larger compared to state-of-the-art efficient CNN <ref type="bibr" target="#b24">[26]</ref> due to the extra FFNs in the introduced sandwich layout. Besides, our models are designed manually based on the derived guidelines on building efficient vision transformers. In future work, we are interested in reducing the model size and incorporating automatic search techniques to further enhance the model capacity and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Runtime profiling on two standard vision transformers Swin-T and DeiT-T. Red text denotes memory-bound operations, i.e., the time taken by the operation is mainly determined by memory accesses, while time spent in computation is much smaller.</figDesc><graphic url="image-2.png" coords="2,318.57,60.04,214.34,92.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The accuracy of downscaled baseline models with different MHSA layer proportions, where the dots on each line represent subnetworks with similar throughput. Left: Swin-T as the baseline. Right: DeiT-T as the baseline. The 1.25?/1.5? denote accelerating the baseline models by 1.25/1.5 times, respectively.</figDesc><graphic url="image-3.png" coords="3,62.79,64.03,208.40,91.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The average maximum cosine similarity of each head in different blocks. Left: downscaled Swin-T models. Right: downscaled DeiT-T models. Blue lines denote Swin-T-1.25?/DeiT-T-1.25? model, while darkblue lines denote the variants that feed each head with only a split of the full feature.</figDesc><graphic url="image-4.png" coords="3,321.54,61.04,208.40,95.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The ratio of the channels to the input embeddings before and after pruning Swin-T. Baseline accuracy: 79.1%; pruned accuracy: 76.5%. Results for DeiT-T are given in the supplementary.</figDesc><graphic url="image-5.png" coords="3,321.54,214.16,208.39,91.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Overview of EfficientViT. (a) Architecture of EfficientViT; (b) Sandwich Layout block; (c) Cascaded Group Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Ablation on the QK dimension of each head and the ratio of V dimension to the input embedding.</figDesc><graphic url="image-9.png" coords="8,309.42,60.05,232.64,53.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Architecture details of EfficientViT model variants.Model {C 1 , C 2 , C 3 } {L 1 , L 2 , L 3 } {H 1 , H 2 , H 3 }</figDesc><table><row><cell>EfficientViT-M0 {64, 128, 192} {1, 2, 3}</cell><cell>{4, 4, 4}</cell></row><row><cell>EfficientViT-M1 {128, 144, 192} {1, 2, 3}</cell><cell>{2, 3, 3}</cell></row><row><cell>EfficientViT-M2 {128, 192, 224} {1, 2, 3}</cell><cell>{4, 3, 2}</cell></row><row><cell>EfficientViT-M3 {128, 240, 320} {1, 2, 3}</cell><cell>{4, 3, 4}</cell></row><row><cell>EfficientViT-M4 {128, 256, 384} {1, 2, 3}</cell><cell>{4, 4, 4}</cell></row><row><cell>EfficientViT-M5 {192, 288, 384} {1, 3, 4}</cell><cell>{3, 3, 4}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the tiny variants of state-of-the-art large-scale ViTs on ImageNet-1K<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell></cell><cell>Top-1</cell><cell cols="3">Throughput (imgs/s)</cell><cell cols="2">Flops Params</cell></row><row><cell>Model</cell><cell>(%)</cell><cell cols="3">GPU CPU ONNX</cell><cell>(G)</cell><cell>(M)</cell></row><row><cell>PVTV2-B0 [76]</cell><cell>70.5</cell><cell>3507</cell><cell>12.7</cell><cell>18.5</cell><cell>0.6</cell><cell>1.4</cell></row><row><cell>T2T-ViT-7 [84]</cell><cell>71.7</cell><cell>1156</cell><cell>22.5</cell><cell>16.1</cell><cell>1.1</cell><cell>4.3</cell></row><row><cell>DeiT-T [69]</cell><cell>72.2</cell><cell>4631</cell><cell>26.0</cell><cell>25.1</cell><cell>1.3</cell><cell>5.9</cell></row><row><cell>PoolFormer-12S [83]</cell><cell>77.2</cell><cell>3534</cell><cell>10.4</cell><cell>14.6</cell><cell>1.9</cell><cell>12.0</cell></row><row><cell>EffFormer-L1 [40]</cell><cell>79.2</cell><cell>4465</cell><cell>12.9</cell><cell>21.2</cell><cell>1.3</cell><cell>12.3</cell></row><row><cell>Swin-T [44]</cell><cell>81.2</cell><cell>1393</cell><cell>4.6</cell><cell>6.4</cell><cell>4.5</cell><cell>29.0</cell></row><row><cell>EfficientViT-M5</cell><cell>77.1</cell><cell cols="2">10621 56.8</cell><cell>62.5</cell><cell>0.5</cell><cell>12.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results of EfficientViT and other efficient models on downstream image classification datasets.</figDesc><table><row><cell>Model</cell><cell>Throughput</cell><cell>ImageNet</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Cars</cell><cell>Flowers</cell><cell>Pets</cell></row><row><cell>MobileNetV1 [27]</cell><cell cols="7">8543 70.6 96.1 82.3 91.4 96.7 89.9</cell></row><row><cell>MobileNetV2 [63]</cell><cell cols="7">6534 72.9 95.7 80.8 91.0 96.6 90.5</cell></row><row><cell>MobileNetV3 [26]</cell><cell cols="7">7560 75.2 97.6 85.5 91.2 97.0 90.1</cell></row><row><cell cols="8">NASNet-A-M [89] 2623 74.1 96.8 83.9 88.5 96.8 89.4</cell></row><row><cell>ViT-S/16 [18]</cell><cell cols="4">2135 81.4 97.6 85.7</cell><cell>-</cell><cell cols="2">86.4 90.4</cell></row><row><cell>EfficientViT-M5</cell><cell cols="7">10621 77.1 98.0 86.4 89.7 97.1 92.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>EfficientViT object detection performance on COCO val2017<ref type="bibr" target="#b40">[42]</ref> with comparisons to other efficient models.AP AP 50 AP 75 AP s AP m AP l (M) (M)</figDesc><table><row><cell></cell><cell>RetinaNet 1?</cell><cell cols="2">Flops Params</cell></row><row><cell>Model</cell><cell></cell><cell></cell></row><row><cell cols="3">MobileNetV2 [63] 28.3 46.7 29.3 14.8 30.7 38.1 300</cell><cell>3.4</cell></row><row><cell cols="3">MobileNetV3 [26] 29.9 49.3 30.8 14.9 33.3 41.1 217</cell><cell>5.4</cell></row><row><cell>SPOS [22]</cell><cell cols="2">30.7 49.8 32.2 15.4 33.9 41.6 365</cell><cell>4.3</cell></row><row><cell cols="3">MNASNet-A2 [66] 30.5 50.2 32.0 16.6 34.1 41.1 340</cell><cell>4.8</cell></row><row><cell>FairNAS-C [12]</cell><cell cols="2">31.2 50.8 32.7 16.3 34.4 42.3 325</cell><cell>5.6</cell></row><row><cell>MixNet-M [68]</cell><cell cols="2">31.3 51.7 32.4 17.0 35.0 41.9 360</cell><cell>5.0</cell></row><row><cell>EfficientViT-M4</cell><cell cols="2">32.7 52.2 34.1 17.6 35.3 46.0 299</cell><cell>8.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation for EfficientViT-M4 on ImageNet-1K<ref type="bibr" target="#b16">[17]</ref> dataset. Top-1 accuracy, GPU and ONNX throughput are reported.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Throughput (imgs/s)</cell></row><row><cell># Ablation</cell><cell>Top-1 (%)</cell><cell>GPU</cell><cell>ONNX</cell></row><row><cell>1 EfficientViT-M4</cell><cell>71.3</cell><cell>15914</cell><cell>108.6</cell></row><row><cell>2 Sandwich ? Swin [44]</cell><cell>68.3</cell><cell>15804</cell><cell>114.5</cell></row><row><cell>3 N = 1 ? 2</cell><cell>70.2</cell><cell>14977</cell><cell>112.3</cell></row><row><cell>4 N = 1 ? 3</cell><cell>65.7</cell><cell>15856</cell><cell>139.7</cell></row><row><cell>5 CGA ? MHSA [18]</cell><cell>70.2</cell><cell>16243</cell><cell>102.2</cell></row><row><cell>6 Cascade ? None</cell><cell>69.8</cell><cell>16411</cell><cell>111.0</cell></row><row><cell>7 QKV allocation ? None</cell><cell>69.9</cell><cell>15132</cell><cell>103.1</cell></row><row><cell>8 FFN ratio 2 ? 4</cell><cell>69.8</cell><cell>15310</cell><cell>112.4</cell></row><row><cell>9 DWConv ? None</cell><cell>69.9</cell><cell>16325</cell><cell>110.4</cell></row><row><cell>10 BN ? LN [2]</cell><cell>70.4</cell><cell>15463</cell><cell>103.6</cell></row><row><cell>11 ReLU ? HSwish [26]</cell><cell>72.2</cell><cell>15887</cell><cell>87.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison on ImageNet-1K<ref type="bibr" target="#b16">[17]</ref> and ImageNet-ReaL<ref type="bibr" target="#b3">[4]</ref>. Results with ? are trained with 1,000 epochs and knowledge distillation following LeViT<ref type="bibr" target="#b18">[20]</ref>. Top-1 ? ReaL ? GPU CPU ONNX (M) (M) LeViT-128S<ref type="bibr" target="#b18">[20]</ref> 73.6 76.6 82.6 14457 82.3 80.9 305 7.8 EfficientViT-M4 74.3 77.1 83.6 15914 88.5 108.6 299 8.8</figDesc><table><row><cell></cell><cell>ImageNet (%)</cell><cell>Throughput (imgs/s) Flops Params</cell></row><row><cell>Model</cell><cell>Top-1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. <rs type="person">Prof. Yuan</rs> was partially supported by <rs type="funder">Hong Kong Research Grants Council (RGC) General Research Fund 11211221</rs>, and <rs type="funder">Innovation and Technology Commission-Innovation and Technology Fund</rs> <rs type="grantNumber">ITS/100/20</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BJQ7vj4">
					<idno type="grant-number">ITS/100/20</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Coremltools: Use coremltools to convert machine learning models from third-party libraries to the core ml format</title>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. arXiv</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<title level="m">Open neural network exchange</title>
		<imprint>
			<date type="published" when="2005">2019. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glit: Neural architecture search for global and local image transformer</title>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Auto-scaling vision transformers without training</title>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mobileformer: Bridging mobilenet and transformer</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2022. 1, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Franc ?ois Chollet</title>
		<imprint>
			<date type="published" when="2008">2017. 3, 4, 5, 8</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2021. 2, 8</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12239" to="12248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flashattention: Fast and memoryefficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14135</idno>
		<imprint>
			<date type="published" when="2008">2022. 1, 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The efficiency misnomer</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li Fei-Fei ; Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009. 1, 5, 6, 7, 8 [18</date>
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database. ICLR, 2021. 1, 2, 3, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Yuandong Tian, qiang liu, and Vikas Chandra. NASVit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training</title>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards memoryefficient neural networks via multi-level in situ generation</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ghostnets on heterogeneous devices via cheap operations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1050" to="1069" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus). arXiv</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008">2019. 2, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformer quality in linear time</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9099" to="9117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Lightvit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05557</idno>
		<title level="m">Towards light-weight convolutionfree vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterizing co-located workloads in alibaba cloud datacenters</title>
		<author>
			<persName><forename type="first">Congfeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Cerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zujie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangbin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cloud Comput</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An optimized dataflow for mitigating attention performance bottlenecks</title>
		<author>
			<persName><forename type="first">Sheng-Chun</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvinay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06419</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2020. 2, 8</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimal brain damage. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ds-net++: Dynamic weight slicing for efficient inference in cnns and vision transformers</title>
		<author>
			<persName><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientformer: Vision transformers at mobilenet speed</title>
		<author>
			<persName><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards lightweight transformer via group-wise transformation for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3386" to="3398" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syed</forename><surname>Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilevit: Lightweight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Separable selfattention for mobile vision transformers</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02680</idno>
		<imprint>
			<date type="published" when="2008">2022. 2, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? NeurIPS</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-03">2019. 3</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Edgevits: Competing light-weight cnns on mobile devices with vision transformers</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast vision transformers with hilo attention</title>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2008">2022. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Combiner: Full attention transformer with sparse computation cost</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22470" to="22482" />
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8583" to="8595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2018. 5, 6, 7, 8</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving the efficiency of transformers for resource-constrained devices</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Tabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Balasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shabbir</forename><surname>Marzban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mixed depthwise convolutional kernels</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2021. 1, 2, 3, 5, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anasosalu</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04040</idno>
		<title level="m">Oncel Tuzel, and Anurag Ranjan. An improved one millisecond mobile backbone</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2008">2017. 2, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Swirl: High-performance many-core cpu code generation for deep neural networks</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tharindu</forename><surname>Rusira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2008">2020. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICCV, 2021. 2, 8</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05">2019. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Tinyvit: Fast pretraining distillation for small vision transformers</title>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pale transformer: A general vision transformer backbone with pale-shaped attention</title>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Lite vision transformer with enhanced self-attention</title>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Nvit: Vision transformer compression and parameter redistribution</title>
		<author>
			<persName><forename type="first">Huanrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04869</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Minivit: Compressing vision transformers with weight multiplexing</title>
		<author>
			<persName><forename type="first">Jinnian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2018. 3, 4, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
