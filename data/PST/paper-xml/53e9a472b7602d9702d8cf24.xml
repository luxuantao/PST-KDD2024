<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POMDP-based control of workflows for crowdsourcing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-06-20">20 June 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
							<email>daipeng@cs.washington.edu</email>
						</author>
						<author>
							<persName><roleName>Mausam</roleName><forename type="first">Christopher</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<email>weld@cs.washington.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Amphitheater Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POMDP-based control of workflows for crowdsourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-06-20">20 June 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">A34F7DD05FDF14D812A3DBC53FABB847</idno>
					<idno type="DOI">10.1016/j.artint.2013.06.002</idno>
					<note type="submission">Received 20 December 2011 Received in revised form 8 June 2013 Accepted 9 June 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Partially-Observable Markov Decision Process POMDP Planning under uncertainty Crowdsourcing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowdsourcing, outsourcing of tasks to a crowd of unknown people ("workers") in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers ("requesters") for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ready availability of the Internet across the world has had far-reaching consequences. Not only has widespread online connectivity revolutionized communication, politics, entertainment, and other aspects of everyday life, it has enabled the ability to easily unite large groups of people around the world (a crowd) for a common purpose. This ability to crowdsource has in turn opened up radical new possibilities and resulted in novel successes, like Wikipedia 1 -a whole encyclopedia written by the crowd, and platforms that are rapidly impacting the world's laborforce by bringing them together in online marketplaces that provide work on-demand.</p><p>We believe that Crowdsourcing, "the act of taking tasks traditionally performed by an employee or contractor, and outsourcing them to a group (crowd) of people or community in the form of an open call," 2 has the potential to revolutionize information-processing services by coupling human workers with intelligent machines in productive workflows <ref type="bibr" target="#b20">[21]</ref>. Fig. <ref type="figure">1</ref>. A handwriting recognition task (almost) successfully solved on Mechanical Turk using an iterative-improvement workflow. Workers were shown the text written by a human and in a few iterations they deduced the message (with minimal errors highlighted). Figure adapted from <ref type="bibr" target="#b36">[37]</ref>.</p><p>While the word "crowdsourcing" was only coined in 2006, the area has grown rapidly in economic significance with the emergence of general-purpose platforms such as Amazon's Mechanical Turk, <ref type="foot" target="#foot_0">3</ref> task-specific sites for call centers, <ref type="foot" target="#foot_1">4</ref> programming jobs <ref type="foot" target="#foot_2">5</ref> and more <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Crowdsourced workers are motivated by a variety of incentives. Common incentives include entertainment, e.g., in the context of playing games with a purpose <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b9">10]</ref>, contribution to science, <ref type="foot" target="#foot_3">6</ref> a sense of community, and monetary rewards. While our work is applicable to several crowdsourcing scenarios, we focus on financially motivated micro-crowdsourcingcrowdsourcing of small jobs in exchange for monetary payments. Labor markets, like oDesk, handle medium and large-sized tasks requiring a diverse range of skills and micro-crowdsourcing has become very popular with requesters, who use a concert of small-sized jobs to handle a wide variety of higher-level tasks, such as audio transcription, language translation, calorie counting <ref type="bibr" target="#b41">[42]</ref>, and helping blind people navigate unknown surroundings <ref type="bibr" target="#b6">[7]</ref>. It has also been immensely popular with workers in both developed and developing countries <ref type="bibr" target="#b48">[49]</ref>.</p><p>On popular micro-crowdsourcing platforms, like Mechanical Turk, requesters often use workflows, a series of steps, to complete tasks. For instance, for a simple image classification task (e.g. "Is there a lion in this picture?"), a workflow as simple as asking several workers the same binary-choice question will suffice. For a more difficult task like the handwriting recognition task shown in Fig. <ref type="figure">1</ref>, a requester might create a more complicated workflow like iterative improvement, in which workers incrementally refine each others' solutions until no further improvement is necessary <ref type="bibr" target="#b36">[37]</ref>. A plethora of research shows that for simple binary classification workflows, micro-crowdsourcing can achieve high-quality results <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b59">60]</ref>. Similarly, iterative improvement, find-fix-verify <ref type="bibr" target="#b2">[3]</ref> and other workflows have been shown to yield excellent output even when individual workers err.</p><p>But the use of these workflows raises many important questions. For example, when designing a workflow to handle problems like the handwriting recognition task shown in Fig. <ref type="figure">1</ref>, we do not know answers to questions like: <ref type="bibr" target="#b0">(1)</ref> What is the optimal number of iterations for such a task? (2) How many ballots should be used for voting? (3) How do these answers change depending on workers' skill levels?</p><p>Our paper offers answers to these questions by constructing AI agents for workflow optimization and control. We study three distinct crowdsourcing scenarios. We start by considering the problem of dynamically controlling the aggregation of the simplest possible workflow: binary classification. Next, we extend our model to control the significantly more complex iterative improvement workflow. Finally, we show how our model can be used to dynamically switch between alternative workflows for a given task.</p><p>We use a shared toolbox of techniques on each of these crowdsourcing scenarios; there are two key components. First, we propose a probabilistic model for worker responses. This model relates worker ability, task difficulty and worker response quality by means of a Bayesian network. Second, we view the problem of workflow control as a problem of decisiontheoretic planning and execution, and cast it as a Partially-Observable Markov Decision Process (POMDP) <ref type="bibr" target="#b5">[6]</ref>. The Bayes net provides the model for the POMDP, and the POMDP policy controls the workflow to obtain a high-quality output in a cost-efficient manner.</p><p>We make several important contributions. First, we provide a principled solution to dynamically decide the number of votes for a task based on the exact history of workers who worked on a task instance, their answers and a notion of varying problem difficulty. We also provide a framework for answering more complex questions in larger workflows (e.g. the number of iterations in an iterative improvement workflow). The commonality of approaches in these diverse tasks suggests an underlying abstraction, which may be exploited for optimizing and controlling many different kinds of workflows. Our experiments consistently outperform best known baselines by large margins, e.g., obtaining 50% error reduction in the scenario of multiple workflows, or 30% cost savings for iterative improvement.</p><p>Moreover, our work reveals some surprising discoveries. First, for the iterative improvement scenario, our AI agent proposes a rather unexpected voting policy, which was not predicted by any human expert (see <ref type="bibr">Section 4)</ref>. Second, we demonstrate that judiciously using alternative workflows for a task is much better than using a single "best" workflow. While the base idea is not novel, the fact that we can do this dynamic switching between workflows automatically to obtain high-quality results is a surprising discovery.</p><p>The rest of the paper is structured as follows. Section 2 formally defines Markov Decision Processes and Partially-Observable Markov Decision Processes (POMDP). Then we review and propose planning algorithms for solving POMDPs. Section 3 details how we model and control the aggregation of a simple binary classification workflow using our first agent, TurKontrol 0 . Section 4 extends our model and agent to iterative improvement workflows to create our second agent, TurKontrol. We present some simulation-based investigation of the performance of TurKontrol, illustrate how to learn the model parameters, and finally demonstrate the usefulness of TurKontrol on Mechanical Turk, with real tasks. Section 5 details how we model the availability of multiple workflows and the design of our third agent, Agen-tHunt, which dynamically selects the next best workflow to use. We illustrate how to learn model parameters and then demonstrate the usefulness of AgentHunt in simulation and on Mechanical Turk. Finally, we present related work, propose future work, and make conclusions. We note software packages of our implementations are available for general use at http://cs.washington.edu/node/7714.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Markov decision processes</head><p>AI researchers typically use Markov Decision Processes (MDPs) to formulate fully-observable decision making under uncertainty problems <ref type="bibr" target="#b39">[40]</ref>. Definition 1. An MDP is a four-tuple S, A, T , R , where</p><p>• S is a finite set of discrete states.</p><p>• A is a finite set of all actions.</p><p>• T : S × A × S → [0, 1] is the transition function describing the probability that taking an action in a given state will result in another state.</p><p>• R : S × A → R is the reward for taking an action in a state.</p><p>An agent executes its actions in discrete time steps starting from some initial state. At each step, the system is at one distinct state s ∈ S. The agent can execute any action a from a set of actions A, and receives a reward R(s, a). The action takes the system to a new state s stochastically. The transition process exhibits the Markov property, i.e., the new state s is independent of all previous states given the current state s. The transition probability is defined by T a (s |s). The model assumes full observability, i.e., after executing an action and transitioning stochastically to a next state as governed by T , the agent has full knowledge of the state.</p><p>Any solution to an MDP problem is in the form of a policy.</p><p>Definition 2. A policy, π : S → A, of an MDP is a mapping from the state space to the action space.</p><p>A policy is static if the action taken at each state is the same at every time step. π(s) indicates which action to execute when the system is at state s. To solve an MDP we need to find an optimal policy (π * : S → A), a probabilistic execution plan that achieves the maximum expected utility, or sum of rewards. We evaluate any policy π by its value function, the set of values that satisfy the following equation:</p><formula xml:id="formula_0">V π (s) = R s, π(s) + β s ∈S T π (s) s s V π s . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>β ∈ (0, 1] is the discount factor, which controls the value of future rewards. Any optimal policy's value function must satisfy the following system of Bellman equations:</p><formula xml:id="formula_2">V * (s) = max a∈ A R(s, a) + β s ∈S T a s s V * s .</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>The corresponding optimal policy can be extracted from the value function:</p><formula xml:id="formula_4">π * (s) = argmax a∈ A R(s, a) + β s ∈S</formula><p>T a s s V * s .</p><p>(</p><p>Given an implicit optimal policy π * in the form of its optimal value function V * (•), we measure the superiority of an action by a Q-function: S × A → R. </p><formula xml:id="formula_6">Q * (s, a) = R(s, a) + β s ∈S T a s s V * s . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>Therefore, the optimal value function can be expressed by:</p><formula xml:id="formula_8">V * (s) = max a∈ A Q * (s, a).</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Solving an MDP</head><p>Many optimal MDP algorithms are based on dynamic programming. A simple, yet powerful such algorithm, value iteration <ref type="bibr" target="#b1">[2]</ref>, is also the basis of many heuristic search and approximate algorithms. It first initializes the value function arbitrarily. Then, the values are updated iteratively using an operator called the Bellman backup to create successively better approximations for the value of each state per iteration. The Bellman residual of a state is the absolute difference of a state value before and after a Bellman backup. Value iteration stops when the value function converges. In implementation, it is typically signaled by when the Bellman error, the largest Bellman residual of all states, becomes less than some pre-defined threshold.</p><p>A simple way to approximate solutions to large MDPs is to use Monte Carlo (MC) simulation algorithms. Such algorithms repeatedly perform simulation trials originating from the initial state. During each simulation, actions are chosen based on some heuristic function h, and the resulting states are chosen stochastically based on the transition probabilities of the action. Different heuristics can be used, depending on the desired tradeoff between exploration and exploitation. For example, a heuristic that purely exploits knowledge of the value function would always pick a greedy action that maximizes the value function. On the other hand, a heuristic that solely explores would always choose a random action. One simulation trial terminates when a terminal state <ref type="foot" target="#foot_4">7</ref> is encountered. At termination, the Q -values of all visited state-action pairs are updated in reverse order.</p><p>Upper Confidence Bounds Applied on Trees (UCT) <ref type="bibr" target="#b28">[29]</ref> is an MC algorithm, whose power has been demonstrated by its application to several challenging problems like Go <ref type="bibr" target="#b17">[18]</ref> and Real-time Strategy Games <ref type="bibr" target="#b0">[1]</ref>. UCT uses a heuristic that considers both actions that have already been proven valuable and under-explored branches, in case better policies can be found. It remembers the total number of times a state s has been visited, n s , and the number of times action a is picked when s is visited, n (s,a) . Its heuristic function, h UCT (s, a), is defined as follows:</p><formula xml:id="formula_9">h UCT (s, a) = Q (s, a) + κ 2 ln n s n (s,a)</formula><p>. <ref type="bibr" target="#b5">(6)</ref> The second term increases the value of actions that are visited less frequently. The exploration parameter κ is used to balance between exploration and exploitation. Theoretical error bounds given a sufficient number of trials have been proven <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Partially-Observable Markov Decision Processes</head><p>Partially-Observable MDPs (POMDPs) <ref type="bibr" target="#b24">[25]</ref> provide a more flexible framework with which to model decision-making under uncertainty by relaxing the assumption that an agent has complete knowledge of the world and can only make noisy observations about its current state. This generality can model many single-agent, real-world problems, since perfect information about the world is rarely available to an agent. Definition 4. A POMDP is a six-tuple S, A, O, T , P, R , where</p><p>• S is a finite set of discrete states.</p><p>• A is a finite set of all actions.</p><p>• O is a finite set of observations. • T : S × A × S → [0, 1] is the transition function describing the probability that taking an action in a given state will result in another state.</p><p>• P : S × O → [0, 1] is the observation function describing the probability that taking an action in a given state will result in an observation.</p><p>• R : S × A → R is the reward for taking an action in a state.</p><p>We see that a POMDP extends an MDP by adding a finite set of observations O and a corresponding observation model P. Since the agent is unable to directly observe the world's current state, it maintains a probability distribution over states, called a belief state b, reflecting its estimate of their likelihood:</p><formula xml:id="formula_10">b(s) ∈ [0, 1], s∈S b(s) = 1,<label>(7)</label></formula><p>where b(s) is the probability that the current state is s, inferred from the previous belief state, the most recent action, and the resulting observation.</p><p>A POMDP can be cast as an MDP by letting the state space of the MDP be the space of all possible belief states. Since the transition between two belief states is completely determined by the action and resulting observation, the resulting model is a fully-observable MDP -but over a vastly larger space. Even when a POMDP is defined over only two world states, s 1 and s 2 , the space of belief states is infinite, since a belief state can be an arbitrary combination of b(s 1 ) = p and b(s 2 ) = 1p where p can be any real number in [0, 1].</p><p>Solving a POMDP is a hard problem. For the simplest case where the set of A, S and O are finite, Sondik <ref type="bibr" target="#b58">[59]</ref> proves that the optimal value function for a finite-horizon problem is piecewise linear and convex (PWLC). Therefore, value iterationtype algorithms converge on these problems. However, the number of reachable belief states grows exponentially in the number of observations |O|. The complexity of one update iteration is exponential in the total number of observations <ref type="bibr" target="#b24">[25]</ref>.</p><p>For an infinite-horizon POMDP one needs to perform an infinite number of iterations in the worst case, as the total number of reachable belief states can be infinite. Therefore, the problem is undecidable <ref type="bibr" target="#b37">[38]</ref> in the worst case. Porta et al. <ref type="bibr" target="#b44">[45]</ref> later proved that finite-horizon continuous state POMDPs are PWLC.</p><p>Researchers seek various approximation methods to solve POMDPs efficiently. Point-based value iteration <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref> is a very successful approach. The basic idea is to approximate the value function of the belief space, B, by only estimating the values of a fixed, sampled subset, B ⊆ B. For all the other belief states, the values are approximately estimated according to the values of the sampled space, e.g., inferred from integration <ref type="bibr" target="#b44">[45]</ref> or a mixture of Gaussians representation <ref type="bibr" target="#b7">[8]</ref>. For a tractable, finite-horizon problem, the algorithm iteratively computes an approximately optimal t-horizon value function.</p><p>The PWLC properties of a discrete POMDP guarantee that the error of the value function computed by the point-based algorithms is bounded <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Planning algorithms</head><p>We now present three algorithms that we use in this paper for solving POMDPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Limited lookahead</head><p>Our first decision-making algorithm is an l-step lookahead. The goal is to evaluate all sequences of up to l decisions and find the best sequence. To determine the value of a given sequence of l decisions, we calculate the expected utility from stopping at the lth decision based on the agent's current belief. Based on these expected utility estimations, the agent picks the sequence with the best utility, executes the first action of that sequence, and then repeats the process until a terminal state is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">A discretized POMDP algorithm</head><p>We also try a discretization-based POMDP algorithm, which we call ADBS, short for approximation and discretization of belief space. The method approximates the belief distribution by the values of its mean and standard deviation. Similar techniques have been used in the robot navigation domain <ref type="bibr" target="#b49">[50]</ref>. ADBS discretizes the range of each variable into small, equal-sized intervals. With discretization and known bounds (we show later that we can bound these variables for our application), the space of belief states is finite. ADBS performs a reachability search from the initial state to build an MDP model of the approximated belief space. It then optimally solves the discretized MDP by value iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">A UCT variant</head><p>For our third algorithm, we use a variant of UCT applied to a space whose states are a simplified history of actions and observations, which we call condensed histories and define in Section 4.4. Although using condensed histories has the unfortunate property that different histories may represent the same belief state, this approximation technique excels in several ways. It saves computation, since we do not need to maintain posterior beliefs. Maintaining beliefs is tricky for Fig. <ref type="figure">2</ref>. Worker accuracies given the intrinsic difficulty under various error parameters γ . With a fixed difficulty, the greater a worker's γ value, the more likely that worker makes a mistake. our problems, since the state space is continuous, and a belief over continuous state space is non-trivial to represent and update. For the same reason, implementing history-based POMDPs is much easier. Finally, it makes execution faster too, since one can directly map the history to the action. Monte Carlo simulation-based POMDP algorithms have been proved to be effective on very large problems <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Decision-theoretic optimization of a binary classification workflow</head><p>We now begin the study of decision-theoretic control of crowdsourcing. We first consider one of the simplest of workflows, the binary classification, or the binary ballot job. These workflows present workers with a question and ask them to pick the best answer from two choices. For instance, a workflow might show workers a picture and ask them if there is a human face in the picture. Then some method of aggregation is used, like a majority vote, to account for worker variability. Such workflows might be used to collect training data for a computer vision algorithm.</p><p>The agent's control problem is defined as follows. As input the agent is given the task, and the agent is asked to return the correct answer. In pursuit of this goal, the agent is allowed to create jobs on a crowdsourcing platform. We model the agent's decision problem as a POMDP.</p><p>To do this, we must first define a generative model for worker responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probabilistic model for a binary classification response</head><p>We assume workers are diligent, so they answer all ballots to the best of their abilities. In other words, we assume workers are not adversarial. Additionally, we also assume that workers do not collaborate.</p><p>We define a worker's accuracy to be</p><formula xml:id="formula_11">a(d, γ ) = 1 2 1 + (1 -d) γ (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where d ∈ [0, 1] is the intrinsic difficulty of the task, which we will motivate shortly, and γ x ∈ [0, ∞] is the error parameter of the worker. As a worker's error parameter and/or the workflow's difficulty increases, a approaches 1/2, suggesting that worker is randomly guessing. On the other hand, as the parameters decrease, a approaches 1, when the worker always produces the correct answer. As γ decreases, the accuracy curve becomes more concave, and thus the expected accuracy increases for a fixed d. Fig. <ref type="figure">2</ref> is an illustration of accuracy.</p><p>The answer b w that worker w with error parameter γ w provides is governed by the following equations: <ref type="bibr" target="#b9">(10)</ref> Fig. <ref type="figure" target="#fig_2">7</ref> illustrates the plate notation for our generative model, which encodes a Bayes Net for responses made by W workers on T tasks. The correct answer, v, the difficulty parameter, d, and the error parameter, γ , influence the final answer, b, that a worker provides, which is the only observed variable.</p><formula xml:id="formula_13">P (b w = v|d) = a(d, γ w ), (9) P (b w = v|d) = 1 -a(d, γ w ),</formula><p>We note that given our assumption that workers do not collaborate, one might believe that the workers' responses b w are independent of each other given the true answer v. However, they are not because of the following subtlety. Even though the different workers are not collaborating, a mistake by one worker changes the error probability of others, because a mistake gives evidence that the question may be intrinsically hard and hence, difficult for others to get it right as well. Thus, as shown in the graphical model, we make the assumption that the worker responses are independent of each other only after we are additionally given the difficulty d.</p><p>With this generative model, we can define our POMDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The POMDP</head><p>Definition 5. The POMDP for our simple binary classification workflow is a six-tuple S, A, R, T , O, P , where</p><formula xml:id="formula_14">• S = {(d, v) | d ∈ [0, 1], v ∈ {0, 1}}<label>d</label></formula><p>is the difficulty of the task and v is the true answer.</p><p>• A = {create another job, submit true, submit false}.</p><p>• R :</p><formula xml:id="formula_15">S × A → R is specified below. • T : S × A × S → [0, 1] = ((d, v), a, (d, v)) → 1.</formula><p>All other probabilities are 0.</p><p>• O = {true, false} is a Boolean response by a worker.</p><p>• P : S × O → [0, 1] is defined by our generative model.</p><p>The reward function maintains the value of submitting a correct answer and the penalty for submitting an incorrect answer. Additionally, it maintains a cost that TurKontrol 0 incurs when it creates a job. We can modify the reward function to match our desired budgets and accuracies.</p><p>We note that this POMDP is a purely sensing-POMDP. None of the actions changes the state of the POMDP. In many crowdsourcing platforms, such as Mechanical Turk, we cannot preselect the workers for a job. However, in order to specify our observation probabilities, which are defined by our generative model, we need access to future workers' parameters. To simplify the computation, our POMDP calculates observation probabilities using an average γ . In other words, it assumes that every future worker is an average worker. Formally, every future worker has an error parameter equal to γ = 1 W w γ w where W is the number of known workers. However, our agent can keep around knowledge about worker accuracy in order to use accurate estimates of γ when updating its belief state. In particular, if someone answers a question correctly (according to the agent's belief), then she is a good worker (and her γ w should decrease) and if someone made an error in a question her γ w should increase. Moreover the increase/decrease amounts should depend on the difficulty of the question. The following simple update strategy may work:</p><p>1. If a worker answers a question of difficulty d correctly then γ w ← γ xdδ. 2. If a worker makes an error when answering a question then γ w ← γ w + (1d)δ.</p><p>We use δ to represent the learning rate, which we can slowly reduce over time so that the accuracy of a worker approaches an asymptote.</p><p>More sophisticately, one may update the parameters using Bayesian updates, in the same flavor as discussed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Simulated experiments</head><p>We evaluate our model in simulation against an agent that uses a majority-vote strategy to assess the viability of using a POMDP to control such a simple workflow for crowdsourcing.</p><p>The POMDP must manage a belief state over the cross product of the Boolean answer and one continuous variablethe task difficulty. Since solving a POMDP with a continuous state space is challenging, we discretize difficulty into eleven possible values, leading to a (world) state space of size 2 × 11 = 22. To solve POMDPs, we run the ZMDP package<ref type="foot" target="#foot_5">8</ref> for 300 s using the default Focused Real-Time Dynamic Programming search strategy <ref type="bibr" target="#b56">[57]</ref>.</p><p>On each run, the simulator draws difficulty uniformly. We fix the reward of returning the correct answer to 0, and vary the reward (penalty) of returning an incorrect answer between the following values: -10, -100, and -1000. We set the cost of creating a job for a (simulated) worker to -1. We use a discount factor of 0.9999 in the POMDP so that the POMDP solver converges quickly. We ensure that majority vote uses a number of ballots that is at least as many and within two of the average number of ballots that TurKontrol 0 uses. Difficulties are drawn uniformly randomly and workers' γ are drawn from Normal (1.0, 0.2). For each setting of reward values we run 1000 simulations and report mean net utilities (Fig. <ref type="figure">3</ref>) and the percent reduction in error that TurKontrol 0 achieves when comparing its accuracy to that of majority voting (Fig. <ref type="figure">4</ref>).</p><p>We see that our model works well when the average difficulty of the task is not too difficult and not too easy and the variance is wide. To gain further insight, we investigate the effectiveness of our model at varying settings of task difficulty. Fig. <ref type="figure">3</ref>. In simulation, as the importance of answer correctness increases, TurKontrol 0 outperforms an agent using a majority-vote strategy by an everincreasing margin. Fig. <ref type="figure">4</ref>. In simulation, as the importance of answer correctness increases, TurKontrol 0 increases percent reduction in error that it achieves when comparing its accuracy to that of majority voting.</p><p>We run 1000 simulations at nine equally spaced difficulty settings, starting from d = 0.1. Workers' γ are drawn from Normal (1.0, 0.2), and we set the reward for an incorrect answer to be -1000. However, we assume that TurKontrol 0 has no knowledge of worker γ , always receives observations from new workers, and only has access to the average γ .</p><p>To further place our agent in the most unfavorable testing conditions as possible, we compare TurKontrol 0 against an agent (MV) that uses the following adaptive majority-vote strategy. For every difficulty setting, we determine, on average, how many ballots TurKontrol 0 uses. Then we set the majority-vote strategy to use at least as many ballots, and at most 1 more. Fig. <ref type="figure" target="#fig_0">5</ref> shows the percent reduction in error that TurKontrol 0 achieves when comparing its accuracy to that of MV. Since TurKontrol 0 always achieves equal or better accuracy than MV, and MV always uses equal or more ballots, TurKontrol 0 also always achieves equal or better net utility than MV. Interestingly, the figure shows an illuminating trend. TurKontrol 0 only achieves slightly better results than MV at the extremes of the difficulty spectrum, but shows larger gains in the middle difficulty settings. Such a result intuitively makes sense because of the following reasons. When the problems are easy, there is no need to do anything dynamic. Similarly, when the problems are extremely difficult, there is nothing TurKontrol 0 can do without more knowledge about the workers. However, when the problem difficulty is in the middle range, sometimes the workers will agree quickly and sometimes they will not, and the ability of our agent to make dynamic decisions contributes to both higher accuracy and higher net utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning the model</head><p>Extensive simulation results in the previous section show the usefulness of decision-theoretic techniques for a simple binary classification workflow. This section addresses learning the model from real Mechanical Turk data <ref type="bibr" target="#b12">[13]</ref>. We will use the following task. Given an image and a pair of descriptions, we ask workers to select the better description. This learning problem is challenging due to the large number of parameters and the sparse, noisy training data.</p><p>We begin by defining the supervised learning problem. Fig. <ref type="figure" target="#fig_1">6</ref> presents our generative model of ballot jobs; we can observe the ballots, the true answer, and the difficulty of the task. We seek to learn the error parameters γ where γ w is the parameter for worker w. To generate training data for our task we select T images and corresponding pairs of descriptions  and post W copies of a ballot job for each task. We use b i,w to denote worker w's ballot on the ith question. Let v i denote whether the first artifact of the ith pair is better than the second, and d i denote the difficulty of answering such a question. The ballot answer of each worker depends on her error parameter, as well as the difficulty of the job, d, and its real truth value, v.</p><p>For our learning problem, we collect values of v and d for the T ballot questions from the consensus of three human experts and treat these values as observed. In our experiments we assume γ are drawn from a uniform prior, though our model can incorporate more informed priors. 9 We use the standard maximum a posteriori approach to estimate the γ parameters:</p><formula xml:id="formula_16">P (γ |b, w, d) ∝ P (γ )P (b|γ , w, d).<label>(11)</label></formula><p>Under the uniform prior and conditional independence of different workers given difficulty and truth value of the task, Eq. ( <ref type="formula" target="#formula_16">11</ref>) can be simplified to:</p><formula xml:id="formula_17">P (γ |b, w, d) ∝ P (b|γ , w, d) = T i=1 W w=1 P (b i,w |γ w , d i , v i ). (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>Taking the log, the maximum a posteriori problem is transformed to the following optimization problem:</p><formula xml:id="formula_19">Constants: d 1 , . . . , d T , v 1 , . . . , v T , b 11 , . . . , b T ,W Variables: γ 1 , . . . , γ W Maximize: T i=1 W w=1 log P (b i,w |γ w , d i , v i )</formula><p>Subject to: ∅ 9 We also tried priors that penalized extreme values but that did not help in our experiments. We also try an unsupervised learning algorithm. The plate notation is shown in Fig. <ref type="figure" target="#fig_2">7</ref>. We don't assume knowledge of correct answers or the difficulties. Instead, we adopt an EM-style algorithm based on Whitehill et al.'s learning mechanism <ref type="bibr" target="#b67">[68]</ref>. We initialize the difficulty values and error parameters with the corresponding average values learned from the supervised learning algorithm. We find applying prior distributions on the parameters does not help, so we do not use a prior. During the expectation step, we compute the probability of the true answers given the workers' answers, and the current values of the difficulty and error parameters. During the maximization step, we update the difficulty and error parameters based on the likelihood function (Eq. ( <ref type="formula" target="#formula_17">12</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Experiments on ballot model</head><p>We evaluate the effectiveness of our learning procedures against each other and against a majority-vote baseline on the image description task. We select 20 pairs of descriptions (T = 20) and collect sets of ballots from 50 workers. 5 spammers were detected manually and dropped from learning (W = 45). We spend $4.50 on the supervised learning process. We solve the optimization problem using the NLopt package <ref type="foot" target="#foot_6">10</ref> and implement the unsupervised learning algorithm based on Whitehill's framework <ref type="bibr" target="#b67">[68]</ref>.</p><p>We run a five-fold cross-validation experiment. We take 4/5th of the image pairs and learn error parameters over them, and then use these parameters to estimate the true ballot answer for the images in the fifth fold. Our supervised learning algorithm obtains an accuracy of 80.01%, our unsupervised learning algorithm obtains an accuracy of 80.0%, and a majorityvoting baseline obtains an accuracy of 80%. We investigate these similar accuracies and see that the four ballots frequently missed by the models are those in which the mass opinion differs from our expert labels.</p><p>We also compare the confidence, or the degree of belief in the correctness of an answer, for two approaches. For the majority-vote baseline, we calculate confidence by dividing the number of votes for the inferred correct answer by the total number of votes. For our supervised learning approach, we use the average posterior probability of the inferred answer. The average confidence values derived from the supervised learning approach is much higher than the majority vote (82.2% against 63.6%). Thus, even though the two approaches achieve the same accuracy on all 45 votes, our ballot model has superior belief in its answer.</p><p>Although the confidence values are different, the ballot models (learned from both supervised and unsupervised learning) seem to offer no distinct advantage over the simple majority-voting baseline given a large number of votes. In hindsight, this result is not surprising, since we are using a large number of workers. In other work, researchers have shown that a simple average of a large number of non-experts often beats even the expert opinion <ref type="bibr" target="#b57">[58]</ref>.</p><p>However, one will rarely have the resources to use 45 voters per question, so we consider the effect of varying the number of available voters. For each image pair, we randomly sample, without replacement, 50 000 sets of 3 to 11 ballots and compute the average accuracies of the three approaches. Fig. <ref type="figure" target="#fig_3">8</ref> shows that using our model (learned from both the supervised and the unsupervised learning algorithms) consistently outperforms the majority-vote baseline. Furthermore, applying the supervised learning algorithm consistently achieves higher accuracy than applying the unsupervised learning algorithm, which shows the usefulness of using expert labeling. The unsupervised learning algorithm gradually catches up as the number of votes increases, which distinguishes itself from majority voting. In contrast, the model learned from supervised learning always outperforms majority voting by a wide margin. With just 11 votes, it is able to achieve an accuracy of 79.3%, which is very close to the accuracy achieved using all 45 votes. Also, the supervised ballot model with only 5 votes achieves an accuracy similar to the one achieved by a majority vote with 11 votes. Our ballot model significantly reduces the number of votes and thus the amount of money needed for a given desired accuracy. Since the unsupervised learning algorithm does not require any labeled data, it can be useful when data labeling is expensive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Decision-theoretic optimization of an iterative improvement workflow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation example</head><p>We now move to the decision-theoretic control of the iterative improvement workflow <ref type="bibr" target="#b11">[12]</ref>, introduced by Little et al. <ref type="bibr" target="#b36">[37]</ref>. This workflow is depicted in Fig. <ref type="figure" target="#fig_4">9</ref>. While the ideas in our paper are applicable to many complex workflows, we choose to apply them to this one because it is representative of a number of flows in commercial use today; at the same time, it is moderately complex making it ideal for a first investigation.</p><p>Iterative text improvement works as follows. There is an initial job, which presents the worker with an image and requests an English description of the image's contents. A subsequent iterative process consists of an improvement job and multiple ballot jobs. In the improvement job, a (different) worker is shown this same image as well as the current description and is requested to generate an improved English description (see Fig. <ref type="figure">22</ref>). Next, n 1 ballot jobs are posted ("Which text best describes the picture?"). See Figs. 23, 24, 25, and 26 for our user interface design. Based on a majority opinion, the best description is selected and the loop continues. Little et al. have shown that this iterative process generates better descriptions for a fixed amount than allocating the total reward to a single author.</p><p>Little et al. support an open-source toolkit, TurKit, that provides a high-level mechanism for defining moderately complex, iterative workflows with voting-controlled conditionals. However, TurKit does not have built-in methods for monitoring the accuracy of workers; nor does it automatically determine the ideal number of voters or estimate the appropriate number of iterations before returns diminish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Problem overview</head><p>The agent's control problem for a workflow like iterative text improvement is defined as follows. As input the agent is given an initial artifact (or a job description for requesting one), and the agent is asked to return an artifact which maximizes some payoff based on the quality of the submission. In our initial model, we assume that requesters will express their utility as a function U from quality to dollars. To accomplish the task, the agent is allowed to post an improvement job or a ballot job.</p><p>To fully specify the problem, we must define what "quality" means. Intuitively, something is high-quality if it is better than most things of the same type. For engineered artifacts (including English descriptions) one may say that an artifact is high quality if it is difficult to improve. Therefore, we define the quality of an artifact as follows. Let the quality be q ∈ [0, 1]. An artifact with quality q means an average dedicated worker has probability 1q of improving the artifact.</p><p>The agent never exactly knows the quality of an artifact. At best, it can estimate q based on domain dynamics and observations (like ballot results). Therefore, we can model the agent's control problem as a POMDP. Since quality is a real number, the state space of the POMDP is continuous <ref type="bibr" target="#b7">[8]</ref>.  </p><formula xml:id="formula_20">• S = {(q, q )|q, q ∈ [0, 1]}.</formula><p>• A = {create a ballot job, create an improvement job, submit best artifact}.</p><p>• R((q, q ), submit) is the reward received from submitting an artifact with quality max{q, q }. • R((q, q ), create) is the cost of creating a job.</p><p>• T : defined below.</p><p>• O = {true, false} is a Boolean answer received for a ballot question.</p><p>• P : defined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">The transition function</head><p>We first note that the underlying state of the POMDP only changes when the agent requests an improvement. The qualities of the artifacts do not change when the agent requests votes. Votes only change the belief state of the agent.</p><p>Suppose we currently have artifacts α, α , with unknown qualities q and q . Therefore, the current state is (q, q ). Suppose without loss of generality that the agent posts an improvement job for α, and a worker x submits another artifact α , whose quality is denoted by q . Since α is a suggested improvement of α, q depends on the initial quality q. Moreover, a higher accuracy worker x may improve it much more, so q also depends on x. In order to determine the next state, we must compute P (q |q, x), the conditional distribution of q when worker x improves an artifact of quality q. We describe how to learn this conditional distribution later. The current state (q, q ) transitions to the new state (q, q ) with probability P (q |q, x). However, since the agent does not know a priori the worker who will submit the improvement, it must assume, for the purpose of lookahead, that the worker has an average ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">The observation function</head><p>Our observation function is only relevant when the agent requests ballot jobs. If the current (hidden) state is (q, q ) with corresponding artifacts α and α , the agent can either observe a vote for α or α . The observation function is defined by P (o|q, q ). Since a ballot job is simply an instantiation of a binary classification task, we can use our already defined generative model by defining the difficulty of the task.</p><p>To make this definition, we observe that the difficulty of the ballot job depends on the relative closeness of the two qualities of the underlying state. If the artifacts are of similar quality, it is difficult to judge whether one is better or not. Thus, we define the relationship between the difficulty and qualities as</p><formula xml:id="formula_21">d q, q = 1 -q -q M , (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>where M is the difficulty constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Discussion</head><p>Fig. <ref type="figure" target="#fig_5">10</ref> is a high-level flow describing our planner's decisions. At each step we track our belief in qualities (q and q ) of the previous (α) and the current artifact (α ) respectively. Each improvement attempt or vote gives us new information, which is reflected in the quality posteriors. These distributions also depend on the accuracy of workers, which we also incrementally estimate based on their previous work.</p><p>Our POMDP lets us answer questions like (1) when to terminate the voting phase (thus switching attention to artifact improvement), (2) which of the two artifacts is the best basis for subsequent improvements, and (3) when to stop the whole iterative process and submit the result to the requester.</p><p>We note that with this general POMDP model, the belief in the quality of the previous artifact (posterior of α) can change based on ballots comparing it with the new artifact. Why should this be the case? We make the following subtle, but important point. If the improvement worker (who has a good accuracy) was unable to create a much better α in the improvement phase, that must be because α already has a high quality and is not easily improvable. Under such evidence we should increase our quality estimation of α. Similarly, if all voting workers unanimously thought that α is much better than α, then α likely incorporates significant improvements over α and the qualities should reflect such knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Updating difficulty and worker accuracy</head><p>To update its knowledge about the quality of each worker after each voting phase, the agent first estimates the expected difficulty of the voting using its estimates of quality as follows:</p><formula xml:id="formula_23">d * = 1 0 1 0</formula><p>d q, q P (q)P q dq dq = 1 0 1 0 1qq M P (q)P q dq dq .</p><p>(</p><formula xml:id="formula_24">)<label>14</label></formula><p>Then, it uses d * and what it believes to be the correct answer to update worker quality records as we discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">Implementation</head><p>For efficient storage and computation TurKontrol 0 could employ the piecewise constant/piecewise linear value function representations or use particle filters. Although approximate, both these techniques are very popular in the literature for efficiently maintaining continuous distributions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17]</ref> and can provide arbitrarily close approximations. Because some of our equations require double integrals and can be time-consuming (e.g., Eq. ( <ref type="formula" target="#formula_24">14</ref>)) these compact representations will help in overall efficiency of the implementation.</p><p>Our piecewise constant function over a distribution density function, P (q), divides the domain into fixed number of intervals and uses the average value 1 u-l u l f (q) dq as the value for each interval <ref type="bibr">[l, u]</ref>. In implementation, a continuous function is represented by an array of values, where each value equals the constant function value of its designated interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Simulations</head><p>This section aims to empirically answer the following questions about the POMDP-solving algorithms we introduced in Section 2.4: (1) For a simple lookahead approach, how deep should an agent's lookahead be to best tradeoff between computation time and utility? (2) How well does the ABDS algorithm perform and scale? (3) What is the error of the UCT algorithm using a history approximation? (4) How well does the UCT algorithm perform? (5) What is the best planner for TurKontrol 0 ? (6) Does TurKontrol 0 make better decisions compared to a non-adaptive workflow? <ref type="bibr" target="#b6">(7)</ref> Can our planner outperform an agent following a well-informed, fixed policy? (8) How well does our planner handle workers that are less effective at completing ballot jobs than improvement jobs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Experimental setup</head><p>We define reward for submitting an artifact of quality q to be the convex function R(q) = 1000 e q -1 e-1 with R(0) = 0 and R (1) = 1000. We assume the quality of the initial artifact follows a Beta distribution, Beta <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b8">9)</ref>, which implies that the mean quality of the first artifact is 0.1.</p><p>We model results of an improvement task in a manner akin to ballot tasks. We know that the higher the quality of an artifact, the less likely that artifact can be improved. Suppose the quality of the current artifact is q, we define the</p><formula xml:id="formula_25">conditional distribution P (q |q, x) to be Beta(10μ Q |q,x , 10(1 -μ Q |q,x )), with mean μ Q |q,x , where μ Q |q,x = q + 0.5 (1 -q) × a x (q) -0.5 + q × a x (q) -1 . (15)</formula><p>Thus, the resulting distribution of qualities is influenced by the worker's accuracy and the hardness of an improvement, indicated by the quality of the original artifact, q.</p><p>We fix the ratio of the costs of improvements and ballots to be c imp /c b = 3, because ballots take less time. We set the difficulty constant M = 0.5. In each of the simulation runs, we build a pool of 1000 workers, whose error coefficients, γ w , follow a bell shaped distribution with a fixed mean γ . We also distinguish the accuracies of performing an improvement and answering a ballot by using one half of γ w when worker w is answering a ballot, since answering a ballot is an easier task, and therefore a worker should have higher accuracy. 11   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">The l-step lookahead algorithm</head><p>We first try to find the optimal depth for the l-step lookahead algorithm we presented earlier. When l = 2, such an algorithm considers the following set of action sequences { stop , ballot, stop , improvement, stop , ballot, ballot , ballot, improvement , improvement, ballot , improvement, improvement }.</p><p>We run 10 000 simulation trials with average error coefficient γ = 1 on three pairs of improvement and ballot costs - <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref>, and (0.3, 0.1) -trying to find the best lookahead depth l for the l-step lookahead algorithm. Fig. <ref type="figure" target="#fig_7">11</ref> shows 11 For simplicity reasons, in our simulations only, we assume a worker has only one γ w . We relax this assumption when we run real experiments on Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Average utility of TurKontrol(2) and performance of the ADBS algorithms with different resolution (discrete interval lengths) on three sets of (improvement, ballot) costs: <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref>, and (0.  (30, 10), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref>, and (0.3, 0.1). Longer lookahead produces better results, but 2-step lookahead is good enough when costs are relatively high: <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref>.</p><p>the average utility, achieved by different lookahead depths, denoted by TurKontrol(l). Note that there is always a performance gap between TurKontrol(1) and TurKontrol(2), but the curves of TurKontrol(3) and TurKontrol(4) generally overlap.</p><p>We also observe that when the costs are high, such that the process usually finishes in a few iterations, the performance difference between TurKontrol(2) and deeper step lookaheads is negligible. Since each additional step of lookahead increases the computational overhead by an order of magnitude, we limit TurKontrol's lookahead to depth 2 in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">The ADBS algorithm</head><p>Next, we try our ADBS algorithm. The states of the MDP are four-tuples μ, σ , μ , σ , an average and a standard deviation for each hidden quality. Since quality is a real number in [0, 1], we have that μ ∈ [0, 1] and σ ∈ [0, 1], and we can finitely discretize these intervals.</p><p>We try different resolutions of discretization, i.e. various constant interval lengths, and run with average error coefficient γ = 1 on three pairs of improvement and ballot costs - <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref>, and (0.3, 0.1). Table <ref type="table">1</ref> lists the average utility of TurKontrol(2) on 10 000 simulations (results incorporated from Fig. <ref type="figure" target="#fig_7">11</ref>). For the ADBS algorithm, we report the size of the reachable belief states under each resolution, |S|, and the value of the initial state, V * (s 0 ), calculated by value iteration.</p><p>We find TurKontrol(2) outperforms ADBS in all settings. Moreover, the smaller the costs, the bigger the performance gap. The poor performance of ADBS is probably due to the errors generated during approximation and discretization. Also notice that with more refined discretization, the reachable state space grows very quickly (at approximately a rate of an order of magnitude per doubly refined interval), yet the optimal value of the initial state increases very slowly. This indicates that the error most likely comes from the approximation as opposed to the discretization. We also try an interval length of 0.0025, but the algorithm terminates prematurely when the reachability search runs out of memory -indicating the limited scalability of the ADBS algorithm. This experiment shows that a simple POMDP method does not work as well as the 2-step lookahead algorithm, yet we cannot yet draw the conclusion that POMDP algorithms do not work on this planning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">The UCT algorithm</head><p>Finally, we try the UCT variant. We first evaluate the amount of suboptimality introduced by the our condensed history approximation. An example of a condensed history is: (Step 1) An initial artifact α 1 is provided. (Step 2) The agent creates an improvement HIT and obtains α 2 . (Step 3) The agent creates a ballot job, and receives a vote that α 2 is better than α 1 .</p><p>(Step 4) The agent submits α 2 .</p><p>Since the worker identities are ignored, the belief state space size is drastically reduced.</p><p>In a new TurKontrol(2) version, we do everything the same as TurKontrol(2) except we regard every worker as an anonymous worker of the same, average accuracy, γ . We denote this new variant TurKontrol(2, anonymous). Note that in this case, the policy is non-adaptive, as there is no uncertainty in workers' accuracies. We perform 10 000 simulations on the following two configurations<ref type="foot" target="#foot_7">12</ref> :</p><p>1. γ = 2 and ballot accuracies are higher than improvement accuracies: TurKontrol(2) gets 4.5% more utility than TurKontrol(2, anonymous).</p><p>2. γ = 1 and ballot accuracies are the same as improvement accuracies: TurKontrol(2) gets 3.7% more utility than TurKontrol(2, anonymous).</p><p>This shows using the average γ is not a bad approximation, because it only reduces the expected utility of TurKontrol <ref type="bibr" target="#b1">(2)</ref> by less than 5% in both cases. We expect it to produce even better results if the approximation is more refined (e.g., if we group the workers by accuracy).</p><p>In investigating the UCT algorithm, we use a fixed κ = 50 and a dynamically decreasing learning parameter θ = 0.2×log 2 10 n (s,a)</p><p>. As an action has been visited more frequently, the effect of randomness decreases, so less weight should be given to the new observation. Fig. <ref type="figure" target="#fig_8">12</ref> plots the average utility of the UCT algorithm as a function of the number of trials completed (from 10 000 to 500 000). The horizontal lines are not x-axes, but represent the average utility of TurKontrol(2) (data as in Fig. <ref type="figure" target="#fig_7">11</ref>). First notice that the performance of UCT improves with the increase in the number of completed trials. We also observe that UCT outperforms TurKontrol(2) after very few (10 000) trials on the two problems where the costs are small, but underperforms TurKontrol(2) on the problem where the costs are high. This behavior is probably because TurKontrol(2) already performs close to optimal on the high cost problem. We also find UCT consistently underperforms TurKontrol(3) (from comparing against Fig. <ref type="figure" target="#fig_7">11</ref>). This is because TurKontrol(3) computes close-to-optimal values, while UCT's performance is influenced by random noises during search trials. This experiment shows that UCT is useful for quickly finding a good suboptimal policy but has limited power in finding a close-to-optimal policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Choosing the best planning algorithm</head><p>From comparing the results of the three algorithms: the l-step lookahead, ADBS and UCT, we conclude that the l-step lookahead is the most efficient planning algorithm for the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6.">The effect of poor workers</head><p>We now consider the effect of worker accuracy on the effectiveness of agent control policies. Using fixed costs of <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref>, we compare the average net utility of three control policies. The first is TurKontrol(2). The second, TurKit, is a non-adaptive policy from the literature <ref type="bibr" target="#b36">[37]</ref>; it performs as many iterations as possible until its fixed allowance (400 in our experiment) is depleted and on each iteration it requests at least two ballots, invoking a third only if the first two disagree. Our third policy, TurKontrol(fixed), combines elements from decision theory with a fixed policy. After simulating the behavior of TurKontrol(2), we compute the integer mean number of iterations, μ imp and mean number of ballots, μ b , and use these values to drive a fixed control policy (μ imp iterations each with μ b ballots). Thus this represents non-adaptive policy whose parameters are tuned to costs and worker accuracies.</p><p>Fig. <ref type="figure" target="#fig_9">13</ref> (top) shows that both decision-theoretic methods work better than the TurKit policy, partly because TurKit runs more iterations than needed. A Student's t-test shows all differences are statistically significant with p &lt; 0.01. We also note that the performance of TurKontrol(fixed) is very similar to that of TurKontrol(2), when workers are very inaccurate, γ = 4. Indeed, in this case TurKontrol(2) executes a nearly fixed policy itself. In all other cases, however, TurKontrol(fixed) consistently underperforms TurKontrol <ref type="bibr" target="#b1">(2)</ref>. A Student's t-test confirms the differences are all statistically significant for γ &lt; 4.</p><p>We attribute this difference to the fact that the dynamic policy makes better use of ballots, e.g., it requests more ballots in late iterations, when the (harder) improvement tasks are more error-prone. The biggest performance gap between the two policies manifests when γ = 2, where TurKontrol(2) generates 19.7% more utility than TurKontrol(fixed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.7.">Robustness in the face of bad voters</head><p>As a final study, we consider the sensitivity of the previous three policies to increasingly noisy voters. Specifically, we repeat the previous experiment using the same error coefficient, γ w , for each worker's improvement and ballot behavior (Fig. <ref type="figure" target="#fig_9">13</ref> (bottom)). (We previously set the error coefficient for ballots to one half γ w to model the fact that voting is easier.) Fig. <ref type="figure" target="#fig_9">13</ref> (bottom) has the same shape as that of Fig. <ref type="figure" target="#fig_9">13</ref> (top) but with lower overall utility. Once again, TurKontrol(2) continues to achieve the highest average utility across all settings. Interestingly, the utility gap between the two TurKontrol variants and TurKit is consistently bigger for all γ than in the previous experiment. In addition, when γ = 1, TurKontrol(2) generates 25% more utility than TurKontrol(fixed) -a bigger gap seen in the previous experiment. A Student's t-test shows all that the differences between TurKontrol(2) and TurKontrol(fixed) are significant when γ &lt; 2 and the differences between both TurKontrol variants and TurKit are significant at all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Learning the improvement model</head><p>Extensive simulation results in the previous section show the usefulness of decision-theoretic techniques for iterative improvement workflows. This section addresses learning the model from real Mechanical Turk data <ref type="bibr" target="#b12">[13]</ref>. We already explored the learning of our ballot model in Section 3.4, so now we learn our improvement model. To learn the effect of a worker trying to improve an artifact, we first need a method for determining the ground truth for the quality of an arbitrary artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Estimating artifact quality gold standards</head><p>Since quality is a partially-observable statistical measure, we consider three ways to approximate it: simulating the definition, direct expert estimation, and averaged worker estimation.</p><p>Our first technique simply simulates the definition. We ask W workers to improve an artifact α and as before use multiple ballots, say l, to judge each improvement. We define the quality of α to be 1 minus the fraction of workers that are able to improve it. Unfortunately, this method requires W + W l jobs in order to estimate the quality of a single artifact; thus, it is both slow and expensive in practice. As an alternative, direct expert estimation is less complex. We teach a statisticallysophisticated computer scientist the definition of quality and ask her to estimate the quality to the nearest decile. <ref type="foot" target="#foot_8">13</ref> Our final method, averaged worker estimation, is similar, but averages the judgments from several Mechanical Turk workers via scoring jobs. These scoring jobs provide a definition of quality along with a few examples, mapped to a 0-10 integer scale; the workers are then asked to score several more artifacts. See Figs. 27, 28, and 29 for our user interface design.</p><p>We collect data on 10 images from the Web and use Mechanical Turk to generate multiple descriptions for each. We then select one description for each image, carefully ensuring that the chosen descriptions span a wide range of detail and language fluency. We also modified a description to obtain one that, we felt, was very hard to improve, thereby accounting for the high-quality region. When simulating the definition, we average over W = 22 workers. 14 We use a single expert for direct expert estimation and an average of 10 worker scores for averaged worker estimation. See Table <ref type="table" target="#tab_2">2</ref>.</p><p>Our hope, following the work of <ref type="bibr" target="#b57">[58]</ref>, is that averaged worker estimation, definitely the cheapest method, proves comparable to expert estimates and especially to the simulated definition. Indeed, we find that all three methods produce similar results. They agree on the two best and worst artifacts, and on average both expert and worker estimates are within 0.1 of the score produced by simulating the definition. We conclude that averaged worker estimation is equally effective and additionally easier and more economical (1 cent per scoring job), so we adopt this method to assess qualities in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Actually learning the model</head><p>Finally, we describe our approach for learning a model for the improvement phase. Our objective is to estimate P (q |q, w), the probability distribution that describes the quality q of a new artifact, α , when worker w improves artifact α of quality q. Moreover, we also learn a prior, P (q |q), in order to model work by a previously unseen worker.</p><p>There are two main challenges in learning this model: first, these functions are over a two-dimensional continuous space, and second, the training data is scant and noisy. To alleviate these difficulties, we break the task into two learning steps:</p><p>(1) learn a mean value for quality using regression, and (2) fit a conditional density function given the mean. We make the second learning task tractable by choosing parametric representations for these functions. Our full solution follows the following steps:</p><p>1. Generate an improvement job that contains T original artifacts α 1 , . . . , α T .</p><p>2. Crowdsource W workers to improve each artifact to generate W T new artifacts.</p><p>3. Estimate the qualities q i and q i,w for all artifacts in the set (see previous section). q i is the quality of α i and q i,w denotes the quality of the new artifact produced by worker w. This data is our training data.</p><p>4. Learn a worker-dependent distribution P (q |q, w) for every participating worker w. 5. Learn a worker-independent distribution P (q |q) to act as a prior for unseen workers.</p><p>We now describe the last two steps in detail: the learning algorithms. We first estimate the mean of worker w's improvement distribution, denoted by μ q w (q).</p><p>We assume that μ q w is a linear function of the quality of the original artifact. <ref type="foot" target="#foot_9">15</ref> By introducing μ Q w , we separate the variance in a worker's ability in improving all artifacts of the same quality from the variance in our training data, which is due to her starting out from artifacts of different qualities. To learn this we perform linear regression on the training data (q i , q i,w ). This yields q w = a w q + b w as the line of regression with standard error e w , which we truncate for values outside [0, 1].</p><p>To model a worker's variance when improving artifacts with the same quality, we consider three parametric representations for P (q |q, w): Triangular, Beta, and Truncated Normal. While clearly making an approximation, restricting attention to these distributions significantly reduces the parameter space and makes our learning problem tractable. Note that we assume the mean of each of these distributions is given by the line of regression. We consider each distribution in turn.</p><p>Triangular. The triangular-shaped probability density function has two fixed vertices (0, 0) and (1, 0). We set the</p><p>x-coordinate of the third vertex to μ q w (q), yielding the following probability density function f q w |q (q w ):</p><formula xml:id="formula_26">f Q w |q q w = ⎧ ⎪ ⎨ ⎪ ⎩ 2q w μ q w (q)</formula><p>if q w &lt; μ q w (q),</p><formula xml:id="formula_27">2(1-q w ) 1-μ q w (q) if q w μ q w (q). (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>Beta. We wish the Beta distribution's mean to be μ q w and its standard deviation to be proportional to e w . Therefore, we train a constant, c 1 , using gradient descent that maximizes the log-likelihood of observing the training data for all workers. <ref type="foot" target="#foot_10">16</ref>resulting in the distribution Beta( c 1 e w × μ q w (q), c 1 e w × (1 -μ q w (q))). The error e w appears in the denominator because the two parameters for the Beta distribution are approximately inversely related to its standard deviation.</p><p>Truncated Normal. As before we set the mean to μ q w and the standard deviation to be c 2 × e w where c 2 is a constant, trained to maximize the log-likelihood of the training data. This yields the distribution = Truncated Normal(μ q w (q), c 2 2 e 2 w ) where the truncated interval is [0, 1].</p><p>We use similar approaches to learn the worker-independent model P (q |q), except that training data is of the form (q i , q i ) where q i is the average improved quality for ith artifact, i.e., the mean of q i,w over all workers. Denote the standard deviation of this set as σ q i |q i . As before, we start with linear regression, q = aq + b. The Triangular distribution is defined exactly as before. For the other two distributions, their standard deviations depend on σ q i |q i . We assume that the conditional standard deviation σ q |q is quadratic in q, and then infer an unknown conditional standard deviation using the existing ones, by running a quadratic regression. As before, we use gradient descent to train constants for the Beta and Truncated Normal distributions.</p><p>We seek to determine which of the three distributions best models the data, and we employ leave-one-out cross validation. We set the number of original artifacts and number of workers to be ten each (T = W = 10), and spend $16.50 for this data collection. The algorithm iteratively trains on nine training examples, e.g. {(q i , q i )} for the worker-independent case, and measures the probability density of observing the tenth. We score a model by summing the ten log probability densities.</p><p>Our results show that Beta distribution with c 1 = 3.76 is the best conditional distribution for worker-dependent models. For the worker-independent model, with an intercept of 0.35 and slope of 0.34 from the linear regression, Truncated</p><p>Normal with c 2 = 1.00 performs the best. We suspect this is the case because most workers have average performance and Truncated Normal has a thinner tail than the Beta. In all cases, the Triangular distribution performs worst. This is probably because Triangular assumes a linear probability density, whereas, in reality, workers tend to provide reasonably consistent results, which translates to higher probabilities around the conditional mean. We use these best performing distributions in all subsequent experiments. In case of a returning worker, we use the corresponding worker-dependent model. Otherwise, we assume the worker performs averagely, and instead, use the worker-independent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">TurKontrol on Mechanical Turk</head><p>Now that we have learned the POMDP model, our final evaluation assesses the benefits of the dynamic workflow controlled by TurKontrol versus a non-adaptive workflow (as originally used in TurKit <ref type="bibr" target="#b36">[37]</ref>) under similar monetary con- sumption settings. We aim to answer the following questions: (1) Is there a significant quality difference between artifacts produced using TurKontrol and TurKit? (2) What are the qualitative differences between the two workflows?</p><p>As before, we run our experiments using the image description task. We use 40 fresh pictures from the Web and employ iterative improvement to generate descriptions for these. For each picture, we restrict a worker to take part in at most one task in each setting (i.e., non-adaptive or dynamic). We set the user interfaces to be identical for both settings and randomize the order in which the two conditions are presented to workers in order to eliminate human learning effects. Altogether there are 655 participating workers, of which 57 take part in both settings.</p><p>We devise automated rules to detect spammers. We reject an improvement job if the new artifact is identical to the original. We reject ballot and scoring jobs if they are returned so quickly that the worker could not have made a reasonable judgment.</p><p>Note that our system does not need to learn a model for a new worker before assigning them jobs; instead, it uses the worker-independent parameters γ and P (q |q) as a prior. These parameters are incrementally updated as TurKontrol obtains more information.</p><p>Recall that TurKontrol performs decision-theoretic control based on a user-defined reward function. We define the reward for submitting an artifact of quality q to be R(q) = $25q for our experiments. We set the cost of an improvement job to be 5 cents and a ballot job to be 1 cent. We use the 3-step lookahead algorithm for the controller. Under these parameters, TurKontrol-workflows run an average of 6.25 iterations with an average of 2.32 ballots per iteration, costing about 46 cents per image description on average. We use TurKit's original non-adaptive policy for ballots, which requests a third ballot if the first two voters disagree. We compute the number of iterations for TurKit so that the total money spent matches TurKontrol's. Since this number comes to be 6.47 we compare against three cases: TurKit 6 with 6 iterations, TurKit 7 with 7 iterations and TurKit 67 a weighted average of the two that equalizes monetary consumption.</p><p>For each final description we create a scoring job in which multiple workers score the descriptions. Fig. <ref type="figure" target="#fig_10">14</ref> compares the artifact qualities generated by TurKontrol and by TurKit 67 for the 40 images. We note that most points are below the y = x line, indicating that the dynamic workflow produces superior descriptions. Furthermore, the quality produced by TurKontrol is greater on average than TurKit's, and the difference is statistically significant: p &lt; 0.01 for TurKit 6 , p &lt; 0.01 for TurKit 67 and p &lt; 0.05 for TurKit 7 , using the Student's t-test.</p><p>Using our parameters, TurKontrol generates some of the highest-quality descriptions with an average quality of 0.67. TurKit 67 's average quality is 0.60; furthermore, it generates the two worst descriptions with qualities below 0.3. Finally, the standard deviation for TurKontrol is lower (0.09) than TurKit's (0.12). These results demonstrate overall superior performance of decision-theoretic control on live workflows.</p><p>While the 11% average quality increase produced by TurKontrol is statistically significant, some wonder if it is material. To better illustrate the importance of quality, we include another experiment. We run the nonadaptive, TurKit policy for additional improvement iterations, until it produces artifacts with an average quality equal to that produced by TurKontrol. Fixing the quality threshold, the TurKit policy has to run an average of 8.76 improvements, compared to the 6.25 improvement iterations used by TurKontrol. As a result the nonadaptive policy spends 28.7% more money than TurKontrol to achieve the same quality results. Note that final artifact quality is neither linear in the number of iterations nor total cost. Intuitively, it is much easier to improve an artifact when its quality is low than when it is high.</p><p>We also qualitatively study TurKontrol's behavior compared to TurKit's and find an interesting difference in the use of ballots. Fig. <ref type="figure" target="#fig_11">15</ref> plots the average number of ballots per iteration number. Since TurKit's ballot policy is fixed, it consistently uses about 2.45 ballots per iteration. TurKontrol, on the other hand, uses ballots much more intelligently. In the first two improvement iterations TurKontrol does not bother with ballots because it expects that most workers will improve the artifact. As iterations increase, TurKontrol increases its use of ballots, because the artifacts are harder to improve in later iterations, and hence TurKontrol needs more information before deciding which artifact to promote to the next iteration. The eighth iteration is an interesting exception; at this point improvements have become so rare that if even the first voter rates the new artifact as a loser, then TurKontrol often believes the verdict.  "This is Gene Hackman, in a scene from his film "The Conversation," in which he plays a man paid to secretly record people's private conversations. He is squatting in a bathroom gazing at tape recorder which he has concealed in a blue toolbox that is now placed on a hotel or motel commode (see paper strip on toilet seat). He is on the left side of the image in a gray jacket while the commode is on the right side of the picture. His fingertips rest on the lid of the commode. He is wearing a black court and a white shirt. He has put on glasses also." It took the nonadaptive workflow 6 improvement jobs and 13 ballot jobs to reach a version: "A thought about repairing: Image shows a person named Gene Hackman is thinking about how to repair the toilet of a hotel room. He has opened his tool box which contains plier, screw diver, wires, etc. He looks seriously in his tool box and thinking which tool he willuse. Wearing a grey coat he sits in front of the toilet seat resting gently the toilet seat".</p><p>Besides using ballots intelligently we believe that TurKontrol adds two other kinds of reasoning. First, six of the seven pictures that TurKontrol finished in 5 iterations have higher qualities than TurKit's. This suggests that its quality tracking is working well. Perhaps due to the agreement among various voters, TurKontrol is able to infer that a description already has quality high enough to warrant termination. Secondly, TurKontrol has the ability to track individual workers, and this also affects its posterior calculations. For example, in one instance TurKontrol decided to trust the first vote because that worker had superior accuracy as reflected in a low error parameter. We expect that for repetitive tasks this will be an enormously valuable ability, since TurKontrol will be able to construct more informed worker models and make more superior decisions.</p><p>We present an image description example in Fig. <ref type="figure" target="#fig_12">16</ref>. It is interesting to note that both processes managed to find the origin of the image. However, the TurKontrol version exemplifies better use of language, factuality and level of detail. In retrospect, we find the nonadaptive workflow probably made a wrong ballot decision in the sixth iteration, where a decision was critical yet only three voters were consulted. TurKontrol on the other hand, reached a decision after 6 unanimous votes at the same stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Decision-theoretic optimization of multiple workflows</head><p>Now that we have explored the decision-theoretic control of a single workflow, we consider the scenario when the requester has more than one <ref type="bibr" target="#b35">[36]</ref>. In our exploration, for simplicity, we will only consider binary classification workflows. However, we note that the ideas we present are equally applicable for more complex workflows, like the iterative improvement workflow we considered earlier. Worker's answer b depends on the difficulty of the question d (generated by δ), worker's error parameter γ and the question's true answer v. There are W workers, who complete T tasks, all of which can be solved using a set of K different workflows. b is the only observed variable.</p><p>To see why it is useful to switch between workflows, consider the process task designers use to design their workflows. Typically, they will quantitatively experiment with several alternative workflows to accomplish the task, but choose a single one for the production runs (e.g. the workflow that achieves the best performance during early testing). In the simplest case, alternative workflows may differ only in their user interfaces or instructions; in other cases, workers may be asked to solve a problem with a very different set of steps.</p><p>Unfortunately, this seemingly natural design paradigm does not achieve the full potential of crowdsourcing. Selecting a single best workflow is suboptimal, because alternative workflows can compose synergistically to attain higher-quality results. While a given workflow may have the best performance on average, it is not necessarily best on every problem.</p><p>Suppose after gathering some answers for a task, one wishes to further increase one's confidence in the results; which workflow should be invoked? Due to the very fact that it is different, an alternative workflow may offer independent evidence, and this can significantly bolster one's confidence in the answer. If the "best" workflow is giving mixed results for a task, then an alternative workflow is often the best way to disambiguate. Instead of selecting one a priori best workflow, a better solution should reason about this potential synergy. Our POMDP model automatically tracks the expected accuracy of alternative workflows, switching away from the "on average best" workflow if intermediate results lead it to conclude another might be superior for the particular instance of the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Probabilistic model for multiple workflows</head><p>We extend our generative model of worker responses to incorporate the existence of multiple workflows to complete the same task. It includes multiple, workflow-specific error parameters for each worker and workflow-specific difficulties. Now, there are K alternative workflows that a worker could use to arrive at an answer. Let d k ∈ [0, 1] denote the inherent difficulty of completing a task using workflow k, and let γ k w ∈ [0, ∞) be worker w's error parameter for workflow k. Notice that every worker has K error parameters. Having several parameters per worker incorporates the insight that some workers may perform well when asked the question in one way (e.g., visually) but not so well when asked in a different way (e.g., when asked in English, since their command of that language may not be great).</p><p>The accuracy of a worker w, a(d k , γ k w ), is the probability that she produces the correct answer using workflow k. We rewrite our original definition of worker accuracy accordingly:</p><formula xml:id="formula_29">a d k , γ k w = 1 2 1 + 1 -d k γ k w . (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>Fig. <ref type="figure" target="#fig_13">17</ref> illustrates the plate notation for our generative model, which encodes a Bayes Net for responses made by W workers on T tasks, all of which can be solved using a set of K alternative workflows. The correct answer, v, the difficulty parameter, d, and the error parameter, γ , influence the final answer, b, that a worker provides, which is the only observed variable.</p><p>d is generated by δ, a K -dimensional random variable describing a joint distribution on workflow difficulties. The answer b k w that worker w with error parameter γ k w provides for a task using workflow k is governed by the following equations:</p><formula xml:id="formula_31">P b k w = v d k = a d k , γ k w , (<label>18</label></formula><formula xml:id="formula_32">)</formula><formula xml:id="formula_33">P b k w = v d k = 1 -a d k , γ k w . (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>As before, an underlying assumption is that given the workflow difficulty, d k , and the true answer v, the b k w 's are independent of each other. δ encodes the assumption that workflows may not be independent of each other. The fact that one workflow is easy might imply that a related workflow is easy. Finally, we still assume that the workers do not collaborate with each other and that they are not adversarial, i.e., they do not purposely submit incorrect answers. Now we discuss how to dynamically switch between workflows to obtain the highest accuracy for a given task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">A decision-theoretic agent</head><p>In this section, we answer the following question: Given a specific task that can be accomplished using alternative workflows, how do we design an agent that can leverage the availability of these alternatives by dynamically switching between them, in order to achieve a high-quality solution? We design an automated agent, named AgentHunt, that uses the following POMDP. Definition 7. The POMDP is a six-tuple S, A, T , R, O, P , where</p><formula xml:id="formula_35">• S = {(d 1 , d 2 , . . . , d K , v)|d k ∈ [0, 1], v ∈ {0, 1}} d k is the difficulty of the kth workflow. v is the true answer. • A = {ballot 1 , ballot 2 , . . . , ballot K , submit true, submit false}. • R : S × A → R is described below.</formula><p>• T : Identity map.</p><p>• O = {true, false} is a Boolean answer received for a ballot question.</p><formula xml:id="formula_36">• P : S × O → [0, 1] is defined by our generative model.</formula><p>As in the POMDP describing our simple binary classification workflow, the reward function maintains the value of submitting a correct answer and the penalty for submitting an incorrect answer. Additionally, it maintains a cost that AgentHunt incurs when it creates a job. We can modify the reward function to match our desired budgets and accuracies. Fig. <ref type="figure" target="#fig_14">18</ref> is a flow-chart of decisions that AgentHunt has to take. As before, we assume that every future worker is an average worker. In other words, for a given workflow k, every future worker has an error parameter equal to γ k = 1 W w γ k w where W is the number of workers. Also as before, after submitting an answer, AgentHunt updates its records about all the workers who participated in the task using what it believes to be the correct answer, using the following modified update rules. For a worker w who submitted an answer using workflow k, γ k w ← γ k wd k α, should the worker answer correctly, and</p><formula xml:id="formula_37">γ k w ← γ k w + (1 -d k )α,</formula><p>should the worker answer incorrectly, where α is a learning rate. Any worker that AgentHunt has not seen previously begins with the average γ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Learning the model</head><p>In order to behave optimally, AgentHunt needs to learn all γ values, average worker error parameters γ , and the joint workflow difficulty prior δ, which is a part of its initial belief. We consider two unsupervised approaches to learningoffline batch learning and online RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Offline learning</head><p>In this approach we first collect training data by having a set of workers complete a set of tasks using a set of workflows. This generates a set of worker responses, b. Since the true answer values v are unknown, an option is supervised learning, where experts label true answers and difficulties. However, this option requires significant expert time upfront, so instead, we use an EM algorithm and learn all parameters jointly.</p><p>For EM purposes, we simplify the model by removing the joint prior δ, and treat the variables d and γ as parameters.</p><p>In the E-step, we keep parameters fixed to compute the posterior probabilities of the hidden true answers: p(v t |b, d, γ )</p><p>for each task t. The M-step uses these probabilities to maximize the standard expected complete log-likelihood Q over d and γ :</p><formula xml:id="formula_38">Q (d, γ ) = E ln p(v, b|d, γ ) (20)</formula><p>where the expectation is taken over v given the old values of γ and d.</p><p>After estimating all the hidden parameters, AgentHunt can compute γ k for each workflow k by taking the average of all the learned γ k parameters. Then, to learn δ, we can fit a Truncated Multivariate Normal distribution to the learned d. This difficulty prior determines a part of the initial belief state of AgentHunt. We complete the initial belief state by assuming the correct answer is distributed uniformly among the 2 alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Online reinforcement learning</head><p>Offline learning of our model can be very expensive, both temporally and monetarily. Moreover, we cannot be sure how much training data is necessary before the agents are ready to act in the real-world. An ideal AI agent will learn while acting in the real-world, tune its parameters as it acquires more knowledge, while still producing meaningful results. We modify AgentHunt to build its RL twin, AgentHunt RL , which is able to accomplish tasks right out of the box.</p><p>AgentHunt RL starts with uniform priors on difficulties of all workflows. When it begins a new task, it uses the existing parameters to recompute the best policy and uses that policy to guide the next set of decisions. After completing the task, AgentHunt RL recalculates the maximum-likelihood estimates of the parameters γ and d using EM as above. The updated parameters define a new POMDP for which our agent computes a new policy for the future tasks. This relearning and POMDP-solving can be time-consuming, but we do not have to relearn and resolve after completing every task. We can easily speed the process by solving a few tasks before launching a relearning phase.</p><p>As in all of RL, AgentHunt RL must also make a tradeoff between taking possibly suboptimal actions in order to learn more about its model of the world (exploration), or taking actions that it believes to be optimal (exploitation). AgentHunt RL uses a modification of the standard -greedy approach <ref type="bibr" target="#b61">[62]</ref>. With probability , AgentHunt RL will uniformly choose between suboptimal actions. The exception is that it will never submit an answer that it believes to be incorrect, since doing so would not help it learn anything about the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiments</head><p>This section addresses the following three questions. <ref type="bibr" target="#b0">(1)</ref> In practice, how much value can be gained from switching between different workflows for a task? <ref type="bibr" target="#b1">(2)</ref> What is the tradeoff between cost and accuracy? and (3) Previous decisiontheoretic crowdsourcing systems have required an initial training phase; can reinforcement learning provide similar benefits without such training? We choose an NLP labeling task, for which we create K = 2 alternative workflows (described below). To answer the first two questions, we compare two agents: TurKontrol 0 , which we will now refer to from now on as TurKontrol, a state-of-the-art controller for optimizing the execution of a single (best) workflow, and our AgentHunt, which can switch between the two workflows dynamically. We first compare them in simulation; then we allow the agents to control live workers on Amazon Mechanical Turk.</p><p>We answer the third question by comparing AgentHunt with AgentHunt RL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Implementation</head><p>The POMDP must manage a belief state over the cross product of the Boolean answer and two continuous variablesthe difficulties of the two workflows. Since solving a POMDP with a continuous state space is challenging, we discretize difficulty into eleven possible values, leading to a (world) state space of size 2 × 11 × 11 = 242. To solve POMDPs, we run the ZMDP package <ref type="foot" target="#foot_11">17</ref> for 300 s using the default Focused Real-Time Dynamic Programming search strategy <ref type="bibr" target="#b56">[57]</ref>. Since we can cache the complete POMDP policy in advance, AgentHunt can control workflows in real time.</p><p>Since we have discretized difficulty, we also modify the learning process slightly. After we learn all values of d, we round the values to the nearest discretizations and construct a histogram to count the number of times every state appears in the training data. Then, before we use the implicit joint distribution as the agent's starting belief state, we smooth it by adding 1 to every bin (Laplace smoothing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Evaluation task: NER tagging</head><p>In order to test our agents, we select a task that is needed by several colleagues: labeling training data for named-entity recognition (NER) <ref type="bibr" target="#b51">[52]</ref>. NER tagging is a common problem in NLP and information extraction: given a body of text (e.g., "Barack Obama thinks this research is not bad.") and a subsequence of that text (e.g., "Barack Obama") that specifies an entity, output a set of tags that classify the type of the entity (e.g., person, politician). Since machine learning techniques are used to create production NER systems, large amounts of labeled data (of the form described above) are needed. Obtaining the most accurate training data at minimal cost, is therefore, an excellent test of our methods.</p><p>In consultation with NER domain experts we develop two workflows for the task (Fig. <ref type="figure" target="#fig_15">19</ref>). Both workflows begin by providing users with a body of text and an entity, like "Nixon concluded five days of private talks with Chinese leaders in Beijing." The first workflow, called "WikiFlow," first uses Wikification <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46]</ref> to find a set of possible Wikipedia articles describing the entity, such as "Nixon (film)" and "Richard Nixon." It displays these articles (including the first sentence of each article) and asks workers to choose the one that best describes the entity. Finally, it returns the Freebase <ref type="foot" target="#foot_12">18</ref> tags associated with the Wikipedia article selected by the worker.</p><p>The second workflow, "TagFlow," asks users to choose the best set of Freebase tags directly. For example, the Freebase tags associated with "Nixon (film)" is {/film/film}, while the tags associated with "Richard Nixon" includes {/people/person, /government/us -congressperson}. TagFlow displays the tag sets corresponding to the different options and asks the worker to choose the tag set that best describes the entity mentioned in the sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Experimental setup</head><p>First, we gather training data using Mechanical Turk. We generate 50 NER tasks. For each task, we submit 40 identical WikiFlow jobs and 40 identical TagFlow jobs to Mechanical Turk. At $0.01 per job, the total cost is $60.00 including Amazon commission. Using our EM technique, we then calculate average worker accuracies, γ WF and γ TF , corresponding to Wiki-Flow and TagFlow respectively. Somewhat to our surprise, we find that γ TF = 0.538 &lt; γ WF = 0.547 -on average, workers found TagFlow to be very slightly easier than WikiFlow. Note that this result implies that AgentHunt will always create a TagFlow job to begin a task. We also note that the difference between γ TF and γ WF influences the switching behavior of AgentHunt. Intuitively, if AgentHunt were given two workflows whose average difficulties were further apart, AgentHunt would become more reluctant to switch to a harder workflow. Because we find TagFlow jobs to be slightly easier, for all experiments, we set TurKontrol so it creates TagFlow jobs. We also use this training data to construct both agents' initial beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4.">Experiments using simulation</head><p>We first run our agents in a simulated environment. On each run, the simulator draws states from the agents' initial belief distributions. We fix the reward of returning the correct answer to 0, and vary the reward (penalty) of returning an incorrect answer between the following values: -10, -100, -1000, and -10 000. We set the cost of creating a job for a (simulated) worker to -1. We use a discount factor of 0.9999 in the POMDP. For each setting of reward values we run 1000 simulations and report mean net utilities (Fig. <ref type="figure">21</ref>).</p><p>We find that when the stakes are low, the two agents behave almost identically. However, as the penalty for an incorrect answer increases, AgentHunt's ability to switch between workflows allows it to capture much more utility than TurKontrol. As expected, both agents submit an increasing number of jobs as the importance of answer correctness rises and in the process, both of their accuracies rise too (Fig. <ref type="figure">20</ref>). However, while both agents become more accurate, TurKontrol does not increase its accuracy enough to compensate for the exponentially growing penalties. Instead, as AgentHunt experiences an almost sublinear decline in net utility, TurKontrol sees an exponential drop (Fig. <ref type="figure">21</ref>). A Student's t-test shows that for all settings of penalty except -10, the differences between the two systems' average net utilities are statistically significant. When the penalty is -10, p &lt; 0.4, and at all other reward settings, p &lt; 0.0001. Thus we find that at least in simulation, AgentHunt outperforms TurKontrol on all our metrics.</p><p>We also analyze the systems' behaviors qualitatively. As expected, AgentHunt always starts by creating a TagFlow job, since γ TF &lt; γ WF implies that TagFlows lead to higher worker accuracy on average. Interestingly, although AgentHunt has  more available workflows, it creates fewer actual jobs than TurKontrol, even as correct answers become increasingly important. We also split our problems into three categories to better understand the agents' behaviors: 1) TagFlow is easy, 2) TagFlow is hard, but WikiFlow is easy, and 3) Both workflows are difficult. In the first case, both agents terminate quickly, though AgentHunt spends a little more money since it also requests WikiFlow jobs to double-check what it learns from TagFlow jobs. In the second case, TurKontrol creates an enormous number of jobs before it decides to submit an answer, while AgentHunt terminates much faster, since it quickly deduces that TagFlow is hard and switches to creating easy WikiFlow jobs. In the third case, AgentHunt expectedly creates more jobs than TurKontrol before terminating, but AgentHunt does not do too much worse than TurKontrol, since it correctly deduces that gathering more information is unlikely to help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5.">Experiments using Mechanical Turk</head><p>We next run the agents on real data gathered from Mechanical Turk. We generate 106 new NER tasks for this experiment, and use gold labels supplied by a single expert. Since in our simulations we found that the agents spend on average about the same amount of money when the reward for an incorrect answer is -100, we use this reward value in our real-world experiments.</p><p>As Table <ref type="table" target="#tab_3">3</ref> shows, AgentHunt fares remarkably better in the real-world than TurKontrol. A Student's t-test shows that the difference between the average net utilities of the two agents is statistically significant with p &lt; 0.03. However, we see that TurKontrol spends less, leading one to naturally wonder whether the difference in utility can be accounted for by the cost discrepancy. Thus, we modify the reward for an incorrect answer (to -300) for TurKontrol to create TurKontrol 300 , which spends about the same amount of money as AgentHunt. But even after the modification, the accuracy of AgentHunt is still much higher. A Student's t-test shows that the difference between the average net utilities of AgentHunt and TurKontrol 300 is statistically significant at p &lt; 0.01 showing that in the real-world, given similar budgets, AgentHunt produces significantly better results than TurKontrol. Indeed, AgentHunt reduces the error of TurKontrol by 45% and the error of TurKontrol 300 by 50%. Surprisingly, the accuracy of TurKontrol 300 is lower than that of TurKontrol despite the additional jobs; we attribute this to statistical variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.6.">Adding reinforcement learning</head><p>Finally, we compare AgentHunt to AgentHunt RL . AgentHunt RL 's starting belief state is a uniform distribution over all world states and it assumes that γ k = 1 for all workflows. To encourage exploration, we set = 0.1. We test it using the same 106 tasks described above. Table <ref type="table" target="#tab_4">4</ref> reproduces the results of AgentHunt from Table <ref type="table" target="#tab_3">3</ref> alongside those of AgentHunt RL . We see that while AgentHunt RL achieves a slightly higher accuracy than AgentHunt, the difference between their net utilities is not statistically significant (p = 0.4), which means AgentHunt RL is comparable to AgentHunt, suggesting that AgentHunt can perform in an "out of the box" mode, without needing a training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related work</head><p>Modeling repeated labeling in the face of noisy workers has received significant attention, but we are the first to use decision theory to dynamically poll for more labels as necessary. Dawid et al. <ref type="bibr" target="#b13">[14]</ref> and Romney et al. <ref type="bibr" target="#b47">[48]</ref> are one of the firsts to incorporate a worker accuracy model to improve label quality. Raykar et al. <ref type="bibr" target="#b46">[47]</ref> propose a model in which the parameters for worker accuracy depend on the true answer. Whitehill et al. <ref type="bibr" target="#b66">[67]</ref> address the concern that worker labels should not be modeled as independent of each other unless given problem difficulty. Welinder et al. <ref type="bibr" target="#b65">[66]</ref> design a multidimensional model for workers that takes into account competence, expertise, and annotator bias. Kamar et al. <ref type="bibr" target="#b26">[27]</ref> extract features from the task at hand and use Bayesian Structure Learning to learn the worker response model. Parameswaran et al. <ref type="bibr" target="#b42">[43]</ref> conduct a policy search to find an optimal dynamic control policy with respect to constraints like cost or accuracy. Karger et al. <ref type="bibr" target="#b27">[28]</ref> develop an algorithm to generate a graph that represents an assignment of tasks to workers and infers correct answers based on low-rank matrix approximation. Given the assumptions that all tasks share an equal level of difficulty and that workers are either always correct or randomly guess, they analytically prove the optimality of their algorithm at minimizing a budget for a target error rate with respect to any other possible algorithm using their assumptions. Our methods use more general models which fall outside their assumptions, but provide the utility maximization guarantees that any POMDP model inherits. Wauthier et al. <ref type="bibr" target="#b63">[64]</ref> treat the entire crowdsourcing pipeline from data collection to learning within a Bayesian framework. Kajino et al. <ref type="bibr" target="#b25">[26]</ref> formulate the repeated labeling problem as a convex optimization problem. Lin et al. <ref type="bibr" target="#b34">[35]</ref> address the case when either requesters cannot enumerate all possible answers for the worker or when the solution space is infinitely large.</p><p>Other innovative workflows have been designed for complex tasks, for example, find-fix-verify for an intelligent editor <ref type="bibr" target="#b4">[5]</ref>, iterative dual pathways for speech-to-text transcription <ref type="bibr" target="#b33">[34]</ref> and others for counting calories on a food plate <ref type="bibr" target="#b41">[42]</ref>. Lasecki et al. <ref type="bibr" target="#b31">[32]</ref> design a system that allows multiple users to control the same interface in real-time. Control can be switched between users depending on who is doing better. Kulkarni et al. <ref type="bibr" target="#b29">[30]</ref> show the crowd itself can help with the design and execution of complex workflows.</p><p>Ipeirotis et al. <ref type="bibr" target="#b22">[23]</ref> observe that workers tend to have bias on multiple-choice, annotation tasks. They learn a confusion matrix to model the error distribution of individual workers. However, their model assumes workers' errors are completely independent, whereas, our model handles situations where workers make correlated errors due to the intrinsic difficulty of the task.</p><p>Kulkarni et al. <ref type="bibr" target="#b30">[31]</ref> design an alternative crowdsourcing platform that is not a marketplace. Instead, it intelligently routes work to workers and controls for accuracy and quality. Ho et al. <ref type="bibr" target="#b19">[20]</ref> consider how to assign tasks to workers on arrival.</p><p>Huang et al. <ref type="bibr" target="#b21">[22]</ref> look at the problem of designing a task under budget and time constraints. They illustrate their approach on an image-tagging task. By wisely setting variables, such as reward per task and the number of labels requested per image, they increase the number of useful tags acquired. Donmez et al. <ref type="bibr" target="#b15">[16]</ref> observe that workers' accuracies often change over time (e.g., due to fatigue, mood, task similarity, etc). Rzeszotarski et al. <ref type="bibr" target="#b50">[51]</ref> propose to use micro-breaks to overcome fatigue effects. As these approaches are orthogonal to ours, we would like to integrate the methods in the future.</p><p>Shahaf and Horvitz <ref type="bibr" target="#b52">[53]</ref> develop an HTN-planner style decomposition algorithm to find a coalition of workers, each with different skill sets, to solve a task. Some members of the coalition may be machines and others humans; different skills may command different prices. In contrast to our work, Shahaf and Horvitz do not consider methods for learning models of their workers.</p><p>The benefits from combining disparate workflows have been previously observed. Babbage's Law of Errors suggests that the accuracy of numerical calculations can be increased by comparing the outputs of two or more methods <ref type="bibr" target="#b18">[19]</ref>. However, in previous work these workflows have been combined manually; AgentHunt embodies the first method for automatically evaluating potential synergy and dynamically switching between workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and future work</head><p>We believe that AI has the potential to impact the growing thousands of requesters who use crowdsourcing to make their processes more efficient. Our work can be extended in several important directions towards realizing this vision. We list a few below.</p><p>Task economic uncertainty. We have so far assumed that the price for each task is a constant and an input. However, figuring out the actual value of a task is an important problem. This problem is difficult due to the highly dynamic nature of crowdsourcing markets. At the supply end, the value of a job can be influenced by workers' expertise, motivation of work, and personal interests. First, we plan to explore approaches that estimate value based on passively observing workflow activity. As a first step we will develop models that allow for the estimation of an effective hourly wage for a job. This problem is non-trivial given the many sources of variance including network delay, a worker's technology adequacy, whether a worker is multi-tasking, etc. Second, we will explore active algorithms for estimating these models. These approaches will actively experiment with payment choices in order to more efficiently learn a model. Further, for both the active and passive cases, we plan to extend this model to handle tasks with time constraints. This will require modeling the relationship between the amount of payment and the task completion speed.</p><p>Incentives. Paying an optional bonus for exceptional work can be a viable way of motivating high-quality results and forming long-term collaborations with competent workers. Questions that need to be answered are; (1) When to pay a bonus? (2) Who should receive a bonus? and (3) How much should the bonus be? An intuitive way is to pay a portion of the monetary savings of a good result. Another form of incentive involves recognition, such as leader boards. Investigations need to be carried out to determine which (combination of) bonus forms (and magnitude) can be the most effective. Here again we plan to study both passive and active approaches for modeling incentive structure in workflows.</p><p>A universal worker model. A worker's productivity varies over time. This is due to her fatigue level, variance of mood and work efficiency during the day, or the skillfulness acquired during her experiences with similar tasks. We plan to generalize our worker model by integrating several temporal parameters: (1) the time of the day and the day of the week or year, (2) a worker's consecutive working hours, and (3) her cumulative hours on the same type of tasks. Further, the performance of any crowdsourcing system can be lowered by noisy answers provided by spammers, or even malicious workers. An important aspect of our model will be to support the inference of spamming and malicious intent.</p><p>Example selection for reinforcement learning. Our work does not consider the problem of carefully curating the reinforcement learning process whether it be online or offline. For example, given a set of tasks, we do not know how large a subset of tasks we should solve before we update parameters, nor do we know which subset of tasks to use, and in which order. All these factors can affect the ability of our agent to learn parameters accurately and quickly. We wish to investigate these questions in future research.</p><p>Additive utility function. Our utility function is additive, in that each task instance is treated independently of each other. The total utility that we achieve for a set of tasks is equal to the sum of the utilities we achieve for each task. Such an assumption may not hold in other scenarios, e.g., if we are trying to train a machine learning classifier using crowdsourced labels. Here, an active learning formalism may be more appropriate <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>Abstractness of utility. To make our work usable for the vast majority, we must be able to elicit a utility function from our users. This is not an issue just with our approach, but with all decision-theoretic formalizations. The concept of utility is abstract, and as such, cannot be easily constructed even with domain knowledge. An extension of this work could consider ways to derive utilities from budgetary or accuracy constraints, which are far more concrete and comprehensible. Alternatively, we may cast this as a utility elicitation problem <ref type="bibr" target="#b8">[9]</ref>.</p><p>General-purpose crowdsourcing control. Our long-term research goal is to automatically control workflows consisting of a broader range of tasks <ref type="bibr" target="#b64">[65]</ref>. To achieve this goal, we need to specify a general language for workflows, and develop methods for translating workflow specifications into decision problems. We must also build a worker model for each task type. However, we believe that many crowdsourcing tasks can be captured with a relatively small number of task classes, such as tasks with discrete alternatives, content generation, etc. By having a task library we can share parameters across similar tasks. Given a new task, we can transfer knowledge from related tasks in the library, thereby simplifying and expediting the model training process. We are currently developing a general-purpose workflow controller called CLOWDER, which, given any new workflow, will automatically convert it into a POMDP and control it for the requester. Such a software will be extremely important in realizing our vision, since then our techniques will be easily accessible to requesters who do not have advanced AI knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>This work applies modern planning and machine learning techniques to the quality control problem of crowdsourcing, a new and rapidly-growing method for accomplishing various tasks, and demonstrates the usefulness of decision theory in a real-world setting, by controlling dynamic workflows that successfully achieve statistically-significant, higher-quality results than traditional, non-adaptive workflows. In particular, we make the following contributions. 19   1. As complex workflows have become more commonplace in crowdsourcing and are regularly employed for high-quality output, we introduce an exciting new application for artificial intelligence, the control of these workflows. 2. We use POMDPs to model single workflows as well as multiple workflows, and define generative models that govern various observation models of the processes. Our agents implement our mathematical framework and use it to optimize and control selection and execution of workflows. Extensive simulations and real-world experiments show that our agents are robust in a variety of scenarios and parameter settings, and achieve higher utilities than previous, non-adaptive policies. 3. We present efficient and cheap mechanisms for learning model parameters from limited and noisy training data. We validate the parameters independently and show that our learned models significantly outperform the popular majorityvote baseline when resources are constrained.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. The percent reduction in error that TurKontrol 0 achieves when comparing its accuracy to that of MV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. A plate model of ballot jobs under a supervised learning setting. b represents the ballot outcome, γ is a worker's individual error parameter, d is the difficulty of the job and v is the truth value of the job. Shaded nodes represent observed variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A plate model of ballot jobs under an unsupervised learning setting. b represents the ballot outcome, γ is the worker's individual error parameter, d is the difficulty of the job, and w is the truth value of the job. Shaded nodes represent observed variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Accuracies of using our ballot model (by applying both supervised and unsupervised learning) and majority vote on random voting sets of different sizes, averaged over 50000 random sets of each size. The models generated by the two learning algorithms both achieve higher accuracy than the majority vote. Using supervised learning, our ballot model achieves significantly higher accuracy than the majority vote (p &lt; 0.01).</figDesc><graphic coords="11,164.87,223.95,218.80,67.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Flow-chart for the iterative text improvement task, reprinted from [37].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Computations needed by TurKontrol 0 for control of an iterative-improvement workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition 6 .</head><label>6</label><figDesc>The POMDP for the iterative improvement workflow is a six-tuple S, A, T , R, O, P , where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Average utility of TurKontrol with various lookahead depths calculated using 10 000 simulation trials on three sets of (improvement, ballot) costs:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12.Average utility of the UCT algorithm while varying the number of trials compared to average utility of 10 000 simulation trials of TurKontrol(2) (the horizontal lines, which do not represent the x-axes) with improvement and ballot costs<ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b9">10)</ref> [top],<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref> [middle], and (0.3, 0.1) [bottom]. UCT achieves higher average utility when the costs are low.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.13. Average utility of three control policies averaged over 10 000 simulation trials, varying mean error coefficient, γ . Top -We set workers to have better accuracies in ballot jobs than improvement jobs. Bottom -We set workers to have equally accurate in both ballot and improvement jobs. TurKontrol(2) produces the best policy in every case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Average qualities of 40 descriptions generated by TurKontrol and by TurKit under the same monetary consumption. TurKontrol generates statistically-significant higher-quality descriptions than TurKit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Average number of ballots for the non-adaptive and dynamic workflows. TurKontrol makes intelligent use of ballots.</figDesc><graphic coords="20,128.21,194.39,283.68,193.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. An image description example. It took TurKontrol 6 improvement jobs and 14 ballot jobs to reach the final version:"This is Gene Hackman, in a scene from his film "The Conversation," in which he plays a man paid to secretly record people's private conversations. He is squatting in a bathroom gazing at tape recorder which he has concealed in a blue toolbox that is now placed on a hotel or motel commode (see paper strip on toilet seat). He is on the left side of the image in a gray jacket while the commode is on the right side of the picture. His fingertips rest on the lid of the commode. He is wearing a black court and a white shirt. He has put on glasses also." It took the nonadaptive workflow 6 improvement jobs and 13 ballot jobs to reach a version: "A thought about repairing: Image shows a person named Gene Hackman is thinking about how to repair the toilet of a hotel room. He has opened his tool box which contains plier, screw diver, wires, etc. He looks seriously in his tool box and thinking which tool he willuse. Wearing a grey coat he sits in front of the toilet seat resting gently the toilet seat".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Worker's answer b depends on the difficulty of the question d (generated by δ), worker's error parameter γ and the question's true answer v. There</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. AgentHunt's decisions when executing a task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. In this NER task, the TagFlow (top) is considerably harder than the WikiFlow (bottom) since the tags are very similar. The correct tag set is {location} since Washington state is neither a county nor a citytown.</figDesc><graphic coords="24,163.69,54.70,212.64,267.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 20 .Fig. 21 .</head><label>2021</label><figDesc>Fig.<ref type="bibr" target="#b19">20</ref>. In simulation, as the importance of answer correctness increases, both agents converge to 100 percent accuracy, but AgentHunt does so more quickly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Snapshot of a complete ballot HIT.</figDesc><graphic coords="29,73.11,81.11,402.48,566.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 24 .</head><label>24</label><figDesc>Fig. 24. Snapshot of the instruction portion of a ballot HIT.</figDesc><graphic coords="30,91.36,245.71,357.36,265.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 25 .Fig. 26 .</head><label>2526</label><figDesc>Fig. 25. Snapshot of the example portion of a ballot HIT.</figDesc><graphic coords="30,156.79,540.75,226.56,133.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 27 .Fig. 28 .</head><label>2728</label><figDesc>Fig. 27. Snapshot of the instruction portion of a scoring HIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 29 .</head><label>29</label><figDesc>Fig. 29. Snapshot of the actual task portion of a scoring HIT.</figDesc><graphic coords="32,78.67,54.48,382.80,430.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="28,42.89,414.75,454.32,222.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3, 0.1). TurKontrol(2) outperforms the ADBS algorithms in all settings.</figDesc><table><row><cell>Costs</cell><cell>TurKontrol(2)</cell><cell>ADBS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Interval length</cell><cell>0.02</cell><cell>0.01</cell><cell>0.005</cell></row><row><cell></cell><cell></cell><cell>|S|</cell><cell>3577</cell><cell>41 072</cell><cell>809 420</cell></row><row><cell>(30, 10) (3, 1) (0.3, 0.1)</cell><cell>264.906 445.721 466.820</cell><cell>V  *  (s 0 ) V  *  (s 0 ) V  *  (s 0 )</cell><cell>253.951 365.866 382.066</cell><cell>253.951 366.798 385.698</cell><cell>253.951 367.674 387.366</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Qualities of 10 artifacts, collected through three methods: definition simulation, direct expert estimation and averaged worker estimation. The averaged worker estimation is the cheapest method that provides comparable results to the other two methods.</figDesc><table><row><cell></cell><cell>α 1</cell><cell>α 2</cell><cell>α 3</cell><cell>α 4</cell><cell>α 5</cell><cell>α 6</cell><cell>α 7</cell><cell>α 8</cell><cell>α 9</cell><cell>α 10</cell></row><row><cell>Def. simulation</cell><cell>0.05</cell><cell>0.53</cell><cell>0.48</cell><cell>0.62</cell><cell>0.33</cell><cell>0.41</cell><cell>0.10</cell><cell>0.4</cell><cell>0.74</cell><cell>0.32</cell></row><row><cell>Avg worker est.</cell><cell>0.12</cell><cell>0.51</cell><cell>0.43</cell><cell>0.45</cell><cell>0.44</cell><cell>0.36</cell><cell>0.27</cell><cell>0.56</cell><cell>0.63</cell><cell>0.44</cell></row><row><cell>Expert est.</cell><cell>0.1</cell><cell>0.5</cell><cell>0.4</cell><cell>0.6</cell><cell>0.4</cell><cell>0.2</cell><cell>0.3</cell><cell>0.5</cell><cell>0.9</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Comparisons of accuracies, costs, and net utilities of various agents when run on Mechanical Turk.</figDesc><table><row><cell></cell><cell>AgentHunt</cell><cell>TurKontrol</cell><cell>TurKontrol 300</cell></row><row><cell>Avg accuracy (%)</cell><cell>92.45</cell><cell>85.85</cell><cell>84.91</cell></row><row><cell>Avg cost</cell><cell>5.81</cell><cell>4.21</cell><cell>6.26</cell></row><row><cell>Avg net utility</cell><cell>-13.36</cell><cell>-18.35</cell><cell>-21.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Comparisons of accuracies, costs, and net utilities of various agents when run on Mechanical Turk.</figDesc><table><row><cell></cell><cell>AgentHunt</cell><cell>AgentHunt RL</cell></row><row><cell>Avg accuracy (%)</cell><cell>92.45</cell><cell>93.40</cell></row><row><cell>Avg cost</cell><cell>5.81</cell><cell>7.25</cell></row><row><cell>Avg net utility</cell><cell>-13.36</cell><cell>-13.85</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://mturk.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>http://liveops.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://topcoder.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://galaxyzoo.org, Audubon Christmas Bird Count, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>A terminal state is a state whose only positive-probability transition is to itself.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>http://www.cs.cmu.edu/~trey/zmdp/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>http://ab-initio.mit.edu/wiki/index.php/NLopt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>We intentionally choose these two cases because they are the most favorable cases for TurKontrol(2), as we will find in the following subsections.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8"><p>The consistency of this type of subjective rating has been carefully evaluated in the literature; see e.g.<ref type="bibr" target="#b10">[11]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9"><p>While this is obviously an approximation, we find it is surprisingly close; R 2 = 0.82 for the worker-independent model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10"><p>We use Newton's method with 1000 random restarts. Initial values are chosen uniformly from the real interval (0, 100.0).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_11"><p>http://www.cs.cmu.edu/~trey/zmdp/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_12"><p>www.freebase.com.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to the anonymous reviewers and Jonathan Bragg for helpful comments on this manuscript. Conversations with Lydian Chilton and Greg Little informed our thoughts on crowdsourcing. This work was supported by the WRF/TJ Cable Professorship, Office of Naval Research grants N00014-12-1-0211 and N00014-06-1-0147, and National Science Foundation grants IIS 1016713 and IIS 1016465.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Snapshots of tasks Fig. <ref type="figure">22</ref>. Snapshot of an improvement HIT. 19 Software packages of our implementations are available for general use at http://cs.washington.edu/node/7714.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UCT for tactical assault planning in real-time strategy games</title>
		<author>
			<persName><forename type="first">Radha-Krishna</forename><surname>Balla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="40" to="45" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m">Dynamic Programming</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Karger</surname></persName>
		</author>
		<title level="m">Crowds in two seconds: Enabling realtime crowd-powered interfaces</title>
		<imprint>
			<publisher>UIST</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Soylent: A word processor with a crowd inside</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Crowell</surname></persName>
		</author>
		<author>
			<persName><surname>Panovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>UIST</publisher>
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Crowell</surname></persName>
		</author>
		<author>
			<persName><surname>Panovich</surname></persName>
		</author>
		<title level="m">Soylent: A word processor with a crowd inside</title>
		<imprint>
			<publisher>UIST</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dynamic Programming and Optimal Control</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
			<pubPlace>Athena Scientific</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vizwiz: Nearly real-time answers to visual questions</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandrika</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aubrey</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandyn</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Yeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>UIST</publisher>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Lozano-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Continuous-state POMDPs with hybrid dynamics</title>
		<title level="s">Symposium on Artificial Intelligence and Mathematics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Making rational decisions using adaptive utility elicitation</title>
		<author>
			<persName><forename type="first">Urszula</forename><surname>Chajewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foldit players, Predicting protein structures with a multiplayer online game</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firas</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janos</forename><surname>Barbero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeehyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Beene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="page" from="756" to="760" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is seeing believing?: How recommender system interfaces affect users&apos; opinions</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Cosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shyong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Istvan</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decision-theoretic control of crowd-sourced workflows</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Artificial intelligence for artificial intelligence</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficiently learning the accuracy of labeling sources for selective sampling</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic framework to learn from multiple annotators with time-varying accuracy</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="826" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo Methods in Practice</title>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploration exploitation in go: UCT for Monte Carlo go</title>
		<author>
			<persName><forename type="first">Yizao</forename><surname>Sylvain Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS On-line Trading of Exploration and Exploitation Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Error identification and correction in human computation: Lessons from the WPA</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Grier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HCOMP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Online task assignment in crowdsourcing markets</title>
		<author>
			<persName><forename type="first">Chien-Ju</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crowd control</title>
		<author>
			<persName><forename type="first">Leah</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="17" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward automatic task design: A progress report</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quality management on Amazon Mechanical Turk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foster</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spam worker filtering and featured-voting based consensus accuracy improvement</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CrowdConf</title>
		<meeting>of CrowdConf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A convex formulation for learning from crowds</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kajino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining human and machine intelligence in large-scale crowdsourcing</title>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Severin</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAMAS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Budget-optimal crowdsourcing using low-rank matrix approximations</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Allerton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bandit based Monte Carlo planning</title>
		<author>
			<persName><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ECML</publisher>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaboratively crowdsourcing workflows with Turkomatic</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CSCW</title>
		<meeting>CSCW</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bjoern Hartmann Mobileworks, Designing for quality in a managed crowdsourcing architecture</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Gutheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prayag</forename><surname>Narula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapan</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>HCOMP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time crowd control of existing interfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">I</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST</title>
		<meeting>UIST</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Edith</forename><surname>Law</surname></persName>
		</author>
		<title level="m">Luis von Ahn, Human Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An iterative dual pathway structure for speech-to-text transcription</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Liem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>HCOMP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crowdsourcing control: Moving beyond multiple choice</title>
		<author>
			<persName><forename type="first">H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Lin Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dynamically switching between synergistic workflows for crowdsourcing</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TurKit: Tools for iterative tasks on Mechanical Turk</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Workshop on Human Computation</title>
		<imprint>
			<biblScope unit="page" from="29" to="30" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the undecidability of probabilistic planning and related stochastic optimization problems</title>
		<author>
			<persName><forename type="first">Omid</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Condon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="34" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Planning with continuous resources in stochastic domains</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Benazera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1244" to="1251" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Planning with Markov Decision Processes: An AI Perspective</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Kolobov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to link with Wikipedia</title>
		<author>
			<persName><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Information and Knowledge Management</title>
		<meeting>the ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Platemate: Crowdsourcing nutrition analysis from food photographs</title>
		<author>
			<persName><forename type="first">John</forename><surname>Noronha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>UIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Crowdscreen: Algorithms for filtering data with humans</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neoklis</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anytime point-based approximations for large POMDPs</title>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="335" to="380" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Point-based value iteration for continuous POMDPs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2329" to="2367" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Valadez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Culture as consensus: A theory of culture and informant accuracy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Kimball</forename><surname>Romney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Batchelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Anthropol</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="338" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Joel</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilly</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Six</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Tomlinson</surname></persName>
		</author>
		<title level="m">Who are the crowdworkers? Shifting demographics in Mechanical Turk</title>
		<imprint>
			<publisher>CHI</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coastal navigation with mobile robots</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1043" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Praveen Paratosh, Peng Dai, And now for something completely different: Introducing micro-breaks into crowdsourcing workflows</title>
		<author>
			<persName><forename type="first">Jeffery</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Submission</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Introduction to the CoNNL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNNL</title>
		<meeting>CoNNL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Generalized markets for human and machine computation</title>
		<author>
			<persName><forename type="first">Dafna</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Solomon Eyal Shimony, Prioritizing point-based POMDP solvers</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern., Part B</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1592" to="1605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Cybern.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Get another label? Improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foster</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monte Carlo planning in large POMDPs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2164" to="2172" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic</title>
		<author>
			<persName><forename type="first">Trey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">G</forename><surname>Simmons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cheap and fast -but is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>EMNLP</publisher>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The optimal control of partially observable Markov processes</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Sondik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Utility data annotation with Amazon Mechanical Turk</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Perseus: Randomized point-based value iteration for POMDPs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="195" to="220" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinforcement</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Games with a purpose</title>
		<author>
			<persName><surname>Luis Von Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Mag</title>
		<imprint>
			<biblScope unit="page" from="96" to="98" />
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bayesian bias mitigation for crowdsourcing</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">L</forename><surname>Wauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Human intelligence needs artificial intelligence</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HCOMP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Movellan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Active learning from crowds</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
