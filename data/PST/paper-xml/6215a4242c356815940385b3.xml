<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Early-Bird GCNs: Graph-Network Co-Optimization Towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets</title>
				<funder ref="#_t4hwJak">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-16">16 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoran</forename><surname>You</surname></persName>
							<email>haoran.you@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijian</forename><surname>Zhou</surname></persName>
							<email>zjzhou@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
							<email>yingyan.lin@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Early-Bird GCNs: Graph-Network Co-Optimization Towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-16">16 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.00794v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. However, it remains notoriously challenging to train and inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because as the graph size grows, the sheer number of node features and the large adjacency matrix can easily explode the required memory and data movements. To tackle the aforementioned challenges, we explore the possibility of drawing lottery tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency matrix yet are capable of achieving accuracy comparable to or even better than their full graphs. Specifically, we for the first time discover the existence of graph early-bird (GEB) tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of such GEB tickets. Furthermore, we advocate graph-model co-optimization and develop a generic efficient GCN early-bird training framework dubbed GEBT that can significantly boost the efficiency of GCN training by (1) drawing joint early-bird tickets between the GCN graphs and models and (2) enabling simultaneously sparsification of both the GCN graphs and models. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2% ? 85.6% and 84.6% ? 87.5% savings of GCN training and inference costs while offering a comparable or even better accuracy as compared to state-of-the-art methods. Our source code and supplementary material are available at https://github.com/RICE-EIC/Early-Bird-GCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b12">(Kipf and Welling 2016)</ref> have emerged as state-of-the-art (SOTA) algorithms for graph-based learning tasks, such as graph classification <ref type="bibr" target="#b22">(Xu et al. 2018</ref>) and node classification <ref type="bibr" target="#b12">(Kipf and Welling 2016)</ref>. It is well recognized that the superior performance largely benefits from GCNs' ability for handling irregularity and unrestricted neighborhood connections. Specifically, for each node in a graph, GCNs first aggregate neighbor nodes' features, and then transform the aggregated feature through (hierarchical) feed-forward propagation to update the feature of the given node.</p><p>Despite their promise, GCN training and inference can be notoriously challenging, hindering their great potential from being unfolded in large real-world graphs. This is because as the graph dataset grows, the large number of node features and the abundant adjacency matrix can easily explode the required memory and data movements <ref type="bibr" target="#b28">(Geng et al. 2021;</ref><ref type="bibr" target="#b28">Zhang et al. 2021)</ref>. For example, a mere 2-layer GCN model with 32 hidden units requires 19 GFLOPs (FLOPs: floating point operations) to process the Reddit graph (Tailor, Fernandez-Marques, and Lane 2020), twice as much as that of a powerful deep neural network (DNN) ResNet50, which has a total of 8 GFLOPs when processing ImageNet <ref type="bibr" target="#b0">(Canziani, Paszke, and Culurciello 2016)</ref>. The giant computational cost of GCNs comes from three aspects. First, graphs (or graph data), especially real-world ones, are often extraordinarily large and irregular as exacerbated by their intertwined complex neighbor connections, e.g., a total of 232,965 nodes in the Reddit graph with each node having about 50 neighbors <ref type="bibr" target="#b11">(Kersting et al. 2016)</ref>. Second, the dimension of GCNs' node feature vectors can be very high, e.g., each node in the Citeseer graph has 3703 features. Third, the extremely high sparsity and unbalanced distribution of non-zero data in GCNs' adjacency matrices imposes a paramount challenge for effectively accelerating GCNs <ref type="bibr" target="#b6">(Geng et al. 2020;</ref><ref type="bibr" target="#b24">Yan et al. 2020</ref>), e.g., as high as 99.9% vs. 10% to 50% generally observed in DNNs.</p><p>To tackle the aforementioned challenges and unleash the full potential of GCNs, various techniques have been developed. For instance, Tailor et al. <ref type="bibr">(Tailor, Fernandez-Marques, and</ref> Lane 2020) leverages quantization-aware training to demonstrate 8-bit GCNs; SGCN <ref type="bibr">(Li et al. 2020b</ref>) is the first to consider GCN sparsification by formulating and solving it as an optimization problem.</p><p>The impressive performance achieved by existing GCN compression works indicates that there are redundancies within GCNs to be leveraged for aggressively trimming down their complexity while maintaining their performance. In this work, we attempt to take a new perspective by drawing inspiration from the tremendous success of DNN compression, particularly the lottery ticket (LT) finding <ref type="bibr" target="#b4">(Frankle and Carbin 2019;</ref><ref type="bibr" target="#b15">Liu et al. 2018;</ref><ref type="bibr" target="#b26">You et al. 2020)</ref>. While conceptually simple, the unique structures of GCNs make it not straightforward to leverage the LT finding to compress GCNs. This is because (1) the graph instead of the MLPs in GCNs dominates the complexity, for which the existence of LT remains unknown; and (2) it is unclear how to jointly optimize the two phases of GCN operations (i.e., feature aggregation and combination) while doing so promises the maximum complexity reduction.</p><p>This paper aims to close the above gap to minimize the complexity of GCNs without hurting their competitive performance, and to make the following contributions:</p><p>? We discover the existence of graph early-bird <ref type="bibr">(GEB)</ref> tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of GEB tickets. To our best knowledge, we are the first to show that the early-bird tickets finding holds for GCN graphs. ? we advocate graph-network co-optimization and develop a generic efficient GCN training framework dubbed GEBT that significantly boosts GCN training efficiency by (1) drawing joint early-bird (EB) tickets between the GCN graphs and models and (2) simultaneously sparsifying both the GCN graphs and models, additionally boosting the GCN inference efficiency. ? Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of the proposed GEBT. For example, our GEBT achieves up to 80.2% ? 85.6% and 84.6% ? 87.5% GCN training and inference costs savings while leading to a comparable or even better accuracy as compared to state-of-theart (SOTA) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Graph Convolutional Networks (GCNs). GCNs have amazed us for processing non-Euclidean and irregular data structures <ref type="bibr">(Zhang et al. 2018)</ref>. Recently developed GCNs can be categorized into two groups: spectral and spatial methods. Specifically, spectral methods <ref type="bibr" target="#b13">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b16">Peng et al. 2020</ref>) model the representation in the graph Fourier transform domain based on eigen-decomposition, which are time-consuming and usually handle the whole graph simultaneously making it difficult to parallel or scale to large graphs <ref type="bibr" target="#b5">(Gao et al. 2019;</ref><ref type="bibr" target="#b21">Wu et al. 2020)</ref>. On the other hand, spatial approaches <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b18">Simonovsky and Komodakis 2017)</ref>, which directly perform the convolution in the graph domain by aggregating the neighbor nodes' information, have rapidly developed recently. To further improve the performance of spatial GCNs, Veli?kovi? et al. <ref type="bibr" target="#b20">(Veli?kovi? et al. 2018)</ref> introduce the attention mechanism to select information which is relatively critical from all inputs; Zeng et al. <ref type="bibr" target="#b27">(Zeng et al. 2019)</ref> propose mini-batch training to improve GCNs' scalability of handling large graphs; and <ref type="bibr" target="#b23">(Xu et al. 2019)</ref> theoretically formalizes an upper bound for the expressiveness of GCNs. Our GEB finding and GEBT enhance the understanding of GCNs and promote efficient GCN training, and can be generally applicable to SOTA GCN models. GCN Compression. The prohibitive complexity and powerful performance of GCNs have motivated growing in-terest in GCN compression. For instance, <ref type="bibr" target="#b19">Tailor et al. (Tailor, Fernandez-Marques, and Lane 2020)</ref> for the first time show the feasibility of adopting 8-bit integer arithmetic representation for GCN inference without sacrificing the classification accuracy; two concurrent pruning works <ref type="bibr">(Li et al. 2020b;</ref><ref type="bibr" target="#b28">Zheng et al. 2020)</ref> aim to sparsify the graph adjacency matrices; and Ying et al. <ref type="bibr" target="#b25">(Ying et al. 2018</ref>) propose a DiffPool layer to reduce the size of GCN graphs by clustering similar nodes during training and inference. Our GEBT explores from a new perspective and is complementary with exiting GCN compression works, i.e., can be applied on top of them to further reduce GCNs' training/inference costs.</p><p>Early-Bird Tickets Hypothesis. <ref type="bibr" target="#b4">Frankle et al. (Frankle and Carbin 2019)</ref> show that winning tickets (i.e., small subnetworks) exist in randomly initialized dense networks, which can be retrained to restore a comparable or even better performance than their dense network counterparts. This finding has attracted lots of research attentions as it implies the potential of training a much smaller network to reach the accuracy of a dense, much larger network without going through the time and cost consuming pipeline of fully training the dense network, pruning and then retraining it to restore the accuracy. Later, You et al. <ref type="bibr" target="#b26">(You et al. 2020</ref>) demonstrate the existence of EB tickets, i.e., the winning tickets can be consistently drawn at the very early training stages across different models and datasets, and leverages this to largely reduce the training costs of DNNs. More recently, the EB finding has been extended to natural language processing (NLP) models (e.g., BERT) <ref type="bibr">(Chen et al. 2021b</ref>) and generative adversarial networks (GANs) (Mukund Kalibhat, Balaji, and Feizi 2020). Our GEB finding and GEBT draw inspirations from the prior arts, and for the first time demonstrate that the EB phenomenon holds for GCNs which have unique and different algorithm structures as compared to DNNs, NLP, and GANs. Furthermore, compared with the iterative pruning method, e.g., UGS <ref type="bibr">(Chen et al. 2021a</ref>), we for the first time show that early-bird (EB) tickets exist in both GCN graphs and networks, and further develop efficient and effective detectors to automatically identify them, boosting both training and inference efficiency, while UGS draws lottery tickets after fully and iteratively (up to 20?) training the dense models for only saving inference costs.</p><p>3 Our Findings and Proposed Techniques </p><formula xml:id="formula_0">d = {d 1 , d 2 , ? ? ? , d N }</formula><p>where d i indicates the number of neighbors connected to the node v i . We define D as the degree matrix whose diagonal elements are formed using d. Given the adjacency matrix A and the feature matrix <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref> can then be formulated as:</p><formula xml:id="formula_1">X = {x 1 , x 2 , ? ? ? , x N } of the graph G, a two-layer GCN model</formula><formula xml:id="formula_2">Z = f (A, X) = softmax ? ReLU ?XW 0 W 1 ,<label>(1)</label></formula><p>Epoch subgraphs drawn from where</p><formula xml:id="formula_3">? = D -1 2 (A + I n )D -1</formula><p>2 is calculated by a preprocessing step, thus multiplying ? captures GCNs' neighbor aggregation; W 0 and W 1 are the weights of the GCN model for the 1st and 2nd layers, e.g., W 0 is an input-tohidden weight matrix for a hidden layer with H feature maps and W 1 is a hidden-to-output weight matrix with F feature maps (i.e., Z ? R N ?F ), where the mapping from the input to the hidden or output layer is called GCN combination which combines each node's features and its neighbors; The softmax function softmax</p><formula xml:id="formula_4">(x i ) = exp(x i )/ i exp(x i )</formula><p>is applied in a row-wise manner <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>. For semi-supervised multiclass classification, the loss function of the cross-entropy errors over all labeled examples:</p><formula xml:id="formula_5">L GCN (W ) = - n?Y N f Y nf ln(Z nf ),<label>(2)</label></formula><p>where Y N is the set of node indices that have labels, Y nf and Z nf are the ground truth label matrix and the GCN output predictions, respectively. During GCN training, W 0 and W 1 are updated. via gradient descents.</p><p>Graph Sparsification. The goal of graph sparsification is to reduce the total number of edges in GCNs' graph (i.e., the size of the adjacency matrices). A SOTA graph sparsification pipeline <ref type="bibr">(Li et al. 2020b</ref>) is to first pretrain GCNs on their full graphs, and then sparsify the graphs based on the pretrained GCNs. The weights of GCNs are not updated during graph sparsification, during which W is replaced with A in Eq. ( <ref type="formula" target="#formula_5">2</ref>) to derive the loss function L GCN (A). The overall loss function during graph sparsification can be written as:</p><formula xml:id="formula_6">L Graph (A) = L GCN (A) + L Reg (A),<label>(3)</label></formula><p>where L Reg denotes the sparse regularization term, which ideally will become zero if the sparsity of the graph adjacency matrices reaches the specified pruning ratio (e.g., Aprune 0 / A 0 ? 1 -p for a given ratio of p). As L Reg is not differentiable, SOTA graph sparsification work <ref type="bibr">(Li et al. 2020b)</ref> formulates Eq. ( <ref type="formula" target="#formula_6">3</ref>) as an alternating optimization problem for updating the graph adjacency matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding 1: EB Tickets Exist in GCN Graphs</head><p>In this subsection, we first conduct an extensive set of experiments to show that GEB tickets can be observed across popular graph datasets, and then propose a simple yet effective method to detect the emergence of GEB tickets.</p><p>Experiment Settings. For this set of experiments, we follow the SOTA graph sparsification work <ref type="bibr">(Li et al. 2020b</ref>) to first pretrain GCNs on unpruned graphs, train and prune the graphs based on the pretrained GCNs, and then retrain GCNs from scratch on the pruned graphs to evaluate the achieved accuracy. In addition, we adopt a two-layer GCN as described in Eq. ( <ref type="formula" target="#formula_2">1</ref>), in which both the GCN and graph training take a total of 100 epochs and an Adam solver is used with a learning rate of 0.01 and 0.001 for training the GCNs and graphs, respectively. For retraining the pruned graphs, we keep the same setting by default.</p><p>Existence of GEB Tickets. We follow the SOTA method <ref type="bibr">(Li et al. 2020b</ref>) to sparsify the graph, but instead prune the graph that have not been fully trained (before the accuracy reaches their final top values), to see if reliable GEB tickets can be observed, i.e., the retraining accuracy reaches the one drawn from the corresponding fully-trained graph. Fig. <ref type="figure" target="#fig_0">1</ref> shows the accuracies achieved by re-training the pruned graphs drawn from different early epochs, considering three different graph datasets and six pruning ratios. Two intriguing observations can be made: (1) there consistently exist GEB tickets drawn from certain early epochs (e.g., as early as 10 epochs w.r.t. the total of 100 epochs), of which the retraining accuracy is comparable or even better than those drawn in a later stage, including the "ground-truth" tickets drawn from the fully-trained graphs (i.e., at the 100th epoch); and (2) some GEB tickets (e.g., P g = 30% on Pumbed) can even outperform their unpruned graphs (denoted using dashed lines), potentially thanks to the sparse regularization as mentioned in <ref type="bibr" target="#b26">(You et al. 2020)</ref>. The first observation implies the possibility of "overcooking" when identifying important graph edges at later training stages.</p><p>Detection of GEB Tickets. The existence of GEB tickets and the prohibitive cost of GCN training motivate us to explore the possibility of automatically detecting the emergence of GEB tickets. To do so, we develop a simple yet effective detector via measuring the "graph distance" between consecutive epochs during graph sparsification. Specifically, we define a binary mask of the drawn GEB tickets (i.e., pruned graphs), where 1 denotes the reserved edges and 0 denotes the pruned edges, and use the hamming distance be- tween the corresponding masks to measure the "distance" between two graphs. Fig. <ref type="figure" target="#fig_2">2</ref> (a) visualizes the pairwise "graph distance" matrices among 100 training epochs, where the (i, j)-th element within the matrices represents the distance between the pruned graphs drawn at the i-th and j-th epochs. We see that the distance deceases rapidly (i.e., color change from green to yellow) at the first few epochs, indicating that the reserved edges in pruned graphs quickly converge at the very early training stages. We therefore measure and record the distance between consecutive three epochs (i.e., look back for three epochs during training), and stop training the graph when all the recorded distances are smaller than a specified threshold ?. Fig. <ref type="figure" target="#fig_2">2</ref> (b) plots the maximum recorded distances as graph training epochs increase, where the red line denotes the threshold we adopt in all experiments with different pruning ratios. The identified GEB tickets are consistently drawn from the early (10-? 26-th) epochs. These experiments validate the effectiveness of our developed GEB detector, which has negligible overheads compared with the total graph training cost (i.e., &lt; 0.1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finding 2: Joint-EB Tickets Exist</head><p>In this subsection, we first develop a co-sparsification framework to prune the GCN graphs and networks, and then show in a set of extensive experiments that joint-EB tickets exist across various models and datasets, and then propose a simple detector to detect the emergence of joint-EB tickets during co-sparsification of the GCN graphs and networks.</p><p>Co-sparsification of the GCN Graph and Network. To explore the possibility of drawing joint-EB tickects between GCN graphs and networks, we first develop a cosparsification framework, as described in Fig. <ref type="figure" target="#fig_5">5 (c</ref>) and Algorithm 2. Specifically, we iteratively update the GCN weights and graph adjacency matrices based on their corresponding loss functions formulated in Eq. ( <ref type="formula" target="#formula_5">2</ref>) and Eq. (3), respectively; after training for a certain epochs (e.g., 100 epochs), we then simultaneously prune the trained GCN graphs and networks using a magnitude-based pruning method <ref type="bibr" target="#b9">(Han, Mao, and Dally 2015;</ref><ref type="bibr" target="#b4">Frankle and Carbin 2019)</ref>, and finally retrain the resulting pruned GCNs on the pruned graphs. Fig. <ref type="figure" target="#fig_3">3</ref> shows the accuracy-FLOPS tradeoffs of our co-sparsification framework when evaluating GCNs (Kipf and Welling 2017) on Cora and CiteSeer graph datasets. We can see that co-sparsification can achieve up to 90% sparsity in GCN weights while maintaining a comparable accuracy over the unpruned GCN graphs/networks. Existence of Joint-EB Tickets. The existence of GEB tickets in GCN graphs and EB tickets in DNNs motivate our curiosity on the existence of joint-EB tickets between GCN graphs and networks. Fig. <ref type="figure" target="#fig_4">4</ref> (a) visualizes the retraining accuracies of the GCN subnetworks on subgraphs with both being drawn from different early epochs, which consistently indicates the existence of joint-EB tickets under an extensive set of experiments with different graph datasets, graph pruning ratios, and weight pruning ratios {G, p g , p w }. Furthermore, we can see that the joint-EB tickets emerge at the very early training stages (as early as 10 epochs w.r.t. a total of 100 epochs), i.e., their retraining accuracy is comparable or even better than that of training the corresponding unpruned GCN graphs and networks or training the pruned graphs and unpruned GCN networks <ref type="bibr">(Li et al. 2020b)</ref>.</p><p>Detection of Joint-EB Tickets. We also develop a simple method to automatically detect the emergence of joint-EB tickets, of which the main idea is similar to the GEB tickets detector but with an additional binary mask for drawing the GCN subnetwork. Similarly, in the binary masks, the pruned weights are set to 0 while the kept ones are set to 1, and the distance between subnetworks is characterize using the hamming distance between the corresponding binary masks following <ref type="bibr" target="#b26">(You et al. 2020</ref>) but we additionally define a binary mask of the drawn GCN subnetwork, where the pruned weights are 0 while the kept ones are 1. Therefore the distance between subnetworks is represented by the hamming distance between the corresponding binary masks following <ref type="bibr" target="#b26">(You et al. 2020)</ref>. For detecting the joint-EB tickets, we measure both the "subgraph distance" d g and "sub- </p><formula xml:id="formula_7">d g &lt; ?; (2) d w &lt; ?; (3) d g + d w &lt; ?.</formula><p>Fig. <ref type="figure" target="#fig_4">4</ref> (b) leverages the third criterion to visualize the distance's trajectories of GCN networks on Cora and CiteSeer datasets, at different graph and network pruning ratio pairs {p g , p w }. The ablation studies of all of the three criteria can be found in the Appendix. We can see that all criteria can effectively identify the emergence of joint-EB tickets, e.g., as early as 9 epochs w.r.t. a total of 100 epochs. Interestingly, the drawn joint-EB tickets can achieve a comparable or even better retraining accuracy than the subgraph and subnetwork pairs drawn at a later stages, which again implies the possibility of "over-cooking" as in the case of DNNs discussed in <ref type="bibr" target="#b26">(You et al. 2020)</ref>. All results in this set of experiments consistently validate the existence of joint-EB tickets and the effectiveness of our joint-EB ticket detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Proposed GEBT:Efficient Training+Inference</head><p>In this subsection, we present our proposed GEBT technique, which aims to leverage the existence of both GEB tickets and joint-EB tickets to develop a generic GCN efficient training framework. Note that GEBT achieves "winwin": both efficient training and inference as the resulting trained GCN graphs and networks are naturally efficient. Here we will first describe the GEBT technique and then provide a complexity analysis to show GEBT's advantages.</p><p>GEBT via GEB Tickets. Fig. <ref type="figure" target="#fig_5">5</ref> (b) illustrates the overall pipeline of the proposed GEBT via drawing GEB tickets. Specifically, GEBT via drawing GEB tickets involves three steps: pretrain GCNs on the full graphs, train and sparsify the graph for identifying GEB tickets, and then retrain the GCN networks on the GEB tickets. The GEB ticket detection scheme is described in Algorithm 1. Specifically, we use a magnitude-based pruning method <ref type="bibr" target="#b9">(Han, Mao, and Dally 2015)</ref> to derive the graph mask (i.e., m) for calculat-ing the graph distance between subgraphs from consecutive epochs and then store them into a first-in-first-out (FIFO) queue with a length of l = 3; The GEBT training will stop when the maximum graph distance is smaller than a specified threshold ? which is set to 0.1 in all our experiments, and return the GEB tickets (i.e., A p ) to be retrained.</p><p>GEBT via joint-EB Tickets. Fig. <ref type="figure" target="#fig_5">5</ref> (c) shows the overall pipeline of the proposed GEBT technique via drawing joint-EB tickets. While SOTA efficient GCN training methods consist of three steps: (1) fully pretrain the GCN networks on the full graphs, (2) train and prune the graphs based on pretrained GCNs, and (3) retrain the GCN networks on pruned graph from scratch. Accordingly, here GEBT via drawing joint-EB tickets only has two steps, it first follows the co-sparsification framework as described in previous sections to prune and derive the GCN subgraph and subnetwork pairs, and then retrain the subnetwork on the drawn subgraph to restore accuracies. The joint-EB tickets detection scheme is described in Algorithm 2, where a FIFO queue is adopted for recording both the distance of subgraphs d g and subnetworks d w between consecutive epochs. GEBT training will stop when d g + d w is smaller than a predefined threshold ? = 0.1, and return the detected joint-EB tickets (i.e., A p and W p ) for further retraining. Note that the initialization for retraining inherits from joint-EB tickets.</p><p>Complexity Analysis of GEBT vs. SOTA Methods. Here we provide complexity analysis to quantify the advantages of our GEBT technique. The time and memory complexity of GCN inferences can be captured by O(LM F + LN F 2 ) and O(LN F + LF 2 ), respectively, where L, N, M and F are the total number of GCN layers, nodes, edges, and features, respectively. <ref type="bibr" target="#b3">(Chiang et al. 2019)</ref>. Assuming that drawing joint-EB tickets leads to p g and p w sparsities in GCN graphs and networks, respectively, then the inference time and memory complexity of GCNs resulting from our GEBT is O((1 -p g )LM F + (1 -p w )LN F 2 ) and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>Models and Datasets. We evaluate the proposed methods over five representative GCN algorithms, i.e., GCN <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, GAT <ref type="bibr" target="#b20">(Veli?kovi? et al. 2018)</ref>, GIN <ref type="bibr" target="#b23">(Xu et al. 2019)</ref>, GraphSAGE <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017)</ref>, and 7/14/28-layer deep ResGCNs <ref type="bibr">(Li et al. 2020a)</ref>, on three citation graph datasets, i.e., Cora, CiteSeer, and Pubmed <ref type="bibr" target="#b17">(Sen et al. 2008</ref>), two inductive datasets, i.e., PPI and Reddit <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017)</ref>, and two large-scale graph datasets from Open Graph Benchmark (OGB) <ref type="bibr" target="#b10">(Hu et al. 2020)</ref>, i.e., Ogbn-ArXiv for node classification and Ogbl-Collab for link prediction. The statistics of these seven datasets are summarized in Tab. 1.</p><p>Training Settings. We follow <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref> to train all the chosen two-layer GCN models on the three citation graph datasets and two inductive graph datasets, and follow <ref type="bibr">(Li et al. 2020a)</ref> to train ResGCNs on OGB graphs. The detailed training settings are elaborated in the Appendix.</p><p>Baselines and Evaluation Metrics. We evaluate the effectiveness of the proposed GEBT's improved training and inference efficiency in terms of the node classification ac-  <ref type="formula" target="#formula_6">3</ref>) Derive graph mask m t and network mask n t based on A, W and pruning ratio p g , p w Calculate the distance d g between m t and m t-1 , d w between n t and n t-1 , and add</p><formula xml:id="formula_8">d g + d w to Q if Max(Q) &lt; ? then t EB = t</formula><p>Return A p = m t A; W p = n t W end end curacy (or F1 Score, Hits@50), inference FLOPs, and total training FLOPs, as compared to other graph sparsifiers, i.e., random pruning <ref type="bibr" target="#b4">(Frankle and Carbin 2019)</ref> and SGCN <ref type="bibr">(Li et al. 2020b)</ref>, and ten standard SOTA GCN algorithms using unpruned graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GEBT over SOTA Sparsifiers</head><p>We compare the proposed GEBT with existing SOTA GCN sparsification pipelines <ref type="bibr">(Li et al. 2020b</ref>) on the three citation graphs to evaluate the effectiveness of GEBT. Fig. <ref type="figure">6</ref> shows that GEBT consistently outperforms all competitors in terms of measured accuracies and computational costs (i.e., training and inference FLOPs) trade-offs. Specifically, GEBT via GEB tickets achieves 24.7%?32.1% training FLOPs reduc-  tion while offering comparable accuracies (?1.4%??4.9%) across a wide range of graph pruning ratios, as compared to SGCN. Furthermore, GEBT via joint-EB tickets even aggressively reaches 80.2%?85.6% and 84.6%?87.5% reduction in training FLOPs and inference FLOPs, respectively, over SGCN when pruning the GCN networks up to 90% sparsity, meanwhile leading to a comparable accuracy range (?1.3%??1.4%). This set of experiments verify (1) the efficiency benefits of the GEBT framework and (2) the highquality of the drawn GEB tickets and joint-EB tickets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GEBT over SOTA GCNs</head><p>To evaluate the benefits of GEBT, we first compare the performance of GEBT over four SOTA GCN algorithms on three citation graphs. As shown in Tab. 2, GEBT consistently outperforms all the baselines in terms of efficiency-accuracy trade-offs. Specifically, GEBT achieves 2.6? ? 10? inference FLOPs reduction, while offering a comparable accuracy (?0.9% ? ?2.0%), as compared to SOTA GCN algo- rithms. We further evaluate GEBT with eight SOTA methods on two large datasets, PPI and Reddit, and show the comparisons in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>, respectively, where (?) and (?) denote improvement over the original models, and "Overall Improv." denotes the best improvement over all SOTA baselines. GEBT again consistently achieves the best efficiencyaccuracy trade-offs, e.g., reducing inference FLOPs (up to 84.1%) and training FLOPs (up to 83.5%) under comparable or even higher F1-micro scores (?0.1% ? ?38%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>GCNs have gained increasing attention thanks to their SOTA performance on graphs. However, the notorious challenge of GCN training and inference limits their application to large real-world graphs and hinders the exploration of deeper and more sophisticated GCN graphs. To this end, we advocate graph-network co-optimization and explore the possibility of drawing early-bird tickets when sparsifying GCN graphs. Specifically, we for the first time discover the existence of GEB tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify their emergence. Furthermore, we develop a generic efficient GCN training framework dubbed GEBT that can significantly boost the efficiency of GCN training and inference by enabling cosparsification and drawing joint-EB of GCNs. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. 1</head><label>1</label><figDesc>Preliminaries of GCNs and GCN SparsificationGCN Notation and Formulation. Let G = (V, E) represents a GCN graph, where v i ? V and (v i , v j ) ? E denote the nodes and edges, respectively; and N = |V | and M = |E| denote the total number of nodes and edges, respectively. The node degrees are denoted as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retraining accuracy vs. epoch numbers at which subgraphs are drawn, when evaluating the GCNs (Kipf and Welling 2017) on three graph datasets: Cora, Citeseer, and Pumbed, where dashed lines show the accuracy of GCNs on corresponding unpruned full graphs, p g denotes the graph pruning ratios, and error bars show the minimum and maximum of ten runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The visualization of (a) pairwise graph distance matrices, and (b) recorded graph distance's evolution along the training trajectories under different graph pruning ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Retraining accuracy vs. inference FLOPs of our co-sparsification framework and a SOTA graph sparsification framework, SGCN (Li et al. 2020b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: (a) Retraining accuracy vs. epoch numbers at which both the subgraphs and subnetworks (i.e., joint-EB tickets) are drawn, for GCN networks<ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref> on Cora and CiteSeer datasets, where p g indicates the graph pruning ratio and p w denotes the network pruning ratio, and (b) the distance's evolution along the training trajectories under different graph and network pruning ratio pairs. network distance" d w among consecutive epochs, resulting in three choices for the stop criteria (for a given the threshold ?): (1)d g &lt; ?; (2) d w &lt; ?; (3) d g + d w &lt; ?.Fig.4(b) leverages the third criterion to visualize the distance's trajectories of GCN networks on Cora and CiteSeer datasets, at different graph and network pruning ratio pairs {p g , p w }. The ablation studies of all of the three criteria can be found in the Appendix. We can see that all criteria can effectively identify the emergence of joint-EB tickets, e.g., as early as 9 epochs w.r.t. a total of 100 epochs. Interestingly, the drawn joint-EB tickets can achieve a comparable or even better retraining accuracy than the subgraph and subnetwork pairs drawn at a later stages, which again implies the possibility of "over-cooking" as in the case of DNNs discussed in<ref type="bibr" target="#b26">(You et al. 2020)</ref>. All results in this set of experiments consistently validate the existence of joint-EB tickets and the effectiveness of our joint-EB ticket detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An overview of the existing efficient GCN training pipeline and our GEBT training schemes via drawing GEB tickets and joint-EB tickets (red circle denotes the training process).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 :</head><label>2</label><figDesc>Joint-EB Tickets Identification Input: Graph G = {V, E, A, X}, graph and weight pruning ratio p g and p w , and a FIFO queqe Q with lenght l Output: The pruned adjacency matrix A p and the pruned GCN weights W p Initialize the GCN weights W while t (epoch) &lt; t max do GCN forward based on Eq. (1) Update W based on the L GCN in Eq. (2) Update A based on the L Graph in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the adopted graph datasets.Figure6: Evaluating the retraining accuracy, training and inference FLOPs of the proposed GEBT over SOTA graph sparsification methods (Random pruning<ref type="bibr" target="#b4">(Frankle and Carbin 2019)</ref> and SGCN(Li et al. 2020b)), under different graph and network sparsity pairs. Note that each method has a series of points for representing different graph sparsities ranging from 10% to 90%.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Features Classes</cell><cell>Metric</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell><cell>Accuracy</cell></row><row><cell>Citeseer</cell><cell>3,312</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>Accuracy</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>Accuracy</cell></row><row><cell>PPI</cell><cell>56,944</cell><cell>818,716</cell><cell>50</cell><cell>121</cell><cell>F1 Score</cell></row><row><cell cols="2">Ogbn-ArXiv 169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell><cell>Accuracy</cell></row><row><cell cols="2">Ogbl-Collab 235,868</cell><cell>1,285,465</cell><cell>128</cell><cell>2</cell><cell>Hits@50</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 114,615,892</cell><cell>602</cell><cell>41</cell><cell>F1 Score</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>GEBT vs. SOTA GCN methods on citation graphs, where ? and ? denote the improvement over original models.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>Accuracy (%) CiteSeer</cell><cell>Pumbed</cell><cell>Cora</cell><cell cols="2">Inference FLOPs (M) CiteSeer</cell><cell>Pumbed</cell></row><row><cell>GCN</cell><cell>80.9</cell><cell>69.4</cell><cell>79.0</cell><cell>77.95</cell><cell>231.6</cell><cell></cell><cell>203.1</cell></row><row><cell>GraphSAGE</cell><cell>82.5</cell><cell>71.0</cell><cell>78.9</cell><cell>6239</cell><cell>19654</cell><cell></cell><cell>15868</cell></row><row><cell>GAT</cell><cell>82.1</cell><cell>72.1</cell><cell>79.0</cell><cell>623.6</cell><cell>1853</cell><cell></cell><cell>1624</cell></row><row><cell>GIN</cell><cell>81.6</cell><cell>70.9</cell><cell>79.1</cell><cell>77.95</cell><cell>231.6</cell><cell></cell><cell>203.1</cell></row><row><cell>GEBT (GCN)</cell><cell cols="7">81.1 (?0.2) 70.5 (?1.1) 78.5 (?0.5) 24.9 (?3.1?) 51.2 (?4.5?) 55.8 (?3.6?)</cell></row><row><cell cols="8">GEBT (GraphSAGE) 82.6 (?0.1) 70.7 (?0.3) 78.0 (?0.9) 624 (?10?) 1965 (?10?) 4760 (?3.3?)</cell></row><row><cell>GEBT (GAT)</cell><cell cols="5">82.2 (?0.1) 74.1 (?2.0) 79.8 (?0.8) 149 (?4.2?) 382 (?4.9?)</cell><cell cols="2">446 (?3.6?)</cell></row><row><cell>GEBT (GIN)</cell><cell cols="7">82.4 (?0.8) 71.4 (?0.5) 79.7 (?0.6) 20.2 (?3.8?) 90.5 (?2.6?) 55.8 (?3.6?)</cell></row><row><cell>Overall Improv.</cell><cell></cell><cell>?0.9 ? ?2.0</cell><cell></cell><cell></cell><cell>?2.6? ? ?10.0?</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>GEBT vs. SOTA efficient GCN methods on PPI.</figDesc><table><row><cell></cell><cell cols="3">PPI (56K nodes and 818K edges)</cell></row><row><cell>Methods</cell><cell cols="3">F1 Scores (%) Infer. FLOPs (G) Train. FLOPs (T)</cell></row><row><cell>GAT</cell><cell>98.2</cell><cell>3.15</cell><cell>18.9</cell></row><row><cell>ResGCN</cell><cell>98.5</cell><cell>47.85</cell><cell>287.1</cell></row><row><cell>ClusterGCN</cell><cell>99.3</cell><cell>35.0</cell><cell>210.0</cell></row><row><cell>GraphSAGE</cell><cell>61.2</cell><cell>155.8</cell><cell>934.8</cell></row><row><cell>VRGCN</cell><cell>97.8</cell><cell>76.75</cell><cell>460.5</cell></row><row><cell>GraphSAINT</cell><cell>98.1</cell><cell>35.0</cell><cell>210.0</cell></row><row><cell>L2-GCN</cell><cell>96.8</cell><cell>35.0</cell><cell>210.0</cell></row><row><cell>N-GCN</cell><cell>65.0</cell><cell>30.42</cell><cell>182.5</cell></row><row><cell>GEBT (GAT) vs. GAT</cell><cell>98.8 (?0.6)</cell><cell>1.84 (?1.7?)</cell><cell>11.2 (?1.7?)</cell></row><row><cell>GEBT (ResGCN) vs. ResGCN</cell><cell>98.6 (?0.1)</cell><cell>24.15 (?2.0?)</cell><cell>147.8 (?1.9?)</cell></row><row><cell>GEBT (ClusterGCN) vs. ClusterGCN</cell><cell>99.2 (?0.1)</cell><cell>19.31 (?1.8?)</cell><cell>118.2 (?1.8?)</cell></row><row><cell>Overall Improv.</cell><cell>?0.1 ? ?38</cell><cell>?1.7? ? ?84.1?</cell><cell>?1.7? ? ? 83.5?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>GEBT vs. SOTA efficient GCN methods on Reddit.</figDesc><table><row><cell></cell><cell cols="3">Reddit (232K nodes and 11M edges)</cell></row><row><cell>Methods</cell><cell cols="3">F1 Scores (%) Infer. FLOPs (G) Train. FLOPs (T)</cell></row><row><cell>GCN</cell><cell>95.6</cell><cell>52.3</cell><cell>470.9</cell></row><row><cell>GraphSAGE</cell><cell>95.4</cell><cell>2396.7</cell><cell>21570.7</cell></row><row><cell>FastGCN</cell><cell>93.7</cell><cell>958.7</cell><cell>8628.3</cell></row><row><cell>VRGCN</cell><cell>96.3</cell><cell>956.6</cell><cell>8609.7</cell></row><row><cell>ClusterGCN</cell><cell>96.6</cell><cell>226.8</cell><cell>2041.1</cell></row><row><cell>GraphSAINT</cell><cell>96.6</cell><cell>226.8</cell><cell>2041.1</cell></row><row><cell>GTTF (GraphSAGE)</cell><cell>95.9</cell><cell>2396.7</cell><cell>21570.7</cell></row><row><cell>L2-GCN</cell><cell>94.0</cell><cell>226.8</cell><cell>2041.1</cell></row><row><cell>GEBT (GCN) vs. GCN</cell><cell>95.8 (?0.2)</cell><cell>29.3 (?1.8?)</cell><cell>266.9 (?1.7?)</cell></row><row><cell>GEBT (GraphSAGE) vs. GraphSAGE</cell><cell>97.1 (?1.7)</cell><cell>1198.4 (?2.0?)</cell><cell>10929.1 (?2.0?)</cell></row><row><cell>Overall Improv.</cell><cell>?0.5 ? ?3.4</cell><cell>?1.8? ? ?81.8?</cell><cell>?1.7? ? ?80.8?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to acknowledge the funding support from the <rs type="funder">NSF</rs> <rs type="programName">EPCN program</rs> (Award ID: <rs type="grantNumber">1934767</rs>) for this project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_t4hwJak">
					<idno type="grant-number">1934767</idno>
					<orgName type="program" subtype="full">EPCN program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified lottery ticket hypothesis for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1695" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">EarlyBERT: Efficient BERT Training via Earlybird Lottery Tickets</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AWB-GCN: A graph convolutional network accelerator with runtime workload rebalancing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int. Symp. Microarchit.(MICRO)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">2021. I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herbordt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1051" to="1063" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Benchmark Data Sets for Graph Kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">Deepergcn: All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SGCN: A Graph Sparsifier Based on Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05270</idno>
		<idno>arXiv-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2020. 2018. 2020</date>
			<biblScope unit="page" from="275" to="287" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rethinking the value of network pruning. Winning Lottery Tickets in Deep Generative Models. arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Graph Convolutional Network for Skeleton-Based Human Action Recognition by Neural Searching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2669" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Degree-Quant: Quantization-Aware Training for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05000</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hygcn: A gcn accelerator with hybrid architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
	<note>Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
	<note>Robust Graph Representation Learning via Neural Sparsification</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
