<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discontinuous Seam-Carving for Video Retargeting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
							<email>grundman@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
							<email>kwatra@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mei</forename><surname>Han</surname></persName>
							<email>meihan@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discontinuous Seam-Carving for Video Retargeting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">091E7458B95C30EE206FAE6FCFE93BF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new algorithm for video retargeting that uses discontinuous seam-carving in both space and time for resizing videos. Our algorithm relies on a novel appearance-based temporal coherence formulation that allows for frame-by-frame processing and results in temporally discontinuous seams, as opposed to geometrically smooth and continuous seams. This formulation optimizes the difference in appearance of the resultant retargeted frame to the optimal temporally coherent one, and allows for carving around fast moving salient regions. Additionally, we generalize the idea of appearance-based coherence to the spatial domain by introducing piece-wise spatial seams. Our spatial coherence measure minimizes the change in gradients during retargeting, which preserves spatial detail better than minimization of color difference alone. We also show that per-frame saliency (gradientbased or feature-based) does not always produce desirable retargeting results and propose a novel automatically computed measure of spatio-temporal saliency. As needed, a user may also augment the saliency by interactive regionbrushing. Our retargeting algorithm processes the video sequentially, making it conducive for streaming applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video retargeting has gained significant importance with the growth of diverse devices (ranging from mobile phones, mobile gaming and video devices, TV receivers, internet video players, etc.) that support video playback with varying formats, resolutions, sizes, and aspect ratios. Video retargeting resizes the video to a new target resolution or aspect ratio, while preserving its salient content.</p><p>Recent approaches to video retargeting aim to preserve salient content and avoid direct scaling or cropping by removing "unwanted" or redundant pixels and regions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Such a removal (or carving) of redundant regions results in complex non-euclidean transformations or deformations of image content, which can lead to artifacts in both space and time. These artifacts are alleviated by enforcing spatial and temporal consistency of salient content in the target video. In this paper, we propose an algorithm for video retargeting that is motivated by seam carving techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> and augments those approaches with several novel ideas.</p><p>Our treatment of video is significantly different than the surface carving approach of <ref type="bibr" target="#b12">[13]</ref>. We observe that geometric smoothness of seams across the video volume -while sufficient -may not be necessary to obtain temporally coherent videos. Instead we optimize for an appearancebased temporal coherence measure for seams. We also extend a similar idea to spatial seams, which allows them to vary by several pixels between adjacent rows (for vertical seams). Such a formulation affords greater flexibility than continuous seam removal. In particular, the seams can circumvent large salient regions by making long lateral moves and also jump location over frames if the region is moving across the frame (see Fig. <ref type="figure" target="#fig_6">6a</ref>).</p><p>To improve the quality of spatial detail over seams as pixels are carved, we propose to use a spatial coherence measure for the visual error that gives greater importance to the variation in gradients as opposed to the gradients themselves. This improves upon the forward energy measure of <ref type="bibr" target="#b12">[13]</ref>. We demonstrate the effectiveness of this formulation on image resizing applications as well.</p><p>Saliency contributes significantly to the outcome of any video retargeting algorithm. Avidan et al. <ref type="bibr" target="#b0">[1]</ref> noted that no "single energy function performs well across all images". While we mostly rely on a simple gradient-based saliency in our examples, we also show results that use an alternative fully automatic definition of saliency. This novel definition of saliency is based on the image based approach of <ref type="bibr" target="#b10">[11]</ref>. To achieve temporal coherence between frames, we segment the video into spatio-temporal regions and average the frame-based saliency over each spatio-temporal region. We also provide examples generated by user-supplied weighting of spatio-temporal regions. We employ the segmentation algorithm of <ref type="bibr" target="#b4">[5]</ref>, extended to video volumes <ref type="bibr" target="#b6">[7]</ref>, for computing spatio-temporal regions, but could have also used segmentations from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. In principle, our method is not limited to a single definition of saliency or a specific video segmentation algorithm. While the use of spatio-temporal saliency improves our results considerably we will show that even on per-frame, gradient-based saliency our algorithm outperforms existing approaches.</p><p>An additional advantage of our resizing technique is that it processes the video sequentially, i.e. on a frame-by-frame basis, and therefore is scalable to arbitrarily long or streaming videos. This allows us to improve the computation time by a factor of at least four compared to the fastest re- ported numbers to-date and achieve performance of about two frames per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The use of seam carving for image resizing was introduced by Avidan and Shamir <ref type="bibr" target="#b0">[1]</ref> and later extended for video retargeting by Rubinstein et al. <ref type="bibr" target="#b12">[13]</ref>. Seams are vertical or horizontal chains of pixels that are successively removed from or added to an image to change its width or height, respectively. To preserve content, seams are chosen as least energy paths through the image. In video, seams are generalized to surfaces that carve through the spatio-temporal volume. Space-time surface carving is also used by Chen and Sen <ref type="bibr" target="#b1">[2]</ref> for video summarization. An issue with space-time carving is the memory required for processing video volumes, which is usually addressed by approximation techniques: <ref type="bibr" target="#b1">[2]</ref> carve the video in small chunks, while <ref type="bibr" target="#b12">[13]</ref> take a banded multi-resolution approach; both use a graph cut algorithm to solve for the surface.</p><p>Seam carving is very effective but needs external saliency maps in cases where salient objects lack texture. Wolf et al. <ref type="bibr" target="#b18">[19]</ref> present a video retargeting technique that combines automatic saliency detection with non-uniform scaling using global optimization. They compute a saliency map for each frame using image gradients as well as face and motion detection. In contrast, we treat the detection of saliency itself as an orthogonal problem. Primarily, we use per-frame gradient-based saliency similar to <ref type="bibr" target="#b0">[1]</ref> but we also generate a temporally coherent saliency based on space-time regions derived from the image-based approach of <ref type="bibr" target="#b10">[11]</ref>. We examine the difference of both saliency definitions in Fig. <ref type="figure" target="#fig_13">10</ref> and our video.</p><p>Other methods that use optimization for generating visual summaries include <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Optimization methods use constraints based on the desired target size. Therefore, they need to be re-run for each desired size. In contrast, seam or surface carving approaches as our proposed algorithm and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> allow retargeting to the chosen size in real-time. Preventing aliasing artifacts in retargeting was recently addressed by <ref type="bibr" target="#b8">[9]</ref> by using a warping technique known as EWA splatting. While producing good results, the approach is mainly constraint to static cameras (e.g. line constraints are not tracked).</p><p>Gal et al. <ref type="bibr" target="#b5">[6]</ref> present a feature-aware texture mapping technique that avoids distorting important features, supplied as user-specified regions, by applying non-uniform warping to the texture image. This is similar to our approach of using regions for saliency. However, our automatic segmentation-aided region selection method scales to video. For video segmentation, we build upon Felzenszwalb and Huttenlocher's graph-based image segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. However other video segmentations techniques such as <ref type="bibr" target="#b11">[12]</ref> could also be used.</p><p>Automatic pan-and-scan and smart cropping have been proposed by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, <ref type="bibr" target="#b13">[14]</ref> introduced a method to find an optimal combination of cropping, non-isotropic scaling and seam carving for image retargeting w.r.t. a cost measure similar to <ref type="bibr" target="#b14">[15]</ref>. The approach is extended to video by applying the method to key-frames and interpolating the operations between them. We demonstrate equivalent results using our approach and compare to <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Retargeting by Seam Removal</head><p>Our video retargeting algorithm resizes a video by sequentially removing seams from it. Seams are 8-connected paths of pixels with the property that each row (vertical seams) or each column (horizontal seams) is incident to exactly one pixel of the seam. Hence removing or duplicating a vertical seam changes the width of a frame by exactly one column. Alternating N times between seam computation and removal for a w × h frame yields N disjoint seams, effectively computing a content-aware resize for 2N target sizes {(w + N ) × h}, . . . , {(w + 1) × h}, {w × h}, . . . , {(w -N ) × h}. This is in contrast to optimization methods that solve for each target size independently. The pre-computed seams enable real-time content-aware resizing as removal or duplication of seams only involves fast memory moves. Rubinstein et al. <ref type="bibr" target="#b12">[13]</ref> presented an approach generalizing the seam in an image to a surface in the video volume by extending the image seam carving approach of <ref type="bibr" target="#b0">[1]</ref>. The proposed solution for altering the width of the video is a vertical surface. The cross-sections of this surface form a vertical seam in every frame and a temporal seam in the x -t plane for any fixed y-location<ref type="foot" target="#foot_1">1</ref> . Therefore, a fundamental property of the surface is that it can only move by at most one pixel-location between adjacent frames.</p><p>Consider the case of an object of interest moving from left to right over the video sequence as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Any vertical surface has to start to the right of the object and end to the left of it. In other words, the seam surface is bound to intersect with the object of interest and thereby distort it. This behavior is not limited to this particular case but occurs in general when there is considerable motion in the video perpendicular to the surface -the surface simply cannot keep up with the motion in the video.</p><p>In the context of seam carving, temporal coherence is established if adjacent resized frames are aligned like in the original video. If we optimize for temporal coherence alone, an obvious solution is to pick the same seam for every frame: all pixels that are neighbors along the temporal dimension in the original video will stay neighbors in the resized video. This is akin to non-uniform scaling, where selective columns may be removed (with blending) to shrink the video. However, this by itself will introduce spatial artifacts because in contrast to non-uniform scaling, seams group in non-salient regions instead of being distributed evenly over the columns of a video.</p><p>We experimented propagating seams based on tracking non-salient objects in the video. However this does not necessarily lead to good results. In case of vertical seams, if the tracked object does not cover the whole height of the video the propagated seam will intersect with the background at a multitude of different positions resulting in seam that get pulled apart in different directions over time (too fragmented).</p><p>Surface carving relaxes the optimal temporal coherence criterion, i.e. replicating the same seam in all frames, by allowing the seam to vary smoothly over time. In other words, it imposes a geometric smoothness constraint upon the seam solution. While this may be a sufficient condition for achieving temporal coherence, it is not necessary. Instead, we show that, it is sufficient (and less restrictive) to compute a seam in the current frame such that the appearance of the resulting resized frame is similar to the appearance obtained by applying the optimal temporally coherent seam. Optimizing against this criterion ensures temporally coherent appearance, but relieves the seams from being geometrically connected to each other across frames, leading to temporally discontinuous seams.</p><p>Our algorithm processes frames sequentially as follows. For each pixel in the current frame, we first determine the spatial and temporal coherence costs (S C and T C ) as well as the saliency (S) cost of removing that pixel. The three cost measures are linearly combined to one measure M , with a weight ratio S C :T C :S of 5:1:2 for most sequences. In case of highly dynamic video content we use a ratio of 5:0.2:2. Video clip classification based on optical flow magnitude could automate this choice. We then compute the minimum cost seams w.r.t. M for that frame using dynamic programming, similar to <ref type="bibr" target="#b0">[1]</ref>. By removing or duplicating and blending N seams from each frame we can change the width of the video by N columns. Changing the height is achieved by transposing each frame, computing and removing seams, and transposing the resulting frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Measuring Temporal Coherence</head><p>Assume we successively compute a seam S i in every m × n frame F i , i ∈ 1, . . . , T . Our objective is to remove a seam from the current frame so that the resulting (m -1) × n frame R i would be visually close to the most temporally coherent one, R c , where R c is obtained by reusing the previous seam S i-1 and applying it to the current frame F i .</p><p>We use R c to inform the process of selecting S i through a look-ahead strategy. For every pixel (x, y), we determine how much the resulting resized frame R i would differ from R c if that pixel were removed. We use the sum-of-squareddifferences (SSD) of the two involved rows as the measure of temporal coherence, T c (x, y):</p><formula xml:id="formula_0">T c = x-1 k=0 ||F i k,y -R c k,y || 2 + m-1 k=x+1 ||F i k,y -R c k-1,y || 2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The temporal coherence cost at a pixel reduces to a perrow difference accumulation that can be determined for every pixel before any seams are computed (see Fig. <ref type="figure" target="#fig_2">3</ref>). This allows us to apply the original seam carving algorithm to </p><formula xml:id="formula_2">A B C D E F A C D E F A B C D E C B E D D C F E © 2008 Paramount Pictures</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Measuring Spatial Coherence</head><p>Our look-ahead strategy for measuring temporal coherence may also be applied to the spatial domain. Here, the question is how much spatial error will be introduced after removing a seam. The basis of this idea is similar to Rubinstein et al.'s <ref type="bibr" target="#b12">[13]</ref> proposed forward energy. However, our formulation leads to a more general model, i.e. piecewise seams, and is not based on the introduced intensity variation but the variation in the gradient of the intensity.</p><p>We motivate our spatial coherence measure by examining several different cases in Fig. <ref type="figure" target="#fig_3">4</ref>. In (a), there is a step between A and B as represented by the color difference. Removing B yields AC, which exhibits the same step as before, hence no detail is lost 2 . On the other hand, in (b), high frequency detail will be lost on removing B. Removing B in (c) compacts the linear ramp, which is the desired behavior as it compresses the local neighborhood without sig- 2 Rubinstein et al.'s forward energy is expressed as a difference in intensity and would be large in this case.  nificantly changing its appearance. In each of these cases, the cost of removing B is well represented by the change in gradient, which is what we use as our measure of spatial coherence, instead of change in intensity.</p><p>Our spatial coherence measure S c = S h + S v consists of two terms, which quantify the error introduced in the horizontal and vertical (including diagonal) directions, respectively, by the removal of a specific pixel. Specifically S h and S v are designed to measure the change in gradients caused by the removal of the pixel. S h only depends on the pixel in question and in some sense adds to its saliency, while S v depends upon the pixel and its potential best seam neighbor in the row above. Therefore S v defines a spatial transition cost between two pixels in adjacent rows. S h is defined such that it is zero for the cases (a) and (c) in Fig. <ref type="figure" target="#fig_3">4</ref> and large for case (b). The equations for interior pixels (E in Fig. <ref type="figure" target="#fig_4">5a</ref>) and border pixels (D in Fig. <ref type="figure" target="#fig_4">5b</ref>) are slightly different, but both measure changes in horizontal gradient magnitude:</p><formula xml:id="formula_3">5a: S h (E) = |D -E| + |E -F | -|D -F | , and 5b: S h (D) = |D -E| -|E -F | .</formula><p>We define S v to measure the change in vertical gradi-ent magnitudes when transitioning between a pair of pixels in adjacent rows. We treat the involved pixels in a symmetric manner to avoid giving undue preference to diagonal neighbors. Hence, S v depends on whether the top neighbor of the pixel in question (say E in Fig. <ref type="figure" target="#fig_4">5a</ref>) is its left (A), center (B), or right (C) neighbor. Fig. <ref type="figure" target="#fig_4">5a</ref> corresponds to S v (E, A), where:</p><formula xml:id="formula_4">S v (E, B) = 0 S v (E, A) = |A -D| -|B -D| + |B -E| -|B -D| S v (E, C) = |C -F | -|B -F | + |B -E| -|B -F | .</formula><p>Piecewise Spatial Seams: We have shown that in order to achieve temporal coherence, a temporally smooth solution is not necessary; the appearance based measure T c is sufficient. A natural generalization of this approach is to apply a similar idea to the spatial domain, which would lead to discontinuous spatial seams. For this purpose, we generalize our spatial coherence cost, particularly the transition cost S v to an accumulated spatial transition cost that allows a pixel to consider not just its three neighbors in the row above but all pixels in that row. An example is shown in Fig. <ref type="figure" target="#fig_4">5c</ref>. For a pixel (x b , y) in the bottom row, the summed spatial transition cost to pixel (x a , y -1) in the top row (for the case x a &lt; x b ) is:</p><formula xml:id="formula_5">S v (x b , x a , y) = x b -1 k=xa |G v k,y -G d k,y |+ x b k=xa+1 |G v k,y -G d k-1,y |</formula><p>where G v k,y = |F k,y -F k,y-1 | is the vertical gradient magnitude between pixel (k, y) and its top neighbor, while G d k,y = |F k,y -F k+1,y-1 | is its diagonal gradient magnitude with the top right neighbor. The diagonal terms appear because previously diagonal gradients become vertical gradients after seam removal. For the example in Fig. <ref type="figure" target="#fig_4">5c</ref>, the first term in the equation above will be |AE -BE|+|BF -CF | + |CG -DG|, where AE is shorthand for |A -E|. The cost for the case x a &gt; x b may be defined similarly, while S v (x, x, y) = 0. In practice, the optimal neighbor x a typically lies in a window of ∼ 15 pixels around x b , allowing us to reduce the computational cost from O(m) to O(1). Another effect of limiting the search window is that we implicitly enforce seams with a limited number of piecewise jumps in contrast to set of totally disconnected pixels.</p><p>Fig. <ref type="figure" target="#fig_6">6</ref> shows examples of both temporally discontinuous and piecewise spatial seams. Fig. <ref type="figure" target="#fig_8">7</ref> demonstrates the effectiveness of our spatial coherence cost in preserving detail. Fig. <ref type="figure" target="#fig_11">9</ref> shows comparisons with image resizing results of <ref type="bibr" target="#b12">[13]</ref> (examples from their web page), which use their forward energy measure. Fig. <ref type="figure" target="#fig_9">8</ref> shows a similar comparison for a video example (also from their paper).     Our result (bottom-right) is able to preserve the shape of the cars and poles better than <ref type="bibr" target="#b12">[13]</ref>'s result (bottom-left). Even the plate on the truck saying "Yellow" is still readable. See accompanying video for complete result.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Automatic spatio-temporal saliency</head><p>There are cases where per-frame gradient based saliency 3 is not sufficient. We can employ higher-level techniques such as face detection, motion cues or learned saliency <ref type="bibr" target="#b10">[11]</ref>, but a major challenge remains in the required temporal coherence for video retargeting. In face detection, for example, the bounding boxes around faces might change considerably between frames or even miss several ones.</p><p>We are interested in designing an automatic saliency measure that is temporally coherent as well as aligned with the outlines in the video. The latter requirement is motivated by the fact that local saliency measures do not capture higher-level context and are inherently sensitive to noise. Therefore, we propose to average external per-frame saliency maps over spatio-temporal regions to address both issues. We obtain spatio-temporal regions for video by extending <ref type="bibr" target="#b4">[5]</ref>'s graph-based image segmentation to video <ref type="bibr" target="#b6">[7]</ref>, but any other video segmentation method could be used as well. We build a 3D graph using a 26-neighborhood in space-time with edge weights based on color difference. We then apply the graph-segmentation algorithm to obtain spatio-temporal regions. The effect of applying our method to frame-based saliency maps is shown in Fig. <ref type="figure" target="#fig_13">10</ref>.</p><p>If the underlying frame-based saliency method fails to detect salient content in a majority of frames, the spatiotemporal smoothing fails as well. In this case we offer a user interface that allows highlighting salient and non-salient regions by simple brush strokes, which are then automatically tracked over multiple frames through the spatio-temporal regions (see Fig. <ref type="figure" target="#fig_15">11</ref>). 3 We use the sum of absolute values of the pixel's gradients in our work.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We demonstrate our results for video retargeting based on gradient-based saliency and spatio-temporal saliency (auto-(a) Our result (b) Result from <ref type="bibr" target="#b8">[9]</ref> (c) Result from <ref type="bibr" target="#b13">[14]</ref> Figure <ref type="figure" target="#fig_1">12</ref>: Comparison to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b13">[14]</ref>. Content is highly dynamic (athlete performing 720 o turn and fast moving camera). In <ref type="bibr" target="#b8">[9]</ref>, the background gets squished on the left, the waterfront at the bottom gets distorted, and the result is less sharp overall compared to our result. The approach of <ref type="bibr" target="#b13">[14]</ref> distorts the head and essentially crops the frame, while our algorithm compresses the background. matic as well as user-selected) in the accompanying video. Fig. <ref type="figure" target="#fig_1">12</ref> shows comparisons to other techniques for a highly dynamic video. Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure" target="#fig_2">13</ref> (top three rows) show frames from example videos that were retargeted using gradient-based saliency. Fig. <ref type="figure" target="#fig_2">13</ref> (bottom row) and Fig. <ref type="figure" target="#fig_3">14</ref> were retargeted by user-selected regions as shown. In both cases it took less than 10 seconds to select the regions.</p><p>Our approach provides the user control over determining what regions to carve in case automatic approaches fail. Fig. <ref type="figure" target="#fig_3">14</ref> demonstrates the usefulness of user-selected regions for non-salient content. Fig. <ref type="figure" target="#fig_4">15</ref> shows that we can achieve results comparable to and with sharper detail than <ref type="bibr" target="#b13">[14]</ref> -we only used per-frame gradient-based saliency in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Limitations</head><p>We have presented a novel video retargeting algorithm based on carving discontinuous seams in space and time that exhibits improved visual quality, affords greater flexibility, and is scalable for large videos. We achieve 2 fps on 400x300 video compared to 0.3-0.4 fps for <ref type="bibr" target="#b12">[13]</ref> and 0.5 fps for our implementation of <ref type="bibr" target="#b18">[19]</ref>. We have presented the novel idea of using spatio-temporal regions for automatic or user-guided saliency. We have also demonstrated the benefits of our novel gradient-variation based spatial coherence measure in preserving detail.</p><p>We can handle videos with long shots as well as streaming videos using frame-by-frame processing. However, if spatio-temporal saliency is also employed, then the video length is limited by the underlying video segmentation algorithm, which in our case is ∼ 30 -40 seconds.</p><p>Fast-paced actions or highly-structured scenes might have little non-salient content. In these cases, just like other approaches, our video retargeting might produce unsatisfactory results as shown in our accompanying video.</p><p>The sequential nature of our video retargeting algorithm can occasionally cause the seam in the initial frames to be sub-optimal w.r.t. the later frames. This can sometimes cause several seams to jump their location across time in the same frame, which leads to a visible temporal discontinuity. However, this problem can be alleviated by looking up the saliency information forward in time (around 5 frames) and averaging it with the current saliency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Six frames from the result of our retargeting algorithm applied to a sub-clip of "Apologize", c 2006 One Republic. Original frames on left, retargeted results on right. We use shot boundary detection to separate the individual shots before processing.</figDesc><graphic coords="2,217.45,126.80,160.65,70.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Traced x-t slice (at knee height) of a person running from left to right (from Weizmann Action Recognition dataset), obtained using background subtraction. Every vertical surface is a seam in the x-t plane (red) and would intersect with the spacetime shape of the person. In contrast our temporally discontinuous solution (green) stays in front of the person (b) and jumps between adjacent frames (c) → (d) to overcome spatial distortion.</figDesc><graphic coords="3,124.35,81.96,51.98,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The previous seam S i-1 (red) is applied to current Frame F i . Removing pixel B results in the row ACDEF . The optimal temporally coherent seam removes pixel F , so that R c would contain ABCDE. The temporal coherence cost for pixel B is |C -B| + |D -C| + |E -D| + |F -E|, which is the SSD between the two rows as well as the sum of gradients from B to F . Original frame from The Duchess, c 2008 Paramount Pictures. a linear combination of saliency and temporal coherence. It turns out that temporal coherence integrates the gradient along the pixels across which the seam jumps between frames. This is desirable because it means that seams can move more freely in homogeneous regions. Eq.1 can be efficiently computed using two m × n integral images. The left sum in Eq. 1 will be represented recursively by I l 0,y = 0, I l x+1,y = I l x,y + ||F i x,y -R c x,y || 2 , and the right sum by I r m-1,y = 0, I r x-1,y = I r x,y + ||F i x,y -R c x-1,y || 2 , resulting in T c = (I l + I r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (See in color.) Spatial error if pixel B is removed.</figDesc><graphic coords="4,80.63,82.04,169.26,70.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Spatial coherence costs: (a) Removing an interior pixel, E w.r.t. A. Bottom row DEF becomes DF , therefore the intensity difference before removing E was |D -E| + |E -F | and is |D -F | afterwards. Between the two rows, the intensity difference was |A -D| and |B -E| and is |B -D| afterwards. (b) Removing a border pixel, here D w.r.t. B. In the bottom row |D -E| becomes |E -F |. (c) Summed spatial transition cost for piecewise seams. Consider transition A → H.We accumulate the change in (LHS) gradient magnitudes before (dotted blue) and after (dashed red) removal (Order: Left to right). We also consider the symmetric case by accumulating the change in RHS gradient magnitudes before (solid orange) and after (dashed red) removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) Camera pans to the right. The new seam (green) jumps to the new redundant content on right and avoids introducing artifacts resulting from having to move smoothly through the whole frame. From Sweeney Todd, c 2007 Paramount Pictures (b) Piecewise seams (here neighborhood of 11 pixels) have the freedom to carve around details and therefore prevent artifacts. From The Dark Knight, c 2008 Warner Bros. Pictures.</figDesc><graphic coords="5,312.22,81.96,130.34,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) with Sc (b) w/o Sc (c)<ref type="bibr" target="#b18">[19]</ref> c 2007 MGM (d)<ref type="bibr" target="#b12">[13]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of spatial coherence measure Sc (a) Our algorithm with Sc (without piecewise seams) (b) Our algorithm without Sc (but with [13]'s forward energy); one plane is clearly distorted (c) Our implementation of [19] (d) [13]'s result. Original frame from Valkyrie, c 2007 MGM.</figDesc><graphic coords="5,314.77,417.41,224.44,96.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Video retargeting comparison for gradient based saliency. Shown is a single frame from a highway video (top).Our result (bottom-right) is able to preserve the shape of the cars and poles better than<ref type="bibr" target="#b12">[13]</ref>'s result (bottom-left). Even the plate on the truck saying "Yellow" is still readable. See accompanying video for complete result.</figDesc><graphic coords="5,313.94,517.86,111.77,80.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Image retargeting results. Top row shows the original images.In bottom row, images labeled A are<ref type="bibr" target="#b12">[13]</ref>'s result, while images labeled B are our results using the novel gradient-variation based spatial coherence cost. In the pliers (left) example, our result better respects the curvature in the handle's shape. For Ratatouille, c 2007 Walt Disney Pictures, (middle) and the snow scene (right), the straight edges are better preserved in our result (shown zoomed in).</figDesc><graphic coords="6,359.24,128.88,157.87,117.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effect of our spatio-temporal saliency. Left column: Saliency maps computed based on [11] for adjacent frames (top/bottom) independently (white = salient content). Notice the abrupt changes in face, coat and right brick wall. Middle column: Saliency averaged over spatio-temporal regions results in smooth variations across frames. Right column: Effect on video retargeting. Top uses spatio-temporal saliency, bottom uses gradient based saliency. Original frame: 88 minutes, c 2007 TriStar Pictures.</figDesc><graphic coords="6,317.57,524.06,109.02,58.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: User selects regions in a single frame (a) by roughly brushing over objects of interest (indicated by dashed line). These regions are automatically extrapolated to other frames (b) of the video. See accompanying video. Original frame from 88 minutes, c 2007 TriStar Pictures.</figDesc><graphic coords="6,429.56,523.83,109.00,59.06" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-4244-6985-7/10/$26.00 ©2010 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Conversely, a horizontal surface forms a horizontal seam in every frame and a temporal seam in the y -t plane for any fixed x-location.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seam carving for content-aware image resizing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2007">2007. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video carving</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<title level="s">Short Papers</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pan, zoom, scan -timecoherent, trained automatic video cropping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Looking into video frames on small displays</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature-aware texturing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006">2010. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object based segmentation of video using color, motion and spatial information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A system for retargeting of streaming video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH ASIA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video retargeting: automating pan and scan</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006">2007. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Edge-preserving smoothing and mean-shift segmentation of video streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved seam carving for video retargeting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008">2008. 1, 2, 3, 4, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-operator media retargeting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008">2009. 2, 7, 8</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image and video segmentation by anisotropic kernel mean shift</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thiesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Optimized scale-and-stretch for image resizing</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM SIG-GRAPH ASIA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inverse texture synthesis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-homogeneous content-driven video-retargeting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guttmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007. 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
