<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalization in Adaptive Data Analysis and Holdout Reuse *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ibm</forename><surname>Almaden</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Research</forename><surname>Center</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalization in Adaptive Data Analysis and Holdout Reuse *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">745B2903F12FAEFB16B3C5AC390B8485</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in <ref type="bibr" target="#b6">[7]</ref>, where we focused on the problem of estimating expectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment. We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [7] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of machine learning is to produce hypotheses or models that generalize well to the unseen instances of the problem. More generally, statistical data analysis is concerned with estimating properties of the underlying data distribution, rather than properties that are specific to the finite data set at hand. Indeed, a large body of theoretical and empirical research was developed for ensuring generalization in a variety of settings. In this work, it is commonly assumed that each analysis procedure (such as a learning algorithm) operates on a freshly sampled dataset -or if not, is validated on a freshly sampled holdout (or testing) set.</p><p>Unfortunately, learning and inference can be more difficult in practice, where data samples are often reused. For example, a common practice is to perform feature selection on a dataset, and then use the features for some supervised learning task. When these two steps are performed on the same dataset, it is no longer clear that the results obtained from the combined algorithm will generalize. Although not usually understood in these terms, "Freedman's paradox" is an elegant demonstration of the powerful (negative) effect of adaptive analysis on the same data <ref type="bibr" target="#b9">[10]</ref>. In Freedman's simulation, variables with significant t-statistic are selected and linear regression is performed on this adaptively chosen subset of variables, with famously misleading results: when the relationship between the dependent and explanatory variables is non-existent, the procedure overfits, erroneously declaring significant relationships.</p><p>Most of machine learning practice does not rely on formal guarantees of generalization for learning algorithms. Instead a dataset is split randomly into two (or sometimes more) parts: the training set and the testing, or holdout, set. The training set is used for learning a predictor, and then the holdout set is used to estimate the accuracy of the predictor on the true distribution (Additional averaging over different partitions is used in cross-validation.). Because the predictor is independent of the holdout dataset, such an estimate is a valid estimate of the true prediction accuracy (formally, this allows one to construct a confidence interval for the prediction accuracy on the data distribution). However, in practice the holdout dataset is rarely used only once, and as a result the predictor may not be independent of the holdout set, resulting in overfitting to the holdout set <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>. One well-known reason for such dependence is that the holdout data is used to test a large number of predictors and only the best one is reported. If the set of all tested hypotheses is known and independent of the holdout set, then it is easy to account for such multiple testing.</p><p>However such static approaches do not apply if the estimates or hypotheses tested on the holdout are chosen adaptively: that is, if the choice of hypotheses depends on previous analyses performed on the dataset. One prominent example in which a holdout set is often adaptively reused is hyperparameter tuning (e.g. <ref type="bibr" target="#b4">[5]</ref>). Similarly, the holdout set in a machine learning competition, such as the famous ImageNet competition, is typically reused many times adaptively. Other examples include using the holdout set for feature selection, generation of base learners (in aggregation techniques such as boosting and bagging), checking a stopping condition, and analyst-in-the-loop decisions. See <ref type="bibr" target="#b12">[13]</ref> for a discussion of several subtle causes of overfitting.</p><p>The concrete practical problem we address is how to ensure that the holdout set can be reused to perform validation in the adaptive setting. Towards addressing this problem we also ask the more general question of how one can ensure that the final output of adaptive data analysis generalizes to the underlying data distribution. This line of research was recently initiated by the authors in <ref type="bibr" target="#b6">[7]</ref>, where we focused on the case of estimating expectations of functions from i.i.d. samples (these are also referred to as statistical queries). .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Results</head><p>We propose a simple and general formulation of the problem of preserving statistical validity in adaptive data analysis. We show that the connection between differentially private algorithms and generalization from <ref type="bibr" target="#b6">[7]</ref> can be extended to this more general setting, and show that similar (but sometimes incomparable) guarantees can be obtained from algorithms whose outputs can be described by short strings. We then define a new notion, approximate max-information, that unifies these two basic techniques and gives a new perspective on the problem. In particular, we give an adaptive composition theorem for max-information, which gives a simple way to obtain generalization guarantees for analyses in which some of the procedures are differentially private and some have short description length outputs. We apply our techniques to the problem of reusing the holdout set for validation in the adaptive setting.</p><p>A reusable holdout: We describe a simple and general method, together with two specific instantiations, for reusing a holdout set for validating results while provably avoiding overfitting to the holdout set. The analyst can perform any analysis on the training dataset, but can only access the holdout set via an algorithm that allows the analyst to validate her hypotheses against the holdout set. Crucially, our algorithm prevents overfitting to the holdout set even when the analyst's hypotheses are chosen adaptively on the basis of the previous responses of our algorithm.</p><p>Our first algorithm, referred to as Thresholdout, derives its guarantees from differential privacy and the results in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. For any function φ : X → [0, 1] given by the analyst, Thresholdout uses the holdout set to validate that φ does not overfit to the training set, that is, it checks that the mean value of φ evaluated on the training set is close to the mean value of φ evaluated on the distribution P from which the data was sampled. The standard approach to such validation would be to compute the mean value of φ on the holdout set. The use of the holdout set in Thresholdout differs from the standard use in that it exposes very little information about the mean of φ on the holdout set: if φ does not overfit to the training set, then the analyst receives only the confirmation of closeness, that is, just a single bit. On the other hand, if φ overfits then Thresholdout returns the mean value of φ on the training set perturbed by carefully calibrated noise.</p><p>Using results from <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> we show that for datasets consisting of i.i.d. samples these modifications provably prevent the analyst from constructing functions that overfit to the holdout set. This ensures correctness of Thresholdout's responses. Naturally, the specific guarantees depend on the number of samples n in the holdout set. The number of queries that Thresholdout can answer is exponential in n as long as the number of times that the analyst overfits is at most quadratic in n.</p><p>Our second algorithm SparseValidate is based on the idea that if most of the time the analystâ Ȃ Źs procedures generate results that do not overfit, then validating them against the holdout set does not reveal much information about the holdout set. Specifically, the generalization guarantees of this method follow from the observation that the transcript of the interaction between a data analyst and the holdout set can be described concisely. More formally, this method allows the analyst to pick any Boolean function of a dataset ψ (described by an algorithm) and receive back its value on the holdout set. A simple example of such a function would be whether the accuracy of a predictor on the holdout set is at least a certain value α. (Unlike in the case of Thresholdout, here there is no need to assume that the function that measures the accuracy has a bounded range or even Lipschitz, making it qualitatively different from the kinds of results achievable subject to differential privacy). A more involved example of validation would be to run an algorithm on the holdout dataset to select an hypothesis and check if the hypothesis is similar to that obtained on the training set (for any desired notion of similarity). Such validation can be applied to other results of analysis; for example one could check if the variables selected on the holdout set have large overlap with those selected on the training set. An instantiation of the SparseValidate algorithm has already been applied to the problem of answering statistical (and more general) queries in the adaptive setting <ref type="bibr" target="#b0">[1]</ref>.</p><p>We describe a simple experiment on synthetic data that illustrates the danger of reusing a standard holdout set, and how this issue can be resolved by our reusable holdout. The design of this experiment is inspired by Freedman's classical experiment, which demonstrated the dangers of performing variable selection and regression on the same data <ref type="bibr" target="#b9">[10]</ref>.</p><p>Generalization in adaptive data analysis: We view adaptive analysis on the same dataset as an execution of a sequence of steps</p><formula xml:id="formula_0">A 1 → A 2 → • • • → A m .</formula><p>Each step is described by an algorithm A i that takes as input a fixed dataset S = (x 1 , . . . , x n ) drawn from some distribution D over X n , which remains unchanged over the course of the analysis. Each algorithm A i also takes as input the outputs of the previously run algorithms A 1 through A i-1 and produces a value in some range Y i . The dependence on previous outputs represents all the adaptive choices that are made at step i of data analysis. For example, depending on the previous outputs, A i can run different types of analysis on S. We note that at this level of generality, the algorithms can represent the choices of the data analyst, and need not be explicitly specified. We assume that the analyst uses algorithms which individually are known to generalize when executed on a fresh dataset sampled independently from a distribution D. We formalize this by assuming that for every fixed value</p><formula xml:id="formula_1">y 1 , . . . , y i-1 ∈ Y 1 × • • • × Y i-1</formula><p>, with probability at least 1 -β i over the choice of S according to distribution D, the output of A i on inputs y 1 , . . . , y i-1 and S has a desired property relative to the data distribution D (for example has low generalization error). Note that in this assumption y 1 , . . . , y i-1 are fixed and independent of the choice of S, whereas the analyst will execute A i on values Y 1 , . . . , Y i-1 , where Y j = A j (S, Y 1 , . . . , Y j-1 ). In other words, in the adaptive setup, the algorithm A i can depend on the previous outputs, which depend on S, and thus the set S given to A i is no longer an independently sampled dataset. Such dependence invalidates the generalization guarantees of individual procedures, potentially leading to overfitting.</p><p>Differential privacy: First, we spell out how the differential privacy based approach from <ref type="bibr" target="#b6">[7]</ref> can be applied to this more general setting. Specifically, a simple corollary of results in <ref type="bibr" target="#b6">[7]</ref> is that for a dataset consisting of i.i.d. samples any output of a differentially-private algorithm can be used in subsequent analysis while controlling the risk of overfitting, even beyond the setting of statistical queries studied in <ref type="bibr" target="#b6">[7]</ref>. A key property of differential privacy in this context is that it composes adaptively: namely if each of the algorithms used by the analyst is differentially private, then the whole procedure will be differentially private (albeit with worse privacy parameters). Therefore, one way to avoid overfitting in the adaptive setting is to use algorithms that satisfy (sufficiently strong) guarantees of differential-privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description length:</head><p>We then show how description length bounds can be applied in the context of guaranteeing generalization in the presence of adaptivity. If the total length of the outputs of algorithms A 1 , . . . , A i-1 can be described with k bits then there are at most 2 k possible values of the input y 1 , . . . , y i-1 to A i . For each of these individual inputs A i generalizes with probability 1 -β i . Taking a union bound over failure probabilities implies generalization with probability at least 1 -2 k β i . Occam's Razor famously implies that shorter hypotheses have lower generalization error. Our observation is that shorter hypotheses (and the results of analysis more generally) are also better in the adaptive setting since they reveal less about the dataset and lead to better generalization of subsequent analyses. Note that this result makes no assumptions about the data distribution D. In the full versionwe also show that description length-based analysis suffices for obtaining an algorithm (albeit not an efficient one) that can answer an exponentially large number of adaptively chosen statistical queries. This provides an alternative proof for one of the results in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Approximate max-information: Our main technical contribution is the introduction and analysis of a new information-theoretic measure, which unifies the generalization arguments that come from both differential privacy and description length, and that quantifies how much information has been learned about the data by the analyst. Formally, for jointly distributed random variables (S, Y ), the max-information is the maximum of the logarithm of the factor by which uncertainty about S is reduced given the value of Y , namely</p><formula xml:id="formula_2">I ∞ (S, Y ) . = log max P[S=S | Y =y] P[S=S]</formula><p>, where the maximum is taken over all S in the support of S and y in the support Y . Approximate max-information is a relaxation of max-information. In our use, S denotes a dataset drawn randomly from the distribution D and Y denotes the output of a (possibly randomized) algorithm on S. We prove that approximate max-information has the following properties</p><p>• An upper bound on (approximate) max-information gives generalization guarantees.</p><p>• Differentially private algorithms have low max-information for any distribution D over datasets. A stronger bound holds for approximate max-information on i.i.d. datasets. These bounds apply only to so-called pure differential privacy (the δ = 0 case). • Bounds on the description length of the output of an algorithm give bounds on the approximate max-information of the algorithm for any D. • Approximate max-information composes adaptively.</p><p>Composition properties of approximate max-information imply that one can easily obtain generalization guarantees for adaptive sequences of algorithms, some of which are differentially private, and others of which have outputs with short description length. These properties also imply that differential privacy can be used to control generalization for any distribution D over datasets, which extends its generalization guarantees beyond the restriction to datasets drawn i.i.d. from a fixed distribution, as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>We remark that (pure) differential privacy and description length are otherwise incomparable. Bounds on max-information or differential privacy of an algorithm can, however, be translated to bounds on randomized description length for a different algorithm with statistically indistinguishable output.</p><p>Here we say that a randomized algorithm has randomized description length of k if for every fixing of the algorithm's random bits, it has description length of k. Details of these results and additional discussion appear in Section 2 and the full version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>This work complements <ref type="bibr" target="#b6">[7]</ref> where we initiated the formal study of adaptivity in data analysis. The primary focus of <ref type="bibr" target="#b6">[7]</ref> is the problem of answering adaptively chosen statistical queries. The main technique is a strong connection between differential privacy and generalization: differential privacy guarantees that the distribution of outputs does not depend too much on any one of the data samples, and thus, differential privacy gives a strong stability guarantee that behaves well under adaptive data analysis. The link between generalization and approximate differential privacy made in <ref type="bibr" target="#b6">[7]</ref> has been subsequently strengthened, both qualitatively -by <ref type="bibr" target="#b0">[1]</ref>, who make the connection for a broader range of queries -and quantitatively, by <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b0">[1]</ref>, who give tighter quantitative bounds. These papers, among other results, give methods for accurately answering exponentially (in the dataset size) many adaptively chosen queries, but the algorithms for this task are not efficient. It turns out this is for fundamental reasons -Hardt and Ullman <ref type="bibr" target="#b10">[11]</ref> and Steinke and Ullman <ref type="bibr" target="#b18">[19]</ref> prove that, under cryptographic assumptions, no efficient algorithm can answer more than quadratically many statistical queries chosen adaptively by an adversary who knows the true data distribution.</p><p>The classical approach in theoretical machine learning to ensure that empirical estimates generalize to the underlying distribution is based on the various notions of complexity of the set of functions output by the algorithm, most notably the VC dimension. If one has a sample of data large enough to guarantee generalization for all functions in some class of bounded complexity, then it does not matter whether the data analyst chooses functions in this class adaptively or non-adaptively. Our goal, in contrast, is to prove generalization bounds without making any assumptions about the class from which the analyst can output functions.</p><p>An important line of work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> establishes connections between the stability of a learning algorithm and its ability to generalize. Stability is a measure of how much the output of a learning algorithm is perturbed by changes to its input. It is known that certain stability notions are necessary and sufficient for generalization. Unfortunately, the stability notions considered in these prior works do not compose in the sense that running multiple stable algorithms sequentially and adaptively may result in a procedure that is not stable. The measure we introduce in this work (max information), like differential privacy, has the strength that it enjoys adaptive composition guarantees. This makes it amenable to reasoning about the generalization properties of adaptively applied sequences of algorithms, while having to analyze only the individual components of these algorithms. Connections between stability, empirical risk minimization and differential privacy in the context of learnability have been recently explored in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Numerous techniques have been developed by statisticians to address common special cases of adaptive data analysis. Most of them address a single round of adaptivity such as variable selection followed by regression on selected variables or model selection followed by testing and are optimized for specific inference procedures (the literature is too vast to adequately cover here, see Ch. 7 in <ref type="bibr" target="#b11">[12]</ref> for a textbook introduction and <ref type="bibr" target="#b19">[20]</ref> for a survey of some recent work). In contrast, our framework addresses multiple stages of adaptive decisions, possible lack of a predetermined analysis protocol and is not restricted to any specific procedures.</p><p>Finally, inspired by our work, Blum and Hardt <ref type="bibr" target="#b1">[2]</ref> showed how to reuse the holdout set to maintain an accurate leaderboard in a machine learning competition that allows the participants to submit adaptively chosen models in the process of the competition (such as those organized by Kaggle Inc.). Their analysis also relies on the description length-based technique we used to analyze SparseValidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Max-Information</head><p>Preliminaries: In the discussion below log refers to binary logarithm and ln refers to the natural logarithm. For two random variables X and Y over the same domain</p><formula xml:id="formula_3">X the max-divergence of X from Y is defined as D ∞ (X Y ) = log max x∈X P[X=x] P[Y =x] . δ-approximate max-divergence is defined as D δ ∞ (X Y ) = log max O⊆X , P[X∈O]&gt;δ P[X ∈ O] -δ P[Y ∈ O]</formula><p>.</p><p>Definition 1. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> A randomized algorithm A with domain X n for n &gt; 0 is (ε, δ)-differentially private if for all pairs of datasets that differ in a single element S, S ∈ X n : D δ ∞ (A(S) A(S )) ≤ log(e ε ). The case when δ = 0 is sometimes referred to as pure differential privacy, and in this case we may say simply that A is ε-differentially private.</p><p>Consider two algorithms A : X n → Y and B : X n × Y → Y that are composed adaptively and assume that for every fixed input y ∈ Y, B generalizes for all but fraction β of datasets. Here we are speaking of generalization informally: our definitions will support any property of input y ∈ Y and dataset S. Intuitively, to preserve generalization of B we want to make sure that the output of A does not reveal too much information about the dataset S. We demonstrate that this intuition can be captured via a notion of max-information and its relaxation approximate max-information.</p><p>For two random variables X and Y we use X × Y to denote the random variable obtained by drawing X and Y independently from their probability distributions. Definition 2. Let X and Y be jointly distributed random variables. The max-information between X and Y is defined as</p><formula xml:id="formula_4">I ∞ (X; Y ) = D ∞ ((X, Y ) X × Y ). The β-approximate max-information is defined as I β ∞ (X; Y ) = D β ∞ ((X, Y ) X × Y ).</formula><p>In our use (X, Y ) is going to be a joint distribution (S, A(S)), where S is a random n-element dataset and A is a (possibly randomized) algorithm taking a dataset as an input. Definition 3. We say that an algorithm A has β-approximate max-information of k if for every distribution S over n-element datasets, I β ∞ (S; A(S)) ≤ k, where S is a dataset chosen randomly according to S. We denote this by</p><formula xml:id="formula_5">I β ∞ (A, n) ≤ k.</formula><p>An immediate corollary of our definition of approximate max-information is that it controls the probability of "bad events" that can happen as a result of the dependence of A(S) on S. Theorem 4. Let S be a random dataset in X n and A be an algorithm with range Y such that for some β ≥ 0,</p><formula xml:id="formula_6">I β ∞ (S; A(S)) = k. Then for any event O ⊆ X n × Y, P[(S, A(S)) ∈ O] ≤ 2 k • P[S × A(S) ∈ O] + β.</formula><p>In particular, P[(S,</p><formula xml:id="formula_7">A(S)) ∈ O] ≤ 2 k • max y∈Y P[(S, y) ∈ O] + β.</formula><p>We remark that mutual information between S and A(S) would not suffice for ensuring that bad events happen with tiny probability. For example mutual information of k allows P[(S, A(S)) ∈ O] to be as high as k/(2 log(1/δ)), where</p><formula xml:id="formula_8">δ = P[S × A(S) ∈ O].</formula><p>Approximate max-information satisfies the following adaptive composition property: Lemma 5. Let A : X n → Y be an algorithm such that I β1 ∞ (A, n) ≤ k 1 , and let B : X n × Y → Z be an algorithm such that for every y ∈ Y, B(•, y) has β 2 -approximate max-information k 2 . Let C : X n → Z be defined such that C(S) = B(S, A(S)).</p><formula xml:id="formula_9">Then I β1+β2 ∞ (C, n) ≤ k 1 + k 2 .</formula><p>Bounds on Max-information: Description length k gives the following bound on max-information. Theorem 6. Let A be a randomized algorithm taking as an input an n-element dataset and outputting a value in a finite set Y. Then for every β &gt; 0,</p><formula xml:id="formula_10">I β ∞ (A, n) ≤ log(|Y|/β).</formula><p>Next we prove a simple bound on max-information of differentially private algorithms that applies to all distributions over datasets. Theorem 7. Let A be an -differentially private algorithm. Then</p><formula xml:id="formula_11">I ∞ (A, n) ≤ log e • n.</formula><p>Finally, we prove a stronger bound on approximate max-information for datasets consisting of i.i.d. samples using the technique from <ref type="bibr" target="#b6">[7]</ref>. Theorem 8. Let A be an ε-differentially private algorithm with range Y. For a distribution P over X , let S be a random variable drawn from P n . Let Y = A(S) denote the random variable output by A on input S. Then for any β &gt; 0, I β ∞ (S; A(S)) ≤ log e(ε 2 n/2 + ε n ln(2/β)/2).</p><p>One way to apply a bound on max-information is to start with a concentration of measure result which ensures that the estimate of predictor's accuracy is correct with high probability when the predictor is chosen independently of the samples. For example for a loss function with range [0, 1], Hoeffding's bound implies that for a dataset consisting of i.i.d. samples the empirical estimate is not within τ of the true accuracy with probability ≤ 2e -2τ 2 n . Now, given a bound of log e • τ 2 n on β-approximate information of the algorithm that produces the estimator, Thm. 4 implies that the produced estimate is not within τ of the true accuracy with probability ≤ 2 log e•τ 2 n • 2e -2τ 2 n + β ≤ 2e -τ 2 n + β. Thm. 7 implies that any τ 2 -differentially private algorithm has max-information of at most log e • τ 2 n. For a dataset consisting of i.i.d. samples Thm. 8 implies that a τ -differentially private algorithm has β-approximate max-information of 1.25 log e • τ 2 n for β = 2e -τ 2 n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reusable Holdout</head><p>We describe two simple algorithms that enable validation of analyst's queries in the adaptive setting. Thresholdout: Our first algorithm Thresholdout follows the approach in <ref type="bibr" target="#b6">[7]</ref> where differentially private algorithms are used to answer adaptively chosen statistical queries. This approach can also be applied to any low-sensitivity functions of the dataset but for simplicity we present the results for statistical queries. Here we address an easier problem in which the analyst's queries only need to be answered when they overfit. Also, unlike in <ref type="bibr" target="#b6">[7]</ref>, the analyst has full access to the training set and the holdout algorithm only prevents overfitting to holdout dataset. As a result, unlike in the general query answering setting, our algorithm can efficiently validate an exponential in n number of queries as long as a relatively small number of them overfit.</p><p>For a function φ : X → R and a dataset S = (x 1 , . ) is below a certain threshold T + η. Here, T is a fixed number such as 0.01 and η is a Laplace noise variable whose standard deviation needs to be chosen depending on the desired guarantees (The Laplace distribution is a symmetric exponential distribution.) If the difference is below the threshold, then the algorithm returns E St <ref type="bibr">[φ]</ref>. If the difference is above the threshold, then the algorithm returns E S h [φ] + ξ for another Laplacian noise variable ξ. Each time the difference is above threshold the "overfitting" budget B is reduced by one. Once it is exhausted, Thresholdout stops answering queries. We provide the pseudocode of Thresholdout below.</p><formula xml:id="formula_12">Input: Training set S t , holdout set S h , threshold T, noise rate σ, budget B 1. sample γ ∼ Lap(2 • σ); T ← T + γ 2. For each query φ do (a) if B &lt; 1 output "⊥" (b) else i. sample η ∼ Lap(4 • σ) ii. if |E S h [φ] -E St [φ]| &gt; T + η A. sample ξ ∼ Lap(σ), γ ∼ Lap(2 • σ) B. B ← B -1 and T ← T + γ C. output E S h [φ] + ξ iii. else output E St [φ].</formula><p>We now establish the formal generalization guarantees that Thresholdout enjoys. Theorem 9. Let β, τ &gt; 0 and m ≥ B &gt; 0. We set T = 3τ /4 and σ = τ /(96 ln(4m/β)). Let S denote a holdout dataset of size n drawn i.i.d. from a distribution P and S t be any additional dataset over X . Consider an algorithm that is given access to S t and adaptively chooses functions φ 1 , . . . , φ m while interacting with Thresholdout which is given datasets S, S t and values σ, B, T . For every i ∈ [m], let a i denote the answer of Thresholdout on function φ i : X → [0, 1]. Further, for every i ∈ [m], we define the counter of overfitting Z</p><formula xml:id="formula_13">i . = |{j ≤ i : |P[φ j ] -E St [φ j ]| &gt; τ /2}| . Then P [∃i ∈ [m], Z i &lt; B &amp; |a i -P[φ i ]| ≥ τ ] ≤ β whenever n ≥ n 0 = O ln(m/β) τ 2</formula><p>• min{B, B ln(ln(m/β)/τ )}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SparseValidate:</head><p>We now present a general algorithm for validation on the holdout set that can validate many arbitrary queries as long as few of them fail the validation. More formally, our algorithm allows the analyst to pick any Boolean function of a dataset ψ (or even any algorithm that outputs a single bit) and provides back the value of ψ on the holdout set ψ(S h ). SparseValidate has a budget m for the total number of queries that can be asked and budget B for the number of queries that returned 1. Once either of the budgets is exhausted, no additional answers are given. We now give a general description of the guarantees of SparseValidate. Theorem 10. Let S denote a randomly chosen holdout set of size n. Let A be an algorithm that is given access to SparseValidate(m, B) and outputs queries ψ 1 , . . . , ψ m such that each ψ i is in some set Ψ i of functions from X n to {0, 1}. Assume that for every i ∈ [m] and ψ i ∈ Ψ i , P[ψ i (S) = 1] ≤ β i . Let ψ i be the random variable equal to the i'th query of A on S. Then</p><formula xml:id="formula_14">P[ψ i (S) = 1] ≤ i • β i , where i = min{i-1,B} j=0 i j ≤ m B .</formula><p>In this general formulation it is the analyst's responsibility to use the budgets economically and pick query functions that do not fail validation often. At the same time, SparseValidate ensures that (for the appropriate values of the parameters) the analyst can think of the holdout set as a fresh sample for the purposes of validation. Hence the analyst can pick queries in such a way that failing the validation reliably indicates overfitting. An example of the application of SparseValidate for answering statistical and low-sensitivity queries that is based on our analysis can be found in <ref type="bibr" target="#b0">[1]</ref>. The analysis of generalization on the holdout set in <ref type="bibr" target="#b1">[2]</ref> and the analysis of the Median Mechanism we give in the full version also rely on this sparsity-based technique.</p><p>Experiments: In our experiment the analyst is given a d-dimensional labeled data set S of size 2n and splits it randomly into a training set S t and a holdout set S h of equal size. We denote an element of S by a tuple (x, y) where x is a d-dimensional vector and y ∈ {-1, 1} is the corresponding class label. The analyst wishes to select variables to be included in her classifier. For various values of the number of variables to select k, she picks k variables with the largest absolute correlations with the label. However, she verifies the correlations (with the label) on the holdout set and uses only those variables whose correlation agrees in sign with the correlation on the training set and both correlations are larger than some threshold in absolute value. She then creates a simple linear threshold classifier on the selected variables using only the signs of the correlations of the selected variables. A final test evaluates the classification accuracy of the classifier on both the training set and the holdout set.</p><p>In our first experiment, each attribute of x is drawn independently from the normal distribution N (0, 1) and we choose the class label y ∈ {-1, 1} uniformly at random so that there is no correlation between the data point and its label. We chose n = 10, 000, d = 10, 000 and varied the number of selected variables k. In this scenario no classifier can achieve true accuracy better than 50%. Nevertheless, reusing a standard holdout results in reported accuracy of over 63% for k = 500 on both the training set and the holdout set (the standard deviation of the error is less than 0.5%). The average and standard deviation of results obtained from 100 independent executions of the experiment are plotted above. For comparison, the plot also includes the accuracy of the classifier on another fresh data set of size n drawn from the same distribution. We then executed the same algorithm with our reusable holdout. Thresholdout was invoked with T = 0.04 and τ = 0.01 explaining why the accuracy of the classifier reported by Thresholdout is off by up to 0.04 whenever the accuracy on the holdout set is within 0.04 of the accuracy on the training set. We also used Gaussian noise instead of Laplacian noise as it has stronger concentration properties. Thresholdout prevents the algorithm from overfitting to the holdout set and gives a valid estimate of classifier accuracy. Additional experiments and discussion are presented in the full version.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. . , x n ), let E S [φ] . Thresholdout is given access to the training dataset S t and holdout dataset S h and a budget limit B. It allows any query of the form φ : X → [0, 1] and its goal is to provide an estimate of P[φ]. To achieve this the algorithm gives an estimate of E S h [φ] in a way that prevents overfitting of functions generated by the analyst to the holdout set. In other words, responses of Thresholdout are designed to ensure that, with high probability, E S h [φ] is close to P[φ] and hence an estimate of E S h [φ] gives an estimate of the true expectation P[φ]. Given a function φ, Thresholdout first checks if the difference between the average value of φ on the training set S t (or E St [φ]) and the average value of φ on the holdout set S h (or E S h [φ]</figDesc><table><row><cell>= 1 n</cell><cell>n i=1 φ(x i ).</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">More general queries and less generalization error in adaptive data analysis</title>
		<author>
			<persName><forename type="first">Raef</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
		<idno>CoRR, abs/1503.04843</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The ladder: A reliable leaderboard for machine learning competitions</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno>CoRR, abs/1502.04585</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On over-fitting in model selection and subsequent selection bias in performance evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">L C</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2079" to="2107" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient multiple hyperparameter learning for log-linear models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Sheng</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generalization in adaptive data analysis and holdout reuse. CoRR, abs/1506</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Extended abstract to appear in NIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Preserving statistical validity in adaptive data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno>CoRR, abs/1411.2664</idno>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Our data, ourselves: Privacy via distributed noise generation</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROCRYPT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="486" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A note on screening regression equations</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="155" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Preventing false discovery in interactive data analysis is hard</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<ptr target="http://hunch.net/?p=22" />
		<title level="m">Clever methods of overfitting</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the generalization properties of differential privacy</title>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Stemmer</surname></persName>
		</author>
		<idno>CoRR, abs/1504.05800</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">General conditions for predictivity in learning theory</title>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="issue">6981</biblScope>
			<biblScope unit="page" from="419" to="422" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the dangers of cross-validation. an experimental evaluation</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="588" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overfitting in making comparisons between variable selection methods</title>
		<author>
			<persName><forename type="first">Juha</forename><surname>Reunanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1371" to="1382" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learnability, stability and uniform convergence</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2635" to="2670" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Interactive fingerprinting codes and the hardness of preventing false discovery</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1228</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical learning and selective inference</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="7629" to="7634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with differential privacy: Stability, learnability and the sufficiency and necessity of ERM principle</title>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<idno>CoRR, abs/1502.06309</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
