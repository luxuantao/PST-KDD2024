<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2023 AUTOGT: AUTOMATED GRAPH TRANSFORMER ARCHI-TECTURE SEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2023 AUTOGT: AUTOMATED GRAPH TRANSFORMER ARCHI-TECTURE SEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Transformer architectures have been successfully applied to graph data with the advent of Graph Transformer, current design of Graph Transformer still heavily relies on human labor and expertise knowledge to decide proper neural architectures and suitable graph encoding strategies at each Transformer layer. In literature, there have been some works on automated design of Transformers focusing on non-graph data such as texts and images without considering graph encoding strategies, which fail to handle the non-euclidean graph data. In this paper, we study the problem of automated graph Transformer, for the first time. However, solving these problems poses the following challenges: i) how can we design a unified search space for graph Transformer, and ii) how to deal with the coupling relations between Transformer architectures and the graph encodings of each Transformer layer. To address these challenges, we propose Automated Graph Transformer (AutoGT), a neural architecture search framework that can automatically discover the optimal graph Transformer architectures by joint optimization of Transformer architecture and graph encoding strategies. Specifically, we first propose a unified graph Transformer formulation that can represent most of state-of-the-art graph Transformer architectures. Based upon the unified formulation, we further design the graph Transformer search space that includes both candidate architectures and various graph encodings. To handle the coupling relations, we propose a novel encoding-aware performance estimation strategy by gradually training and splitting the supernets according to the correlations between graph encodings and architectures. The proposed strategy can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized graph encodings and architectures. Extensive experiments and ablation studies show that our proposed AutoGT gains sufficient improvement over state-of-the-art hand-crafted baselines on all datasets, demonstrating its effectiveness and wide applicability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, designing Transformer for graph data has attracted intensive research interests <ref type="bibr" target="#b5">(Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b30">Ying et al., 2021)</ref>. As a powerful architecture to extract meaningful information from relational data, the graph Transformers have been successfully applied in natural language processing <ref type="bibr">(Zhang &amp; Zhang, 2020;</ref><ref type="bibr" target="#b0">Cai &amp; Lam, 2020)</ref>, social networks <ref type="bibr">(Hu et al., 2020b)</ref>, chemistry <ref type="bibr" target="#b1">(Chen et al., 2019;</ref><ref type="bibr" target="#b22">Rong et al., 2020)</ref>, and recommendation <ref type="bibr" target="#b25">(Xia et al., 2021)</ref> etc. However, developing the state-of-the-art graph Transformer for downstream tasks is still challenging because it heavily relies on the tedious trial-and-error hand-crafted human design, including determining the best Transformer architecture and the choices of proper graph encoding strategies to utilize, etc. In addition, the inefficient hand-crafted design will also inevitably introduce the human bias, which leads to sub-optimal solutions for developing graph Transformer. In literature, there have been works on automatically searching for the architectures of Transformer, which are designed specifically for data in Natural Language Processing <ref type="bibr" target="#b26">(Xu et al., 2021)</ref> and Computer Vision <ref type="bibr" target="#b3">(Chen et al., 2021b)</ref>. These works only focus on non-graph data without considering the graph encoding strategies which are shown to be very important in capturing graph information <ref type="bibr">(Min et al., 2022a)</ref>, thus failing to handle graph data with non-euclidean properties.</p><p>In this paper, we study the problem of automated graph Transformer for the first time. However, previous work <ref type="bibr">(Min et al., 2022a)</ref> has demonstrated that a good graph Transformer architecture is expected to not only select proper neural architectures for every layer, but also utilize appropriate encoding strategies capable of capturing various meaningful graph structure information to boost graph Transformer performance. Therefore, there exist two key challenges for automated graph Transformer:</p><p>? How to design a unified search space appropriate for graph Transformer? A good graph Transformer needs to handle the non-euclidean graph data, requiring explicit consideration of node relations within the search space, where the architectures as well as the encoding strategies can be incorporated simultaneously.</p><p>? How to conduct encoding-aware architecture search strategy to tackle the coupling relations between Transformer architectures and graph encoding? Although one simple solution may resort to one-shot formulation enabling efficient searching in vanilla Transformer operation space which can change its functionality during supernet training, the graph encoding strategies differ from vanilla Transformer in containing certain meanings related to structure information. How to train an encoding-aware supernet specifically designed for graph is challenging.</p><p>To address these challenges, we propose Automated Graph Transformer, AutoGT, a novel neural architecture search method for graph Transformer. In particular, we propose a unified graph Transformer formulation to cover most of the state-of-the-art graph Transformer architectures in our searching space. Besides the general search space of Transformer with hidden dimension, feed forward dimension, number of attention head, attention head dimension and number of layers, our unified search space introduces two new kinds of augmentation strategies to attain graph information: node attribution augmentation and attention map augmentation. To handle the coupling relations, we further propose a novel encoding-aware performance estimation strategy tailored for graphs. As the encoding strategy and architecture have strong coupling relations when generating results, our AutoGT split the supernet based on the important encoding strategy during evaluation to handle the coupling relations. As such, we propose to gradually train and split the supernets according to the most coupled augmentation, attention map augmentation, using various supernets to evaluate different architectures in our unified searching space, which can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized architecture and encoding. In summary, we made the following contributions:</p><p>? We propose Automated Graph Transformer, AutoGT, a novel neural architecture search framework for graph Transformer, which can automatically discover the optimal graph Transformer architectures for various down-streaming tasks. To the best of our knowledge, AutoGT is the first automated graph Transformer framework.</p><p>? We design a unified search space containing both the Transformer architectures and the essential graph encoding strategies, covering most of the state-of-the-art graph Transformer, which can lead to global optimal for structure information excavation and node information retrieval.</p><p>? We propose an encoding-aware performance estimation strategy tailored for graph to provide a more accurate and consistent performance prediction without bringing heavier computation costs.</p><p>The encoding strategy and the Transformer architecture are jointly optimized to discover the best graph Transformers.</p><p>? The extensive experiments show that our proposed AutoGT model can significantly outperform the state-of-the-art baselines on graph classification tasks over several datasets with different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The Graph Transformer. Graph Transformer, as a category of neural networks, enables Transformer to handle graph data <ref type="bibr">(Min et al., 2022a)</ref>. Several works <ref type="bibr" target="#b5">(Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b30">Ying et al., 2021;</ref><ref type="bibr" target="#b13">Hussain et al., 2021;</ref><ref type="bibr">Zhang et al., 2020;</ref><ref type="bibr" target="#b15">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b23">Shi et al., 2021)</ref> propose to pre-calculate some node positional encoding from graph structure and add them to the node attributes after a linear or embedding layer. Some works <ref type="bibr" target="#b5">(Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b35">Zhao et al., 2021;</ref><ref type="bibr" target="#b30">Ying et al., 2021;</ref><ref type="bibr" target="#b14">Khoo et al., 2020</ref>) also propose to add manually designed graph structural information into the attention matrix in Transformer layers. Others <ref type="bibr" target="#b29">(Yao et al., 2020;</ref><ref type="bibr">Min et al., 2022b)</ref> explore the mask mechanism in the attention matrix, masking the influence of non-neighbor nodes. In particular, UniMP <ref type="bibr" target="#b23">(Shi et al., 2021)</ref> achieves new state-of-the-art results on Open Graph Benchmark <ref type="bibr">(Hu et al., 2020a)</ref> datasets, Graphormer <ref type="bibr" target="#b30">(Ying et al., 2021)</ref> won first place in KDD Cup Challenge on Large-SCale graph classification by encoding various information about graph structures into graph Transformer.</p><p>Neural Architecture Search. Neural architecture search has drawn increasing attention in the past few years <ref type="bibr" target="#b6">(Elsken et al., 2019;</ref><ref type="bibr" target="#b37">Zoph &amp; Le, 2016;</ref><ref type="bibr" target="#b17">Ma et al., 2018;</ref><ref type="bibr" target="#b21">Pham et al., 2018)</ref>. There are many efforts to automate the design of Transformers. <ref type="bibr" target="#b24">(So et al., 2019)</ref> propose the first automated framework for Transformer in neural machine translation tasks. AutoTrans <ref type="bibr" target="#b36">(Zhu et al., 2021)</ref> improves the search efficiency of the NLP Transformer through a one-shot supernet training. NAS-BERT <ref type="bibr" target="#b26">(Xu et al., 2021)</ref> further leverages the neural architecture search for big language model distillation and compression. AutoFormer <ref type="bibr" target="#b3">(Chen et al., 2021b)</ref> migrates the automation of Transformer for vision tasks, where they utilize weight-entanglement to improve the consistency of the supernet training. GLiT <ref type="bibr" target="#b2">(Chen et al., 2021a)</ref> proposes to search both global and local attention for the Vision Transformer using a hierarchical evolutionary search algorithm. Chen et al., <ref type="bibr">(Chen et al., 2021c)</ref> further propose to evolve the search space of the Vision Transformer to solve the exponential explosion problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AUTOMATED GRAPH TRANSFORMER ARCHITECTURE SEARCH (AUTOGT)</head><p>To automatically design graph Transformer architectures, we first unify the formulation of current graph Transformers in Section 3.1. Based on the unified formulation, we design the search space tailored for the graph Transformers in Section 3.2. We propose a novel encoding-aware performance estimation strategy in Section 3.3, and introduce our evolutionary search strategy in Section 3.4. The whole algorithm is presented by Figure <ref type="figure">2</ref>.   <ref type="table" target="#tab_2">1</ref>. The graph specific encoding search space is to decide whether each encoding strategy should be adopted or not and the mask threshold for the attention mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE UNIFIED GRAPH TRANSFORMER FRAMEWORK</head><p>Current representative graph Transformer designs can be regarded as improving the input and attention map in Transformer architecture through various graph encoding strategies. We first introduce the basic Transformer architecture and then show how to combine various graph encoding strategies. </p><formula xml:id="formula_0">Let G = (V, E) denote a graph where V = {v 1 , v 2 , ? ? ?, v n } rep</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">BASIC TRANSFORMER</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a basic Transformer consists of several stacked blocks, with each block containing two modules, namely the multi-head attention (MHA) module and the feed-forward network (FFN) module.</p><p>At block l, the node representation H (l) ? R n?d first goes through the MHA module to interact with each other and pass information through self-attention:</p><formula xml:id="formula_1">A (l) h = softmax Q (l) h K (l) h T ? d k , O (l) h = A (l) h V (l) h ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">A (l) h ? R n?n is the message passing matrix, O<label>(l)</label></formula><p>h is the output of the self-attention mechanism of the h th attention head, h = 1, 2, ? ? ?, Head, Head is the number of attention heads, and</p><formula xml:id="formula_3">K (l) h , Q (l) h , V (l)</formula><p>h ? R n?d k are the key, query, value calculated as:</p><formula xml:id="formula_4">K (l) h , = H (l) W (l) k,h , Q (l) h = H (l) W (l) q,h , V (l) h = H (l) W (l) v,h ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">W (l) k,h , W (l) q,h , W<label>(l)</label></formula><p>v,h ? R d?d k are learnable parameters. Then, the representations of different heads are concatenated and further transformed as:</p><formula xml:id="formula_6">O (l) = (O (l) 1 ? O (l) 2 ? ... ? O (l) Head )W (l) O + H (l) ,<label>(3)</label></formula><p>where l) is the multi-head result. Then, the attended representation will go through the FFN module to further refine the information of each node:</p><formula xml:id="formula_7">W (l) O ? R (d k * Head)?dt is the parameter and O (</formula><formula xml:id="formula_8">H (l+1) = ?(O (l) W (l) 1 )W (l) 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_9">O (l) ? R n?dt is the output, W (l) 1 ? R d k ?d h , W<label>(l)</label></formula><p>2 ? R d h ?d are weight matrices. As for the input of the first block, we concatenate all the node features</p><formula xml:id="formula_10">H (0) = [v 1 , ..., v n ].</formula><p>After L blocks, we obtain the final representation of each node H (L) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">GRAPH ENCODING STRATEGY</head><p>From Section 3.1.1, we can observe that directly using the basic Transformer on graphs can only process node attributes, ignoring important edge attributes and graph topology information in the graph. To make the Transformer architecture aware of the graph structure, several works resort to various graph encoding strategies, which can be divided into two kinds of categories: node attribution augmentation and attention map augmentation.</p><p>The node attribution augmentations take the whole graph G as input and generate the topology-aware features Enc node (G) for each node to directly improve the node representations:</p><formula xml:id="formula_11">H (l) aug = H (l) + Enc node (G).<label>(5)</label></formula><p>On the other hand, the attention map augmentations generate an additional attention map Enc map (G), which represents the relationships of any two nodes and improves the attention map generated by self-attention in Eq equation 1 as:</p><formula xml:id="formula_12">A (l) h,aug = softmax Q (l) h K (l) h T ? d + Encmap(G) .<label>(6)</label></formula><p>Combining node attribution augmentations and attention map augmentations together, our proposed framework is as follows:</p><formula xml:id="formula_13">H (l+1) = ? Concat softmax H (l) aug W (l) q,h (H (l) aug W (l) k,h ) T ? d k + Encmap(G) H (l) aug W (l) v,h W (l) 1 W (l) 2 . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>where</p><formula xml:id="formula_15">H (l) aug = H (l) + Enc node (G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">THE GRAPH TRANSFORMER SEARCH SPACE</head><p>Based on the unified graph Transformer formulation, we propose our unified search space design, which can be decomposed into two parts, i.e., Transformer Architecture space and graph encoding space. Figure <ref type="figure" target="#fig_0">1</ref> shows the unified graph Transformer search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">TRANSFORMER ARCHITECTURE SPACE</head><p>Following Section 3.  The framework of our work. Firstly, we construct the search space for each layer, consisting of the Transformer architecture space (above) and the graph encoding strategy space (below). Then, we carry out our encoding-aware supernet training method in two stages: before splitting, we train a supernet by randomly sampling architectures from the search space, while after splitting, we train multiple subnets (inheriting the weights from the supernet) by randomly sampling architectures with fixed attention map augmentation strategies (except for the attention mask). Finally, we conduct an evolutionary search based on the subnets and obtain our final architecture and results.</p><p>A suitable search space should be not only expressive enough to allow powerful architectures, but also compact enough to enable efficient searches. With this principle in mind, we propose two search spaces for these components with different size ranges. Table <ref type="table" target="#tab_2">1</ref> gives the detailed search space for these two spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">GRAPH ENCODING SPACE</head><p>To fully exploit the potential of the graph encoding strategies, we further determine whether and which graph encoding strategies to use for each layer of the graph Transformer. Specifically, we explore the node attribution augmentations encoding and attention map augmentations encoding as below.</p><p>Node Attribution Augmentations:</p><p>? Centrality Encoding <ref type="bibr" target="#b30">(Ying et al., 2021)</ref>. Use two node embeddings with the same size representing the in-degree and the out-degree of nodes, i.e.,</p><formula xml:id="formula_16">h (l) i = x (l) i + z - deg -(v i ) + z + deg -(v i )<label>(8)</label></formula><p>where h</p><p>(l)</p><p>i is the input embedding in layer l, x i is the input attribution of node i in layer l, and z -and z + are the embedding generated by the in-degree and out-degree.</p><p>? Laplacian Eigenvector <ref type="bibr" target="#b5">(Dwivedi &amp; Bresson, 2020)</ref>. Conducting spectral decomposition of the graph Laplacian matrix:</p><formula xml:id="formula_17">U T ?U = I -D -1/2 A G D -1/2 (9)</formula><p>where A G is the adjacency matrix of graph G, D is the diagonal degree matrix, and U and ? are the eigenvectors and eigenvalues, respectively. We only select the eigenvectors of the k smallest non-zero eigenvalues as the final embedding and concatenate them to the input node attribute matrix for each layer. ? SVD-based Positional Encoding <ref type="bibr" target="#b13">(Hussain et al., 2021)</ref>. Conducting singular value decomposition to the graph adjacency matrix:</p><formula xml:id="formula_18">A G SVD ? U?V T = (U ? ?) ? (V ? ?) T = ? VT (10)</formula><p>where U, V ? R n?r contains the left and right singular vectors of the top r singular values in the diagonal matrix ? ? R r?r . Without loss of generality, we only choose ? as the final embedding since they are highly correlated for symmetric graphs (with differences in signs, to be specific). Similar to the Laplacian eigenvector, we concatenate it to the input node attribute matrix for each layer.</p><p>Attention Map Augmentations Space:</p><p>? Spatial Encoding <ref type="bibr" target="#b30">(Ying et al., 2021)</ref>. Spatial encoding is added to the attention result before softmax:</p><formula xml:id="formula_19">Aij = (hiWQ)(hjWK ) T ? d k + b ?(v i ,v j )<label>(11)</label></formula><p>where ?(v i , v j ) is the length of the shortest path from v i to v j , and b ? R is a weight parameter generated by ?(v i , v j ). ? Edge Encoding <ref type="bibr" target="#b30">(Ying et al., 2021)</ref>. Edge encoding is also added to the attention result before softmax:</p><formula xml:id="formula_20">Aij = (hiWQ)(hjWK ) T ? d k + 1 N N n=1 xe n (w E n ) T (12)</formula><p>where x en is the feature of the n-th edge e n on the shortest path between v i and v j , and w E n is the n-th learnable embedding vector.</p><p>? Proximity-Enhanced Attention <ref type="bibr" target="#b35">(Zhao et al., 2021)</ref>. Edge encoding is also added to the attention result before softmax:</p><formula xml:id="formula_21">Aij = (hiWQ)(hjWK ) T ? d k + ? T ij b (13) where b ? R M ?1 is a learnable parameter, ? ij = Concat(? m (v i , v j )|m ? 0, 1, ? ? ? , M -1) is the structural encoding generated from: ? m (v i , v j ) = ?m [i, j],</formula><p>where ? = Norm(A + I) represents the normalized adjacency matrix. Thus the augmentation denotes the reachable probabilities between nodes. ? Attention Mask <ref type="bibr">(Min et al., 2022b;</ref><ref type="bibr" target="#b29">Yao et al., 2020)</ref>. Attention Mask is added to the attention result before softmax:</p><formula xml:id="formula_22">Aij = (hiWQ)(hjWK ) T ? d k + Maskm(vi, vj) (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>where m is the mask threshold, Mask m (v i , v j ) depends on the relationship between m and ?(v i , v j ), i.e. the shortest path length between v i and v</p><formula xml:id="formula_24">j . When m ? ?(v i , v j ), Mask m (v i , v j ) = 0.</formula><p>Otherwise, Mask m (v i , v j ) is -?, masking the corresponding attention in practical terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ENCODING-AWARE SUPERNET TRAINING</head><p>We next introduce our proposed encoding-aware performance estimation strategy for efficient training.</p><p>Similar to general NAS problems, the graph Transformer architecture search can be formulated as a bi-level optimization problem: </p><formula xml:id="formula_25">a * = argmax a?A Acc val (W * (a), a), s.t.W * (a) = argmin W Ltrain(W, a),<label>(15)</label></formula><formula xml:id="formula_26">? ? ? Ours ? ? ? ? ? ? ?</formula><p>where a ? A is the architecture in the search space A, Acc val stands for the validation accuracy, W represents the learnable weights, and a * and W * (a) denotes the optimal architecture and the optimal weights for the architecture a.</p><p>Following one-shot NAS methods <ref type="bibr" target="#b16">(Liu et al., 2019;</ref><ref type="bibr" target="#b21">Pham et al., 2018)</ref>, we encode all candidate architectures in the search space into a supernet and transform Eq. equation 15 into a two-step optimization <ref type="bibr" target="#b9">(Guo et al., 2020)</ref>:</p><formula xml:id="formula_27">a * = argmax a?A Acc val (W * , a), W * = argmin W Ea?ALtrain(W, a), (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>where W denotes the shared learnable weights in the supernet with its optimal value W * for all the architectures in the search space.</p><p>To further improve the optimization efficiency of the supernet training, we leverage weight entanglement <ref type="bibr" target="#b8">(Guan et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2021b;</ref><ref type="bibr" target="#b9">Guo et al., 2020)</ref> to deeply share the weights of architectures with different hidden sizes. Specifically, for every architecture sampled from the supernet, we use a 0-1 mask to discard unnecessary hidden channels instead of maintaining a new set of weights. In this way, the number of parameters in the supernet will remain the same as the largest (i.e., with the most parameters) model in the search space, thus leading to efficient optimization.</p><p>Although this strategy is fast and convenient, using the same supernet parameters W for all architectures will decrease the consistency between the estimation of the supernet and the ground-truth architecture performance. To improve the consistency and accuracy of supernet, we propose an encoding-aware supernet training strategy. Based on the contribution of coupling of different encoding strategies, we split the search space into different sub-spaces based on whether adopting three kinds of attention map augmentation strategies: spatial encoding, edge encoding, and proximity-enhanced attention. Therefore, there are 2 3 = 8 supernets.</p><p>To be specific, we first train a single supernet for certain epochs and split the supernet into 8 subnets according to the sub-spaces afterward. Then, we continuously train the weights in each subnet W i by only sampling the architecture from the corresponding subspace A i . Experiments to support such a design are provided in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">EVOLUTIONARY SEARCH</head><p>Similar to other NAS research, our proposed graph transformer search space is too large to enumerate. Therefore, we propose to utilize the evolutionary algorithm to efficiently explore the search space to obtain the architecture with optimal accuracy on the validation dataset.</p><p>Specifically, we first maintain a population consisting of T architectures by random sample. Then, we evolve the architectures through our designed mutation and crossover operations. In the mutation operation, we randomly choose from the top-k architectures with the highest performance in the last generation and change its architecture choices with probabilities. In the crossover operation, we randomly select pairs of architectures with the same number of layers from the remaining architectures, and randomly switch their architecture choices.</p><p>Under review as a conference paper at ICLR 2023 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we present detailed experimental results as well as the ablation studies to empirically show the effectiveness of our proposed AutoGT.</p><p>Datasets and Baselines. We first consider six graph classification datasets from Deep Graph Kernels Benchmark( <ref type="bibr" target="#b28">(Yanardag &amp; Vishwanathan, 2015)</ref>) and TUDataset <ref type="bibr" target="#b20">(Morris et al., 2020)</ref>, namely COX2_MD, BZR_MD, PTC_FM, DHFR_MD, PROTEINS, and DBLP. We also adopt three datasets from Open Graph Benchmark (OGB) <ref type="bibr">(Hu et al., 2020a)</ref>, including OGBG-MolHIV, OGBG-MolBACE, and OGBG-MolBBBP. The task is to predict the label of each graph using node/edge attributes and graph structures. The detailed statistics of the datasets are shown in Table <ref type="table" target="#tab_9">6</ref> in the appendix.</p><p>We compare AutoGT with state-of-the-art hand-crafted baselines, including GIN <ref type="bibr" target="#b27">(Xu et al., 2019)</ref>, DGCNN <ref type="bibr" target="#b34">(Zhang et al., 2018)</ref>, DiffPool <ref type="bibr" target="#b31">(Ying et al., 2018)</ref>, GraphSAGE <ref type="bibr" target="#b10">(Hamilton et al., 2017)</ref>, and Graphormer <ref type="bibr" target="#b30">(Ying et al., 2021)</ref>. Notice that Graphormer is a state-of-the-art graph Transformer architecture that won first place in the graph classification task of KDD Cup 2021 (OGB-LSC).</p><p>For all the datasets, we follow Errica et al., <ref type="bibr" target="#b7">(Errica et al., 2020)</ref> to utilize 10-fold cross-validation for all the baselines and our proposed method. All the hyper-parameters and training strategies of baselines are implemented according to the publicly available codes <ref type="bibr" target="#b7">(Errica et al., 2020)</ref> <ref type="foot" target="#foot_0">1</ref> .</p><p>Implementation Details. Recall that our proposed architecture space has two variants, a larger AutoGT(L = 8, d = 128) and a smaller AutoGT base (L = 4, d = 32). In our experiments, we adopt the smaller search space for five relatively small datasets, i.e., all datasets except DBLP, and the larger search space for DBLP. We use the Adam optimizer, and the learning rate is 3e -4. For the smaller/larger datasets, we set the number of iterations to split (i.e., T s in Algorithm 1 in Appendix) as 50/6 and the maximum number of iterations (i.e., T m in Algorithm 1) as 200/50. The batch size is 128. The hyperparameters of these baselines are kept consistent with our method for a fair comparison.</p><p>We also report the results of our unified framework in Section 3.1, i.e. mixing all the encodings in our search space with the supernet but without the search part, denoted as GT(Graph Transformer).</p><p>Experimental Results. We report the results of AutoGT and baselines in Table <ref type="table" target="#tab_4">3</ref>. We can make the following observations. First, AutoGT consistently outperforms all the existing hand-crafted methods on all datasets, demonstrating the effectiveness of our proposed method. Graphormer shows remarkable performance and achieves the second-best results on three datasets, showing the great potential of Transformer architectures in processing graph data. However, since Graphormer is a manually designed architecture and cannot adapt to different datasets, it fails to be as effective as our proposed automatic solution. Lastly, GT, our proposed unified framework, fails to show strong performance in most cases. The results indicate that simply mixing different graph Transformers cannot produce satisfactory results, demonstrating the importance of searching for effective architectures to handle different datasets.</p><p>We also conduct experiments on Open Graph Benchmark (OGB) <ref type="bibr">(Hu et al., 2020a)</ref>. On the three binary classification datasets of OGB, we report the AUC score of our method and all the baselines. The results also show that our method outperforms all the hand-crafted baselines on these datasets. Ablation Studies. We verify the effectiveness of the proposed encoding-aware supernet training strategy by reporting the results on the PROTEINS dataset, while other datasets show similar patterns.</p><p>To show the importance of considering encoding strategies when training the supernet, we design two variants of AutoGT and compare the results:</p><p>? One-Shot. We only train a single supernet and use it to evaluate all the architectures.</p><p>? Positional-Aware. We also split up the supernet into 8 subnets but based on three node attribute augmentations instead of the three attention map augmentation as in AutoGT. The results of AutoGT and two variants are shown in Table 5. From the table, we can observe that, compared with the result of one-shot NAS, positional-aware and AutoGT methods achieve different levels of improvement. Further comparing the accuracy gain, we find that the result of AutoGT (1.25%) is nearly 5 times larger than the result of positional-aware (0.27%), even though both methods adopt 8 subnets. We attribute the significant difference in accuracy gain from supernet splitting to the different degrees of coupling of graph encoding strategies with the Transformer architecture. For example, the dimensionality of node attribution augmentation is the same as the number of nodes, while the attention map augmentation has a quadratic dimensionality, resulting in different coupling degrees. Our proposed encoding-aware performance estimation based on three attention map augmentation strategies is shown to be effective in practice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET</head><p>We provide the statistics of the adopted datasets in Table <ref type="table" target="#tab_9">6</ref> and<ref type="table" target="#tab_10">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL EXPERIMENTS</head><p>In Table <ref type="table" target="#tab_4">3</ref>, the results on COX2_MD and BZR_MD show larger standard deviations than other datasets. One plausible reason is that the number of graphs of these two datasets are relatively small, so that the results of the model can be sensitive to dataset splits. To obtain more convincing results on these two datasets, we conduct additional experiments by still utilizing 10-fold cross-validation for all the baselines and our proposed method, and repeat the 10-fold cross-validation with 10 random seeds. We report the results in Table <ref type="table" target="#tab_11">8</ref>. The results are consistent with Table <ref type="table" target="#tab_4">3</ref>, i.e., our method consistently outperforms other baselines, while the standard deviations are considerably smaller by adopting more repeated experiments.</p><p>In addition, to further explore how the number of supernets affects our proposed method, we carry out experiments with 1, 2, 4, 8, 16 supernets on the PROTEINS dataset, and report our results in Table <ref type="table" target="#tab_12">9</ref>. We can observe that as the number of subnets increases, the performance of our method increases. One possible reason is that more well-trained subnets can bring more consistent performance estimation results, which improves performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The unified graph Transformer search space. It consists of the Transformer architecture space and the graph specific encoding space. The Transformer architecture search space is detailed in Table1. The graph specific encoding search space is to decide whether each encoding strategy should be adopted or not and the mask threshold for the attention mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>resents the set of nodes and E = {e 1 , e 2 , ? ? ?, e m } represents the set of edges, and denote n = |V | and m = |E| as the number of nodes and edges, respectively. Let v i , i ? {1, ..., n} represents the features of node v i , and e j , j ? {1, ..., m} represents the features of edge e j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure2: The framework of our work. Firstly, we construct the search space for each layer, consisting of the Transformer architecture space (above) and the graph encoding strategy space (below). Then, we carry out our encoding-aware supernet training method in two stages: before splitting, we train a supernet by randomly sampling architectures from the search space, while after splitting, we train multiple subnets (inheriting the weights from the supernet) by randomly sampling architectures with fixed attention map augmentation strategies (except for the attention mask). Finally, we conduct an evolutionary search based on the subnets and obtain our final architecture and results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The Transformer Architecture Search Space for AutoGT base and AutoGT.</figDesc><table><row><cell></cell><cell cols="2">AutoGT base</cell><cell cols="2">AutoGT</cell></row><row><cell></cell><cell>Choices</cell><cell>Supernet Size</cell><cell>Choices</cell><cell>Supernet Size</cell></row><row><cell>#Layers</cell><cell>{2,3,4}</cell><cell>4</cell><cell>{5,6,7,8}</cell><cell>8</cell></row><row><cell>Input Dimension d</cell><cell>{24,28,32}</cell><cell>32</cell><cell>{96,112,128}</cell><cell>128</cell></row><row><cell>Intermediate Dimension dt</cell><cell>{24,28,32}</cell><cell>32</cell><cell>{96,112,128}</cell><cell>128</cell></row><row><cell>Hidden Dimension d h</cell><cell>{24,28,32}</cell><cell>32</cell><cell>{96,112,128}</cell><cell>128</cell></row><row><cell>#Attention Heads</cell><cell>{2,3,4}</cell><cell>4</cell><cell>{6,7,8}</cell><cell>8</cell></row><row><cell>Attention Head Dimension d k</cell><cell>{6,8}</cell><cell>8</cell><cell>{12,14,16}</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our proposed unified framework with state-of-the-art graph Transformer models. CE, LPE, SVD, SE, EE, PMA, Mask denote Centrality Encoding, Laplacian Eigenvector, SVD-based Positional Encoding, Spatial Encoding, Edge Encoding, Proximity-Enhanced Attention, and Attention Mask respectively.</figDesc><table><row><cell></cell><cell cols="2">CE LPE SVD SE EE PMA Mask</cell></row><row><cell>EGT (Hussain et al., 2021)</cell><cell>?</cell><cell></cell></row><row><cell>Gophormer (Zhao et al., 2021)</cell><cell>?</cell><cell></cell></row><row><cell>Graph Trans (Dwivedi &amp; Bresson, 2020)</cell><cell>?</cell><cell>?</cell></row><row><cell>Graphormer (Ying et al., 2021)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of AutoGT against state-of-the-art hand-crafted baselines. We report the average accuracy (%) and the standard deviation on all the datasets. Out-of-time (OOT) indicates the method cannot produce results in 1 GPU day.</figDesc><table><row><cell>Dataset</cell><cell cols="4">COX2_MD BZR_MD PTC_FM DHFR_MD PROTEINS</cell><cell>DBLP</cell></row><row><cell>GIN</cell><cell>45.8214.35</cell><cell>59.6814.65 57.878.86</cell><cell>62.888.26</cell><cell>73.764.61</cell><cell>91.180.42</cell></row><row><cell>DGCNN</cell><cell>54.8118.51</cell><cell>62.7420.59 62.173.62</cell><cell>63.895.91</cell><cell>72.683.75</cell><cell>91.570.54</cell></row><row><cell>DiffPool</cell><cell>51.4514.28</cell><cell>65.0114.74 60.165.87</cell><cell>61.069.42</cell><cell>73.313.75</cell><cell>OOT</cell></row><row><cell>GraphSAGE</cell><cell>49.5912.80</cell><cell>57.4313.50 64.173.28</cell><cell>66.922.35</cell><cell>67.196.97</cell><cell>51.010.02</cell></row><row><cell>Graphormer</cell><cell>56.3915.03</cell><cell>63.9412.58 64.887.58</cell><cell>64.887.58</cell><cell>75.293.10</cell><cell>89.362.31</cell></row><row><cell>GT(ours)</cell><cell>54.4416.84</cell><cell>63.3311.67 64.182.60</cell><cell>65.685.64</cell><cell>73.943.78</cell><cell>90.671.01</cell></row><row><cell>AutoGT(ours)</cell><cell>59.7223.26</cell><cell>65.9210.00 65.603.71</cell><cell>68.225.02</cell><cell>77.173.40</cell><cell>91.660.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of AutoGT against state-of-the-art hand-crafted baselines. We report the area under the curve (AUC) [%] and the standard deviation on all the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">OGBG-MolHIV OGBG-MolBACE OGBG-MolBBBP</cell></row><row><cell>GIN</cell><cell>71.112.57</cell><cell>70.424.78</cell><cell>63.371.81</cell></row><row><cell>DGCNN</cell><cell>69.972.16</cell><cell>75.622.64</cell><cell>60.921.78</cell></row><row><cell>DiffPool</cell><cell>74.581.71</cell><cell>73.874.50</cell><cell>66.686.08</cell></row><row><cell>GraphSAGE</cell><cell>67.823.67</cell><cell>72.911.24</cell><cell>64.193.50</cell></row><row><cell>Graphormer</cell><cell>71.892.66</cell><cell>76.421.67</cell><cell>66.520.74</cell></row><row><cell>AutoGT(ours)</cell><cell>74.951.02</cell><cell>76.701.42</cell><cell>67.291.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Time Cost. We further show the time comparison of our proposed AutoGT with hand-crafted graph transformer Graphormer. On OGBG-MolHIV dataset, both Graphormer and AutoGT cost 2 minutes for one epoch on single GPU. The default Graphormer is trained for 300 epochs, which costs 10 hours to obtain the result of one random seed. For AutoGT, we train shared supernet for 50 epochs, 8 supernets inherit, and continue to train for 150 epochs. So the training process costs totally 1250 epochs with 40 hours. And on the evolutionary search stage, we evaluate 2000 architectures' inheriting weight performances, which costs about 900 epochs with 30 hours. In summary, the total time cost for our proposed AutoGT is only 7 times total time cost for a hand-crafted graph transformer Graphormer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">: The ablation study on the effec-</cell></row><row><cell cols="2">tiveness of the proposed encoding-aware</cell></row><row><cell cols="2">supernet training strategy. We report the</cell></row><row><cell cols="2">average accuracy[%] with the variance</cell></row><row><cell>on PROTEINS.</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>One-Shot</cell><cell>75.923.10</cell></row><row><cell cols="2">Positional-Aware 76.193.42</cell></row><row><cell>AutoGT</cell><cell>77.173.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>In this paper, we propose AutoGT, a neural architecture search framework for graph Transformer. We design a search space tailored for graph Transformer architectures, and an encoding-aware supernet training strategy to provide reliable graph Transformer supernets considering various graph encoding strategies. Our method integrates the existing graph Transformer into a unified framework, where different Transformer encoding can enhance each other. Extensive experiments on six datasets demonstrate that our proposed AutoGT consistently outperforms state-of-the-art baselines on all datasets, demonstrating its strength on various graph tasks. Initialize. Supernet weights W, subnet weights Wi, search space A, subspace Ai, the split iteration Ts, the max iteration Tm, a dataset D. Output. Subnets with weights Wi.</figDesc><table><row><cell cols="2">A TRAINING PROCEDURE</cell></row><row><cell cols="2">We list the training procedure of our method in Algorithm 1.</cell></row><row><cell cols="2">Algorithm 1 Our proposed encoding-aware supernet training strategy</cell></row><row><cell cols="2">1: 2: for t = 1 : Ts do</cell></row><row><cell>3:</cell><cell>Sample architecture and encoding strategies a ? A.</cell></row><row><cell>4:</cell><cell>Sample a batch of graph data Ds ? D.</cell></row><row><cell>5:</cell><cell>Calculate the training loss Ltrain over the sampled data.</cell></row><row><cell>6:</cell><cell>Update the supernet weights W through gradient descents.</cell></row><row><cell cols="2">7: end for</cell></row><row><cell cols="2">8: for i = 1 : n do</cell></row><row><cell>9:</cell><cell>Let subnet Wi inherit the weights from supernet W.</cell></row><row><cell>10:</cell><cell>for t = Ts : Tm do</cell></row><row><cell>11:</cell><cell>Sample architectures and encoding strategies from the subspace a ? Ai.</cell></row><row><cell>12:</cell><cell>Sample a batch of graph data Ds ? D.</cell></row><row><cell>13:</cell><cell>Calculate the training loss Ltrain over the sampled batch.</cell></row><row><cell>14:</cell><cell>Update the subnet weights Wi through gradient descents.</cell></row><row><cell>15:</cell><cell>end for</cell></row><row><cell cols="2">16: end for</cell></row><row><cell>17:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Statistics of graph classification datasets (precision) used to compare AutoGT with baselines. We adopt five datasets with relatively small numbers of graphs (upper part) and one dataset with a larger size (lower part) to demonstrate the efficiency of the proposed AutoGT.</figDesc><table><row><cell>Dataset</cell><cell cols="6">#Graph #Class #Avg. Nodes #Avg. Edges # Node Feature # Edge Feature</cell></row><row><cell>COX2_MD</cell><cell>303</cell><cell>2</cell><cell>26.28</cell><cell>335.12</cell><cell>7</cell><cell>5</cell></row><row><cell>BZR_MD</cell><cell>306</cell><cell>2</cell><cell>21.3</cell><cell>225.06</cell><cell>8</cell><cell>5</cell></row><row><cell>PTC_FM</cell><cell>349</cell><cell>2</cell><cell>14.11</cell><cell>14.48</cell><cell>18</cell><cell>4</cell></row><row><cell>DHFR_MD</cell><cell>393</cell><cell>2</cell><cell>23.87</cell><cell>283.01</cell><cell>7</cell><cell>5</cell></row><row><cell>PROTEINS</cell><cell>1,133</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell><cell>3</cell><cell>0</cell></row><row><cell>DBLP</cell><cell>19,456</cell><cell>2</cell><cell>10.48</cell><cell>19.65</cell><cell>41,325</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Statistics of graph classification datasets (AUC) used to compare AutoGT with baselines. We adopt two datasets with relatively small numbers of graphs (upper part) and one dataset with a larger size (lower part) to demonstrate the efficiency of the proposed AutoGT.</figDesc><table><row><cell>Dataset</cell><cell cols="6">#Graph #Class #Avg. Nodes #Avg. Edges # Node Feature # Edge Feature</cell></row><row><cell>OGBG-MolHIV</cell><cell>1,513</cell><cell>2</cell><cell>25.51</cell><cell>27.47</cell><cell>9</cell><cell>3</cell></row><row><cell>OGBG-MolBACE</cell><cell>2,039</cell><cell>2</cell><cell>34.09</cell><cell>36.86</cell><cell>9</cell><cell>3</cell></row><row><cell cols="2">OGBG-MolBBBP 41,127</cell><cell>2</cell><cell>24.06</cell><cell>25.95</cell><cell>9</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of AutoGT against state-of-the-art hand-crafted baselines. We report the average accuracy (%) and the standard deviation on all the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">COX2_MD BZR_MD</cell></row><row><cell>GIN</cell><cell>57.22 9.74</cell><cell>62.64 8.23</cell></row><row><cell>DGCNN</cell><cell>60.33 7.56</cell><cell>64.91 9.42</cell></row><row><cell>DiffPool</cell><cell>59.52 8.20</cell><cell>64.84 8.51</cell></row><row><cell>GraphSAGE</cell><cell>53.62 6.95</cell><cell>55.83 8.27</cell></row><row><cell>Graphormer</cell><cell>59.22 7.04</cell><cell>64.53 9.43</cell></row><row><cell>AutoGT(ours)</cell><cell>63.45 8.04</cell><cell>67.18 9.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of AutoGT with the different number of supernets. We report the average accuracy (%) and the standard deviation on the datasets.</figDesc><table><row><cell>Dataset</cell><cell>PROTEINS</cell></row><row><cell>1 supernet</cell><cell>75.92 3.10</cell></row><row><cell>2 supernet</cell><cell>76.73 3.25</cell></row><row><cell>4 supernet</cell><cell>76.91 3.35</cell></row><row><cell>8 supernet</cell><cell>77.17 3.40</cell></row><row><cell>16 supernet</cell><cell>77.27 3.65</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/diningphil/gnn-comparison</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Benson</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12712</idno>
		<title level="m">Path-augmented graph transformer network</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glit: Neural architecture search for global and local image transformer</title>
		<author>
			<persName><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021a</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021b</date>
			<biblScope unit="page" from="12270" to="12280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Searching the search space of vision transformer</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno>CoRR, abs/2111.14725</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot neural channel search: What works and what&apos;s next</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on NAS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="volume">12361</biblScope>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Ulrike Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 20-24, 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2, 2020b</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Edge-augmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Shamim Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharmashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretable rumor detection in microblogs by attending to user interactions</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8783" to="8790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runfa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08455</idno>
		<title level="m">Transformer for graphs: An overview from architecture perspective</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Masked transformer for neighhourhood-aware click-through rate prediction</title>
		<author>
			<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-27">19-27 August 2021. 2021</date>
			<biblScope unit="page" from="1548" to="1554" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4486" to="4493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
		<editor>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Beng</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chin</forename><surname>Ooi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">August 14-18, 2021. 2021</date>
			<biblScope unit="page" from="1933" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer for graph-tosequence learning</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OeWooOxFwDa" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text graph transformer for document classification</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gophormer: Ego-graph transformer for node classification</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13094</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autotrans: Automating transformer design via reinforced architecture search</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing -10th CCF International Conference, NLPCC 2021</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Qingdao, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">October 13-17, 2021. 2021</date>
			<biblScope unit="volume">13028</biblScope>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
