<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spherical and Hyperbolic Embeddings of Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
							<email>wilson@cs.york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<settlement>Heslington</settlement>
									<region>York</region>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<addrLine>Oxford Road</addrLine>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE, El _ zbieta Pe ¸kalska</roleName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<settlement>Heslington</settlement>
									<region>York</region>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<addrLine>Oxford Road</addrLine>
									<postCode>M13 9PL</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Robert</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
							<email>r.duin@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of York</orgName>
								<address>
									<postCode>YO10 5DD</postCode>
									<settlement>Heslington</settlement>
									<region>York</region>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<postBox>PO Box 5031</postBox>
									<postCode>2628 CJ</postCode>
									<settlement>Delft</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spherical and Hyperbolic Embeddings of Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0F874D5543C3B65B6242A19D75FBDB93</idno>
					<idno type="DOI">10.1109/TPAMI.2014.2316836</idno>
					<note type="submission">received 7 Dec. 2011; revised 9 Oct. 2013; accepted 3 Apr. 2014. Date of publication 10 Apr. 2014; date of current version 9 Oct. 2014. Recommended for acceptance by M. Belkin.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Embedding</term>
					<term>non-euclidean</term>
					<term>spherical</term>
					<term>hyperbolic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many computer vision and pattern recognition problems may be posed as the analysis of a set of dissimilarities between objects. For many types of data, these dissimilarities are not euclidean (i.e., they do not represent the distances between points in a euclidean space), and therefore cannot be isometrically embedded in a euclidean space. Examples include shape-dissimilarities, graph distances and mesh geodesic distances. In this paper, we provide a means of embedding such non-euclidean data onto surfaces of constant curvature. We aim to embed the data on a space whose radius of curvature is determined by the dissimilarity data. The space can be either of positive curvature (spherical) or of negative curvature (hyperbolic). We give an efficient method for solving the spherical and hyperbolic embedding problems on symmetric dissimilarity data. Our approach gives the radius of curvature and a method for approximating the objects as points on a hyperspherical manifold without optimisation. For objects which do not reside exactly on the manifold, we develop a optimisation-based procedure for approximate embedding on a hyperspherical manifold. We use the exponential map between the manifold and its local tangent space to solve the optimisation problem locally in the euclidean tangent space. This process is efficient enough to allow us to embed data sets of several thousand objects. We apply our method to a variety of data including time warping functions, shape similarities, graph similarity and gesture similarity data. In each case the embedding maintains the local structure of the data while placing the points in a metric space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç 1 INTRODUCTION</head><p>T HERE are many problems in computer vision and pattern recognition which can be posed in terms of a set of measured dissimilarities between objects <ref type="bibr" target="#b14">[15]</ref>. In other words, there are no intrinsic features or vectors associated with the objects at hand, but instead there is a set of dissimilarities between the objects. Some examples include shapesimilarities, gesture interpretation, mesh geodesic distances and graph comparison, but there are many more. There are two challenges to the analysis of such data. First, since they are not characterized by pattern-vectors, the objects cannot be clustered or classified using standard pattern recognition techniques. For example, pairwise rather than central clustering techniques must be used on such data. Alternatively, the objects can be embedded into a vector-space using techniques such as multidimensional scaling (MDS) <ref type="bibr" target="#b5">[6]</ref> or IsoMap <ref type="bibr" target="#b30">[31]</ref>. Once embedded in such a space then the objects can be characterized by their embedding co-ordinate vectors, and analyzed in a conventional manner.</p><p>Most embedding methods produce an embedding that is euclidean. However, dissimilarity data cannot always be embedded exactly into a euclidean space. This is the case when the symmetric similarity matrix (the equivalent of a kernel matrix) contains negative eigenvalues. Examples of such dissimilarity data occur in a number of data sources furnished by applications in computer vision. For instance, shape-similarity measures and graph-similarity measures are rarely euclidean. Previous work <ref type="bibr" target="#b21">[22]</ref> has shown that there is potentially useful information in the non-euclidean component of the dissimilarities. Such data can be embedded in a pseudo-euclidean space, i.e., one where some dimensions are characterized by negative eigenvalues and the squared-distance between objects has both positive and negative components which sum together to give the total distance. A pseudo-euclidean (pE) space is however nonmetric, which makes it difficult to correctly compute geometric properties. An alternative, which we explore in this paper, is to embed the data on a curved manifold, which is metric but non-euclidean. The use of a metric space is important because it provides the possibility of defining classifiers based on geometric concepts such as boundaries, regions and margins.</p><p>Riemannian manifolds offer an interesting alternative to euclidean methods. A Riemannian manifold is curved, and the geodesic distances between points on the manifold are metric. However the distances can also be indefinite (in the sense that the similarity matrix is indefinite) and so can represent indefinite dissimilarities in a natural way. Our goal in this paper is to embed objects onto the constant curvature space and its associated spherical or hyperbolic geometry. This is a potentially very useful model since, although it has intrinsic curvature, geodesics are easily computed and the geometry is well understood.</p><p>In our formulation, the space can be either of positive curvature (i.e., a spherical surface) or of negative curvature (i.e., a hyperbolic surface). We show how to approximate a distribution of dissimilarity data by a suitable hypersphere.</p><p>Our analysis commences by defining the embedding in terms of a co-ordinate matrix that minimises the Frobenius norm with a similarity matrix. We show how the curvature of the embedding hypersphere is related to the eigenvalues of this matrix. In the case of a spherical embedding, the radius of curvature is given by an optimisation problem involving the smallest eigenvalues of a similarity matrix, while in the case of a hyperbolic embedding it is dependent on the second-smallest eigenvalue. Under the embedding, the geodesic distances between points are metric but noneuclidean. Once embedded, we can characterize the objects using a revised dissimilarity matrix based on geodesic distances on the hypersphere. We apply our method to a variety of data including time warping functions, shape similarities, graph comparison and gesture interpretation data. In each case the embedding maintains the local structure of the data while placing the points in a metric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multidimensional scaling has its roots in Psychometrics and has a long history. Initially, the goal was to analyze perceptual similarities in order to visualize the results of psychological experiments. In classical MDS, the embedding space is generally euclidean and an embedding space often sought which is two or three dimensional for visualization purposes. However, it was soon realized that some types of data do not seem to lie naturally on a euclidean manifold <ref type="bibr" target="#b28">[29]</ref>; the perceptual similarities of color and musical notes are good examples. Rather, these seem to lie on curves or circles in the embedding space.</p><p>Over the last decade or so, there has been a resurgence in research into embedding techniques, fueled by the appearance of a family of spectral embedding methods, typified by ISOMAP <ref type="bibr" target="#b30">[31]</ref>, Laplacian Eigenmaps <ref type="bibr" target="#b1">[2]</ref> and locally linear embedding (LLE) <ref type="bibr" target="#b24">[25]</ref>. These are non-linear embedding techniques, which are able to embed points from a nonlinear high-dimensional manifold into a low-dimensional space while preserving the structure. The essence of these methods is to examine the structure of the points using local neighbourhoods, and to preserve this structure in the final embedding. While these methods successfully embed nonlinear data-manifolds, the final embedding is euclidean. They therefore cannot represent manifolds with non-zero intrinsic curvature (as opposed to extrinsically curved manifolds).</p><p>To overcome some of the limitations imposed by a euclidean embedding space, a family of methods has been proposed which model the data-manifold as a set of piecewise connected manifolds. Local tangent space alignment (LTSA) <ref type="bibr" target="#b35">[36]</ref> and manifold charting <ref type="bibr" target="#b2">[3]</ref> are examples of this approach. As an example, LTSA discovers a set of local tangent spaces and the neighbourhood relationships between them, via a set of global to local transformations. Such methods can approximate intrinsically curved manifolds, but can be computationally expensive. The piecewise model also makes computations such as the geodesic distance between arbitrary points more complex.</p><p>Motivated by the observation that many data sets seem to lie on arcs or circles, there have been a number of works which look at the problem of embedding dissimilarities onto curved manifolds, most typically circles or spheres. For example, Hubert et al. <ref type="bibr" target="#b15">[16]</ref> have investigated the use of unidimensional embeddings on circles. In particular, the problem of mapping distances onto the sphere S 2 has received particular attention since it has a number of applications such as the embedding of points on the surface of the Earth, and texture mapping spheroid objects <ref type="bibr" target="#b11">[12]</ref>. Cox and Cox <ref type="bibr" target="#b6">[7]</ref> were some of the first to look in detail at the problem of spherical embedding. They employ the Kruskal stress <ref type="bibr" target="#b16">[17]</ref> of the point configuration and use spherical-polar coordinates to parameterise the point-positions. The stress can then be optimized with respect to the zenith and azimuth angles of the points. Similarly, Elad et al. <ref type="bibr" target="#b11">[12]</ref> use a stress measure which is then optimized with respect to the spherical polar coordinates of the points (Equation ( <ref type="formula" target="#formula_0">1</ref>)):</p><formula xml:id="formula_0">¼ X n i;j w ij À D G ði; jÞ À D E G ði; jÞ Á 2 :<label>(1)</label></formula><p>These methods are effective and specifically designed for the two-dimensional sphere S 2 . However, they do not easily extend to spheres of higher dimension.</p><p>These methods all follow a pattern which is typically of approaches to non-euclidean MDS. The key idea is to define a measure of the quality of the embedding, called the stress, and then optimize the position of the points to minimize the stress. This is a very general approach which can be used to embed into all kinds of manifold. The optimization is an important step; here the stress majorization (SMACOF) algorithm has proved very popular <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For more details of these approaches to MDS, readers are referred to <ref type="bibr" target="#b5">[6]</ref>.</p><p>The possibility of embedding onto higher dimensional spheres has been explored by Lindman and Caelli in the context of interpreting psychological data <ref type="bibr" target="#b18">[19]</ref>. As with other methods, their method involves optimizing a stress which is an extension of the original MDS method of Torgerson <ref type="bibr" target="#b31">[32]</ref>. Interestingly, Lindman and Caelli note that the mathematics of hyperbolic space is very similar to that of spherical space, and propose a method for embedding into hyperbolic space as well. This suggests that hyperbolic space may also be a viable alternative for representing dissimilarity-based data, although problems may arise from the different topologiesfor example, spherical space is closed, whereas hyperbolic space is not. Hyperbolic embeddings have also been explored by Shavitt and Tankel, who have used the hyperbolic embedding as a model of internet connectivity <ref type="bibr" target="#b27">[28]</ref>. In other work, Robles-Kelly and Hancock <ref type="bibr" target="#b23">[24]</ref> show how to preprocess the available dissimilarity data so that it conforms either to spherical or hyperbolic geometry. In practice the former corresponds to a scaling of the distance using a sine function, and the latter scaling the data using a hyperbolic sine function.</p><p>Non-euclidean data has recently been receiving increasing attention. In particular, hyperspherical data has found application in many diverse areas of computer vision and pattern recognition. In the spectral analysis of materials, spectra are commonly length-normalized and therefore exist on a hypersphere. Similarly, it is common to use bagof-words descriptors in document analysis along with the cosine distance, which is implicitly a spherical representation. In computer vision, a wide range of problems involving probability density functions, dynamic time-warping and non-rigid registration of curves can be cast in a form where the data is embedded on a hypersphere <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b32">[33]</ref>, Veeraraghavan et al. demonstrate the utility of modelling time-warping priors on a sphere for activity recognition. Spherical embedding therefore has a central role in many problems.</p><p>In this paper, we propose a number of novel extensions to address the problem of embedding into spherical and hyperbolic spaces. First, we show how to find and appropriate radius of curvature for the manifold directly from the data. We then develop a method of embedding into these spaces which, in contrast to other approaches, is not based on optimization. Finally, we develop an optimization scheme to refine the results which is specifically tailored to the problem of constant-curvature embeddings and easily extends to any number of dimensions in spherical or hyperbolic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INDEFINITE SPACES</head><p>We begin with the assumption that we have measured a set of dissimilarities between all pairs of patterns in our data set. This information is represented by the matrix D, where D ij is dissimilarity between the objects indexed by i and j. We can define an equivalent set of similarities by using the matrix of squared dissimilarities D 0 , where</p><formula xml:id="formula_1">D 0 ij ¼ D 2 ij</formula><p>. This is achieved by identifying the similarities as À 1  2 D 0 and centering the resulting matrix:</p><formula xml:id="formula_2">S ¼ À 1 2 I À 1 n J D 0 I À 1 n J : (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Here J is the matrix of all-ones, and n is the number of objects. In euclidean space, this procedure gives the innerproduct or kernel matrix for the points.</p><p>If S is positive semi-definite, then the original dissimilarities are euclidean and we can use the kernel embedding to find position-vectors x i for the points in euclidean space; Let the matrix X be the matrix of point-positions, such that the position-vector x i of the ith point corresponds to the ith row of X. Then we have</p><formula xml:id="formula_4">X ¼ U S ffiffiffiffiffiffi L S p ;</formula><p>where U S and L S are the eigenvector and eigenvalue matrices of S, respectively. In this case, the relationship between the squared distance matrix and the kernel matrix is</p><formula xml:id="formula_5">D 0 ij ¼ S ii þ S jj À 2S ij :<label>(3)</label></formula><p>This is precisely the procedure used in classical MDS. If S is indefinite, which is often the case, then the objects cannot exist in euclidean space with the given dissimilarities represented as euclidean distances. In this case S is not a kernel matrix. This does not necessarily mean the dissimilarities are non-metric; metricity is a separate issue which we discuss below. One measure of the deviation from definiteness which has proved useful is the negative eigenfraction (NEF) <ref type="bibr" target="#b21">[22]</ref> which measures the fractional weight of eigenvalues which are negative:</p><formula xml:id="formula_6">NEF ¼ P i &lt; 0 j i j P i j i j :<label>(4)</label></formula><p>If NEF ¼ 0, then the data is euclidean. We can assess the non-metricity of the data by measuring violations of metric properties. It is very rare to have an initial distance measure which gives negative distance, so we will assume than the dissimilarities are all positive. The two measures of interest are then the fraction of triples which violate the triangle inequality (TV) and the degree of asymmetry of the dissimilarities. The methods applied in this paper assume symmetry-some of the data we have studied shows mild asymmetry which is corrected before processing. We give figures in the experimental section for triangle violations and asymmetry.</p><p>One way to treat such non-euclidean data is to correct it before embedding. An example of this is to disregard the negative eigenvalues present in S. We then obtain</p><formula xml:id="formula_7">S þ ¼ U S L þ U T S ;<label>(5)</label></formula><p>where L þ is the eigenvalue matrix with negative eigenvalues set to zero. Now S þ is a kernel matrix and we can find the embedding in the standard way. We refer to this as the kernel embedding of S to highlight its derivation from the kernel matrix, but essentially this is identical to classical multidimensional scaling.</p><p>Another alternative is to embed the non-euclidean dissimilarity data in a non-Riemannian pseudo-euclidean space <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>. This space uses the non-euclidean inner product</p><formula xml:id="formula_8">&lt; x; y &gt;¼ x T My; M ¼ I p 0 0 ÀI q ; (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where I a denotes the a Â a identity matrix. The diagonal values of À1 in M indicate dimensions corresponding to the 'negative' part of the space. The space has a signature ðp; qÞ with p positive dimensions and q negative dimensions. This inner-product induces a norm, or distance measure:</p><formula xml:id="formula_10">jxj 2 ¼ &lt; x; x &gt;¼ x T Mx ¼ X i þ x 2 i À X iÀ x 2 i ;<label>(7)</label></formula><p>where i þ runs over the positive dimensions of the space, and i À runs over the negative dimensions. We can then write the similarity as</p><formula xml:id="formula_11">S ¼ U S L 1 2 Sk ML 1 2 Sk U T S ;<label>(8)</label></formula><p>and the pseudo-euclidean embedding is</p><formula xml:id="formula_12">X ¼ U S L 1 2 Sk ;<label>(9)</label></formula><p>where L Sk indicates that we have take the absolute value of the eigenvalues. So the pseudo-euclidean embedding reproduces precisely the original distance and similarity matrices. However, while the pseudo-euclidean embedding reproduces the original distance matrix, it introduces a number of other problems. The embedding space is non-metric and the squared-distance between pairs of points in the space can be negative. Locality is not preserved in such a space, and geometric constructions such as lines are difficult to define in a consistent way. The space is more general than needed to represent the given dissimilarities (as it allows negative squared-distances) and the projection of new objects is ill defined. In order to overcome these problems, we would like to embed the points in a space with a metric distance measure which produces indefinite similarity matrices; this means that the space must be curved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GEOMETRY OF CONSTANT-CURVATURE MANIFOLDS 4.1 Spherical Geometry</head><p>Spherical geometry is the geometry on the surface of a hypersphere. The hypersphere can be straightforwardly embedded in euclidean space; for example, the embedding of a sphere of radius r in three dimensions is well known:</p><formula xml:id="formula_13">x 2 þ y 2 þ z 2 ¼ r 2 ;</formula><p>x ¼ ðr sin u sin v; r cos u sin v; r cos vÞ T :</p><p>The embedding of an ðn À 1Þ-dimensional sphere in n-dimensional space is a straightforward extension of this:</p><formula xml:id="formula_15">X i x 2 i ¼ r 2 :<label>(11)</label></formula><p>This surface is curved and has a constant sectional curvature of K ¼ 1=r 2 everywhere. The geodesic distance between two points in a curved space is the length of the shortest curve lying in the space and joining the two points. For a spherical space, the geodesic is a great circle on the hypersphere. The distance is the length of the arc of the great circle which joins the two points. If the angle subtended by two points at the centre of the hypersphere is u ij , then the distance between them is</p><formula xml:id="formula_16">d ij ¼ ru ij :<label>(12)</label></formula><p>With the coordinate origin at the centre of the hypersphere, we can represent a point by a position vector x i of length r.</p><p>Since the inner product is &lt; x i ; x j &gt; ¼ r 2 cos u ij we can also write</p><formula xml:id="formula_17">d ij ¼ r cos À1 À hx i ; x j i=r 2 Á : (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>The hypersphere is metric but not euclidean. It is therefore a good candidate for representing points which produce indefinite kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperbolic Geometry</head><p>As we previously observed, the pseudo-euclidean space has been used to embed points derived from indefinite kernels. The pE space is clearly non-Riemannian as points may have negative distances to each other. However, it is still possible to define a sub-space which is Riemannian. As an example, take the 3D pE space with a single negative dimension (z) and the 'sphere' defined by</p><formula xml:id="formula_19">x 2 þ y 2 À z 2 ¼ Àr 2 ;</formula><p>x ¼ ðr sin u sinh v; r cos u sinh v; r cosh vÞ T : <ref type="bibr" target="#b13">(14)</ref> This space is called hyperbolic and is Riemannian. Distances measured on the surface are metric, even though the embedding pE space is non-Riemannian. We can extend this hyperbolic space to more dimensions in a straightforward way:</p><formula xml:id="formula_20">X i þ x 2 i À z 2 ¼ Àr 2 : (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>If there is more than one negative dimension, the surface is no longer Riemannian. The hyperbolic space is therefore restricted to any number of positive dimensions but just one negative dimension. Finally, the sectional curvature of this space, as with the hypersphere, is constant everywhere. In this case, the curvature is negative and given by K ¼ À1=r 2 . For the hyperbolic space, the geodesic is the analogue of a great circle. While the notion of angle in euclidean space is geometrically intuitive, it is less so in pE space. However, we can define a notion of angle from the inner product. The inner product is defined as</p><formula xml:id="formula_22">x i ; x j ¼ X k þ x ik x jk À z i z j<label>(16)</label></formula><formula xml:id="formula_23">¼ Àjx i jjx j jcosh u ij :<label>(17)</label></formula><p>In four dimensions this is the familiar Minkowski inner product with signature ðþ; þ; þ; ÀÞ. This inner product defines the notion of hyperbolic angle. From this angle, the distance between two points in the space is</p><formula xml:id="formula_24">d ij ¼ ru ij .</formula><p>With the coordinate origin at the centre of the hypersphere, we can represent a point by a position vector x i of length r. Since the inner product is x i ; x j ¼ Àr 2 cosh u ij we can also write</p><formula xml:id="formula_25">d ij ¼ r cosh À1 ðÀ x i ; x j =r 2 Þ:<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EMBEDDING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Embedding in Spherical Space</head><p>Given a distance matrix D, we wish to find the set of points on a hypersphere which produce the same distance matrix. Since the curvature of the space is unknown, we must additionally find the radius of the hypersphere. We have n objects of interest, and therefore we would normally look for an n À 1 dimensional euclidean space. Since we have freedom to set the curvature, we must instead seek a ðn À 2Þ-dimensional spherical space embedded in the ðn À 1Þ-dimensional euclidean space. We begin by constructing a space with the coordinate origin at the centre of the hypersphere. If the point positionvectors are given by x i ; i ¼ 1; . . . n, then we have</p><formula xml:id="formula_26">x i ; x j ¼ r 2 cos u ij ¼ r 2 cosðd ij =rÞ:<label>(19)</label></formula><p>Next, we construct the matrix of point position-vectors X, with each position-vector as a row. Then we have</p><formula xml:id="formula_27">XX T ¼ Z;<label>(20)</label></formula><p>where Z ij ¼ r 2 cosðd ij =rÞ. Since the embedding space has dimension n À 1, X consists of n points which lie in a space of dimension n À 1 and Z should then be an n by n matrix which is positive semi-definite with rank n À 1. In other words, Z should have a single zero eigenvalue, with the rest positive <ref type="bibr" target="#b26">[27]</ref>. We can use this observation to determine the radius of curvature. Given a radius r and a distance matrix D, we can construct ZðrÞ and find the smallest eigenvalue 1 . By minimising the magnitude of this eigenvalue, we can find the correct radius of the hypersphere:</p><formula xml:id="formula_28">r Ã ¼ arg min r j 1 ZðrÞ ½ j:<label>(21)</label></formula><p>In practice we locate the optimal radius via multisection search. The search is lower-bounded by the fact that the largest distance on the sphere is pr and therefore r ! d min =p, and upper-bounded by r 3d min as the data is essentially euclidean for such large radius (this is discussed below in more detail in Section 5.2.1). The smallest eigenvalue can be determined efficiently using the power method without the expense of performing the full eigendecompositon. After the radius is determined, the embedding positions may be determined using the full eigendecomposition:</p><formula xml:id="formula_29">Zðr Ã Þ ¼ U Z L Z U T Z ;<label>(22)</label></formula><formula xml:id="formula_30">X ¼ U Z L 1 2 Z :<label>(23)</label></formula><p>This procedure can also be used to locate a subspace embedding with dimension less than n À 1. If we wish to find an embedding space of dimension m, then we can try to minimise the remaining n À m eigenvalues by finding r Ã ¼ arg min r P i nÀm j i ZðrÞ ½ j. If the points truly lie on a hypersphere, then this procedure is sufficient to correctly locate them. However, in general this is not the case. The optimal smallest eigenvalue 1 will be less than zero, and there will be residual negative eigenvalues. The embedding is then onto a 'hypersphere' of radius r, but embedded in a pseudo-euclidean space. In order to obtain points on the hypersphere, we must correct the recovered points. The magnitude of the residual negative eigenvalues can be used to gauge how well the data conforms to spherical geometry; a small residual indicates the data is close to spherical.</p><p>The traditional method in kernel embedding is to discard the negative eigenvalues. Unfortunately, this will not suffice since this will change the length of the vectors and therefore the points will no longer lie on the hypersphere (constraint 11 will be violated). Although we can subsequently project the points back onto the hypersphere, in many cases this procedure proves unsatisfactory. In the next section we present an analysis of this problem and propose a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation of the Points in Spherical Space</head><p>For a general set of dissimilarities, the points do not lie on a hypersphere, and their positions require correction to place them on the manifold. The conventional correction for a kernel embedding is to drop the negative eigenvalues with the corresponding dimensions. We show in the next section that this process is only justified for the spherical embedding in the case of a large radius of curvature. For more highly curved spaces, we propose a different approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Limits of Large Radius</head><p>When the radius of curvature is large, clearly the manifold is nearly flat, and we might hope to recover the standard kernel embedding of the data. In fact we can write</p><formula xml:id="formula_31">Z ¼ r 2 cosðD=rÞ ' r 2 J À 1 2 D 0 ðr ) 1Þ:<label>(24)</label></formula><p>As before, J is the matrix of all-ones. The squared distance matrix D 0 is related to the kernel matrix by</p><formula xml:id="formula_32">D 0 ¼ 2K 0 À 2K; (<label>25</label></formula><formula xml:id="formula_33">)</formula><p>where</p><formula xml:id="formula_34">K 0 ij ¼ ðK ii þ K jj Þ=2 is constructed from the diagonal elements of the kernel (Eqn. (3)), giving Z ' r 2 J þ K À K 0 : (<label>26</label></formula><formula xml:id="formula_35">)</formula><p>Since K is small compared to r 2 J we can use degenerate eigenperturbation theory to show that Z and K have the same eigenvectors and eigenvalues. The exception is the leading eigenvalue of Z, which is 0 ¼ nr 2 À TrðKÞ, but the corresponding eigenvalue is zero for K. As a result, we recover the kernel embedding for large r. This motivates us to use the standard approach of neglecting negative eigenvalues when embeddings onto the hypersphere with large radius(r ) 1). As the resulting points no longer lie on the hypersphere, the final step is to renormalise the lengths of the position vectors to r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Small Radius Approximation</head><p>We pose the problem as follows: The task is to find a pointposition matrix X on the spherical manifold which minimises the Frobenius distance to the euclidean-equivalent matrix Z. Given the manifold radius r, determined by the method in the previous section, we begin with the normalised matrix Ẑ ¼ Z=r 2 . The problem is then</p><formula xml:id="formula_36">min X jXX T À Ẑj x T i x i ¼ 1:<label>(27)</label></formula><p>This can be simplified in the usual way by observing that the Frobenius norm is invariant under an orthogonal similarity transform. Given Ẑ ¼ ULU T , we apply the matrix U as an orthogonal similarity transform to obtain the equivalent minimisation problem</p><formula xml:id="formula_37">min X jU T XX T U À Lj;<label>(28)</label></formula><p>which has then solution X ¼ UB where B is some diagonal matrix. The minimisation problem then becomes</p><formula xml:id="formula_38">min B jB 2 À Lj:<label>(29)</label></formula><p>Of course, B 2 ¼ L is an exact solution if all the eigenvalues are non-negative, and this is the case if the points lie precisely on a hypersphere. In the general case, there will be negative eigenvalues and we must find a minimum of the constrained optimisation problem. In the constrained setting, we are no longer guaranteed that B should be a diagonal matrix. Nevertheless, here we make the simplifying approximation that we can find a diagonal matrix which gives a suitable approximate solution. This will be true if the points lie close to a hypersphere. Let b be the vector of squared diagonal elements of B, i.e., b i ¼ B 2  ii , and be the vector of eigenvalues. Finally U s is the matrix of squared elements of U, U sij ¼ U 2 ij . Then we can write the constrained minimisation problem as</p><formula xml:id="formula_39">min b ðb À Þ T ðb À Þ; (<label>30</label></formula><formula xml:id="formula_40">) b i &gt; 0 8i;<label>(31)</label></formula><formula xml:id="formula_41">U s b ¼ 1: (<label>32</label></formula><formula xml:id="formula_42">)</formula><p>While this is a quadratic problem, which can be solved by quadratic programming, the solution actually has a simple form which can be found by noting that the matrix U s should have rank n À 1 and hence one singular value equal to zero. To proceed we make the following observations: The vector of eigenvalues is an unconstrained minimiser of this problem, i.e., b ¼ minimises the Frobenius norm (and satisfies constraint (32)), but not constraint <ref type="bibr" target="#b30">(31)</ref>. Second, b ¼ 1 satisfies both constraints since P i U 2 ij ¼ 1 (as U is orthogonal). These observations, together with the fact that U s is rank n À 1, means that the general solution to the second constraint is</p><formula xml:id="formula_43">b ¼ 1 þ að À 1Þ:<label>(33)</label></formula><p>It only remains then to find the value of a which satisfies constraint <ref type="bibr" target="#b30">(31)</ref> and minimises the criterion. Since the criterion is quadratic, the solution is simply given by the largest value of a for which constraint (31) is satisfied. Given the optimal value a Ã we can find b Ã and</p><formula xml:id="formula_44">X Ã ¼ UB Ã :<label>(34)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Embedding in Hyperbolic Space</head><p>In hyperbolic space, we have</p><formula xml:id="formula_45">x i ; x j ¼ Àr 2 cosh u ij ¼ Àr 2 coshðd ij =rÞ;<label>(35)</label></formula><p>with the inner product defined by Eqn. <ref type="bibr" target="#b5">(6)</ref>. Constructing Z as before, we get</p><formula xml:id="formula_46">XMX T ¼ Z:<label>(36)</label></formula><p>Again we have an embedding space of dimension n À 1, but Z is no longer positive semi-definite. In fact, Z should have precisely one negative eigenvalue (since the hyperbolic space has just one negative dimension) and again a single zero eigenvalue. We must now minimise the magnitude of the second smallest eigenvalue to find the radius:</p><formula xml:id="formula_47">r Ã ¼ arg min r j 2 ZðrÞ ½ j:<label>(37)</label></formula><p>The embedded point positions are now given by</p><formula xml:id="formula_48">X ¼ U Z L 1 2</formula><p>Zk :</p><p>(38)</p><p>As with the spherical embedding, in general the points do not lie on the embedding manifold and there will be residual negative eigenvalues, beyond the single allowed negative eigenvalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Approximation of the Points in Hyperbolic Space</head><p>A similar procedure to that used for the spherical space may also applied for hyperbolic space, but the optimisation problem is modified by the indefinite inner product. As with the spherical embedding, we may drop residual negative eigenvalues for large r. For small radius, the equivalent analysis is as follows. The matrix M is defined by Eqn. ( <ref type="formula" target="#formula_8">6</ref>), with q ¼ 1.</p><p>min</p><formula xml:id="formula_49">X jXMX T À Ẑj x T i Mx i ¼ À1:<label>(39)</label></formula><p>As before, we apply the orthogonal similarity transform given by U, where Ẑ ¼ ULU T to obtain the equivalent minimisation problem</p><formula xml:id="formula_50">min X jU T XMX T U À Lj;<label>(40)</label></formula><p>which has a solution X ¼ UB where D is some diagonal matrix, giving the minimisation</p><formula xml:id="formula_51">min B jBMB À Lj:<label>(41)</label></formula><p>Now we have a vector of diagonal elements given by b i ¼ ðBMBÞ ii . Exactly one of the b i 's must be negative (the one corresponding to the most negative element of ). Let b n be the component of b corresponding to the negative dimension. Finally, we then obtain a new constrained quadratic minimisation problem of</p><formula xml:id="formula_52">min b ðb À Þ T ðb À Þ; (42) b n &lt; 0; b i &gt; 0; 8i 6 ¼ n;<label>(43)</label></formula><formula xml:id="formula_53">U s b ¼ À1:<label>(44)</label></formula><p>The value of b ¼ is global minimiser which satisfies constraint (44) and a second solution of the constraint is given by b ¼ À1. We must therefore find the optimal value for a in the solution</p><formula xml:id="formula_54">b ¼ À1 þ að þ 1Þ:<label>(45)</label></formula><p>The solution is more complicated than in the spherical case, due to the constraint b n &lt; 0. This means that it is possible that there is no solution (a case which is easily detected). If a solution exists, the optimal point will lie on one of the two boundaries of the feasible region. Given the optimal solution of a Ã , we get b Ã and X Ã ¼ UB Ã . If there is not a feasible solution, this means that we cannot find a set of eigenvalues for the inner-product matrix Z Ã which both satisfy the conditions that only one is negative and that have unit length. We must abandon one of these properties-in this case we return to our standard procedure of neglecting negative eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OPTIMISATION</head><p>The methods above provide the correct embeddings when the points lie exactly on the surface of a constant-curvature manifold, and a good approximation for points nearly on the manifold. Although the embeddings become unsatisfactory for larger approximations, they still provide a good initialisation for optimisation-based approaches. In this section, we develop an optimisation procedure based on the properties of the manifold. This method involves the greedy optimisation of a distance error function (essentially a stress-like measure). We apply the exponential map to transfer the optimisation to the tangent space where the updates are much more straightforward. This allows the method to extend elegantly to any number of dimensions. Under this map, the tangent space and manifold are locally isomorphic and so the computed gradients are the same and local minima of the error on the manifold are also local minima in the tangent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Exponential Map</head><p>Non-euclidean geometry can involve demanding calculations, and many problems are intractable on general Riemannian manifolds. However, by choosing a simple noneuclidean manifold such as the hypersphere, we can hope to simplify problems such as embedding. To do so, we require one important tool of Riemannian geometry, which is the exponential map. The exponential map has previous found use in the statistical analysis of data on manifolds, for example, in Principal Geodesic Analysis <ref type="bibr" target="#b12">[13]</ref> and in the analysis of diffusion tensor data <ref type="bibr" target="#b22">[23]</ref>.</p><p>The exponential map is a map from points on the manifold to points on a tangent space of the manifold. The map has an origin, which defines the point at which we construct the tangent space of the manifold. The map has an important property which simplifies geometric computations; the geodesic distance between the origin of the map and a point on the manifold is the same as the euclidean distance between the projections of the two points on the tangent space. As the tangent space is a euclidean space, we can compute various geometric and statistical quantities in the tangent space in the standard way. Formally, the definition of these properties as follows: Let T M be the tangent space at some point M on the manifold, P be a point on the manifold and X be a point on the tangent space. We have X ¼ Log M P;</p><p>(46) where d g ð:; :Þ denotes the geodesic distance between the points on the manifold, and d e ð:; :Þ is the euclidean distance between the points on the tangent space. The Log and Exp notation defines a log-map from the manifold to the tangent space and an exp-map from the tangent space to the manifold. This is a formal notation and does not imply the familiar log and exp functions. Although they do coincide for some types of data, they are not the same for the spherical space. The origin of the map M and is mapped onto the origin of the tangent space.</p><formula xml:id="formula_55">P ¼ Exp M X;<label>(47</label></formula><p>For the spherical manifold, the exponential map is as follows. We represent a point P on our manifold as a position vector p with fixed length jpj ¼ r (the origin is at the centre of the hypersphere). Similarly, the point M (corresponding to the origin of the map) is represented by the vector m. The maps are then</p><formula xml:id="formula_56">x ¼ u sin u ðp À m cos uÞ; (49) p ¼ m cos u þ sin u u x;</formula><p>(50)</p><formula xml:id="formula_57">d g ðP; MÞ ¼ ru ¼ jxj ¼ d e ðX; MÞ; (<label>51</label></formula><formula xml:id="formula_58">)</formula><p>where u ¼ cos À1 ðhp; mi=r 2 Þ. The vector x is the image of P in the tangent space, and the image of M is at the origin of the tangent space.</p><p>For the hyperbolic manifold, the exponential map simply becomes</p><formula xml:id="formula_59">x ¼ u sinh u ðp À m cosh uÞ; (52) p ¼ m cosh u þ sinh u u x; (<label>53</label></formula><formula xml:id="formula_60">)</formula><p>where u ¼ cosh À1 ðhp; mi=r 2 Þ. The required lengths and inner-products are calculated in the pseudo-euclidean space (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Spherical Optimisation</head><p>Given a dissimilarity matrix D, we want to find the embedding of a set of points on the surface of a hypersphere of radius r, such that the geodesic distances are as similar as possible to D. Unfortunately, this is a difficult problem and requires an approximate optimisation-based approach. We simplify the problem by considering an incremental approach using just the distances to a single point at a time.</p><p>Let the point of interest be p i ; we then want to find a new position for this point on the hypersphere such that the geodesic distance to point j is d Ã ij where Ã denotes that this is the target distance. We formulate the estimation of position as a least-squares problem which minimises the square error</p><formula xml:id="formula_61">E ¼ X j6 ¼i À d 2 ij À d Ã2 ij Á 2 ; (<label>54</label></formula><formula xml:id="formula_62">)</formula><p>where d ij is the actual distance between the points. Direct optimisation on the sphere is complicated by the need to restrict points to the manifold. However, as we are considering a single point at a time, we can construct an embedding onto tangent space using the log-map and then optimise the positions in the euclidean tangent space. If the current pointpositions on the hypersphere are p j ; 8j, we can use the logmap to obtain point-positions x j for each object j in the tangent space as follows:</p><formula xml:id="formula_63">x j ¼ u ij sin u ij ðp j À p i cos u ij Þ; (<label>55</label></formula><formula xml:id="formula_64">)</formula><formula xml:id="formula_65">with x i ¼ 0.</formula><p>We have found standard optimisation schemes to be infeasible on larger data sets, so here we propose a gradient descent scheme with optimal step-size determined by linesearch. In this iterative scheme, we update the position of the point x i in the tangent space so as to obtain a better fit to the given distances. At iteration k, the point is at position x ðkÞ i . Initially, the point is at the origin, so x ð0Þ i ¼ 0. Since the points lie in tangent space, which is euclidean, we then have</p><formula xml:id="formula_66">d 2 ij ¼ ðx j À x i Þ T ðx j À x i Þ: (56)</formula><p>The gradient of the square-error function (Eqn. ( <ref type="formula" target="#formula_61">54</ref>)) is</p><formula xml:id="formula_67">rE ¼ 4 X j6 ¼i À d 2 ij À d Ã2 ij Á ðx i À x j Þ; (<label>57</label></formula><formula xml:id="formula_68">)</formula><p>and our iterative update procedure is</p><formula xml:id="formula_69">x ðkþ1Þ i ¼ x ðkÞ i þ hrE:<label>(58)</label></formula><p>Finally, we can determine the optimal step size as follows: let D j ¼ d 2 ij À d Ã2 ij and a j ¼ rE T ðx i À x j Þ, then the optimal step size is the smallest root of the cubic</p><formula xml:id="formula_70">njrEj 4 h 3 þ 3jrEj 2 X j a j h 2 þ 2 X j a 2 j þ jrEj 2 X j D j h þ X j a j D j :<label>(59)</label></formula><p>After finding a new point position x i , we apply the expmap to locate the new point position on the spherical manifold:</p><formula xml:id="formula_71">p 0 i ¼ p i cos u þ sin u u x i :<label>(60)</label></formula><p>The optimisation proceeds on a point-by-point basis until a stationary point of the squared-error E is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hyperbolic Optimisation</head><p>Optimisation on the hyperbolic manifold proceeds in a very similar way. However, in the hyperbolic case, we need to use the hyperbolic exponential map</p><formula xml:id="formula_72">x j ¼ u ij sinh u ij ðp j À p i cosh u ij Þ;<label>(61)</label></formula><p>with x i ¼ 0 and bear in mind the inner product is modified by the pseudo-euclidean embedding space. As a result, the squared distance is</p><formula xml:id="formula_73">d 2 ij ¼ ðx j À x i Þ T Mðx j À x i Þ: (62)</formula><p>The gradient of the square-error function (Eqn. ( <ref type="formula" target="#formula_61">54</ref>)) is therefore</p><formula xml:id="formula_74">rE ¼ 4 X j6 ¼i À d 2 ij À d Ã2 ij Á Mðx i À x j Þ; (63)</formula><p>which gives a j ¼ rE T Mðx i À x j Þ. Additionally, the squared length of the gradient is jrEj 2 ¼ rE T MrE. With these ingredients, Equation ( <ref type="formula" target="#formula_70">59</ref>) can be used without change to determine the optimal step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We investigate the efficacy of constant curvature embeddings using a variety of data sets including synthetic and real-world distances. Since scaling distances by a constant factor does not alter the geometry of the points, we first rescale the distance matrix so that the mean distance between points is 1. By doing this, we ensure that radii and distance errors are directly comparable between different data sets.</p><p>Our baseline comparison is with the kernel embedding (or classical MDS). For euclidean distances, where the similarity matrix is positive semidefinite, this embedding is exact. For non-euclidean distances, this is given by kernalising the similarity matrix (by eliminating negative eigenvalues):</p><formula xml:id="formula_75">S ¼ U S LU T S ; X K ¼ U S L 1=2 þ :<label>(64)</label></formula><p>For experiments involving the surface of a sphere (S 2 ) we also compare our results to the spherical embedding method of Elad et al. <ref type="bibr" target="#b11">[12]</ref>, which typifies the stressminimising approach.</p><p>We use two measures of distortion for the embedded points. The first is the RMS difference between the distance matrix and the distances between the embedded points (in the embedding space). This measures the overall distortion introduced by the embedding and is a standard measurement. We have found in practice that although the RMS error measures embedding distortion, it does not reveal everything about the quality of the embedding for certain tasks such as classification and clustering. This is because there could be a local distortion which alters the local position of points close to each other, but which is small when measured over the whole space. This is essentially the motivation behind locally linear embedding <ref type="bibr" target="#b24">[25]</ref> which embeds each local neighborhood in a linear space.</p><p>Since local configuration is important in some applications we have also used a measure of the change in neighbourhood order. This is achieved first choosing a central point and then ordering the points by distance away from the center and measuring the distortion in the ranked list using Spearman's rank correlation coefficient r. The structural error is 1 À r. This measure is then averaged over the choice of all points as the central point to obtain a structural error measure. This is zero if there is no change in the distance-ranking of points, and one if the order is completely reversed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Texture Mapping</head><p>As an initial evaluation and comparison to the literature, we begin with a set of texture mapping experiments, similar to those conducted by Elad et al. <ref type="bibr" target="#b11">[12]</ref>. which involve a triangulated mesh describing the 3D surface of an object. Such mappings have been used by Bronstein et al. <ref type="bibr" target="#b3">[4]</ref> for representing face textures in a way that is more robust to expression changes. We then compute the geodesic distances across the mesh <ref type="bibr" target="#b19">[20]</ref>. These distances between the vertices form the starting point for our algorithm. We then 'unwrap' the geodesic distances onto a two dimensional surface; the sphere S 2 for the spherical embeddings and the plane R 2 for the kernel embedding. This embedding is used to map a texture back onto the surface of the object. The texture is defined on the surface of a sphere for the spherical embeddings and on the plane for kernel embedding. For visualization purposes only, we subdivide the mesh after embedding to enable us to view the texture in high resolution. Any distortions in the embedding are revealed as distortions in the surface texture map.</p><p>The first model is a simple test case of the sphere. Fig. <ref type="figure" target="#fig_1">1</ref> shows the results of texture-mapping the surface with a triangular texture. These results are summarized in Table <ref type="table" target="#tab_0">1</ref>. Both spherical embedding methods produce near-perfect embeddings of the surface. Since our method is not based on optimization, it is considerably quicker than the method of Elad et al. <ref type="bibr" target="#b11">[12]</ref>. However, these times are only indicative as neither algorithm was optimized for speed. <ref type="foot" target="#foot_1">1</ref> While the kernel method is much quicker, there is naturally considerable distortion in mapping the sphere to a plane.</p><p>The second model is the Stanford bunny. This model is subsampled to 1,016 vertices and 2,000 faces. The embedding results are shown in Fig. <ref type="figure" target="#fig_2">2</ref> and Table <ref type="table" target="#tab_1">2</ref>. Again, the spherical embedding methods produce very similar results. However, our method finds a radius of curvature of 0.78 which is considerably different from 1. Note the distortion of the texture around the ears for the kernel methods, which is not present in the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Embedding Time-Warping Functions</head><p>We now turn our attention to a computer vision problem to assess the ability of our spherical embedding method to discover low-dimensional embeddings. As we discussed in Section 2, there are a wide range of problems where data exists on a spherical manifold. Examples include document analysis, comparison of spectra, histogram distances and time-warping functions. Here we take an example from Veeraraghavan et al. <ref type="bibr" target="#b32">[33]</ref> based on sets of time-warping functions gðtÞ. Under the reparameterisation cðtÞ ¼ ffiffiffiffiffiffiffiffi ffi _ gðtÞ p the square-root density form lies on a hypersphere of dimension equal to the number of time-samples of cðtÞ. Veeraraghavan et al. demonstrate the advantage of modelling the prior distributions on a hypersphere. We generate some time-warping functions in two classes. Each class also has one free parameter which we vary to generate different examples, and a small amount of uniform noise on the class parameters. We sample 50 points from each time warping function gðtÞ and re-parameterize it into the form cðtÞ. The data therefore lies on a 50-dimensional hypersphere, but the underlying structure is approximately two-dimensional. Samples of the time-warping functions for the two classes are shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>Using spherical embedding we can recover a twodimensional embedding of the points. We begin by computing a distance matrix for the points and rescaling so that the mean distance is one. This step is not strictly necessary for this problem as we can directly compute the matrix Z, but we use this method for consistency with the other experiments. Using the embedding procedure described in Section 5, we recover a two-dimensional spherical embedding as expected. This is shown in Fig. <ref type="figure">4</ref> and clearly shows  the structure of the data. The embedding radius is r ¼ 1:96, the RMS embedding error is 4 Â 10 À3 and the structural error is 10 À4 , illustrating that the method accurately recovers the low-dimensional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Robustness to Noise</head><p>We now turn our attention to more challenging problems, where the embedding manifold is more than two-dimensional and noise and distortions are present. The methods based on spherical-polar coordinates are difficult to use in this situation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b6">[7]</ref> as it becomes increasingly complex to model the coordinate system in a larger number of dimensions. Although our method finds the embedding exactly when the points lie on the hypothesised surface, in realistic situations there is invariably noise in the measurements. In order to assess the performance of our methods on noisy data, we generate synthetic data with controlled levels of noise as follows. We begin by generating points on the surface of a sphere (or hyperboloid). In this experiment the sphere is embedded in 50-dimensional space and 50 points are generated. We then construct the distance matrix for the points using the geodesic distance on the sphere. These distances are then perturbed by chisquared noise on the squared distances of varying amounts; this preserves the positivity of the distances, which would not be true of Gaussian noise. The same noise is applied symmetrically to D to maintain symmetry of the distances. We finally apply our embedding methods to the noisy distances. The results are shown in Fig. <ref type="figure">5</ref> for the spherical space and Fig. <ref type="figure" target="#fig_6">6</ref> for the hyperbolic space. The errors are computed between the noisy distance matrix and its embedding (i.e., they are the errors caused by the embedding process only). For comparison, we include the difference between the noisy distance matrix and the original(with no noise). It is clear from Fig. <ref type="figure">5</ref> that the spherical embedding is effective even in the presence of large amounts of noise. At all noise levels, the distortion of the spherical embedding is less than that caused by the kernel embedding. The spherical embedding also shows a remarkable ability to preserve the neighborhood order (structural error). This is also apparent, although to a lesser extent, in real-world data sets (Section 7.4. Optimization of the spherical embedding produces good embedding results but increases the structural error significantly.</p><p>The hyperbolic embedding (Fig. <ref type="figure" target="#fig_6">6</ref>) is affected more significantly by noise-the performance is good for low noise levels but at moderate to high noise the errors are similar to that of the kernel embedding. Optimization does give a significant improvement in embedding accuracy. It appears to be more difficult to accurately locate the correct radius of curvature for the hyperbolic space in the presence of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Dissimilarity-Based Data Sets</head><p>Finally, we use some data from a selection of similaritybased pattern recognition problems <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b17">[18]</ref>. These are a subset of the data analyzed under the SIMBAD project (simbad-fp7.eu), and more details of the data sets are available in the relevant project report <ref type="bibr" target="#b10">[11]</ref>. They are selected on the basis that they have small radius-ofcurvature under the spherical model, and therefore are   significantly non-euclidean under those models. The data is based on classification problems, and so as well as showing measurements of distortion, we also calculate the k-nearest-neighbour classification error rate. We choose this classifier as it can be operated in both euclidean and noneuclidean spaces as it uses only dissimilarity data. Information about the data sets is given in Table <ref type="table" target="#tab_2">3</ref>. Here we provide a brief description of each data set. DelftGestures. This data set consists of the dissimilarities computed from a set of gestures in a sign-language study. They are measured by two video cameras observing the positions the two hands in 75 repetitions of 20 different signs. The dissimilarities result from a dynamic time warping procedure <ref type="bibr" target="#b17">[18]</ref>.</p><p>FlowCyto-1. This dissimilarity data set is based on 612 FL3-A DNA flowcytometer histograms from breast cancer tissues in 256-bin resolution. The initial data were acquired by M. Nap and N. van Rodijnen of the Atrium Medical Center in Heerlen, The Netherlands, during 2000-2004. Histograms are labeled in three classes: aneuploid (335 patients), diploid (131) and tetraploid (146). Dissimilarities between normalized histograms are computed using the L1 norm, correcting for possible different calibration factors.</p><p>Chickenpieces-25-45. This is one of the chickenpieces dissimilarity matrices as made available by Bunke and Buhler <ref type="bibr" target="#b4">[5]</ref> Every entry is a weighted string edit distance between two strings representing the contours of 2D blobs. Contours are approximated by vectors of length 25. Angles between vectors are used as replacement costs. The costs for insertion and deletion are 45.</p><p>Catcortex The cat-cortex data set is provided as a 65 Â 65 dissimilarity matrix describing the connection strengths between 65 cortical areas of a cat from four regions (classes): auditory (A), frontolimbic (F), somatosensory (S) and visual (V). The data was collected by Scannell et al. <ref type="bibr" target="#b25">[26]</ref>. The dissimilarity values are measured on an ordinal scale.</p><p>CoilYork. These distances represent the approximate graph edit distances between a set of graphs derived from views of four objects in the COIL image database. The graphs are the Delaunay graphs created from the corner feature points in the images <ref type="bibr" target="#b34">[35]</ref>. The distances are computed with the graduated assignment method of Gold and Rangarajan <ref type="bibr" target="#b13">[14]</ref>.</p><p>Newsgroups. This is a small part of the commonly used 20 Newsgroups data. A non-metric correlation measure for messages from four classes of newsgroups, 'comp. Ã ', 'rec. Ã ',' sci. Ã ' and 'talk. Ã ' are computed on the occurrence for 100 words across 16,242 postings.</p><p>In a realistic scenario for such data, we might use a manifold learning technique to embed the data into a small number of dimensions. Here we use 10 embedding dimensions for all our techniques. We compare the spherical embedding (with and without optimization) to kernel embedding, ISOMAP <ref type="bibr" target="#b30">[31]</ref> and Laplacian Eigenmaps <ref type="bibr" target="#b1">[2]</ref>. As well as the RMS embedding error and structural error as described earlier, we also include the k-NN leave-one-out crossvalidation error for these classification problems. This is computed from the distances in the embedding space. In all cases we choose k to achieve the best classification performance. There is an uncertainty associated with the LOO error-rate due to the limited number of testing samples, which is given as one standard deviation errors in the table.</p><p>First we note that the kernel embedding has no free parameters. Spherical embedding has one parameter, the radius, which is determined from the data as discussed earlier. ISOMAP and Laplacian Eigenmap also have a free parameter, the order of the nearest-neighbor graph to use. For ISOMAP, we choose this parameter to achieve the lowest RMS embedding error. Laplacian Eigenmap is not a distance preserving embedding, and so it does not make sense to compare the RMS embedding errors. For LEM, we choose the graph order to minimize the structural error.</p><p>The results are summarized in Table <ref type="table" target="#tab_4">4</ref>. The values are shown in bold for the best result. The goal of the spherical embedding is to minimise the embedding error of the points and we can see that in every case the optimized spherical embedding achieves the lowest RMS embedding error. The spherical embedding (either optimized or unoptimized) achieves the lowest structural error in four out of the six cases. The exceptions is the DelftGestures data where kernel embedding is best, but very similar to optimized spherical embedding (0.028 versus 0.030) and the Newsgroups data, where LEM performs better. The classification rates are best with spherical embedding again in four out of six cases, the exceptions being FlowCyto-1 where the results are all very similar, and DelftGestures where kernel embedding performs particularly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Hyperbolic Embedding of Trees</head><p>We also analyzed our similarity-based data sets (discussed in the previous section) for hyperbolic-like examples by looking for those with significant curvatures under the hyperbolic model. When embedding in 10 dimensions, there were no such examples. We believe that this is because hyperbolic space is unbounded and the volume available increases exponentially, as opposed to polynomially for euclidean and spherical space. Since (dis)similarities are usually bounded, the spherical space is a more natural setting. However, hierarchical data such as trees and complex networks may naturally be embedded in hyperbolic spaces <ref type="bibr" target="#b7">[8]</ref> when the number of elements at each level of the hierarchy increases exponentially. From the embedding method in Krioukov et al. <ref type="bibr" target="#b7">[8]</ref> we can derive a simple distance heuristic for the nodes of a tree which approximates the hyperbolic disk model.</p><p>We take a tree with branching factor b and place the root node at the centre of the disk. The depth in the tree of node i is r i . We define the angular distance between nodes i and j as</p><formula xml:id="formula_76">Du ij ¼ p 4 b À r i þr j Àp ij 2 ;</formula><p>If i; j are directly related; Armed with these distances, we can use hyperbolic embedding to recover an embedding of the tree. The results are summarized in Table <ref type="table" target="#tab_3">5</ref> for trees of depth 5 and varying branching factors. In all cases, the hyperbolic embedding gives excellent embeddings with low distortion and close to the theoretical curvature / ln b. Optimization gives nearperfect embeddings in terms of the RMS error which are not achievable by the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Spaces of constant-curvature offer a useful alternative for the embedding of non-euclidean data sets. This allows  intrinsically non-flat geometry between objects and may be a better description for many data sets.</p><p>In this paper we have presented efficient methods of embedding points into spherical and hyperbolic spaces using their distance matrices. This simple method is based on the eigendecomposition of a similarity matrix which determines the curvature, followed by a correction to project into constant-curvature space. We also developed an optimization procedure for improving the accuracy of the embedding for more difficult data sets which utilized the exponential map to transform the problem into an optimization in tangent space.</p><p>Our results on synthetic and real data show that the spherical embedding performs well under noisy conditions and can deliver low-distortion embeddings for a wide variety of data sets. Hyperbolic-like data seems to be much less common (at least in our data sets) and is more difficult to accurately embed. Nevertheless, in low-noise cases the hyperbolic space can also be used to accurately embed noneuclidean dissimilarity data. In all the data tested here (with significant non-euclidean behaviour) the spherical embedding delivers the lowest RMS distortion error.</p><p>While accurate embedding is our goal here, it is natural to want to apply pattern recognition techniques to the embedded data. In most cases, the spherical embeddings give a competitive kNN classification error rate. However, the key advantage of these spaces is that they are metric, and so it should be possible to apply geometric techniques (such as LDA or the SVM) in these spaces. However, many methods currently rely, either explicitly or implicitly, on an underlying kernel space which is euclidean. We believe that much more work needs to be done in the future on applying such techniques in non-flat spaces. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) d g ðP; MÞ ¼ d e ðX; MÞ;(48)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Texture mapping of the sphere. Top: using our method, middle: the method of Elad et al., and bottom: kernel embedding. Both spherical methods produce virtually perfect embeddings, whereas the expected distortion is evident in the kernel embedding.</figDesc><graphic coords="9,89.97,51.19,123.84,379.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Texture mapping of the Stanford bunny. Left: using our method, middle: the method of Elad et al., and right: kernel embedding. The spherical embeddings produce similar results. Note the distortion around the ears in the kernel embedding.</figDesc><graphic coords="10,81.64,51.19,403.20,187.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The two classes and distributions of time-warp functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Spherical embedding of the time-warp functions in two dimension, when encoded in square-root density form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2pðbþ1Þ 3b b À r i þr j Àp ij 2 ;</head><label>2</label><figDesc>If i; j otherwise: ( These distances are the expected angular separations of the nodes. The hyperbolic distance d ij between nodes is then coshðd ij ln bÞ ¼ coshðr i ln bÞ þ coshðr j ln bÞ À sinhðr i ln bÞsinhðr j ln bÞcos Du ij :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Reconstruction of noisy distances via a range of different embedding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 The</head><label>1</label><figDesc>Performance of Embedding Methods on the Sphere Texturing Problem</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 The</head><label>2</label><figDesc>Performance of Embedding Methods on the Stanford Bunny Texturing Problem</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Data</head><label>3</label><figDesc>Sets Used in the Similarity Based ExperimentsSee text for more details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5</head><label>5</label><figDesc>Embeddings of Trees with Depth 5 and Various Branching Factors</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 Spherical</head><label>4</label><figDesc>Embeddings of Some Similarity-Based Data Sets, Compared to the Kernel Embedding</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>0162-8828 ß 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The methods were implemented in Matlab on a Intel Core2 Duo 3 GHz machine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 36, NO. 11, NOVEMBER 2014</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors acknowledge financial support from the FET programme within the EU FP7, under the SIMBAD project (contract 213250). Edwin Hancock was also supported by a Royal Society Wolfson Research Merit Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selecting the toroidal self-organizing feature maps (TSOFM) best organized to object recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Valiente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Netw</title>
		<meeting>Int. Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1341" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Charting a manifold</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf</title>
		<meeting>Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expressioninvariant face recognition via spherical embedding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="756" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Applications of approximate string matching to 2D shape recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Buhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1797" to="1812" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multidimensional scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Data Visualization</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="315" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional scaling on a sphere</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Statist.-Theory Methods</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2943" to="2953" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Applications of convex analysis to multidimensional scaling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Development in Statistics</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Barra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Brodeau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Romier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Van Cutsem</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="133" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multidimensional scaling with restrictions on the configuration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Leeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Heiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Analysis</title>
		<imprint>
			<biblScope unit="page" from="501" to="522" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
	<note>P. R. Krishnaiah</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Datasets and tools for dissimilarity analysis in pattern recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<ptr target="http://www.simbad-fp7.eu/images/D3.3_Data-sets_and_tools_for_dissimilarity_analysis_in_pattern_recogni-tion.pdf" />
	</analytic>
	<monogr>
		<title level="m">SIMBAD Deliverable 3.3</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture mapping via spherical multi-dimensional scaling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Scale Space PDE Methods Comput. Vision</title>
		<meeting>5th Int. Conf. Scale Space PDE Methods Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3459</biblScope>
			<biblScope unit="page" from="443" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal geodesic analysis for the study of nonlinear statistics of shape</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="995" to="1005" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A graduated assignment algorithm for graph matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="388" />
			<date type="published" when="1996-04">Apr. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified approach to pattern recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linear and circular unidimensional scaling for symmetric proximity matrices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. J. Math. Statist. Psychol</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="253" to="284" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sign language recognition by combining statistical DTW and independent classfication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2040" to="2046" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constant curvature Riemannian scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Caelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Psychol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The discrete geodesic problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="647" to="668" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<title level="m">The Dissimilarity Representation for Pattern Recognition</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-euclidean or non-metric measures can be informative</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spillmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint IAPR Int. Conf. Struct., Syntatic, Stat. Pattern Recog</title>
		<meeting>Joint IAPR Int. Conf. Struct., Syntatic, Stat. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Riemannian framework for tensor computing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="66" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Riemannian approach to graph embedding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robles-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1042" to="1056" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of connectivity in the cat cerebral cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Scannell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blakemore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1463" to="1483" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On certain metric spaces arising from euclidean spaces by a change of metric and their imbedding in hilbert space</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="787" to="797" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hyperbolic embedding of internet graph for distance estimation and overlay construction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tankel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation of structure in similarity data: Problems and prospects</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pyschometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">37300421</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Riemannian analysis of probability density functions with applications in vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Theory and Methods of Scaling</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Torgerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rate-invariant recognition of humans and their activities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1326" to="1339" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometric characterisation of graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Anal</title>
		<meeting>Int. Conf. Image Anal</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3617</biblScope>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric characterisation of graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Anal</title>
		<meeting>Int. Conf. Image Anal</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3617</biblScope>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Principal manifolds and nonlinear dimension reduction via local tangent space alignment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="313" to="338" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">he took up a lecturing post and he is currently a professor in the Department of Computer Science at the University of York. He has published more than 150 papers in international journals and refereed conferences. He received an outstanding paper award in the 1997 Pattern Recognition Society awards and has received the best paper prize in ACCV 2002. His research interests include statistical and structural pattern recognition, graph methods for computer vision, high-level vision and scene understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wilson received the BA degree in physics from the University of Oxford</title>
		<imprint>
			<date type="published" when="1992">1992. 1996. 2003</date>
		</imprint>
	</monogr>
	<note>he received the PhD degree from the University of York in the area of pattern recognition. From 1996 to 1998, he was a research associate at the University of York. After a period of postdoctoral research, he was awarded an EPSRC Advanced Research Fellowship in 1998. He is a member of the editorial board of the Journal Pattern Recognition, a fellow of the International Association for Pattern Recognition, and a senior member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
