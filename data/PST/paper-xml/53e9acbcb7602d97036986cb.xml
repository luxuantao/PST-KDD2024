<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Scheduling in Map-Reduce and Flow-Shops</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Moseley</surname></persName>
							<email>bmosele2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
							<email>anirban@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
							<email>ravikumar@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">SPAA&apos;11</orgName>
								<address>
									<addrLine>June 4-6</addrLine>
									<postCode>2011</postCode>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Scheduling in Map-Reduce and Flow-Shops</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D174B8F0081C64CE7438F22F4D7F6CD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems Algorithms</term>
					<term>Theory Scheduling and resource allocation</term>
					<term>Algorithm analysis</term>
					<term>Approximation algorithms</term>
					<term>On-line problems</term>
					<term>Map-reduce</term>
					<term>Flow-shops</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The map-reduce paradigm is now standard in industry and academia for processing large-scale data. In this work, we formalize job scheduling in map-reduce as a novel generalization of the two-stage classical flexible flow shop (FFS) problem: instead of a single task at each stage, a job now consists of a set of tasks per stage. For this generalization, we consider the problem of minimizing the total flowtime and give an efficient 12-approximation in the offline setting and an online (1 + )-speed O( <ref type="formula">1</ref>2 )-competitive algorithm.</p><p>Motivated by map-reduce, we revisit the two-stage flow shop problem, where we give a dynamic program for minimizing the total flowtime when all jobs arrive at the same time. If there are fixed number of job-types the dynamic program yields a PTAS; it is also a QPTAS when the processing times of jobs are polynomially bounded. This gives the first improvement in approximation of flowtime for the two-stage flow shop problem since the trivial 2approximation algorithm of Gonzalez and Sahni [29]  in 1978, and the first known approximation for the FFS problem. We then consider the generalization of the two-stage FFS problem to the unrelated machines case, where we give an offline 6-approximation and an online (1 + )-speed O( 1 4 )-competitive algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Map-reduce <ref type="bibr" target="#b9">[9]</ref> has already established itself as the computing paradigm of choice to process massive data. The underlying idea in map-reduce is elegant. The input data is viewed as a stream of records comprising of key-value pairs. A map-reduce computation consists of a map phase, consisting of map tasks, followed by a reduce phase, consisting of reduce tasks. Each map task runs on a map machine and processes a portion of the input, outputting (possibly new) key-value pairs en route; the map tasks can be run in parallel. In the reduce phase, the key-value pairs output by the map machines are processed in parallel by reduce tasks, which run on the reduce machines, under the guarantee that all the records with the same key will be available together in one reduce machine. The reduce phase of a job therefore cannot begin until the map phase ends, i.e., all the map machines complete their work. Google's MapReduce and Apache Hadoop (hadoop.apache.org) are two existing implementations of map-reduce; both these implementations have useful enhancements such as job priorities, queues, batch processing, etc. The success of map-reduce as a parallel programming model can be attributed to its simplicity and its ability to hide low-level issues such as scheduling, failures, data locality, network bandwidth, and machine availability, from the end user.</p><p>Although map-reduce is a distributed computational model, the task-scheduling decisions are coordinated by a centralized jobtracker process that runs in a "master node." Designing new scheduling policies has been one of the active research topics in map-reduce because of the need to balance often contradictory needs, e.g., system utilization, fairness, and response times. There has been a great deal of empirical work demonstrating the value of a well-designed job-scheduling scheme in finding a good tradeoff point among these different objectives <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35]</ref>. On the other hand, there has been very little work from a theoretical point of view. Wolf et al. <ref type="bibr" target="#b33">[33]</ref> formalize the problem of allocation of slots among jobs by the Hadoop Fair scheduler (which is the default in many implementations), and present heuristic allocation schemes designed to optimize several scheduling metricsthese heuristics come with theoretical guarantees only in an idealized model that assumes infinitesimal task times, linear relation of processing times and allocated resources, no map-reduce dependencies, and one-shot allocation of resources. Fischer, Su, and Yin <ref type="bibr" target="#b11">[11]</ref> show hardness results and present algorithms for the taskassignment problem with costs that reflect data locality. None of these papers presents theoretical guarantees for the underlying jobscheduling problem in terms of the commonly studied metrics in scheduling theory.</p><p>In this paper we study a scheduling model that captures the core challenges in map-reduce scheduling. The problem here is to assign jobs consisting of several map and reduce tasks to the available map and reduce machines in the best possible manner. However, it is tricky to adapt existing scheduling techniques to this setting due to the following non-negotiable reasons: (i) there are multi-ple map and reduce tasks in a job and multiple machines to which they can be assigned, (ii) map tasks have to be assigned to map machines and reduce tasks have to be assigned to reduce machines 1 , (iii) no reduce task can be run before all map tasks from this job finish 2 , (iv) the schedule can be preemptive but non-migratory, i.e., a task should be run on a single machine since it is wasteful to ship around partially processed data. It is also preferable to take data locality into account during the assignment of tasks to machines. Furthermore, both online and offline scenarios are relevant since map-reduce implementations typically permit batch as well online processing of jobs. Given that large map-reduce clusters are usually shared among several users, the most natural metric is to minimize the time between the arrival and the completion of a job, i.e., the flowtime <ref type="bibr" target="#b27">[27]</ref>. Problem formulation. We formulate the problem of map-reduce scheduling by abstracting the above requirements and desiderata in scheduling terms. In particular, we focus on multiple-task multiplemachine two-stage non-migratory scheduling with precedence constraints; these constraints exist between each map task and reduce task for a job. This essentially captures the properties (i)-(iv) outlined above. We allow preemption in all our settings, unless noted, and focus on the objective of minimizing the total (equivalently, average) flowtime.</p><p>At a high-level, we consider two job scenarios, namely, the offline arrival of jobs and the online arrival. In the offline case, jobs arrive together; the algorithmic focus is on optimizing the approximation ratio. In the online case, jobs arrive over time and the scheduler makes decisions without knowing the jobs that are yet to arrive; the algorithmic focus is on optimizing the competitive ratio. Orthogonally, we consider two processing-time configurations. First, in the identical machines setting, all map machines have the same speed and all reduce machines have the same speed. Second, in the unrelated machines setting, the processing time for each task is a vector, specifying the task's running time on each of the machines; this is aimed at capturing the data locality desideratum. The unrelated machines model can also be used when different map machines have different amounts of memory and each task carries a minimum memory requirement to run. There is a large amount of literature on the unrelated machines model in scheduling theory and this is perhaps the most general machine model (see <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32]</ref>). Our results. Our main contribution is to model map-reduce scheduling as a generalization of the two-stage flexible flow-shop problem (FFS) 3 . The map-reduce scheduling problem generalizes FFS by having a set of map tasks per job that need to be scheduled on the map machines and a set of reduce tasks that are to be scheduled on the reduce machines. Our aim is to design schedules that minimize the total flowtime. Our main results are in the identical machines setting, where we obtain a 12-approximation algorithm 1 This requirement does not follow immediately from the mapreduce programming model. However map and reduce tasks have typically different resource requirements and dividing a physical machine into multiple virtual map and reduce machines helps balance the load; Hadoop follows this model. Our results also extend to the case when a machine can execute both map and reduce tasks; see Section 3. 2 We ignore in our model the data-aggregation (shuffle) phase of reduce that precedes the running of the user-code in reduce tasks, and can start before maps finish. 3 In FFS each job consists of two tasks and the first task can be scheduled on a set of identical machines (say, map machines) and the second task can be scheduled on a different set of identical machines (say, reduce machines); the first task must be completed before the second can be started. for the offline case and a (1 + )-speed O(1/ 2 )-competitive algorithm for the online case where 0 &lt; ≤ 1; the online result assumes resource augmentation <ref type="bibr" target="#b21">[21]</ref>, which is necessary to circumvent lower bounds. It is important to note that in the offline setting, we consider the case where all jobs arrive simultaneously; otherwise, if jobs arrive over time, a constant approximation cannot be achieved without resource augmentation <ref type="bibr" target="#b14">[14]</ref>.</p><p>Using the ideas developed for the identical machine case, we consider the unrelated machines case. However, it seems difficult to find good schedulers when there are multiple map and reduce tasks per job. In an effort to find such algorithms, we consider a natural generalization of FFS to the unrelated machines case while each job still has one map and one reduce task. We obtain a 6approximation algorithm for the offline case and a (1 + )-speed O(1/ 5 )-competitive algorithm for the online case where 0 &lt; ≤ 1; these results can be found in Section 5.</p><p>The two-stage flow (not flexible) shop problem FlS is an important special case of FFS, where there is only one map machine and one reduce machine. FlS is known to be strongly NP-hard <ref type="bibr" target="#b13">[13]</ref>. We give the first non-trivial approximation algorithm for FlS and FFS offline. Specifically, we give a quasi-polynomial time approximation scheme (QPTAS) when the largest job is polynomial-sized. Our algorithm is also a polynomial time approximation scheme (PTAS) in the case that there are a fixed number of processing times for each job. This is the only approximation algorithm known for FFS or FlS problems besides the trivial 2-approximation for FlS shown in <ref type="bibr" target="#b29">[29]</ref> over three decades ago.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work.</head><p>The two stage flexible flow shop problem (FFS) has been studied extensively; see <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b16">16]</ref> for pointers to recent work. In the map-reduce case, on the other hand, we have multiple tasks per job per stage. We feel that beyond being practically important, the map-reduce scheduling problem is also a novel generalization of FFS that has not been previously studied.</p><p>Almost all previous work on FFS has focused on the case where all jobs arrive at the same time and the objective is to minimize the maximum completion time of any job. For this problem a PTAS was given in <ref type="bibr" target="#b16">[16]</ref> for a fixed number of machines per stage and extended in <ref type="bibr" target="#b31">[31]</ref> to a variable number of machines. Johnson's wellknown algorithm <ref type="bibr" target="#b20">[20]</ref> is also optimal for FlS when minimizing the maximum completion time. As mentioned, throughout this paper we focus on minimizing the total flowtime. Little is known about this objective for the FFS and FlS problems; there are no approximation algorithms known for FFS, while a trivial 2-approximation was shown for FlS <ref type="bibr" target="#b29">[29]</ref>. It was suggested by Schuurman and Woeginger in the survey <ref type="bibr" target="#b31">[31]</ref> that improving the 2-approximation for FlS is an important open question.</p><p>Most of the algorithmic work on map-reduce that has been done so far falls into one of the following three categories. The first is the development of computational models that faithfully capture the power and limitations of map-reduce, e.g., the work of Feldman et al. <ref type="bibr" target="#b10">[10]</ref> and Karloff et al. <ref type="bibr" target="#b24">[24]</ref>. The second is the development of map-reduce algorithms for several basic problems <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b7">7]</ref>; problems such as MST, maximum cover, connectivity were shown to have efficient map-reduce algorithms. The third is the development of practical map-reduce-based heuristics to solve large-scale data problems, especially in text processing, graph analysis, and machine learning; see, for example, <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b8">8]</ref>.</p><p>A critique. Since our goal is to provide a simple formalization of the scheduling problem in the map-reduce framework, we have deliberately ignored many issues in real systems that often have a large effect on the performance. We discuss some of these issues here. In the real system, intermediate data is transferred from the map machines to the reduce machines and thus the network bandwidth forms a significant bottleneck. Data locality -running the map tasks in the machines where data is located -is another important issue, as we mentioned earlier. Instead of modeling processing times as function of the network topology, we chose to model the effect of locality by the rather stylized unrelated machine setting. We also do not model task and machine failures, another important topic. In the map-reduce setting the dependence between map and reduce-tasks is more subtle than what we have described here due to the presence of the intermediate shuffle phase, which happens in parallel with the map tasks. We assume preemption, which is not yet a feature in Hadoop; interestingly, it is not hard to show that an online algorithm will have a large competitive ratio if preemption is not allowed without resource augmentation. Finally, we assume that the scheduler is aware of the job sizes. These may not be immediately available in practice, but in nearly all circumstances approximate job sizes can be determined based on historical data <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b33">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>Job and task. A job consists of sets of tasks, where tasks in a set can be run in parallel, but the sets themselves have to be run sequentially. In the map-reduce setting, we assume that each job has two sets of tasks, namely, a set of map tasks and a set of reduce tasks, where no reduce task can be started until all map tasks for the job are completed. Thus, the scheduling problem is precedenceconstrained.</p><p>Let J be the set of jobs and let J ∈ J denote a generic job. Let {J m i } and {J r i } be the set of map and reduce tasks of J, respectively. When both these sets are singletons, we call this the single task case; otherwise, it is the multiple task case. Let the function p(•) be the processing time of a job or a task. In the case where the processing time depends on the machine assignment, let px(•) be the processing time of a job or a task on machine x. If a machine runs a task J at speed s, then it needs p(J)/s time to complete the task; unless otherwise noted, we assume all machines run at unit speed. We also assume that the processing times of the tasks are known to the algorithms.</p><p>To avoid being repetitive, throughout the paper, let b ∈ {m, r}; this will be used to capture both map-and reduce-related statements for both machines and tasks, i.e., when b is used, it is fixed to either m or r. Let J b, * = arg maxi p(J b i ) be the task with the maximum processing time in a set of tasks and let J * = arg max{p(J m, * ), p(J r, * )} be the task with the maximum processing time. Let aJ be the arrival time of job J.</p><p>Schedule. Let σ be a schedule of jobs. Given σ, for any job or task, let the function sσ(•) denote its starting time and the function fσ(•) denote its completion time, both with respect to the schedule σ. We also define s b σ (J) = mini sσ(J b i ) and f b σ (J) = maxi f(J b i ), the starting and finishing times for a set of tasks; thus, sσ(J) = s m σ (J), fσ(J) = f r σ (J). Let Nm be the number of map machines, i.e., machines on which map tasks can be run and let Nr be the number of reduce machines, i.e., machines on which reduce tasks can be run. A schedule σ is called viable if for each job J: (i) all map tasks of J are scheduled only on the map machines, (ii) all reduce tasks of J are scheduled only on the reduce machines, and (iii) every reduce task for job J is scheduled only after all map tasks for job J are completed, i.e., f m σ (J) ≤ s r σ (J). A schedule σ is called non-migratory if each task is run only on one machine.</p><p>The flowtime of a job J with respect to a schedule σ is flowσ(J) = fσ(J) -aJ ; let flowσ = J flowσ(J) be the total flowtime. The total completion time of a schedule σ is J fσ(J). When all jobs arrive at time 0, completion time is the same as flowtime. We will consider two different scheduling metrics: minimizing the total flowtime (equivalently, the average flowtime) and minimizing the total completion time. For a time interval I, let |I| denote its length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MAP-REDUCE: IDENTICAL MA-CHINES CASE</head><p>Consider the map-reduce scheduling problem when all the map machines are identical, all the reduce machines are identical, and each job can have multiple map tasks and multiple reduce tasks. We will construct the map-reduce schedule out of two individual schedules for the map and the reduce tasks. Let σm denote some schedule on a single map machine of speed Nm for just the map tasks. Likewise, ignoring the precedence constraints between map and reduce, let σr denote some schedule for all the reduce tasks on a single reduce machine of speed Nr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Offline scheduling</head><p>We first construct a non-migratory schedule σ for the offline setting where all jobs arrive at time 0. Our goal is to reduce the precedence constrained map-reduce scheduling problem to simpler scheduling problems. To do this, we will use σm and σr to assign priorities to tasks and construct the final schedule σ using these priorities. THEOREM 1. Given schedules σm and σr, there is a viable non-migratory schedule σ such that for all job J it holds that fσ(J) ≤ 4 max{f m σm (J), f r σr (J), p(J * )}.</p><p>PROOF. We now describe the algorithm to construct σ. Define wJ , the width of job J, as the maximum of map and reduce finish times of the job and the maximum task length, i.e., wJ = max{f m σm (J), f r σr (J), p(J * )}. Note that while width incorporates the maximum flowtime that J incurs in σr or σm, it also incorporates the processing time of the largest task of job J; this will later be used to ensure that a unit speed scheduler can finish the largest task of J in time wJ . The width of a job is its priority and a smaller width means higher priority.</p><p>We first show a generic bound on the finish time of the task in terms of when it is available for scheduling and its width. Let aσ(J b i ) be the earliest time that task J b i is available to schedule by our algorithm. As we will see later, if b = m, then aσ(J b i ) = 0 and if b = r, then the task will be available at time 2wJ .</p><p>Algorithm: Offline Schedule Simulate the schedules σm on single Nm-speed and σr on a single Nr-speed machine respectively wJ ← max{f m σm (J), f r σr (J), p(J * )} for each job J by wJ increasing do for each map task J m i of job J do Assign J m i to the least loaded map machine end for for each reduce task J r i of job J do Let x be the earliest available reduce machine if x is available before time wJ then Idle x till time 2wJ end if Assign J r i to x end for end for</p><formula xml:id="formula_0">LEMMA 2. For any task J b i , it is the case that fσ(J b i ) ≤ aσ(J b i ) + 2wJ .</formula><p>PROOF. Assume that the statement is false and consider a task</p><formula xml:id="formula_1">J b i where fσ(J b i ) &gt; aσ(J b i ) + 2wJ</formula><p>. By definition, this task was available from time aσ(J b i ) and the hence schedule σ must have been working on tasks with width at most wJ in the time interval</p><formula xml:id="formula_2">[aσ(J b i ), fσ(J b i ) -p(J * )]</formula><p>. By definition of width and the assumption, we know fσ(J</p><formula xml:id="formula_3">b i ) -p(J * ) -aσ(J b i ) &gt; 2wJ -p(J * ) ≥ wJ .</formula><p>Note that also by definition, σ uses N b machines for this interval and is busy. Therefore, the above tasks that have width at most wJ represent strictly more than N b • wJ volume of work. However, the width of a job is at least the completion time of the job in σ b . This implies that σ b must complete strictly more than a N b • wJ volume of work by time wJ . But this is a contradiction since σ b has a single machine of speed N b .</p><p>To schedule the map tasks, the algorithm runs the Nm map tasks with the smallest width across the identical machines, breaking ties arbitrarily but consistently. Notice that any map task is scheduled on a single machine since no task will be preempted. Furthermore, map tasks are scheduled only on map machines. By setting aσ(J m i ) = 0 for all tasks, Lemma 2 yields fσ(J m i ) ≤ 2wJ . Scheduling reduce tasks is less obvious since we have to ensure that each reduce task is processed by only one machine. Consider a reduce task J r i . Our algorithm will not consider scheduling this task until time 2wJ . The algorithm then runs the set of at most Nr reduce tasks that are available to schedule with minimum width. By Lemma 2, it must be the case that all map tasks for a job J are finished by time 2wJ . Hence, the reduce tasks of a job are scheduled after the map tasks. Further, by definition of this algorithm, after time 2wJ the only reduce tasks that become available to schedule have width greater than wJ . This implies that the algorithm will never preempt a reduce task. Thus this schedule assigns each reduce task to only one machine. By once again appealing to Lemma 2 with aσ(J r i ) = 2wJ yields fσ(J r i ) ≤ 4wJ . Combining the bounds completes the proof.</p><p>We now show an application of the theorem. COROLLARY 3. There exists a non-migratory 12approximation algorithm for flowtime (completion time) in the offline, identical machines, multiple task, map-reduce setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF. It is known that the algorithm Shortest Remaining</head><p>Processing Time (SRPT) is optimal for average flowtime on a single machine where there is one task per job and no precedence constraints. Knowing that on a single machine having more than one task per job is irrelevant, we can use SRPT to generate the two schedules σm and σr. Let flowσ m denote SRPT's flowtime for the schedule σm and let flowσ r denote SRPT's flowtime for the schedule σr. Let OPT be the optimal schedule. Notice that flowOPT ≥ max{flowσ m , flowσ r } and that flowOPT ≥ J p(J * ). Theorem 1 implies that flowσ ≤ 4(flowσ m + flowσ r + J p(J * )) ≤ 12flowOPT.</p><p>This analysis can be extended to the case when map and reduce machines are indistinguishable. COROLLARY 4. There exists a non-migratory 12approximation algorithm for total flowtime (completion time) in the offline, identical machines, multiple task, map-reduce setting when tasks can be assigned to any machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online scheduling</head><p>In this section, we consider a similar scheduling instance as in Section 3.1 except, now jobs can arrive over time and the scheduler must be online. Consider a fixed sequence of jobs. As before, our plan is to construct a schedule σ by using σm and σr, which are schedules of the map and reduce tasks on Nm and Nr machines respectively.</p><p>In the online scheduling case, when there are no precedence constraints (no map-reduce phases), there are N identical machines, each of the n jobs has only one task, and the ratio of the maximum job size to the minimum job size is P , it is known that there is an Ω(min{log P, log n/N }) lower bound on the competitive ratio for flowtime <ref type="bibr" target="#b25">[25]</ref>. Our scheduling model strictly generalizes this setting, therefore this is also a lower bound on flowtime in our setting. Thus, for an algorithm to be O(1)-competitive, resource augmentation <ref type="bibr" target="#b21">[21]</ref> is necessary. I.e., we assume that the schedule σ is given Nm map machines each of speed (1 + ) and Nr reduce machine each of speed (1 + ) where 0 &lt; ≤ 1. THEOREM 5. Given online schedules σr and σm, there is a viable online non-migratory (1 + )-resource augmented schedule σ such that fσ(J) ≤ aJ + 128 2 max{(max{f m σm (J), f r σr (J)} -aJ ), p(J * )}.</p><p>To construct the schedule σ, we will use the following algorithm, which employs ideas from <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b5">5]</ref>. The algorithm simulates the schedules σm and σr, but needs to be more sophisticated than the offline case, since online load balancing between the machines will be necessary. For a job J, we will define its width to be wJ = max{(max{f m σm (J), f r σr (J)} -aJ ), p(J * )}. Our algorithm will group tasks according to their width. A job J together with its tasks is said to be in class k if wJ ∈ [2 k , 2 k+1 ). The algorithm will maintain the total processing time (volume) of map jobs assigned to a map machine x for each class k. Let U m,x =k (t) denote the total processing time of tasks in class k assigned to map machine x by time t. Likewise, let U r,x =k (t) denote the total processing time of tasks in class k assigned to reduce machine x by time t.</p><p>The idea behind the algorithm is to use the schedules σr and σm to give priorities to the jobs, where the priority of a job is captured by its width. We group tasks geometrically according to their width to balance the volume of work for a specific width across the machines. Notice that the assignment is not based on the current volume of unfinished work, but is based on the total volume of jobs that were assigned to machines up until now. This algorithm is online if σm and σr are online, since no task for some job J is scheduled by the algorithm unless all tasks for job J are Algorithm: Online Schedule(t) Simulate the schedules σm and σr if time t is the first time all map tasks for job J are finished in σm and all reduce tasks for job J are finished in σr then</p><p>Let k be J's class for each map task J m i of job J do Assign J m i to the map machine x where U m,x =k (t) = miny U m,y =k (t) U m,x =k (t) ← U m,x =k (t) + p(J m i ) end for end if if time t is the first time that all map tasks for job J are finished in the new schedule σ then Let k be J's class for each reduce task J r i of job J do Assign J r i to the reduce machine x where U r,x =k (t) = miny U r,y =k (t) U r,x =k (t) ← U r,x =k (t) + p(J r i ) end for end if On each map and reduce machine run the task assigned to that machine such that the job associated with the task has minimum width. completed in σm and σr. It can also been seen that the algorithm is non-migratory, since each task is assigned to a single machine, and viable. Thus, we only need to show the guarantee on the job completion time.</p><p>Before we begin the analysis, we will introduce a fair bit of notation. As before, let b ∈ {m, r}. Since we deal with viable schedules, when we mean machine or task, it will be clear from the context if it is map-related or reduce-related. For each time t, a machine x, and class k we define several quantities. The notation "≤ k" will indicate classes 1 to k. Thus, U b,x ≤k (t) is the total volume of tasks in classes 1 to k assigned to machine x. Let R b,x =k (t) denote the remaining processing time of tasks in class k on machine x. Let P b,x =k (t) denote the total volume of tasks in class k machine x has processed up to time t. It can be noted that P b,x =k (t) = U b,x =k (t) -R b,x =k (t). Each of the previously discussed quantities refers to our algorithm. Let V * b =k (t) be the total remaining volume of unsatisfied tasks in class k in the optimal solution's schedule at time t. Let V b =k (t) = x R b,x =k (t) be the total remaining volume of tasks in class k in our algorithm solution's schedule at time t. We now state some basic facts about these quantities, which will be used to show that our algorithm properly load balanced jobs in each class; the proofs are an extension of those in <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">6]</ref>. LEMMA 6. At any time t and any two machines x and y, we have the following:</p><formula xml:id="formula_4">(i) |U b,x =k (t) -U b,y =k (t)| ≤ 2 k+1 and |U b,x ≤k (t) -U b,y ≤k (t)| ≤ 2 k+2 ; (ii) |P b,x ≤k (t) -P b,y ≤k (t)| ≤ 2 k+2 ; and (iii) |R b,x ≤k (t) -R b,y ≤k (t)| ≤ 2 k+3 . PROOF. (i)</formula><p>The first inequality is true because the size of a task that belongs to some job J has processing at most wJ ≤ 2 k+1 . The second inequality is immediate given the first.</p><p>(ii) For the sake of contradiction assume the statement is false. Let t0 be the first time when |P m,x ≤k (t0) -P m,y ≤k (t0)| = 2 k+2 and a small constant δ such that |P m,x ≤k (t0 + δ) -P m,y ≤k (t0 + δ)| &gt; 2 k+2 . This can only occur if machine x processes a task of class ≤ k during I = [t0, t0 + δ] while y processes some task of class &gt; k. Knowing that each machine always processes the task of minimum width, the machine y must have no tasks in class ≤ k during I. This shows that U m,y ≤k (t0 + δ) = P m,y (t0 + δ). Thus we have, U m,y ≤k (t0 + δ) = P m,y (t0 + δ) &lt; P m,x (t0 + δ) -2 k+2 ≤ U m,x ≤k (t0 + δ) -2 k+2 , knowing that P m,x ≤k (t0 + δ) ≤ U m,x ≤k (t0 + δ). However, then we have that U m,y ≤k (t0 + δ) &lt; U m,x ≤k (t0 + δ) -2 k+2 , but this is a contradiction to (i). The proof is similar for any two reduce machines. (iii) We know that R(t) = U (t) -P (t). Combining this with (ii), we have that</p><formula xml:id="formula_5">|R m,x ≤k (t) -R m,y ≤k (t)| ≤ |U m,x ≤k (t) -U m,y ≤k (t)| + |P m,x ≤k (t) -P m,y ≤k (t)| ≤ 2 • 2 k+2 = 2 k+3 .</formula><p>The proof is similar for a reduce machine.</p><p>The remainder of the analysis differs from <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">6]</ref>. We first concentrate on showing that each task for each job J is not completed too long after aJ + wJ . To do this, our analysis will use the fact that our algorithm is given resource augmentation over the schedules σ b . We now prove a generic bound on the time gap between the dispatching of a task by our algorithm and its completion. Fix k to be some class and fix b ∈ {m, r}. Let job J be the job in class k such that f b σ (J) -aJ is maximized. Let task J b i be the task for job J that was finished last by our algorithm and let machine x be the machine to which the task J b i was assigned. Let the time t b be the last time before time f b σ (J) that our algorithm processed a task of class greater than k on machine x; this implies that machine x is busy processing tasks of class ≤ k during [t b , f b σ (J)).</p><p>LEMMA 7. For any job J that is in some class k and arrived after time t b -βJ , it is the case that f b σ (J) -t b ≤ (2 k+4 + βJ )/ and therefore (f b σ (J) -aJ ) ≤ (2 k+4 + βJ )/ , where βJ &gt; 0. PROOF. By definition of our algorithm, machine x processes a total volume of (1 + )(f b σ (J) -t b ) of work on tasks of class ≤ k during [t b , f b σ (J)). This and Lemma 6(ii) show that any other machine y also processes a volume of</p><formula xml:id="formula_6">(1 + )(f b σ (J) -t b ) -2 k+3 on tasks of class ≤ k during [t b , f b σ (J))</formula><p>. Further, by definition of time t b and our algorithm, the machine x has no tasks of class ≤ k at time t b . Thus by Lemma 6(iii) for any machine y we have that R b,y ≤k ≤ 2 k+3 . Together this shows that the total volume processed by our algorithm on machines during [t b , f b σ (J)) of jobs of class ≤ k that were dispatched to machines after time t b is at most</p><formula xml:id="formula_7">N b (1 + )(f b σ (J) -t b ) -N b 2 k+4</formula><p>. Note that our algorithm does not process any task until the schedule σ b completes the task. Thus the schedule σ b must process this volume of work during the interval</p><formula xml:id="formula_8">[t b -βJ , f b σ (J)]. This implies that N b (1+ )(f b σ (J)-t b )-N b 2 k+4 ≤ N b (f b σ (J)-t b +βJ )</formula><p>, since the schedule σ b has a single machine of speed N b . However, this implies that (f b σ (J) -t b ) ≤ 2 k+4 + βJ , completing the proof. Now we apply Lemma 7 to the map tasks and show that our algorithm completes all map tasks in a relatively short amount of time when compared to σm. To do this, recall that a map task associated with some job J is dispatched by our algorithm by time aJ + wJ . For this case, set b = m. The definition of tm implies aJ + wJ ≥ tm. It must be the case that job J arrived after time tm -2 k+1 since the job is of class ≤ k and therefore has width ≤ 2 k+1 . Hence, Lemma 7 with βJ = 2 k+1 yields that (f m σ (J) -aJ ) ≤ 2 k+5 / ≤ (32/ )wJ .</p><p>Next, we would like to to show the same thing about reduce tasks. First set b = r. Recall the reduce task is dispatched by our algorithm at time f m σ (J), the time that all map tasks of J are completed.</p><p>From the above argument we have</p><formula xml:id="formula_9">(f m σ (J) -aJ ) ≤ (32/ )wJ ≤ 2 k+6 / . Thus aJ ≥ f m σ (J) -2 k+6 / ≥ tr -2 k+6 / .</formula><p>Appealing to Lemma 7 with βJ = 2 k+6 / yields (f r σ (J) -aJ ) ≤ 2 k+4 +2 k+6 / ≤ 2 k+7 / 2 ≤ (128/ 2 )wJ . This completes the proof of Theorem 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">An application of Theorem 5</head><p>Using SRPT to generate the schedules σm and σr we can show the following. COROLLARY 8. There exists a non-migratory (1 + )-speed O( <ref type="formula">1</ref>2 )-competitive algorithm for average flowtime in the online, identical machines, multiple task, map-reduce setting where 0 &lt; ≤ 1.</p><p>PROOF. It is well know that the online algorithm SRPT is optimal for average flowtime in a standard scheduling instance when there is a single machine. We use SRPT to generate the two schedules σm and σr. The rest of the proof follows from Theorem 5 and the proof of Corollary 3.</p><p>Considering a simple extension of the previous analysis gives a scheduler that is competitive with resource augmentation when there is no separation between map and reduce machines. When considering this setting, simple extensions of the previous analysis lose a factor of 2 in this speed. This is because it is difficult for the scheduler to decide how to prioritize between map and reduce tasks on a single machine. REMARK 1. There exists a non-migratory (2+ )-speed O( <ref type="formula">1</ref>2 )competitive algorithm for average flowtime in the online, identical machines, multiple task, map-reduce setting when tasks can be scheduled on any machine where 0 &lt; ≤ 1.</p><p>Chekuri et al. <ref type="bibr" target="#b5">[5]</ref> introduce another model of resource augmentation where the online algorithm is provided with (1 + ) as many 1-speed machines. It is not hard to see that section's results hold in this setting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FLOW SHOP: PTAS AND QPTAS</head><p>In the section we describe our approximation scheme when each job consists of one map task and one reduce task. For the ease of presentation, we assume that there is only one map machine and only one reduce machine; later, we will show how to extend this to the multiple machine case. Our dynamic programming algorithm and proof follow the ideas presented in <ref type="bibr" target="#b1">[1]</ref>. We begin by assuming that the processing time of a task is polynomially bounded and we give a quasi-PTAS in this case; this problem is NP-hard even under this assumption <ref type="bibr" target="#b13">[13]</ref>. The analysis extends to the case where there is a fixed number of jobs types; our algorithm yields a PTAS in this case. Since there is only one map and one reduce task per job, we let J b denote job J's map or reduce task, where b ∈ {m, r} . We assume preemption is not allowed and unlike the other offline settings we consider, we will allow jobs to arrive over time, but we focus on the objective of total completion time. Recall that this is the same as flowtime if jobs arrive at time 0.</p><p>The approximation schema that we present is a dynamic program. We first apply a number of structural modifications to the input -these are all inspired by <ref type="bibr" target="#b1">[1]</ref>, where they have shown to be useful in the problem instance before applying a dynamic program.</p><p>The core intuition in the design of the dynamic program is the use of Johnson's celebrated algorithm <ref type="bibr" target="#b20">[20]</ref> that optimizes makespan as a subroutine to verify feasibility.</p><p>Structural modifications. In this section we give an informal description of the various structural modifications applied to the problem in preprocessing. Appendix A outlines the formal lemma statements and proofs corresponding to these modifications.</p><p>Let 0 &lt; ≤ 1/2. For an arbitrary integer x, define Rx = (1+ ) x . We partition the time interval (0, ∞) into disjoint intervals of the form Ix = [Rx, Rx+1); we will use Ix to refer to both the interval and the size, Rx+1 -Rx, of the interval. We will often use the fact that Ix = Rx, i.e., the length of the interval is times its start time.</p><p>We apply the preprocessing in a number of steps, while causing only a (1+ ) factor loss to the optimal value for each step. Each arrival time is rounded to be of the form Rx for some x (Lemma 21). Each task processing time is also rounded to be a power of (1 + ) (Lemma 21). With another (1 + ) factor loss (Lemma 22), we ensure that the task J m is not started before aJ + p(J m ), and task J r before max(f m (J m ), p(J r )). As a result of these modifications, Lemma 24 shows that no task crosses too many intervals. We define time to be stretched by a factor of 1 + when we multiply each interval endpoint by a factor of 1 + ; this again causes the OPT to degrade by another factor of at most 1 + .</p><p>We say that a job with its map task running in interval I m J and reduce task in interval I r J is small if p(J m ) ≤ I m J and max(p(J m ), p(J r )) ≤ I r J . Otherwise the task is large. If a task has a processing time (1 + ) x we say it is of type x. We call a job J to be of type (x, y) if J m is of type x and J r is of type y. Sets of jobs are denoted as vectors, e.g., a vector s of counts sxy corresponding to each of the types (x, y). For any two sequences a and b, define a b if axy ≤ bxy for all (x, y). All sequences s that we consider will satisfy 0 s. Algorithm. Our algorithm is a dynamic program that creates the schedule interval-wise. The crucial observation is that given a set of map-reduce tasks, we can test the feasibility of scheduling these jobs in an interval by using Johnson's algorithm <ref type="bibr" target="#b20">[20]</ref> for minimizing makespan in a two-stage two-machine flowshop. Our algorithm also guesses the last 25  10 tasks that will be completed in the optimal schedule and does not consider these tasks in the dynamic program.</p><p>At time Rt, let n be the set of all jobs that have arrived up until now and let c denote the set of completed jobs. Define a job to be partially done if the map task is completed. Let p denote the set of partially done jobs at Rt. The dynamic program at iteration t will select a set of tasks to be scheduled in some interval It+1. This set could consist of both the tasks of a job, only the map task of a job, or a reduce task whose corresponding map task has been completed. If a reduce task is scheduled in an interval and the map task for this job was scheduled in an earlier interval, we will implicitly incorporate a map task of length zero to precede this reduce. We now describe the algorithm.</p><p>(1) Assume the arrival time of each job to be max(aJ , p(J m )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>). Also assume that the reduce task of J is not available before p(J r )/ 2 (and of course, before completing the corresponding map task).</p><p>(2) If n jobs have arrived up until time interval It, define C[c, p, n, t] be the total completion time when scheduling all tasks in c completely and just the map tasks for jobs in pc during [0, Rt+1). Since the tasks in pc are not completed, we do not include their completion time in the total. If n does not correspond to the number of arrived jobs, or if c p n is not satisfied, then C[c, p, n, t] = ∞. We also define C[0, 0, 0, -1] = 0. We use the following dynamic program for all but the last 25  10 that we guessed. (3) Suppose It+1 = [Rt+1, Rt+2) be the current interval. Let v be the sequence of new jobs that arrive at the beginning of this interval. Let q, m, and r be such that m + q n + vp and r pc. Intuitively, q denotes the set of jobs for which we are going to schedule both the map and the reduce tasks in It+1, m the set of jobs for which we are going to schedule only the map task, and r the set of jobs for which we schedule only the reduce. Given these sets our algorithm would like to schedule the tasks in q + m + r during the interval It+1. We verify the feasibility of this schedule using Johnson's algorithm <ref type="bibr" target="#b20">[20]</ref> for optimal makespan in two-stage two machine setting. Define the scheduling cost for this interval to be Q(q, m, r, It+1) = Rt+2 ij (qij + rij) if Johnson's algorithm returns a feasible schedule and Q(q, m, r, It+1) = ∞ else. Notice that this cost function assumes that completion time of the jobs scheduled in interval It+1 is Rt+2. This is because each task completed during It+1 is finished before time Rt+2. We justify this in the analysis by showing that this increases the schedule's cost by at most a (1 + ) factor. Thus, the dynamic program computes</p><formula xml:id="formula_10">C[c, p, n + v, t + 1] = min q,m,r C[c -q -r, p -m -q -r, n, t] + Q(q, m, r, It+1).</formula><p>(4) After all jobs are scheduled using the dynamic program, enumerate the possible ways to finish the final 25  10 jobs that were guessed.</p><p>Let P = maxJ p(J * ) be the processing time of the largest task and T = J p(J m ) + p(J r ) be an upper-bound on the schedule length. The time taken by the above procedure is given by n O(log 1+ (P )) log T for fixed . As a first step in establishing our claims, we motivate our algorithm with the following lemma whose proof follows the proof of Lemma 3.1 in <ref type="bibr" target="#b1">[1]</ref>. LEMMA 9. If in the optimal solution, all jobs are small, then the above algorithm gives a (1 + 3 )-approximate solution.</p><p>PROOF. Let R(I) denote the starting point of interval I. If the jobs are small in the optimal solution, then from Lemma 22 it follows that</p><formula xml:id="formula_11">s m * (J) ≥ R(I m J ) = I m J ≥ p(J m )/ 2 . Also, similarly, s r * (J) ≥ R(I r J ) ≥ p(J r )/ 2 .</formula><p>Hence, changing the arrival times does not matter for the optimal solution. Also, since each job is small for the interval, by stretching each interval by a factor of (1 + ) we ensure that all jobs finish completely inside their interval (Lemma 25). Given this condition, we first claim that the dynamic program computes a (1 + ) approximation to the optimal solution. Consider the C[c, p, m, n, t -1] that corresponds to the optimal algorithm's choice of c, p, m up until interval It-1 -by inductive hypothesis this is a 1 + approximation. If the optimal solution chooses the task sets from the jobs in q * + m * + r * to schedule in this interval It = [Rt, Rt+1), then it incurs at least Rt ij (q * ij + r * ij ). By looking at all possible assignments, our algorithm makes a choice such that the resulting minimum value of C is at most (1 + ) of the value incurred by the OPT at the end of It. This is because Johnson's algorithm ensures that the tasks are scheduled within the interval It. Further, a task's completion time is at most Rt+1 if the task was scheduled during It. Hence at the end of the dynamic program, we still have a (1 + ) approximation.</p><p>Since stretching the time by a factor of (1 + ) increases the total completion time by the same factor, and we pay another (1 + ) in the dynamic program. Overall we have a (1 + 3 ) factor approximation.</p><p>Let t h = 2 10 OPT be a threshold time. Notice that in the optimal solution there are at most 1 2 10 jobs that are completed after time t h . The goal of the next lemma is to show that large jobs can be postponed. To do this, we show that there is an approximate optimal solution where either a job is small or it is done after time t h . To prove the lemma, we consider shifting large jobs in the optimal solution to intervals where the jobs are small. We create room for these jobs by expanding the interval lengths by a factor of (1 + O( )). We also show that shifting the large jobs does not effect the value of the optimal solution by more than a factor of (1 + O( )). The proof of the lemma is quite technical since we have to be careful on how map and reduce tasks for the same job are shifted. LEMMA 10. There exists a (1 + 13 )-approximate optimal schedule in which, for each job J, s m * (J) ≥ min( p(J m ) 2 , t h ) and s r * (J) ≥ min(max(f m * (J), p(J r )/ 2 ), t h ). PROOF. The proof argument is similar to that of Lemma 3.2 in <ref type="bibr" target="#b1">[1]</ref>. Fix some job J. Let I m = [R m , S m ) be the interval the J m is processed during and</p><formula xml:id="formula_12">I r = [R r , S r ) be the interval J r is processed during. If J is small, then p(J m ) ≤ I m ≤ 2 R m ≤ 2 s m * (J).</formula><p>Also, for the reduce job, s r * (J) ≥ f m * (J). Furthermore, by the smallness of J, p(J r ) ≤ I r ≤ 2 R r ≤ 2 s r * (J). Hence, if J is small both the conditions are satisfied.</p><p>The two cases we need to worry about are i) max(p(J r ), p(J m )) &gt; I r and ii) p(J m ) &gt; I m . To handle both of these cases, we will show how jobs in the optimal solution can be shifted to satisfy the lemma.</p><p>We handle case (i) first. The first subcase (i.A) is p(J r ) &gt; I r . Let s = log 1+ ( 1 6 ). We move the map and reduce task of J to the interval I = I r + s = [R , R ). In this case, if s m (J) and s r (J) denotes the new starting point of the map and reduce tasks, then similar to the argument in Lemma 3.2 of <ref type="bibr" target="#b1">[1]</ref>, we have that p(J m ) ≤ s m * (J)/ ≤ 5 R ≤ 5 s m (J). We can show similarly p(J r ) &lt; 5 s r (J). We now need to show that we can fit the shifted jobs in this interval. Since p(J r ) &gt; Ir, there are at most 1 such jobs from interval Ir that move into interval I , where each of then requires a total time of p(J m ) + p(J r ) &lt; 2 4 I . Hence, the total time required by these shifted jobs in interval I is 2 3 I . By stretching time by a (1 + 2 ) factor, we can easily accommodate these jobs. Hence the condition is fulfilled.</p><p>The only case where the above construction will not work is when there is a single task (either in map or reduce machine) that spans the entire interval I . Then, we use Lemma 25 to say that we could as well insert this I space at the beginning of the crossing job. That is, shift the single crossing task and place the space before the task. If the new interval is I , as at most log 1+ 1 intervals are crossed by the job, I = I, and thus each of p(J m ) and p(J r ) are at most 2 I and are small. For the second subcase (i.B), p(J m ) &gt; I r . We again move the entire job to I = I r + s. Now, we need to justify as before that there are small number of such jobs being shifted to I . In this case, since both the map and reduce happen by the interval I r and the interval lengths are geometrically increasing, the total time taken by such jobs is at most I r / and thus there can be at most 1 2 of such jobs. After shifting to the interval I , such jobs take up at most 1 2 • 2 3 I ≤ 2 I . The case where there is a single task covering I can be handled similar as before.</p><p>Next, we handle case (ii). The first subcase (ii.A) is when I = I m + s ≤ I r . In this case, only the map task is moved to the interval I .</p><p>In case (ii.B), I = I m + s &gt; I r . In this case, we move both the map and the reduce to the interval I . The number of jobs shifted to interval I can be bounded now by 1 by the fact that p(J m ) &gt; I m . Hence, again by stretching time by a 1 + 2 factor, we can accommodate all jobs. Now we bound the cost of the solution after performing these shifting operations. We might need to expand by schedule twice by factors of 1 + 2 because of the two cases -this increases the cost by a factor of 1 + 2 . By doing the shift, we increase the completion time of any job ending in Rx to at most Rx+s+1 ≤ (1+ )Rx 6 factor, and there are at most 2 2 jobs being shifted overall. The last interval from which jobs are being shifted ends at t h . Thus, the total completion time of the shifted jobs is</p><formula xml:id="formula_13">Rx&lt;t h 2 2 (1 + )Rx 6 ≤ 2t h 8 i≥0 1 (1 + ) i ≤ 2t h 8 (1 + ) 2 ≤ (1 + ) 2 OPT &lt; 2 OPT,</formula><p>since t h = 2<ref type="foot" target="#foot_0">10</ref> OPT. Since we stretched time by a 1 + factor for rounding processing times, 1 + factor for Lemma 22, and 1 + 2 factor for this lemma, and added a 2 cost, we have a 1 + 13 factor approximation overall.</p><p>By combining the Lemmas 9 and 10, we now show how to get a (1 + O( )) approximation. Note that if s m * (J) ≥ p(J m ) 2 and s r * (J) ≥ max(f m * (J), p(J r )/ 2 ), then the job J is small when run. By the Lemma 10, we have a (1 + 13 ) approximate solution for these jobs. Now suppose we fix the positions of the last 25 10 tasks. Given the fixed position of these non-small tasks, the dynamic program will still find a (1 + 3 ) approximate solution to the current OPT, and hence a (1 + 13 )(1 + 3 ) ≤ (1 + 50 ) approximate solution overall assuming ≤ 1/2. Hence, if we run this dynamic program, at the end of time t h = 2 10 OPT, the number of jobs left is at most (1 + 50 ) 1  2 10 ≤ 25 10 . Hence, all tasks scheduled by the dynamic program finish by time t h and the last jobs were enumerated. THEOREM 11. For the offline case with arrival times, and one map and one reduce task per job on identical machines, there exists a 1 + O( ) approximate algorithm that runs in time n O( <ref type="formula">1</ref>10 ) (n log 1+ (P ) log T + 25 10 !).</p><p>Notice that this theorem gives a quasi-polynomial time algorithm when maximum processing time a task is polynomially bounded. Now consider the case where there are a constant δ number of tasks types. In this case, the dynamic program needs to enumerate over each of the task types. Thus, for this case we have the following theorem. THEOREM 12. For the offline case with arrival times, one map and one reduce task per job on identical machines and there are δ task types, there exists a 1 + O( ) approximate algorithm that runs in time n O( <ref type="formula">1</ref>The final case we consider is when there are multiple map and reduce machines. Notice that in the dynamic program, Johnson's algorithm was used to actually schedule the tasks assigned to an interval. This is the only part of the analysis where we used the fact that there was a single machine at each stage. We can consider the case where there are multiple map and reduce machines by using the PTAS for maximum completion time in the two stage flexible flow shop problem to determine how to schedule tasks within an interval <ref type="bibr" target="#b31">[31]</ref>. By stretching time by another factor of (1 + ) it can be ensured that this PTAS is able to fit all jobs into an interval.</p><p>Lastly we remark that although the run-time of the (Q)PTAS might seem daunting at first sight, Hepner and Stein <ref type="bibr" target="#b17">[17]</ref> have already demonstrated how a related algorithm can be implemented in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FLEXIBLE FLOW SHOP: UNRELATED MACHINES CASE</head><p>In this section we consider the most general multiple machine scheduling model known as the unrelated machines model. In the unrelated machines model, the processing time of a task depends on the machine to which the task is assigned. In general, for x = y, px(•) and py(•) may be uncorrelated. In fact, the processing time of a task may be ∞ on some machines, capturing the case where a task cannot be assigned to a specific machine, e.g., when there is not enough memory on a machine to run a specific task.</p><p>Due to the generality of the unrelated machines model, it seems difficult to find an algorithm that performs well when there are multiple map and reduce tasks per job. Working towards the goal of finding good algorithms for multiple task instances, we consider the single task case in this section. This is the FFS problem generalized to unrelated machines.</p><p>Let σm be a non-migratory schedule on the unrelated map machines for only map jobs and let σr be a non-migratory schedule on the unrelated reduce machines for only reduce jobs. Unlike the identical machine cases, these two schedules are not on a single machine, but rather they are on the original set of machines.</p><p>We assign each job J width wJ = max{fσ m (J), fσ r (J)}. Notice that in this case, the width of a job only depends on the simulated schedules and does not include the maximum processing time of task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Offline scheduling</head><p>First we address the case where the scheduler is offline and all jobs arrive at time 0. THEOREM 13. Given σm and σr, there is a non-migratory viable schedule σ such that all tasks for job J are completed by time 2 max{fσ m (J), fσ r (J)}.</p><p>Our algorithm simulates the schedules σm and σr. The algorithm assigns each map (reduce) task to the same machine it was processed on in the schedule σm (σr). A map machine runs the task with shortest width assigned to it. At any time, a reduce machine only runs a reduce task whose corresponding map task is complete. The algorithm always runs the reduce task with smallest width amongst the reduce tasks which are available to schedule. It is easy to check that this schedule is non-migratory and viable. To bound the completion time of the tasks, first we consider the map tasks. Since there is only one map and one reduce task per job, we drop the index of the tasks. Thus, J b denotes the task for job J and fσ(J b ) denote the time the task of job J is completed in σ. Again, we give a generic bound on the completion times of tasks based on their earliest availability and width. Recall that for a task J b , b ∈ {m, r}, aσ(J b ) is the earliest time when the task is available to the schedule σ. LEMMA 14. For any task J b , fσ(J b ) ≤ aσ(J b ) + wJ . PROOF. For the sake of contradiction, assume that the lemma is false. Consider a task J b where fσ(J b ) &gt; aσ(J b ) + wJ . We know that this task has been available to schedule since time aσ(J b ). By definition of our algorithm, this implies that the machine task J b is assigned to has been busy processing jobs with width at most wJ during [aσ(J b ), fσ(J b )]. By definition of our algorithm and width, the schedule σ b must processes strictly more than a wJ volume of work on this machine by time wJ , a contradiction. By using the fact aσ(J m ) = 0 for all map tasks, we have that for any map task J m , fσ(J m ) ≤ wJ . Similarly, the completion time of a reduce task is bounded by using the above lemma that aσ(J r ) = maxJm∈J fσ(J m ) ≤ wJ . We have now bounded the completion times of the jobs. Using Theorem 13, we can construct an approximation algorithm for average flowtime in the scheduling setting. COROLLARY 15. There exists a non-migratory 6approximation algorithm for flowtime (completion time) in the offline, unrelated machines, single task, map-reduce setting.</p><p>PROOF. Skutella in <ref type="bibr" target="#b32">[32]</ref> gave a 3  2 -approximation algorithm for minimizing the total completion time on unrelated machines where there is one task per job, no precedence constraints and all jobs arrive at time 0. Since there are only one map and one reduce task per job in our scheduling instance, in the scheduling instances the schedules σm and σr consider there is one task per job. Thus, the algorithm of Skutella can be used to construct the schedules σm and σr. Since flowOPT ≥ 2 3 max{flowσ m , flowσ r }, Let Fσ denote the total flow Theorem 13 implies that flowσ ≤ 2(flowσ m + flowσ r ) ≤ 6flowOPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Online scheduling</head><p>We now consider the case when jobs arrive over time in an online fashion. In the online unrelated machines setting, even when there are no precedence constraints and all jobs consist of one task, it is known that no online algorithm has bounded competitive ratio for the objective of flowtime <ref type="bibr" target="#b14">[14]</ref>. Thus, like in the identical machines setting, we resort to resource augmentation. THEOREM 16. Given online non-migratory schedules σm and σr, there is a viable online non-migratory (1 + )-resourceaugmented schedule σ such that all tasks for job J are completed by time aJ + 4</p><p>2 (max{fσ m (J), fσ r (J)} -aJ ).</p><p>Our algorithm simulates the schedules σm and σr similarly to the offline algorithm. It is easy to check that the scheduler is online, non-migratory, and viable.</p><p>We first present a common lemma that we will use in bounding both the map and the reduce finish times. LEMMA 17. Let α &gt; 0. Suppose that task J b , b ∈ {m, r}, is available for scheduling by our schedule σ at time aJ +αwJ . Then it is the case that fσ(J b ) ≤ aJ + 2α wJ .</p><p>PROOF. Let x be the machine (map or reduce) that the task J b is assigned to. Let time t b be the earliest time such that every task processed during [t b , fσ(J b )] has width at most wJ on machine x. By the given condition, we know that task J b is available to schedule at time aJ + αwJ . Knowing that our algorithm always schedules the task with minimum width on each machine, we have that t b ≤ aJ + αwJ .</p><p>We also claim that any task scheduled during [t b , fσ(J b )] arrived at earliest t b -αwJ . This is because any task J scheduled during [t b , fσ(J b )] has width w J ≤ wJ , and hence by given assumption has arrival time a J ≥ t b -αw J ≥ t b -αwJ .</p><p>Our algorithm has speed 1 + , thus the algorithm processes (1 + )(fσ(J b ) -t b ) volume of work in total during [t b , fσ(J b )]. All of the tasks processed by our algorithm during [t b , fσ(J b )] must be processed on the interval [t b -αwJ , fσ(J b )] by the schedule σ b on machine x itself. This is because all of the tasks processed by σ during [t b , fσ(J b )] arrived no earlier than t b -αwJ by the previous argument. Further, our algorithm assigns any task to the same machine the schedule σ b processed the task on. Therefore it must be the case that (1 + )(fσ(J b ) -t b ) ≤ fσ(J b ) -t b + αwJ . This implies that fσ(J b ) ≤ t b + α wJ ≤ aJ + αwJ + α wJ ≤ aJ + 2α wJ since &lt; 1.</p><p>We now use the above Lemma to deduct the following two corollaries. The first corollary is obtained from the above Lemma, combined with the fact that by construction of our algorithm, the map task Jm is available to σ at time aJ + wJ . COROLLARY 18. For any map task J m it is the case that fσ(J m ) ≤ aJ + 2 wJ .</p><p>Similarly, using the above corollary, the reduce task J r is available when all the corresponding maps are finished, and hence at time aJ + 2 wJ . Using this bound in Lemma 17, we have the following corollary. COROLLARY 19. For any reduce task J r it is the case that fσ(J r ) ≤ aJ + 4 2 wJ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Application of Theorem 16</head><p>First we consider the objective of total flowtime. As mentioned, it is known that no algorithm has bounded competitive ratio without resource augmentation <ref type="bibr" target="#b14">[14]</ref>. Since our algorithm only uses resource augmentation, our algorithm is constant competitive when given the minimum advantage over the adversary. COROLLARY 20. There exists a non-migratory (1 + )-speed O( <ref type="formula">1</ref>4 )-competitive online algorithm for average flowtime in the online, unrelated machines, single task, map-reduce setting.</p><p>PROOF. In a recent breakthrough result Chadha et al. showed a (1 + )-speed O( <ref type="formula">1</ref>2 )-competitive online non-migratory algorithm for average flowtime in the unrelated machine setting when there is one task per job and no precedence constraints <ref type="bibr" target="#b4">[4]</ref>. Using this algorithm we can generate the schedules σm and σr. The result of Chadha et al. implies that OPT ≥ Ω( 2 ) max{flowσ m , flowσ r }. Theorem 16 shows that flowσ ≤ 4 2 (flowσ m + flowσ r ). Thus, flowσ ≤ O( <ref type="formula">1</ref>4 )flowOPT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of results on minimizing total flow-time. P is the ratio of the largest task size to the smallest, N is the total number of machines, and n is the number of jobs. For the FFS identical machines case, we obtain a QPTAS for polynomial job sizes (Theorem 11) and a PTAS for fixed processing times (Theorem 12).</figDesc><table><row><cell></cell><cell>Offline</cell><cell>Online</cell></row><row><cell>Map-reduce,</cell><cell>NP-hard,</cell><cell>Ω(min{log P, log n/N }) [25],</cell></row><row><cell>Identical</cell><cell>12-approx.</cell><cell>(1 + )-speed O( 1 2 )-comp.</cell></row><row><cell cols="2">machines (Corollary 3)</cell><cell>(Corollary 8)</cell></row><row><cell>FFS,</cell><cell>NP-hard,</cell><cell>unbounded [14],</cell></row><row><cell>Unrelated</cell><cell>6-approx.</cell><cell>(1 + )-speed O( 1 4 )-comp.</cell></row><row><cell cols="2">machines (Corollary 15)</cell><cell>(Corollary 20)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_0"><p>) (n δ log T + 25 10 !).</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Work done while visiting Yahoo! Labs. Partially supported by NSF grants CCF-0728782 and CCF-1016684.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. FLOW SHOP: PTAS AND QPTAS STRUCTURAL LEMMAS</head><p>In this section we state a few structural lemmas for our PTAS. These lemmas are adopted from <ref type="bibr" target="#b1">[1]</ref> and the proofs are almost identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 21 ([1]</head><p>). With 1 + loss we can assume that all processing and arrival times are integer powers of 1 + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 22 ([1]</head><p>). With 1 + O( ) loss, we can ensure that a job arrives after p(J m ) and a reduce task starts later than p(J m ) + p(J r ). Further, the arrival of jobs occur only at Rx for some x.</p><p>PROOF. The same as in <ref type="bibr" target="#b1">[1]</ref>. The second part follows by the 1 + stretch and since the reduce task cannot start earlier than map completion. DEFINITION 23. We say that a task crosses an interval Ix if its execution overlaps with Ix but it is not contained in Ix completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 24 ([1]</head><p>). Each task crosses at most s = log 1+ 1 + 1 intervals.</p><p>PROOF. The proof follows the same idea as in <ref type="bibr" target="#b1">[1]</ref>. Suppose that a task of job J starts in interval Ix = [Rx, Rx+1). Since Rx ≥ s b (J) for both b = {m, r}, i.e., both map and reduce tasks and s b (J) ≥ p b (J) by Lemma 22, we have Ix = Rx ≥ 2 p b (J). The s intervals following x sum in size to Ix/ 2 ≥ p b (J).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 25 ([1]</head><p>). With 1+ loss we can restrict our attention to schedules in which no small task crosses an interval.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximation schemes for minimizing average weighted completion time with release dates</title>
		<author>
			<persName><forename type="first">F</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Milis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Queyranne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Skutella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th FOCS</title>
		<meeting>40th FOCS</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="32" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Minimizing total flow time and total completion time with immediate dispatching</title>
		<author>
			<persName><forename type="first">N</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="268" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better scalable algorithms for broadcast scheduling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th ICALP</title>
		<meeting>37th ICALP</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="324" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A competitive algorithm for minimizing weighted flow time on unrelated machines with speed augmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Muralidhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st STOC</title>
		<meeting>41st STOC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="679" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-processor scheduling to minimize flow time with epsilon resource augmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th STOC</title>
		<meeting>36th STOC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online scheduling to minimize the maximum delay factor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th SODA</title>
		<meeting>19th SODA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Max-cover in map-reduce</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th WWW</title>
		<meeting>19th WWW</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Map-reduce for machine learning on multicore</title>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th NIPS</title>
		<meeting>20th NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On distributing symmetric streaming computations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Svitkina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th SODA</title>
		<meeting>19th SODA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="710" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Assigning tasks for efficiency in hadoop: Extended abstract</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd SPAA</title>
		<meeting>22nd SPAA</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistics-driven workload modeling for the Cloud</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Engineering Workshops at 26th ICDE</title>
		<meeting>Data Engineering Workshops at 26th ICDE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The complexity of flowshop and jobshop scheduling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R D</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1129" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimizing average flow-time : Upper and lower bounds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="603" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimizing total flow-time: The unrelated case</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Muralidhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ISAAC</title>
		<meeting>19th ISAAC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximability of flow shop scheduling</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th FOCS</title>
		<meeting>36th FOCS</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Implementation of a PTAS for scheduling with release dates. Algorithm Engineering and Experimentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="202" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An online scalable algorithm for minimizing k -norms of weighted flow time on unrelated machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st SODA</title>
		<meeting>21st SODA</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="95" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quincy: Fair scheduling for distributed computing clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd SOSP</title>
		<meeting>22nd SOSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal two-and three-stage production schedules with setup times included</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speed is as powerful as clairvoyance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kalyanasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pruhs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="617" to="643" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HADI: Fast diameter estimation and mining in massive graphs with Hadoop</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>CMU-ML-08-117</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scheduling algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Algorithms and Theory of Computation</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Atallah</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>chapter 34</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A model of computation for MapReduce</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th SODA</title>
		<meeting>20th SODA</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="938" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximating total flow time on parallel machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="875" to="891" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Data-Intensive Text Processing with MapReduce. Number 7 in Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Handbook of Scheduling: Algorithms, Models, and Performance Analysis, chapter Online Scheduling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pruhs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MapReduce optimization using regulated dynamic prioritization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th SIGMETRICS</title>
		<meeting>11th SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flowshop and jobshop schedules: Complexity and approximation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schuurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Woeginger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="136" to="152" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polynomial time approximation algorithms for machine scheduling: ten open problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schuurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Woeginger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scheduling</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="203" to="213" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A polynomial time approximation scheme for the two-stage multiprocessor flow shop problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schuurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Woeginger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convex quadratic and semidefinite programming relaxations in scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skutella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="242" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">FLEX: A slot allocation scheduling optimizer for MapReduce workloads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balmin</surname></persName>
		</author>
		<editor>Middleware</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th EuroSys</title>
		<meeting>5th EuroSys</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving MapReduce performance in heterogeneous environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX OSDI</title>
		<meeting>USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
