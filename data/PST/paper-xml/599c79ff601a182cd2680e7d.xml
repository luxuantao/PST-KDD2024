<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMP-Inspired Deep Networks for Sparse Linear Inverse Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Borgerding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Schniter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sundeep</forename><surname>Rangan</surname></persName>
						</author>
						<title level="a" type="main">AMP-Inspired Deep Networks for Sparse Linear Inverse Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0C5407D73898E107DB16C781CCE1281</idno>
					<idno type="DOI">10.1109/TSP.2017.2708040</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSP.2017.2708040, IEEE Transactions on Signal Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSP.2017.2708040, IEEE Transactions on Signal Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>compressive sensing</term>
					<term>approximate message passing</term>
					<term>random access</term>
					<term>massive MIMO</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has gained great popularity due to its widespread success on many inference problems. We consider the application of deep learning to the sparse linear inverse problem, where one seeks to recover a sparse signal from a few noisy linear measurements. In this paper, we propose two novel neuralnetwork architectures that decouple prediction errors across layers in the same way that the approximate message passing (AMP) algorithms decouple them across iterations: through Onsager correction. First, we propose a "learned AMP" network that significantly improves upon Gregor and LeCun's "learned ISTA." Second, inspired by the recently proposed "vector AMP" (VAMP) algorithm, we propose a "learned VAMP" network that offers increased robustness to deviations in the measurement matrix from i.i.d. Gaussian. In both cases, we jointly learn the linear transforms and scalar nonlinearities of the network. Interestingly, with i.i.d. signals, the linear transforms and scalar nonlinearities prescribed by the VAMP algorithm coincide with the values learned through back-propagation, leading to an intuitive interpretation of learned VAMP. Finally, we apply our methods to two problems from 5G wireless communications: compressive random access and massive-MIMO channel estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We consider the problem of recovering a signal s 0 ∈ R N from a noisy linear measurement y ∈ R M of the form 1 y = Φs 0 + w,</p><p>where Φ ∈ R M ×N represents a linear operator and w ∈ R M is additive white Gaussian noise (AWGN). In many cases of interest, M ≪ N . We will assume that the signal vector s 0 has an (approximately) sparse 2 representation in a known orthonormal basis Ψ ∈ R N ×N , i.e., that s 0 = Ψx 0 for some (approximately) sparse vector x 0 ∈ R N . Thus we define A ΦΨ ∈ R M ×N , write <ref type="bibr" target="#b0">(1)</ref> as</p><formula xml:id="formula_1">y = Ax 0 + w,<label>(2)</label></formula><p>M. Borgerding (email: borgerding.7@osu.edu) and P. Schniter (email: schniter.1@osu.edu) are with the Department of Electrical and Computer Engineering, The Ohio State University, Columbus OH. Their work was supported in part by the National Science Foundation under grants 1527162 and 1539960. S. Rangan (email: srangan@nyu.edu) is with the Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, 11201. His work was supported by the National Science Foundation under Grants 1302336, 1547332, and 1564142.</p><p>Portions of this work were presented at the 2016 IEEE Global Conference on Signal and Information Processing <ref type="bibr" target="#b0">[1]</ref>. 1 Although we focus on real-valued quantities for ease of illustration, the methods in this paper could be easily extended to the complex-valued case. 2 Although we focus on sparse signals, the methods in this paper can be applied to other signals, such as the finite-alphabet signals used in digital communications.</p><p>and seek to recover a sparse x 0 from y. In the sequel, we will refer to this problem as the "sparse linear inverse" problem. The resulting estimate x of x 0 can then be converted into an estimate s of s 0 via s = Ψ x.</p><p>The sparse linear inverse problem has received enormous attention over the last few years, in large part because it is central to compressive sensing <ref type="bibr" target="#b1">[2]</ref>. Many methods have been developed to solve this problem. Most of the existing methods involve a reconstruction algorithm that inputs a pair (y, A) and produces a sparse estimate x. A myriad of such algorithms have been proposed, including both sequential (e.g., greedy) and iterative varieties. Some relevant algorithms will be reviewed in Section II-A.</p><p>Recently, a different approach to solving this problem has emerged along the lines of "deep learning" <ref type="bibr" target="#b2">[3]</ref>, whereby a many-layer neural network is optimized to minimize reconstruction mean-squared error (MSE) on a large set of training examples<ref type="foot" target="#foot_0">3</ref> {(y (d) , x (d) )} D d=1 . Once trained, the network can be used to predict the sparse x 0 that corresponds to a new input y. Although the operator A and signal/noise statistics are not explicitly used when training, the learned network will be implicitly dependent on those parameters. Previous work (e.g., <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[8]</ref>) has shown that the deep-learning approach to solving sparse linear inverse problems has the potential to offer significant improvements, in both accuracy and complexity, over traditional algorithms like ISTA <ref type="bibr" target="#b8">[9]</ref> and FISTA <ref type="bibr" target="#b9">[10]</ref>. A short review of relevant concepts from deep learning will be provided in Section II-B.</p><p>In this paper, we show how recent advances in iterative reconstruction algorithms suggest modifications to traditional neural-network architectures that yield improved accuracy and complexity when solving sparse linear inverse problems. In particular, we show how "Onsager correction," which lies at the heart of the approximate message passing (AMP) <ref type="bibr" target="#b10">[11]</ref> and vector AMP (VAMP) <ref type="bibr" target="#b11">[12]</ref> algorithms, can be employed to construct deep networks that i) require fewer layers to reach a given level of accuracy and ii) yield greater accuracy overall. To our knowledge, the use of Onsager correction in deep networks is novel.</p><p>The contributions of our work are as follows. First, in Section III, we show how the soft-thresholding-based AMP algorithm from <ref type="bibr" target="#b10">[11]</ref> can be "unfolded" to form a feedforward neural network whose MSE-optimal parameters can be learned using a variant of back-propagation. The structure of the resulting "learned AMP" (LAMP) network is similar to that of learned ISTA (LISTA) <ref type="bibr" target="#b3">[4]</ref> but contains additional "bypass" paths whose gains are set in a particular way. While bypass paths can also be found in recently proposed "residual networks" <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and "highway networks" <ref type="bibr" target="#b14">[15]</ref>, the bypass paths in LAMP have a different topology and a different gain-control mechanism. We show numerically that LAMP's outputs are more accurate than those of LISTA at each iteration, in some cases by more than a factor of 10. To isolate the effects of LAMP's change in network topology, the aforementioned experiments restrict the shrinkage function to classical soft-thresholding.</p><p>Next, in Section IV, we show that the accuracy of LAMP can be significantly improved by learning jointly MSE-optimal shrinkage functions and linear transforms. In particular, we consider several families of shrinkage functions, each controlled by a small number of learnable parameters: piecewise linear functions, exponential shrinkage functions, cubic Bsplines, and Bernoulli-Gaussian denoisers. Our work in this section is inspired by <ref type="bibr" target="#b5">[6]</ref>, which learned cubic B-splines for ISTA, but goes farther in that it i) considers shrinkage families beyond splines, ii) jointly learns the shrinkage functions and linear transforms, and iii) includes Onsager correction.</p><p>Then, in Section V, we show how the VAMP algorithm from <ref type="bibr" target="#b11">[12]</ref> can be unfolded to form a feedforward neural network whose MSE-optimal linear-transforms and shrinkage-functions can be jointly learned using a variant of back-propagation. Interestingly, we find that learned LVAMP parameters are nearly identical to the prescribed matched-VAMP parameters (i.e., VAMP under statistically matched prior and likelihood) when the signal x is i.i.d. In this sense, matched VAMP "predicts" the parameters learned by back-propagation. Furthermore, since the parameters prescribed by VAMP have an intuitive interpretation based on MMSE estimation principles, VAMP "explains" the parameters learned by back-propagation.</p><p>Finally, in Section VII, we apply the proposed networks to two problems arising in 5th-generation (5G) wireless communications: the compressive random access problem and the massive-MIMO channel-estimation problem.</p><p>An early version of this work appeared in <ref type="bibr" target="#b0">[1]</ref>. There, we proposed the LAMP-ℓ 1 algorithm and compared it to LISTA. In this work, we go beyond <ref type="bibr" target="#b0">[1]</ref> by i) providing justification for our LAMP-ℓ 1 parameterization (in Appendix A), ii) jointly optimizing the shrinkage functions and the linear stages of LAMP, iii) proposing the LVAMP method, and iv) detailing two applications to 5G communications.</p><p>Notation: We use capital boldface letters like A for matrices, small boldface letters like a for vectors, (•) T for transposition, and a n = [a] n to denote the nth element of a. Also, we use A 2 for the spectral norm of A, a p = ( n |a n | p ) 1/p for the ℓ p norm of a when p &gt; 0, and a 0 = |{a n : a n = 0}| for the ℓ 0 or "counting" pseudo-norm of a. Likewise, we use Diag(a) for the diagonal matrix created from vector a, I N for the N × N identity matrix and 0 for the zero vector. For a random vector x, we denote its probability density function (pdf) by p(x) and its expectation by E[x]. For a random variable x, we denote its variance by var[x]. Similarly, we use p(•|y), E[•|y], and var[•|y] for the pdf, expectation, and variance (respectively) conditioned on y. We refer to the Dirac delta pdf using δ(x) and to the pdf of a Gaussian random vector x ∈ R N with mean a and covariance C using</p><formula xml:id="formula_2">N (x; a, C) = exp(-(x -a) T C -1 (x -a)/2)/ (2π) N |C|.</formula><p>Finally, we use sgn(•) to denote the signum function, where sgn(x) = 1 when x ≥ 0 and sgn(x) = -1 when x &lt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ITERATIVE ALGORITHMS AND DEEP LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Iterative Algorithms</head><p>One of the best known algorithmic approaches to solving the sparse linear inverse problem is through solving the convex optimization problem <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> x = arg min</p><formula xml:id="formula_3">x 1 2 y -Ax 2 2 + λ x 1 ,<label>(3)</label></formula><p>where λ &gt; 0 is a tunable parameter that controls the tradeoff between sparsity and measurement fidelity in x. The convexity of (3) leads to provably convergent algorithms and bounds on the performance of the estimate x (see, e.g., <ref type="bibr" target="#b17">[18]</ref>). In the sequel, we will refer to (3) as the "ℓ 1 " problem. 1) ISTA: One of the simplest approaches to solving (3) is the iterative shrinkage/thresholding algorithm (ISTA) <ref type="bibr" target="#b8">[9]</ref>, which iterates the steps (for t = 0, 1, 2, . . . and x 0 = 0)</p><formula xml:id="formula_4">v t = y -A x t (4a) x t+1 = η st x t + βA T v t ; λ ,<label>(4b)</label></formula><p>where β is a stepsize, v t is the iteration-t residual measurement error, and η st (•; λ) : R N → R N is the "soft thresholding" shrinkage function, defined componentwise as</p><formula xml:id="formula_5">[η st (r; λ)] j sgn(r j ) max{|r j | -λ, 0}.<label>(5)</label></formula><p>2) FISTA: Although ISTA is guaranteed to converge under β ∈ (0, 1/ A 2</p><p>2 ) <ref type="bibr" target="#b18">[19]</ref>, it converges somewhat slowly and so many modifications have been proposed to speed it up. Among the most famous is "fast ISTA" (FISTA) <ref type="bibr" target="#b9">[10]</ref>,</p><formula xml:id="formula_6">v t = y -A x t (6a) x t+1 = η st x t + βA T v t + t-2 t+1 ( x t -x t-1 ) ; λ ,<label>(6b)</label></formula><p>which converges in roughly an order-of-magnitude fewer iterations than ISTA (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>3) AMP: Recently, an approximate message passing (AMP) algorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref> was proposed for the ℓ 1 problem. The resulting algorithm, which we call AMP-ℓ 1 , manifests as</p><formula xml:id="formula_7">v t = y -A x t + b t v t-1 (7a) x t+1 = η st x t + A T v t ; λ t ,<label>(7b)</label></formula><p>where</p><formula xml:id="formula_8">x 0 = 0, v -1 = 0, t ∈ {0, 1, 2, . . . }, and b t = 1 M x t 0 (8) λ t = α √ M v t 2 .<label>(9)</label></formula><p>In <ref type="bibr" target="#b8">(9)</ref>, α is a tuning parameter that has a one-to-one correspondence with λ in (3) <ref type="bibr" target="#b19">[20]</ref>. Comparing AMP-ℓ 1 to ISTA, we see two major differences: i) AMP's residual v t in (7a) includes the "Onsager correction" term b t v t-1 , and ii) AMP's shrinkage threshold λ t in (7b) takes the prescribed, t-dependent value <ref type="bibr" target="#b8">(9)</ref>. In the sequel, we explain the rationale behind these differences.</p><p>AMP can in fact be used with any Lipschitz-continuous shrinkage function. For this, we write the AMP algorithm as</p><formula xml:id="formula_9">v t = y -A x t + b t v t-1 (10a) x t+1 = η x t + A T v t ; σ t , θ t ,<label>(10b)</label></formula><p>where x 0 = 0, v -1 = 0, t ∈ {0, 1, 2, . . . }, and</p><formula xml:id="formula_10">b t+1 = 1 M N j=1 ∂[η(r; σ t , θ t )] j ∂r j r= xt+A T vt<label>(11)</label></formula><formula xml:id="formula_11">σ 2 t = 1 M v t 2 2 .<label>(12)</label></formula><p>In writing (10b), we assume that the shrinkage function η accepts the noise-standard-deviation estimate σ t as an argument. Although this is not a required feature of AMP, we find it useful in the sequel. It is straightforward to show that AMP in ( <ref type="formula" target="#formula_9">10</ref>)-( <ref type="formula" target="#formula_11">12</ref>) reduces to AMP-ℓ 1 from ( <ref type="formula" target="#formula_7">7</ref>)-( <ref type="formula" target="#formula_8">9</ref>) when η(r t ; σ t , α) = η st (r t ; ασ t ) and θ t = α.</p><p>When A is a typical realization of a large i.i.d. sub-Gaussian random matrix with variance-M -1 entries and η(•) has identical scalar components, the Onsager correction decouples the AMP iterations in the sense that the input to the shrinkage function,</p><formula xml:id="formula_12">r t x t + A T v t ,<label>(13)</label></formula><p>can be accurately modeled as<ref type="foot" target="#foot_1">4</ref> </p><formula xml:id="formula_13">r t = x 0 + N (0, σ 2 t I N )<label>(14)</label></formula><p>with σ 2 t from <ref type="bibr" target="#b11">(12)</ref>. In other words, the Onsager correction ensures that the shrinkage input is an AWGN-corrupted version of the true signal x 0 with known variance σ 2 t . (See Fig. <ref type="figure">5</ref>(b) for numerical evidence.) The resulting "denoising" problem, that of estimating x 0 from r t , is well understood.</p><p>For example, when the elements of x 0 are statistically independent with known prior p(x) = N j=1 p j (x j ), the MSEoptimal denoiser<ref type="foot" target="#foot_2">5</ref> is simply the posterior mean estimator (i.e., x t+1,j = E{x j |r t,j ; σ t }), which can be computed in closed form for many distributions p j (•). In the case that p j (•) are unknown, we may be more interested in the minimax denoiser, i.e., the minimizer of the maximum MSE over an assumed family of priors. Remarkably, for generic sparse priors, i.e., p j (x j ) = (1γ)δ(x j ) + γ p j (x j ) with γ ∈ (0, 1) and arbitrary unknown p j (•), soft-thresholding (5) with a threshold proportional to the AWGN standard deviation (i.e., λ t = ασ t recalling <ref type="bibr" target="#b11">(12)</ref>) is nearly minimax optimal <ref type="bibr" target="#b19">[20]</ref>. Thus, we can interpret the AMP-ℓ 1 algorithm (7) as a nearly minimax approach to the sparse linear inverse problem under unknown p j (•)</p><p>The behavior of AMP is well understood when A is i.i.d. sub-Gaussian <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, but even small deviations from this model can lead AMP to diverge <ref type="bibr" target="#b23">[24]</ref> or at least behave in ways that are not well understood.</p><p>Algorithm 1 Vector AMP <ref type="bibr" target="#b11">[12]</ref> Require: LMMSE estimator η(•; σ, θ) from ( <ref type="formula" target="#formula_19">16</ref>), shrinkage η(•; σ, θ), max iterations T , parameters {θ t } T t=1 and θ. x t = η r t ; σ t , θ 5:</p><formula xml:id="formula_14">ν t = η ′ r t ; σ t , θ 6: r t = ( x t -ν t r t )/(1 -ν t ) 7: σ 2 t = σ 2 t ν t /(1 -ν t ) 8:</formula><p>// Shrinkage stage:</p><p>9:</p><p>x t = η(r t ; σ t , θ t ) 10:</p><formula xml:id="formula_15">ν t = η ′ (r t , σ t , θ t )</formula><p>11:</p><formula xml:id="formula_16">r t+1 = ( x t -ν t r t )/(1 -ν t ) 12:</formula><p>σ 2 t+1 = σ 2 t ν t /(1ν t ) 13: end for 14: Return x T .</p><p>4) Vector AMP: Very recently, the VAMP algorithm (see Algorithm 1) was proposed in <ref type="bibr" target="#b11">[12]</ref> to address AMP's fragility with respect to the matrix A. The VAMP algorithm retains all the desirable properties of the original AMP (i.e., low periteration complexity, very few iterations to convergence, and shrinkage inputs r t that obey the AWGN model ( <ref type="formula" target="#formula_13">14</ref>)), but over a much larger class of matrices: those that are large and right-rotationally invariant A.</p><p>A right-rotationally invariant matrix A is a random matrix whose distribution remains the same after right multiplication by any fixed orthogonal matrix. An intuitive understanding of such matrices arises from their singular value decomposition (SVD). Suppose that</p><formula xml:id="formula_17">A = U SV T (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>is the economy-sized 6 SVD of A ∈ R M ×N . For rightrotationally invariant A, the matrix V will contain the first R columns of a matrix that is uniformly distributed on the group of N × N orthogonal matrices. Note that i.i.d. Gaussian matrices are a special case of right-rotationally invariant, one where U is random orthogonal and s has a particular distribution. Importantly, VAMP behaves well under any orthogonal matrix U and any singular values s, as long as the dimensions M, N are large enough <ref type="bibr" target="#b11">[12]</ref>. The VAMP algorithm is defined in Algorithm 1. The algorithm can be seen to consist of two stages, each comprising the same four steps: estimation (lines 4 and 9), divergence computation (lines 5 and 10), Onsager correction (lines 6 and 11), and variance computation (lines 7 and 12). The only difference between the two stages is their choice of estimator. The first stage uses 6 By "economy-sized," we mean that if R rank(A) and s ∈ R R + contains the positive singular values of A, then S = Diag(s) ∈ R R×R , U T U = I R , and</p><formula xml:id="formula_19">η r t ; σ t , θ<label>(16)</label></formula><formula xml:id="formula_20">V Diag(s) 2 + σ 2 w σ 2 t I R -1 Diag(s)U T y + σ 2 w σ 2 t V T r t ,</formula><formula xml:id="formula_21">V T V = I R .</formula><p>which depends on the measurements y and the parameters θ {U , s, V , σ w }, <ref type="bibr" target="#b16">(17)</ref> while the second stage performs componentwise nonlinear shrinkage via η(r t ; σ t , θ t ), just as in step (10b) of the AMP algorithm. Lines 5 and 10 in Algorithm 1 compute the average of the diagonal entries of the Jacobian of η(•; σ t , θ) and η(•; σ t , θ t ), respectively. That is,</p><formula xml:id="formula_22">η ′ (r; σ, θ) 1 N N j=1 ∂[η(r; σ, θ)] j ∂r j .<label>(18)</label></formula><p>From ( <ref type="formula" target="#formula_19">16</ref>), we see that the Jacobian of η(•; σ t , θ) is</p><formula xml:id="formula_23">σ 2 w σ 2 t V Diag(s) 2 + σ 2 w σ 2 t I R -1 V T ,<label>(19)</label></formula><p>and so the average of its diagonal (or N -1 times its trace) is</p><formula xml:id="formula_24">η ′ r t ; σ t , θ = 1 N R i=1 1 s 2 i σ 2 t /σ 2 w + 1 . (<label>20</label></formula><formula xml:id="formula_25">)</formula><p>The first-stage estimator η(•; σ t , θ) in ( <ref type="formula" target="#formula_19">16</ref>) can be interpreted as computing the MMSE estimate of x 0 under the likelihood function</p><formula xml:id="formula_26">p(y|x 0 ) = N (y; Ax 0 , σ 2 w I),<label>(21)</label></formula><p>which follows from (2) under the assumption that w ∼ N (0, σ 2 w I) and the pseudo-prior</p><formula xml:id="formula_27">x 0 ∼ N ( r t , σ 2 t I).<label>(22)</label></formula><p>We refer to <ref type="bibr" target="#b21">(22)</ref> as a "pseudo" prior because it is constructed internally by VAMP at each iteration t. The MMSE estimate of x is then given by the conditional mean E{x|y}, which in the case of ( <ref type="formula" target="#formula_26">21</ref>)-( <ref type="formula" target="#formula_27">22</ref>) is</p><formula xml:id="formula_28">A T A + σ 2 w σ 2 t I N -1 A T y + σ 2 w σ 2 t r t .<label>(23)</label></formula><p>Replacing A in <ref type="bibr" target="#b22">(23)</ref> with its SVD from (15) yields the expression in <ref type="bibr" target="#b15">(16)</ref>. Since the estimate is linear in r t , we refer to the first stage as the "linear MMSE" (LMMSE) stage. The 2nd-stage estimator η(•; σ t , θ t ), in line 9 of Algorithm 1, essentially denoises the pseudo-measurement</p><formula xml:id="formula_29">r t = x 0 + N (0, σ 2 t ).<label>(24)</label></formula><p>The AWGN-corruption model <ref type="bibr" target="#b23">(24)</ref> holds under large, rightrotationally invariant A and η(•) with identical components, as proven in <ref type="bibr" target="#b11">[12]</ref>. If the prior p(x 0 ) on x 0 was known, <ref type="foot" target="#foot_3">7</ref> then it would be appropriate to choose the MMSE denoiser for η:</p><formula xml:id="formula_30">η(r t ; σ t , θ t ) = E{x 0 |r t },<label>(25)</label></formula><p>With an i.i.d. signal and MMSE denoiser, VAMP produces a sequence { x t } whose fixed points have MSE consistent with the replica prediction of MMSE from <ref type="bibr" target="#b25">[26]</ref>. In the sequel, we shall refer to VAMP with MMSE i.i.d.-signal denoising and known σ 2 w as "matched VAMP." In summary, VAMP alternates between i) MMSE inference of x 0 under likelihood N (y; Ax 0 , σ 2 w I) and pseudoprior N (x 0 ; r t , σ 2 t I), and ii) MMSE inference of x 0 under pseudo-likelihood N (r t ; x 0 , σ 2 t I) and prior x 0 ∼ p(x 0 ). The intermediate quantities r t and r t are updated in each stage of VAMP using the Onsager correction terms -ν t r t andν t r t , respectively, where ν t and ν t are the divergences<ref type="foot" target="#foot_4">8</ref> associated with the estimators η and η. Essentially, the Onsager correction acts to decouple the two stages (and iterations) of VAMP from each other so that local MSE optimization at each stage leads to global MSE optimization of the algorithm.</p><p>5) Comparison of ISTA, FISTA, AMP-ℓ 1 , and VAMP-ℓ 1 : For illustration, we now compare the average per-iteration behavior of ISTA, FISTA, AMP-ℓ 1 , and VAMP-ℓ 1 in two scenarios: i) for an A drawn i.i.d. N (0, M -1 ), and ii) when the singular values of the same A are replaced by a geometric series that yields the condition number κ(A) = 15. That is, s i /s i-1 = ρ ∀i &gt; 1, with ρ set to achieve the conditionnumber s 1 /s M = 15 and s 1 set to yield A 2 F = N . In both cases, the problem dimensions were N = 500 and M = 250; the elements of x 0 were i.i.d. N (0, 1) with probability γ = 0.1 and were otherwise set to zero (i.e., x 0 was Bernoulli-Gaussian); and the noise w was i.i.d. N (0, σ 2 w ), with σ 2 w set to yield a signal-to-noise ratio (SNR) E{ Ax 0 2 }/ E{ w 2 } of 40 dB. Recall that ISTA, FISTA, AMP-ℓ 1 , and VAMPℓ 1 all estimate x by iteratively minimizing (3) for a chosen value of λ (selected via α in the case of AMP and VAMP). We chose the minimax optimal value of α for AMP (which equals 1.1402 since γ = 0.1 <ref type="bibr" target="#b19">[20]</ref>) and VAMP, and we used the corresponding λ for ISTA and FISTA.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the average normalized MSE (NMSE) versus iteration t, where NMSE t</p><p>x tx 0 2 2 / x 0 2 2 and 1000 realizations of (x, w) were averaged. In Fig. <ref type="figure" target="#fig_0">1</ref>(a), we see that AMP-ℓ 1 required an order-of-magnitude fewer iterations than FISTA, which required an order-of-magnitude fewer iterations than ISTA. Meanwhile, we see that VAMP-ℓ 1 required about half the iterations of AMP-ℓ 1 . In Fig. <ref type="figure" target="#fig_0">1</ref>(b), AMP-ℓ 1 is not shown because it diverged. But VAMP-ℓ 1 required an orderof-magnitude fewer iterations than FISTA, which required an order-of-magnitude fewer iterations than ISTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning</head><p>In deep learning <ref type="bibr" target="#b2">[3]</ref>, training data {(y (d) , x (d) )} D d=1 comprised of (feature,label) pairs are used to train the parameters of a deep neural network, with the goal of accurately predicting the unknown label x 0 associated with a newly observed feature y. The deep network accepts y and subjects it to many layers of processing, where each layer usually consists of a linear transformation followed by a simple, componentwise nonlinearity.</p><p>Typically, the label space is discrete (e.g., y is an image and x is its class in {cat, dog, . . . , tree}). In our sparse Note that the horizontal axis is plotted on a log scale. linear inverse problem, however, the "labels" x are continuous and high-dimensional. Remarkably, Gregor and LeCun demonstrated in <ref type="bibr" target="#b3">[4]</ref> that a well-constructed deep network can accurately predict even labels such as these.</p><formula xml:id="formula_31">+ + + y B S S S x 1 x 2 x 3 x 4 η st η st η st η st</formula><p>The neural network architecture proposed in <ref type="bibr" target="#b3">[4]</ref> is closely related to the ISTA algorithm discussed in Section II-A1. To understand the relation, we rewrite the ISTA iteration (4) as</p><formula xml:id="formula_32">x t+1 = η st (S x t + By; λ) with B = βA T S = I N -BA<label>(26)</label></formula><p>and "unfold" the iterations t = 1, . . . , T , resulting in the Tlayer feed-forward neural network shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Whereas ISTA uses the values of S and B prescribed in <ref type="bibr" target="#b25">(26)</ref> and a common value of λ at all layers, Gregor and LeCun <ref type="bibr" target="#b3">[4]</ref> proposed to use layer-dependent thresholds λ [λ 1 , λ 2 , . . . , λ T ] and "learn" both the thresholds λ and the matrices B, S from the training data {(y (d) , x (d) )} D d=1 by minimizing the quadratic loss</p><formula xml:id="formula_33">L T (Θ) = 1 D D d=1 x T (y (d) ; Θ) -x (d) 2 2 .<label>(27)</label></formula><p>Here, Θ = [B, S, λ] denotes the set of learnable parameters and x T (y (d) ; Θ) the output of the T -layer network with input y (d) and parameters Θ. The resulting approach was coined "learned ISTA" (LISTA).</p><p>The LISTA network generated estimates of comparable MSE with significantly fewer matrix-vector multiplications than existing algorithms for the ℓ 1 problem (3) with optimally tuned regularization parameters (e.g., λ or α). As an example, for the i.i.d. Gaussian version of the problem described in Section II-A5, LISTA took only 16 layers to reach an NMSE</p><formula xml:id="formula_34">+ + - x t x t+1 v t v t+1 y y r t B t A t η st (r t ; λ t )</formula><p>Fig. <ref type="figure">3</ref>. The tth layer of the LISTA network, with learnable parameters At, Bt, and λt.</p><p>of -35 dB, whereas AMP-ℓ 1 took 25 iterations, <ref type="foot" target="#foot_5">9</ref> FISTA took 216, and ISTA took 4402. (More details will be given in Section VI-A.) Other authors have also applied ideas from deep learning to the sparse linear inverse problem. For example, <ref type="bibr" target="#b4">[5]</ref> extended the LISTA approach <ref type="bibr" target="#b3">[4]</ref> to handle structured sparsity and dictionary learning (when the training data are {y (d) } D d=1 and A is unknown). More recently, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> extended LISTA from the ℓ 2 + ℓ 1 objective of (3) to the ℓ 2 + ℓ 0 objective, and <ref type="bibr" target="#b5">[6]</ref> proposed to learn the MSE-optimal scalar shrinkage functions η by learning the parameters of a B-spline. It has also been proposed to recover signals using deep networks other than the "unfolded" type. For example, convolutional neural networks and stacked denoising autoencoders have been applied to speech enhancement <ref type="bibr" target="#b26">[27]</ref>, image denoising <ref type="bibr" target="#b27">[28]</ref>, image deblurring <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, image super resolution <ref type="bibr" target="#b30">[31]</ref>, 3D imaging <ref type="bibr" target="#b31">[32]</ref>, compressive imaging <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b34">[35]</ref>, and video compressive sensing <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEARNED AMP-ℓ 1</head><p>As described earlier, LISTA learns the value of the linear transform S ∈ R N ×N that minimizes MSE on the training data. As noted in <ref type="bibr" target="#b3">[4]</ref>, however, the LISTA's performance does not degrade after imposing the structure</p><formula xml:id="formula_35">S = I N -BA,<label>(28)</label></formula><p>where B ∈ R N ×M and A ∈ R M ×N , as suggested by <ref type="bibr" target="#b25">(26)</ref>.</p><p>Since the form of S in (28) involves 2M N free parameters, it is advantageous (in memory and training) over unstructured S when M &lt; N/2, which is often the case in compressive sensing. The structured S from (28) leads to network layers of the form shown in Fig. <ref type="figure">3</ref>, with first-layer inputs x 0 = 0 and v 0 = y.</p><p>Although not considered in <ref type="bibr" target="#b3">[4]</ref>, the network in Fig. <ref type="figure">3</ref> allows both A and B to vary with the layer t, allowing for a modest performance improvement (as will be demonstrated in Section VI-A) at the expense of a T -fold increase in memory and training complexity. We will refer to networks that use fixed A and B over all layers t as "tied," and those that allow t-dependent A t and B t as "untied."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The LAMP-ℓ 1 Network</head><p>We propose to construct a neural network by unfolding the iterations of AMP-ℓ 1 from <ref type="bibr" target="#b6">(7)</ref>. We then propose to learn the MSE-optimal values of the network parameters,</p><formula xml:id="formula_36">{A t , B t , α t } T -1 t=0 , from training data {(y (d) , x (d) )} D d=1</formula><p>. We will refer to this approach as "learned AMP-ℓ 1 " (LAMP-ℓ 1 ).</p><formula xml:id="formula_37">+ + + - × x t x t+1 v t v t+1 y y r t αt vt 2 √ M λ t B t A t η st (r t ; λ t ) b t+1</formula><p>Fig. <ref type="figure">4</ref>. The tth layer of the LAMP-ℓ 1 network, with learnable parameters At, Bt, and αt.</p><p>The hope is that it will require fewer layers than LISTA to yield an accurate reconstruction, just as AMP-ℓ 1 requires many fewer iterations than ISTA to do the same (when A is drawn i.i.d. Gaussian).</p><p>Figure <ref type="figure">4</ref> shows one layer of the LAMP-ℓ 1 network. Comparing LAMP-ℓ 1 to LISTA, we see two main differences:</p><p>1) LAMP-ℓ 1 includes a "bypass" path from v t to v t+1 that is not present in LISTA. This path implements an "Onsager correction" whose goal is to decouple the layers of the network, just as it decoupled the iterations of the AMP algorithm (recall Section II-A3). 2) LAMP-ℓ 1 's tth shrinkage threshold λ t = α t v t 2 / √ M varies with the realization v t , whereas LISTA's does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameterizing LAMP-ℓ 1</head><p>It is important to realize that LAMP-ℓ 1 implements a generalization of the AMP-ℓ 1 algorithm (7), wherein the matrices (A, A T ) manifest as (A t , B t ) at iteration t. In other words, the AMP algorithm enforces B t = A T t and A t = A 0 ∀t, whereas the LAMP-ℓ 1 network does not. An important question is whether this generalization preserves the independent-Gaussian nature <ref type="bibr" target="#b13">(14)</ref> of the shrinkage input error-the most important feature of AMP. We will show, numerically, that the desired behavior does seem to occur when</p><formula xml:id="formula_38">A t = β t A<label>(29)</label></formula><p>with β t &gt; 0, at least when A is i.i.d. Gaussian. Note that, in <ref type="bibr" target="#b28">(29)</ref>, "A" refers to the true measurement matrix from (2). If A was unknown, we could instead use an estimate of A computed from the training data, as described in Section III-C. But, in many applications of the sparse linear inverse problem, A is known. Furthermore, if matrix-vector multiplication with A was known to have a fast implementation (e.g., FFT), then it could be exploited in <ref type="bibr" target="#b28">(29)</ref>.</p><p>In Appendix A, we show that, under the parameterization (29) and some redefinitions of variables, the t th layer of the LAMP-ℓ 1 network can be summarized as</p><formula xml:id="formula_39">x t+1 = β t η st x t + B t v t ; αt √ M v t 2 (30a) v t+1 = y -A x t+1 + βt M x t+1 0 v t ,<label>(30b)</label></formula><p>with first-layer inputs x 0 = 0 and v 0 = y. The LAMP-ℓ 1 parameters are then</p><formula xml:id="formula_40">Θ = B, {α t , β t } T -1 t=0 in the tied case, or Θ = {B t , α t , β t } T -1</formula><p>t=0 in the untied case. Figure <ref type="figure">5</ref>(c) shows a quantile-quantile (QQ) plot for the error in the input to untied-LAMP's shrinkage function, ( x t + B t v t )x 0 , at a middle layer t, using the data from Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Also shown are the shrinkage inputs for ISTA and AMP. The figure shows that the quantiles of AMP-ℓ 1 and Standard Normal Quantiles Quantiles of Input Sample Fig. <ref type="figure">5</ref>. QQ plots of the shrinkage input error evaluated at the first iteration/layer t for which NMSE( xt) &lt; -15 dB (i.e., t = 1478 for ISTA, t = 6 for AMP-ℓ 1 , and t = 3 for untied LAMP-ℓ 1 .) The plots show that ISTA's error is heavy tailed while AMP-ℓ 1 's and LAMP-ℓ 1 's errors are Gaussian due to Onsager correction.</p><p>LAMP-ℓ 1 fall on the dashed diagonal line, confirming that they are Gaussian distributed. In contrast, the quantiles of ISTA are heavy-tailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning the LAMP-ℓ 1 Parameters</head><p>For the "tied" case of LAMP-ℓ 1 , we aim to learn the parameters</p><formula xml:id="formula_41">Θ tied T -1 B, {α t , β t } T -1 t=0</formula><p>that minimize the MSE on the training data, i.e., <ref type="bibr" target="#b26">(27)</ref>. In a first attempt to do this, we tried the standard back-propagation approach, where Θ tied T -1 were jointly optimized from the initialization B = A T , α 0 = 1, β 0 = 1, but we found that the parameters converged to a bad local minimum. We conjecture that this failure was a result of overfitting, since B had many free parameters in our experiments: 125 000, since B ∈ R 500×250 . Thus we propose a hybrid of "layer-wise" and "global" optimization that appears to avoid this problem.</p><p>Roughly speaking, our approach is to learn Θ tied 0 , then Θ tied 1 , and so on, until Θ tied T -1 . Recall that Θ tied t are not the parameters of layer t but the parameters of all layers up to and including layer t. The details of our approach are specified in Algorithm 2. There, line 5 performs layer-wise learning (of layer t) and line 6 performs global learning (of all layers up to and including t). Note that, in line 2, we do not learn the parameter β 0 but instead leave it at its initial value. The reason is that the triple {B, α 0 , β 0 } is over-parameterized, in that {µB, µα 0 , β 0 /µ} gives the same layer-0 output x 0 for any µ &gt; 0, due to the property η st (r; λ) = η st (µr; µλ)/µ of the soft-thresholding function.</p><p>To avoid this over-parameterization, we fix the value of β 0 .</p><p>Algorithm 2 Tied LAMP-ℓ 1 parameter learning</p><formula xml:id="formula_42">1: Initialize B = A T , α 0 = 1, β 0 = 1 2: Learn Θ tied 0 = {B, α 0 } 3: for t = 1 to T -1 do 4: Initialize α t = α t-1 , β t = β t-1 5: Learn {α t , β t } with fixed Θ tied t-1 6: Re-learn Θ tied t = B, {α i , β i } t i=1 , α 0 7: end for 8: Return Θ tied T -1</formula><p>For the untied case of LAMP-ℓ 1 , we aim to learn the parameters</p><formula xml:id="formula_43">Θ untied T -1 = {B t , α t , β t } T -1 t=0 .</formula><p>Here we found that extra care was needed to avoid bad local minima. To this end, we implemented a bootstrapping method based on the following rationale: a network that can choose a different B t for each layer t should perform at least as well as one that is constrained to use the same B for all layers t. In particular, our bootstrapping method checks performance against tied LAMPℓ 1 at each layer t and reinitializes using the tied parameters when appropriate. The details are given in Algorithm 3. As described in Section III-A, our LAMP-ℓ 1 parameterization (29) assumes that A is known. If A is unknown, it could be estimated using a least-squares (LS) fit 10 to the training data and further optimized along with the parameters</p><formula xml:id="formula_44">Algorithm 3 Untied LAMP-ℓ 1 parameter learning 1: Compute {Θ tied t } T -1 t=1 using Algorithm 2 2: Initialize B 0 = A T , α 0 = 1, β 0 = 1 3: Learn Θ untied 0 = {B 0 , α 0 } 4: for t = 1 to T -1 do 5: Initialize B t = B t-1 , α t = α t-1 , β t = β t-1 6: Learn {B t , α t , β t } with fixed Θ untied t-1 7: Set Θ untied t = {B i , α i , β i } t i=0 \ β 0 8: if Θ tied</formula><formula xml:id="formula_45">Θ tied T -1 or Θ untied</formula><p>T -1 to minimize the loss L T from <ref type="bibr" target="#b26">(27)</ref>. Empirically, we find (in experiments not detailed here) that there is essentially no difference between the final test MSEs of LAMP-ℓ 1 networks trained with known A or LS-estimated A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>In this section, we proposed a LAMP network whose nonlinear stages were constrained to the soft-thresholding shrinkage η st (•) from <ref type="bibr" target="#b4">(5)</ref>. Under this constraint, the resulting LAMP-ℓ 1 network differs from LISTA only in the presence of Onsager correction, allowing us to study the effect of Onsager correction in deep networks. The numerical experiments in Section VI-A show that, as expected, the LAMP-ℓ 1 network outperforms the LISTA network at every layer for the numerical data used to create Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LEARNED AMP</head><p>We now consider the use of generic shrinkage functions η(•) within LAMP with the goal of improving its performance over that of LAMP-ℓ 1 . In particular, we aim to learn the jointly MSE-optimal shrinkage functions and linear transforms across all layers of the LAMP network. To make this optimization tractable, we consider several families of shrinkage functions, where each family is parameterized by a finite-dimensional vector θ t at layer t. We then use back-propagation to learn the jointly MSE-optimal values of {θ t } T -1 t=0 and the lineartransform parameters. 10 For the least-squares learning of A, one could either use the one-shot approach A = Y X + where Y = [y (1) , ..., y (D) ] and X = [x (1) , ..., x (D) ] and X + is the pseudo-inverse of X, or one could use back-propagation to minimize the loss D d=1 y</p><formula xml:id="formula_46">(d) -Ax (d) 2 2 . + + + - × x t x t+1 v t v t+1 y y r t vt 2 √ M σ t B t A η (r t ; σ t , θ t ) b t+1</formula><p>Fig. <ref type="figure">6</ref>. The tth layer of the (general) LAMP network, with learnable parameters Bt and θt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The LAMP Network</head><p>For LAMP, we unfold the generic AMP algorithm (10) into a network. As with AMP-ℓ 1 , we relax the linear transform pair (A, A T ) to the layer-dependent learnable pair (A t , B t ), and then place the restrictions on A t to facilitate Onsager correction. With AMP-ℓ 1 , the restrictions came in the form of <ref type="bibr" target="#b28">(29)</ref>, where β t and B t emerged as the tunable parameters. It was then shown, in Appendix A, that β t acted to scale the output of the soft-thresholding function. Since the shrinkage functions that we use in this section will have their own scaling mechanisms, it now suffices to use <ref type="bibr" target="#b28">(29)</ref> with β t = 1. Under this parameterization, the tth layer of (general) LAMP becomes</p><formula xml:id="formula_47">x t+1 = η ( x t + B t v t ; σ t , θ t ) (31a) v t+1 = y -A x t+1 + b t+1 v t ,<label>(31b)</label></formula><p>with learnable parameters B t and θ t . See Fig. <ref type="figure">6</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameterizing the Shrinkage Functions</head><p>In the sequel, we consider families of shrinkage functions η (r; σ, θ) that are both separable and odd symmetric. By separable, we mean that [η (r; σ, θ)] j = η(r j ; σ, θ) ∀j for some scalar function η, and by odd symmetric we mean that η (r; σ, θ) = -η (-r; σ, θ) for all r ∈ R N . Several such shrinkage families are detailed below.</p><p>1) Scaled Soft-Threshold: We first consider</p><formula xml:id="formula_48">[η sst (r; σ, θ)] j θ 1 sgn(r j ) max{|r j | -θ 2 σ, 0},<label>(32)</label></formula><p>which can be recognized as a scaled version of the softthreshold operator from <ref type="bibr" target="#b4">(5)</ref>. Note that θ ∈ R 2 . It can be readily seen that LAMP-ℓ 1 from ( <ref type="formula" target="#formula_39">30</ref>) is a special case of LAMP from <ref type="bibr" target="#b30">(31)</ref> for which η = η sst and</p><formula xml:id="formula_49">θ t = [β t , α t ].</formula><p>2) Piecewise Linear: Next we consider (odd symmetric) piecewise linear functions with five segments:</p><formula xml:id="formula_50">η pwlin (r; σ, θ) j (33)                θ 3 r j if |r j | ≤ θ 1 σ sgn(r j ) θ 4 (|r j | -θ 1 σ) + θ 3 θ 1 σ if θ 1 σ &lt; |r j | ≤ θ 2 σ sgn(r j ) θ 5 (|r j | -θ 2 σ) + θ 4 (θ 2 -θ 1 )σ + θ 3 θ 1 σ if θ 2 σ &lt; |r j |.</formula><p>Here, the shrinkage-family parameters θ ∈ R 5 determine the abscissae of the four vertices where the line segments meet (i.e., [-θ 2 σ, -θ 1 σ, θ 1 σ, θ 2 σ]) and the slopes of the five segments (i.e., [θ 5 , θ 4 , θ 3 , θ 4 , θ 5 ]). The shrinkage in (33) can be considered as a generalization of (32) from three to five segments with a possibly non-zero slope on the middle segment. It is inspired by the design from [37, Eq. ( <ref type="formula" target="#formula_12">13</ref>)-( <ref type="formula" target="#formula_17">15</ref>)] but has a different parameterization and includes a dependence on the estimated noise level σ.</p><p>3) Exponential: We now consider the exponential shrinkage family</p><formula xml:id="formula_51">η exp (r; σ, θ) j θ 2 r j + θ 3 r j exp - r 2 j 2θ 2 1 σ 2 . (<label>34</label></formula><formula xml:id="formula_52">)</formula><p>The parameters θ ∈ R 3 control the asymptotic slope (i.e., θ 2 ), the slope at the origin (i.e., θ 2 + θ 3 ), and the rate of transition between those two slopes (where larger θ 1 gives a slower transition). The shrinkage in <ref type="bibr" target="#b33">(34)</ref> is inspired by the design from [37, Eq. ( <ref type="formula" target="#formula_23">19</ref>)-( <ref type="formula" target="#formula_24">20</ref>)] but includes a dependence on the estimated noise level σ. 4) Spline: Next we consider the spline shrinkage family</p><formula xml:id="formula_53">η spline (r; σ, θ) j θ 2 r j + θ 3 r j β r j θ 1 σ , (<label>35</label></formula><formula xml:id="formula_54">)</formula><p>where β is the cubic B-spline <ref type="bibr" target="#b37">[38]</ref> β(z)</p><formula xml:id="formula_55">     2 3 -|z| 2 + |z| 3 2 if 0 ≤ |z| ≤ 1 1 6 (2 -|z|) 3 if 1 ≤ |z| ≤ 2 0 if 2 ≤ |z|.<label>(36)</label></formula><p>Similar to <ref type="bibr" target="#b33">(34)</ref>, the parameters θ ∈ R 3 in (35) control the asymptotic slope (i.e., θ 2 ), the slope at the origin (i.e., θ 2 + 2 3 θ 3 ), and the rate of transition between those two slopes (where larger θ 1 gives a slower transition). The shrinkage in <ref type="bibr" target="#b34">(35)</ref> is inspired by that used in <ref type="bibr" target="#b5">[6]</ref>, but is parameterized differently. The shrinkage in <ref type="bibr" target="#b5">[6]</ref> was constructed using 8000 shifts of β(z) spread uniformly over the dynamic range of the signal, each scaled by an adjustable weight. By contrast, the shrinkage in <ref type="bibr" target="#b34">(35)</ref> has only three adjustable parameters but includes a dependence on the noise level σ. Furthermore, <ref type="bibr" target="#b5">[6]</ref> used identical shrinkage parameters at all layers of the ISTA network, whereas we allow the shrinkage parameters θ to vary across the layers of the LAMP network.</p><p>5) Bernoulli-Gaussian: Finally, we consider shrinkage functions that correspond to MSE-optimal denoisers under zero-mean Bernoulli-Gaussian (BG) priors. That is, x = E{x|r}, where x has the BG prior</p><formula xml:id="formula_56">p(x; γ, φ) = (1 -γ)δ(x) + γN (x; 0, φ)<label>(37)</label></formula><p>(with γ ∈ (0, 1) and φ &gt; 0) and r is an AWGN-corrupted measurement of x:</p><formula xml:id="formula_57">r = x + e for e ∼ N (0, σ 2 ).<label>(38)</label></formula><p>The MSE-optimal denoiser is then (see, e.g., <ref type="bibr" target="#b38">[39]</ref>)</p><formula xml:id="formula_58">x = r 1 + σ 2 φ 1 + 1-γ N (r;0,σ 2 ) N (r;0,σ 2 +φ) . (<label>39</label></formula><formula xml:id="formula_59">)</formula><p>To turn (39) into a learnable shrinkage function, we set θ 1 = φ and θ 2 = log 1-γ  γ and then simplify, giving η bg (r; σ, θ) j (40</p><formula xml:id="formula_60">) = r j 1 + σ 2 θ1 1 + 1 + θ1 σ 2 exp θ 2 - r 2 j 2(σ 2 +σ 4 /θ1)</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning the LAMP Parameters</head><p>As with LAMP-ℓ 1 , we consider two cases of LAMP: the "tied" case, where the same linear transform is used at all layers of the network, and the "untied" case where a different linear transform is allowed in each layer. Thus, the parameters for the tied LAMP are B, {θ t } T -1 t=0 and those for untied LAMP are {B t , θ t } T -1 t=0 . The LAMP parameters are then learned using the method described in Section III-C, now with {α t , β t } replaced by θ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>In this section, we constructed a "LAMP" deep network by unfolding the AMP algorithm from <ref type="bibr" target="#b10">[11]</ref>, parameterizing its linear and nonlinear stages in novel ways, and learning the parameters using a hybrid of layer-wise and global optimization. The numerical experiments in Section VI suggest that LAMP performs quite well with i.i.d. Gaussian A. For example, after 10 layers, untied LAMP's NMSE is 0.5 dB from the supportoracle bound and as much as 16 dB better than that of the (tied) LISTA approach from <ref type="bibr" target="#b3">[4]</ref>.</p><p>For non-i.i.d.-Gaussian A, and especially ill-conditioned A, however, the performance of LAMP suffers. Also, it is not clear how to interpret the parameters learned by LAMP, even in the case of i.i.d. Gaussian A. Both problems stem from the fact that LAMP can be viewed as a generalization of AMP that uses the matrices (β t A, B t ) in place of (A, A T ) at the t iteration. We aim to resolve these issues using the method presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNED VECTOR-AMP</head><p>As described in Section II-A, the behavior of AMP is well understood when A is i.i.d. sub-Gaussian, but even small deviations from this model can lead AMP to diverge or at least behave in ways that are not well understood. Very recently, however, the VAMP algorithm has been proposed as a partial solution to this problem. That is, VAMP enjoys the same benefits of AMP but works with a much larger class of matrices A: those that are right-rotationally invariant. Perhaps, by building a deep network around the VAMP algorithm, we can circumvent the problems with LAMP that arise with noni.i.d.-Gaussian matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The LVAMP Network</head><p>We propose to unfold the VAMP algorithm into a network and learn the MSE-optimal values of its parameters. The tth layer of the learned VAMP (LVAMP) network is illustrated in Fig. <ref type="figure" target="#fig_3">7</ref>. Essentially it consists of four operations: 1) LMMSE estimation, 2) decoupling, 3) shrinkage, and 4) decoupling, where the two decoupling stages are identical.</p><p>With an i.i.d. signal, the LMMSE estimator takes the form <ref type="bibr" target="#b22">(23)</ref>. Plugging the SVD (15) into ( <ref type="formula" target="#formula_28">23</ref>) yields <ref type="bibr" target="#b15">(16)</ref>. Thus, since VAMP assumes an i.i.d. signal, its LMMSE stage is parameterized by θ = {U , s, V , σ 2 w } for all iterations t (recall ( <ref type="formula">17</ref>)). For generality, we allow the LVAMP to vary these parameters with the layer t, giving θ t = {U t , s t , V t , σ 2 wt }. With non-i.i.d. (e.g., correlated) signals, the LMMSE estimator also depends on the signal covariance matrix, which may not be explicitly known. In this case, it makes more sense to parameterize LVAMP's layer-t LMMSE stage as</p><formula xml:id="formula_61">x t x t ν t ν t r t r t r t+1 σ t σ t σ t+1 η r t ; σ t , θ t η r t ; σ t , θ t xt-νt rt 1-νt = xt-νtrt 1-νt = σ t √ νt √ 1-νt = σ t √ νt √ 1-νt = LMMSE shrinkage decouple decouple</formula><formula xml:id="formula_62">η r t ; σ t , θ t = G t r t + H t y<label>(41)</label></formula><p>with unconstrained G t ∈ R N ×N and H t ∈ R N ×M , in which case θ t = {G t , H t }. In either case, the nonlinear stage is characterized by the shrinkage parameters θ t , whose format depends on which shrinkage family is being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning the LVAMP Parameters</head><p>As before, one can imagine "tied" and "untied" network parameterizations. In the tied case, the network parameters would be θ, {θ t } T t=1 , while in the untied case, they would be { θ t , θ t } T t=1 . But note that, with the SVD parameterization of η(•), even tied parameters θ yield an LMMSE estimator ( <ref type="formula" target="#formula_19">16</ref>) that varies with the layer t due to its dependence on σ t .</p><p>To learn the LVAMP parameters, we propose to use Algorithm 2 for the tied case and Algorithm 3 for the untied case (with θ t replacing B t and with θ t replacing {α t , β t }). When A is known, we suggest to initialize {U , s, V } at the SVD values from <ref type="bibr" target="#b14">(15)</ref>. When A is unknown, we suggest to initialize with an SVD of the least-squares estimate of A from the training data, as discussed in Section III-C. Finally, we suggest to initialize σ 2 w at the average value of M -1 y 2 across the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The numerical results in Section VI show that, with i.i.d. signals and the SVD parameterization of θ, the tied and untied versions of LVAMP perform near-identically. Furthermore they show that, as conjectured, the LVAMP network is much more robust to the matrix A than the LAMP network. And even for i.i.d. Gaussian A, LVAMP converges a bit faster than LAMP to a near-oracle MSE level.</p><p>Perhaps even more interesting is the finding that, with i.i.d. signals, the parameter values learned by the LVAMP network are essentially identical to the ones prescribed by the matched VAMP algorithm. Thus, the interpretability of the VAMP algorithm (i.e., the fact that it alternates between linear MMSE vector estimation and non-linear MMSE scalar estimation) translates directly to the LVAMP network. These and other findings will be discussed in more detail in Section VI-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary of Computational and Memory Complexity</head><p>We now outline the complexity and memory costs of the T -layer LISTA, LAMP, and LVAMP networks, assuming that</p><formula xml:id="formula_63">untied tied untied tied untied LISTA LISTA LAMP LAMP LVAMP computational complexity T N 2 T N 2 2T N M 2T N M 2T N M memory complexity T N 2 N 2 T M N M N T |θ|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I APPROXIMATE COMPUTATIONAL COMPLEXITY (PER VECTOR INPUT y)</head><p>AND MEMORY COMPLEXITY FOR T -LAYER NETWORKS.</p><p>M ≪ N and |θ| ≪ M 2 , where |θ| denotes the number of shrinkage parameters in θ. See Table <ref type="table">I</ref> for a summary. Untied LISTA learns B ∈ R N ×M , S t ∈ R N ×N , and θ t for t = 1 . . . T and does one matrix-vector multiply with S t in the t layer. Thus, if M ≪ N , its computational and memory complexities are ≈ T N 2 over T stages. Tied LISTA is similar except that there is only one S to learn, reducing its memory complexity to N 2 .</p><p>Untied LAMP learns B t ∈ R N ×M and θ t for t = 1 . . . T and does one matrix-vector multiply with B t and with A in the tth layer. Thus, its computational complexity is ≈ 2T N M and its memory complexity is ≈ T M N over T stages. Tied LAMP is similar except that there is only one B to learn, reducing its memory complexity to M N .</p><p>For LVAMP with i.i.d. signals and SVD-parameterized θ, we saw that untied and tied versions performed nearly identically. Furthermore, their learned parameters coincided with the ones prescribed by the matched VAMP algorithm. Thus, there is no need for LVAMP to learn and store the U , s, V quantities in θ, since they are known. LVAMP needs to learn and store only σ 2 w and the shrinkage parameters {θ t } T t=1 , for a total memory complexity of ≈ T |θ|. Meanwhile, each layer does a matrix-vector multiply with V and V T , since U y can be computed in advance. Thus the computational complexity over T layers is ≈ 2T N M . With the (G t , H t )-parameterized θ, the computational and memory complexities would both be ≈ T N 2 , as with untied LISTA.</p><p>Finally, we note that the computational complexities of LAMP and LVAMP decrease when A and V (or G t , H t ) have fast implementations (e.g., FFT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. NUMERICAL INVESTIGATION</head><p>We now investigate the effects of learning, Onsager correction, choice of shrinkage η(•), network untying, and matrix A through a sequence of experiments on synthetic data. The data was constructed in the same way as that for Fig. <ref type="figure" target="#fig_0">1</ref>, which we review now for convenience.</p><p>Recall the sparse linear inverse problem (2). For both training and test data, we constructed random realizations of BG-distributed sparse x 0 by drawing its elements i.i.d. N (0, 1) with probability γ = 0.1 and otherwise setting them equal to zero. Likewise, we generated random noise vectors w with i.i.d. N (0, σ 2 w ) elements, with σ 2 w set to yield an SNR E{ Ax 0 2 }/ E{ w 2 } of 40 dB. We considered two realizations of random A ∈ R M ×N with M = 250 and N = 500. The first was i.i.d. Gaussian, with elements distributed N (0, M -1 ) so that A 2 F ≈ N (i.e., the scaling expected by AMP). The second was constructed to have condition number κ(A) = 15. To construct this latter matrix, we started with the i.i.d. Gaussian A and replaced its singular values s i by a sequence constructed so that s i /s i-1 = ρ ∀i &gt; 1, with ρ and s 1 chosen so that s 1 /s M = 15 and A 2 F = N . We used mini-batches of size D = 1000 for training and a single mini-batch of size 1000 for testing (drawn independent of the training data, but from the same distribution). The training and testing methods were implemented 11 in Python using TensorFlow <ref type="bibr" target="#b39">[40]</ref> with the Adam optimizer <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Effect of Onsager Correction and Parameter Learning</head><p>First we study the effect of Onsager correction on deep networks. We do this by comparing the performance of LAMP-ℓ 1 and LISTA, which differ only in the use of Onsager correction. Simultaneously, we study the effect of parameter learning. We do this by comparing the performance of LAMPℓ 1 and AMP-ℓ 1 , which differ only in the use of parameter learning. For LAMP-ℓ 1 , we performed the learning as described in Section III-C. For LISTA, we used the same approach to learn "tied" Θ = B, S, {λ t } T -1 t=0 and "untied"</p><formula xml:id="formula_64">Θ = B, {S t , λ t } T -1</formula><p>t=0 , with no constraints on S t or B. Figure <ref type="figure">8</ref> shows average test-NMSE versus layer t for i.i.d. Gaussian A. The figure shows tied LAMP-ℓ 1 significantly outperforming both tied LISTA and AMP-ℓ 1 at each layer. For example, to reach NMSE = -34 dB, AMP-ℓ 1 took 25 iterations (see also Fig. <ref type="figure" target="#fig_0">1(a)</ref>), tied-LISTA took 15 layers, and tied-LAMP-ℓ 1 took only 7 layers.</p><p>Figure <ref type="figure">9</ref> shows the corresponding results for A with condition number κ = 15. For this A, AMP-ℓ 1 diverged (see also Fig. <ref type="figure" target="#fig_0">1(b)</ref>) but LAMP-ℓ 1 did not. Rather, tied LAMP-ℓ 1 gave roughly the same performance relative to tied LISTA as it did for the i.i.d. Gaussian case of A.</p><p>These figures also show that the untied versions of LAMPℓ 1 and LISTA yielded modest improvements over the tied versions for i.i.d. Gaussian A (i.e., ≤ 2 dB in Fig. <ref type="figure">8</ref>) and more significant benefits for A with κ = 15 (i.e., ≤ 3 dB in Fig. <ref type="figure">9</ref>). However, the untied versions incur a T -fold increase in parameter storage and significantly increased training time. Note that the greatest beneficiary of the untied configuration was LAMP-ℓ 1 with non-i.i.d.-Gaussian A. We conjecture that the LAMP-ℓ 1 network somehow used the extra freedom available in the untied case to counteract the non-i.i.d.-Gaussian nature of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Shrinkage η(•) and Matrix A</head><p>Next we study the effect of the shrinkage choice η(•) on network performance. We begin by examining the performance of LAMP under the different shrinkage families proposed in Section IV-B. In doing so, we will expose LAMP's lack of robustness to the matrix A. As a baseline, we also consider the support-oracle bound, which is now described. Suppose that an oracle provides knowledge of the support of x 0 . Then, since both the measurement noise w from (1) and the non-zero coefficients in x 0 are Gaussian, the minimum MSE (MMSE) estimate of x 0 from y can be computed in closed form. This support-oracle MMSE lower bounds the MSE of any practical estimator of x 0 , which does not know the support.</p><p>Figure <ref type="figure" target="#fig_4">10</ref> shows test-NMSE versus layer when the measurement matrix A is i.i.d. Gaussian. In the tied case, Fig. <ref type="figure" target="#fig_4">10</ref> shows that the NMSEs achieved by LAMP with the BG, exponential, piecewise linear, and spline shrinkage functions are about 5 dB better than those achieved by LAMP-ℓ 1 (or, equivalently, LAMP with scaled-soft-threshold shrinkage). Furthermore, the figure shows that there is relatively little difference in NMSE among the tied-LAMP networks with piecewise linear, exponential, spline, and BG shrinkage functions in this experiment.</p><p>Figure <ref type="figure" target="#fig_4">10</ref> also shows that, for the BG and piecewiselinear shrinkages, the NMSE achieved by untied <ref type="foot" target="#foot_6">12</ref> LAMP is about 1.5 dB better than that of tied LAMP and only about 0.5 dB away from the support-oracle bound after 10 layers. The difference between untied LAMP and (tied) LISTA from   [4] is remarkable, suggesting that the combination of Onsager cancellation and optimized shrinkage is quite powerful. Figure <ref type="figure" target="#fig_6">11</ref> shows test-NMSE versus layer when the measurement matrix A has condition number κ = 15. In the tied case, Fig. <ref type="figure" target="#fig_6">11</ref> shows that the NMSEs achieved by LAMP with the BG, exponential, and spline shrinkage functions are about 5 dB better than those achieved by LAMP-ℓ 1 , and that there is little difference among the NMSEs achieved by these shrinkage functions. But, surprisingly, the piecewise linear shrinkage performs significantly better than the other shrinkages with ≥ 10 layers and significantly worse with &lt; 10 layers.</p><p>With untied LAMP, Fig. <ref type="figure" target="#fig_6">11</ref> shows that BG shrinkage works very well: it dominates the other schemes at all layers t and comes within 1 dB of the support-oracle bound for t ≥ 13 layers. The piecewise-linear shrinkage works equally well with untied-LAMP for t ≥ 13 layers, but significantly worse with fewer layers.</p><p>Together, Figs. 10-11 suggest that LAMP behaves predictably with i.i.d. Gaussian A, but less predictably with non-i.i.d.-Gaussian A. That is, since the true signal has a BG distribution, we would expect that the use of BG shrinkage would yield performance at least as good as other shrinkages and close to oracle bounds. And this is precisely what happens with untied LAMP and i.i.d. Gaussian A. The fact that piecewise-linear shrinkage performs equally well under the same conditions can be explained by the fact that the piecewise-linear shrinkage function is flexible enough to mimic the BG shrinkage function. But when A is not i.i.d. Gaussian, Figs. 10-11 showed a strange gap in LAMP's performance with BG versus piecewise-linear shrinkages. This suggests that LAMP might not be properly handling the noni.i.d. Gaussian A. That said, LAMP is doing much better than AMP with this matrix, since AMP diverges. We conjecture that the B matrix (or B t matrices) learned by LAMP perform some sort of preconditioning that compensates for the noni.i.d.-Gaussian singular-value spectrum of A.</p><p>To further investigate the effect of measurement matrix A, we examine the behavior of LAMP and (SVD-parameterized) LVAMP on a matrix A with condition number κ = 100. (This matrix was constructed in the same way as the κ = 15 matrix but with a different singular-value ratio s i /s i-1 = ρ.) Figure <ref type="figure" target="#fig_7">12</ref> shows that tied LAMP converges much more slowly with this κ = 100 matrix; it takes many more layers for LAMP to attain a low NMSE. Moreover, there is a huge gap between the BG and piecewise-linear versions of LAMP, which again suggests that LAMP is not properly handling the κ = 100 matrix. In contrast, Fig. <ref type="figure" target="#fig_7">12</ref> shows tied LVAMP converging in 15 iterations to an NMSE that is not far from the oracle bound. The proximity between LVAMP and matched VAMP in Fig. <ref type="figure" target="#fig_7">12</ref> is also interesting and will be discussed further below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LVAMP's Robustness to the Matrix A</head><p>The experiments above suggest that LAMP performs well with i.i.d.-Gaussian A, but that its convergence rate (in layers) slows as the matrix A becomes less well conditioned. LVAMP, however, seems robust to ill-conditioning in A, based on the results in Fig. <ref type="figure" target="#fig_7">12</ref>. Thus, we now concentrate on evaluating LVAMP. In doing so, we focus on the BG and piecewise- linear shrinkage families, for the reasons below. Because x 0 is itself BG, the BG shrinkage should be optimal if the AWGN-corruption model ( <ref type="formula" target="#formula_29">24</ref>) holds. But, in practice, we may not always know the distribution of the true signal, which motivates the use of a flexible shrinkage family, like exponential, spline, or piecewise linear. Among those, our previous experiments showed that piecewise-linear shrinkage exposed weaknesses in the LAMP framework, although it performed well after many layers. Thus, we focus on the BG and piecewise-linear shrinkages when evaluating LVAMP. Figures <ref type="figure" target="#fig_0">13</ref><ref type="figure" target="#fig_0">14</ref>show test-NMSE versus layer for i.i.d. Gaussian A and A with condition number κ = 15, respectively. In those figures, "LVAMP" refers to both the tied and untied versions of LVAMP, which gave essentially identical NMSE. In fact, the connection is even stronger: at every layer t, the values of the parameters ( θ t , θ t ) learned by untied LVAMP were nearly identical to the values of the parameters ( θ, θ t ) learned by tied LVAMP (where here θ = {U , s, V , σ 2 w }). We will discuss this connection further in the Section VI-D.</p><formula xml:id="formula_66">2 4 6 8 10<label>12 14 -45 -40 -35 -30 -25 -20 -15 -10 LAMP</label></formula><p>Figure <ref type="figure" target="#fig_0">13</ref> shows test-NMSE versus layer when the measurement matrix A is i.i.d. Gaussian. There, we first notice that NMSE of LVAMP is about 2 dB better than that of tied LAMP for networks with &gt; 4 layers, for both BG and piecewise-linear shrinkage. Second, the NMSE of LVAMP is noticeably better than of untied LAMP for networks with 4-8 layers. But, with &gt; 10 layers, the two schemes perform equally well and within 0.5 dB of the support-oracle bound.</p><p>Figure <ref type="figure" target="#fig_0">14</ref> shows test-NMSE versus layer when the measurement matrix A has condition number κ(A) = 15. There, we first notice that NMSE of LVAMP is 2-5 dB better than that of tied LAMP for networks with &gt; 4 layers, for both BG and piecewise-linear shrinkage. Second, the NMSE of LVAMP is 0.5-2 dB better than of untied LAMP at all layers and within 0.5 dB of the support-oracle bound for ≥ 10 layers.</p><p>Looking at Figs. 12-14 together, we see that the advantage of LVAMP over untied-LAMP is relatively small for i.i.d. Gaussian A but grows with the condition number of A. We  wt }, those learned by LVAMP-BG <ref type="foot" target="#foot_7">13</ref> coincide almost perfectly with those prescribed by matched VAMP. In this sense, matched VAMP "predicts" the parameters learned by back-propagation.</p><p>But, beyond merely a prediction, matched VAMP offers an explanation of the parameters learned by LVAMP. Recall that the tth iteration of matched VAMP comprises four operations: 1) MSE-optimal vector estimation of x from measurements y = Ax + N (0, σ 2 w I) and pseudo-prior x ∼ N ( r t , σ 2 t I), 2) an Onsager decoupling stage that yields the pseudo-measurement r t = x + N (0, σ 2 t I), 3) MSE-optimal scalar estimation of i.i.d. x under pseudo-measurement r t and prior x ∼ j p j (x j ), and 4) an Onsager decoupling stage that yields the pseudo-prior parameters ( r t , σ 2 t ). From this understanding of matched VAMP, it follows that the linear stage of LVAMP learns parameters θ t that are MSEoptimal under the pseudo-prior x ∼ N ( r t , σ 2 t I) generated by the preceding Onsager decoupling stage. Likewise, the nonlinear stage of LVAMP learns shrinkage-function parameters θ t that are MSE-optimal under the pseudo-measurements r t = x + N (0, σ 2 t I) generated by the preceding Onsager decoupling stage.</p><p>From a practical standpoint, the significance of the agreement between LVAMP-BG and matched VAMP is somewhat diminished by the fact that both approaches used knowledge of the prior family on x 0 (in this case, BG). But LVAMP with piece-linear shrinkage performed just as well as matched VAMP in Figures <ref type="figure" target="#fig_7">12</ref><ref type="figure" target="#fig_0">13</ref><ref type="figure" target="#fig_0">14</ref>. And, for piecewise-linear shrinkage, no knowledge of the prior on x 0 was used (beyond i.i.d.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. APPLICATION TO 5G COMMUNICATIONS</head><p>In this section we demonstrate the application of our proposed methods to two important problems from 5th-generation (5G) wireless communications <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>: compressive random access and massive-MIMO channel estimation. As we describe in the sequel, both can be posed as instances of the sparse linear inverse problem described in Section I. For LVAMP, we used the LMMSE parameterization <ref type="bibr" target="#b40">(41)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Application to Compressive Random Access</head><p>5G communications systems will need to support the "internet of things," which will bring billions of everyday objects (e.g., light bulbs, washer/dryers, ovens, etc.) online. Since these devices will connect only sporadically and often have little data to communicate when they do, it is important that they can access the system with little control overhead.</p><p>Towards this aim, it has been suggested to assign, to each user (i.e., device) in a given cell, a unique length-M pilot sequence. When a user wants to connect the base station (BS), it waits to hear a synchronization beacon emitted by the BS and then broadcasts its pilots. The signal y received by the BS then takes the form in <ref type="bibr" target="#b1">(2)</ref>, where the nth column of A is the pilot sequence of the nth user; the nth entry of x 0 is determined by the activity of the nth user (i.e., x 0 n = 0 if inactive) as well as its propagation channel to the BS; and the vector w models out-of-cell interference and thermal noise. (See the detailed model in Appendix B.) Assuming that users are sporadically connected, the x 0 vector will be sparse, allowing the use of sparse signal recovery for joint user-activity detection and channel estimation <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>.</p><p>If the pilots A are drawn i.i.d. Gaussian and the number of users N is large, then the support-recovery analysis from <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">Corollary 2]</ref> says that, in order to accurately 14 detect the active subset of N users under activity rate γ ∈ (0, 1), the ℓ 1 approach (3) requires pilots of length M &gt; 2γN log[(1-γ)N ]. For example, with N = 512 users and activity rate γ = 0.01, this analysis suggests to use pilots of length M ≥ 64. Because M ≪ N and the user activities are random, this formulation is often referred to as "compressive random access."</p><p>We now numerically investigate the performance of LAMP and LVAMP on the compressive random access problem described above. For our experiment, we assumed that the pilots in A were i.i.d. QPSK (i.e., uniformly distributed over {j, 1, -j, -1}, where j √ -1). Such pilots are common, as they result in low peak-to-average power ratio at the transmitter. Also, we assumed that the activity/channel coefficients x 0 14 By "accurately" we mean that the probability of detection error converges to zero as N → ∞ <ref type="bibr" target="#b45">[46]</ref>. were distributed as described in Appendix B, assuming users uniformly distributed over a single hexagonal cell with a oneantenna BS (for simplicity). Finally, we assumed AWGN w with power adjusted to achieve SNR = 10 dB. For training, we used a single realization of A ∈ C M ×N and 1024 random draws of x 0 ∈ C N for each mini-batch, and for testing we used the same A and 1024 new random draws of x 0 . Finally, we assumed N = 512 users, activity rate γ = 0.01, and-inspired by the ℓ 1 -analysis from <ref type="bibr" target="#b45">[46]</ref>-pilots of length M = 64.</p><p>Figure <ref type="figure" target="#fig_9">15</ref> shows test-NMSE versus layer for the compressive random access problem described above. There we see that the LAMP and LVAMP methods significantly outperformed both tied and untied LISTA. For both the LAMP and LVAMP methods, the piecewise linear shrinkage performed about 0.5 dB better than the BG shrinkage, untied LVAMP performed about 0.5 dB better than untied LAMP, and untied LAMP performed about 0.5 dB better than tied LAMP. We conjecture that the small difference between untied LAMP and LVAMP is due to the i.i.d. property of the matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application to Massive-MIMO Channel Estimation</head><p>So-called "massive-MIMO" <ref type="bibr" target="#b46">[47]</ref> is likely to play a large role in 5G wireless <ref type="bibr" target="#b41">[42]</ref>. In such systems, the BS has a massive antenna array (i.e., dozens or hundreds of elements) and the user devices have single antennas. The idea is that, by making the number of BS antennas N r very large, the array gain becomes very large, which then drives both (in-cell) multiuser interference and thermal noise to very low levels. But doing so requires accurate channel-state information (CSI).</p><p>To obtain this CSI, it is envisioned that the users will simultaneously broadcast known pilots, which the BS will use to estimate the uplink channels. Through time-division duplex and channel reciprocity, the same estimates can be used for the downlink. The main bottleneck in such systems results from "pilot contamination" <ref type="bibr" target="#b46">[47]</ref>. That is, the pilots used in a given cell may be the same as those used in a neighboring cell, which results in contaminated channel estimates and hence out-of-cell interference that does not vanish as N r increases. One way to circumvent pilot contamination is to assign random pilots in every cell and estimate both the in-and out-of-cell user channels at each BS (assuming knowledge of the pilots in neighboring cells) <ref type="bibr" target="#b47">[48]</ref>. Although the computational complexity of such an approach may seem high, it can be reduced by processing each (of the N r ) receive angles separately. Because relatively few users contribute significant energy to a given receive angle, the per-angle channel coefficients are approximately sparse. The resulting channel-estimation problem takes the form of of <ref type="bibr" target="#b1">(2)</ref>, where now y represents the temporal measurements for a given receive angle, A ∈ C M ×N the pilots, x 0 the per-angle channel coefficients, and w thermal noise. Finally, M represents the pilot duration and N represents the total number of users in the primary and neighboring cells. (See Appendix B for details.)</p><p>We now numerically investigate the performance of LAMP and LVAMP on the massive-MIMO channel-estimation problem described above. For this, we assumed i.i.d. QPSK pilots A; 1 primary cell and 6 interfering cells (all hexagonal) with 64 users uniformly distributed within each cell (so that N = 7 × 64 = 448); pilot sequences of length M = 64; N r = 64 BS antennas; and an SNR of 20 dB. Channels x 0 were generated as described in Appendix B and w was AWGN. Different from our random-access formulation, all users transmit pilots, and w does not model interference from nearby cells (yielding higher SNR E{ Ax 0 2 }/ E{ w 2 }).</p><p>Figure <ref type="figure" target="#fig_0">16</ref> shows test-NMSE versus layer for the massive-MIMO channel estimation problem described above, where NMSE is measured only on the channels of primary-cell users. The results in the figure look as expected: for piecewise-linear shrinkage, the ranking (from best to worst at 6 layers) is LVAMP, untied LAMP, tied LAMP, untied LISTA, and tied LISTA. Meanwhile, piecewise-linear shrinkage outperformed BG shrinkage by roughly 0.5 dB. We conjecture that the small difference between untied LAMP and LVAMP is due to the i.i.d. property of the matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>We also tried implementing a convolutional neural network (CNN) to solve the two 5G problems above, but we did not obtain good results. In particular, we tried an implementation of the DeepInverse approach from <ref type="bibr" target="#b34">[35]</ref>. Although CNNs give state-of-the-art performance in image recovery, they do not appear to be well suited to problems where there is little structure other than sparsity. Conversely, CNNs are known to work very effectively in recovering richly structured signals, such as images, where our preliminary experiments with LAMP and LVAMP have not show state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we proposed two deep-learning approaches to the sparse linear inverse problem described in Section I. Our first approach, LAMP, is obtained by unfolding the AMP algorithm <ref type="bibr" target="#b10">[11]</ref> into a deep network and learning the network parameters that best fit a large training dataset. Although reminiscent of Gregor and LeCun's LISTA <ref type="bibr" target="#b3">[4]</ref>, it differs in i) the inclusion of Onsager correction paths that decouple errors across layers, and ii) joint learning of the linear transforms and nonlinear shrinkage functions. To avoid convergence to bad local minima, we proposed a reparameterization of AMP and a hybrid layer-wise/global learning strategy. Our second approach, LVAMP, is obtained by unfolding the VAMP algorithm <ref type="bibr" target="#b11">[12]</ref> into a deep network and learning its linear and nonlinear parameters using similar methods.</p><p>A synthetic numerical study showed that LAMP and LVAMP significantly outperformed LISTA in both convergence rate (in layers) and final MSE. And, while the performance of LAMP deteriorated with ill-conditioning in the matrix A, that for LVAMP did not. Interestingly, with i.i.d. signals, the network parameters learned by LVAMP were nearly identical to the ones prescribed by the matched VAMP algorithm, i.e., VAMP with statistically matched prior and likelihood. Thus, the MMSE-estimation principles that underlie VAMP offer an intuitive interpretation of LVAMP.</p><p>We also applied LAMP and LVAMP to two problems in 5G wireless communications: compressive random access and massive-MIMO channel estimation, where we saw gains relative to LISTA and more conventional deep CNNs. We conjecture that, for image recovery applications, it would be more appropriate to unfold and learn a multi-layer AMP <ref type="bibr" target="#b48">[49]</ref> or VAMP algorithm, which is a topic of ongoing work.</p><p>We also see value in extending the LAMP and LVAMP methods from the linear model <ref type="bibr" target="#b1">(2)</ref> to the generalized linear model y = f (Ax + w), where f (•) is a known, componentwise nonlinearity. For this, it may be possible to unfold the generalized AMP <ref type="bibr" target="#b49">[50]</ref> and VAMP <ref type="bibr" target="#b50">[51]</ref> algorithms into networks and learn improved network parameters from training data. Doing so would facilitate the application of AMPinspired deep networks to problems such as phase retrieval <ref type="bibr" target="#b51">[52]</ref> and quantized compressive sensing <ref type="bibr" target="#b52">[53]</ref>.</p><p>APPENDIX A DERIVATION OF LAMP-ℓ 1 EQUATIONS <ref type="bibr" target="#b29">(30)</ref> From Fig. <ref type="figure">4</ref>, the tth layer of LAMP implements x t+1 = η st ( x t + B t v t ; λ t ) (42a) v t+1 = y -A t x t+1 + b t+1 v t .</p><p>(42b) Substituting ( <ref type="formula" target="#formula_38">29</ref>) into <ref type="bibr" target="#b41">(42)</ref> gives </p><formula xml:id="formula_67">x t+1 = η st ( x t + B t v t ; λ t ) (43a) v t+1 = y -β t A x t+1 + b t+1 v t .<label>(43b)</label></formula><p>where β t β t+1 /β t and λ t β t λ t . Finally, using the definitions of λ t and b t from ( <ref type="formula" target="#formula_8">9</ref>) and <ref type="bibr" target="#b10">(11)</ref>, and defining α t β t α t , equations (44b) and <ref type="bibr" target="#b45">(46)</ref> imply that the tth layer of LAMP implements</p><formula xml:id="formula_69">x t+1 = β t η st x t + B t v t ; αt √ M v t 2<label>(47a)</label></formula><formula xml:id="formula_70">v t+1 = y -Ax t+1 + β t M x t+1 0 v t ,<label>(47b)</label></formula><p>where B t , β t , α t are freely adjustable parameters. To avoid an overabundance of notation in the main body of the paper, we rewrite (47) as (30) by redefining x t ← x t and dropping the bars on the remainder of the variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B 5G CHANNEL MODELING DETAILS</head><p>In this section we provide details for the system model used in Section VII. To save space, we present a general model that yields both compressive random access and massive-MIMO channel estimation as special cases. Consider a wireless system with N c nearby cells, where each cell contains up to N u single-antenna users and a BS with N r antennas. Each BS is assumed to use a uniform linear array with half-wavelength element spacing. The BSs are time-synchronized and periodically broadcast a beacon. Upon hearing the beacon, the active users simultaneously broadcast pilot waveforms that reach each BS through multipath propagation. The BS of interest will then measure, at discrete time m = 1 . . . M and antenna q = 1 . . . δ n a n (mTτ np )g np e jθnpq + [W ] mq , <ref type="bibr" target="#b47">(48)</ref> where, for user n = 1 . . . N c N u , the quantity δ n ∈ {0, 1} is the activity indicator; a n (t) is the pilot waveform; P n are the number of propagation paths; g np , τ np , and θ np are the gain, delay, and arrival angle of the pth path; T is the sampling interval; and W is noise and residual interference from faraway cells.</p><p>We model the path gain/loss as g np = h np /(1 + d ρ n ), where d n is the distance from the nth user to the BS, ρ is the path-loss exponent, and h np is a random fluctuation such that E{ p |h np | 2 } = 1. In our experiments, we used P n = 5 paths with angle spread 10 • and Rician fading with k-factor 10 for h np , and we used ρ = 4 for the path-loss exponent.</p><p>We assume that the waveforms a n (t) are approximately bandlimited to T -1 Hz and-for simplicity-that τ np ≪ T , yielding the "narrowband" approximation a n (mTτ np ) ≈ a n (mT ) a mn , so that</p><formula xml:id="formula_71">[Y ] mq = N n=1 a mn z nq + [W ] mq ,<label>(49)</label></formula><p>for N N c N u and z nq δ n Pn p=1 g np e jθnpq . Defining matrices A and Z elementwise as [A] mn a mn and [Z] nq z nq , equation <ref type="bibr" target="#b48">(49)</ref> reduces to Y = AZ + W .</p><p>The above path-based parameterization of Z is not convenient because the angles {θ np } Pn p=1 vary over the users n and are unknown. Without loss of generality, we instead work with the critically sampled <ref type="bibr" target="#b53">[54]</ref> angles {2πl/N r } Nr-1 l=0 , leading to</p><formula xml:id="formula_72">z nq = Nr-1 l=0</formula><p>x nl e j 2π Nr lq ,</p><p>where x nl can be interpreted as the nth user's contribution to the lth discrete receive direction. Defining X elementwise as [X] nl x nl , we can write Z = XF using DFT matrix F ∈ C Nr×Nr . Thus, after transforming the measurements Y into the angle domain via Y Y F H /N r and W W F H /N r , we obtain the linear model</p><formula xml:id="formula_74">Y = AX + W . (<label>51</label></formula><formula xml:id="formula_75">)</formula><p>We note that, if each user contributed significantly to at most D receive directions, then X would have at most N = DN c N u significant coefficients, meaning that each column would have at most DN c N u /N r . So, the columns of X become more sparse as the number of antennas N r grows.</p><p>By restricting attention to a particular receive angle (or using a single-antenna BS), we obtain a model of the form y = Ax + w, which coincides with the sparse linear inverse problem from (2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Average NMSE versus iteration for VAMP-ℓ 1 , AMP-ℓ 1 , FISTA, and ISTA under (a) i.i.d. Gaussian A and (b) A with condition number κ = 15.Note that the horizontal axis is plotted on a log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The feed-forward neural network constructed by unfolding T = 4 iterations of ISTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The tth layer of the LVAMP network, with learnable LMMSE parameters θt and learnable shrinkage parameters θt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Test NMSE versus layer under i.i.d. Gaussian A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Test NMSE versus layer under A with condition number 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Test NMSE versus layer (or versus iteration for matched VAMP) under A with condition number 100. The LVAMP traces represent both tied and untied SVD-parameter learning, which gave nearly identical results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Test NMSE versus layer for compressive random access. LVAMP used the LMMSE parameterization<ref type="bibr" target="#b40">(41)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>N r , [Y ] mq = N n=1 Pn p=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1: Select initial r 1 and σ 1 &gt; 0. 2: for t = 1, 2, . . . , T do</figDesc><table><row><cell>3:</cell><cell>// LMMSE stage:</cell></row></table><note><p>4:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Fig. 13. Test NMSE versus layer (or versus iteration for matched VAMP) under i.i.d. Gaussian A. The LVAMP traces represent both tied and untied SVD-parameter learning, which gave nearly identical results.</figDesc><table><row><cell></cell><cell>-bg tied</cell></row><row><cell></cell><cell>LAMP-pwlin tied</cell></row><row><cell></cell><cell>LAMP-bg untied</cell></row><row><cell></cell><cell>LAMP-pwlin untied</cell></row><row><cell></cell><cell>LVAMP-bg</cell></row><row><cell></cell><cell>LVAMP-pwlin</cell></row><row><cell>average NMSE [dB]</cell><cell>matched VAMP support oracle</cell></row><row><cell>layer / iteration</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Perhaps the most interesting behavior inFigures 12-14 is the following. The NMSEs achieved by the LVAMP networks are indistinguishable from those of the matched VAMP algorithm (i.e., VAMP under statistically matched i.i.d. signal and noise models) for all A under test. And looking at the parameters { θ t , θ t }, where θ t = {U t , s t , V t , σ 2</figDesc><table><row><cell></cell><cell>LAMP-bg tied</cell></row><row><cell></cell><cell>LAMP-pwlin tied</cell></row><row><cell></cell><cell>LAMP-bg untied</cell></row><row><cell></cell><cell>LAMP-pwlin untied</cell></row><row><cell></cell><cell>LVAMP-bg</cell></row><row><cell></cell><cell>LVAMP-pwlin</cell></row><row><cell>average NMSE [dB]</cell><cell>matched VAMP support oracle</cell></row><row><cell>layer / iteration</cell><cell></cell></row><row><cell cols="2">Fig. 14. Test NMSE versus layer (or versus iteration for matched VAMP)</cell></row><row><cell cols="2">under A with condition number 15. The LVAMP traces represent both tied</cell></row><row><cell cols="2">and untied SVD-parameter learning, which gave nearly identical results.</cell></row><row><cell cols="2">also see that, with LVAMP, there is essentially no difference</cell></row><row><cell cols="2">in the performance of BG shrinkage versus piecewise-linear</cell></row><row><cell>shrinkage for any A.</cell><cell></cell></row><row><cell cols="2">D. Equivalence of LVAMP and Matched VAMP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1053-587X (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSP.2017.2708040, IEEE Transactions on Signal Processing</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell></row><row><cell></cell><cell>-7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LISTA tied</cell></row><row><cell></cell><cell>-8</cell><cell></cell><cell></cell><cell></cell><cell>LISTA untied LAMP-bg tied</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LVAMP-bg untied</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LAMP-pwlin tied</cell></row><row><cell></cell><cell>-9</cell><cell></cell><cell></cell><cell></cell><cell>LAMP-pwlin untied</cell></row><row><cell>average NMSE [dB]</cell><cell>-11 -10</cell><cell></cell><cell></cell><cell></cell><cell>LVAMP-pwlin untied</cell></row><row><cell></cell><cell>-12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>layer</cell><cell></cell><cell></cell></row><row><cell cols="7">Fig. 16. Test NMSE versus layer for massive-MIMO channel estimation.</cell></row><row><cell cols="5">LVAMP used the LMMSE parameterization (41).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Defining x t β t x t and B t β t B t , we can write<ref type="bibr" target="#b42">(43)</ref> asx t+1 = β t+1 η st x t + B t v t β t ; λ t (44a) v t+1 = y -Ax t+1 + b t+1 v t .(44b)Since the soft thresholder (5) obeys η st (r; λ) = η st (βr; βλ)/β for any β &gt; 0, equation (44a) can be written asx t+1 = β t+1 β t η st x t + B t v t ; β t λ t(45)= β t η st x t + B t v t ; λ t ,</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Since orthonormal Ψ implies x = Ψ T s, training examples of the form {(y (d) , s (d) )} can be converted to {(y (d) , x (d) )} D d=1 via x (d) = Ψ T s (d) .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The AMP model (14)-(12) is provably accurate in the large-system limit (i.e., M, N → ∞ with M/N converging to a positive constant)<ref type="bibr" target="#b20">[21]</ref>,<ref type="bibr" target="#b21">[22]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>AMP with MSE-optimal denoising was first described in<ref type="bibr" target="#b22">[23]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>Although the prior and noise variance are often unknown in practice, they can be learned online using the EM-VAMP approach from<ref type="bibr" target="#b24">[25]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>Notice that the Onsager correction term b t+1 vt in AMP step (10a) also involves a (N/M -scaled) divergence, b t+1 , defined in<ref type="bibr" target="#b10">(11)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>The computational complexity of one layer of LISTA is essentially equal to one iteration of ISTA, FISTA, or AMP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>Figure 10  shows untied LAMP performance with only piecewise linear and BG shrinkage functions, but the performance with exponential and spline shrinkage functions is very similar.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>For the LVAMP traces in Figures 12-14, we did not use an {U , s, V } initialization that matched the SVD of A, as recommended in Section V-B. Rather, the {U , s, V } initialization was chosen randomly, to test if backpropagation would learn the matched values.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Onsager-corrected deep learning for sparse linear inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Borgerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Conf. Signal Info. Process</title>
		<meeting>IEEE Global Conf. Signal Info. ess</meeting>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
			<biblScope unit="page" from="227" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Compressed Sensing: Theory and Applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learning</title>
		<meeting>Int. Conf. Mach. Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning efficient structured-sparse models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learning</title>
		<meeting>Int. Conf. Mach. Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="615" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning optimal nonlinearities for iterative thresholding algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kamilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="747" to="751" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comp. Vision Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep ℓ 0 encoders</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2194" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlinear wavelet image processing: Variational problems, compression, and noise removal through wavelet shrinkage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="319" to="335" />
			<date type="published" when="1998-03">Mar. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Message passing algorithms for compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="18914" to="18919" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vector approximate message passing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03082</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Neural Inform. Process. Syst. Conf.</title>
		<imprint>
			<biblScope unit="page" from="550" to="558" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure &amp; Appl. Math</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphical models concepts in compressed sensing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compressed Sensing: Theory and Applications</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The dynamics of message passing on dense graphs, with applications to compressed sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="764" to="785" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universality in polytope phase transitions and message passing algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lelarge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. App. Prob</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="753" to="822" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Message passing algorithms for compressed sensing: I. Motivation and construction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inform. Theory Workshop</title>
		<meeting>Inform. Theory Workshop<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01">Jan. 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive damping and mean removal for the generalized approximate message passing algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech &amp; Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech &amp; Signal ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning and free energies for vector approximate message passing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech &amp; Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech &amp; Signal ess</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Support recovery with sparsely sampled free random matrices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Shitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="4243" to="4271" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep unfolding: Modelbased inspiration of novel deep architectures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Mitsubishi Electric Research Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR2014-117</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comp. Vision Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comp. Vision Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximal sparsity from deep networks?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4340" to="4348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A deep learning approach to structured signal recovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Allerton Conf</title>
		<meeting>Allerton Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1336" to="1343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ReconNet: Non-iterative reconstruction of images from compressively sensed random measurements</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kerviche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vision Pattern Recog</title>
		<meeting>IEEE Conf. Comp. Vision Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to invert: Signal recovery via deep convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, to appear</title>
		<meeting>ICASSP, to appear</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep fully-connected networks for video compressive sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spinoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<idno>arXiv:1603:04930</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Near optimal compressed sensing without priors: Parametric SURE approximate message passing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="2130" to="2141" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Splines: A perfect fit for signal and image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="38" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expectation-maximization Gaussian-mixture approximate message passing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="4658" to="4672" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Conf. on Learning Repres</title>
		<meeting>Internat. Conf. on Learning Repres</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What will 5G be?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Hanly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1065" to="1082" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse signal processing concepts for efficient 5G system design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wunder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="195" to="208" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A sparsity detection framework for on-off random access channels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Inform. Thy</title>
		<meeting>IEEE Int. Symp. Inform. Thy</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="169" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Compressive sensing based multi-user detection for machine-to-machine communication</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bockelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Schepker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dekorsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Emerging Telecomm. Tech</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="400" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sharp thresholds for high-dimensional and noisy recovery of sparsity using ℓ 1 -constrained quadratic programming (lasso)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2183" to="2202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling up MIMO: Opportunities and challenges with very large arrays</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Marzetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Edfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tufvesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="40" to="46" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Channel estimation for massive MIMO using Gaussian-mixture Bayesian learning</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1356" to="1368" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-layer generalized linear estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Manoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zdeborová</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06981</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalized approximate message passing for estimation with random linear mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.5141</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Inform. Thy</title>
		<meeting>IEEE Int. Symp. Inform. Thy</meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
			<biblScope unit="page" from="2168" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Vector approximate message passing for the generalized linear model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asilomar Conf. Signals Syst. Comput</title>
		<meeting>Asilomar Conf. Signals Syst. Comput</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1525" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Compressive phase retrieval via generalized approximate message passing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1043" to="1055" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Message-passing dequantization with applications to compressed sensing</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Kamilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="6270" to="6281" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deconstructing multi-antenna fading channels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sayeed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="2563" to="2579" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">He is now a Ph.D. student in ECE at the Ohio State Univ</title>
	</analytic>
	<monogr>
		<title level="m">Mark Borgerding received a B.S. in Biomedical and M.S in Electrical Engineering from Wright State Univ</title>
		<meeting><address><addrLine>Dayton OH</addrLine></address></meeting>
		<imprint>
			<publisher>Labs, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Flarion Technologies, a spin off of Bell Labs, which was acquired by Qualcomm Technologies in 2006. At Qualcomm, Dr. Rangan was a Director of Engineering</title>
	</analytic>
	<monogr>
		<title level="m">2000, he joined the ECE department of the Ohio State Univ., where he remains on the faculty. Sundeep Rangan (F&apos;16) received a B.A.Sc. from the University of Waterloo, Canada and an M.Sc. and Ph.D. from the</title>
		<meeting><address><addrLine>Ithaca, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Philip Schniter (F&apos;14) received B.S. and M.S. degrees from the Univ. of Illinois (Urbana-Champaign) and a Ph.D. degree from Cornell Univ</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of California, Berkeley, all in Electrical Engineering</orgName>
		</respStmt>
	</monogr>
	<note>all in Electrical Engineering. From 1993 to 1996, he worked at Tektronix Inc. in Beaverton, OR, as a systems engineer. In 2010, he joined the ECE department at the New York Univ. Polytechnic School of Engineering, where he remains on the faculty</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
