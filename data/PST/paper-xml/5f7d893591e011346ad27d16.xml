<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-05">5 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
							<email>hua.x@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Khoury College of Computer Sciences Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
							<email>wangluxy@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-05">5 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.02301v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often "rambling" without coherently arranged content.</p><p>In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-tosequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b31">Liu et al., 2019;</ref><ref type="bibr" target="#b27">Lewis et al., 2020</ref>), yet they are far from perfect. In generation tasks, although models like GPT-2 <ref type="bibr" target="#b41">(Radford et al., 2019)</ref> are able to produce plausible text, their spontaneous nature limits their utility in actual applications, e.g., users cannot specify what contents to include, and in what order.</p><p>To make large models more useful in practice, and to improve their generation quality, we believe it is critical to inform them of when to say what, which is addressed as content planning in traditional generation systems <ref type="bibr">(Duboue and McKeown,</ref> Content Plan (output by planning model):</p><p>(1) a communist 3 ▷ begin with 8 ▷ coherent ideology 15 ▷</p><p>[SEN] 21 (2) <ref type="bibr">[SEN]</ref> 4 (3) no evidence 2 ▷ any coherent 8 ▷ held beliefs 12 ▷ any topic 15 ▷ <ref type="bibr">[SEN]</ref> 18</p><p>Prompt: CMV. Donald Trump is a communist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template:</head><p>(1) __ 0 __ 1 __ 2 a communist __ 5 __ 6 __ 7 begin with __ 10 __ 11 __ 12 __ 13 __ 14 coherent ideology__ 17 __ 18 __ 19 __ 20 (2) __ 0 __ 1 __ 2 __ 3 (3) __ 0 __ 1 no evidence __ 4 __ 5 __ 6 __ 7 any coherent __ 10 __ 11 held beliefs __ 14 any topic __ 17</p><p>Draft (initial generation):</p><p>(1) Well call him a communist, you must begin with that Donald Trump has some kind of coherent ideology to begin with. (2) Which is unlikely.</p><p>(3) There is no evidence to suggest Donald Trump has any coherent or commonly held beliefs on any topic. Refined (final generation):</p><p>(1) To call him a communist, you must begin with that he has some kind of coherent ideology in the first place.</p><p>(2) He does not.</p><p>(3) There is no evidence whatsoever that Trump has any coherent, commonly held beliefs on any topic. for each sentence, segmented by special token <ref type="bibr">[SEN]</ref>, from which a template is constructed.</p><p>[Bottom] A draft is first produced and then refined, with updated words highlighted in italics.</p><p>2001; <ref type="bibr" target="#b47">Stent et al., 2004)</ref>. Specially designed control codes and auxiliary planning modules have been integrated into neural models <ref type="bibr" target="#b22">(Keskar et al., 2019;</ref><ref type="bibr" target="#b35">Moryossef et al., 2019;</ref><ref type="bibr" target="#b19">Hua and Wang, 2019)</ref>, yet those solutions require model architecture modification or retraining, making text generation with large models a very costly endeavor.</p><p>To this end, this work aims to bring new insights into how to effectively incorporate content plans into large models to generate more rele-vant and coherent text. We first study a planning model trained from BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> to produce the initial content plan, which assigns keyphrases to different sentences and predicts their positions. Next, we propose a contentcontrolled text generation framework, built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our generation model takes in a content plan consisting of keyphrase assignments and their corresponding positions for each sentence. The plan is encoded as a template, with <ref type="bibr">[MASK]</ref> tokens added at positions where no content is specified. Our model then outputs a fluent and coherent multi-sentence text (draft) to reflect the plan. This is done by fine-tuning BART without modifying its architecture.</p><p>Furthermore, we present an iterative refinement algorithm to improve the generation in multiple passes, within the seq2seq framework. At each iteration, tokens with low generation confidence are replaced with <ref type="bibr">[MASK]</ref> to compose a new template, from which a new output is produced. Unlike prior refinement algorithms that only permit editing in place, our solution offers more flexibility. Figure <ref type="figure" target="#fig_0">1</ref> exemplifies the refinement outcome. We call our system PAIR (Planning And Iterative Refinement). 1 It is experimented on three distinct domains: counter-argument generation with Reddit ChangeMyView data, opinion article writing with the New York Times (NYT) corpus 2 <ref type="bibr" target="#b44">(Sandhaus, 2008)</ref>, and news report production on NYT. Automatic evaluation with BLEU, ROUGE, and METEOR shows that, by informing the generation model with sentence-level content plans, our model significantly outperforms a BART model fine-tuned with the same set of keyphrases as input <ref type="bibr">( § 5.1)</ref>. Human judges also rate our system outputs as more relevant and coherent ( § 5.2). Additionally, our iterative refinement strategy consistently improves the generation quality according to both automatic scores and human evaluation. Finally, our model achieves better content control by reflecting the specified keyphrases in the content plan, whose outputs are preferred by human to another version with weaker control.</p><p>To summarize, our major contributions include:</p><p>• We propose a novel content planner built upon</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Content Planning as a Generation Component.</p><p>Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content <ref type="bibr" target="#b51">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b43">Rohrbach et al., 2018)</ref>, often with low relevance <ref type="bibr" target="#b28">(Li et al., 2016)</ref> and poor discourse structure <ref type="bibr">(Zhao et al., 2017;</ref><ref type="bibr" target="#b56">Xu et al., 2020)</ref>. Consequently, planning modules are designed and added into neural systems to enhance content relevance <ref type="bibr" target="#b52">(Wiseman et al., 2018;</ref><ref type="bibr" target="#b35">Moryossef et al., 2019;</ref><ref type="bibr">Yao et al., 2019;</ref><ref type="bibr" target="#b19">Hua and Wang, 2019)</ref>. However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change.</p><p>Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax <ref type="bibr" target="#b8">(Dušek and Jurčíček, 2016;</ref><ref type="bibr" target="#b13">Goyal and Durrett, 2020)</ref> and semantics <ref type="bibr" target="#b49">(Wen et al., 2015;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref> of the output. Specific applications encourage the model to cover a given topic <ref type="bibr" target="#b48">(Wang et al., 2017;</ref><ref type="bibr" target="#b45">See et al., 2019)</ref>, mention specified entities <ref type="bibr" target="#b10">(Fan et al., 2018)</ref>, or display a certain attribute <ref type="bibr" target="#b18">(Hu et al., 2017;</ref><ref type="bibr" target="#b32">Luo et al., 2019;</ref><ref type="bibr" target="#b0">Balakrishnan et al., 2019)</ref>. However, most existing work relies on model engineering, limiting the generalizability to new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model <ref type="bibr" target="#b4">(Dathathri et al., 2020)</ref>, which directly modifies the key and value states of GPT-2 <ref type="bibr" target="#b41">(Radford et al., 2019)</ref>. However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted to any off-the-shelf pre-trained Transformer generators.</p><p>Iterative Refinement has been studied in machine translation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Freitag et al., 2019;</ref><ref type="bibr" target="#b33">Mansimov et al., 2019;</ref><ref type="bibr" target="#b21">Kasai et al., 2020)</ref> to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs <ref type="bibr" target="#b12">(Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b25">Lawrence et al., 2019)</ref>. Our work uses BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref>, a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions <ref type="bibr" target="#b36">(Novak et al., 2016;</ref><ref type="bibr" target="#b55">Xia et al., 2017;</ref><ref type="bibr" target="#b50">Weston et al., 2018)</ref>, as we leverage the seq2seq architecture to offer more flexible edits.</p><p>3 Content-controlled Text Generation with PAIR Task Description. Our input consists of (1) a sentence-level prompt x, such as a news headline, or a proposition in an argument, and (2) a set of keyphrases m that are relevant to the prompt. The system aims to generate y that contains multiple sentences, as in a news report or an argument, by reflecting the keyphrases in a coherent way.</p><p>In this section, we first introduce content planning built upon BERT, that assigns keyphrases into sentences and predicts their positions ( § 3.1). Then we propose a seq2seq generation framework with BART fine-tuning that includes a given content plan derived from keyphrases m ( § 3.2). Finally, § 3.3 discusses improving generation quality by iteratively masking the less confident predictions and regenerating within our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content Planning with BERT</head><p>Our content planner is trained from BERT to assign keyphrases to different sentences and predict their corresponding positions. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the concatenation of prompt x and unordered keyphrases m is encoded with bidirectional selfattentions. Keyphrase assignments are produced autoregressively as a sequence of tokens m = {w j }, with their positions in the sentence s = {s j } predicted as a sequence tagging task.</p><p>We choose BERT because it has been shown to be effective at both language modeling and sequence tagging. Moreover, we leverage its segment embedding to distinguish the input and output sequences. Specifically, we reuse its pre-trained language model output layer for keyphrase assignment. We further design a separate keyphrase positioning layer to predict token position s j as the relative distance from each sentence's beginning:</p><formula xml:id="formula_0">p(s j |w ≤j ) = softmax(H L W s )<label>(1)</label></formula><p>where H L is the last layer hidden states of the Transformer, and W s are the newly added keyphrase positioning parameters learned during BERT fine-tuning. The range of allowed positions is from 0 to 127.</p><p>Noticeably, as our prediction is done autoregressively, attentions should only consider the generated tokens, but not the future tokens. However, BERT relies on bidirectional self-attentions to attend to both left and right. To resolve this discrepancy, we apply causal attention masks <ref type="bibr" target="#b6">(Dong et al., 2019)</ref> over m to disallow attending to the future (gray arrows in Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>Training the Planner. We extract keyphrases and acquire their ground-truth positions from humanwritten references, and fine-tune BERT with crossentropy losses for both assignment and positioning, with a scaling factor 0.1 over the positioning loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference. A [BOK]</head><p>token signals the beginning of keyphrase assignment generation. We employ a greedy decoding algorithm, and limit the output vocabulary to tokens in m and ensure each keyphrase is generated at most once. To allow sentence-level content planning, a special <ref type="bibr">[SEN]</ref> token is generated to represent the sentence boundary, with its predicted position indicating the length. The planning process terminates when [EOS] is produced.</p><formula xml:id="formula_1">Masked Template $ (&amp;'() Draft: * (&amp;)</formula><p>Decoder Encoder $ (&amp;)  mask update a communist begin with coherent ideology</p><formula xml:id="formula_2">[SEN] […] [M][M][M]a communist 3 [M][M][M]begin with 8 [M][M][M][M][M][M]coherent ideology 15 […]</formula><p>Well call him a communist 3 , you must begin with 8 Donald Trump has some kind of coherent ideology 15 <ref type="bibr">[…]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial template construction</head><p>Generation with content plan</p><formula xml:id="formula_3">$ (+)</formula><p>* (()   positions Figure 3: Our content-controlled text generation framework, PAIR, which is built on BART. Decoding is executed iteratively. At each iteration, the encoder consumes the input prompt x, the keyphrase assignments m , as well as a partially masked template (t (r−1) for the r-th iteration, [M] for masks). The autoregressive decoder produces a complete sequence y (r) , a subset of which is further masked, to serve as the next iteration's template t (r) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adding Content Plan with a Template</head><p>Mask-and-Fill Procedure</p><p>Given a content planning model, we invoke it to output keyphrase assignments to different sentences (m ), their corresponding positions s, along with each sentence's length (based on the prediction of <ref type="bibr">[SEN]</ref>). We first employ a post-processing step to convert between different tokenizers, and correct erroneous position predictions that violate the assignment ordering or break the consecutivity of the phrase (Appendix A). We then convert the plan into a template t (0) as follows: For each sentence, the assigned keyphrases are placed at their predicted positions, and empty slots are filled with [MASK] symbols. Figure <ref type="figure">3</ref> illustrates the template construction process and our seq2seq generation model. In Appendix B, we show statistics on the constructed templates.</p><p>The input prompt x, keyphrase assignments m , and template t (0) are concatenated as the input to the encoder. The decoder then generates an output y (1) according to the model's estimation of p(y (1) |x, m , t (0) ). y (1) is treated as a draft, to be further refined as described in the next section.</p><p>Our method is substantially different from prior work that uses constrained decoding to enforce words to appear at specific positions <ref type="bibr" target="#b15">(Hokamp and Liu, 2017;</ref><ref type="bibr" target="#b39">Post and Vilar, 2018;</ref><ref type="bibr">Hu et al., 2019)</ref>, which is highly biased by the surrounding few words and suffers from disfluency. Since BART is trained to denoise the masked input with contextual understanding, it naturally benefits our method.</p><p>Decoding. We employ the nucleus sampling strategy <ref type="bibr" target="#b16">(Holtzman et al., 2019)</ref>, which is shown to yield superior output quality in long text generation. In addition to the standard top-k sampling from tokens with the highest probabilities, nucleus sam-pling further limits possible choices based on a cumulative probability threshold (set to 0.9 in all experiments below). We also require the keyphrases to be generated at or nearby their predicted positions. Concretely, for positions that match any keyphrase token, we force the decoder to copy the keyphrase unless it has already been generated in the previous five tokens. We sample three times to choose the one with the lowest perplexity, as estimated by GPT-2 base <ref type="bibr" target="#b41">(Radford et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iterative Refinement</head><p>Outputs generated in a single pass may suffer from incorrectness and incoherence (see Figure <ref type="figure" target="#fig_0">1</ref>), therefore we propose an iterative refinement procedure to improve the quality. In each pass, tokens with low generation confidence are masked (Algorithm 1). This is inspired by iterative decoding designed for inference acceleration in non-autoregressive generation <ref type="bibr" target="#b26">(Lee et al., 2018;</ref><ref type="bibr" target="#b25">Lawrence et al., 2019)</ref>, though their refinement mostly focuses on word substitution and lacks the flexibility for other operations. Moreover, our goal is to improve fluency while ensuring the generation of given keyphrases.</p><p>At each iteration, the n least confident tokens are replaced with <ref type="bibr">[MASK]</ref>. Similar as the maskpredict algorithm <ref type="bibr" target="#b12">(Ghazvininejad et al., 2019)</ref>, we gradually reduce the number of masks. In our experiments, each sample is refined for 5 iterations, with n decaying linearly from 80% of |y (r) | to 0.</p><p>Training the Generator. Our training scheme is similar to masked language model pre-training. Given the training corpus D = {(x i , m i , y i )}, we consider two approaches that add noise to the target y i by randomly masking a subset of (1) any tokens, or (2) tokens that are not within the span Algorithm 1: Iteratively refinement via template mask-and-fill. The sample with the lowest perplexity (thus with better fluency) is selected for each iteration.</p><p>Data: prompt x, keyphrase assignments m , keyphrase positions s, R refinement iterations, ρ nucleus sampling runs Result: final output y (R) Construct template t (0) based on m and s ; for r = 1 to R do Run encoder over x ⊕ m ⊕ t (r−1) ; Y ← ∅ ; for i = 1 to ρ do Run nucleus sampling to generate y i with keyphrase position enforcement; Append y i to Y;</p><formula xml:id="formula_4">y (r) ← argmin y i ∈Y GPT2-PPL(y i ); n ← |y (r) | × (1 − r/R);</formula><p>Mask n tokens with the lowest probabilities to create new template t (r) ; of any keyphrase. The latter is better aligned with our decoding objective, since keyphrases are never masked. We concatenate x i , m i , and the corrupted target y i as input, and fine-tine BART to reconstruct the original y i with a cross-entropy loss.</p><p>4 Experiment Setups</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks and Datasets</head><p>We evaluate our generation and planning models on datasets from three distinct domains for multiparagraph-level text generation: (1) argument generation (ARGGEN) <ref type="bibr" target="#b19">(Hua et al., 2019)</ref>, to produce a counter-argument to refute a given proposition; (2) writing opinionated articles (OPINION), e.g., editorials and op-eds, to show idea exchange on a given subject; and (3) composing news reports (NEWS) to describe events. The three domains are selected with diverse levels of subjectivity and various communicative goals (persuading vs. informing), with statistics shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Task 1: Argument Generation. We first evaluate our models on persuasive argument generation, based on a dataset collected from Reddit r/ChangeMyView (CMV) in our prior work <ref type="bibr" target="#b19">(Hua et al., 2019)</ref>. This dataset contains pairs of original post (OP) statement on a contro- versial issue about politics and filtered high-quality counter-arguments, covering 14, 833 threads from 2013 to 2018. We use the OP title, which contains a proposition (e.g. the minimum wage should be abolished), to form the input prompt x. In our prior work, only the first paragraphs of high-quality counter-arguments are used for generation. Here we consider generating the full post, which is significantly longer. Keyphrases are identified as noun phrases and verb phrases that contain at least one topic signature word <ref type="bibr" target="#b30">(Lin and Hovy, 2000)</ref>, which is determined by a log-likelihood ratio test that indicates word salience. Following our prior work, we expand the set of topic signatures with their synonyms, hyponyms, hypernyms, and antonyms according to WordNet <ref type="bibr" target="#b34">(Miller, 1994)</ref>. The keyphrases longer than 10 tokens are further discarded.</p><p>Task 2: Opinion Article Generation. We collect opinion articles from the New York Times (NYT) corpus <ref type="bibr" target="#b44">(Sandhaus, 2008</ref>). An article is selected if its taxonomies label has a prefix of Top/Opinion. We eliminate articles with an empty headline or less than three sentences. Keyphrases are extracted in a similar manner as done in argument generation. Samples without any keyphrase are removed. The article headline is treated as the input, and our target is to construct the full article. Table <ref type="table" target="#tab_0">1</ref> shows that opinion samples have shorter input than arguments, and the keyphrase set also covers fewer content words in the target outputs, requiring the model to generalize well to capture the unseen tokens.</p><p>Task 3: News Report Generation.</p><p>Similarly, we collect and process news reports from NYT, filtering by taxonomy labels starting with "Top/News", removing articles that have no content word overlap with the headline, and ones with material-types labeled as one of "statistics", "list", "correction", "biography", or "review." News reports describe events and facts, and in this domain we aim to study and emphasize the impor- tance of faithfully reflecting content plans during generation and refinement.</p><formula xml:id="formula_5">ARGGEN OPINION NEWS B-4 R-L MTR Len. B-4 R-L MTR Len. B-4 R-L MTR Len. SEQ2SEQ 0.</formula><p>Data Split and Preprocessing. For argument generation, we split the data into 75%, 12.5%, and 12.5% for training, validation, and test sets. To avoid test set contamination, the split is conducted on thread level. For opinion and news generation, we reserve the most recent 5k articles for testing, another 5k for validation, and the rest (23k for news and 10k for opinion) are used for training. We apply the BPE tokenization <ref type="bibr" target="#b46">(Sennrich et al., 2016)</ref> for the generation model as BART does, and use WordPiece <ref type="bibr" target="#b54">(Wu et al., 2016)</ref> for BERT-based planner. To fit the data into our GPUs, we truncate the target size to 140 tokens for argument, sizes of 243 and 335 are applied for opinion and news, for both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our code is written in PyTorch <ref type="bibr" target="#b38">(Paszke et al., 2019)</ref>. For fine-tuning, we adopt the standard linear warmup and inverse square root decaying scheme for learning rates, with a maximum value of 5 × 10 −5 . Adam (Kingma and Ba, 2014) is used as the optimizer, with a batch size of 10 for refinement and 20 for content planning, and a maximum gradient clipped at 1.0. All hyperparameters are tuned on validation set, with early stopping used to avoid overfitting. More details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Comparisons</head><p>We consider two baselines, both are fine-tuned from BART as in our models: (1) SEQ2SEQ directly generates the target from the prompt; (2) KPSEQ2SEQ encodes the concatenation of the prompt and the unordered keyphrase set. To study if using only sentence-level keyphrase assignments helps, we include a model variant (PAIR light ) by removing keyphrase position information (s) from the input of our generator and using an initial template with all [MASK] symbols. Our model with full plans is denoted as PAIR full . We first report generation results using ground-truth content plans constructed from human-written text, and also show the end-to-end results with predicted content plans by our planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluation</head><p>We report scores with BLEU <ref type="bibr" target="#b37">(Papineni et al., 2002)</ref>, which is based on n-gram precision (up to 4-grams); ROUGE-L <ref type="bibr" target="#b29">(Lin, 2004)</ref>, measuring recall of the longest common subsequences; and METEOR <ref type="bibr" target="#b24">(Lavie and Agarwal, 2007)</ref>, which accounts for paraphrase. For our models PAIR full and PAIR light , we evaluate both the first draft and the final output after refinement. Table <ref type="table" target="#tab_1">2</ref> lists the results when ground-truth content plans are applied. First, our content-controlled generation model with planning consistently outperforms comparisons and other model variants on all datasets, with or without iterative refinement. Among our model variants, PAIR full that has access to full content plans obtains significantly better scores than PAIR light that only includes keyphrase assignments but not their positions. Lengths of PAIR full 's outputs are also closer to those of human references. Both imply the benefit of keyphrase positioning.</p><p>Table <ref type="table" target="#tab_1">2</ref> also shows that the iterative refinement strategy can steadily boost performance on both of our setups. By inspecting the performance of refinement in different iterations (Figure <ref type="figure" target="#fig_2">4</ref>), we observe that both BLEU and ROUGE-L scores gradually increase while perplexity lowers as the  refinement progresses. This indicates that iterative post-editing improves both content and fluency.</p><p>Results with Predicted Content Plans. We further report results by using content plans predicted by our BERT-based planner. Figure <ref type="figure" target="#fig_3">5</ref> compares PAIR full and PAIR light with KPSEQ2SEQ. Our models yield better METEOR scores on all three domains. That said, the improvement from predicted plans is not as pronounced as that from ground-truth plans. Upon inspection, we find that our planner often falls short of accurately positioning the given keyphrases, leading to degraded generation performance. This points to a potential direction for future work where better positioning model should be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>We hire four proficient English speakers<ref type="foot" target="#foot_0">3</ref> to rate three aspects of the generated arguments on a scale of 1 (worst) to 5 (best): fluency, coherence-if the information organization is natural and logical, and relevance-if the topic is related to the prompt and whether the stance is correct. 50 samples are randomly selected, with system outputs by KPSEQ2SEQ, PAIR full and PAIR light shown to human judges in random order. The evaluation PAIR full : Once again, the appalling problem of increasing debt in financing higher education that has taken place is more intolerable, particularly because of the arrogance of our colleges and universities. The burden of saddling college students with debt in the middle of their teenage years, when they were in debt, is essential for a good education. Our educational system is designed to allow kids to develop the skills necessary, but it does not create optimal conditions for mature students who know they will not be able <ref type="bibr">[. . . ]</ref> Table <ref type="table">4</ref>: Sample outputs in the news and opinion domain. Keyphrases assigned to different sentences are in boldface and color-coded.</p><p>guideline is in the supplementary material.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows that both of our models are rated with better coherence and relevance than KPSEQ2SEQ which uses the same but unordered keyphrases as input. Interestingly, outputs by PAIR light are regarded as more fluent and coherent, though the difference is not significant. However, discourse analysis in § 6 reveals that clauses produced by PAIR light are more locally related, compared to PAIR full , which can be perceived as easier to read. In addition to the sample argument in Figure <ref type="figure" target="#fig_0">1</ref>, Table <ref type="table">4</ref> shows PAIR full 's output in the news and opinion domains. More samples by different systems are in the supplementary material. Effect of Refinement and Keyphrase Enforce-ment. We further ask whether human judges prefer the refined text and whether enforcing keyphrases to be generated yields noticeable content improvement. In a second study, we present the same 50 prompts from the previous evaluation on argument generation, and an additional 50 samples for opinion article writing to the same group of human judge. For each sample, PAIR full 's outputs with and without refinement are shown in random order. Judges indicate their preference based on the overall quality. The same procedure is conducted to compare with a version where we do not enforce keyphrases to be copied at their predicted positions during decoding. What is updated during iterative refinement? Since refinement yields better text, we compare generations before and after the refinement. First, we find that masks are regularly put on "functional" words and phrases. For example, stopwords and punctuation along with their bigrams are often swapped out, with new words filled in to improve fluency. Moreover, about 85% of the refinement operations result in new content being generated. This includes changing prepositions and paraphrasing, e.g., replacing "a research fellow" with "a graduate student." On both news and opinion domains, numerical and temporal expressions are often incorrectly substituted, suggesting that better fact control needs to be designed to maintain factuality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Further Discussions on Discourse</head><p>Prior work's evaluation mainly focuses on fluency and content relevance, and largely ignores the discourse structure exposed by the generated text. However, unnatural discourse and lack of focus are indeed perceived as major problems of longform neural generations, as identified by human ex- perts. <ref type="foot" target="#foot_1">4</ref> Here, we aim to investigate whether contentcontrolled generation with ground-truth content plans resembles human-written text by studying discourse phenomena.</p><p>Are PAIR generations similar to humanwritten text in discourse structure? We utilize DPLP <ref type="bibr" target="#b20">(Ji and Eisenstein, 2014)</ref>, an off-theshelf Rhetorical Structure Theory (RST) discourse parser. DPLP converts a given text into a binary tree, with elementary discourse units (EDUs, usually clauses) as nucleus and satellite nodes. For instance, a relation NS-elaboration indicates the second node as a satellite (S) elaborating on the first nucleus (N) node. DPLP achieves F1 scores of 81.6 for EDU detection and 71.0 for relation prediction on news articles from the annotated RST Discourse Treebank <ref type="bibr" target="#b2">(Carlson et al., 2001)</ref>. We run this trained model on our data for both human references and model generations.</p><p>First, we analyze the depth of RST parse trees, which exhibits whether the text is more locally or globally connected. For all trees, we truncate at a maximum number of EDUs based on the 90 percentile of EDU count for human references. Distributions of tree depth are displayed in Figure <ref type="figure" target="#fig_4">6</ref>. As can be seen, generations by PAIR full show similar patterns to human-written arguments and articles. We also find that trees by PAIR light tend to have a more "linear" structure, highlighting the dominance of local relations between adjacent EDUs, compared with PAIR full which uses knowledge of keyphrases positions. This implies that content positioning helps with structure at a more global level. We further look into the ratios of NS, NN, SN relations, and observe that most model outputs have similar trends as human-written texts, except for KPSEQ2SEQ which has more SN relations, e.g., it produces twice as many SNs than others on arguments.</p><p>if b e c a u s e g iv e n le s t u n t il le s t Can PAIR correctly generate discourse markers? Since discourse markers are crucial for coherence <ref type="bibr" target="#b14">(Grote and Stede, 1998;</ref><ref type="bibr" target="#b1">Callaway, 2003)</ref> and have received dedicated research efforts in rulebased systems <ref type="bibr" target="#b42">(Reed et al., 2018;</ref><ref type="bibr" target="#b0">Balakrishnan et al., 2019)</ref>, we examine if PAIR full can properly generate them. For each sample, we construct sentence pairs based on content word overlaps between system generation and human reference. We manually select a set of unambiguous discourse markers from Appendix A of the Penn Discourse Treebank manual <ref type="bibr" target="#b40">(Prasad et al., 2008)</ref>. When a marker is present in the first three words in a reference sentence, we check if the corresponding system output does the same. Figure <ref type="figure">7</ref> displays the numbers of generated sentences with markers produced as the same in human references (correct) or not (wrong). The markers are grouped into three senses: CONTINGENCY, COMPARISON, and EXPANSION. The charts indicates that PAIR full does better at reproducing markers for CONTINGENCY, followed by COMPARISON and EXPANSION. Manual inspections show that certain missed cases are in fact plausible replacements, such as using at the same time for in addition, or also for further, while in other cases the markers tend to be omitted. Overall, we believe that content control alone is still insufficient to capture discourse relations, motivating future work on discourse planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethics Statement</head><p>We recognize that the proposed system can generate fabricated and inaccurate information due to the systematic biases introduced during model pretraining based on web corpora. We urge the users to cautiously examine the ethical implications of the generated output in real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We present a novel content-controlled generation framework that adds content planning to large pretrained Transformers without modifying model architecture. A BERT-based planning model is first designed to assign and position keyphrases into different sentences. We then investigate an iterative refinement algorithm that works with the sequenceto-sequence models to improve generation quality with flexible editing. Both automatic evaluation and human judgments show that our model with planning and refinement enhances the relevance and coherence of the generated content.  <ref type="bibr">2019)</ref> for training routines. To improve training efficiency, we adopt mixed-precision floating point (FP16) computation using the O2 option of NVIDIA apex<ref type="foot" target="#foot_2">5</ref> . For both training and decoding, we utilize the Titan RTX GPU card with 24 GB memory.</p><p>Model Sizes. Our generation model has the same architecture as BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref> with 406M parameters. The content planner is built on top of BERT base , which has 110M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Time.</head><p>Training the generation model takes 2.5 hours for argument, 5 hours for opinion, and 24 hours for news. The content planning model converges in 2.5-4 hours for three domains. Decoding Settings. At inference time, we set k = 50, temperature=1.0, and p = 0.9 for nucleus sampling. The relatively large k value is determined based on a pilot study, where we find that the refinement lacks diversity if k is set to small values. Moreover, since the Transformer states need to be cached during autoregressive decoding and we perform three complete nucleus sampling runs in each refinement iteration, the GPU memory consumption is substantially increased. We therefore limit the maximum generation steps to 140 for argument, 243 and 335 for opinion and news.</p><p>Auto-Correction for Content Plan. When the content plan is predicted by the planner, the following post-processing steps are employed prior to the  <ref type="bibr" target="#b46">(Sennrich et al., 2016)</ref>. KP distance denotes the average number of tokens between two keyphrases that are in the same sentence. Both system output (sys) and human reference (ref ) are reported. masked template construction: (1) For a predicted keyphrase, its token positions are adjusted to a consecutive segment, so that the phrase is kept intact in the template. (2) If the predicted positions are not monotonic to the assignment ordering, they will be rearranged. For instance, if the assignment contains KP 1 KP 2 , but position of KP 2 is not strictly larger than that of KP 1 , we instead place KP 2 immediately after KP 1 in the template. (3) Finally, since the planner and generator have different subword vocabularies, it is necessary to detokenize the predicted keyphrase assignment, and re-tokenize with the BPE vocabulary of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Template Construction Statistics</head><p>We characterize the content planning results in Table 6. Specifically, we show the statistics on the automatically created templates based on the planner's output. As we can see, our system predicted templates approach human reference in terms of length, per sentence keyphrase count, and the average keyphrase spacing. Sentence segmentation occurs more often in our templates than the reference text, likely due to the frequent generation of [SEN] tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Human Evaluation</head><p>As described in § 5.2 of the paper, we carry out two human evaluation studies. In the first study, the goal is to assess the output quality of three aspects. The detailed evaluation guideline and examples are listed in Figure <ref type="figure" target="#fig_5">8</ref> and Figure <ref type="figure" target="#fig_6">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Sample Output</head><p>From Figure <ref type="figure" target="#fig_0">10</ref> to Figure <ref type="figure" target="#fig_3">15</ref> we show more sample outputs, together with their human-written references.</p><p>This study aims to evaluate three text generation systems for counter-argument generation resembling the reddit ChangeMyView style (CMV). In total, 53 sets of samples will be presented. Each entry starts with a statement that has a stance over a certain topic. Machine generated responses will be listed under the title in random orders. Please first read the title and each of the three system outputs. Then rate each output over the following aspects on a likert scale (1-worst, 5-best). At the end of each entry, please also indicate the overall ranking of the four systems:</p><p>• Fluency: whether the output is free of grammar errors and easy to read -1. the output contains multiple major grammar errors that significantly reduce readability, e.g., "It suggesting looks you that eu perhaps a higher tax rate.". -3. the output contains at most one major grammar error, or up to three minor grammar errors, e.g., "Gender make complete senses, but not so with you. All sex is is the difference between masculinity and femininity." -5. the output is fluent and free of any grammar errors, e.g., "Perhaps the name "Aesop" is a reference to the religious philosophy of this sect, which channels Ramtha, as a spokesman for the Catholic Church."</p><p>• Coherence: whether the information transition is natural and well-structured -1. the output either has obvious self-contradiction or at least two major incoherent sections, e.g., "The EU is the way forward. Its not that different from other empires that didn't work out at the end." -3. the output contains at most one major incoherent section or up to three minor incoherent problems, e.g., "It may be that you have a ticket to die and you need to take it. That's why we are soldiers." -5. the information transition is natural and the overall message is clearly delivered, e.g., "The primary advantage a EU military has is the authority of the EU institutions, without which individual states cannot have coalitions on this level."</p><p>• Relevance: whether the response is on topic and has a clear opposing stance Topic: We shouldn't raise the minimum wage to $15 an hour.</p><p>-1. the output is generic or completely irrelevant, e.g., "I don't think it is untrue to believe in such an assumption." -3. the output mentions at least one major overlapping concepts to the topic, e.g., "Arguments for raising minimum wage should be the same as arguments for universal basic income. It will result in higher prices eventually." -5. the output is on topic and has a clear opposing stance, e.g., "The minimum wage was designed specifically for someone to be able to pay rent, utilities, and food. However, this standard is not met in most states." This study aims to compare some intervention strategies over the same model. The same 53 entries from study 1 will be evaluated. Additionally, 53 entries from a different domain (opinion) will be presented. For each entry, please read the title and three samples, then determine an overall rank, from best to the worst, e.g., "B &gt; A &gt; C" to indicate B is better than A, and A is better than C. We allow ties among samples if such order is difficult to decide, in which case please indicate with a "-" symbol, such as "B -A &gt; C".</p><p>When evaluating the examples, please consider fluency, coherence, and semantic correctness (e.g., whether the sample contradicts common sense, or misuses subjective/objective for certain predicates). An example is shown below:</p><p>• Entry: The Dog in My Life (and Maybe a Cat, Too)</p><p>-A: Thank you to John Tierney shedding light on the many health problems endured by adopted cats and dogs. I have my share of those problems and have recently adopted a purebred Himalayan cat. His concave face and abdomen makes breathing difficult, and none of the hairs on his ears are watted back, and resulting in irritation. He also has fur mats like cotton candy and requires constant clipping to keep him from being dirty and smeared. I told that some Himalayan owners consider him a "lion's skin." I am happy to say he is not, though I don't know why. I did not need anesthesia to remove all the fur from his face, tail and paws. Clearly, my adopted cat is healthy and is a lot of fun to be around. I am not afraid of him being around the house alone. -B: I thank Maureen Dowd for shedding light on the many health problems of Himalayans. I not, however, read the article about those problems when she first adopted a purebred Himalayan cat. Her concave face, which makes breathing difficult, and none of the hairs around her eyes are allowed to stand up, resulting in irritation and scratching. Her fur mats like cotton candy and requires constant clipping (even on the off-duty days). I have been told that some Himalayan owners think that their pet would be a "lion" or a "elephant," but I can't tell her that. The treatment, which I am told, is anesthesia and applies to the fur on her face, tail and paws. Like my first adopted cat, I was told if I didn't to stand in front of the electric chair. Instead, let her meow Iona. -C: Thank you to John Tierney shedding light on the many health problems facing dogs and cats in America. I can add one of my those problems: I recently adopted a purebred Himalayan cat. His concave face and body makes breathing difficult and the back of the hairs behind his ears are pulling down, pulling and resulting in irritation. He also has fur mats like cotton candy and requires constant clipping, which can be painful and painful. I have told that some Himalayan owners put on a "lion's skin." If the owner decides that this is correct, then he is right, and I should be prepared to anesthesia to remove all the fur from my face, tail and paws. Luckily, my adopted cat is extremely intelligent, which means that he would understand me if I asked him to leave me alone. We live in a very close community.</p><p>• &gt;&gt;&gt; overall ranking: A &gt; B -C</p><p>• Reasons: A is generally grammatical and coherent. C has some grammatical errors such as "I can add one of my those problems" and repetitions such as "pulling down, pulling and. . . ", "which can be painful and painful", and some semantic problems such as "remove all the fur from my face, tail and paws". B has grammatical errors such as "I not, however, read. . . ", "if I didn't to stand. . . ", and some semantic problems, such as "Her fur mats like cotton candy and requires constant clipping" and "stand in front of the electric chair".</p><p>(Annotators do not need to provide reasons for the rank.) Prompt: CMV : The EU need its own independent armed forces HUMAN:</p><p>I wonder what you think the primary benefit of a military under the authority of EU institutions would be. It strikes me that the EU is already able to deploy its armed forces without UN Security Council authorisation --the EU member states, or any subset of EU member states, can decide to do so. Do you think the primary advantage of a EU military is that military operations conducted under the authority of EU institutions would have a level of legitimacy that neither individual national states ( or coalitions of such states ) or the UN security council could provide? If so, can you say more about why you think that? KPSEQ2SEQ:</p><p>Well, if the EU is armed forces it has the authority. If the EU is armed forces it has the authority. Even the "authority" does not give them that advantage. If it does they are just not armed. PAIR light :</p><p>To me, the primary benefit of having a military is the authority of EU institutions. By having the EU armed forces, you have UN Security Council authorisation from the EU member states ( or any subset of EU member states, depending on who you ask ). I think the primary advantage of having a EU military is a level of legitimacy to have your own, your military operations conducted under the authority of EU institutions, and a level of legitimacy that it simply would not exist if they are conducted under the authority of individual national states or coalitions such as most states in the UN security council. 2 PAIR full w/o refine: I don't see the primary benefit of a military under the authority of EU institutions. Personally, I don't think the EU is much different from the UN armed forces... UN Security Council authorisation applies to the EU member states, but any subset of EU member states that is not fully armed cannot. Also, the primary advantage of a EU military is that military operations conducted under the authority of EU institutions still have a level of legitimacy which allows individual national states, or coalitions of such states, to the UN security council, so it's less likely to fall under the lessdelegated. So, I would argue that it PAIR full : I don't see the primary benefit of a military under the authority of EU institutions. Instead, I think the EU should set up it's own armed forces with UN Security Council authorisation, to the EU member states, or any subset of EU member states that wishes not to participate. Ultimately, the primary advantage of a EU military is that military operations conducted under the authority of EU institutions already have a level of legitimacy comparable to individual national states, or coalitions of such states, to the UN security council. I think there's likely to be less the other way around but likely to be more the other. )</p><p>Figure <ref type="figure" target="#fig_0">10</ref>: Sample outputs on argument generation.</p><p>Prompt: CMV : We shouldn't raise the minimum wage to $ 15 an hour or something similar. HUMAN: People need to live. They need to be able to afford basic things like food, housing, education, transportation and clothes. Currently, that isn't possible with most minimum wages in the US. It's disgraceful that businesses are allowed to let workers have one or more full time jobs without paying them enough to live a very basic life. Increasing the wages will decrease the profits of the company. But who does that hurt? In most cases it's the really wealthy owners, the CEO's and others in the top 1%. Is that really a bad thing? I'm not saying that they shouldn't be allowed to make lot of money, but do you really believe that they should be able to make that money at the cost of the people they employ? If the businesses aren't able to survive while paying living wages to their workers, maybe they shoud give the place to someone that can? KPSEQ2SEQ:</p><p>So why are there cases where wages will skyrocket, and not people who just do basic things. Why not? People should be able to have their wages grow and if you say that that money is the best then yes. But if that is not what you're arguing then it doesn't say much. PAIR light :</p><p>People want to earn as much as possible. They want to be able to afford basic things like food, housing, education, transportation, clothes, and so on. If minimum wages go up they will have to go up. Increasing the wages will decrease the profits of the company. In most cases these tend to be wealthy owners who are often the CEO's of the top 1% of the US. This is a bad thing. Because if you make lot of money, you make that money at the cost of the people around you. So the businesses need to be willing to pay living wages to their workers and give the place to it's workers. PAIR full w/o refine:</p><p>People should be given as little as it takes to afford basic things like food, housing, education, transportation, clothes, etc. However, when you think minimum wages should never be set. It's disgraceful for businesses to discriminate against their workers and work in more full time jobs, where someone has to pay for their basic life. Increasing the wages will decrease the profits for the company where it hurts most. And in most cases it's the extremely wealthy owners or the CEO, not necessarily about the top 1%. But this is a bad thing for everyone. You have to be willing to set aside somewhere between make lot's and make that money, but the cost is not all the PAIR full :</p><p>People can work as much as it takes to afford basic things like food, housing, education, transportation, clothes, etc. So, that is what minimum wages should be based on. It is disgraceful for businesses to discriminate against their workers because to get more full time jobs is not for them to live for their basic life. Increasing the wages will decrease the profits of the company they pay them for. In most cases, they do have wealthy owners -the CEO may be already in the top 1%. And that is a bad thing. It is much harder to find something to live for. Now they make lot more to make that money, but the cost is higher for the people )</p><p>Figure <ref type="figure" target="#fig_0">11</ref>: Sample outputs on argument generation.</p><p>Prompt: Putting Profit Above Health HUMAN:</p><p>Re "Chicken With Arsenic? Is That O.K.?" (Eating Well, April 5): Chicken with arsenic is most certainly not O.K. Most chickens raised for meat in the United States continue to be fed this poison, making most chicken meat at risk for contamination. It says a lot about the filthy, overcrowded conditions in which most factory-farmed chickens are confined that the addition of antibiotics or antiparasitic compounds like arsenic can result in such profitable increases in growth. By selectively breeding chickens for healthier immune systems instead of just for plumper meat, the industry can move away from risky crutches like arsenic. Especially now, with the threat of avian influenza, the industry can no longer be allowed to let its bottom line trump human or animal health. KPSEQ2SEQ:</p><p>Re "The Real Face of Chicken" (editorial, Feb. 16): A major health hazard of factory-farmed chickens raised for meat comes from contamination of meat and poultry with arsenic, pesticides and hormones. Arsenic is made by breeding chickens in the presence of antiparasitic compounds. As an alternative, people with higher blood levels may have healthier immune systems. People with high cholesterol may be able to eat more plumper meat. People with high blood pressure may be able to eat more of their own blood. The only question in this whole controversy is whether the animal farming industry values animal health over human health. For the vast majority of chickens raised for meat, the health risks are less than if they had been raised in natural settings instead of in the filthy, overcrowded conditions of factory farms. Even with the most sophisticated new technology, the industry continues to use antibiotics and heavy metal, making the supply of chicken meat much more vulnerable to contamination, including by avian influenza. It also puts people on risky crutches. PAIR light :</p><p>Re "Chicken With Arsenic? It Defends itself," by Michael O'Hanlon (Editorial Observer, April 2): Chicken with arsenic and cancer-causing, too, poses threat to humans by being raised for meat, making one of the world's largest source of chicken meat contamination. The F.D.A.'s recent report on meat-producing farms shows that at least half of the cases in the United States -the filthy, overcrowded conditions of factory-farmed chickens -have been related to diseases that range to the use of antibiotics and antiparasitic compounds like arsenic, which causes cancer in the body. Yet, in the name of cutting costs and breeding chickens with healthier immune systems and plumper meat, the industry has put the healthiest members of its ranks on, risky crutches, on the line -and, in this case, in the case of China, exposing them to arsenic. This is a practice that is a source of profit for the industry and, such as avian influenza, the industry's bottom line. PAIR full w/o refine:</p><p>Re "Chicken Arsenic? Not Really" (editorial, March 13): Your editorial raises several important Chicken Against arsenic issues. First, the fact that millions of chickens raised for meat are slaughtered each year means that there are millions of pieces of chicken meat that are without contamination. Second, the effects of the filthy, overcrowded conditions on factory and factory-farmed chickens mean that the natural addition of antibiotics and antiparasitic compounds to arsenic is unnecessary. And third, in addition to raising and breeding chickens with healthier immune systems and producing healthier and plumper meat, the industry also uses the equally risky crutches of arsenic. When it comes to the spread of avian influenza, the industry's bottom line should be a bottom-line decision, not an ethical health one. PAIR full :</p><p>Re "Chicken Arsenic? Get Out!" (editorial, April 21): Your editorial is right. Chicken has arsenic in its blood, and given that millions of chickens raised for meat are slaughtered every year, that is a large portion of all of chicken meat's sources of contamination. Second, another consequence of the filthy, overcrowded conditions of factory and factory-farmed chickens is that they are given antibiotics and antiparasitic compounds, arsenic and antibiotics. And third, by trying to save money and breeding chickens with healthier immune systems in the name of plumper meat, the industry is surviving on the risky crutches of arsenic. When it comes to the fear of avian influenza, the industry's bottom line should be bottom-line profits, not public health and safety. Re "Kraft to hire an ad agency" (front page, Jan. 12): Not only has the Chicago office of JWT, which is part of Kraft Foods, made an important contribution to the campaign to take the campaign off the ground, but it is also part of an industry trend that has seen major companies like JWT shift creative assignments to independent agencies. Both JWT and Kraft remain on the Kraft agency roster. The Chicago office has not changed its agency roster, so it is not part of the process. In fact, in the last month, we have created ads for potato chips, salad dressings and barbecue sauces that have never before been used by anyone bearing the Kraft brand name in an advertising campaign. It is also one of the last major, publicly traded advertising campaigns to be financed by one of the nation's big marketers. A 17-year-old girl and a 16-year-old boy were arraigned as adults in State Superior Court here yesterday in the stabbing death of a 34-year-old New Britain man attacked outside his home earlier this week. The teenagers, Elsie Figueroa, 17, and Luis Gabriel Rivera, 16, did not enter pleas to the charges of murder and conspiracy to commit murder, and each was being held in $1 million bail. In addition, a 13-year-old boy was charged as a juvenile in the killing and was scheduled to appear in juvenile court on Monday. His name was withheld because of his age. On Wednesday night, the New Britain man, Jaroslaw Sawicki, was found bleeding from stab wounds to the chest near the home he shared with his parents, the authorities said. KPSEQ2SEQ:</p><p>I am a 16-year-old boy, whose brother was murdered at the age of 13, a victim of Elsie Figueroa's violence in 1987, and who will be charged as a juvenile. I was one of the people at the scene when Luis Gabriel Rivera stabbed his first wife, Jaroslaw Sawicki, to death at the age of 14, and I was the only witness to a 34-year-old New man's stabbing death, with all of the help of a 13-year-old boy, who was bleeding from stab wounds. The teenagers charged are all in custody, and they have a good chance of not getting to see the New Britain man again. The young people charged, who have no previous criminal record, did not commit murder, and they should not be held accountable in juvenile court. Instead, they should be allowed to enter pleas of guilty, and they should be given the benefit of the doubt. PAIR light :</p><p>The parents of a 16-year-old boy charged with manslaughter in the stabbing death of a 34-year-old New Britain man were arrested. The teenagers were arraigned in Superior Court here on Thursday charges that they and two of the parents, Elsie Figueroa, and her son, identified as Luis Gabriel Rivera, were arraigned and forced to enter pleas to two counts of murder in the first degree, one conspiracy to commit murder and weapons possession. Mr. Castillo's father, Antonio Castillo Jr., who is also the father of a 13-year-old boy, was not released here as he was arraigned and charged as a juvenile in juvenile court. The indictments, which were reported Friday in The News of Westport, were made by the Suffolk County prosecutor's office. The authorities say they believe the New Britain man, Jaroslaw Sawicki, was stabbed six times in the head. The police said the cause of death of Mr. Castillo, who was bleeding from stab wounds, has not been determined but that surgically, Sawicki was dead by the time. Mr. Castillo's lawyer, Robert R. Gross, said, "We don't know anything that indicates that he was ever stabbed" and denied that his client was stabbed because of his age. "Our client is continuing to fight," Mr. Gross said. "We are going to continue to try to get to the bottom of this of the whole thing." PAIR full w/o refine:</p><p>The mother of a suspect here and a 16-year-old boy were arraigned yesterday on charges that they took part in the fatal stabbing death of a 34-year-old New Britain man on Nov. 3, officials said. The teenagers, Elsie Figueroa, 17, and Luis Gabriel Rivera, were not required to enter pleas; the two were charged with conspiracy to commit murder, a felony. Another suspect in the case, Jose Rodriguez, 17, a 13-year-old boy, charged as a juvenile, is to be tried as an adult in juvenile court on Nov. 15. All three are free on bond. The police said that the New Britain man, Jaroslaw Sawicki, was found bleeding from stab wounds to the neck and head. He was pronounced dead on Nov. 3 at PAIR full :</p><p>A pair of teenagers, one and a 16-year-old boy, were arraigned yesterday on charges that they took part in the stabbing death of a 34-year-old New Britain man on Dec. 18, officials said. The teenagers, Elsie Figueroa, 17, and Luis Gabriel Rivera, 16, declined to enter pleas for themselves and were charged with conspiracy to commit murder, a felony. Also yesterday in the case, the police, said, a 13-year-old boy was charged as a juvenile, and will likely be tried as an adult in court. Ms. Figueroa was released on bond. The police said that the New Britain man, Jaroslaw Sawicki, was found bleeding from stab wounds in the basement of his home and was pronounced dead at St. Joseph.</p><p>Figure <ref type="figure" target="#fig_3">15</ref>: Sample outputs on news generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An argument generation example using Reddit ChangeMyView. [Top] Partial output by our planner with keyphrase assignment and positions (in subscripts) for each sentence, segmented by special token [SEN], from which a template is constructed. [Bottom] A draft is first produced and then refined, with updated words highlighted in italics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Content planning with BERT. We use bidirectional self-attentions for input encoding, and apply causal self-attentions for keyphrase assignment and position prediction. The input (x, m) and output keyphrase assignments (m ) are distinguished by different segment embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results on iterative refinement with five iterations. Both BLEU and ROUGE-L scores steadily increase, with perplexity lowers in later iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: End-to-end generation results with automatically predicted content plans. Our models outperform KPSEQ2SEQ in both metrics, except for BLEU-4 on opinion articles where results are comparable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distributions of RST tree depth. PAIR full better resembles the patterns in human-written texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Evaluation guidelines on the first human study and representative examples on rating scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Evaluation guidelines on the second human study and annotation examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the three datasets. We report average lengths of the prompt and the target generation, number of unique keyphrases (# KP) used in the input, and the percentage of content words in target covered by the keyphrases (KP Cov.).</figDesc><table><row><cell></cell><cell cols="2"># Sample |Prompt| |Target| # KP KP Cov.</cell></row><row><cell cols="2">ARGGEN 56,504 19.4</cell><cell>116.6 20.6 30.5%</cell></row><row><cell cols="2">OPINION 104,610 6.1</cell><cell>205.6 19.0 26.0%</cell></row><row><cell>NEWS</cell><cell>239,959 7.0</cell><cell>282.7 30.3 32.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Key results on argument generation, opinion article writing, and news report generation. BLEU-4 (B-4), ROUGE-L (R-L), METEOR (MTR), and average output lengths are reported (for references, the lengths are 100, 166, and 250, respectively). PAIR light , using keyphrase assignments only, consistently outperforms baselines; adding keyphrase positions, PAIR full further boosts scores. Improvements by our models over baselines are all significant (p &lt; 0.0001, approximate randomization test). Iterative refinement helps on both setups.</figDesc><table><row><cell></cell><cell>76 13.80 9.36 97</cell><cell>1.42 15.97 10.97 156</cell><cell>1.11 15.60 10.10 242</cell></row><row><cell>KPSEQ2SEQ</cell><cell>6.78 19.43 15.98 97</cell><cell>11.38 22.75 18.38 164</cell><cell>11.61 21.05 18.61 286</cell></row><row><cell>PAIR light</cell><cell>26.38 47.97 31.64 119</cell><cell>16.27 33.30 24.32 210</cell><cell>28.03 43.39 27.70 272</cell></row><row><cell cols="2">PAIR light w/o refine 25.17 46.84 31.31 120</cell><cell>15.45 32.35 24.11 214</cell><cell>27.32 43.08 27.35 278</cell></row><row><cell>PAIR full</cell><cell>36.09 56.86 33.30 102</cell><cell>23.12 40.53 24.73 167</cell><cell>34.37 51.10 29.50 259</cell></row><row><cell cols="2">PAIR full w/o refine 34.09 55.42 32.74 101</cell><cell>22.17 39.71 24.65 169</cell><cell>33.48 50.27 29.26 260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation for argument generation on fluency, coherence, and relevance, with 5 as the best. Arrested in Theft of Baby Jesus Figurines PAIR full : Four New Jersey teenagers arrested yesterday were accused of stealing more than 25 plastic baby Jesus figurines from a church before they burn in a bonfire, the police said. The police in Sayreville, N.J., arrested Michael Payne, 18, and T.J. Jones, 18, of Union City; Nicholas M.</figDesc><table><row><cell>ARGGEN</cell><cell cols="3">Fluency Coherence Relevance</cell></row><row><cell cols="2">KPSEQ2SEQ 4.63</cell><cell>3.28</cell><cell>2.79</cell></row><row><cell>PAIR light</cell><cell>4.75</cell><cell>3.97  *</cell><cell>3.85  *</cell></row><row><cell>PAIR full</cell><cell>4.46</cell><cell>3.76  *</cell><cell>3.79  *</cell></row><row><cell cols="4">The Krippendorff's α are 0.28, 0.30, and 0.37, respec-</cell></row><row><cell cols="4">tively. Our model outputs are significantly more coher-</cell></row><row><cell cols="3">ent and relevant than KPSEQ2SEQ (</cell><cell></cell></row></table><note>* : p &lt; 0.0001), with comparable fluency. Prompt (News): 4 Hess, 18, of Matawan; and Jason L. O'Neill, 18, of Port New York, N.J., and charged them with burglary. Their vandals removed more than 100 figurines of Jesus from a cemetery outside St. Stanislaus Kostka Church in Sayreville, the police said. Detectives said the four had taken their flashlights to the cemetery and jotted down the license plates of several cars in the vicinity [. . . ] Prompt (Opinion): Drowning in a Sea of College Debt</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Table5demonstrates that the refined text is preferred in more than half of the cases, for both domains. Enforcing keyphrase generation based on their positions is also more favorable than not enforcing such constraint. Percentages of samples preferred by human judges before and after refinement[Left]; with and without enforcing keyphrases to appear at the predicted positions[Right]. Ties are omitted.</figDesc><table><row><cell cols="3">PAIR full w/o refine PAIR full w/o enforce</cell></row><row><cell>ARGGEN 52.7% 33.3%</cell><cell>45.3%</cell><cell>40.0%</cell></row><row><cell>OPINION 52.7% 30.7%</cell><cell>50.0%</cell><cell>29.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Discourse markers that are correctly and incorrectly (shaded) generated by PAIR full , compared to aligned sentences in human references. Discourse markers are grouped (from left to right) into senses of CONTINGENCY (higher marker generation accuracy observed), COMPARISON, and EXPANSION. y-axis: # of generated sentences with the corresponding marker.</figDesc><table><row><cell>10 3</cell><cell>correct Argument</cell><cell>wrong Opinion</cell><cell>News</cell></row><row><cell>10 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 0</cell><cell cols="3">t h o u g h h o w e v e r w h e t h e r d e s p it e in f a c t n o r y e t f o r e x a m p le f u r t h e r in a d d it io n in s t e a d n o t o n ly t h a t is</cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Statistics on generated templates by our content planner. Tokens are measured in units of Word-Piece</figDesc><table><row><cell></cell><cell cols="2">ARGGEN OPINION</cell><cell>NEWS</cell></row><row><cell></cell><cell>sys ref</cell><cell>sys ref</cell><cell>sys ref</cell></row><row><cell># tokens</cell><cell cols="3">133.3 130.2 228.5 246.3 424.5 435.5</cell></row><row><cell cols="4"># sentences 8.6 5.6 11.1 8.2 19.2 13.5</cell></row><row><cell cols="4"># KP per sent. 2.96 3.77 2.22 2.49 3.40 3.24</cell></row><row><cell cols="4">KP distance 2.61 2.95 5.70 6.02 3.76 5.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>interest your article about travel writers. While I'm sure that those travel writers who research guides to Goa and Romania have a rough time, the article really doesn't paint a complete picture of all travel writers. It's true that most of us don't make a fortune on our books and that research trips can be hectic, but some of us clearly do have the ideal job or hobby. I am an author of "Eating &amp; Drinking in Paris" and "Eating &amp; Drinking in Italy," among other travel guides. Can you imagine a better job than researching restaurants, bars and cafes in France and Italy? KPSEQ2SEQ: Regarding the article about travel writers ["Drinking, Eating and Pining" in April 15]: I've found that my hobby is researching restaurants and cafes. It is the ideal job for a traveler. I have been doing research trips to Goa, Greece, Romania, Italy, South America and Europe, among other travel guides. That's why I think all travel writers should get back on their feet. Being a part owner, traveling writer, my partner and I have traveled to India and have taken our sons and daughters on trips. I can't imagine a better job for me. If I have a rough time with time management, just write and I'll get it. My job does require overtime, but my life is not hectic enough. And the best part is that I can share my stories with all those travel writers who write about their trips. Travel writers should ask about research guides and why they write about it. PAIR light :I would like to thank Kerry Brown who reminded us what a wonderful part of the world it is for travel writers and the part of the world that requires so much of us. While she is probably one of those travel writers who "research and I," research guides in Goa, Romania or anywhere else she chooses, the travel writer has a rough time finding a home for all travel writers no matter where we are or how to do it. And for the most part, when we are not at home we do our research trips in a very short period of time, which is hectic and mind-nanding, but it is still the ideal job and hobby for us. Ms. Brown's article was timely, because one of the offices of the Food and Drug Administration, which is mentioned in the article as a source, wrote about a guide, "Eating and Drinking in Italy, along with the work of other travel guides, on how to eat well and drink there." I cannot imagine a better job than researching restaurants and cafes in Italy, as it would be to do so every day if I traveled to places as far as Italy and as far as parts of the world. PAIR full w/o refine:It was good to see about travel writers. I am a little sorry those travel writers and research guides from Goa and Romania had a rough time there, but there is a whole industry, which is all travel writers. They work very hard, very well, they can travel around the world (and do research trips that are hectic), and they are certainly more than the ideal job and hobby. I was very happy to find "Eating and Drinking in Italy," by Pia Portora Drinking in Italy, and other travel guides. Can you imagine a better job than researching restaurants, stores and cafes in Paris and Italy?. PAIR full :I was happy to see that travel writers are getting better. Many of those travel writers and research guides in Goa and Romania had a rough time there, but the vast majority of them, as is all travel writers. They all work hard and do a lot of research trips all over the world. They research trips, are hectic, and they are much more than the ideal job or hobby. I was particularly struck by the "Eating and Drinking in Italy," by Maliafra Drinking in Italy, and other travel guides. Can you imagine a better job than researching restaurants, hotels and cafes in places like Italy? I can. McGarry Bowen Added To Kraft Agency Roster HUMAN: Kraft Foods is expanding its agency roster by naming McGarry Bowen in New York to create campaigns for salad dressings, mayonnaise and barbecue sauces bearing the Kraft brand name. Spending was estimated at $30 million to $40 million. The hiring of McGarry Bowen is part of a trend on Madison Avenue, as big marketers like Kraft shift creative assignments to smaller, independent agencies from larger agencies owned by holding companies. The Chicago office of JWT, part of the WPP Group, had created ads for Kraft salad dressings, while the Chicago office of Foote Cone &amp; Belding, part of the Interpublic Group of Companies, had created ads for the other products. Both JWT and Foote Cone remain on the Kraft agency roster. KPSEQ2SEQ:</figDesc><table><row><cell>Prompt: The World's Best Job</cell></row><row><cell>HUMAN:</cell></row><row><cell>I read with Figure 13: Sample outputs on opinion generation.</cell></row><row><cell>Chicago, April 23, 2005</cell></row><row><cell>Figure 12: Sample outputs on opinion generation.</cell></row></table><note>Prompt:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Instead of naming McGarry Bowen to represent us, McGarry Bowen is now being hired by one of our larger agencies, Foote Cone of New York. McGarry Bowen is one of the world's largest marketers of Kaffeeklatsch, Belding and other soft-drink brands and the maker of Kraft salad dressings and mayonnaise. OURS order : + refine Kraft Foods in Cincinnati is expanding its agency roster by naming McGarry &amp; Bowen in Chicago to create ads for its salad dressings, mayonnaise and even barbecue sauces bearing the Kraft brand name. Spending was estimated at more than $200 million. McGarry &amp; Bowen's hiring is part of a trend by big marketers like Kraft to shift creative assignments from independent agencies to larger agencies. The Chicago office of JWT, part of the WPP Group, created ads for Kraft's "soup" dressings and mayonnaise as well as those for other products; the Chicago office of Foote Cone &amp; Belding, part of the Interpublic Group of Companies, created ads for "soup" dressings, for which Kraft is a producer of record, as well as for General Mills. Both JWT, part of the Omnicom Group, and Foote Cone still remain on the Kraft agency roster. The decision comes after Foote Cone, part of Foote Cone &amp; Be, agreed to become a consultant to McGarry &amp; Bowen on the account. Both McGarry &amp; Bowen, part of the Omnicom Group, and J. Walter Thompson Worldwide in New York, and WPP Worldwide Advertising of Kansas City, Mo., continue to create ads for Kraft. PAIR full w/o refine: Kraft Foods expanded its agency roster by naming McGarry Bowen in New York to create campaigns for salad dressings, mayonnaise and barbecue sauces bearing the Kraft brand name. Spending was estimated at $20 million. The decision to open an agency with McGarry Bowen is part of a trend in which large clients like big marketers like Kraft shift creative assignments from large, independent agencies to larger agencies in the same market. The Chicago office of JWT, part of the WPP Group, had created ads for Kraft salad dressings, and the Chicago office of Foote Cone &amp; Belding, part of the Interpublic Group of Companies, had created ads for its ice cream. Both JWT and Foote Cone remain on the Kraft agency roster. PAIR full : Kraft Foods expanded its agency roster by naming McGarry Bowen in New York to create campaigns for salad dressings, mayonnaise and barbecue sauces bearing the Kraft brand name. Spending was estimated at $10 million. The decision to expand its relationship with McGarry Bowen is part of a trend in which big marketers like big marketers like Kraft shift creative assignments from small, independent agencies to larger agencies at the same time. The Chicago office of JWT, part of the WPP Group, had created ads for Kraft salad dressings, while the Chicago office of Foote Cone &amp; Belding, part of the Interpublic Group of Companies, had created ads for ice cream. Both JWT and Foote Cone remain on the Kraft agency roster. Figure 14: Sample outputs on news generation.</figDesc><table><row><cell>Prompt: Two Held in Man's Death</cell></row><row><cell>HUMAN:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">They are all US-based college students. Each of them is paid $15 hourly for the task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://www.economist.com/open-future/2019/10/01/ how-to-respond-to-climate-change-if-you-are-an-algorithm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">https://github.com/NVIDIA/apex</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by National Science Foundation through Grant IIS-1813341 and Nvidia GPU gifts. We thank three anonymous reviewers for their constructive suggestions on many aspects of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constrained decoding for neural NLG from compositional representations in task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="831" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating discourse markers into a pipelined natural language generation architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><surname>Callaway</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075096.1075130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGdial Workshop on Discourse and Dialogue</title>
				<meeting>the Second SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-task approach for disentangling syntax and semantics in sentence representations</title>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1254</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2453" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Empirically estimating order constraints for content planning in generation</title>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073012.1073035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><surname>Wa Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/williamFalcon/pytorch-lightningCitedby" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Note</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
				<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">APE at scale and its implications on MT evaluation biases</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
				<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural syntactic preordering for controlled paraphrase generation</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discourse marker choice in sentence planning</title>
		<author>
			<persName><forename type="first">Brigitte</forename><surname>Grote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Generation</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lexically constrained decoding for sequence generation using grid beam search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved lexically constrained decoding for translation and monolingual rewriting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Culkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Argument generation with retrieval, planning, and realization</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric P Xing ; Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1255</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2019</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2661" to="2672" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence-level content planning and style specification for neural text generation</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="591" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with disentangled context transformer</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
				<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Czech Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attending to future tokens for bidirectional sequence generation</title>
		<author>
			<persName><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 18th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING 2000</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to control the fine-grained sentiment for story ending generation</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1603</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6020" to="6026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A generalized framework of sequence generation with application to undirected sequence models</title>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12790</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey</title>
				<imprint>
			<date type="published" when="1994-03-08">1994. March 8-11, 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Iterative refinement for machine translation</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06602</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast lexically constrained decoding with dynamic beam allocation for neural machine translation</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Can neural generators for dialogue learn sentence planning and discourse</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6535</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
				<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="284" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4035" to="4045" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">e26752</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">What makes a good conversation? how controllable attributes affect human judgments</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1702" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentations in spoken dialog systems</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1218966</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
				<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Steering output style and topic in neural response generation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2140" to="2150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1199</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Retrieve and refine: Improved sequence generation models for dialogue</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</title>
				<meeting>the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning neural templates for text generation</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3174" to="3187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R'emi</forename><surname>Louf</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="m">Morgan Funtowicz, and Jamie Brew</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the</title>
				<meeting>the 58th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
