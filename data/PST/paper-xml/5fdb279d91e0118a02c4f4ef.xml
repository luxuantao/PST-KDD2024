<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimpleChrome: Encoding of Combinatorial Effects for Predicting Gene Expression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-14">14 December 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ghulam</forename><surname>Murtaza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Aaron</forename><surname>Wang</surname></persName>
							<email>aaronjwang@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<postCode>02912</postCode>
									<settlement>Providence</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimpleChrome: Encoding of Combinatorial Effects for Predicting Gene Expression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-14">14 December 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/42069</idno>
					<idno type="arXiv">arXiv:2012.08671v1[q-bio.GN]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the breakthrough of recent state-of-the-art sequencing technology, genomics data sets have become ubiquitous. The emergence of large-scale data sets provides great opportunities for better understanding of genomics and one fundamental task is to understand gene regulation. Although each cell in the human body contains the same set of DNA information, gene expression controls the functions of these cells. There are two important factors that control the expression level of each gene: (1) Gene regulation such as histone modification can directly regulate gene expression. (2) Neighbor genes that are functional related or interact to each other can effect the gene expression level. Previous efforts tried to address the former using Attention-based model. To address the second problem, it requires incorporation of all potential related gene information into the model. Though modern machine learning and deep learning models were able to capture signals when applied to moderately sized data, they struggled to recover the underlying signals of the data due to the data's higher dimensionality. To remedy this issue, we present SimpleChrome, a deep learning model that learns the latent histone modification representations of genes. The features learned from the model allow us to better understand the combinatorial effects of cross-gene interactions and direct gene regulation on the target gene expression. The results of this paper show outstanding improvements on the predictive capabilities of downstream models and greatly relaxes the need for a large data set to learn a robust, generalized neural network. These results have immediate downstream effects in epigenomics research and drug development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The human body has thousands of different cell types, ranging from muscle cells to brain cells, and even though these cells contain the same DNA, their roles differ drastically due to the differential expression levels of different genes within cells, leading to downstream changes in the cell's overall functions. These changes may affect which genes in the cell are expressed.</p><p>One critical factor of gene expression is the modification of histone proteins, bead-like structures that provide support to chromosomes. The cell uses these histone proteins to organize the DNA in a condensed state, controlling which parts of the DNA are "exposed" and thus allowed to express. Histone proteins are prone to chemical modification which may change which parts of the DNA are exposed and expressed. This is known as the Histone Code Hypothesis. Unlike genetic mutations, another source of gene expression modification, histone modifications have shown to be potentially reversible, a powerful juxtaposition that has advanced the development of a new generation of targeted drugs that cure diseases resulting from reversible aberrant histone modifications. Recent advancements in sequencing technology have allowed researchers to quantify gene expression and histone modification, the latest and most comprehensive data set being REMC (Roadmap Epigenome Project). Initial studies to understand the combinatorial effects of histone modifications on gene expression experimentally validated that there exists a correlation between histone modifications and Gene expression. Following computational methods, most notably DeepChrome by <ref type="bibr" target="#b12">Singh et al. (2016)</ref> and AttentiveChrome by <ref type="bibr" target="#b13">Singh et al. (2017)</ref> that employ deep learning to learn complex combinatorial interactions outperform all the previous Machine Learning based methods. Even though these models show superior predictive performance, they only consider a limited amount of data around the Transcription Start Site (TSS) to perform their predictions and utilized complex models with many parameters. Previous literature has shown that neighbor genes 2 SimpleChrome may also play important roles in determining the gene expression level as they may interact with each other in a functional related pathway. Unfortunately, including more potential candidate genes such as neighbor genes into conventional deep learning models would increase the number of input features significantly, hindering the power and computational sustainability of the model. To overcome this issue, we develop SimpleChrome and DeepNeighbors. DeepNeighbors is trained in two steps. First, it utilizes unsupervised learning to derive a lower dimensional representation of histone modifications for each gene as new input. In the second step, the representation of the target gene or the fused representations of both target gene and its neighbor genes is fed into a simple model for predicting gene expression. SimpleChrome refers to the first step of the training step only and does not include the neighboring genes for predicting gene expression. We showed that the representations learned by SimpleChrome can successfully preserve useful information of original data for predicting gene expression. We demonstrate that this can reduce required sample size and model complexity for achieving competitive prediction performance. We further showed that the neighbor genes do not contribute significantly for predicting gene expression, seen in the performance of DeepNeighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous Methods</head><p>Over the past decade, many researchers have developed machine learning and deep learning-based models to predict the effect of Histone Modifications (HMs) on gene expressions. These studies primarily use the REMC data set, which contains the intensity and location of HMs across the entire genome across 56 different cell types. Recent studies have attempted to model this either as a classification or a regression task. The studies that have attempted to model it as a classification task have applied a slew of different techniques. These techniques include Linear Regression by <ref type="bibr" target="#b6">Karlic et al. (2010)</ref>, SVM by <ref type="bibr" target="#b0">Cheng et al. (2011)</ref>, Random Forests by <ref type="bibr" target="#b1">Dong et al. (2012)</ref>, Rule-Based Learning by <ref type="bibr" target="#b5">Ho et al. (2015)</ref>, and Deep Learning by <ref type="bibr" target="#b13">Singh et al. (2017</ref><ref type="bibr" target="#b12">Singh et al. ( , 2016))</ref>. DeepChrome and AttentiveChrome are cell-specific gene expression prediction frameworks that outperform the previously published machine learning based techniques. DeepChrome utilizes CNNs to capture the histone marks' combinatorial interactions to autonomously learn the latent biological interactions. Whereas, AttentiveChrome uses a hierarchical attention-based model to understand the latent biological interactions between the HM to predict the gene expression. Similarly, other studies have tried to model the same problem as a regression task that includes SVR (Support Vector Regression) model by <ref type="bibr" target="#b0">Cheng et al. (2011)</ref> and DeepDIFF (Attention Based). These models attempt to estimate the differential gene expression across different cell lines based on the HM's difference around the TSS (Transcription Start Site) and TTS (Transcription Termination Site). Though these models achieve greater performance, the models are much more complex and do not account for impact of neighbor genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed ideas</head><p>Recent works have used the combination of gated neural networks with dual attention networks to predict chromatin accessibility from <ref type="bibr">Guo et al. (2020a)</ref>. As suggested by the authors, the gated network structure can stabilize variances and also avoid gradients vanishing and exploding and thus may converge to a better model. Beside, multiple embedding modules were also proposed by <ref type="bibr" target="#b4">Guo et al. (2020b)</ref> to learn DNA representations which was shown with better performance compared with just RNN, or gated networks. Thus, learning more efficient embedding could be a potential improvement over existing models. In recent years, there were many studies proposed to use generative models to learn input embeddings as the generative modeling captures the underlying mechanism of how the data is generated and could thus learn more useful information from them. Some well known models include Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) or their hybrid Adversarial Autoencoders (AAEs). Previous studies showed that using these models to first learn latent representations without labels and then use latent representations for downstream supervised tasks can significantly improve prediction accuracy, as seen by <ref type="bibr" target="#b15">Yu and Lee (2019)</ref>; <ref type="bibr" target="#b10">Makhzani et al. (2016)</ref>; <ref type="bibr" target="#b9">Kingma et al. (2014)</ref>. Therefore, we propose to combine generative models with previous techniques to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Learning and Autoencoders</head><p>We are given an input matrix X with dimension of N ? P , where N denotes the number of samples and P indicates the number of features. As P becomes larger, input matrix X becomes harder to work with for two main reasons. Firstly, storing and parsing the data in memory as well as passing and processing the data with the models can be computational expensive. Secondly, the objective of the model becomes harder to optimize and requires significantly larger N to derive an optimal solution. Hence, we are interested in converting X into Z with dimension of N ?K where K &lt;&lt; P . Each column of Z is a lower dimension representation of the original data X. Since the representations are supposed to preserve the essence of the original input and thus discard any redundant information within lower dimensions, downstream tasks such as predictions will greatly benefit.There exist multiple dimension reduction techniques in literature, specifically by <ref type="bibr" target="#b11">Scholz et al. (2008)</ref>; <ref type="bibr" target="#b7">Kasun et al. (2016)</ref>, such as Principle Component Analysis (PCA), Tensor Factorization, Singular Value Decomposition (SVD) which are based on linear transformations; while linear models are easier to interpret and compute, they will lose nonlinearity in the original data. Hence, in this paper, we consider non-linear models such as Autoencoders.</p><p>An Autoencoder contains an encoder f ? (x) and a decoder g ? (z) parameterized with ? and ?, respectively. The encoder f ? (x) takes one sample of original data x as input and outputs a vector contains latent variables z in a lower dimension. The decoder g ? (z) takes the latent variables z as input and outputs reconstructed data x ? with the same dimensions of x. The intuition behind the Autoencoder is that fitting latent variables z will enable better construction of x. Thus, Autoencoders are often trained with the following objective,</p><formula xml:id="formula_0">N i=1 ||x i -g ? (f ? (x i ))|| 2 + ? w?{?,?} ||w||p,<label>(1)</label></formula><p>where || ? || 2 denotes the mean square error and ||w||p denotes regularization on the weights w of the model (with p denotes p-th norm).</p><p>As the encoder and the decoder contains nonlinear activation function such as ReLU, sigmoid, etc, Autoencoders are powerful tools for learning lower dimensional representation of raw, non-linear data x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Modeling and Variational Autoencoder</head><p>Generative Modeling has drawn a lot of attention in the deep learning field for the past several years. Unlike most black-box methods in deep learning, generative models aim to learn the underlying distribution of data P (x) which enables better understanding of how the data is generated, thus benefiting downstream tasks such as representation learning, generating new samples, etc. Two of the most popular methods are Generative Adversarial Networks (GANs) by <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref> and Variational 2.z combines with x for the target gene and is fed into an MLP model for predicting gene expression. Autoencoders (VAEs) by <ref type="bibr" target="#b8">Kingma and Welling (2013)</ref>. While GANs were shown to generate high quality samples, the lack of theoretical support and the difficulty in adversarial learning hinders its utilities in the field, as seen by <ref type="bibr" target="#b14">Tolstikhin et al. (2017)</ref>. In this paper, we focus on Variational Autoencoders (VAEs) which are easier to train and enjoy strong theoretical support from Bayesian inference. Similar to the Autoencoder, a Variational Autoencoder has an encoder f ? (x) and a decoder g ? (z). An intuitive way to understand VAEs is such that while they aim to reconstruct inputs similar to vanilla Autoencoders, they also attempt to regularize the latent space z simultaneously. In brief, while the Autoencoders are assumed to only encode isolated points of x in the latent space, VAEs aim to derive a smooth latent space; more specifically, the goal of VAEs is to find a model that maximizes the marginal log-likelihood logp ? (x). While the marginal likelihood can be intractable in practice, Bayesian inference theory provides an alternative for practical optimization. According to Jensen's inequality, one can derive an Evidence Lower Bound (ELBO) of the marginal likelihood,</p><formula xml:id="formula_1">log p ? (x) ? E[log p ? (x | z)] -KL(q ? (z | x) p ? (z)),<label>(2)</label></formula><p>where p ? (x | z) denotes the conditional likelihood (given latent variables) of reconstructed data output by the decoder. One can interpret this by treating the output of the decoder as a probabilistic distribution (i.e. a Gaussian distribution for continuous sample); q ? (z | x) is the conditional posterior of latent variable z given an input x outputted by the encoder; p ? (z) denotes the prior distribution which is commonly assumed to be standard Gaussian N (0, I); the last term on the right hand side is the Kullback-Leibler divergence (relative entropy) between two distributions. When the prior and conditional posterior are both Gaussian, the KL term has a closed-form solution that can be directly trained with a gradient-based method. While the expectation of the likelihood term can be hard to derive in practice, <ref type="bibr" target="#b8">Kingma and Welling (2013)</ref> introduce a Stochastic Gradient Variational Bayes (SGVB) method to approximate the expectation to overcome such an issue. Hence, modern VAEs can be easily trained with gradient descent methods. As the latent variables are appropriately 'regularized' such that it maximizes the data probability, the latent variables should encode the inputs very well once the model converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-Step Training for Predicting Gene Expression</head><p>The training of the model can be dissected into two parts as shown in 1. In step one, we prepare relevant data and compress high dimensional data that may contain redundant information to lower dimensional representations.</p><p>In step two, we consider two prediction models: i). A prediction model (i.e. Multilayer perceptron) that takes compressed data of the target gene as input. ii). A concatenation of target gene data with compressed neighbor genes data as input into the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Data Preparation and Dimension Reduction</head><p>Following the work from <ref type="bibr" target="#b12">Singh et al. (2016</ref><ref type="bibr" target="#b13">Singh et al. ( , 2017))</ref>, we use five core Histone Modification marks for 56 different cell types derived from REMC datasbase; these five HM marks are H3K27me3, H3K36ME3, H3K4me1, H3K4me3 and H3K9ME3. For each cell type, of which there are 19,802 genes in total, we follow previous works and divide them into training (6601 genes), validation (6601 genes), and testing (6600 genes). In this study, we also consider using even smaller training sizes (100 genes or 1000 genes randomly sampled from 6601). For each gene, the 10000 bp (+/-5000) region around the TSS (Transcription Start Site) is divided into bins of size 100. For each bin, we calculate the frequency of each Histone Modification, which gives a 5 by 100 matrix for the target gene i. Similar to AttentiveChrome and DeepChrome, we formulate the gene expression prediction as a binary classification task. Each target gene is represented as a 5 by 100 matrix x i , the raw input for predicting the gene expression level. As a first step, we used either a Autoencoder or Variational Autoencoder architecture to learn a lower dimension of k = {2, 5, 10} latent representation (we find that k = 10 optimizes performance) z i . Then, we consider using either i)z i , or ii). x i concatenated with l = 20 neighbor z j as inputs into the prediction model. As a comparison, we also include prediction models that use x i as inputs such as DeepChrome. Due to the limitation of computational source, we only randomly picked three cell lines out of 56 and report them as the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Baseline Models and Evaluation Metric</head><p>We evaluate our model primarily based on predictive performance against well-known, state-of-the-art deep learning models. Since DeepChrome and AttentiveChrome outperform previous machine learning methods by a statistically significant margin, we focus our efforts on comparing our performance against DeepChrome and AttentiveChrome. However, while the focus of this work is to improve predictive capabilities, we choose DeepChrome as our sole baseline because as shows similar (or better) performance compared to AttentiveChrome, while still being interpretable. The first baseline is DeepChrome, and it takes x i (5 by 100) as input matrix. We also included a two-layer MLP that takes x i as input as a comparison as well.</p><p>For VAEs and AEs, we used a two layer convolution encoder (64 and 128 filters) with batch normalization and ReLU activation, followed by two dense layers (one for learning means and one for learning variances). The decoder first uses a dense layer and then two convolution transpose layers with a final tanh activation layer. The tanh function compresses all data into the range of -1 and 1, which is then normalized. The AE is trained with MSE as the reconstruction loss and the VAE has an additional KL divergence for regularization loss.</p><p>There are two main prediction models to evaluate, which are i). MLP, which uses z i as input. For all MLPs, we simply used a two layer architecture with 50 and 20 hidden neurons and ReLU activations. ii) a CNN similar to DeepChrome that will take x i and z j of 20 neighbored genes, in which neighbored genes are chosen by prioritizing those with the smallest Euclidean distances to the target gene measured using latent variables z i . The CNN has the same convolution layer as DeepChrome that takes x i as input (50 filters with kernel size 10) and another convolution layer that takes all z j (l = 20 by k = 10) as input. Then, the output of these two convolution layers are concatenated and fed into a two layered MLP (same to DeepChrome) for prediction. All models are trained using the Adam optimizer with batch sizes of 100 and 10 epochs using training data or subsets of the training data. To qualitatively evaluate (a), we use tSNE to visualize the clustering of all the genes in the test set. We first run tSNE using flattened x i ; the plot is shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), which shows that genes with high expression and low expression seem to form two distinct clusters. We next run tSNE on z i . We hypothesize that if VAEs or AEs can preserve useful information for predicting gene expression, then the tSNE results should preserve the distinct clusters. Indeed, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(b), one can see two clusters that are at least or even more distinct compared to Fig. <ref type="figure" target="#fig_1">2(a</ref>). As a quantitative evaluation, we compared the performance of prediction (measured by accuracy, AUROC, AUPR) of DeepChrome using x i as input, MLP using x i as input, with MLP using z i from AE as input and MLP using z i from VAE as input. As shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>). DeepChrome and MLP utilizing latent variables z i from VAE have similar performances overall while the AUPR of the latter is slightly better. Hence, we conclude that VAEs can better encode useful information than AEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>To evaluate (b), we evaluate whether concatenating neighboring genetic information can better improve performance. Unfortunately, we did not see any improvement in performance overall, as the performance is statistically insignificant when compared to DeepChrome.</p><p>We found the most exciting results when evaluating (c). As introduced before, we hypothesized that generative modeling may have better performance when the sample size is limited. Thus, we sub-sampled 100 and 1000 samples for training all the previous models. The results are summarized in Fig. <ref type="figure" target="#fig_2">3</ref>. Surprisingly, we found out that MLP + VAE has similar and even better power when trained on only 100 or 1000 samples compared with DeepChrome using all 6601 training samples. The MLP model, with only 2 layers of 50 and 20 neurons, is much simpler and thus trains significantly faster compared to DeepChrome (Note: We tested the speed on the full data set using a MacBook Pro. MLP model with latent variables takes less than 10 seconds while DeepChrome takes roughly one minute), yet maintained rivalling performance with less training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a joint model system which attempts to include the histone modifications of spatially neighbored genes to improve the predictive capability in gene expression of existing state-of-the art deep learning models. However, in our preliminary evaluation, we find that including the spatially neighboring genetic information brings at most negligible improvement in the predictive power of deep learning models. However, we show that our method of utilizing VAEs to encode the data set significantly improves the predictive capabilities of downstream models and greatly relaxes the requirements of a large data set to learn a robust, generalized model.</p><p>As an extension of our work, we intend to look at functionally relevant genes rather than solely spatially close genes. This novel representation of the cell would allow our model to potentially improve its predictive capabilities and use an attention style mechanism to provide insights into how and which proteins regulate gene expression. Furthermore, our training and evaluations were done on a single cell (E004) due to the limited timeline of the project, and thus future work could experiment on other cell lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Training process of DeepNeighbor: 1. Data are preprocessed and neighbor gene input matrices are transformed into lower dimension using VAE and concatenated into a matrix z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Figure a and b shows the plot of t-SNE of encoder outputs. Figure c) compares the performance of Multi Layer Perceptron (with different data inputs) against DeepChrome. Figure d) compares the predictive performance of SimpleChrome against DeepChrome</figDesc><graphic url="image-1.png" coords="3,168.72,291.88,369.12,267.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. table summarizes Accuracy, AuROC and AuPR scores of different models with varying sizes.</figDesc><graphic url="image-2.png" coords="4,388.56,271.21,185.00,129.75" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was conducted under <rs type="person">Ritambhara Singh</rs> as part of the final course project for the graduate-level course Deep Learning in Genomics taught by <rs type="institution">Singh at Brown University</rs>. We appreciate the feedback from the instructor and our fellow students during the project's development.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code and data is available at https://github.com/aaronwangj/SimpleChrome</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A statistical framework for modeling gene expression using chromatin features and application to modencode datasets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling gene expression using chromatin features in various cellular contexts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Greven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Djebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gingeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guig?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Birney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attentive gated neural networks for identifying chromatin accessibility</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="15557" to="15571" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepanf: A deep attentive neural framework with distributed representation for chromatin accessibility prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="305" to="318" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combinatorial roles of dna methylation and histone modifications on gene expression</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M K</forename><surname>Hassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Some Current Advanced Researches on Information and Computer Science in Vietnam</title>
		<editor>
			<persName><forename type="first">Q</forename><forename type="middle">A</forename><surname>Dang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Le</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">N Q</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histone modification levels are predictive for gene expression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vlahovicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vingron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2926" to="2931" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimension reduction with extreme learning machine</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L C</forename><surname>Kasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3906" to="3918" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Adversarial autoencoders</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis: neural network models and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fraunholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selbig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal manifolds for data visualization and dimension reduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="44" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepchrome: deep-learning for predicting gene expression from histone modifications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="639" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attend and predict: Understanding gene regulation by selective attention on chromatin</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6769" to="6779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Zero-shot learning via simultaneous generating and learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
