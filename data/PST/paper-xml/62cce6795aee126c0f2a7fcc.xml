<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TensorIR: An Abstraction for Automatic Tensorized Program Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-09">9 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyuan</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bohan</forename><surname>Hou</surname></persName>
							<email>bohanhou@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Hongyi</forename><surname>Jin</surname></persName>
							<email>hongyij@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wuwei</forename><surname>Lin</surname></persName>
							<email>wlin@octoml.ai</email>
						</author>
						<author>
							<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
							<email>jshao@octoml.ai</email>
						</author>
						<author>
							<persName><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
							<email>ruihangl@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
							<email>zhye@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
							<email>lmzheng@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cody</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
							<email>tqchen@cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TensorIR: An Abstraction for Automatic Tensorized Program Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-09">9 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.04296v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional tensor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deploying high-performance machine learning models has become an emerging challenge in various areas, including image recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>, natural language processing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>, and games <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. The advances in machine learning bring demands to support a broad range of models. In the meantime, there are increasing demands to deploy smart applications to a broad spectrum of devices ranging from servers to embedded environments. * Both authors contributed equally to the paper ? Part of this work was done at Shanghai Jiao Tong University The classical acceleration technique uses vector units to process multiple scalar computations simultaneously, which is still widely used on CPU platforms. However, to cater to increasingly heavier computation throughput requirements, modern accelerators usually contain specialized high-dimensional tensor computation instructions, creating the need for tensorized program optimization.</p><p>The wave of hardware specialization further complicates the problem (Figure <ref type="figure" target="#fig_0">1</ref>). Driven by the goal of machine learning acceleration, modern hardware backends introduce specialized primitives to speed up tensor computations. Domain experts also start to develop micro-kernel primitives, which carefully organize a series of highly optimized instructions to perform a sub-computation to speed up domain-specific tensor operator libraries. These hardware instructions and micro-kernel primitives typically operate on for y, x, k in grid(64, 64, 64):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C[y, x] += A[y, k] * B[k, x]</head><p>for yo, xo, ko in grid <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16</ref>): matmul_add4x4(C, A, B, yo, xo, ko) for yo, xo, ko in grid <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16)</ref>: for yi, xi in grid <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4)</ref> Figure <ref type="figure">2</ref>: An expert developer can choose to divide the problem into 4x4 matmul and loops that uses the matmul, then optimize them separately. This way we can effectively make use of specialized tensor instructions in the target hardware. multi-dimensional tensor regions and effectively perform tensor operations such as multi-dimensional loads, dot product, and matrix multiplication (Figure <ref type="figure" target="#fig_0">1</ref>). We call these opaque tensor computation acceleration constructs tensorized intrinsics and transformation procedure to use these intrinsic tensorization. In order to get the best out of these hardware backends, modern machine learning systems need to optimize programs that contain hierarchical loop nests, multi-dimensional loads, and tensor intrinsics -we call this problem tensorized program optimization.</p><p>Most of the current tensorized programs are optimized by domain experts, who compose tensorized primitives together with multi-dimensional loops, threading patterns, and data caching to craft specialized kernel libraries such as Intel MKL-DNN <ref type="bibr" target="#b19">[20]</ref>, ARM Compute Library <ref type="bibr" target="#b2">[3]</ref> and NVIDIA cuDNN <ref type="bibr" target="#b10">[11]</ref>. These libraries are then used by machine learning frameworks such as TensorFlow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b31">[32]</ref> and MXNet <ref type="bibr" target="#b7">[8]</ref>. However, huge engineering efforts are required to support the growing sets of models and backends.</p><p>In this paper, we propose to address the tensorized program optimization problem using a compilation approach. Most past works in machine learning compilation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref> search over a program space of loop nest transformations and do not handle tensorized programs automatically. Bringing automatic program optimization to tensorized programs would unlock the benefits from domainspecific accelerations in modern hardware backends. We identify the following key challenges to achieving this goal:</p><p>Abstraction for Tensorized Programs. To build an automated compiler for tensorized programs, we need an abstraction that can pragmatically capture possible equivalent tensorized computations for a given machine learning operator. Notably, the abstraction needs to represent multi-dimensional memory accesses, threading hierarchies, and tensorized computation primitives from different hardware backends. The abstraction also needs to be expressive enough to represent most of the operators of interest in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large Design Space of Possible Tensorized Program Optimizations.</head><p>Another challenge is to produce an optimized tensorized program for a given operator automatically. A compiler needs to make use of a rich set of techniques that domain experts might use, including making effective use of loop tiling, threading, and data layout transformations. Importantly, these transformations now need to be made in conjunction with tensorized computations, bringing additional complexities to analysis and automation. The combinations of these transformations form a large search space. We need an effective way to find an optimized tensorized program for a given search space.</p><p>To address these challenges, we introduce TensorIR, an abstraction for automatic tensor program optimization. To begin with, we introduce a new construct called block that allows us to divide and isolate tensorized computation region from the outer loop nests. The new abstraction allows us to effectively represent tensorized computations and combine them with loop nests, threading, and memory hierarchy. We also introduce program transformation primitives to express a rich space of potential optimizations. We build a novel automatic scheduling algorithm on top of the abstraction and transformation primitives. Additionally, TensorIR abstraction also allows us to represent and optimize programs that contain a mixture of irregular computations and tensor computations, expanding the possible support beyond a normal tensor expression <ref type="bibr" target="#b8">[9]</ref>. This paper makes the following contributions:</p><p>? We propose a novel abstraction for tensorized programs that separates tensorized computation from the loop transformations. Meanwhile, the same abstraction allows us to uniformly represent tensor intrinsics and hardware constraints.   Figure <ref type="figure" target="#fig_9">3</ref>: Overview of our approach. We use a key abstraction named block to divide and isolate the tensorized computations, and enables further loop transformations with tensorized operations.</p><p>We integrate TensorIR with an end-to-end compilation framework and show that it outperforms existing machine learning compilation solutions by up to 7x and automatically brings competitive performance to heavily optimized platform-specific solutions .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW</head><p>This section describes the key insights of our approach and gives an overview of the paper. To motivate our approach, we start with an example flow of how a domain expert optimizes a tensorized program in Figure <ref type="figure">2</ref>. Tensorized computation primitives usually correspond to a sub-problem of the original tensor operator computation. As a result, it is natural for domain experts to choose a divide and conquer approach -divide the original program into sub-problems of tensorized computation and loop nests that use the tensorized computation, then optimize them separately. The divide and conquer approach allows developers to focus on a sub-problem without worrying about the others. Additionally, it also enables us to target multiple tensorized computation implementations. Most existing machine learning compilers take two kinds of approaches (Figure <ref type="figure" target="#fig_9">3</ref>). Halide <ref type="bibr" target="#b33">[34]</ref>, TVM <ref type="bibr" target="#b8">[9]</ref>, Tiramisu <ref type="bibr" target="#b4">[5]</ref>, AKG <ref type="bibr" target="#b44">[45]</ref>, MLIR/Affine <ref type="bibr" target="#b24">[25]</ref> and AMOS <ref type="bibr" target="#b46">[47]</ref> take a bottom-up approach that models the search space using loop nests iterators around scalar operation bodies, and then optimizes the program by finding the best loop nest transformation (through search or polyhedral optimization). HTA <ref type="bibr" target="#b15">[16]</ref>, Fireiron <ref type="bibr" target="#b16">[17]</ref> and Stripe <ref type="bibr" target="#b43">[44]</ref> use a top-down approach that gradually decomposes the problem into sub-problems through nested polyhedral structures. Given the significance of the divide and conquer approach in manual tensorized program optimizations, it is natural to ask whether it is possible to bring the same insight to machine learning compiler design.</p><p>We give a positive answer in this paper. Specifically, we introduce a new abstraction called block into the loop nests. A block contains the right amount of signature information to isolate the inner problem space and outer problem. With block, we can continue to perform transformations on both outer and inner problem independently, using the block signature as the interface. Similar to the manual divide and conquer approach, a common use case of a block is to represent a tensorized computation primitive in a hardware backend, but we can also use the block to isolate bigger sub-problems of interest when divide and conquer makes sense. Importantly, tensor computation is the first-class citizen in TensorIR. Loop nests with blocks can be viewed as a generalized abstraction of iteration space. We present the detailed design of the TensorIR abstraction in section 3.</p><p>To automate the tensorized program optimization, we construct a search space of possible ways to divide the problem guided by the hardware tensor computation primitives, then further search over possible ways to solve sub-problems using program transformations. We present the automatic scheduling algorithm for tensorized programs in section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loop nests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational block</head><p>Multi-dimensional buffer Figure <ref type="figure">4</ref>: An example TensorIR program with three major elements -multi-dimensional buffers, loop nests and computational block. Details of block is omitted for simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TENSORIR ABSTRACTION</head><p>This section introduces the TensorIR abstraction. Figure <ref type="figure">4</ref> gives an example of TensorIR program. We introduce a Python-AST (abstract syntax tree) dialect of TensorIR to let developers directly construct and transform programs in Python. A TensorIR program contains three main elements: multi-dimensional buffers, loop nests (with possible thread bindings in GPU settings), and blocks. A block can contain one or more nested loop nests with sub-blocks or sequence of imperative statements that correspond to the content of the computation. This representation allows us to divide computations into the corresponding sub(block)-regions and do effective program transformations using dependency information stored in the block signature. We discuss the design details in ?3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Block</head><p>A block in TensorIR represents a tensorized computation on a subregion of the multi-dimensional buffers. Figure <ref type="figure" target="#fig_4">5</ref> shows an example block for the matrix multiplication (matmul) computation. The body of a block is parameterized by a set of block iterator variables ? ? , ? ? , ? ? which represents an abstract tensorized computation. Instantiated with different value combinations of these block iterator variables, the block maps to different concrete running block instances. These iterator variables can be bound to expressions that contain the outer loop iterators, which implies the execution order of block instances.</p><p>Rationale. The main design rationale of a block is to isolate tensorized computation -we want to be able to transform loop nests outside the block without looking into its body. However, unlike scalar computation, we may not be able to extract the dependency  information needed for transformation from an opaque tensor computation body. As a result, we introduce a block signature that contains sufficient dependency information for transformations. We discuss these transformations in ?3.2. Additionally, the signature can be used to independently verify the correctness of the iterator bindings during transformations (more details in ?3.3).</p><p>Block Iterator Domain. While it is possible to instantiate a block's body computation by binding the block iterators to any loop nests, most instantiations do not correspond to the same computation. To ensure the consistency of computation among transformations, we store the iterator domain information and the constraints of iterators in the block signature. For the particular example in Figure <ref type="figure" target="#fig_4">5</ref>, we know that ??, ?? and ?? must bind to iterators in domain [0, 16). Additionally, because ?? is a reduction axis, we know that we cannot bind it to a parallel loop unless the reduction is atomic. The domain constraints still leave massive room for outer loop transformations, as there are multiple ways to construct loops that satisfy the constraint. Our domain signature can be viewed as a specific way to represent the integer domain sets and relations of the iterators. We choose the particular representation due to its implementation efficiency and simplicity in reasoning, but would also point out that the same design philosophy applies to other formal domain representations of integer sets and relations <ref type="bibr" target="#b39">[40]</ref>.</p><p>Access Region and Dependency. To provide sufficient dependency information, a block signature contains the access regions and read/write dependencies that a block has with respect to the multiple dimensional buffers. In Figure <ref type="figure" target="#fig_4">5</ref>  The dependency information is used during transformations. We only mark each block's dependency with respect to the multidimensional buffers instead of other statements (blocks). This indirection enables a broader range of transformations, such as data layout transformation and re-computation which are essential in tensorized program optimization.</p><p>Reduction Block and Initialization. A reduction computation usually contains an initialization step and an update step. We can naturally map the reduction computation into two blocks. But, on the other hand, it is usually helpful to jointly make the scheduling decisions (such as tiling and computation location) of the two steps. We introduce an optional initialization statement for blocks that perform reduction. An initialization statement is executed during the first iteration of a reduction. This reduction block representation is mainly useful during transformations. We provide transformation primitives to transform between the two-block-based representation and the init-block-based representation so we can pick the best representation for low-level code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scheduling Transformations</head><p>For a given input program, we need to generate a rich search space of programs with equivalent semantics. We introduce primitives to transform a TensorIR program to equivalent optimized programs. Following the existing convention of tensor program optimizations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, we call this procedure scheduling.</p><p>A block is schedulable if it only contains loop nests with subblocks as its leaves. We can transform the loop nests and sub-block  computation locations within a schedulable block by analyzing the sub-block signatures and their dependency information. Notably, a schedulable block can contain non-schedulable sub-blocks (e.g., opaque Tensor Core computation). An opaque block can also contain a schedulable sub-block. Based on the block isolation, we can still effectively explore the search space of the schedulable part independently while keeping the same opaque block. We describe the schedule primitives in the rest part of this subsection.</p><p>Loop Transformations. Loop transformations such as loop tiling (split, reorder) and compute location mutation are important ways to optimize programs for better memory locality. We also provide these loop transformation primitives (see examples in Figure <ref type="figure" target="#fig_6">6</ref>). Unlike existing tensor compilers that directly extract the dependency of each leaf scalar computation statement, we calculate the dependencies by only inspecting the block signature. Besides loop transformations, we also support primitives that bind loops to GPU threads and provide annotation hints such as vectorization and unrolling.</p><p>Blockization. The loop transformation primitives preserve the overall hierarchy of blocks. As we alluded in section 2, sometimes dividing the problem by isolating a sub-region computation into a new sub-block is helpful. We call this transformation blockization Figure <ref type="figure" target="#fig_8">7</ref>. A blockized program is no longer scalar-based as the new sub-block corresponds to a tensorized computation. We can use blockization to isolate possible candidates for tensorization. Besides blockization, we also introduce primitives that can change the block hierarchies. For example, we provide caching primitives that introduce sub-blocks to cache input data into shared memory. We also provide back and forth transformations between a single reduction block and the corresponding init-and update blocks.</p><p>Separation of Scheduling and TensorIR. Many previous tensor compilers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> rely on a declarative scheduling language to construct a schedule tree. Adding new scheduling primitives to these compilers requires changes to both the schedule tree data structure and the corresponding lowering rule in these compilers. We take a different approach and implement each schedule primitive as a standalone transformation from one TensorIR program to another. This design is easier to extend, as different developers can develop new primitives concurrently based on a stable TensorIR abstraction. Figure <ref type="figure">8</ref>: Automatic optimization for tensorized program with hardware intrinsics. We take 64x64x64 matrix multiplication followed by a RELU operator as the input workload and 4x4x4 matmul as the synthetic tensor intrinsic which is implemented by a dot product instruction. The tensorization candidate generation step tiles the 64x64x64 GEMM into 4x4x4 sub-tiles and isolate the sub-computation. Then the tensorized program sketch generation step schedules the computation and insert the resulting data movement (AutoCopy) blocks which are scheduled independently. Finally, we use evolutionary search to fill the random decisions in sketches with a validation mechanism to filter out incorrect programs.</p><p>Additionally, developers can print out the program at any transformation stage for debugging and mix automatic rewriting with schedule transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation</head><p>The blocks and their buffer read/write relations capture a complete picture of the original computation and are used to validate the correctness of loop nests and threading assignments.</p><p>Loop Nest Validation. Loop nest validation checks whether the iterator binding provided by the loop nests matches the constraints of the iterator domain, including the domain size and iterator independence information. For example, if two data-parallel block iterators are bound as ? 1 = ?; ? 2 = ? * 2, then the corresponding program is invalid because ? 1 and ? 2 are not independent. But ? 1 = ?/4; ? 2 = ?%4 can be a legal binding. We build pattern-matchers to find a quasi-affine mapping from the loop iterators to the block iterator variables and use the pattern to validate the independence and domain of the bindings. Besides the iterator domain validation, it is also important to check the producer-consumer relations to make sure producer blocks that write to buffer regions always cover the read region of downstream consumers.</p><p>Threading Validation. When building a program for GPUs and other accelerators with threading support, we also need to do additional validations with respect to the threading and memory hierarchies. We do three kinds of validations:</p><p>? Thread binding: Ensure different iterators bound to the same thread are consistent and meet the launching constraints of the backend. ? Cooperative memory access: For blocks that produce buffers stored in shared memory collaboratively across threads, we need to ensure the block covers downstream requirements from all the threads in the same group. Meanwhile, upstream blocks that provide inputs for this block need to cover the read requirement of this block from all the threads in this group. ? Execution scope: Validate that tensor intrinsic runs at the correct execution scope (e.g., TensorCore needs to run at the warp-level).</p><p>Correctness of Schedule Primitives. We add checks to each schedule primitive to ensure the correctness of the transformation. When a schedule primitive only changes the loop nests, we can also use the validation procedure to ensure correctness. Because the block iteration domains and dependencies stay the same in these cases. We find primitive-specific necessary conditions for schedule primitives that change the blocks (e.g., blockization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AUTO-SCHEDULING TENSORIZED PROGRAMS</head><p>In the last section, we introduced TensorIR abstraction and a set of transformation primitives. In order to fully make use of the set of improvements, we need an automatic solution to optimize over a set of transformations and map computations to the native  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Propose Match TransformLayout</head><p>Figure <ref type="figure">9</ref>: Example flow of tensorization candidate generation. We take standard NHWC 2D convolution as the input workload and 16x16x16 matrix multiplication as the hardware backend intrinsic. First, the system converts the buffer access expressions to intermediate iterators. Based on the buffer access patterns, we calculate characteristic functions for each iterator and build a mapping between iterators that share the same characteristic vector. The mapping further guides the transformation of block instance space and ReIndex buffers. Note that although the ReIndex stages of B and C are redundant, they will be inlined into consumers during the sketch generation phase and as a result do not affect the performance.</p><p>tensor intrinsics. In this section, we describe a tensorization-aware automatic scheduler to solve this problem. Figure <ref type="figure">8</ref> shows an overview of our approach. Our system takes a workload description from users and tensor intrinsic descriptions about the hardware platform as inputs. The auto-scheduler first generates candidates for tensorization by inspecting the computation pattern. It then generates program sketch candidates that use the tensorized computations and then decide the data movements according to the compute patterns. For a given search space induced by the tensorized program sketches, we perform evolutionary search guided by a learning-based cost model. The entire process centers itself around the tensorization and leverages the new block abstraction to isolate tensorized computations. We discuss the details of each step in the subsequent subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Abstraction for Tensor Intrinsics</head><p>To make use of a tensor intrinsic in our optimization, we need a way to provide its semantics and backend implementation to the system. We leverage the same TensorIR abstraction to describe the tensor intrinsics of a given hardware backend. For each tensorized instruction, we introduce a TensorIntrin construct composed of two blocks. One block describes the computation semantics, and the other provides the low-level implementation of the tensorized computation.</p><p>In Figure <ref type="figure">8</ref>'s example, we use a normal loop nest with scalar body ? [?, ?] += ?[?, ?] * ? [?, ?] to represent the computation semantics and implement the intrinsic using inner dot product instruction ????? .???. We also include the data type, storage scope, memory layout, and contiguity constraints through the multi-dimensional buffer specification in a TensorIntrin. Those constraints are used during the validation step.</p><p>Notably, tensor intrinsics are usually applied together with special memory scopes, data layouts, and corresponding load/store instructions in common platforms. For instance, on NVIDIA GPUs, if we decide to use nvcuda::wmma::mma_sync API to perform dense computation, then we need to apply nvcuda::wmma::load_matrix_sync and nvcuda::wmma::store_matrix_sync to prepare input operands and retrieve output results respectively. On ARM CPUs, microkernels like a64_gemm_u8_8?12 require operands to be stored in interleaved layout. Developers can inform the system about these constraints by specifying special storage scopes for each input and output operands of the tensor computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tensorization Candidate Generation</head><p>Given a pair of backend target and an input program, we first match the program body to possible TensorIntrin to generate tensorization candidates. The match is performed in a gradual way. We first match the expression pattern ? [.] += ?[.] ? ? [.] without considering the indices. We then refine the matches by proposing possible mappings between the indices. Figure <ref type="figure">9</ref> gives an example that walks through the matching process. In this example, we take a matrix multiplication intrinsic as the backend description. The computation of this tensor intrinsic can be described by the following formula</p><formula xml:id="formula_0">? [?, ?] += ?[?, ?] ? ? [?, ?]<label>(1)</label></formula><p>It is easy for us to match the tensor intrinsic to workloads like batch matrix multiplication, which can be described by</p><formula xml:id="formula_1">? [?, ?, ?] += ?[?, ?, ? ] ? ? [?, ?, ?],</formula><p>by mapping ?, ?, ? to ?, ?, ? . But for workloads like 2-dimensional Convolution (Conv2D) with more complex index expression patterns</p><formula xml:id="formula_2">? [?, ?, ?, ??] +=?[?, ? * ? ? + ? ? * ? ? , ? * ? ? + ? ? * ? ? , ? ? ] ?? [? ? , ? ? , ? ? , ??],</formula><p>the mapping between ?, ?, ? and ?, ?, ?, ?, ? ? , ? ? , ? ? is not straightforward.</p><p>On these more general cases, we rewrite the computation expression into an equivalent form:</p><formula xml:id="formula_3">? [?, ?, ?, ??] += ? ? [?, ?, ?, ? ? , ? ? , ? ? ] ? ? [? ? , ? ? , ? ? , ??], ? ? [?, ?, ?, ? ? , ? ? , ? ? ] = ?[?, ? * ? ? + ? ? * ? ? , ? * ? ? + ? ? * ? ? , ? ? ].</formula><p>We call this transformation ReIndex which uses intermediate iterators that appear in the buffer access indices to rewrite the buffer access expressions. To match the new computation to the tensor intrinsic, we check the buffer access where each iterator appears. For example, we notice that ?, ?, ? and ? appear in indices of ?(?? ), ?, ?? and ? appear in indices of ?, ?, and ? ? , ? ? , ? ? and ? appear in indices of ?, ?. We can then match the iterators in the computation to the iterators in the tensor intrinsic by inspecting their appearance patterns. Specifically, we map fuse(?, ?, ?) to ?, ?? to ?, and fuse(? ? , ? ? , ? ? ) to ?. Here fuse() is to fuse multiple iterators together and can be recursively defined by fuse(? 1 ) = ? 1 fuse(? 1 , ? 2 , . . . , ? ? ) = fuse(? 1 , ? 2 , . . . , ? ? -1 ) * extent(? ? ) + ? ? , where extent() is the extent of iterator ? ? . We can then transform the computation to</p><formula xml:id="formula_4">? ? [fuse(?, ?, ?), ??] +=? ? [fuse(?, ?, ?), fuse(? ? , ? ? , ? ? )] ?? ? [fuse(? ? , ? ? , ? ? ), ??],</formula><p>where</p><formula xml:id="formula_5">? ? [fuse(?, ?, ?), ??] =? [?, ?, ?, ??], ? ? [fuse(?, ?, ?), fuse(? ? , ? ? , ? ? )] =? ? [?, ?, ?, ? ? , ? ? , ? ? ]</formula><p>? ? [fuse(? ? , ? ? , ? ? ), ??] =? [? ? , ? ? , ? ? , ??]. We use this mapping to reshape the block instance space and the outer loops and transform the layout of ReIndex buffers. We insert layout rewrite blocks to rewrite ?, ?, ? to ? ? , ? ? , ? ? respectively and use ? ? , ? ? , ? ? to rewrite the computation body. After these steps, the computation body is compatible with the tensor intrinsic.</p><p>Loop Reorganization and Early Blockize. Besides the computation body, we also need to ensure that the tensor computation region matches the description provided by the TensorIntrin. The shape of the reorganized block instance space might not be divisible by the sub-problem size of the tensor intrinsic. For each computation body from the last step, we do necessary padding on the computation block and input/output operands to the closest divisible shape. We then perform tiling to create inner loops to match the loop nest of the tensor intrinsic and further blockize the inner loop to isolate the corresponding tensor computations. Notably, the candidates generated in this step do not always lead to successful tensorizations. This is because other constraints, such as memory layout and threading depend on later transformations. These constraints are embedded in the tensorization candidates and checked during validation.</p><p>Formal Description of the Process. So far we gave a high-level overview of the tensorization candidate generation process. In the the remainder part of this subsection, we provide a formal description of the process. Suppose the intrinsic scalar expression can be formalized as</p><formula xml:id="formula_6">? [v 0 ] = ? (? [v 0 ], ? 1 [v 1 ? 2 [v 2 ], . . . , ? ? [v ? ]). (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>where ? is the output operand, ? [1:? ] are input operands, v is the set of iterators that parameterized this computation, v [0:? ] are all lists of iterators that belong to v, and ? is the expression pattern detected. Note that it accommodates common dot product and matrix multiplication intrinsics. Furthermore, suppose that the workload scalar expression can be formalized as</p><formula xml:id="formula_8">? [? 0 ( ?0 )] = ? ( ? [? 0 ( ?0 )], ?1 [? 1 ( ?1 )], . . . , ?? [? ? ( ?? )]),</formula><p>where ?, ? [1:? ] , ?[0:? ] corresponds to their counterparts and ? is exactly the same. ? [0:? ] are mappings that map lists of iterators to the actual buffer access position. In our Conv2D case, for instance, we have</p><formula xml:id="formula_9">? ? (?, ?, ?, ? ? , ? ? , ? ? ) = (?, ? * ? ? +? ? * ? ? , ? * ? ? +? ? * ? ? , ? ? ).</formula><p>To reduce the problem to a simpler canonical form, we apply the ReIndex schedule transformation, which creates an intermediate cache buffer for an operand but with the layout changed according to the iterators. Formally, if we run ReIndex ?1 [? 1 ( ?1 )], we create the following rewrite block before the computation</p><formula xml:id="formula_10">?1 [ ?1 ] = ?1 [? 1 ( ?1 )].</formula><p>Then if we apply ReIndex to all the operands, the workload scalar expression is reduced to</p><formula xml:id="formula_11">? [ ?0 ] = ? ( ? [ ?0 ], ?1 [ ?1 ], . . . , ?? [ ?? ]),<label>(3)</label></formula><p>where buffer access indices in both 2 and 3 directly correspond to iterators.</p><p>To match 2 and 3, we define the characteristic vector ? (?) ? {0, 1} ?+1 of an iterator ? ? v by inspecting whether each of v 0 , v 1 , v 2 , . . . v ? contains ?. Formally,</p><formula xml:id="formula_12">? (?) ? = [? ? v ? ] ? ? [0, ?],</formula><p>where [] is the Iverson bracket that returns 1 if the corresponding condition is true or 0 otherwise (Figure <ref type="figure">9</ref>). We can successfully propose the mapping as long as ?? ? v, ? ? ? ?, ? (?) = ? ( ?). In the current implementation, we can further safely assume that iterators in v all have different characteristic vectors. Then for all ? ? v, we fuse all such ? where ? (?) = ? ( ?) and map the fused iterator to ?.</p><p>Notably, the iterator order inside each of v [0:? ] or ?[0:? ] does not affect the value of characteristic function ? or ?. But when we fuse all ? where ? (?) = ? ( ?), the order of fusion affects how the operands are reorganized in memory to be compatible with the tensor intrinsic. Our implementation now uses a default order for all the workloads and can generalize to different fusion orders in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tensorized Program Sketch Generation</head><p>For a given set of tensorization candidates, we need to construct a large program search space that contains the tensorization. We generalize existing hierarchical search space generation <ref type="bibr" target="#b45">[46]</ref> to tensor computations. We construct the search space by generating program sketches that contain the tensorized computation, then enumerate over choices induced by the generated sketches. As shown in the right part in Figure <ref type="figure">8</ref>, a program sketch fixes parts of program structures while leaving space for remaining choices of parameters such as loop tiling size and computation caching decisions. We generate sketches by applying pre-defined sketch generation rules iteratively. Importantly, we need to build sketch generation rules that work on tensorized computations by looking at the block signatures and make use of the access region information during our analysis.</p><p>Data Movement as First-Class Citizen. Existing auto-schedulers for tensor programs focus their designs on the schedule of computations and treat data movement between different memory scopes with secondary priority. However, since tensor intrinsics vastly improve the throughput of computations, data movements become the bottleneck of tensor programs. Moreover, data movement decisions usually depend on computation schedule decisions like tilings, thread bindings, execution scopes, and producer-consumer data flow granularity. We take these insights and bring data movements as first-class citizens in our automatic scheduler and decouple them from computation schedules. Specifically, we insert AutoCopy blocks into the places where the sketch generation rules decide to perform data movements (Figure <ref type="figure">8</ref>). The copy block hides the memory schedule details and only exposes the necessary buffer access information at the block signature level. The isolated copy blocks allow the sketch generation to independently make computation schedule decisions without considering how to do data movements. The body of the AutoCopy block describes the details of the data movement task, including buffer position mapping, threading, and storage scope requirements. A data movement scheduler takes this information as input and performs memory-related schedule transformations, such as inserting intermediate cache stages, utilizing data movement tensor intrinsics, vectorization, cooperative fetching, or stride padding to avoid bank conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evolutionary Search</head><p>After the tensorized program sketch generation phase, we can get billions of possible induced programs. We use evolutionary search to explore the space and find an optimized tensorized program. Our search starts from random initializations of choices for given program sketches. We then perform mutations on the current set of programs. We then select promising programs from the mutated candidates and benchmark them on our hardware backend of interest. We collect data from the evaluation phase to update the learned cost model.</p><p>Cost Model for Tensorized Computation. We build a boosting tree ensemble <ref type="bibr" target="#b6">[7]</ref> based cost models that use features extracted from the program. The feature vector contains information related to memory access patterns, reuse, and loop annotations. Importantly, we extract features from both block signatures in an isolated way as well as the body of the block (e.g., to mark the use of Tensor Core). Our cost model can be viewed as a generalization of previous approaches to tensorized programs. We believe an effective cost model for tensorized programs is a promising area for future research.</p><p>Validation. Randomly mutating programs during the search can generate invalid programs due to the unmet constraints of tensor intrinsic or invalid loop nest candidates. The possibility of false positives necessitates a validation step during the search. We apply techniques in subsection 3.3 to validate a program candidate in the evolutionary search to identify and reject invalid programs. The validation step reduces the burden on evolutionary search algorithms and allows us to generate a small number of false positives during the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We implement TensorIR on top of Apache TVM <ref type="bibr" target="#b8">[9]</ref>. Notably, the insights described in the paper can benefit other machine learning compilation frameworks as well. This section provides evaluations to answer the following questions:</p><p>? Can TensorIR optimize common set of machine learning operators ( ?5.1)? ? Can TensorIR bring performance boost to end-to-end network execution ( ?5.2)? ? Can TensorIR support tensor intrinsics on different hardware platforms ( ?5.3)? To evaluate TensorIR along those axes, we compare our solution to existing machine learning compilation solutions on GPU and CPU platforms. We will discuss the specific setups in the corresponding subsections.</p><p>Additionally, we are interested in the following question throughout all evaluations: How does TensorIR compare to vendor-specific libraries and frameworks that rely on these libraries? Importantly, most of these libraries are heavily optimized by a dedicated team of engineers. In all of these settings, TensorIR performs end-toend automatic optimization without the need to call into external libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single Operator Evaluation</head><p>This section evaluates TensorIR on operators in deep learning models. We pick a common collection of workloads, including: 1D convolution (C1D), 2D convolution (C2D), 3D convolution (C3D), depthwise convolution (DEP), dilated convolution (DIL), general matrix multiply (GMM), group convolution (GRP), and transposed 2D convolution (T2D). The evaluations are done on an NVIDIA RTX 3080 platform with Tensor Cores. We pick this platform as it has a wide spectrum of machine learning compiler solutions and libraries that we can use as comparison reference points. We use float16 as the input and accumulator data type for all operators. We include TVM (commit: 27b0aad5, with auto-scheduler <ref type="bibr" target="#b45">[46]</ref>) and AMOS (commit: 6aee6fe2) as two machine learning compiler baselines. Finally, we compare against two vendor-specific solutions: CUTLASS (version 2.9) and TensorRT (PyTorch-TensorRT container Release 22.06).</p><p>Comparisons to Machine Learning Compilers. Figure <ref type="figure" target="#fig_11">10</ref> shows the comparisons to AMOS and TVM. TVM <ref type="bibr" target="#b8">[9]</ref> works well on less compute-intensive workloads (e.g.DEP), but has poor performance on heavy ones (e.g.C2D, C3D, GMM) due to the limited Tensor Core support. AMOS <ref type="bibr" target="#b46">[47]</ref> can use Tensor Core for every workload but not doing as well as TensorIR. Overall, TensorIR brings up to 7.5? number of improvement over existing machine learning compilation solutions. These improvements come from better abstraction and automatic scheduling that leverages tensor compute intrinsics and corresponding data movements.</p><p>Comparisons to Platform Specific Libraries. Figure <ref type="figure" target="#fig_12">11</ref> shows the comparisons of TensorIR to two platform specific solutions: CUT-LASS <ref type="bibr" target="#b21">[22]</ref> and TensorRT <ref type="bibr" target="#b30">[31]</ref>. TensorIR outperforms the baselines  We did not show the numbers of CUTLASS on DEP, GRP, and T2D as the library does not support them. Ten-sorIR outperforms the baselines on C1D, C2D, DEP, T2D, and DIL by up to 13.9x and gets to more than 75% throughput on C3D, GMM, and GRP. on C1D, C2D, DEP, T2D, and DIL by up to 13.9?. These results shows the advantage of automatic optimizations provided by Ten-sorIR. Notably, TensorIR gets to more than 75% on C3D, GRP an GMM. These results show that even on workloads that are intensively optimized by dedicated engineering teams, TensorIR can still get close to or match existing vendor-specific solutions. We expect the remaining gap continues to close as we bring additional insights from these libraries to TensorIR. In all cases, the baseline solutions are optimized by dedicated engineering teams, while Ten-sorIR enables automated compilation for a given tensor intrinsic declaration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">End-to-end Model Evaluation</head><p>In this section, we evaluate the impacts that TensorIR can bring to end-to-end model execution. We evaluate our solutions on four widely-used models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> on on NVIDIA RTX 3080. We include TVM and AMOS as machine learning compiler baselines. Additionally, we also include PyTorch (version 1.13.0.dev20220612) as a framework reference point. Finally, we include TensorRT, which is a vendor-specific solution that is heavily optimized by engineering teams at Nvidia.</p><p>The results are shown in Figure <ref type="figure" target="#fig_13">12</ref>. TensorIR outperforms Py-Torch, TVM, and AMOS by 1.2 -8.8?. Additionally, TensorIR brings 30% better performance on MobileNet V2 comparing to TensorRT, and achieves the 88% -100% throughput on ResNet50 and BERT_large. Additionally, TensorIR can automatically support emerging models such as Vision Transformer, which TensorRT does not yet support. These results show that our abstraction and the automatic scheduler can bring close or even better performance than the best effort libraries on common machine learning models. Additionally, the automated solution enables us to bring faster support for emerging models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ARM CPU Evaluation</head><p>The last two subsections evaluate TensorIR on an Nvidia GPU. In this section, we study how easy it is to generalize TensorIR to different platforms. We evaluate results on an ARM platform by providing the description with 8-bit integer dot(sdot). This instruction is different from the Tensor Core used in the last two subsections. Importantly, we use the same TensorIR framework by providing the new description of the tensor intrinsic to the system. The evaluations are done on an AWS Graviton2 CPU.</p><p>Single Operator Results. We evaluate the results on two commonly used operators: C2D and GMM. We include TVM as a machine learning compiler baseline and ARMComputeLib <ref type="bibr" target="#b2">[3]</ref>   TensorIR achieves up to 12.5? speed up compared with TVM thanks to the ability to leverage native hardware acceleration. In the meantime, TensorIR reaches 85% -105% throughput of ARMComputeLib <ref type="bibr" target="#b2">[3]</ref>, showing our ability to get to the same level of performance as vendor-specific solutions on this platform.</p><p>End-to-End Results. Finally, we evaluate the end-to-end neural network executions on this platform. Our baselines include PyTorch and TVM. We achieve up to 2.3? on this platform in Figure <ref type="figure" target="#fig_15">14</ref>. Notably, PyTorch contains a specialized quantized model support with QNNPACK <ref type="bibr" target="#b26">[27]</ref> backend. However, QNNPACK has not yet added sdot support. This result alludes to the maintenance cost for these frameworks to keep up with hardware changes. TensorIR can help to reduce the maintenance burden through automation and still bring competitive performance to hand-optimized systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>Deep learning frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref> optimize deep neural networks by invoking vendor optimized libraries (e.g., cuDNN <ref type="bibr" target="#b10">[11]</ref>, MKL-DNN <ref type="bibr" target="#b19">[20]</ref>). These frameworks can leverage TensorIR to generate optimized tensorized programs for various hardware backends.</p><p>Machine learning and tensor compilers introduce different abstractions for tensor programs. Halide <ref type="bibr" target="#b33">[34]</ref> and TVM <ref type="bibr" target="#b8">[9]</ref> use a scheduling language that can describe loop optimization primitives of loop nests with a scalar body. Tensor Comprehensions <ref type="bibr" target="#b40">[41]</ref>, Tiramisu <ref type="bibr" target="#b4">[5]</ref> and MLIR/Affine <ref type="bibr" target="#b24">[25]</ref> use polyhedral model <ref type="bibr" target="#b39">[40]</ref> to analyze loop nest dependencies. These works optimize loop nests with scalar computation in a bottom-up way. Fireiron <ref type="bibr" target="#b16">[17]</ref> and Stripe <ref type="bibr" target="#b43">[44]</ref> use nested polyhedral structures to model tensor programs in a top-down fashion. TensorIR combines insights from both approaches and generalizes the representation to tensorized programs. TACO <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref> is a compiler for sparse tensor algebra. Cortex <ref type="bibr" target="#b14">[15]</ref> generalized tensor compilation to recursive computations. Our work is orthogonal to these efforts. We believe the TensorIR abstraction can be combined with insights from these works in the future to enable an even broader range of computations.</p><p>Automation is an essential topic in machine learning compilation and tensor program optimization. AutoTVM <ref type="bibr" target="#b9">[10]</ref> introduced a learning-based approach to optimize tensor programs via a learned cost model and template-guided search. Triton <ref type="bibr" target="#b38">[39]</ref> introduces a tile-based template representation for effective program optimization. FlexTensor <ref type="bibr" target="#b47">[48]</ref> automatically generates the template. Halide builds an automatic scheduler using Monte-Carlo tree search <ref type="bibr" target="#b1">[2]</ref>. Ansor <ref type="bibr" target="#b45">[46]</ref> improves automatic scheduling using a hierarchical search space. Our automatic scheduling algorithm takes lessons from these approaches and generalizes them to tensorized computation best for domain-specific hardware acceleration.</p><p>Auto-vectorization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> is a long-standing topic in compiler research. Tensorization can be viewed as a generalization of the vectorization problem to enable tensor intrinsic in modern accelerators <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. There are some existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> on this topic. AKG <ref type="bibr" target="#b44">[45]</ref> uses the polyhedral method to explore tensorized search space, UNIT <ref type="bibr" target="#b42">[43]</ref> introduces a generic flow for tensorization, while AMOS <ref type="bibr" target="#b46">[47]</ref> enables automatic mapping to tensorized intrinsic through tensor expression. Our method generalizes these previous approaches by proposing a novel abstraction for tensorization computation and jointly performing tensorization along with other optimizations. TensorIR serves as a foundation to further develop tensorization-aware automatic scheduling methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Trends of hardware specialization. The classical acceleration technique uses vector units to process multiple scalar computations simultaneously, which is still widely used on CPU platforms. However, to cater to increasingly heavier computation throughput requirements, modern accelerators usually contain specialized high-dimensional tensor computation instructions, creating the need for tensorized program optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: C[...] += A[...] * B[...] Divide the Problem into loop nests and matmul kernel Optimize Tensorized Computation Body def matmul_add4x4_v1(C, A, B, yo, xo, ko): for yi, xi in grid(4, 4): C[yo*4+yi, xo*4+xi] += accel.dot(A[yo*4+yi, ko*4:ko*4+4], B[ko*4:ko*4+4, xo*4+xi]) Optimize Outer Loop Nests def matmul_add4x4_v0(C, A, B, yo, xo, ko): accel.matmul_add4x4(C[yo*4+yi, xo*4+xi], A[yo*4+yi, ko*4:ko*4+4], B[ko*4:ko*4+4, xo*4 + xi]) Operator Definition for yo, xo, k in grid(4, 4, 16): for yi, xi in grid(4, 4): matmul_add4x4(C, A, B, yo*4 + yi, xo*4 + xi, k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>((64, 64), "float32") for i, j in grid(64, 64): with block("block_B"): vi = spatial_axis(64, i) vj = spatial_axis(64, j) B[vi, vj] = A[vi, vj] + 1 for i in range(64): with block("block_C"): vi = spatial_axis(64, i) for j in range(64): C[vi, j] = exp(B[vi, j]) Computation: C = exp(A + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for yo, xo, ko in grid<ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16)</ref>: with block(): vy, vx, vk = ... block_siguatures with init(): for y, x in grid(4, 4): C[vy*4+y, vx*4+x] = 0.0 for y, x, k in grid(4, 4, 4): C[vy*4+y, vx*4+x] += A[vy*4+y, vk*4+k] * B[vk*4+k, vx*4+x] outer loop body Producer-consumer dependency relations signatures Block Signature read A[vy*4:vy*4 + 4, vk*4:vk*4 + 4] read B[vk*4:vk*4 + 4, vx*4:vx*4 + 4] write C[vy*4:vy*4 + 4, vx*4:vx*4 + 4] vy: spatial_axis(length=16, binding_value=yo) vx: spatial_axis(length=16, binding_value=xo) vk: reduce_axis (length=16, binding_value=ko) init stmt Iterator domain and binding values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Blocks contain complete signature for dependency analysis and we make it an isolation level between body computation and outer loop nesting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, the block writes the region ? [?? * 4 : ?? * 4 + 4, ?? * 4 : ?? * 4 + 4] by reading ?[?? * 4 : for i, j in grid(64, 64): C[vi, vj] = dot(A[vi, :], B[:, vj]) for i, j in grid(64, 64): D[vi, vj] = max(C[vi, vj], 0) block_C (vi, vj = i, j) block_D (vi, vj = i, j) for i0, j0 in grid(8, 8): for i1, j1 in grid(8, 8): C[vi, vj] = dot(A[vi, :], B[:, vj]) for i, j in grid(64, 64): D[vi, vj] = max(C[vi, vj], 0) Loop Tiling Reverse Compute_at block_C(vi, vj = i0*8 + i1, j0*8 + j1) block_D (vi, vj = i, j) for i0, j0 in grid(8, 8): for i1, j1 in grid(8, 8): C[vi, vj] = dot(A[vi, :], B[:, vj]) for i1, j1 in grid(8, 8): D[vi, vj] = max(C[vi, vj], 0) block_D(vi, vj = i0*8 + i1, j0*8 + j1) block_C(vi, vj = i0*8 + i1, j0*8 + j1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Loop transformations mutate outside loop nests but change nothing inside the block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>for i, j, k0 in grid(64, 64, 16): for k1 in range(4): C[vi, vj] += A[vi, vk] * B[vk, vj] block (vi, vj, vk = i, j, k0*4 + k1) for i, j, k0 in grid(64, 64, 16): for k1 in range(4): C[vi, vj] += A[vi, vk] * B[vk, vj] blockized (vi0, vj0, vk0 = i, j, k0) Blockize block (vi, vj, vk = vi0, vj0, vk0*4 + k1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Blockization creates a new block to isolation inside computation and outside loop nesting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 )</head><label>3</label><figDesc>for y0, x0 in grid(?, ?, ?): for y1, x1, k0 in grid(?, ?, ?): for y2, x2, k1 in grid(?, ?, ?): for y3, x3, k0 in grid(?, ?): for y1, x1 in grid(?, ?): C.cache[...] = C[...] for y1, x1 in grid(?, ?, ?): D[...] = max(C[...], 0) for yo, xo, ko in grid(16, 16, 16): for y, x in grid(64, 64): D[y, x] = max(C[y, x], 0) for y, x, k in grid(64, 64, 64): C[y, x] += A[x, k] * B[k, y] for y, x in grid(64, 64): D[y, x] = max(C[y, x]x, k in grid(4, 4, 4): C[y, x] += A[x, k] * B[k, y] # implementation (dot) for yi, xi in grid(4, 4): C[yo*4+yi, xo*4+xi] += accel.dot(A[xo*4+xi, ko*4:ko*4+4], B[ko*4:ko*4+4, yo*4+yi]) for y0, x0 in grid(?, ?, ?): for y1, x1, k0 in grid(?, ?, ?): for y2, x2, k1 in grid(?, ?, ?): for y3, x3, k0 in grid(?, ?): for y1, x1 in grid(?, ?, ?): for y2, x2 in grid(?, ?): D.cache[...] = max(C.cache[...], 0) block (matmul4x4) Computation body (matmul4x4) block (AutoCopy) Data movement body (A-&gt;A.cache) block (AutoCopy) Data movement body (B-&gt;B.cache) Data movement body (D.cache-&gt;d) for y2, x2 in grid(?, ?): A.cache[...] = A[...] block (AutoCopy) for y, x, k in grid(4, 4, 4): C[y, x] += A[x, k] * B[k, y] Computation body (matmul4x4) Sub-computation for yi, xi in grid(4, 4): C.cache[yo*4+yi, xo*4+xi] += accel.dot( A.cache[xo*4+xi, ko*4:ko*4+4], B.cache[ko*4:ko*4+4, yo*4+yi]) Sub-computation (tensorized) for y2, x2o in grid(?, ?): for vec in vectorized(?): A.cache[...] = A[...] Data movement (before schedule) Data movement (after schedule) TensorIntrin (matmul4x4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>TensorIntrinC#</head><label></label><figDesc>[x, y] = C[x, y] + A[x, k] * B[y, k] ?(n) = ?(h) = ?(w) Layout Transformation Block for A for n, h, w, c, rh, rw, rc in grid(...): At[n*h*w, rh*rw*rc] = A[n, h*2+rh, w*2+rw, rc] # Layout Transformation Block for B for c, rh, rw, rc in grid(...): Bt[c, rh*rw*rc] = B[c, rh, rw, rc] # Transformed Conv2D for nhw, c, rhwc in grid(16*56*56, 64, 7*7*3): Ct[nhw, c] += At[nhw, rhwc] * Bt[c, rhwc] # Layout Transformation Block for C for n, h, w, c in grid(...): C[n, h, w, c] = Ct[n*h*w, c]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Single operator comparison to existing machine learning compilers on Nvidia GPU. TensorIR brings up to 7.5x speed across workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Single operator comparison to platform-specific libraries. We did not show the numbers of CUTLASS on DEP, GRP, and T2D as the library does not support them. Ten-sorIR outperforms the baselines on C1D, C2D, DEP, T2D, and DIL by up to 13.9x and gets to more than 75% throughput on C3D, GMM, and GRP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: End-to-End model evaluations on NVIDIA GPU. TensorIR significantly outperforms existing machine learning compilation solutions and achieves similar or better throughputs on popular networks compared with the inference libraries on GPUs. TensorIR get better performance on ViT, an emerging model that TensorRT does not yet support.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Single operator evaluations on ARM CPU. Ten-sorIR get up to 12.6x faster than TVM due to the use of native tensor instrinsic acceleration. It also gets to the same level of performance as heavily optimized platform specific library (ArmComputeLib).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: End-to-end networks evaluation results on ARM CPU. TensorIR outperform with 1.2x-2.5x than PyTorch and TVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>We build transformation primitives to generate a rich search space of tensorized program optimization with correctness validation.</figDesc><table /><note><p>? We design and implement a new tensorization-aware automatic scheduler.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">CONCLUSION</head><p>We propose TensorIR, an abstraction for automatic tensorized program optimization. We design a key abstraction called block that can isolate tensorized computations and provide effective transformation primitives for program optimization. We build an automatic scheduling algorithm that performs tensorization jointly with other optimizations and generates performant programs. We hope this work will encourage additional studies of tensorized program optimization and provide new opportunities for hardware and software specialization.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to optimize halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karima</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/compute-library/" />
		<title level="m">ARM Compute Library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring the Arm dot product instructions</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/exploring-the-arm-dot-product-instructions" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic kernel generation for volta tensor cores</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Somashekaracharya G Bhaskaracharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12645</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Generation of Efficient Sparse Tensor Format Conversion Routines</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1145/3385412.3385963</idno>
		<ptr target="https://doi.org/10.1145/3385412.3385963" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>London, UK; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="823" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Fegade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cortex: A Compiler for Recursive Deep Learning Models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Hierarchically Tiled Arrays Programming Approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Basilio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Bikshandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mar?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gheorghe</forename><surname>Garzar?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Alm?si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><surname>Padua</surname></persName>
		</author>
		<idno type="DOI">10.1145/1066650.1066657</idno>
		<ptr target="https://doi.org/10.1145/1066650.1066657" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Workshop on Languages, Compilers, and Run-Time Support for Scalable Systems</title>
		<meeting>the 7th Workshop on Workshop on Languages, Compilers, and Run-Time Support for Scalable Systems<address><addrLine>Houston, Texas, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>LCR &apos;04)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fireiron: A Data-Movement-Aware Scheduling Language for GPUs</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Hagedorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archibald</forename><surname>Samuel Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Barthels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rastislav</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the ACM International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intel-mkl-dnn-part-1-library-overview-and-installation" />
		<title level="m">Intel? Math Kernel Library for Deep Learning Networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/introduction-to-intel-deep-learning-boost-on-second-generation-intel-xeon-scalable.html" />
		<title level="m">Introduction to Intel? Deep Learning Boost on Second Generation Intel? Xeon? Scalable Processors</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haicheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustyn</forename><surname>Blasig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ramini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Shivam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Majcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hohnerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nicely</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/cutlass" />
	</analytic>
	<monogr>
		<title level="j">CUTLASS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Tensor Algebra Compiler</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lugato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1145/3133901</idno>
		<ptr target="https://doi.org/10.1145/3133901" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Program. Lang. 1, OOPSLA, Article</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017-10">2017. Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When polyhedral transformations meet SIMD code generation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-No?l</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation</title>
		<meeting>the 34th ACM SIGPLAN conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mlir: Scaling compiler infrastructure for domain specific computation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">River</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">QNNPACK: Open source library for optimized mobile deep learning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">VTA: an open hardware-software stack for deep learning</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04188</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/tensorcore/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">NVIDIA Tensor Cores</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<title level="m">NVIDIA TensorRT: Programmable Inference Accelerator</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Loop-aware SLP in GCC</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorit</forename><surname>Nuzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayal</forename><surname>Zaks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCC Developers Summit</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Sparse Iteration Space Transformation Framework for Sparse Tensor Algebra</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Senanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amalee</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<idno type="DOI">10.1145/3428226</idno>
		<ptr target="https://doi.org/10.1145/3428226" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Program. Lang. 4, OOPSLA, Article 158</title>
		<meeting>ACM Program. Lang. 4, OOPSLA, Article 158</meeting>
		<imprint>
			<date type="published" when="2020-11">2020. Nov. 2020</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1145/3315508.3329973</idno>
		<ptr target="https://doi.org/10.1145/3315508.3329973" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages<address><addrLine>Phoenix, AZ, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Polyhedral code generation in the real world</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?dric</forename><surname>Bastoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compiler Construction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">UNIT: Unifying tensorized instruction compilation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="77" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Zerrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bruestle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06498</idno>
		<title level="m">Stripe: Tensor compilation via the nested polyhedral model</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AKG: automatic kernel generation for neural processing units using polyhedral transformations</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1233" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ansor: generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
	<note>{OSDI} 20</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AMOS: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3470496.3527440</idno>
		<ptr target="https://doi.org/10.1145/3470496.3527440" />
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;22: The 49th Annual International Symposium on Computer Architecture</title>
		<editor>
			<persName><forename type="first">Valentina</forename><surname>Salapura</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohamed</forename><surname>Zahran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fred</forename><surname>Chong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-06-18">2022. June 18 -22, 2022</date>
			<biblScope unit="page" from="874" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Flex-Tensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
