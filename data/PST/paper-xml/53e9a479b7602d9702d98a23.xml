<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-04-16">April 16, 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-04-16">April 16, 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">7F0E7AAD147CB652184EC4C467258A4B</idno>
					<idno type="DOI">10.1109/TIP.2010.2042098</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Inpainting by Patch Propagation Using Patch Sparsity Zongben Xu and Jian Sun</head><p>Abstract-This paper introduces a novel examplar-based inpainting algorithm through investigating the sparsity of natural image patches. Two novel concepts of sparsity at the patch level are proposed for modeling the patch priority and patch representation, which are two crucial steps for patch propagation in the examplar-based inpainting approach. First, patch structure sparsity is designed to measure the confidence of a patch located at the image structure (e.g., the edge or corner) by the sparseness of its nonzero similarities to the neighboring patches. The patch with larger structure sparsity will be assigned higher priority for further inpainting. Second, it is assumed that the patch to be filled can be represented by the sparse linear combination of candidate patches under the local patch consistency constraint in a framework of sparse representation. Compared with the traditional examplar-based inpainting approach, structure sparsity enables better discrimination of structure and texture, and the patch sparse representation forces the newly inpainted regions to be sharp and consistent with the surrounding textures. Experiments on synthetic and natural images show the advantages of the proposed approach.</p><p>Index Terms-Image inpainting, patch propagation, patch sparsity, sparse representation, texture synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE filling-in of missing region in an image, which is called image inpainting, is an important topic in the field of computer vision and image processing. Image inpainting has been widely investigated in the applications of digital effect (e.g., object removal), image restoration (e.g., scratch or text removal in photograph), image coding and transmission (e.g., recovery of the missing blocks), etc.</p><p>The most fundamental inpainting approach is the diffusionbased approach <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, in which the missing region is filled by diffusing the image information from the known region into the missing region at the pixel level. These algorithms are well founded on the theory of partial differential equation (PDE) and variational method. Bertalmio et al. <ref type="bibr" target="#b0">[1]</ref> filled in holes by continuously propagating the isophote (i.e., lines of equal gray values) into the missing region. They further introduced the Navier-Strokes equation in fluid dynamics into the task of inpainting <ref type="bibr" target="#b1">[2]</ref>. Chan and Shen <ref type="bibr" target="#b2">[3]</ref> proposed a variational framework based on total variation (TV) to recover the missing information. Then a curvature-driven diffusion equation was proposed to realize the connectivity principle which does not hold in the TV model <ref type="bibr" target="#b3">[4]</ref>. A joint interpolation of isophote directions and gray-levels was also designed to incorporate the principle of continuity in a variational framework <ref type="bibr" target="#b4">[5]</ref>. Recently, image statistics learned from the natural images are applied to the task of image inpainting <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The diffusion-based inpainting algorithms have achieved convincingly excellent results for filling the nontextured or relatively smaller missing region. However, they tend to introduce smooth effect in the textured region or larger missing region.</p><p>The second category of approaches is the examplar-based inpainting algorithm. This approach propagates the image information from the known region into the missing region at the patch level. This idea stems from the texture synthesis technique proposed in <ref type="bibr" target="#b8">[9]</ref>, in which the texture is synthesized by sampling the best match patch from the known region. However, natural images are composed of structures and textures, in which the structures constitute the primal sketches of an image (e.g., the edges, corners, etc.) and the textures are image regions with homogenous patterns or feature statistics (including the flat patterns). Pure texture synthesis technique cannot handle the missing region with composite textures and structures. Bertalmio et al. <ref type="bibr" target="#b9">[10]</ref> proposed to decompose the image into structure and texture layers, then inpaint the structure layer using diffusion-based method and texture layer using texture synthesis technique <ref type="bibr" target="#b8">[9]</ref>. It overcomes the smooth effect of the diffusion-based inpainting algorithm; however, it is still hard to recover larger missing structures. Criminisi et al. <ref type="bibr" target="#b10">[11]</ref> designed an examplar-based inpainting algorithm by propagating the known patches (i.e., examplars) into the missing patches gradually. To handle the missing region with composite textures and structures, patch priority is defined to encourage the filling-in of patches on the structure. Wu <ref type="bibr" target="#b11">[12]</ref> proposed a cross-isophotes examplar-based inpainting algorithm, in which a cross-isophotes patch priority term was designed based on the analysis of anisotropic diffusion. Wong <ref type="bibr" target="#b12">[13]</ref> proposed a nonlocal means approach for the examplar-based inpainting algorithm. The image patch is inferred by the nonlocal means of a set of candidate patches in the known region instead of a single best match patch. More examplar-based inpainting algorithms <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>were also proposed for image completion. Compared with the diffusion-based inpainting algorithm, the examplar-based inpainting algorithms have performed plausible results for inpainting the large missing region.</p><p>Recently, image sparse representation is also introduced to the inpainting problem <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>. The basic idea of this approach is to represent image by sparse combination of an overcomplete set of transforms (e.g., wavelet, contourlet, DCT, etc.), then the missing pixels are inferred by adaptively updating this sparse representation. Guleryuz et al. <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> proposed an image inpainting algorithm using adaptive sparse representation of image. Elad et al. <ref type="bibr" target="#b16">[17]</ref> improved this approach by separating the image into cartoon and texture layers, and sparsely represented these two layers by two incoherent over-complete transforms. This approach can effectively fill in the region with composite textures and structures, especially in the application of missing block completion. However, similar to the diffusionbased approach, it may fail to recover structure or introduce smooth effect when filling large missing region.</p><p>This paper focuses on the examplar-based inpainting algorithm through patch propagation. The two basic procedures of patch propagation are patch selection and patch inpainting. In the patch selection, a patch on the missing region boundary with the highest priority is selected for further inpainting. The priority is defined to encourage the filling-in of patches on structure such that the structures are more quickly filled than the textures, then missing region with composite structures and textures can be better inpainted <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Traditionally, the patch priority is defined based on the inner product between isophote direction and the normal direction of the missing region boundary <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In the patch inpainting, the selected patch is inpainted by the candidate patches (i.e., examplars) in the known region. The approach in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b21">[22]</ref> utilizes the best match candidate patch to inpaint the selected patch. The approach in <ref type="bibr" target="#b12">[13]</ref> uses a nonlocal means of the candidate patches for robust patch inpainting.</p><p>To better address the problems of patch selection and patch inpainting, two novel concepts of patch sparsity of natural image, i.e., patch structure sparsity and patch sparse representation, are proposed and applied to the examplar-based inpainting algorithm. First, we define a novel patch priority based on the sparseness of the patch's nonzero similarities to its neighboring patches. This sparseness is called structure sparsity in this paper. It is based on the observation that a patch on the structure has sparser nonzero similarities with its neighboring patches compared with the patch within a textured region. Compared with the priority defined on isophote, this definition can better distinguish the texture and structure, and be more robust to the orientation of the boundary of missing region.</p><p>Second, to inpaint a selected patch on the boundary of missing region, we use a sparse linear combination of examplars to infer the patch in a framework of sparse representation. This linear combination of patches are regularized by the sparseness prior ( regularization) on the combination coefficients. It means that only very few examplars contribute to the linear combination of patches with nonzero coefficients. This representation is called patch sparse representation in this paper. The patch sparse representation is also constrained by the local patch consistency constraint. This model extends the patch diversity by linear combination and preserves texture without introducing smooth effect by sparseness assumption. More importantly, the inpainted patches are more consistent with their surrounding textures or structures due to the local patch consistency constraint.</p><p>In summary, the structure sparsity and patch sparse representation at the patch level constitute the patch sparsity in this paper. The patch structure sparsity is inspired by the recent progress on the research of sparseness prior of natural image. The previous sparseness prior generally models the sparseness of image's nonzero features, e.g., gradients or filter responses. This kind of sparseness prior has been successfully applied to the image denoising <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, super-resolution <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, inpainting <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, deblurring <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and so on. The structure sparsity also models the sparsity of natural image. However, it models the sparseness of nonzero similarities of a patch with its neighboring patches instead of high-frequency features.</p><p>The patch sparse representation is inspired by the recent progress on sparse representation <ref type="bibr" target="#b27">[28]</ref>, which assumes that the image or signal is represented by the sparse linear combination of an over-complete library of bases or transforms under sparseness regularization. This framework has been widely applied to image denoising <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, edge detection <ref type="bibr" target="#b30">[31]</ref>, recognition <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, super-resolution <ref type="bibr" target="#b33">[34]</ref>, texture synthesis <ref type="bibr" target="#b34">[35]</ref>, etc., and achieved state-of-the-art performance. In this work, the idea of sparse representation is introduced to the examplar-based inpainting algorithm under the assumption that the missing patch can be represented by the sparse linear combination of candidate patches. Then a novel constrained optimization model is designed for patch inpainting.</p><p>The paper is organized as follows. In Section II, an overview of the proposed examplar-based inpainting algorithm is presented. In Section III, the details of the inpainting algorithm, including the patch sparsity and patch sparse representation, are introduced. The experiments and comparisons with the previous algorithms are performed in the Section IV. Finally, we conclude this work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ALGORITHM OVERVIEW</head><p>Given an image with the missing region and the known region , the task of image inpainting is to fill in the target region (i.e., the missing region ) using the image information in the source region (i.e., the known region ). The boundary of the target region is denoted by , which is called the fill-front in the examplar-based inpainting algorithm. We further denote as a patch centered at a pixel . The main procedures of the proposed examplar-based inpainting algorithm are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. This algorithm is based on patch propagation by inwardly propagating the image patches from the source region into the interior of the target region patch by patch. In each iteration of patch propagation, the algorithm is decomposed into two procedures: patch selection and patch inpainting.</p><p>In the procedure of patch selection, patch priority should be defined to encourage the filling-in of patches on the structure with higher priority. We define structure sparsity by measuring the sparseness of the similarities of a patch with its neighboring patches. Then patch priority is defined using the structure sparsity. In the example shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), the patches and are centered at pixel and which lie in the edge structure The patch 9 on the fill-front with the larger sparseness of nonzero similarities to its neighboring patches is encouraged to be selected with higher priority. The top left of (a) shows two examples of known image parts surrounding patch 9 and 9 , which locate at edge and flat texture region respectively. The bottom-left of (a) shows the similarities of 9 and 9 with their surrounding known patches, the higher brightness means larger similarity. Obviously, similarity map of 9 is much sparser than that of 9 . (b) Patch inpainting: For the selected patch 9 , sparse linear combination of candidate patches f9 ; 9 ; . . . ; 9 g is used to infer the missing pixels in patch 9 under the constraint of local patch consistency in N(p) .</p><p>and the flat texture region respectively. The left-down part of Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows the maps of their similarities with neighboring known patches. Obviously, the patch has sparser nonzero similarities; therefore, it has larger patch priority. The patch on the fill-front with the highest priority is selected to be inpainted firstly.</p><p>In the procedure of patch inpainting, the selected patch on the fill-front should be filled in. Instead of using a single best match examplar or a certain number of examplars in the known region to infer the missing patch, we assume that the selected patch on the fill-front is the sparse linear combination of the patches in the source region regularized by sparseness prior. In the example shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, the patch on the fill-front is inpainted by the sparse linear combination of candidate patches weighted by coefficients , in which only very sparse nonzero elements exist. The neighboring patches in are also used to constrain the appearance of patch by local patch consistency constraint.</p><p>In Section III, we will introduce the definitions of patch priority and patch sparse representation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INPAINTING BY PATCH SPARSITY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Patch Priority Using Structure Sparsity</head><p>The natural images are generally composed of structures and textures. A good definition of patch priority should be able to better distinguish the structures and textures, and also be robust to the orientation of the fill-front. In this paper, a novel definition of patch priority is proposed to meet these requirements. We now introduce the key component of our definition of patch priority, i.e., structure sparsity.</p><p>1) Structure Sparsity: The structure sparsity is defined to measure the confidence of a patch located at structure instead of texture. Structure sparsity is inspired by the following observations: structures are sparsely distributed in the image domain, e.g., the edges and corners are distributed as 1-D curves or 0-D points in the 2-D image domain. Nevertheless, the textures are distributed in 2-D sub-regions of the image domain, which are less sparsely distributed. On the other hand, for a certain patch, its neighboring patches with larger similarities are also distributed in the same structure or texture as the patch of interest. Therefore, we can model the confidence of structure for a patch by measuring the sparseness of its nonzero similarities to the neighboring patches. The patch with more sparsely distributed nonzero similarities are prone to be located at structure due to the high sparseness of structures.</p><p>Suppose is a patch on the fill-front , its neighboring patch is defined as the patch that is in the known region and with the center in the neighborhood of pixel , i.e., belongs to the set (1) is a neighborhood window centered at , which is set to be larger than the size of patch . Suppose is a matrix to extract the missing pixels of , and extracts the already known pixels of , then the similarity between and is defined as <ref type="bibr" target="#b1">(2)</ref> where measures the mean squared distance, is a normalization constant such that , and is set to 5.0 in our implementation.</p><p>For the patch , we measure the sparseness of its similarities to the neighboring patches in region by</p><p>(3) where is the vector of elements , and means the number of elements.</p><p>is incorporated to restrict in the interval , though its value is same for different patches. This definition embodies the fact that the larger sum of squared similarities in the larger region means larger sparseness. In the following paragraph, we call the sparseness of patch similarities as the structure sparsity.</p><p>For the definition of structure sparsity , we prove the following conclusion.</p><p>Theorem 1: The structure sparsity for patch achieves the maximal value if a single nonzero similarity exists, and it achieves the minimal value if all the similarities are same and equal to . Proof: We observe that consistently increases with respect to . To find the maximum and minimum values of , we only need to compute the maximum and minimum values of .</p><p>Firstly, to find the minimum value of , we minimize under the normalization constraint . This can be achieved by Lagrangian multiplier method, i.e., maximizing:</p><p>. It is easy to prove that achieves minimal value when for each . Then we maximize . Due to the fact that , so . The equality holds when only a single equals to 1, and all the other similarities equal to 0. So achieves its maximal value 1 when only a single similarity is nonzero and equals to 1.</p><p>Finally, it is easy to derive the maximal and minimal values of by inserting the maximum and minimum values of into (3).</p><p>This theorem tells us that the structure sparsity achieves its maximum and minimum values when the patch similarities are distributed in the sparsest and smoothest fashion respectively, and the structure sparsity increases with respect to the sparseness of patch's nonzero similarities to its neighboring patches.</p><p>We now investigate how the structure sparsity measures the structure confidence for different types of patches in the natural images. For the patch on the 0-D corners [e.g., Fig. <ref type="figure" target="#fig_1">2(a)</ref>], it is saliently distributed within the local region; therefore, it has the highest structure sparsity. Due to the sparsity of image Fig. <ref type="figure">3</ref>. Comparison of patch priority. (a) Process of inpainting using isophotebased priority <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>. (b) Process of inpainting using structure sparsity based priority. The texture synthesis technique in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b21">[22]</ref> is used for both cases.</p><p>Patch size and neighborhood window size are set to 11 2 11 and 51 2 51. The missing illusory edge is accurately recovered using structure sparsity-based priority; however, it is not perfectly recovered using isophote-based priority.</p><p>edges, the patch on 1-D edge [e.g., Fig. <ref type="figure" target="#fig_1">2(b)</ref>] has similar patches sparsely distributed along the same edge; therefore, they have higher structure sparsity. However, for the texture patches [e.g., Fig. <ref type="figure" target="#fig_1">2(c)</ref> and<ref type="figure">(d)]</ref>, they have similar patches in the 2-D local regions; therefore, they have smaller structure sparsity values. Under the guidance of structure sparsity, the patches located at structures (e.g., edges and corners) have higher priority for patch inpainting compared with the patches in texture regions.</p><p>2) Patch Priority: The final patch priority is defined by multiplying the transformed structure sparsity term with patch confidence term:</p><p>. is the confidence of patch , which specifies the reliability of color or intensity in the patch. It is same defined as in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b21">[22]</ref>:</p><p>, where is the confidence of the color or intensity of pixel , and initialized to 0 in the missing region or 1 in the known region. After each procedure of patch inpainting, the confidence of the newly filled pixels in the patch are updated by the confidence of the patch's central pixel.</p><p>is a linear transformation of from its original interval to the interval , where is set to 0.2. This transformation is necessary to make the structure sparsity varies in a comparable scale with . By multiplying these two terms in , the inpainting algorithm is encouraged to firstly inpaint the patch located at image structures (i.e., edges or corners) and with larger confidence of its colors or intensities, then the missing region with composite texture and structure can be more robustly inpainted <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison With the Isophote-Based Priority</head><p>Now we show that structure sparsity based priority is more robust to identify the structure than the isophote-based definition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, which uses the inner product of isophote direction and the normal direction of the fill-front.</p><p>Fig. <ref type="figure">3</ref> presents an example of inpainting for an image with composite textures and illusory edge. Fig. <ref type="figure">3(a)</ref> shows the process of inpainting using isophote-based priority, and Fig. <ref type="figure">3(b)</ref> shows the process of inpainting using structure sparsity based priority. The texture synthesis technique in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b21">[22]</ref> is incorporated for both cases. Using isophote-based priority, the patch at the top-right part of missing region has the larger priority because the isophote direction is nearly same to the orthogonal direction of the fill-front at its central pixel. For example, the patch in the texture region of the first image in Fig. <ref type="figure">3(a</ref>) is with the highest priority, and the illusory edge is failed to be accurately recovered in the final result. However, structure sparsity based priority is able to robustly identify the structure regardless of the shape of fill-front. For example, the patch in Fig. <ref type="figure">3</ref>(b) along structure is with the highest priority using structure sparsity based priority, and the missing region is inpainted perfectly in the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch Inpainting Using Patch Sparse Representation</head><p>The patch on the fill-front with the highest patch priority is selected to be filled firstly. In the traditional examplar-based inpainting technique <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>,</p><p>is filled by sampling the best match patch from the known region. Recently, a nonlocal means approach <ref type="bibr" target="#b12">[13]</ref> is proposed to fill in patch by the nonlocal means of several top similar patches instead of a single best match patch. Due to multiple samples are utilized, it can more robustly estimate the missing information and produce better result. However, it tends to introduce smooth effect in the recovered image. In this work, we propose a novel model to inpaint patch by the sparse combination of multiple examplars in the framework of sparse representation. This method achieves sharp inpainting result by sparseness prior on the combination coefficients, and achieves consistent inpainting results with the surrounding textures by the constraints on the patch appearance in local neighborhood.</p><p>1) Patch Sparse Representation: Given the patch to be inpainted, a set of candidate patches are sampled from the image source region, where is the number of candidate patches for . Similar to the previous work <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>, the candidate patches are selected as the top most similar patches, and the selection of will be discussed in Section IV-E. Denote as a matrix to extract the unknown pixels in patch . Basically, is approximated as the linear combination of , i.e., <ref type="bibr" target="#b3">(4)</ref> Then the unknown pixels in patch is filled by the corresponding pixels in , i.e.,</p><p>The combination coefficients are inferred by minimizing a constrained optimization problem in the framework of sparse representation. Since we have assumed that the patch is the sparsest linear combination of , the objective of this constrained optimization problem is to minimize the norm of , i.e., the number of nonzero elements in vector . Next we will introduce the constraints for the linear combination.</p><p>We now introduce the first type of constraint on the appearance of , which is called local patch consistency constraint. Firstly, we constrain that the estimated patch should approximate the target patch over the already known pixels, i.e., <ref type="bibr" target="#b5">(6)</ref> where is a parameter to control the error tolerance of this approximation. Secondly, we further assume that the newly filled pixels in the estimated patch should be consistent with the neighboring patches in appearance, i.e., <ref type="bibr" target="#b6">(7)</ref> where is same defined as (2). This constraint measures the similarity between the estimated patch and the weighted mean of the neighboring patches over the missing pixels. balances the strength of the constraints in ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. It is set to 0.25 in our implementation.</p><p>The local patch consistency constraint can be rewritten in a more compact formulation <ref type="bibr" target="#b7">(8)</ref> where <ref type="bibr" target="#b8">(9)</ref> The second type of constraint is that the summation of the coefficients vector equals to one:</p><p>. This constraint is widely used in the local linear embedding literature for invariance to transform when reconstructing the target patch from its neighboring candidate patches <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Under this constraint, when only one element in is nonzero, the coefficient of that element must be one. Then the model will degrade to the same fashion as traditional examplar-based inpainting method <ref type="bibr" target="#b21">[22]</ref>, in which a single best match patch is selected for filling in the patch to be inpainted.</p><p>Finally, the linear combination coefficients can be inferred by optimizing the following constrained optimization problem: <ref type="bibr" target="#b9">(10)</ref> This constrained optimization model can also be formulated as an energy minimization problem <ref type="bibr" target="#b10">(11)</ref> It is equivalent to the constrained optimization problem in <ref type="bibr" target="#b9">(10)</ref> when proper and are selected.</p><p>2) Optimization Algorithm: Generally, the -norm regularized reconstruction model is hard to be solved due to its combinatorial nature. Matching pursuit (MP) or orthogonal matching pursuit (OMP) <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> algorithm and basis pursuit (BP) <ref type="bibr" target="#b39">[40]</ref> algorithm can efficiently retrieve the sparse representation and approximate the optimal solution in a greedy fashion. Another method for optimizing the -norm regularized model is to convexify the problem by -norm regularization. The -norm regularized reconstruction model is the well-known Lasso <ref type="bibr" target="#b40">[41]</ref> in the statistical literatures. In applications, due to the simplicity of OMP algorithm, it is widely used in image sparse representation, and applied to image denoising <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, coding <ref type="bibr" target="#b41">[42]</ref>, edge detection <ref type="bibr" target="#b31">[32]</ref>, audio source separation <ref type="bibr" target="#b42">[43]</ref>, and so on.</p><p>For this optimization problem in <ref type="bibr" target="#b9">(10)</ref>, we propose a novel algorithm to derive the sparse linear combination coefficients in a greedy fashion. Similar to the Matching Pursuit Algorithm, we gradually select nonzero elements from the candidate set of patches step by step. Suppose we have selected nonzero candidate patches in the step (denoted as ), so the sparse representation in this step is <ref type="bibr" target="#b11">(12)</ref> In the next step , we select a new candidate patch from the remaining candidate patches in . The patch with the best Local Patch Consistency in ( <ref type="formula">8</ref>) is selected as the newly selected nonzero element, i.e., <ref type="bibr" target="#b12">(13)</ref> For each candidate patch , the combination coefficients in <ref type="bibr" target="#b12">(13)</ref> are derived by optimizing <ref type="bibr" target="#b13">(14)</ref> This optimization problem ( <ref type="formula">14</ref>) is well studied the literature of Locally Linear Embedding (LLE) <ref type="bibr" target="#b36">[37]</ref> in manifold learning. We define the Gram matrix , where is a matrix with columns , and is a column vector of ones. Then it has a close form solution <ref type="bibr" target="#b14">(15)</ref> We iterate the procedure of selecting a patch in each step until the Local Patch Consistency Constraint ( <ref type="formula">8</ref>) is satisfied or the value of increases. In summary, the algorithm for filling in the missing pixels in patch using patch sparse representation is listed in Fig. <ref type="figure" target="#fig_2">4</ref>. The final inpainting algorithm based on patch sparsity is listed in Fig. <ref type="figure" target="#fig_3">5</ref>. Fig. <ref type="figure">6</ref> presents three examples in which the top corner region of pyramid, the crossing structure of window frame and the curved missing structures are removed. As shown in Fig. <ref type="figure">6(b)-(d)</ref>, the missing regions are gradually completed by the proposed method, and Fig. <ref type="figure">6(d</ref>) are the inpainting results of our method, in which the removed structures are successfully filled in. In Fig. <ref type="figure">6</ref>(e), we also present the results of the most related algorithm in <ref type="bibr" target="#b21">[22]</ref>. The structures in rectangles are not perfectly recovered compared with our results. The keys to the success of our method in completing the complex structures are that, first, the sparsity-based priority better controls the filling order of patches. Second, the patch sparse representation improves the generalization ability of examplars over the single best match examplar used in <ref type="bibr" target="#b21">[22]</ref>. Fig. <ref type="figure">7</ref> gives an example to illustrate the number of nonzero coefficients for 200 patches in the missing region of the second example in Fig. <ref type="figure">8</ref>. It is shown that only sparse number of candidate patches are selected adaptively due to the sparseness constraint on the coefficients. That is the key difference to the previous work that a single best match patch or the combination of constant number of patches are used. The quantitative advantage of this sparse representation over the previous methods will be shown in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND COMPARISONS</head><p>In this section, we test the proposed examplar-based patch propagation algorithm on a variety of natural images. We apply our algorithm to the applications of scratch/text removal, object removal and block completion. In these examples, we compare ouralgorithmwiththepreviousdiffusion-based,examplar-based, and sparsity-based inpainting algorithms. In the following  examples, if not specially stated, the size of patch is set to 7 7, the size of neighborhood (i.e., around in ( <ref type="formula">1</ref>)) for computing patch similarities is set to 51 51, the error tolerance in ( <ref type="formula">10</ref>) is set to 25 times of the number of pixels in a patch, and the number of candidate patches is set to 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments and Comparisons for Scratch and Text Removal</head><p>We now compare the proposed inpainting algorithm with the previous diffusion-based and examplar-based inpainting algorithms for scratch and text removal. Due to the over-smoothing effect of diffusion-based approach for texture inpainting, we select the simultanous texture and structure inpainting algorithm <ref type="bibr" target="#b9">[10]</ref> as an improved version of the diffusion-based approach for comparison. The simultanous texture and structure inpainting algorithm performs inpainting in the texture layer and structure layer simutanously. In our implementation, the structure/texture decomposition method and the inpainting method in the structure layer are chosen as the same methods in the original paper <ref type="bibr" target="#b9">[10]</ref>. However, Fig. <ref type="figure">8</ref>. Comparison for scratch and text removal. The first row shows five original images. In the remaining five rows, the first to the fifth columns show the degraded images, results of simultaneous texture and structure inpainting approach <ref type="bibr" target="#b9">[10]</ref>, the Criminisi's examplar-based approach <ref type="bibr" target="#b21">[22]</ref>, Wong's examplar-based approach <ref type="bibr" target="#b12">[13]</ref>, and our proposed approach. PSNR values of the inpainted image and its each color channel (shown in bracket for R; G; B channels) are presented for each result.</p><p>the texture layer is inpainted by Criminisi's examplar-based algorithm <ref type="bibr" target="#b10">[11]</ref>, which is better for texture inpainting than the tex-ture systhesis method <ref type="bibr" target="#b8">[9]</ref> used by the original paper <ref type="bibr" target="#b9">[10]</ref>. We will also compare our algorithm with Criminisi's examplar-based al-  <ref type="figure">8</ref> gorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref> and Wong's examplar-based algorithm <ref type="bibr" target="#b12">[13]</ref>, which are most related to our work.</p><p>Fig. <ref type="figure">8</ref> presents five examples for scratch and text removal. The first row are the original nondegraded images. In the remaining rows, from the first to the fifth columns are the degraded images, results of simultanous texture and structure inpainting algorithm <ref type="bibr" target="#b9">[10]</ref>, examplar-based inpainting algorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, Wong's examplar-based inpainting algorithm <ref type="bibr" target="#b12">[13]</ref>, and the proposed algorithm. Peak signal-to-noise ratio (PSNR) between the inpainted images and the original images are measured for qualitative comparison. Furthermore, PSNR values in each color channel are also presented in the brackets. As shown in Fig. <ref type="figure">8</ref>, the Criminisi's algorithm produces sharp inpainting results shown in the third column. However, due to the fact that only a single best match patch is used, some unpleasant artifacts are introduced in the results. For example, the unwanted structure appears within the red rectangle of Criminisi's result in the second row of Fig. <ref type="figure">8</ref>. The Wong's algorithm produces more pleasant results because more candidate patches are combined. For example, the unwanted structure shown in the red rectangle is alleviated in the result of Wong's algorithm. The simultaneous texture and structure inpainting algorithm well recovers the texture and structure of the images, and achieves high PSNR values. However, it introduces blurring effect along structure caused by diffusion, which was also observed in <ref type="bibr" target="#b10">[11]</ref>. For our proposed algorithm, the patch priority is defined more robustly, and the candidate patches are adaptively combined in the framework of sparse representation, it achieves sharp and consistently better inpainting results with the best PSNR values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Patch Sparsity on Inpainting Performance</head><p>In this section, we quantitatively justify the improvement of inpainting performance caused by structure sparsity and patch sparse representation. To this end, we take the traditional Criminisi's examplar-based inpainting algorithm <ref type="bibr" target="#b21">[22]</ref> as the baseline, then measure the performance improvement after replacing isophote-based priority <ref type="bibr" target="#b21">[22]</ref> by structure sparsity based priority or(and) replacing the texture synthesis based patch inpainting <ref type="bibr" target="#b21">[22]</ref> by the sparse representation based patch inpainting.</p><p>Table <ref type="table" target="#tab_0">I</ref> presents the performances of six inpainting algorithms: Wong's examplar-based algorithm <ref type="bibr" target="#b12">[13]</ref> (Wong), Crimimisi's examplar-based algorithm <ref type="bibr" target="#b21">[22]</ref> (Crim), the approach using isophote-based priority and sparse representation based patch inpainting model in <ref type="bibr" target="#b43">[44]</ref> (Crim_Spar1), the approach using isophote-based priority and sparse representation based patch inpainting model in (10) (Crim_Spar2), the approach using structure sparsity based priority and texture synthesis based patch inpainting (Spar_Crim), and the proposed inpainting algorithm using patch sparsity (Spar). It is shown that, the mean performances of Spar_Crim and Crim_Spar2 gain 1.27 and 3.27 dB over the baseline respectively. This implies that the patch inpainting using sparse representation model in <ref type="bibr" target="#b9">(10)</ref> contributes more to the performance improvement than the structure sparsity based priority. If updating both of patch priority and patch inpaining by the proposed patch sparsity, the performance of Spar gains 4.16 dB over the baseline. The results of Wong's algorithm <ref type="bibr" target="#b12">[13]</ref> gain 2.55 dB in mean performance over the baseline which is lower than (Crim_Spar2). The inpainting model in <ref type="bibr" target="#b43">[44]</ref> utilizes the isophote-based priority and the sparse representation model without local patch consistency constraint, so the performances of Crim_Spar1 are significantly worse than the performance of Crim_Spar2 and Spar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments and Comparisons for Object Removal</head><p>We now apply the proposed algorithm to inpaint the missing region after object removal. We also compare the proposed algorithm with the related examplar-based inpainting algorithms. See Fig. <ref type="figure" target="#fig_5">9</ref> for examples. The first and second columns are the original images and the degraded images. The third to the fifth columns are the results of Criminisi's algorithm <ref type="bibr" target="#b21">[22]</ref>, Wong's algorithm <ref type="bibr" target="#b12">[13]</ref> and our proposed algorithm. In the results of Criminisi's algorithm, the inpainted patches are not always consistent with the surrounding textures, for example the texture of trees occurs in the texture of water in the first example, and texture of grass appears in the texture of rock in the third example. The Wong's algorithm uses several top best examplars to infer the unknown patch, so the results have less effect of patch inconsistency. However, it introduces smooth effect as shown in the results. As for the proposed algorithm, local patch consistency is constrained, so the inpainted patches are more consistent with the surrounding textures. In addition, the patch is inferred by the sparse combination of candidate patches, so the results have rare smooth effect in appearance.</p><p>In Fig. <ref type="figure" target="#fig_6">10</ref>, we compare our algorithm with the semi-automatic inpainting algorithm <ref type="bibr" target="#b44">[45]</ref> and the Criminisi's algorithm in <ref type="bibr" target="#b21">[22]</ref> on the examples from <ref type="bibr" target="#b44">[45]</ref>. The semi-automatic inpainting algorithm <ref type="bibr" target="#b44">[45]</ref> resolves the structure ambiguity in the missing region by utilizing the human-labeled structures as the guidance, and achieves state-of-the-art results for completing the highly complex structures. In the first example, our method automatically completes the large missing region after removing the car, and the result is comparable to the result in <ref type="bibr" target="#b44">[45]</ref>, which uses human-labeled structures as guidance. In the second example, the structures hidden by the horse are reasonably recovered by the algorithm in <ref type="bibr" target="#b44">[45]</ref> guided by the labeled structures. However,  our algorithm only recovers the horizontal structures and fails to recover the vertical fence structures which are totally hidden by the object. This shows a limitation of our algorithm, i.e., it cannot recover the missing structures without any structure cue in the known region. In both examples, our results are significantly better than the results of algorithm in <ref type="bibr" target="#b21">[22]</ref>, in which the completion results are not visually pleasant.</p><p>We also compare our method with the fragment-based method <ref type="bibr" target="#b15">[16]</ref> on two challenging examples presented in <ref type="bibr" target="#b15">[16]</ref>,</p><p>and the results are shown in Fig. <ref type="figure" target="#fig_7">11</ref>. Both of the fragment-based method <ref type="bibr" target="#b15">[16]</ref> and our proposed method achieve visually pleasant results in the first example. In the second example, the contour of apple in the result of <ref type="bibr" target="#b15">[16]</ref> is blurry and not visually pleasant. However, our method recovers sharp and reasonable contour of apple compared with the original image before removal. The reason is that the sparse combination of patch examplars extends the representation ability of examplars and preserves the sharpness of structure at the same time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons With the Sparsity-Based Inpainting Algorithms for Missing Block Completion</head><p>We also compare our algorithm with the other sparse representation based inpainting algorithms. Guleryuz <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> recovered the missing blocks by adaptively updating image sparse representation in the transform domain. Elad et al. <ref type="bibr" target="#b16">[17]</ref> improved the approach by decomposing image into texture and structure layers <ref type="bibr" target="#b45">[46]</ref>, and recover each layer by sparse representation. Fadili et al. <ref type="bibr" target="#b20">[21]</ref> proposed an expectation minimization (EM) based Bayesian model for inpainting using sparse representation. They have achieved excellent inpainiting results for completion of missing blocks, especially the blocks with size not larger than <ref type="bibr">16 16</ref>. For comparison, we downloaded the source codes of Guleryuz's algorithm, <ref type="foot" target="#foot_0">1</ref> Elad's and Fadili's algorithms<ref type="foot" target="#foot_1">2</ref> from their web page and run the algorithms on examples with large missing blocks. For the Guleryuz's algorithm, we use the Discrete Cosine Transform (DCT, with size of <ref type="bibr">32 32)</ref> as the basis for sparse representation. For the Elad's and Fadili algorithms, we run the codes of MCALab designed for image decomposition and inpainting. We set the dictionary of texture layer as the DCT with block size 32 32, and the dictionary of cartoon layer as the curvelet transform with 5 resolutions.</p><p>Fig. <ref type="figure" target="#fig_8">12</ref> shows four examples for missing block completion. The first column shows the input images, and the second to the fifth columns show the results of Guleryuz's algorithm, Elad's algorithm and Fadili's algorithm and the proposed algorithm. PSNR values are presented for performance comparison. All of these examples are with complicated missing structures, e.g., the structure of eaves in the second example, and the crossing of textures in the third and fourth examples. Generally, Elad's algorithm performs better than Guleryuz's algorithm due to the texture and structure decomposition. Fadili's algorithm well recovers the structures; however, the inpainting results are blurry in appearance. In contrast, the proposed algorithm is an examplar-based approach, which is more powerful for recovering large missing regions with complicated structures, so it better recovers missing regions appeared in these examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of the Number of Candidate Patches on Inpainting Results</head><p>We now test the effect of the number of candidate patches on inpainting results. We take the five test images in Fig. <ref type="figure">8</ref> as examples, and measure the inpainting quality in PSNR with the increasing from 10 to 55. The inpainting quality results are shown in Fig. <ref type="figure" target="#fig_9">13</ref>. We observe that, first, the mean PSNR values vary within a limited range of ; therefore, the inpainting quality is relatively stable to the number of candidate patches. Second, the algorithm performs best when , and larger does not always mean higher performance. The reason might be that the candidate patches provide a subspace for the sparse linear combination in patch inpainting model. Setting larger will include more irrelevant patches as candidate patches which may decrease the generalization ability of the model when inferring the unknown pixels of patches. Based on the above observations, the number of candidate patches in the inpainting model is set to 25 in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details and Computation Time</head><p>In our implementation, the patch similarities defined in (2) are incrementally computed only for the newly introduced patches on the fill-front in each iteration, and the similarities are also used for the computation of local patch consistency constraint in <ref type="bibr" target="#b6">(7)</ref>. On the other hand, only a small subset of candidate patches are used for the optimization of sparse representation model, which deceases its computational overhead. It takes 103 s to fill in the missing region with 5310 missing pixels in the first example of Fig. <ref type="figure">6</ref> using C++ programming language on Intel 2.0GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposed a novel patch propagation based inpainting algorithm for scratch or text removal, object removal and missing block completion. The major novelty of this work is that two types of patch sparsity were proposed and introduced into the examplar-based inpainting algorithm. This was inspired from the recent progress of the research in the fields of image sparse representation and natural image statistics.</p><p>Structure sparsity was designed by measuring the sparseness of the patch similarities in the local neighborhood. The patch with larger structure sparsity, which is generally located at the structure, tends to be selected for further inpainting with higher priority. On the other hand, the patch sparse representation was proposed to synthesize the selected patch by the sparsest linear combination of candidate patches under the local consistency constraint. Experiments and comparisons showed that the proposed examplar-based patch propagation algorithm can better infer the structures and textures of the missing region, and produce sharp inpainting results consistent with the surrounding textures.</p><p>In the future, we will further investigate the sparsity of natural images at multiple scales and orientations, and apply it to the image inpainting, super-resolution and texture synthesis. We are also interested in incorporating the human-labeled structures into our framework in order to recover the totally removed structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Image inpainting using patch sparsity. is the missing region, is the known region, and @ is the fill-front of patch propagation. Iterate the following two steps until completion. (a) Patch selection: The patch 9 on the fill-front with the larger sparseness of nonzero similarities to its neighboring patches is encouraged to be selected with higher priority. The top left of (a) shows two examples of known image parts surrounding patch 9 and 9 , which locate at edge and flat texture region respectively. The bottom-left of (a) shows the similarities of 9 and 9 with their surrounding known patches, the higher brightness means larger similarity. Obviously, similarity map of 9 is much sparser than that of 9 . (b) Patch inpainting: For the selected patch 9 , sparse linear combination of candidate patches f9 ; 9 ; . . . ; 9 g is used to infer the missing pixels in patch 9 under the constraint of local patch consistency in N(p) .</figDesc><graphic coords="3,38.10,66.90,516.00,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Structure sparsity values of patches on corner, edge and textures. The first row shows the patches in rectangle and their surrounding regions. The similarities of patches 9 , 9 , 9 , and 9 with their neighboring patches are com- puted and shown in the second row. The patch 9 and 9 on corner and edge have the larger structure sparsity values compared with the patches 9 and 9 in texture regions. (a) Patch on corner. (b) Patch on edge. (c) Patch in texture. (d) Patch in flat region.</figDesc><graphic coords="4,56.40,66.80,217.00,124.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Inpainting algorithm using patch sparse representation for a patch on the fill front.</figDesc><graphic coords="6,91.98,66.70,409.00,158.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examplar-based inpainting algorithm by patch sparsity.</figDesc><graphic coords="7,90.48,66.68,409.00,169.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Examples of inpainting. (a) Input images with unknown regions. (b)-(d) The inpainting process and the final results of our method. (e) The results of method in [22]. (f) Ground-truth images.</figDesc><graphic coords="7,38.10,275.84,516.00,229.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison for object removal (please refer to the color images in the electronic version for better comparison). (a) The original image. (b) The degraded image. (c)-(e) The results of Criminisi's examplar-based algorithm [22], Wong's examplar-based algorithm [13], and our examplar-based algorithm. Our algorithm achieves sharp inpainting results more consistent with the surrounding textures or structures.</figDesc><graphic coords="10,90.60,67.02,412.00,363.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparison on the examples in [45] (please refer to the color images in the electronic version for better comparison). (b), (c) Human labeled structures and the completion results from [45]. (a) Original images. (b) Unknown regions and human provided structures. (c) Results of structure propagation. (d) Our results. (e) Results of algorithm in [22].</figDesc><graphic coords="10,106.08,468.16,381.00,131.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Image completion results on the examples in [16] using fragment-based method and our method. (a) Input images. (b) Images with unknown region. (c) Results in [16]. (d) Our completion results.</figDesc><graphic coords="11,103.86,66.78,382.00,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Comparison with sparsity-based inpainting algorithms. (a) The input images. (b)-(e) The results of Guleryuz's algorithm [20], Elad's algorithm [17], Fadili's algorithm [21], and the proposed algorithm. (f) The ground-truth images.</figDesc><graphic coords="11,40.62,251.06,246.00,205.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Effect of the number of candidate patches (N ) on inpainting quality. The left figure shows the PSNR values of inpainting results for the five examples in Fig. 8 with different N. The right figure shows the mean PSNR values with different N. The inpainting algorithm is stable to N, and achieves the highest quality when N = 25.</figDesc><graphic coords="12,39.60,70.92,516.00,213.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EFFECT</head><label>I</label><figDesc>OF PATCH SPARSITY ON INPAINTING PERFORMANCE MEASURED IN PSNR. THE EXAMPLES ARE FROM FIG.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://eeweb.poly.edu/%7Eonur/source.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.greyc.ensicaen.fr/~jfadili/demos/WaveRestore/downloads /mcalab/Home.html</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported by the National Key Fundamental Research Program (the 973 Program) of China under Grant 2007CB311002. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Mark (Hong-Yuan) Liao.</p><p>The authors are with the School of Science, Xi'an Jiaotong University, Xi'an, Shaanxi 710049, China</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Navier-Strokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local inpainting models and tv inpainting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1019" to="1043" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-texture inpainting by curvature-driven diffusions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="436" to="449" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision</title>
		<meeting>Int. Conf. Comp. Vision</meeting>
		<imprint>
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Steerable random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision, 1999</title>
		<meeting>Int. Conf. Comp. Vision, 1999</meeting>
		<imprint>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object removal by examplarbased image inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vision, 2003</title>
		<meeting>Int. Conf. Comp. Vision, 2003</meeting>
		<imprint>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object removal by cross isophotes examplar-based image inpainting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognition</title>
		<meeting>Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="810" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A nonlocal-means approach to examplarbased inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image completion using efficient belief propagation via priority scheduling and dynamic pruning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2649" to="2661" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image repairing: Robust image synthesis by adaptive nd tensor voting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recogition</title>
		<meeting>IEEE Computer Society Conf. Computer Vision and Pattern Recogition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fragment-based image completion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="2003">2003. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous cartoon and texture image inpainting using morphological component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Querre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="340" to="358" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlinear approximation based image recovery using adaptive sparse reconstructions</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Guleryuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear approximation based image recovery using adaptive sparse reconstructures and iterated denoising-part i: Theory</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Guleryuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="539" to="554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear approximation based image recovery using adaptive sparse reconstructures and iterated denoising-part ii: Adaptive algorithms</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Guleryuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="555" to="571" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inpainting and zooming using sparse representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Comput. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="79" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Region filling and object removal by examplar-based image inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse long-range random field and its application to image denoising</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting the sparse derivative prior for super-resolution and image demosaicing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Statistical and Computational Theories of Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. Computer Vision and Pattern Recogition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="787" to="794" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative sparse image models for class-specific edge detextion and image interpretation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. Computer Vision and Pattern Recogition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative learned dictionary for local image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comp. Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. Computer Vision and Pattern Recogition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-negative sparse modeling of textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching pursuit in a time-frequency dictionary</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Non-negative sparse modeling of textures,&quot; presented at the Scale Space and Variational Methods in Computer Vision</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Krishnaprasad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regression shrinkge and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Color image scalable coding with matching pursuit</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acoust. Speech, Signal Process</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fvotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech, Audio, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image inpainting via sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="861" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image decomposition via the combination of sparse representations and a variational approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1570" to="1582" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
