<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
							<email>suhangss@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. and Tech</orgName>
								<orgName type="department" key="dep2">State Key Lab for Intell. Tech. &amp; Sys</orgName>
								<orgName type="institution">Institute for AI</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">THBI Lab</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed</head><p>to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the great success, deep neural networks have been shown to be highly vulnerable to adversarial examples <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b11">10]</ref>. These maliciously generated adversarial examples are indistinguishable from legitimate ones by adding small perturbations, but make deep models produce unreasonable predictions. The existence of adversarial examples, even in the physical world <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b3">2]</ref>, has raised concerns in security-sensitive applications, e.g., self-driving cars, healthcare and finance. The adversarial examples generated by the fast gradient sign method (FGSM) <ref type="bibr" target="#b11">[10]</ref> and the proposed translation-invariant FGSM (TI-FGSM) for the Inception v3 <ref type="bibr" target="#b32">[31]</ref> model.</p><p>Attacking deep neural networks has drawn an increasing attention since the generated adversarial examples can serve as an important surrogate to evaluate the robustness of different models <ref type="bibr" target="#b6">[5]</ref> and improve the robustness <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b21">20]</ref>. Several methods have been proposed to generate adversarial examples with the knowledge of the gradient information of a given model, such as fast gradient sign method <ref type="bibr" target="#b11">[10]</ref>, basic iterative method <ref type="bibr" target="#b16">[15]</ref>, and Carlini &amp; Wagner's method <ref type="bibr" target="#b6">[5]</ref>, which are known as white-box attacks. Moreover, it is shown that adversarial examples have cross-model transferability <ref type="bibr" target="#b20">[19]</ref>, i.e., the adversarial examples crafted for one model can fool a different model with a high probability. The transferability enables practical black-box attacks to real-world applications and induces serious security issues.</p><p>The threat of adversarial examples has motivated extensive research on building robust models or techniques to defend against adversarial attacks. These include training with adversarial examples <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b21">20]</ref>, image denoising/transformation <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref>, theoretically-certified defenses <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b36">35]</ref>, and others <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b29">28]</ref>. Although the noncertified defenses have demonstrated robustness against common attacks, they do so by causing obfuscated gradients, which can be easily circumvented by new attacks <ref type="bibr" target="#b2">[1]</ref>.</p><p>Figure <ref type="figure">2</ref>. Demonstration of the different discriminative regions of the defense models compared with normally trained models. We adopt class activation mapping <ref type="bibr" target="#b39">[38]</ref> to visualize the attention maps of three normally trained models-Inception v3 <ref type="bibr" target="#b32">[31]</ref>, Inception ResNet v2 <ref type="bibr" target="#b31">[30]</ref>, ResNet 152 <ref type="bibr" target="#b13">[12]</ref> and four defense models <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref>. These defense models rely on different discriminative regions for predictions compared with normally trained models, which could affect the transferability of adversarial examples.</p><p>However, some of the defenses <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref> claim to be resistant to transferable adversarial examples, making it difficult to evade them by black-box attacks.</p><p>The resistance of the defense models against transferable adversarial examples is largely due to the phenomenon that the defenses make predictions based on different discriminative regions compared with normally trained models. For example, we show the attention maps of several normally trained models and defense models in Fig. <ref type="figure">2</ref>, to represent the discriminative regions for their predictions. It can be seen that the normally trained models have similar attention maps while the defenses induce different attention maps. A similar observation is also found in <ref type="bibr" target="#b35">[34]</ref> that the gradients of the defenses in the input space align well with human perception, while those of normally trained models appear very noisy. This phenomenon of the defenses is caused by either training under different data distributions <ref type="bibr" target="#b34">[33]</ref> or transforming the inputs before classification <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref>. For blackbox attacks based on the transferability <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b8">7]</ref>, an adversarial example is usually generated for a single input against a white-box model. So the generated adversarial example is highly correlated with the discriminative region or gradient of the white-box model at the given input point, making it hard to transfer to other defense models that depend on different regions for predictions. Therefore, the transferability of adversarial examples is largely reduced to the defenses.</p><p>To mitigate the effect of different discriminative regions between models and evade the defenses by transferable adversarial examples, we propose a translation-invariant attack method. In particular, we generate an adversarial example for an ensemble of images composed of a legitimate one and its translated versions. We expect that the resultant adversarial example is less sensitive to the discriminative region of the white-box model being attacked, and has a higher probability to fool another black-box model with a defense mechanism. However, to generate such a perturbation, we need to calculate the gradients for all images in the ensemble, which brings much more computations. To improve the efficiency of our attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel under a mild assumption. By combining the proposed method with any gradient-based attack method (e.g., fast gradient sign method <ref type="bibr" target="#b11">[10]</ref>, etc.), we obtain more transferable adversarial examples with similar computation complexity.</p><p>Extensive experiments on the ImageNet dataset <ref type="bibr" target="#b28">[27]</ref> demonstrate that the proposed translation-invariant attack method helps to improve the success rates of black-box attacks against the defense models by a large margin. Our best attack reaches an average success rate of 82% to evade eight state-of-the-art defenses based only on the transferability, thus demonstrating the insecurity of the current defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial examples. Deep neural networks have been shown to be vulnerable to adversarial examples first in the visual domain <ref type="bibr" target="#b33">[32]</ref>. Then several methods are proposed to generate adversarial examples for the purpose of high success rates and minimal size of perturbations <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b6">5]</ref>. They also exist in the physical world <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b3">2]</ref>. Although adversarial examples are recently crafted for many other domains, we focus on image classification tasks in this paper.</p><p>Black-box attacks. Black-box adversaries have no access to the model parameters or gradients. The transferability <ref type="bibr" target="#b20">[19]</ref> of adversarial examples can be used to attack a black-box model. Several methods <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37]</ref> have been proposed to improve the transferability, which enable powerful black-box attacks. Besides the transfer-based black-box attacks, there is another line of work that performs attacks based on adaptive queries. For example, Papernot et al. <ref type="bibr" target="#b26">[25]</ref> use queries to distill the knowledge of the target model and train a surrogate model. They therefore turn the blackbox attacks to the white-box attacks. Recent methods use queries to estimate the gradient or the decision boundary of the black-box model <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b5">4]</ref> to generate adversarial examples.</p><p>However, these methods usually require a large number of queries, which is impractical in real-world applications. In this paper, we resort to transfer-based black-box attacks.</p><p>Attacks for an ensemble of examples. An adversarial perturbation can be generated for an ensemble of legitimate examples. In <ref type="bibr" target="#b23">[22]</ref>, the universal perturbations are generated for the entire data distribution, which can fool the models on most of natural images. In <ref type="bibr" target="#b3">[2]</ref>, the adversarial perturbation is optimized over a distribution of transformations, which is similar to our method. The major difference between the method in <ref type="bibr" target="#b3">[2]</ref> and ours is three-fold. First, we want to generate transferable adversarial examples against the defense models, while the authors in <ref type="bibr" target="#b3">[2]</ref> propose to synthesize robust adversarial examples in the physical world. Second, we only use the translation operation, while they use a lot of transformations such as rotation, translation, addition of noise, etc. Third, we develop an efficient algorithm for optimization that only needs to calculate the gradient for the untranslated image, while they calculate the gradients for a batch of transformed images by sampling.</p><p>Defend against adversarial attacks. A large variety of methods have been proposed to increase the robustness of deep learning models. Besides directly making the models produce correct predictions for adversarial examples, some methods attempt to detect them instead <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b24">23]</ref>. However, most of the non-certified defenses demonstrate the robustness by causing obfuscated gradients, which can be successfully circumvented by new attacks <ref type="bibr" target="#b2">[1]</ref>. Although these defenses are not robust in the white-box setting, some of them <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref> empirically show the resistance against transferable adversarial examples in the black-box setting. In this paper, we focus on generating more transferable adversarial examples against these defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we provide the detailed description of our algorithm. Let x real denote a real example and y denote the corresponding ground-truth label. Given a classifier f (x) : X → Y that outputs a label as the prediction for an input, we want to generate an adversarial example x adv which is visually indistinguishable from x real but fools the classifier, i.e., f (x adv ) = y. <ref type="foot" target="#foot_0">1</ref> In most cases, the L p norm of the adversarial perturbation is required to be smaller than a threshold ǫ as ||x adv − x real || p ≤ ǫ. In this paper, we use the L ∞ norm as the measurement. For adversarial example generation, the objective is to maximize the loss function J(x adv , y) of the classifier, where J is often the crossentropy loss. So the constrained optimization problem can be written as arg max</p><formula xml:id="formula_0">x adv J(x adv , y), s.t. x adv − x real ∞ ≤ ǫ.<label>(1)</label></formula><p>To solve this optimization problem, the gradient of the loss function with respect to the input needs to be calculated, termed as white-box attacks. However, in some cases, we cannot get access to the gradients of the classifier, where we need to perform attacks in the black-box manner. We resort to transferable adversarial examples which are generated for a different white-box classifier but have high transferability for black-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gradient-based Adversarial Attack Methods</head><p>Several methods have been proposed to solve the optimization problem in Eq. ( <ref type="formula" target="#formula_0">1</ref>). We give a brief introduction of them in this section.</p><p>Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b11">[10]</ref> generates an adversarial example x adv by linearizing the loss function in the input space and performing one-step update as</p><formula xml:id="formula_1">x adv = x real + ǫ • sign(∇ x J(x real , y)),<label>(2)</label></formula><p>where ∇ x J is the gradient of the loss function with respect to x. sign(•) is the sign function to make the perturbation meet the L ∞ norm bound. FGSM can generate more transferable adversarial examples but is usually not effective enough for attacking white-box models <ref type="bibr" target="#b17">[16]</ref>.</p><p>Basic Iterative Method (BIM) <ref type="bibr" target="#b16">[15]</ref> extends FGSM by iteratively applying gradient updates multiple times with a small step size α, which can be expressed as</p><formula xml:id="formula_2">x adv t+1 = x adv t + α • sign(∇ x J(x adv t , y)),<label>(3)</label></formula><p>where x adv 0 = x real . To restrict the generated adversarial examples within the ǫ-ball of x real , we can clip x adv t after each update, or set α = ǫ /T , with T being the number of iterations. It has been shown that BIM induces much more powerful white-box attacks than FGSM at the cost of worse transferability <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b8">7]</ref>.</p><p>Momentum Iterative Fast Gradient Sign Method (MI-FGSM) <ref type="bibr" target="#b8">[7]</ref> proposes to improve the transferability of adversarial examples by integrating a momentum term into the iterative attack method. The update procedure is</p><formula xml:id="formula_3">g t+1 = µ • g t + ∇ x J(x adv t , y) ∇ x J(x adv t , y) 1 ,<label>(4)</label></formula><formula xml:id="formula_4">x adv t+1 = x adv t + α • sign(g t+1 ),<label>(5)</label></formula><p>where g t gathers the gradient information up to the t-th iteration with a decay factor µ. Diverse Inputs Method <ref type="bibr" target="#b38">[37]</ref> applies random transformations to the inputs and feeds the transformed images into the classifier for gradient calculation. The transformation includes random resizing and padding with a given probability. This method can be combined with the momentumbased method to further improve the transferability.</p><p>Carlini &amp; Wagner's method (C&amp;W) <ref type="bibr" target="#b6">[5]</ref> is a powerful optimization-based method, which solves arg min</p><formula xml:id="formula_5">x adv x adv − x real p − c • J(x adv , y),<label>(6)</label></formula><p>where the loss function J could be different from the crossentropy loss. This method aims to find adversarial examples with minimal size of perturbations, to measure the robustness of different models. It also lacks the effectiveness for black-box attacks like BIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Translation-Invariant Attack Method</head><p>Although many attack methods <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b38">37]</ref> can generate adversarial examples with very high transferability across normally trained models, they are less effective to attack defense models in the black-box manner. Some of the defenses <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b12">11]</ref> are shown to be quite robust against black-box attacks. So we want to answer that: Are these defenses really free from transferable adversarial examples?</p><p>We find that the discriminative regions used by the defenses to identify object categories are different from those used by normally trained models, as shown in Fig. <ref type="figure">2</ref>. When generating an adversarial example by the methods introduced in Sec. 3.1, the adversarial example is only optimized for a single legitimate example. So it may be highly correlated with the discriminative region or gradient of the whitebox model being attacked at the input data point. For other black-box defense models that have different discriminative regions or gradients, the adversarial example can hardly remain adversarial. Therefore, the defenses are shown to be robust against transferable adversarial examples.</p><p>To generate adversarial examples that are less sensitive to the discriminative regions of the white-box model, we propose a translation-invariant attack method. In particular, rather than optimizing the objective function at a single point as Eq. ( <ref type="formula" target="#formula_0">1</ref>), the proposed method uses a set of translated images to optimize an adversarial example as arg max</p><formula xml:id="formula_6">x adv i,j w ij J(T ij (x adv ), y), s.t. x adv − x real ∞ ≤ ǫ,<label>(7)</label></formula><p>where T ij (x) is the translation operation that shifts image x by i and j pixels along the two-dimensions respectively, i.e., each pixel (a, b) of the translated image is T ij (x) a,b = x a−i,b−j , and w ij is the weight for the loss J(T ij (x adv ), y).</p><p>We set i, j ∈ {−k, ..., 0, ..., k} with k being the maximal number of pixels to shift. With this method, the generated adversarial examples are less sensitive to the discriminative regions of the white-box model being attacked, which may be transferred to another model with a higher success rate. We choose the translation operation in this paper rather than other transformations (e.g., rotation, scaling, etc.), because we can develop an efficient algorithm to calculate the gradient of the loss function by the assumption of the translationinvariance <ref type="bibr" target="#b18">[17]</ref> in convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Gradient Calculation</head><p>To solve the optimization problem in Eq. ( <ref type="formula" target="#formula_6">7</ref>), we need to calculate the gradients for (2k + 1) 2 images, which introduces much more computations. Sampling a small number of translated images for gradient calculation is a feasible way <ref type="bibr" target="#b3">[2]</ref>. But we show that we can calculate the gradient for only one image under a mild assumption.</p><p>Convolutional neural networks are supposed to have the translation-invariant property <ref type="bibr" target="#b18">[17]</ref>, that an object in the input can be recognized in spite of its position. In practice, CNNs are not truly translation-invariant <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b15">14]</ref>. So we make an assumption that the translation-invariant property is nearly held with very small translations (which is empirically validated in Sec. 4.2). In our problem, we shift the image by no more than 10 pixels along each dimension (i.e., k ≤ 10). Therefore, based on this assumption, the translated image T ij (x) is almost the same as x as inputs to the models, as well as their gradients</p><formula xml:id="formula_7">∇ x J(x, y) x=Tij ( x) ≈ ∇ x J(x, y) x= x.<label>(8)</label></formula><p>We then calculate the gradient of the loss function defined in Eq. ( <ref type="formula" target="#formula_6">7</ref>) at a point x as</p><formula xml:id="formula_8">∇ x i,j w ij J(T ij (x), y) x= x = i,j w ij ∇ x J(T ij (x), y) x= x = i,j w ij ∇ Tij (x) J(T ij (x), y) • ∂T ij (x) ∂x x= x = i,j w ij T −i−j ∇ x J(x, y) x=Tij ( x) ≈ i,j w ij T −i−j (∇ x J(x, y) x= x).<label>(9)</label></formula><p>Given Eq. ( <ref type="formula" target="#formula_8">9</ref>), we do not need to calculate the gradients for (2k + 1) 2 images. Instead, we only need to get the gradient at the untranslated image x and then average all the shifted gradients. This procedure is equivalent to convolving the gradient with a kernel composed of all the weights w ij as i,j</p><formula xml:id="formula_9">w ij T −i−j (∇ x J(x, y) x= x) ⇔ W * ∇ x J(x, y) x= x,</formula><p>where W is the kernel matrix of size (2k + 1) × (2k + 1), with W i,j = w −i−j . We will specify W in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Kernel Matrix</head><p>There are many options to generate the kernel matrix W . A basic design principle is that the images with bigger shifts should have relatively lower weights to make the adversarial perturbation fool the model at the untranslated image effectively. In this paper, we consider three different choices:</p><p>(1) A uniform kernel that W i,j = 1 /(2k+1) 2 ;</p><p>(2) A linear kernel that Wi,j = (1 − |i| /k+1) • (1 − |j| /k+1), and W i,j = Wi,j / i,j Wi,j ;</p><p>(3) A Gaussian kernel that Wi,j = 1 2πσ 2 exp(− i 2 +j 2 2σ 2 ) where the standard deviation σ = k / √ 3 to make the radius of the kernel be 3σ, and W i,j = Wi,j / i,j Wi,j .</p><p>We will empirically compare the three kernels in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Attack Algorithms</head><p>Note that in Sec. 3.2.1, we only illustrate how to calculate the gradient of the loss function defined in Eq. ( <ref type="formula" target="#formula_6">7</ref>), but do not specify the update algorithm for generating adversarial examples. This indicates that our method can be integrated into any gradient-based attack method, e.g., FGSM, BIM, MI-FGSM, etc. For gradient-based attack methods presented in Sec. 3.1, in each step we calculate the gradient ∇ x J(x adv t , y) at the current solution x adv t , then convolve the gradient with the pre-defined kernel W , and finally obtain the new solution x adv t+1 following the update rule in different attack methods. For example, the combination of our translation-invariant method and the fast gradient sign method <ref type="bibr" target="#b11">[10]</ref> (TI-FGSM) has the following update rule</p><formula xml:id="formula_10">x adv = x real + ǫ • sign(W * ∇ x J(x real , y)). (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>Also, the integration of the translation-invariant method into the basic iterative method <ref type="bibr" target="#b16">[15]</ref> yields the TI-BIM algorithm</p><formula xml:id="formula_12">x adv t+1 = x adv t + α • sign(W * ∇ x J(x adv t , y)). (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>The translation-invariant method can be similarly integrated into MI-FGSM <ref type="bibr" target="#b8">[7]</ref> and DIM <ref type="bibr" target="#b38">[37]</ref> as TI-MI-FGSM and TI-DIM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the experimental results to demonstrate the effectiveness of the proposed method. We first specify the experimental settings in Sec. 4.1. Then we validate the translation-invariant property of convolutional neural networks in Sec. 4.2. We further conduct two experiments to study the effects of different kernels and size of kernels in Sec. 4.3 and Sec. 4.4. We finally compare the results of the proposed method with baseline methods in Sec. 4.5 and Sec. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>We use an ImageNet-compatible dataset 2 comprised of 1,000 images to conduct experiments. This dataset was used 2 https://github.com/tensorflow/cleverhans/tree/ master/examples/nips17_adversarial_competition/ dataset in the NIPS 2017 adversarial competition. We include eight defense models which are shown to be robust against blackbox attacks on the ImageNet dataset. These are • Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens <ref type="bibr" target="#b34">[33]</ref>;</p><p>• high-level representation guided denoiser (HGD, rank-1 submission in the NIPS 2017 defense competition) <ref type="bibr" target="#b19">[18]</ref>; • input transformation through random resizing and padding (R&amp;P, rank-2 submission in the NIPS 2017 defense competition) <ref type="bibr" target="#b37">[36]</ref>; • input transformation through JPEG compression or total variance minimization (TVM) <ref type="bibr" target="#b12">[11]</ref>; • rank-3 submission<ref type="foot" target="#foot_1">3</ref> in the NIPS 2017 defense competition (NIPS-r3). To attack these defenses based on the transferability, we also include four normally trained models-Inception v3 (Inc-v3) <ref type="bibr" target="#b32">[31]</ref>, Inception v4 (Inc-v4), Inception ResNet v2 (IncRes-v2) <ref type="bibr" target="#b31">[30]</ref>, and ResNet v2-152 (Res-v2-152) <ref type="bibr" target="#b14">[13]</ref>, as the white-box models to generate adversarial examples.</p><p>In our experiments, we integrate our method into the fast gradient sign method (FGSM) <ref type="bibr" target="#b11">[10]</ref>, momentum iterative fast gradient sign method (MI-FGSM) <ref type="bibr" target="#b8">[7]</ref>, and diverse inputs method (DIM) <ref type="bibr" target="#b38">[37]</ref>. We do not include the basic iterative method <ref type="bibr" target="#b16">[15]</ref> and C&amp;W's method <ref type="bibr" target="#b6">[5]</ref> since that they are not good at generating transferable adversarial examples <ref type="bibr" target="#b8">[7]</ref>. We denote the attacks combined with our translation-invariant method as TI-FGSM, TI-MI-FGSM, and TI-DIM, respectively.</p><p>For the settings of hyper-parameters, we set the maximum perturbation to be ǫ = 16 among all experiments with pixel values in [0, 255]. For the iterative attack methods, we set the number of iteration as 10 and the step size as α = 1.6. For MI-FGSM and TI-MI-FGSM, we adopt the default decay factor µ = 1.0. For DIM and TI-DIM, the  transformation probability is set to 0.7. Please note that the settings for each attack method and its translation-invariant version are the same, because our method is not concerned with the specific attack procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Translation-Invariant Property of CNNs</head><p>We first verify the translation-invariant property of convolutional neural networks in this section. We use the original 1,000 images from the dataset and shift them by −10 to 10 pixels in each dimension. We input the original images as well as the translated images into Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152, respectively. The loss of each input image is given by the models. We average the loss over all translated images at each position, and show the loss surfaces in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>It can be seen that the loss surfaces are generally smooth with the translations going from −10 to 10 in each dimension. So we could make the assumption that the translationinvariant property is almost held within a small range. In our attacks, the images are shifted by no more than 10 pixels along each dimension. The loss values would be very similar for the original and translated images. Therefore, we regard that a translated image is almost the same as the corresponding original image as inputs to the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Results of Different Kernels</head><p>In the section, we show the experimental results of the proposed translation-invariant attack method with different choices of kernels. We attack the Inc-v3 model by TI-FGSM, TI-MI-FGSM, and TI-DIM with three types of kernels, i.e., uniform kernel, linear kernel, and Gaussian kernel, as introduced in Sec. 3.2.2. In Table <ref type="table" target="#tab_0">1</ref>, we report the success rates of black-box attacks against the eight defense models we study, where the success rates are the misclassification rates of the corresponding defense models with the generated adversarial images as inputs.</p><p>We can see that for TI-FGSM, the linear kernel leads to better results than the uniform kernel and the Gaussian kernel. And for more powerful attacks such as TI-MI-FGSM and TI-DIM, the Gaussian kernel achieves similar or even better results than the linear kernel. However, both of the linear kernel and the Gaussian kernel are more effective than the uniform kernel. It indicates that we should design the kernel that has lower weights for bigger shifts, as discussed in Sec. 3.2.2. We simply adopt the Gaussian kernel in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Effect of Kernel Size</head><p>The size of the kernel W also plays a key role for improving the success rates of black-box attacks. If the kernel size equals to 1 × 1, the translation-invariant based attacks degenerate to their vanilla versions. Therefore, we conduct an ablation study to examine the effect of kernel sizes.</p><p>We attack the Inc-v3 model by TI-FGSM, TI-MI-FGSM, and TI-DIM with the Gaussian kernel, whose length ranges from 1 to 21 with a granularity 2. In Fig. <ref type="figure" target="#fig_2">4</ref>, we show the suc-  cess rates against five defense models-IncRes-v2 ens , HGD, R&amp;P, TVM, and NIPS-r3. The success rate continues increasing at first, and turns to remain stable after the kernel size exceeds 15 × 15. Therefore, the size of the kernel is set to 15 × 15 in the following.</p><p>We also show the adversarial images generated for the Inc-v3 model by TI-FGSM with different kernel sizes in Fig. <ref type="figure" target="#fig_3">5</ref>. Due to the smooth effect given by the kernel, we can see that the adversarial perturbations are smoother when using a bigger kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Single-Model Attacks</head><p>In this section, we compare the black-box success rates of the translation-invariant based attacks with baseline attacks. We first perform adversarial attacks for Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152 respectively using FGSM, MI-FGSM, DIM, and their extensions by combining with the translation-invariant attack method as TI-FGSM, TI-MI-FGSM, and TI-DIM. We adopt the 15 × 15 Gaussian kernel in this set of experiments. We then use the generated adversarial examples to attack the eight defense models we consider based only on the transferability. We report the success rates of black-box attacks in Table <ref type="table">2</ref> for FGSM and TI-FGSM, Table <ref type="table" target="#tab_1">3</ref> for MI-FGSM and TI-MI-FGSM, and Table <ref type="table">4</ref> for DIM and TI-DIM.</p><p>From the tables, we observe that the success rates against the defenses are improved by a large margin when using the proposed method regardless of the attack algorithms or the white-box models being attacked. In general, the translation-invariant based attacks consistently outperform the baseline attacks by 5% ∼ 30%. In particular, when using TI-DIM, the combination of our method and DIM, to attack the IncRes-v2 model, the resultant adversarial examples have about 60% success rates against the defenses (as shown in Table <ref type="table">4</ref>). It demonstrates the vulnerability of the current defenses against black-box attacks. The results also validate the effectiveness of the proposed method. Although we only compare the results of our attack method with base-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ensemble-based Attacks</head><p>In this section, we further present the results when adversarial examples are generated for an ensemble of models. Liu et al. <ref type="bibr" target="#b20">[19]</ref> have shown that attacking multiple models at the same time can improve the transferability of the generated adversarial examples. It is due to that if an example remains adversarial for multiple models, it is more likely to transfer to another black-box model.</p><p>We adopt the ensemble method proposed in <ref type="bibr" target="#b8">[7]</ref>, which fuses the logit activations of different models. We attack the ensemble of Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152 with equal ensemble weights using FGSM, TI-FGSM, MI-FGSM, TI-MI-FGSM, DIM, and TI-DIM respectively. We also use the 15 × 15 Gaussian kernel in the translationinvariant based attacks.</p><p>In Table <ref type="table">5</ref>, we show the results of black-box attacks against the eight defenses. The proposed method also improves the success rates across all experiments over the baseline attacks. It should be noted that the adversarial examples generated by TI-DIM can fool the state-of-the-art defenses at an 82% success rate on average based on the transferability. And the adversarial examples are generated for normally trained models unaware of the defense strategies. The results in the paper demonstrate that the current defenses are far from real security, and cannot be deployed in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a translation-invariant attack method to generate adversarial examples that are less sensitive to the discriminative regions of the white-box model being attacked, and have higher transferability against the defense models. Our method optimizes an adversarial image by using a set of translated images. Based on an assumption, our method is efficiently implemented by convolving the gradient with a pre-defined kernel, and can be integrated into any gradient-based attack method. We conducted experiments to validate the effectiveness of the proposed method. Our best attack, TI-DIM, the combination of the proposed translation-invariant method and diverse inputs method <ref type="bibr" target="#b38">[37]</ref>, can fool eight state-of-the-art defenses at an 82% success rate on average, where the adversarial examples are generated against four normally trained models. The results identify the vulnerability of the current defenses, and thus raise security issues for the development of more robust deep learning models. We make our codes public at https://github.com/dongyp13/ Translation-Invariant-Attacks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The adversarial examples generated by the fast gradient sign method (FGSM) [10] and the proposed translation-invariant FGSM (TI-FGSM) for the Inception v3 [31] model.</figDesc><graphic url="image-4.png" coords="1,325.43,317.13,60.84,60.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. We show the loss surfaces of Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152 given the translated images at each position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The success rates (%) of black-box attacks against IncRes-v2ens, HGD, R&amp;P, TVM, and NIPS-r3. The adversarial examples are generated for Inc-v3 with the kernel length ranging from 1 to 21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The adversarial examples generated for Inc-v3 by TI-FGSM with different kernel sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The success rates (%) of black-box attacks against eight defenses with different choices of kernels. The adversarial examples are crafted for Inc-v3 by TI-FGSM, TI-MI-FGSM and TI-DIM with the uniform kernel, the linear kernel, and the Gaussian kernel, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attack</cell><cell>Inc-v3 ens3</cell><cell cols="2">Inc-v3 ens4</cell><cell></cell><cell cols="2">IncRes-v2ens</cell><cell>HGD</cell><cell cols="2">R&amp;P</cell><cell></cell><cell>JPEG</cell><cell>TVM</cell><cell>NIPS-r3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Uniform</cell><cell>25.0</cell><cell>27.9</cell><cell></cell><cell></cell><cell></cell><cell>21.1</cell><cell>15.7</cell><cell cols="2">19.1</cell><cell></cell><cell>24.8</cell><cell>32.3</cell><cell>21.9</cell></row><row><cell></cell><cell cols="3">TI-FGSM</cell><cell></cell><cell></cell><cell>Linear</cell><cell>30.7</cell><cell>32.4</cell><cell></cell><cell></cell><cell></cell><cell>24.2</cell><cell>20.9</cell><cell cols="2">23.3</cell><cell></cell><cell>28.1</cell><cell>34.6</cell><cell>25.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian</cell><cell>28.2</cell><cell>28.9</cell><cell></cell><cell></cell><cell></cell><cell>22.3</cell><cell>18.4</cell><cell cols="2">19.8</cell><cell></cell><cell>25.5</cell><cell>30.7</cell><cell>24.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Uniform</cell><cell>30.0</cell><cell>32.2</cell><cell></cell><cell></cell><cell></cell><cell>22.8</cell><cell>21.7</cell><cell cols="2">22.8</cell><cell></cell><cell>26.4</cell><cell>32.7</cell><cell>25.9</cell></row><row><cell cols="5">TI-MI-FGSM</cell><cell></cell><cell>Linear</cell><cell>35.8</cell><cell>35.0</cell><cell></cell><cell></cell><cell></cell><cell>26.8</cell><cell>25.5</cell><cell cols="2">23.4</cell><cell></cell><cell>29.0</cell><cell>35.8</cell><cell>27.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian</cell><cell>35.8</cell><cell>35.1</cell><cell></cell><cell></cell><cell></cell><cell>25.8</cell><cell>25.7</cell><cell cols="2">23.9</cell><cell></cell><cell>28.2</cell><cell>34.9</cell><cell>26.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Uniform</cell><cell>32.6</cell><cell>34.6</cell><cell></cell><cell></cell><cell></cell><cell>25.6</cell><cell>24.1</cell><cell cols="2">27.2</cell><cell></cell><cell>30.2</cell><cell>34.9</cell><cell>28.8</cell></row><row><cell></cell><cell cols="3">TI-DIM</cell><cell></cell><cell></cell><cell>Linear</cell><cell>45.2</cell><cell>47.0</cell><cell></cell><cell></cell><cell></cell><cell>34.9</cell><cell>35.6</cell><cell cols="2">35.2</cell><cell></cell><cell>38.5</cell><cell>43.6</cell><cell>39.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian</cell><cell>46.9</cell><cell>47.1</cell><cell></cell><cell></cell><cell></cell><cell>37.4</cell><cell>38.3</cell><cell cols="2">36.8</cell><cell></cell><cell>37.0</cell><cell>44.2</cell><cell>41.4</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell></row><row><cell>Success Rate (%)</cell><cell>20 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IncRes-v2-ens</cell><cell>Success Rate (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success Rate (%)</cell><cell>20 30</cell><cell></cell><cell>IncRes-v2-ens</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HGD R&amp;P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>HGD R&amp;P</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TVM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TVM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NIPS-r3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NIPS-r3</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell cols="2">9 11 13 15 17 19 21</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell cols="2">9 11 13 15 17 19 21</cell><cell></cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9 11 13 15 17 19 21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kernel Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kernel Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kernel Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The success rates (%) of black-box attacks against eight defenses. The adversarial examples are crafted for Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152 respectively using MI-FGSM and TI-MI-FGSM.</figDesc><table><row><cell>Kernel Length=1 Kernel Length=3 Kernel Length=5 Kernel Length=7 Kernel Length=9 Kernel Length=11 Kernel Length=13 Kernel Length=15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This corresponds to untargeted attack. The method in this paper can be simply extended to targeted attack.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/anlthms/nips-2017/tree/ master/mmd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61571261), Beijing NSF Project (No. L172037), DITD Program JCKY2017204B064, Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, and the projects from Siemens and Intel.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>line methods against the defense models, our attacks remain the success rates of baseline attacks in the white-box setting and the black-box setting against normally trained models, which will be shown in the Appendix. We show two adversarial images generated for the Inc-v3 model by FGSM and TI-FGSM in Fig. <ref type="figure">1</ref>. It can be seen that by using TI-FGSM, in which the gradients are convolved by a kernel W before applying to the raw images, the adversarial perturbations are much smoother than those generated by FGSM. The smooth effect also exists in other translationinvariant based attacks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Attack Inc-v3 ens3 Inc-v3 ens4 IncRes-v2ens HGD R&amp;P JPEG TVM NIPS-r3</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Table 2. The success rates (%) of black-box attacks against eight defenses. The adversarial examples are crafted for Inc-v3, Inc-v4, IncRes-v2, and Res-v2-152 respectively using FGSM and TI-FGSM</title>
	</analytic>
	<monogr>
		<title level="m">Attack Inc-v3 ens3 Inc-v3 ens4 IncRes-v2ens HGD R&amp;P JPEG TVM NIPS-r3</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2004">2018. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2005">2017. 1, 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based blackbox attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin</forename><surname>Yu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Workshop on Artificial Intelligence and Security (AISec)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2018. 2, 3, 4, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2005">2015. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2005">2018. 1, 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Quantifying translation-invariance in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kauderer-Abrams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01450</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2005">2016. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2018. 1, 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards robust detection of adversarial examples</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Max-mahalanobis linear discriminant analysis networks</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2005">2018. 1, 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2005">2018. 1, 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06978</idno>
		<imprint>
			<date type="published" when="2008">2018. 2, 3, 4, 5, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
