<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Deep Learning for Robust RGB-D Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-08-18">18 Aug 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><forename type="middle">Eitel</forename><surname>Jost</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Springenberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luciano</forename><surname>Spinello</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Deep Learning for Robust RGB-D Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-08-18">18 Aug 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1507.06821v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams -one for each modality -which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present stateof-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>RGB-D object recognition is a challenging task that is at the core of many applications in robotics, indoor and outdoor. Nowadays, RGB-D sensors are ubiquitous in many robotic systems. They are inexpensive, widely supported by open source software, do not require complicated hardware and provide unique sensing capabilities. Compared to RGB data, which provides information about appearance and texture, depth data contains additional information about object shape and it is invariant to lighting or color variations.</p><p>In this paper, we propose a new method for object recognition from RGB-D data. In particular, we focus on making recognition robust to imperfect sensor data. A scenario typical for many robotics tasks. Our approach builds on recent advances from the machine learning and computer vision community. Specifically, we extend classical convolutional neural network networks (CNNs), which have recently been shown to be remarkably successful for recognition on RGB images <ref type="bibr" target="#b12">[13]</ref>, to the domain of RGB-D data. Our architecture, which is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, consists of two convolutional network streams operating on color and depth information respectively. The network automatically learns to combine these two processing streams in a late fusion approach. This architecture bears similarity to other recent multi-stream approaches <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Training of the individual stream networks as well as the combined architecture follows a stage-wise approach. We start by separately training the networks for each modality, followed by a third training stage in which the two streams are jointly finetuned, together with a fusion network that performs the final All authors are with the Department of Computer Science, University of Freiburg, Germany. This work was partially funded by the DFG under the priority programm "Autonomous Learning" (SPP 1527). {eitel, springj, riedmiller, spinello, burgard}@cs.uni-freiburg.de classification. We initialize both the RGB and depth stream network with weights from a network pre-trained on the ImageNet dataset <ref type="bibr" target="#b18">[19]</ref>. While initializing an RGB network from a pre-trained ImageNet network is straight-forward, using such a network for processing depth data is not. Ideally, one would want to directly train a network for recognition from depth data without pre-training on a different modality which, however, is infeasible due to lack of large scale labeled depth datasets. Due to this lack of labeled training data, a pre-training phase for the depth-modality -leveraging RGB data -becomes of key importance. We therefore propose a depth data encoding to enable re-use of CNNs trained on ImageNet for recognition from depth data. The intuition -proved experimentally -is to simply encode a depth image as a rendered RGB image, spreading the information contained in the depth data over all three RGB channels and then using a standard (pre-trained) CNN for recongition.</p><p>In real-world environments, objects are often subject to occlusions and sensor noise. In this paper, we propose a data augmentation technique for depth data that can be used for robust training. We augment the available training examples by corrupting the depth data with missing data patterns sampled from real-world environments. Using these two techniques, our system can both learn robust depth features and implicitly weight the importance of the two modalities.</p><p>We tested our method to support our claims: first, we report on RGB-D recognition accuracy, then on robustness with respect to real-world noise. For the first, we show that our work outperforms the current state of the art on the RGB-D Object dataset of Lai et al. <ref type="bibr" target="#b14">[15]</ref>. For the second, we show that our data augmentation approach improves object recognition accuracy in a challenging real-world and noisy environment using the RGB-D Scenes dataset <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our approach is related to a large body of work on both convolutional neural networks (CNNs) for object recognition as well as applications of computer vision techniques to the problem of recognition from RGB-D data. Although a comprehensive review of the literature on CNNs and object recognition is out of the scope of this paper, we will briefly highlight connections and differences between our approach and existing work with a focus on recent literature.</p><p>Among the many successful algorithms for RGB-D object recognition a large portion still relies on hand designed features such as SIFT in combination with multiple shape features on the depth channel <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b13">[14]</ref>. However, following their success in many computer vision problems, unsupervised feature learning methods have recently been extended to RGB-D recognition settings. Blum et al. <ref type="bibr" target="#b2">[3]</ref> proposed an RGB-D descriptor that relies on a K-Means based feature learning approach. More recently Bo et al. <ref type="bibr" target="#b4">[5]</ref> proposed hierarchical matching pursuit (HMP), a hierarchical sparsecoding method that can learn features from multiple channel input. A different approach pursued by Socher et al. <ref type="bibr" target="#b21">[22]</ref> relies on combining convolutional filters with a recursive neural network (a specialized form of recurrent neural network) as the recognition architecture. Asif et al. <ref type="bibr" target="#b0">[1]</ref> report improved recognition performance using a cascade of Random Forest classifiers that are fused in a hierarchical manner. Finally, in recent independent work Schwarz et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use features extracted from CNNs pre-trained on ImageNet for RGB-D object recognition. While they also make use of a two-stream network they do not fine-tune the CNN for RGB-D recognition, but rather just use the pre-trained network as is. Interestingly, they also discovered that simple colorization methods for depth are competitive to more involved preprocessing techniques. In contrast to their work, ours achieves higher accuracy by training our fusion CNN end-to-end: mapping from raw pixels to object classes in a supervised manner (with pre-training on a related recognition task). The features learned in our CNN are therefore by construction discriminative for the task at hand. Using CNNs trained for object recognition has a long history in computer vision and machine learning. While they have been known to yield good results on supervised image classification tasks such as MNIST for a long time <ref type="bibr" target="#b16">[17]</ref>, recently they were not only shown to outperform classical methods in large scale image classification tasks <ref type="bibr" target="#b12">[13]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref> and semantic segmentation <ref type="bibr" target="#b7">[8]</ref> but also to produce features that transfer between tasks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b1">[2]</ref>. This recent success story has been made possible through optimized implementations for high-performance computing systems, as well as the availability of large amounts of labeled image data through, e.g., the ImageNet dataset <ref type="bibr" target="#b18">[19]</ref>.</p><p>While the majority of work in deep learning has focused on 2D images, recent research has also been directed towards using depth information for improving scene labeling and object detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Among them, the work most similar to ours is the one on object detection by Gupta et al. <ref type="bibr" target="#b9">[10]</ref> who introduces a generalized method of the R-CNN detector <ref type="bibr" target="#b8">[9]</ref> that can be applied to depth data. Specifically, they use large CNNs already trained on RGB images to also extract features from depth data, encoding depth information into three channels (HHA encoding). Specifically, they encode for each pixel the height above ground, the horizontal disparity and the pixelwise angle between a surface normal and the gravity direction. Our fusion network architecture shares similarities with their work in the usage of pre-trained networks on RGB images. Our method differs in both the encoding of depth into color image data and in the fusion approach taken to combine information from both modalities. For the encoding step, we propose an encoding method for depth images ('colorizing' depth) that does not rely on complicated preprocessing and results in improved performance when compared to the HHA encoding. To accomplish sensor fusion we introduce additional layers to our CNN pipeline (see Fig. <ref type="figure" target="#fig_0">1</ref>) allowing us to automatically learn a fusion strategy for the recognition task -in contrast to simply training a linear classifier on top of features extracted from both modalities. Multi-stream architectures have also been used for tasks such as action recognition <ref type="bibr" target="#b20">[21]</ref>, detection <ref type="bibr" target="#b10">[11]</ref> and image retrieval <ref type="bibr" target="#b22">[23]</ref>. An interesting recent overview of different network architectures for fusing depth and image information is given in Saxena et al. <ref type="bibr" target="#b17">[18]</ref>. There, the authors compared different models for multimodal learning: (1) early fusion, in which the input image is concatenated to the existing image RGB channels and processed alongside; (2) an approach we denote as late fusion, where features are trained separately for each modality and then merged at higher layers; (3) combining early and late fusion; concluding that late fusion (2) and the combined approach perform best for the problem of grasp detection. Compared to their work, our model is similar to the late fusion approach but widely differs in training -Saxena et al. <ref type="bibr" target="#b17">[18]</ref> use a layer-wise unsupervised training approachand scale (the size of both their networks and input images is an order of magnitude smaller than in our settings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTIMODAL ARCHITECTURE FOR RGB-D OBJECT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECOGNITION</head><p>An overview of the architecture is given in Fig. <ref type="figure" target="#fig_0">1</ref>. Our network consists of two streams (top-blue and bottom-green part in the figure) -processing RGB and depth data independently -which are combined in a late fusion approach. Each stream consists of a deep CNN that has been pretrained for object classification on the ImageNet database (we use the CaffeNet <ref type="bibr" target="#b11">[12]</ref> implementation of the CNN from Krizhevsky et al. <ref type="bibr" target="#b12">[13]</ref>). The key reason behind starting from Fig. <ref type="figure">2</ref>: Different approaches for color encoding of depth images. From left to right: RGB, depth-gray, surface normals <ref type="bibr" target="#b4">[5]</ref>, HHA <ref type="bibr" target="#b9">[10]</ref>, our method. Fig. <ref type="figure">3</ref>: CNNs require a fixed size input. Instead of the widely used image warping approach (middle), our method (bottom) preserves shape information and ratio of the objects. We rescale the longer side and create additional image context, by tiling the pixels at the border of the longer side, e.g., 1.</p><p>We assume that the depth image is already transformed to three channels using our colorization method.</p><p>a pre-trained network is to enable training a large CNN with millions of parameters using the limited training data available from the Washington RGB-D Object dataset (see, e.g., Yosinski et al. <ref type="bibr" target="#b24">[25]</ref> for a recent discussion). We first pre-process data from both modalities to fully leverage the ImageNet pre-training. Then, we train our multimodal CNN in a stage-wise manner. We fine-tune the parameters of each individual stream network for classification of the target data and proceed with the final training stage in which we jointly train the parameters of the fusion network. The different steps will be outlined in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input preprocessing</head><p>To fully leverage the power of CNNs pre-trained on ImageNet, we pre-process the RGB and depth input data such that it is compatible with the kind of original ImageNet input. Specifically, we use the reference implementation of the CaffeNet <ref type="bibr" target="#b11">[12]</ref> that expects 227 × 227 pixel RGB images as input which are typically randomly cropped from larger 256 × 256 RGB images (see implementation details on data augmentation). The first processing step consists of scaling the images to the appropriate image size. The simplest approach to achieve this is to use image warping by directly rescaling the original image to the required image dimensions, disregarding the original object ratio. This is depicted in Fig. <ref type="figure">3</ref> (middle). We found in our experiments that this process is detrimental to object recognition performance -an effect that we attribute to a loss of shape information (see also Section IV-C). We therefore devise a different preprocessing approach: we scale the longest side of the original image to 256 pixels, resulting in a 256 × N or an N × 256 sized image. We then tile the borders of the longest side along the axis of the shorter side. The resulting RGB or depth image shows an artificial context around the object borders (see Fig. <ref type="figure">3</ref>). The same scaling operation is applied to both RGB and depth images.</p><p>While the RGB images can be directly used as inputs for the CNNs after this processing step, the rescaled depth data requires additional steps. To realize this, recall that a network trained on ImageNet has been trained to recognize objects in images that follow a specific input distribution (that of natural camera images) that is incompatible with data coming from a depth sensor -which essentially encodes distance of objects from the sensor. Nonetheless, by looking at a typical depth image from a household object scene (c.f., Fig. <ref type="figure">4</ref>) one can conclude that many features that qualitatively appear in RGB images -such as edges, corners, shaded regions -are also visible in, e.g., a grayscale rendering of depth data. This realization has previously led to the idea of simply using a rendered version of the recorded depth data as an input for CNNs trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. We compare different such encoding strategies for rendering depth to images in our experiments. The two most prevalent such encodings are (1) rendering of depth data into grayscale and replicating the grayscale values to the three channels required as network input; (2) using surface normals where each dimension of a normal vector corresponds to one channel in the resulting image. A more involved method, called HHA encoding <ref type="bibr" target="#b9">[10]</ref>, encodes in the three channels the height above ground, horizontal disparity and the pixelwise angle between a surface normal and the gravity direction.</p><p>We propose a fourth, effective and computationally inexpensive, encoding of depth to color images, which we found to outperform the HHA encoding for object recognition. Our method first normalizes all depth values to lie between 0 and 255. Then, we apply a jet colormap on the given image that transforms the input from a single to a three channel image (colorizing the depth). For each pixel (i, j) in the depth image d of size W × H, we map the distance to color values ranging from red (near) over green to blue (far), essen-tially distributing the depth information over all three RGB channels. Edges in these three channels often correspond to interesting object boundaries. Since the network is designed for RGB images, the colorization procedure provides enough common structure between a depth and an RGB image to learn suitable feature representations (see Fig. <ref type="figure">2</ref> for a comparison between different depth preprocessing methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network training</head><p>Let D = {(x 1 , d 1 , y 1 ), . . . , (x N , d N , y N )} be the labeled data available for training our multimodal CNN; with x i , d i denoting the RGB and pre-processed depth image respectively and y i corresponding to the image label in one-hot encoding -i.e., y i ∈ R M is a vector of dimensionality M (the number of labels) with y i k = 1 for the position k denoting the image label. We train our model using a three-stage approach, first training the two stream networks individually followed by a joint fine-tuning stage.</p><p>1) Training the stream networks: We first proceed by training the two individual stream networks (c.f., the blue and green streams in Fig. <ref type="figure" target="#fig_0">1</ref>). Let g I (x i ; θ I ) be the representation extracted from the last fully connected layer (fc7) of the Caf-feNet -with parameters θ I -when applied to an RGB image x i . Analogously, let g D (d i ; θ D ) be the representation for the depth image. We will assume that all parameters θ I and θ D (the network weights and biases) are initialized by copying the parameters of a CaffeNet trained on the ImageNet dataset. We can then train an individual stream network by placing a randomly initialized softmax classification layer on top of f D and f I and minimizing the negative log likelihood L of the training data. That is, for the depth image stream network we solve min</p><formula xml:id="formula_0">W D ,θ D N i=1 L softmax W D g D (d i ; θ D ) , y i , (1)</formula><p>where W D are the weights of the softmax layer mapping from g(•) to R M , the softmax function is given by softmax(z) = exp(z)/ z 1 and the loss is computed as</p><formula xml:id="formula_1">L(s, y) = − k y k log s k .</formula><p>Training the RGB stream network then can be performed by an analogous optimization. After training, the resulting networks can be used to perform separate classification of each modality.</p><p>2) Training the fusion network: Once the two individual stream networks are trained we discard their softmax weights, concatenate their -now fine-tuned -last layer responses g I (x i ; θ I ) and g D (d i ; θ D ) and feed them through an additional fusion stream f ([g I (x i ; θ I ), g D (d i ; θ D )]; θ F ) with parameters θ F . This fusion network again ends in a softmax classification layer. The complete setup is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, where the two fc7 layers (blue and green) are concatenated and merge into the fusion network (here the inner product layer fc1-fus depicted in gray). Analogous to Eq. ( <ref type="formula">1</ref>) the fusion network can therefore be trained by jointly optimizing all parameters to minimize the negative </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robust classification from depth images</head><p>Finally, we are interested in using our approach in real world robotics scenarios. Robots are supposed to perform object recognition in cluttered scenes where the perceived sensor data is subject to changing external conditions (such as lighting) and sensor noise. Depth sensors are especially affected by a non-negligible amount of noise in such setups. This is mainly due to the fact that reflective properties of materials as well as their coating, often result in missing depth information. An example of noisy depth data is depicted in Fig. <ref type="figure">4</ref>. In contrast to the relatively clean training data from the Washington RGB-D Object dataset, the depicted scene contains considerable amounts of missing depth values and partial occlusions (the black pixels in the figure). To achieve robustness against such unpredictable factors, we propose a new data augmentation scheme that generates new, noised training examples for training and is tailored specifically to robust classification from depth data.</p><p>Our approach utilizes the observation that noise in depth data often shows a characteristic pattern and appears at object boundaries or object surfaces. Concretely, we sampled a representative set of noise patterns P = {P 1 , . . . , P K } that occur when recording typical indoor scenes through a Kinect sensor. For sampling the noise patterns we used the RGB-D SLAM dataset <ref type="bibr" target="#b23">[24]</ref>. First, we extract 33,000 random noise patches of size 256 × 256 from different sequences at varying positions and divide them into five groups, based on the number of missing depth readings they contain. Those noise patches are 2D binary masks patterns. We randomly sample pairs of noise patches from two different groups that are randomly added or subtracted and optionally inverted to produce a final noise mask pattern. We repeat this process until we have collected K = 50, 000 noise patterns in total. Examples of the resulting noise patterns and their application to training examples are shown in Fig. <ref type="figure">5</ref>.</p><p>Training the depth network with artificial noise patterns then proceeds by minimizing the objective from Equation Eq. ( <ref type="formula">1</ref>) in which each depth sample d i is randomly replaced with a noised variant with probability 50%. Formally,</p><formula xml:id="formula_2">d i = d i if p = 1 P k • d i else with p ∼ B{0.5} k ∼ U{1, K},<label>(3)</label></formula><p>where • denotes the Hadamard product, B the Bernoulli distribution and U the discrete uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our multimodal network architecture on the Washington RGB-D Object Dataset <ref type="bibr" target="#b14">[15]</ref> which consists of household objects belonging to 51 different classes. As an additional experiment -to evaluate the robustness of our approach for classification in real-world environments -we considered classification of objects from the RGB-D Scenes dataset whose class distribution partially overlaps with the RGB-D Object Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>All experiments were performed using the publicly available Caffe framework <ref type="bibr" target="#b11">[12]</ref>. As described previously we use the CaffeNet as the basis for our fusion network. It consists of five convolutional layers (with max-pooling after the first, second and fifth convolution layer) followed by two fully connected layers and a softmax classification layer. Rectified linear units are used in all but the final classification layer. We initialized both stream networks with the weights and biases of the first eight layers from this pre-trained network, discarding the softmax layer. We then proceeded with our stage-wise training. In the first stage (training the RGB and depth streams independently) the parameters of all layers were adapted using a fixed learning rate schedule (with initial learning rate of 0.01 that is reduced to 0.001 after 20K iterations and training is stopped after 30K iterations). In the second stage (training the fusion network, 20k iterations, mini-batch size of 50) we experimented with fine-tuning all weights but found that fixing the individual stream networks (by setting their learning rate to zero) and only training the fusion part of the network resulted in the best performance. The number of training iterations were chosen based on the validation performance on a training validation split in TABLE I: Comparisons of our fusion network with other approaches reported for the RGB-D dataset. Results are recognition accuracy in percent. Our multi-modal CNN outperforms all the previous approaches.</p><p>Method RGB Depth RGB-D Nonlinear SVM <ref type="bibr" target="#b14">[15]</ref> 74.5 ± 3.1 64.7 ± 2.2 83.9 ± 3.5 HKDES <ref type="bibr" target="#b3">[4]</ref> 76.1 ± 2.2 75.7 ± 2.6 84.1 ± 2.2 Kernel Desc. <ref type="bibr" target="#b13">[14]</ref> 77.7 ± 1.9 78.8 ± 2.7 86.2 ± 2.1 CKM Desc. <ref type="bibr" target="#b2">[3]</ref> N/A N/A 86.4 ± 2.3 CNN-RNN <ref type="bibr" target="#b21">[22]</ref> 80.8 ± 4.2 78.9 ± 3.8 86.8 ± 3.3 Upgraded HMP <ref type="bibr" target="#b4">[5]</ref> 82.4 ± 3.1 81.2 ± 2.3 87.5 ± 2.9 CaRFs <ref type="bibr" target="#b0">[1]</ref> N/A N/A 88.1 ± 2.4 CNN Features <ref type="bibr" target="#b19">[20]</ref> 83. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D Object dataset</head><p>The Washington RGB-D Object Dataset consists of 41,877 RGB-D images containing household objects organized into 51 different classes and a total of 300 instances of these classes which are captured under three different viewpoint angles. For the evaluation every 5th frame is subsampled. We evaluate our method on the challenging category recognition task, using the same ten cross-validation splits as in Lai et al. <ref type="bibr" target="#b14">[15]</ref>. Each split consists of roughly 35,000 training images and 7,000 images for testing. From each object class one instance is left out for testing and training is performed on the remaining 300−51 = 249 instances. At test time the task of the CNN is to assign the correct class label to a previously unseen object instance.</p><p>Table <ref type="table">I</ref> shows the average accuracy of our multi-modal CNN in comparison to the best results reported in the literature. Our best multi-modal CNN, using the jet-colorization, (Fus-CNN jet) yields an overall accuracy of 91.3 ± 1.4% when using RGB and depth (84.1 ± 2.7% and 83.8 ± 2.7% when only the RGB or depth modality is used respectively), which -to the best of our knowledge -is the highest accuracy reported for this dataset to date. We also report results for combining the more computationally intensive HHA with our network (Fus-CNN HHA). As can be seen in the table, this did not result in an increased performance. The depth colorization method slightly outperforms the HHA fusion network (Fus-CNN HHA) while being computationally cheaper. Overall our experiments show that a pretrained CNN can be adapted for recognition from depth data using our depth colorization method. Apart from the results reported in the table, we also experimented with different fusion architectures. Specifically, performance slightly drops to 91% when the intermediate fusion layer (fc1-fus) is removed from the network. Adding additional fusion layers  The worst class recall belongs to mushrooms and peaches. also did not yield an improvement. Finally, Fig. <ref type="figure" target="#fig_3">6</ref> shows the per-class recall, where roughly half of the objects achieve a recall of ≈ 99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth domain adaptation for RGB-D Scenes</head><p>To test the effectiveness of our depth augmentation technique in real world scenes, we performed additional recognition experiments on the more challenging RGB-D Scenes dataset. This dataset consists of six object classes (which overlap with the RGB-D Object Dataset) and a large amount of depth images subjected to noise.</p><p>For this experiment we trained two single-stream depthonly networks using the Object dataset and used the Scenes dataset for testing. Further, we assume that the groundtruth bounding box is given in order to report only on recognition performance. The first "baseline" network is trained by following the procedure described in Section III-B.1, with the total number of labels M = 6. The second network is trained by making use of the depth augmentation outlined in III-C. The results of this experiment are shown in Table <ref type="table" target="#tab_2">II</ref> (middle and right column) that reports the recognition accuracy for each object class averaged over all eight video sequences. As is evident from the table, the adapted network (right column) trained with data augmentation outperforms the baseline model for all classes, clearly indicating that additional domain adaptation is necessary for robust recognition in real world scenes. However, some classes (e.g., cap, bowl, soda can) benefit more from noise aware training than others (e.g., flashlight, coffe mug). The kitchen scene depicted in Fig. <ref type="figure">4</ref> gives a visual intuition for this result. On the one hand, some objects (e.g., soda cans) often present very noisy object boundaries and surfaces, thus they show improved recognition performance using the adapted approach. On the other hand, small objects (e.g. a flashlight), which are often captured lying on a table, are either less noisy or just small, hence susceptible to be completely erased by the noise from our data augmentation approach. Fig. <ref type="figure" target="#fig_4">7</ref> shows several exemplary noisy depth images from the test set that are correctly classified by the domain-adapted network while the baseline network labels them incorrectly. We also tested the effect of different input image rescaling techniquespreviously described in Fig. <ref type="figure">3</ref> -in this setting. As shown in the left column of Table <ref type="table" target="#tab_2">II</ref>, standard image warping performs poorly, which supports our intuition that shape information gets lost during preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison of depth encoding methods</head><p>Finally, we conducted experiments to compare the different depth encoding methods described in Fig. <ref type="figure">2</ref>. For rescaling the images, we use our proposed preprocessing method described in Fig. <ref type="figure">3</ref> and tested the different depth encoding. Two scenarios are considered: 1) training from scratch using single channel depth images 2) for each encoding method, only fine-tuning the network by using the procedure described in Section III-B.1. When training from scratch, the initial learning rate is set to 0.01, then changed to 0.001 after 40K iterations thus stopped after 60K iterations. Training with more iterations did not further improve the accuracy. From the results, presented in Table <ref type="table" target="#tab_3">III</ref>, it is clear that training the network from scratch -solely on the RGB-D Dataset -is inferior to fine-tuning. In the latter setting, the results suggest that the simplest encoding method (depth-gray) performs considerably worse than the other three methods. Among these other encodings (which all produce colorized images), surface normals and HHA encoding require additional image preprocessing -meanwhile colorizing depth using our depthjet encoding has negligible computational overhead. One potential reason why the HHA encoding underperforms in this setup is that all objects are captured on a turntable with the same height above the ground. The height channel used in the HHA encoding therefore does not encode any additional information for solving the classification task. In this experiment, using surface normals yields slightly better performance than the depth-jet encoding. Therefore, we tested the fusion architecture on the ten splits of the RGB-D Object Dataset using the surface normals encoding but this did not further improve the performance. Specifically, the recognition accuracy on the test-set was 91.1 ± 1.6 which is comparable to our reported results in Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We introduce a novel multimodal neural network architecture for RGB-D object recognition, which achieves state of the art performance on the RGB-D Object dataset <ref type="bibr" target="#b14">[15]</ref>. Our method consists of a two-stream convolutional neural network that can learn to fuse information from both RGB and depth automatically before classification. We make use of an effective encoding method from depth to image data that allows us to leverage large CNNs trained for object recognition on the ImageNet dataset. We present a novel  depth data augmentation that aims at improving recognition in noisy real-world setups, situations typical of many robotics scenarios. We present extensive experimental results and confirm that our method is accurate and it is able to learn rich features from both domains. We also show robust object recognition in real-world environments and prove that noiseaware training is effective and improves recognition accuracy on the RGB-D Scenes dataset <ref type="bibr" target="#b15">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Two-stream convolutional neural network for RGB-D object recognition. The input of the network is an RGB and depth image pair of size 227 × 227 × 3. Each stream (blue, green) consists of five convolutional layers and two fully connected layers. Both streams converge in one fully connected layer and a softmax classifier (gray).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Kitchen scene in the RGB-D Scenes dataset showing objects subjected to noise and occlusions.</figDesc><graphic url="image-17.png" coords="4,325.44,50.08,220.32,148.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Per-class recall of our trained model on all test-splits. The worst class recall belongs to mushrooms and peaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Objects from the RGB-D Scenes test-set for which the domain adapted CNN predicts the correct label, while the baseline (no adapt.) CNN fails. Most of these examples are subject to noise or partial occlusion.</figDesc><graphic url="image-38.png" coords="7,239.10,173.45,61.74,61.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison of the domain adapted depth network with the baseline: six-class recognition results (in percent) on the RGB-D Scenes dataset<ref type="bibr" target="#b15">[16]</ref> that contains everyday objects in real-world environments.</figDesc><table><row><cell>Class</cell><cell>Ours, warp.</cell><cell>Ours, no adapt.</cell><cell>Ours, adapt.</cell></row><row><cell>flashlight</cell><cell>93.4</cell><cell>97.5</cell><cell>96.4</cell></row><row><cell>cap</cell><cell>62.1</cell><cell>68.5</cell><cell>77.4</cell></row><row><cell>bowl</cell><cell>57.4</cell><cell>66.5</cell><cell>69.8</cell></row><row><cell>soda can</cell><cell>64.5</cell><cell>66.6</cell><cell>71.8</cell></row><row><cell>cereal box</cell><cell>98.3</cell><cell>96.2</cell><cell>97.6</cell></row><row><cell>coffee mug</cell><cell>61.9</cell><cell>79.1</cell><cell>79.8</cell></row><row><cell>class avg.</cell><cell>73.6 ± 17.9</cell><cell>79.1 ± 14.5</cell><cell>82.1 ± 12.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Comparison of different depth encoding methods on the ten test-splits of the RGB-D Object dataset.</figDesc><table><row><cell>Depth Encoding</cell><cell>Accuracy</cell></row><row><cell>Depth-gray (single channel), from scratch</cell><cell>80.1 ± 2.6</cell></row><row><cell>Depth-gray</cell><cell>82.0 ± 2.8</cell></row><row><cell>Surface normals</cell><cell>84.7 ± 2.3</cell></row><row><cell>HHA</cell><cell>83.0 ± 2.7</cell></row><row><cell>Depth-jet encoding</cell><cell>83.8 ± 2.7</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient rgb-d object categorization using cascaded ensembles of randomized decision trees</title>
		<author>
			<persName><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
				<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>arxiv:1406.5774</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A learned feature descriptor for object recognition in rgb-d data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wuelfing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
				<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for rgb-d based object recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Symposium on Experimental Robotics (ISER)</title>
				<meeting>of the Int. Symposium on Experimental Robotics (ISER)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth kernel descriptors for object recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
				<meeting>of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multiview rgb-d object dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
				<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection-based object labeling in 3d scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
				<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
				<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Robotics: Science and Systems (RSS)</title>
				<meeting>of Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RGB-D object recognition and pose estimation based on pre-trained convolutional neural network features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
				<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
				<meeting>of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
