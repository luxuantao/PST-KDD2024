<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mortal Multi-Armed Bandits</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
							<email>ravikumar@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
							<email>filiprad@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Upfal</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Yahoo! Research Sunnyvale</orgName>
								<address>
									<postCode>94089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research Sunnyvale</orgName>
								<address>
									<postCode>94089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mortal Multi-Armed Bandits</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE3B2165D8419FDC62BEB6AA456A0C29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate and study a new variant of the k-armed bandit problem, motivated by e-commerce applications. In our model, arms have (stochastic) lifetime after which they expire. In this setting an algorithm needs to continuously explore new arms, in contrast to the standard k-armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with nearcertainty. The main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budgets. An algorithm needs to choose among a large collection of ads, more than can be fully explored within the typical ad lifetime. We present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case. Empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online advertisements (ads) are a rapidly growing source of income for many Internet content providers. The content providers and the ad brokers who match ads to content are paid only when ads are clicked; this is commonly referred to as the pay-per-click model. In this setting, the goal of the ad brokers is to select ads to display from a large corpus, so as to generate the most ad clicks and revenue. The selection problem involves a natural exploration vs. exploitation tradeoff: balancing exploration for ads with better click rates against exploitation of the best ads found so far.</p><p>Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, we model the ad selection task as a multi-armed bandit problem <ref type="bibr" target="#b4">[5]</ref>. A multiarmed bandit models a casino with k slot machines (one-armed bandits), where each machine (arm) has a different and unknown expected payoff. The goal is to sequentially select the optimal sequence of slot machines to play (i.e., slot machine arms to pull) to maximize the expected total reward. Considering each ad as a slot machine, that may or may not provide a reward when presented to users, allows any multi-armed bandit strategy to be used for the ad selection problem.</p><p>A standard assumption in the multi-armed bandit setting, however, is that each arm exists perpetually. Although the payoff function of an arm is allowed to evolve over time, the evolution is assumed to be slow. Ads, on the other hand, are regularly created while others are removed from circulation. This occurs as advertisers' budgets run out, when advertising campaigns change, when holiday shopping seasons end, and due to other factors beyond the control of the ad selection system. The advertising problem is even more challenging as the set of available ads is often huge (in the tens of millions), while standard multi-armed bandit strategies converge only slowly and require time linear in the number of available options.</p><p>In this paper we initiate the study of a rapidly changing variant of the multi-armed bandit problem. We call it the mortal multi-armed bandit problem since ads (or equivalently, available bandit arms) are assumed to be born and die regularly. In particular, we will show that while the standard multiarmed bandit setting allows for algorithms that only deviate from the optimal total payoff by O(ln t) <ref type="bibr" target="#b20">[21]</ref>, in the mortal arm setting a regret of Ω(t) is possible.</p><p>Our analysis of the mortal multi-arm bandit problem considers two settings. First, in the less realistic but simpler state-aware (deterministic reward) case, pulling arm i always provides a reward that equals the expected payoff of the arm. Second, in the more realistic state-oblivious (stochastic reward) case, the reward from arm i is a binomial random variable indicating the true payoff of the arm only in expectation. We provide an optimal algorithm for the state-aware case. This algorithm is based on characterizing the precise payoff threshold below which repeated arm pulls become suboptimal. This characterization also shows that there are cases when a linear regret is inevitable. We then extend the algorithm to the state-oblivious case, and show that it is near-optimal. Following this, we provide a general heuristic recipe for modifying standard multi-armed bandit algorithms to be more suitable in the mortal-arm setting. We validate the efficacy of our algorithms on various payoff distributions including one empirically derived from real ads. In all cases, we show that the algorithms presented significantly outperform standard multi-armed bandit approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling mortality</head><p>Suppose we wish to select the ads to display on a webpage. Every time a user visits this webpage, we may choose one ad to display. Each ad has a different potential to provide revenue, and we wish to sequentially select the ads to maximize the total expected revenue. Formally, say that at time t, we have ads A(t) = {ad 1t , . . . , ad kt } from which we must pick one to show. Each ad it has a payoff µ it ∈ [0, 1] that is drawn from some known cumulative distribution F (µ)<ref type="foot" target="#foot_0">1</ref> . Presenting ad it at time t provides a (financial) reward R(µ it ); the reward function R(•) will be specified below.</p><p>If the pool of available ads A(t) were static, or if the payoffs were only slowly changing with t, this problem could be solved using any standard multi-armed bandit approach. As described earlier, in reality the available ads are rapidly changing. We propose the following simple model for this change: at the end of each time step t, one or more ads may die and be replaced with new ads. The process then continues with time t + 1. Note that since change happens only through replacement of ads, the number of ads k = |A(t)| remains fixed. Also, as long as an ad is alive, we assume that its payoff is fixed.</p><p>Death can be modeled in two ways, and we will address both in this work. An ad i may have a budget L i that is known a priori and revealed to the algorithm. The ad dies immediately after it has been selected L i times; we assume that L i values are drawn from a geometric distribution, with an expected budget of L. We refer to this case as budgeted death. Alternatively, each ad may die with a fixed probability p after every time step, whether it was selected or not. This is equivalent to each ad being allocated a lifetime budget L i , drawn from a geometric distribution with parameter p, that is fixed when the arm is born but is never revealed to the algorithm; in this case new arms have an expected lifetime of L = 1/p. We call this timed death. In both death settings, we assume in our theoretical analysis that at any time there is always at least one previously unexplored ad available. This reflects reality where the number of ads is practically unlimited.</p><p>Finally, we model the reward function in two ways, the first being simpler to analyze and the latter more realistic. In the state-aware (deterministic reward) case, we assume R(µ it ) = µ it . This provides us with complete information about each ad immediately after it is chosen to be displayed. In the state-oblivious (stochastic reward) case, we take R(µ it ) to be a random variable that is 1 with probability µ it and 0 otherwise.</p><p>The mortal multi-armed bandit setting requires different performance measures than the ones used with static multi-armed bandits. In the static setting, very little exploration is needed once an optimal arm is identified with near-certainty; therefore the quality measure is the total regret over time. In our setting the algorithm needs to continuously explore newly available arms. We therefore study the long term, steady-state, mean regret per time step of various solutions. We define this regret as the expected payoff of the best currently alive arm minus the payoff actually obtained by the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Our work is most related to the study of dynamic versions of the multi-arm bandit (MAB) paradigm where either the set of arms or their expected reward may change over time. Motivated by task scheduling, Gittins <ref type="bibr" target="#b9">[10]</ref> proposed a policy where only the state of the active arm (the arm currently being played) can change in a given step, and proved its optimality for the Bayesian formulation with time discounting. This seminal result gave rise to a rich line of work, a proper review of which is beyond the scope of this paper. In particular, Whittle <ref type="bibr" target="#b22">[23]</ref> introduced an extension termed restless bandits <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>, where the states of all arms can change in each step according to a known (but arbitrary) stochastic transition function. Restless bandits have been shown to be intractable: e.g., even with deterministic transitions the problem of computing an (approximately) optimal strategy is PSPACE-hard <ref type="bibr" target="#b17">[18]</ref>. Sleeping bandits problem, where the set of strategies is fixed but only a subset of them available in each step, were studied in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> and recently, using a different evaluation criteria, in <ref type="bibr" target="#b12">[13]</ref>. Strategies with expected rewards that change gradually over time were studied in <ref type="bibr" target="#b18">[19]</ref>. The mixture-of-experts paradigm is related <ref type="bibr" target="#b10">[11]</ref>, but assumes that data tuples are provided to each expert, instead of the tuples being picked by the algorithm, as in the bandit setting.</p><p>Auer et al. <ref type="bibr" target="#b2">[3]</ref> adopted an adversarial approach: they defined the adversarial MAB problem where the reward distributions are allowed to change arbitrarily over time, and the goal is to approach the performance of the best time-invariant policy. This formulation has been further studied in several other papers. Auer et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> also considered a more general definition of regret, where the comparison is to the best policy that can change arms a limited number of times. Due to the overwhelming strength of the adversary, the guarantees obtained in this line of work are relatively weak when applied to the setting that we consider in this paper.</p><p>Another aspect of our model is that unexplored arms are always available. Related work broadly comes in three flavors. First, new arms can become available over time; the optimality of Gittins' index was shown to extend to this case <ref type="bibr" target="#b21">[22]</ref>. The second case is that of infinite-armed bandits with discrete arms, first studied by <ref type="bibr" target="#b3">[4]</ref> and recently extended to the case of unknown payoff distributions and an unknown time horizon <ref type="bibr" target="#b19">[20]</ref>. Finally, the bandit arms may be indexed by numbers from the real line, implying uncountably infinite bandit arms, but where "nearby" arms (in terms of distance along the real line) have similar payoffs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. However, none of these approaches allows for arms to appear then disappear, which as we show later critically affects any regret bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Upper bound on mortal reward</head><p>In this section we show that in the mortal multi-armed bandit setting, the regret per time step of any algorithm can never go to zero, unlike in the standard MAB setting. Specifically, we develop an upper bound on the mean reward per step of any such algorithm for the state-aware, budgeted death case. We then use reductions between the different models to show that this bound holds for the state-oblivious, timed death cases as well.</p><p>We prove the bound assuming we always have new arms available. The expected reward of an arm is drawn from a cumulative distribution F (µ) with support in [0, 1]. For X ∼ F (µ), let E[X] be the expectation of X over F (µ). We assume that the lifetime of an arm has an exponential distribution with parameter p, and denote its expectation by L = 1/p. The following function captures the tradeoff between exploration and exploitation in our setting and plays a major role in our analysis:</p><formula xml:id="formula_0">Γ(µ) = E[X] + (1 -F (µ))(L -1)E[X|X ≥ µ] 1 + (1 -F (µ))(L -1) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Theorem 1. Let μ(t) denote the maximum mean reward that any algorithm for the state-aware mortal multi-armed bandit problem can obtain in t steps in the budgeted death case. Then lim t→∞ μ(t) ≤ max µ Γ(µ).</p><p>Proof sketch. We distinguish between fresh arm pulls, i.e., pulls of arms that were not pulled before, and repeat arm pulls. Assume that the optimal algorithm pulls τ (t) distinct (fresh) arms in t steps, and hence makes t -τ (t) repeat pulls. The expected number of repeat pulls to an arm before it expires is (1 -p)/p. Thus, using Wald's equation <ref type="bibr" target="#b7">[8]</ref>, the expected number of different arms the algorithm must use for the repeat pulls is (t -τ (t)) • p/(1 -p). Let (t) ≤ τ (t) be the number of distinct arms that get pulled more than once. Using Chernoff bounds, we can show that for any δ &gt; 0, for sufficiently large t, with probability ≥ 1 -1/t 2 the algorithm uses at least (t) = p(t -τ (t))/(1 -p) • (1 -δ) different arms for the repeat pulls. Call this event E 1 (δ).</p><p>Next, we upper bound the expected reward of the best (t) arms found in τ (t) fresh probes. For any</p><formula xml:id="formula_2">h &gt; 0, µ(h) = F -1 (1 -( (t)/τ (t))(1 -h)).</formula><p>In other words, the probability of picking an arm with expected reward greater or equal to µ(h) is ( (t)/τ (t))(1 -h). Applying the Chernoff bound, for any δ, h &gt; 0 there exists t(δ, h) such that for all t ≥ t(δ, h) the probability that the algorithm finds at least (t) arms with expected reward at least µ(δ, h</p><formula xml:id="formula_3">) = µ(h)(1 -δ) is bounded by 1/t 2 . Call this event E 2 (δ, h).</formula><p>Let E(δ, h) be the event E 1 (δ) ∧ ¬E 2 (δ, h). The expected reward of the algorithm in this event after t steps is then bounded by</p><formula xml:id="formula_4">τ (t)E[X] + (t -τ (t))E[X | X ≥ µ(δ, h)] Pr(E(δ, h)) + (t -τ (t))(1 - Pr(E(δ, h)).</formula><p>As δ, h → 0, Pr(E(δ, h)) → 1, and the expected reward per step when the algorithm pulls τ (t) fresh arms is given by</p><formula xml:id="formula_5">lim sup t→∞ μ(t) ≤ 1 t τ (t)E[X] + (t -τ (t))E[X | X ≥ µ] ,</formula><p>where</p><formula xml:id="formula_6">µ = F -1 (1 -(t)/τ (t)) and (t) = (t -τ (t))p/(1 -p).</formula><p>After some calculations, we get lim sup t→∞ μ(t) ≤ max µ Γ(µ).</p><p>In Section 5.1 we present an algorithm that achieves this performance bound in the state-aware case.</p><p>The following two simple reductions establish the lower bound for the timed death and the stateoblivious models. Lemma 2. Assuming that new arms are always available, any algorithm for the timed death model obtains at least the same reward per timestep in the budgeted death model.</p><p>Although we omit the proof due to space constraints, the intuition behind this lemma is that an arm in the timed case can die no sooner than in the budgeted case (i.e., when it is always pulled). As a result, we get: Lemma 3. Let μdet (t) and μsto (t) denote the respective maximum mean expected rewards that any algorithm for the state-aware and state-oblivious mortal multi-armed bandit problems can obtain after running for t steps. Then μsto (t) ≤ μdet (t).</p><p>We now present two applications of the upper bound. The first simply observes that if the time to find an optimal arm is greater than the lifetime of such an arm, the the mean reward per step of any algorithm must be smaller than the best value. This is in contrast to the standard MAB problem with the same reward distribution, where the mean regret per step tends to 0. Corollary 4. Assume that the expected reward of a bandit arms is 1 with probability p &lt; 1/2 and 1 -δ otherwise, for some δ ∈ (0, 1]. Let the lifetime of arms have geometric distribution with the same parameter p. The mean reward per step of any algorithm for this supply of arms is at most 1 -δ + δp, while the maximum expected reward is 1, yielding an expected regret per step of Ω(1). Corollary 5. Assume arm payoffs are drawn from a uniform distribution,</p><formula xml:id="formula_7">F (x) = x, x ∈ [0, 1].</formula><p>Consider the timed death case with parameter p ∈ (0, 1). Then the mean reward per step in bounded by</p><formula xml:id="formula_8">1- √ p</formula><p>1-p and expected regret per step of any algorithm is Ω( √ p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bandit algorithms for mortal arms</head><p>In this section we present and analyze a number of algorithms specifically designed for the mortal multi-armed bandit task. We develop the optimal algorithm for the state-aware case and then modify the algorithm to the state-oblivious case, yielding near-optimal regret. We also study a subset approach that can be used in tandem with any standard multi-armed bandit algorithm to substantially improve performance in the mortal multi-armed bandit setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The state-aware case</head><p>We now show that the algorithm DETOPT is optimal for this deterministic reward setting. Assume the same setting as in the previous section, with a constant supply of new arms. The expected reward of an arm is drawn from cumulative distribution F (µ). Let X be a random variable with that distribution, and E[X] be its expectation over F (µ). Assume that the lifetime of an arm has an exponential distribution with parameter p, and denote its expectation by L = 1/p. Recall Γ(µ) from ( <ref type="formula" target="#formula_0">1</ref>) and let µ * = argmax µ Γ(µ). Now, Theorem 6. Let DETOPT(t) denote the mean per turn reward obtained by DETOPT after running for t steps with µ * = argmax µ Γ(µ), then lim t→∞ DETOPT(t) = max µ Γ(µ).</p><formula xml:id="formula_9">Algorithm DETOPT input: Distribution F (µ), expected lifetime L µ * ← argmax µ Γ(µ) [Γ is defined in (1)] while we keep playing i ← random new arm Pull arm i; R ← R(µi) = µi if R &gt; µ * [If</formula><p>Note that the analysis of the algorithm holds for both budgeted and timed death models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The state-oblivious case</head><p>We now present a modified version of DETOPT for the state-oblivious case. The intuition behind this modification, STOCHASTIC, is simple: instead of pulling an arm once to determine its payoff µ i , the algorithm pulls each arm n times and abandons it unless it looks promising. A variant, called STOCHASTIC WITH EARLY STOPPING, abandons the arm earlier if its maximum possible future reward will still not justify its retention. For n = O log L/ 2 , STOCHASTIC gets an expected reward per step of Γ(µ * -) and is thus near-optimal; the details are omitted due to space constraints. The subset heuristic. Why can't we simply use a standard multi-armed bandit (MAB) algorithm for mortal bandits as well? Intuitively, MAB algorithms invest a lot of pulls on all arms (at least logarithmic in the total number of pulls) to guarantee convergence to the optimal arm. This is necessary in the traditional bandit settings, but in the limit as t → ∞, the cost is recouped and leads to sublinear regret. However, such an investment is not justified for mortal bandits: the most gain we can get from an arm is L (if the arm has payoff 1), which reduces the importance of convergence to the best arm. In fact, as shown by Corollary 4, converging to a reasonably good arm suffices. However, standard MAB algorithms do identify better arms very well. This suggests the following epoch-based heuristic: (a) select a subset of k/c arms uniformly at random from the total k arms at the beginning of each epoch, (b) operate a standard bandit algorithm on these until the epoch ends, and repeat. Intuitively, step (a) reduces the load on the bandit algorithm, allowing it to explore less and converge faster, in return for finding an arm that is probably optimal only among the k/c subset. Picking the right c and the epoch length then depends on balancing the speed of convergence of the bandit algorithm, the arm lifetimes, and the difference between the k-th and the k/c-th order statistics of the arm payoff distribution; in our experiments, c is chosen empirically.</p><formula xml:id="formula_10">Algorithm STOCHASTIC input: Distribution F (µ), expected lifetime L µ * ← argmax µ Γ(µ) [Γ is defined in (1)</formula><p>Using the subset heuristic, we propose an extension of the UCB1 algorithm<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b1">[2]</ref>, called UCB1K/C, for the state-oblivious case. Note that this is just one example of the use of this heuristic; any standard bandit algorithm could have been used in place of UCB1 here. In the next section, UCB1K/C is shown to perform far better than UCB1 in the mortal arms setting.</p><p>The ADAPTIVEGREEDY heuristic. Empirically, simple greedy MAB algorithms have previously been shown to perform well due to fast convergence. Hence for the purpose of evaluation, we also compare to an adaptive greedy heuristic for mortal bandits. Note that the n -greedy algorithm <ref type="bibr" target="#b1">[2]</ref> does not apply directly to mortal bandits since the probability t of random exploration decays to zero for large t, which can leave the algorithm with no good choices should the best arm expire.</p><formula xml:id="formula_11">Algorithm UCB1K/C input: k-armed bandit, c while we keep playing S ← k/c random arms dead ← 0 A U CB1 (S) ← Initialize UCB1 over arms S repeat i ← arm selected by A U CB1 (S) Pull arm i, provide reward to A U CB1 (S)</formula><p>x ← total arms that died this turn Check for newly dead arms in S, remove any</p><formula xml:id="formula_12">dead ← dead + x until dead ≥ k/2 or |S| = 0 end while Algorithm ADAPTIVEGREEDY input: k-armed bandit, c Initialization: ∀i ∈ [1, k], ri, ni ← 0 while we keep playing m ← argmax i ri/ni [Find best arm so far] pm ← rm/nm With probability min(1, c • pm) j ← m Otherwise [Pull a random arm] j ← uniform(1, k) r ← R(j) rj ← rj + r [Update the observed rewards] nj ← nj + 1 end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical evaluation</head><p>In this section we evaluate the performance of UCB1K/C, STOCHASTIC, STOCHASTIC WITH EARLY STOPPING, and ADAPTIVEGREEDY in the mortal arm state-oblivious setting. We also compare these to the UCB1 algorithm <ref type="bibr" target="#b1">[2]</ref>, that does not consider arm mortality in its policy but is among the faster converging standard multi-armed bandit algorithms. We present the results of simulation studies using three different distributions of arm payoffs F (•).</p><p>Uniform distributed arm payoffs. Our performance analyses assume that the cumulative payoff distribution F (•) of new arms is known. A particularly simple one is the uniform distribution, µ it ∼ uniform(0, 1). Figure <ref type="figure" target="#fig_1">1(a)</ref> shows the performance of these algorithms as a function of the expected lifetime of each arm, using a timed death and state-oblivious model. The evaluation was performed over k = 1000 arms, with each curve showing the mean regret per turn obtained by each algorithm when averaged over ten runs. Each run was simulated for ten times the expected lifetime of the arms, and all parameters were empirically optimized for each algorithm and each lifetime. Repeating the evaluation with k = 100, 000 arms produces qualitatively very similar performance.</p><p>We first note the striking difference between UCB1 and UCB1K/C, with the latter performing far better. In particular, even with the longest lifetimes, each arm can be sampled in expectation at most 100 times. With such limited sampling, UCB1 spends almost all the time exploring and generates almost the same regret of 0.5 per turn as would an algorithm that pulls arms at random. In contrast, UCB1K/C is able to obtain a substantially lower regret by limiting the exploration to a subset of the arms. This demonstrates the usefulness of the K/C idea: by running the UCB1 algorithm on an appropriately sized subset of arms, the overall regret per turn is reduced drastically. In practice,  with k = 1000 arms, the best performance was obtained with K/C between 4 and 40, depending on the arm lifetime.</p><p>Second, we see that STOCHASTIC outperformed UCB1K/C with optimally chosen parameters. Moreover, STOCHASTIC WITH EARLY STOPPING performs as well as ADAPTIVEGREEDY, which matches the best performance we were able to obtain by any algorithm. This demonstrates that (a) the state-oblivious versions of the optimal deterministic algorithm is effective in general, and (b) the early stopping criterion allows arms with poor payoff to be quickly weeded out.</p><p>Beta distributed arm payoffs. While the strategies discussed perform well when arm payoffs are uniformly distributed, it is unlikely that in a real setting the payoffs would be so well distributed. In particular, if there are occasional arms with substantially higher payoffs, we could expect any algorithm that does not exhaustively search available arms may obtain very high regret per turn.</p><p>Figure <ref type="figure" target="#fig_1">1</ref>(b) shows the results when the arm payoff probabilities are drawn from the beta(1, 3) distribution. We chose this distribution as it has finite support yet tends to select small payoffs for most arms while selecting high payoffs occasionally. Once again, we see that STOCHASTIC WITH EARLY STOPPING and ADAPTIVEGREEDY perform best, with the relative ranking of all other algorithms the same as in the uniform case above. The absolute regret of the algorithms we have proposed is increased relative to that seen in Figure <ref type="figure" target="#fig_1">1</ref>(a), but still substantially better than that of the UCB1. In fact, the regret of the UCB1 has increased more under this distribution than any other algorithm.</p><p>Real-world arm payoffs. Considering the application that motivated this work, we now evaluate the performance of the four new algorithms when the arm payoffs come from the empirically observed distribution of clickthrough rates on real ads served by a large ad broker.</p><p>Figure <ref type="figure" target="#fig_2">2</ref>(a) shows a histogram of the payoff probabilities for a random sample of approximately 300 real ads belonging to a shopping-related category when presented on web pages classified as belonging to the same category. The probabilities have been linearly scaled such that all ads have payoff between 0 and 1. We see that the distribution is unimodal, and is fairly tightly concentrated.</p><p>By sampling arm payoffs from a smoothed version of this empirical distribution, we evaluated the performance of the algorithms presented earlier. Figure <ref type="figure" target="#fig_2">2(b)</ref> shows that the performance of all the algorithms is consistent with that seen for both the uniform and beta payoff distributions. In particular, while the mean regret per turn is somewhat higher than that seen for the uniform distribution, it is still lower than when payoffs are from the beta distribution. As before, STOCHASTIC WITH EARLY STOPPING and ADAPTIVEGREEDY perform best, indistinguishable from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have introduced a new formulation of the multi-armed bandit problem motivated by the real world problem of selecting ads to display on webpages. In this setting the set of strategies available to a multi-armed bandit algorithm changes rapidly over time. We provided a lower bound of linear regret under certain payoff distributions. Further, we presented a number of algorithms that perform substantially better in this setting than previous multi-armed bandit algorithms, including one that is optimal under the state-aware setting, and one that is near-optimal under the state-oblivious setting. Finally, we provided an extension that allows any previous multi-armed bandit algorithm to be used in the case of mortal arms. Simulations on multiple payoff distributions, including one derived from real-world ad serving application, demonstrate the efficacy of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the regret per turn obtained by five different algorithms assuming that new arm payoffs come from the (a) uniform distribution and (b) beta(1, 3) distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Distribution of real world ad payoffs, scaled linearly such that the maximum payoff is 1 and (b) Regret per turn under the real-world ad payoff distribution.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We limit our analysis to the case where F (µ) is stationary and known, as we are particularly interested in the long-term steady-state setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>UCB1 plays the arm j, previously pulled nj times, with highest mean historical payoff plus (2 ln n)/nj.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Most of this work was done while the author was at Yahoo! Research. † Part of this work was done while the author was visiting the Department of Information Engineering at the University of Padova, Italy, supported in part by the FP6 EC/IST Project 15964 AEOLUS. Work supported in part by NSF award DMI-0600384 and ONR Award N000140610607.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The nonstochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bandit problems with infinitely many arms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Shepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2103" to="2116" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bandit Problems: Sequential Allocation of Experiments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fristedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Restless bandits, linear programming relaxations, and a primal-dual index heuristic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nino-Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From external to internal regret</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th COLT</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="621" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Introduction to Probability Theory and Its Applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using and combining predictors that specialize</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>In 29th STOC</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A dynamic allocation index for the sequential design of experiments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Statistics</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</editor>
		<meeting><address><addrLine>North-Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="241" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tracking the best expert</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herbster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="151" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online Decision Problems with Large Strategy Sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regret bounds for sleeping experts and bandits</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st COLT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="425" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonmyopic active learning of Gaussian processes: An explorationexploitation approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Restless bandits, partial conservation laws and indexability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nino-Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="76" to="98" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bandits for taxonomies: A model-based approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="216" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-armed bandit problems with dependent arms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The complexity of optimal queueing network control</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="318" to="322" />
		</imprint>
	</monogr>
	<note>In 9th CCC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adapting to a changing environment: The Brownian restless bandits</title>
		<author>
			<persName><forename type="first">A</forename><surname>Slivkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st COLT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anytime many-armed bandits</title>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arm-acquiring bandits</title>
		<author>
			<persName><forename type="first">P</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="292" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Restless bandits: Activity allocation in a changing world</title>
		<author>
			<persName><forename type="first">P</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="287" to="298" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
