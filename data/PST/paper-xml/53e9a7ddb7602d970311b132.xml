<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Selective Sampling Approach to Active Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>hliu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287-8809</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
							<email>motoda@sanken.osaka-u.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Scientific &amp; Industrial Research</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<postCode>567-0047</postCode>
									<settlement>Ibaraki</settlement>
									<region>Osaka</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<email>leiyu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287-8809</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Selective Sampling Approach to Active Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4DA209D98B2AAB7B69F5E80A57ED8E03</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dimensionality Reduction</term>
					<term>Feature Selection and Ranking</term>
					<term>Sampling</term>
					<term>Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. Traditional feature selection methods resort to random sampling in dealing with data sets with a huge number of instances. In this paper, we introduce the concept of active feature selection, and investigate a selective sampling approach to active feature selection in a filter model setting. We present a formalism of selective sampling based on data variance, and apply it to a widely used feature selection algorithm Relief. Further, we show how it realizes active feature selection and reduces the required number of training instances to achieve time savings without performance deterioration. We design objective evaluation measures of performance, conduct extensive experiments using both synthetic and benchmark data sets, and observe consistent and significant improvement. We suggest some further work based on our study and experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inductive learning is one of the major approaches to automatic extraction of useful patterns (or knowledge) from data. Data becomes increasingly larger in both number of features and number of instances in many applications such as genome projects, text mining, customer relationship management, and market basket analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57]</ref>. This trend poses a severe challenge to inductive learning systems in terms of efficiency and effectiveness. Feature selection has proven to be an effective means when dealing with large dimensionality with many irrelevant features <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">57]</ref>. In particular, feature selection removes irrelevant features, increases efficiency of learning tasks, improves learning performance (e.g., predictive accuracy), and enhances comprehensibility of learned results <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. Although there exist numerous feature selection algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>, new challenging research issues arise for feature selection: from handling a huge number of instances, large dimensionality (e.g., thousands of features), to dealing with data without class labels. This work is concerned with the number of instances in the context of feature selection.</p><p>When the number of instances is large, how can one use a portion of data to achieve the original objective without performance deterioration? Sampling is a common approach to this problem and many sampling methods are available <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Random sampling is a method of selecting a sample of n out of N (n ≤ N ) units such that every one of the N n distinct samples has an equal chance of being drawn. It has proven to be an effective approach to dealing with large data sets for many machine learning tasks including classification, clustering, and association rule mining <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52]</ref>. However, random sampling is blind because it does not exploit any data characteristic. In this work, we explore if sampling can use data characteristics and achieve better performance than random sampling in the following sense: either maintaining the performance of feature selection with much fewer instances, or improving the performance of feature selection with the same amount of instances.</p><p>This work is about feature selection on labeled data, i.e., class information is available. For feature selection on unlabeled data, various work can be found in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">53]</ref>. In Section 2, we briefly review the development of feature selection, its models, and active learning. In Section 3, we describe active feature selection and a formalism of selective sampling that exploits data variance. In Section 4, we demonstrate active feature selection using Relief and discuss implementation details. In Section 5, we design objective measures for performance evaluation including time savings. Section 6 is an empirical study in which we evaluate the performance improvement obtained with active feature selection and discuss the implications of the findings. Section 7 presents a conclusion and some work to be completed in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature selection refers to the study of algorithms selecting an optimal subset of the input feature set. Optimality is normally dependent on the evaluation criteria or the application's needs <ref type="bibr" target="#b36">[37]</ref>. Major aspects of feature selection <ref type="bibr" target="#b33">[34]</ref> include feature subset generation, search strategies, goodness evaluation, etc. Feature subset generation studies how a subset is generated following search directions. Search strategies cover exhaustive and complete search, random search, and heuristic search. A search can start from an empty feature set and add new features, or begin with a full set and remove irrelevant features. The goodness of a feature subset can be evaluated using various measures: consistency, distance, information, dependency, accuracy, etc. Feature selection algorithms fall into two broad categories, the filter model or the wrapper model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. The filter model relies on the general characteristics of the training data to select some features independently of any learning algorithm, therefore it does not inherit any bias of a learning algorithm. The wrapper model requires one predetermined learning algorithm and uses the performance of the learning algorithm to evaluate and determine which features are selected. The wrapper model needs to learn a new hypothesis (or a classifier) <ref type="bibr" target="#b39">[40]</ref> for each new feature subset. It tends to find features better suited to the predetermined learning algorithm resulting in good learning performance, but it also tends to be more computationally expensive than the filter model <ref type="bibr" target="#b33">[34]</ref>. When the number of instances becomes large, the filter model is usually chosen due to its computational efficiency. In the context of this work, therefore, we focus on the filter model for feature selection.</p><p>The concept of active feature selection is inspired by the successful use of selected instances (or data points) in active learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49]</ref>. Active learning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> is different from traditional supervised learning: an active learner has the freedom to select which instances are added to the training set. An active learner may begin with a very small number of labeled instances, carefully select a few additional instances for which it requests labels, learn from the result of that request, and then using its newly-gained knowledge, carefully choose another few instances to request next. Several methods have been developed for active learning such as uncertainty sampling, adaptive sampling, and query-by-committee <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref>. The advantage of active learning is that the data requirements for some problems decrease drastically when the instances to be labeled are properly selected. In <ref type="bibr" target="#b49">[50]</ref>, they reported that when training the support vector machine (SVM) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> on a small subset of the available data chosen by their heuristic, its performance is frequently better than that of an SVM trained on all available data. Thus, an active learner provides better generalization and requires less data than a passive learner trained on the entire data set. Similar results were observed in <ref type="bibr" target="#b35">[36]</ref> with their probabilistic classifiers.</p><p>Before we delve into the details of active feature selection, let us briefly describe the difference and commonality between active learning and active feature selection. The essence of active learning lies in its control over the choice of instances used in the iterative learning process <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54]</ref>. Active feature selection shares this essential characteristic with active learning in that it can influence the instances used for feature selection by exploiting some characteristics of the data. The selection process to form a representative sample data set is not iterative. As discussed earlier, we work with a filter model of feature selection and thus we do not employ a learning algorithm to actively choose instances. Therefore, the problem of active feature selection boils down to how we can employ selective sampling to choose representative instances for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Active Feature Selection via Selective Sampling</head><p>Traditional feature selection methods perform dimensionality reduction using whatever training data is given to them. When the training data set is very large, random sampling is commonly used to deal with memory and performance issues. Active feature selection avoids pure random sampling and is realized by selective sampling. The idea of selective sampling stems from the fact that instances are not uniformly distributed and some instances are more representative than others <ref type="bibr" target="#b2">[3]</ref>. If one can identify and select representative instances, fewer instances are needed to achieve similar performance. Therefore, the objective of selective sampling for feature selection is to select only those instances with a high probability to be informative in determining feature relevance. By adopting the filter model for feature selection, we have ruled out the possibility to use a learning algorithm to determine which instances are most relevant <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>. In order to select representative instances for feature selection, we need to explore data characteristics: we first try to partition data according to data dissimilarity and then select representative instances from the resulting partitions. There are many data partitioning techniques <ref type="bibr" target="#b20">[21]</ref> in the literature on multidimensional indexing. We choose kd-tree <ref type="bibr" target="#b19">[20]</ref> in this work because of its simplicity and popularity. A kd-tree is an index structure often used for fast nearest neighbor search <ref type="bibr" target="#b40">[41]</ref>. It is a generalization of the simple binary tree which uses k dimensions (features) instead of a single dimension (feature) to split data points (instances) in a multi-dimensional space. In a kd-tree, the root presents all the instances. Each interior node has an associated splitting feature A i and a splitting value V i (1 ≤ i ≤ k) that divide the instances into two partitions: those with A ivalues less than V i and those with A i -values equal to or greater than V i . Splitting features at different levels of the tree are different, with levels rotating among the features of all dimensions. The splitting is done recursively in each of the successor nodes until the node contains no more than a predefined number of instances (called bucket size) or cannot be split further. The order in which features are chosen to split can result in different kd-trees.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> provides a typical example of kd-tree with four instances (2, 5), (3, 7), <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> in a 2-dimension space: (a) shows how the instances are partitioned in the X, Y plane, and (b) gives a tree representation of the four instances. The root node, with point (2, 5), splits the plane along the Y -axis into two subspaces. The point <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b3">4)</ref> lies in the lower subspace (i.e., (x, y) | y &lt; 5), and thus is in the left subtree. The points (3, 7) and (8, 9) lie in the upper subspace (i.e., (x, y) | y ≥ 5), and thus are in the right subtree. The point (3, 7) further splits the upper subspace into two parts along the X-axis. For the purpose of selective sampling, we want to select features that can split instances into different groups based on their dissimilarity measures as early as possible. Hence, in our building a kd-tree, a splitting feature is chosen if the data variance is maximized along the dimension associated with the feature. The variance of a feature A i is calculated according to the formula V ar</p><formula xml:id="formula_0">(A i ) = N j=1 (V j -V i ) 2 N</formula><p>, where V i is the median value of feature A i , and j is the index of each instance in a data set with N instances. Once feature A i is determined, value V i is then used to split the instances into two partitions. Figure <ref type="figure" target="#fig_1">2</ref> shows a modified kd-tree of the example shown in Figure <ref type="figure" target="#fig_0">1</ref>. At the root level, since V ar(X) = 5.5 (with median value 4) and V ar(Y ) = 3.75 (with median value 6), the space is split along the X-axis (x = 4) into two subspaces. The points (2, 5) and (3, 7) lie in the left subspace (i.e., (x, y) | x &lt; 4), and thus are in the left subtree. The points <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b3">4)</ref> and <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9)</ref> lie in the right subspace (i.e., (x, y) | x ≥ 4), and thus are in the right subtree. Each of the two subspaces are further split along the y-axis (y = 6, y = 6.5, respectively) into two parts, producing four leaf nodes in the tree. Since the median value (instead of mean) of feature A i is used in the calculation of variance, the resulting kd-tree is theoretically a balanced tree. The time complexity of building such an optimized kd-tree is O(kN logN ). However, in some cases, a feature selected to split the tree may contain duplicate values, which may result in an unbalanced tree. The kd-tree can be built once if necessary, or dynamically when required.</p><p>In building a modified kd-tree (as shown in Figure <ref type="figure" target="#fig_1">2</ref>), the leaf nodes (buckets) represent mutually exclusive small subsets of instances which collectively form a partition of the whole data set <ref type="foot" target="#foot_0">1</ref> . The size of a bucket can be an input parameter and determined a priori. Since instances in each bucket are relatively close to each other, we can randomly select one instance from each bucket to represent the effect of all the instances in its corresponding bucket. Therefore, a subset of the instances chosen by selective sampling is used to approximate the full sample space. Selective sampling can be summarized by a 3-step procedure:</p><p>1. Determine the size t of a bucket 2. Build a variance-based kd-tree with m buckets 3. Randomly select an instance from each bucket We provide next details of selective sampling for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applying Selective Sampling to Feature Selection</head><p>Efficiency is critical for feature selection algorithms in the context of large data sets. To demonstrate selective sampling, we use a well-known and efficient algorithm Relief that can select statistically relevant features in linear time of the numbers of features and instances <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>. After introducing Relief in Section 4.1, we illustrate how selective sampling can be applied to Relief and present a new algorithm of active feature selection in Section 4.2. In Section 4.3, we discuss other related work on Relief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relief algorithm</head><p>The key idea of Relief (given in Figure <ref type="figure">3</ref>) is to estimate the quality of features according to how well their values distinguish between instances that are near to each other. For this purpose, given a randomly selected instance X from a data set S with k features, Relief searches the data set for its two nearest neighbors: one from the same class, called nearest hit H, and the other from a different class, called nearest miss M . It updates the quality estimation W [A i ] for all the features A i based on the values of difference function dif f () about X, H, and M . The process is repeated m times, where m is a user-defined parameter <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>. For Given m -desired number of sampled instances, and k -number of features, 1. set all weights W [A i ] := 0.0; 2. for j := 1 to m do begin 3. randomly select an instance X; 4. find nearest hit H and nearest miss M ; 5. for i := 1 to k do begin 6.</p><p>W</p><formula xml:id="formula_1">[A i ] := W [A i ] -dif f (A i , X, H)/m + dif f (A i , X, M )/m; 7. end; 8. end; Figure 3: Original Relief algorithm. instances X 1 , X 2 , dif f (A i , X 1 , X 2 ) calculates the difference between the values 2 (x 1i and x 2i ) of feature A i : dif f (A i , x 1i , x 2i ) =      |x 1i -x 2i | if A i is numeric 0 if A i is nominal &amp; x 1i = x 2i 1 if A i is nominal &amp; x 1i = x 2i</formula><p>Normalization with m in calculation of W [A i ] (line 6 in Figure <ref type="figure">3</ref>) guarantees that all weights are in the interval of [-1,1]. The time complexity of Relief for a data set with N instances is O(mkN ). Efficiency is one of the major advantages of the Relief family over other algorithms <ref type="bibr" target="#b11">[12]</ref>. With m being a constant, the time complexity becomes O(kN ). However, since m is the number of instances used to approximate probabilities, a larger m implies more reliable approximations. When N is very large, it is often required that m N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relief with selective sampling</head><p>Although random sampling of m instances for Relief reduces the time complexity from O(kN 2 ) to O(kN ), the optimal results of Relief are not guaranteed. When applying selective sampling to Relief, we aim to obtain results that are better than using random sampling and similar to the results using all the instances. In our attempt to selectively sample m instances based on the modified kdtree introduced in Section 3, we can use either sample size m or bucket size t Given t -bucket size, 1. set all weights W [A i ] := 0.0; 2. buildKDTree(t); 3. m := number of buckets; 4. for j := 1 to m do begin 5.</p><p>randomly select an instance X from Bucket[j]; 6.</p><p>find nearest hit H and nearest miss M ; 7.</p><p>for i := 1 to k do begin 8.</p><p>W</p><formula xml:id="formula_2">[A i ] := W [A i ] -dif f (A i , X, H)/m + dif f (A i , X, M )/m; 9.</formula><p>end; 10. end; to control the kd-tree splitting process. Since only one instance is selected from each bucket, one can easily establish that t = N/m, where N is the total number of instances. Therefore, if m is predetermined, the splitting process stops when it reaches the level where each bucket contains N/m or fewer instances. For example, in Figure <ref type="figure" target="#fig_1">2</ref> (b), if m = 2, the splitting process stops after the first split when each bucket contains two instances. On the other hand, given a t, we can estimate m based on t. In Figure <ref type="figure" target="#fig_1">2</ref> (b), if we choose t to be 1 (i.e., each bucket contains only one instance), all the four instances will be selected. For a bucket size t = N , the root node is not split at all, and only one instance will be selected. As we mentioned earlier, in practical, the resulting kd-tree may not be a balanced tree due to duplicate feature values. Therefore, for 1 &lt; t &lt; N , the number of selected instances m will be within ( 1 t N , N ). In this work, we use bucket size t to control the number of selected instances m which is equal to the number of buckets. The algorithm of Relief with selective sampling is detailed in Figure <ref type="figure" target="#fig_2">4</ref>. One important point in buildKDTree(t) is that each feature should be normalized <ref type="bibr" target="#b56">[56]</ref> before the variance calculation in order to choose to split on the feature with largest variance.</p><p>In the empirical study, we use ReliefF <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">56]</ref> which is an extension to the original Relief. It handles multiple classes and searches for several nearest neighbors to be robust to noise. We compare ReliefF with its counterpart ReliefS which applies selective sampling for instance selection in the same way as described in shows ReliefF with random sampling of 4 instances from the data. The bottom flow shows ReliefS with random sampling of one instance from each of the four buckets of the kd-tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Related work on Relief</head><p>There has been substantial research on the Relief family of algorithms (Relief, ReliefF, and RReliefF) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. Relief (introduced in Section 4.1) was designed for feature subset selection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> and it is considered one of the best algorithms for this purpose <ref type="bibr" target="#b15">[16]</ref>. Relief only deals with binary classes. This limitation was overcome by ReliefF <ref type="bibr" target="#b31">[32]</ref> which handles multiple classes and incomplete and noisy data. ReliefF was further extended to RReliefF <ref type="bibr" target="#b44">[45]</ref> in order to handle continuous classes in regression. The Relief family of algorithms are general and successful feature estimators and are especially good in detecting conditional dependencies between features <ref type="bibr" target="#b47">[48]</ref>. In inductive learning, ReliefF was successfully employed in building decision trees for classification. The resulting decision trees achieved superior predictive accuracy over Naive Bayesian classifiers and k-NN classifiers across various artificial and real-world data sets <ref type="bibr" target="#b32">[33]</ref>. It was also shown in <ref type="bibr" target="#b44">[45]</ref> that RReliefF is effective in selecting features in learning regression trees for regression problems.</p><p>Albeit a broad spectrum of successful uses of the Relief family of algorithms, little work has shown how to determine an optimal sample size<ref type="foot" target="#foot_2">3</ref> . Therefore, work regarding Relief usually circumvents the issue of optimal m by simply choosing m to be the full size of the data set <ref type="bibr" target="#b31">[32]</ref> as the larger m leads to better performance <ref type="foot" target="#foot_3">4</ref> . Robnik-Sikonja and Kononenko showed in <ref type="bibr" target="#b47">[48]</ref> that although feature selection results become stable after a number of iterations for simple data sets, the quality of results keeps improving as the sample size increases for more complicated data sets. The goal of this work is to show that for a given sample size m, instances obtained by selective sampling are more effective for feature selection than those selected by random sampling. Hence, in our work, we observe how selective sampling differs from random sampling by varying m from 10% to 100% of N using ReliefF with all N instances as the performance reference.</p><p>Recall that Relief relies on the search of a predefined number of nearest neighbors <ref type="bibr" target="#b31">[32]</ref>, and the kd-tree data structure is often used for fast nearest neighbor search <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b50">[51]</ref>, kd-trees are applied to speed up Relief and its extensions by providing a fast way to locate nearest neighbors for each class, thus reducing the overall time complexity of the algorithms to O(kN logN ). Additional time savings can be achieved by building the kd-tree once and using it for both bucket generation and the nearest neighbor search. However, the focus of this work is not to speed up current algorithms in the Relief family, but to investigate the effectiveness of selective sampling in partitioning data points for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Issues of Performance Evaluation</head><p>Before we proceed to define performance measures for selective sampling, let us first discuss some criteria for a suitable performance measure:</p><p>1. It is a function of the features of the data.</p><p>2. Its value improves as m increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Its value reaches the best when</head><formula xml:id="formula_3">m = N .</formula><p>In ReliefF, since m is the number of instances used to approximate probabilities, a larger m implies more reliable approximations. Therefore, it is reasonable to assume that the optimal result ReliefF can achieve is the features ranked according to their weights when m = N . This ranked list of features is named S N . Given various sizes of m, the results of ReliefF and ReliefS can be compared in two aspects: (1) subset selection -we compare which resulting subset is more similar to the optimal subset obtained from S N ; and (2) feature order -we compare the order of features determined by ReliefF or ReliefS to the order of features in S N . In Section 5.1, we discuss the reason why it is important to consider the order of features. In Section 5.2, we present three different measures with different emphasis on feature order and check if they satisfy the above three criteria. In Section 5.3, we discuss time savings when using active feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Importance of feature order</head><p>Based on the output type, feature selection algorithms can be grouped into two categories, minimum subset algorithms and feature ranking algorithms <ref type="bibr" target="#b36">[37]</ref>. Minimum subset algorithms return a minimum feature subset but do not rank features in the subset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">53]</ref>. Features in the subset are relevant, others are irrelevant. Feature ranking algorithms assign a weight to each feature of the data set and rank the relevance of features according to their weights <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b58">58]</ref>. Order information is important for these algorithms because it indicates relative relevance. Order information is also important if one aims to select a feature subset from the resulting ranked list. Below, we illustrate how order information can affect the selection of relevant features for ReliefF and other feature ranking algorithms.</p><p>We first define some terms. For an optimal list of features S N , a target subset of features T is defined as an optimal subset of features which contains the top n weighted features in S N . For a data set with an unknown number of relevant features (n), T contains the top n features whose weights ≥ γ, where γ is a threshold equal to W [i] (the ith largest weight in S N ) and the gap defined by W [i] and W [i+1] is sufficiently large (e.g., greater than the average gap among the k-1 gaps). Let S Relief F and S Relief S be the two resulting lists obtained by ReliefF and ReliefS, respectively. To compare the performance of both ReliefF and ReliefS with different sizes of m, we can define a performance measure P(S N , R) where R can be either S Relief F or S Relief S with varying m. Figure <ref type="figure" target="#fig_4">6</ref> (a) illustrates the relationships among S N , T , R, and R n (the subset of the top n features in R). Figure <ref type="figure" target="#fig_4">6</ref> (b) shows five cases for a set of five features. Column I shows the optimal ranking of the five features (S N ), and each of the remaining four columns (II-V) For a given n (either known a priori or determined by threshold γ), the order of features in R directly affects the selection of a feature subset R n . For example, in Figure <ref type="figure" target="#fig_4">6</ref> (b), the threshold γ is chosen to include the top three features from each resulting list. All the three relevant features (A 1 , A 2 , A 3 ) are selected into R n in cases II and III, while only two of them are selected in cases IV and V. However, given the same ordered lists II-V, if γ is chosen to include the top two features, the selected subset R n in case III will fail to include both of the two relevant features A 1 and A 2 , while R n in case IV will become an optimal subset. Therefore, it is important to develop performance measures that take order information into consideration. Below we examine sensible candidates for P().</p><formula xml:id="formula_4">S N T A 1 A 2 A 3 A n A k A 2 A 3 A 5 A n A k R n R ( S ReliefF / S ReliefS ) A 1 A 2 A 3 A 4 A 5 A 2 A 1 A 4 A 3 A 5 A 2 A 3 A 5 A 1 A 4 A 3 A 1 A 2 A 5 A 4 A 1 A 2 A 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Precision</head><p>Precision (P) is computed as the number of features that are in both T and R n , divided by the number of features in T :</p><formula xml:id="formula_5">P = |T ∩ R n | |T | .</formula><p>P ranges from 0 to 1, where P is 1 when subsets T and R n are equal and 0 when none of the features in T appears in R n . In Figure <ref type="figure" target="#fig_4">6</ref> (b), P is 1 for cases II and III and 2/3 for cases IV and V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Distance</head><p>Precision treats all features in T and R n equally without considering the orders of the features. In order to account for the order of features in both S N and R, Distance (D) is defined based on the sum of distances between common features in T and R. The distance of a feature between two sets is the difference between its positions in the two ranked lists. Let S N be S N in reverse order. The maximum possible ranking distance between two sets that share the same features is:</p><formula xml:id="formula_6">D max = ∀A i ∈S N |position(A i ∈ S N ) -position(A i ∈ S N )|.</formula><p>D is then defined as the follows:</p><formula xml:id="formula_7">D = ∀A i ∈T |position(A i ∈ T ) -position(A i ∈ R)| D max</formula><p>Since the subset R n may not contain all the features in T , we use the full set R in the definition of D. D max is used to normalize D so that D ranges from 0 to 1, where D is 0 when the two sets T and R n have identical ranking (as shown in case II), otherwise, D is larger than 0. In Figure <ref type="figure" target="#fig_4">6</ref> (b), with D max = 12, the D values for cases II, III, IV, and V are 0, 4/12, 3/12, and 5/12, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Raw Distance</head><p>Both Precision and Distance require choosing a threshold γ to decide the target set T . In some cases, it is difficult to determine an optimal threshold. If γ is wrongly estimated, meaning that the target set T is not an optimal subset, evaluation using these measures will be incorrect. For example, in Figure <ref type="figure" target="#fig_4">6</ref> (b), if γ is chosen between features A 2 and A 3 in S N , the target set will be {A 1 , A 2 } instead of {A 1 , A 2 , A 3 }. Thus, Precision for case III will become 1/2 instead of 1, and Precision for case IV will become 1 instead of 2/3. A straightforward performance measure is to directly calculate the sum of the differences of weights for each of the feature pairs (same features) in the optimal list S N and the resulting </p><formula xml:id="formula_8">RD = k i=1 |W S [A i ] -W R [A i ]|,</formula><p>where W S [A i ] and W R [A i ] are associated with S N and R, respectively. RD considers all k features in the two results. Thus, this measure does not rely on threshold γ. It is designed to compare the results of ReliefF and ReliefS, but it cannot be used for measuring the performance of subset selection as it uses all the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Comparison of measures</head><p>Each measure serves a unique purpose in the evaluation of feature selection results. Table <ref type="table" target="#tab_0">1</ref> provides a summary of these four measures. The setting of γ is required in ReliefF for feature subset selection. Precision is a simple measure of subset selection, but it is not sufficient due to its insensitivity to the order of features. Both Distance and Raw Distance are order-sensitive, but Raw Distance does not require the threshold setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Measuring time savings</head><p>It is sensible to question whether the use of selective sampling in feature selection would result in any time savings overall because the initial building of kd-tree incurs certain costs. This question is best answered by measuring computation time during experiments. ReliefS and ReliefF require different numbers of instances to achieve the same level of performance. We can compare the running times required to achieve a given level of performance. Let T kd-tree , T Relief S , and T Relief F be the times for kd-tree building, running ReliefS, and running ReliefF, respectively, with a given performance. A straightforward approach is to report T kd-tree , T Relief S , and T Relief F and compare T kd-tree + T Relief S with T Relief F . Their difference can be either the saving or the loss of computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Measuring accuracy improvement</head><p>One can indirectly measure the results of feature selection using a learning algorithm to check its effect on accuracy. Likewise, the effectiveness of selective sampling for feature selection can be further verified by comparing the learning accuracy on the subsets of features chosen by ReliefF and ReliefS. For the two resulting lists produced by RelieF and ReliefS with the same number of sampled instances of a data set, two different feature subsets of the same cardinality can be chosen from the top of the two lists and then used to obtain the learning accuracy for a certain leaning algorithm. We expect an accuracy gain from the subset chosen by ReliefS over the subset chosen by ReliefF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Study</head><p>The objective of this section is to empirically evaluate if selective sampling can do better in selecting m instances than random sampling in the context of Reli-efF. We examine if the results of feature selection are consistently better when using instances sampled from kd-tree buckets than when using the same number of instances selected by random sampling. In Section 6.1, we present two groups of data sets (synthetic and benchmark) and the experimental procedures. In Section 6.2 and Section 6.3, we present and discuss results for synthetic data sets and benchmark data sets. In Section 6.4, we further examine the effectiveness of selective sampling in terms of learning accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data and experimental procedures</head><p>We choose synthetic data sets in our experiments because the relevant features of these data sets are known beforehand. The use of synthetic data in our experiments serves three purposes: (1) to verify the effectiveness of ReliefF, (2) to avoid choosing the optimal threshold γ for subset selection, and (3) to evaluate the effectiveness of active feature selection. Since we rarely know the relevant features beforehand in practice, we also conduct experiments on benchmark data sets to evaluate selective sampling. We describe the two groups of data sets below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Synthetic data</head><p>We use functions described in <ref type="bibr" target="#b0">[1]</ref> to generate synthetic data sets so that we know exactly which features are relevant. The nine features are described in Table <ref type="table" target="#tab_1">2</ref>. Ten classification functions of Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> were used to generate classification problems with different complexities. Efforts were made to generate data sets as described in the original functions. Each data set consists of 5000 instances. The values of the features of each instance were generated randomly according to the distributions given in the table. For each instance, a class label was determined according to the rules that define the functions.</p><p>As an example, we give Function 2 that uses two features and classifies an instance into Group A if</p><formula xml:id="formula_9">((age &lt; 40) ∧ (50000 ≤ salary ≤ 100000))∨ ((40 ≤ age &lt; 60) ∧ (75000 ≤ salary ≤ 125000))∨ ((age ≥ 60) ∧ (25000 ≤ salary ≤ 75000)).</formula><p>Otherwise, the instance is classified into Group B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Benchmark data</head><p>All together 23 data sets are selected from the UCI Machine Learning Data Repository <ref type="bibr" target="#b4">[5]</ref> and the UCI KDD Archive <ref type="bibr" target="#b3">[4]</ref>. They all have nominal classes with varied numbers of instances (from 150 to 145000), numbers of features (from 4 to 85), and numbers of classes (from 2 to 22). A summary of these data sets is presented in Table <ref type="table" target="#tab_3">3</ref> which is divided into three groups: Group 1 contains only numeric data, Group 2 only nominal data, and Group 3 mixed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Experimental procedures</head><p>The experiments are conducted using Weka's implementation of ReliefF, and Re-liefS is also implemented in the Weka environment <ref type="bibr" target="#b56">[56]</ref>. We use different percentages of data (varying m). The performance of ReliefF with m = N is set as the performance reference point. The departure from the reference point is a measure of performance deterioration. In particular, we choose six increasing bucket sizes t i (1 ≤ i ≤ 6) from 1 to 6 corresponding to six percentage values P i . For example, t 2 corresponds to P 2 ≈ 50%. For each data set, the experiment is conducted as follows:</p><p>1. Run ReliefF with bucket size t 1 (P 1 = 100%), and obtain the optimal ranked list of features (S N ) according to their weights. The parameter for k in the k-nearest neighbor search is set to 5 (neighbors). This parameter remains the same for all experiments. A curve is plotted for each measure for comparison. Some of the curves are shown in Figures <ref type="figure" target="#fig_5">7</ref> and<ref type="figure" target="#fig_6">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on synthetic data</head><p>Table <ref type="table" target="#tab_4">4</ref> reports the set of relevant features used to determine the class labels in the definition of each function and the optimal ranking list obtained from running ReliefF on the whole data set of 5000 instances generated by each function. From Table <ref type="table" target="#tab_4">4</ref>, we observe that given the cardinality of a relevant feature set (n), for each of these 10 functions except Function 4 and Function 10<ref type="foot" target="#foot_4">5</ref> , the set of the top n features in each ranking list matches exactly with the known relevant feature set. This verifies that ReliefF can indeed find the relevant features of a data set in most cases. Thus we can use the results obtained from the first step of the experimental procedures as a reference to evaluate the performance of ReliefF and ReliefS with varying size m.</p><p>Table <ref type="table" target="#tab_5">5</ref> presents a summary of the three performance measures for each synthetic data set. For a given measure, let P i be the averaged value (over 30 runs) obtained at percentage P i (2 ≤ i ≤ 6). Each value in Table <ref type="table" target="#tab_5">5</ref> is averaged over five percentage values, i.e., val Avg = (</p><formula xml:id="formula_10">6 i=2 P i )/5.</formula><p>Recall that Precision varies from 0 (worst) to 1 (best); Distance varies from 0 (best) to 1 (worst); and Raw Distance starts at 0 (best) and increases. The last  <ref type="table" target="#tab_5">5</ref> summarizes Win/Loss/Tie in comparing ReliefS with Re-liefF for each measure. It is clear that for the ten synthetic data sets, taking all the three measures into account, ReliefS is as good as or better than ReliefF. This suggests that m instances selected using kd-trees are more effective than m instances selected at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results on benchmark data</head><p>We present and discuss separately the results on numeric data and non-numeric data below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Results on numeric data</head><p>Two sets of results on numeric data sets are reported in Table <ref type="table" target="#tab_6">6</ref> and Figure <ref type="figure" target="#fig_5">7</ref>, respectively. Like Table <ref type="table" target="#tab_5">5</ref>, Table <ref type="table" target="#tab_6">6</ref> shows that ReliefS is better than or as good as ReliefF in determining relevant instances for feature selection. Now let us look at the trends of the three performance measures when the number of instances increases for both ReliefF and ReliefS. Figure <ref type="figure" target="#fig_5">7</ref> shows the results of three illustrative data sets for Precision, Distance, and Raw Distance. For the Segment data,  that all feature subsets selected are the same as if we use all N instances (recall that the order of selected features is not considered by Precision). Distance and Raw Distance have values greater than 0, which suggests that the selected results are not in the same order as those obtained using all N instances. It can be observed that the more instances used for both ReliefF and ReliefS, the better the performance of feature selection. A similar trend can be observed for the Vehicle data and Satimage data as well. Figure <ref type="figure" target="#fig_5">7</ref> and Table <ref type="table" target="#tab_6">6</ref> indicate that active feature selection can significantly improve performance on numeric data over feature selection with random sampling with the same amount of instances. In other words, the use of kd-trees to partition the data makes the difference. However, as mentioned earlier, the building of kdtrees incurs certain costs. To actually compare the running time of ReliefS with that of ReliefF, we consider the case of bucket size 2 for ReliefF (i.e., m Relief F Based on previous results, it is clear that ReliefS works well on numeric data. We then experiment if ReliefS can be directly extended to non-numeric data (Groups 2 and 3 in Table <ref type="table" target="#tab_3">3</ref>). Since variance is calculated on numeric data when building the kd-tree, in our experiments we apply ReliefS to non-numeric data by associating a distinct number to a nominal value, e.g., assigning 1, 2 and 3 to nominal values A, B, and C, respectively. Results for the three performance measures are reported in Table <ref type="table" target="#tab_8">8</ref> and Figure <ref type="figure" target="#fig_6">8</ref>. From Table <ref type="table" target="#tab_8">8</ref>, we still notice that for each data set, ReliefS is better than or as good as ReliefF. This suggests that active feature selection works for nonnumeric data as well. However, by comparing the results in Table <ref type="table" target="#tab_8">8</ref> with Table <ref type="table" target="#tab_6">6</ref>, we observe that both ReliefF and ReliefS generally work better on numeric data than on non-numeric data. In addition, the performance gains obtained by ReliefS on non-numeric data are not as significant as those on numeric data, especially for small data sets (this can also be observed in Figure <ref type="figure" target="#fig_6">8</ref>). Through three illustrative data sets from Groups 2 and 3, Figure <ref type="figure" target="#fig_6">8</ref> demonstrates similar trends of performance measures as those in Figure <ref type="figure" target="#fig_5">7</ref> when the number of instances increases for both ReliefF and ReliefS. Take Autos data for example, ReliefS performs better on all the three measures than ReliefF, but as we can see that the two curves for  each measure are very close to each other, which suggests that the performance gains obtained by selecting m instances using kd-trees are not significant. Table <ref type="table" target="#tab_10">9</ref> records the running times T kd-tree , T Relief S and T Relief F as well as m Relief S and m Relief F for data sets in Groups 2 and 3. From this table, we observe similar results: (1) the larger the data set, the more savings in time; and (2) the ratio of T kd-tree /T Relief S decreases when data size increases. However, five small data sets out of the nine do not show significant time savings. This observation is consistent with what we saw in Table <ref type="table" target="#tab_8">8</ref> and Figure <ref type="figure" target="#fig_6">8</ref>. For these data sets, selective sampling using ReliefS does not significantly reduce the number of instances required to achieve a given level of performance when compared to random sampling using ReliefF. Thus the extra time spent on building a kd-tree is not compensated by the modest time savings obtained from using fewer instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Examining learning accuracy</head><p>In this section, we first show how ReliefF using all instances as suggested by <ref type="bibr" target="#b31">[32]</ref> affects the learning accuracy, and then examine how the reduction of training instances affect the results of feature selection in terms of learning accuracy. The  <ref type="table" target="#tab_12">11</ref>. In Section 6.2, we have demonstrated the effectiveness of ReliefF using all instances on a group of synthetic data sets for which we know the relevant features in advance. We now examine the effectiveness of ReliefF using all instances on benchmark data through a learning algorithm. Table <ref type="table" target="#tab_11">10</ref> records the 10-fold cross validation results of the 5-NN (nearest neighbor) classifier on the full sets of features and the target sets of features chosen by ReliefF with all instances (defined in Section 5.1) for the 14 numeric data sets (shown in Table <ref type="table" target="#tab_3">3</ref>). In order to evaluate the statistical significance of the difference between the two averaged accuracy values for a given data set, a Student's paired two-tailed t-Test is conducted for the two underlying samples of individual accuracy values. The P value in each row of Table <ref type="table" target="#tab_11">10</ref> reports the probability that the two underlying samples are different. The smaller the P value, the more significant the difference of the two average values is. The last row (W/L/T) summarizes Win/Loss/Tie in comparing the averaged accuracy values on the target sets with those on the full sets based on a significance threshold 0.1. It is clear that out of the 14 data sets, seven pairs of results are significantly different. ReliefF significantly improves the accuracy of the 5-NN classifier for 3 data sets and maintains the accuracy for 7 data sets with selected subsets of features.</p><p>To examine the effect of reduction of training instances and verify the effectiveness of ReliefS, for each data set, two feature subsets of the same cardinality are selected from the top of the two resulting lists produced by ReliefF and ReliefS with the same number of sampled instances (we choose m ≈ 10%N , corresponding to the smallest bucket size, in our experiments). Table <ref type="table" target="#tab_12">11</ref> records the 10-fold cross validation results of 5-NN on the subsets of features chosen by ReliefF (column B) or ReliefS (column C). We use the results on the target sets of features chosen by ReliefF with all instances (m = N ) as the reference point (column A) in comparison of ReliefF and ReliefS. As discussed in Section 5.1, for feature ranking methods like ReliefF, the order of features in a resulting list is important for subset selection. However, learning accuracy on a selected subset may not be sensitive to the order of features; as long as two subsets contain the same features (having equal Precision values), they can result in the same learning accuracy for a given learning algorithm. Therefore, in order to show the effect of the different orders of features, we choose two subsets of different features of cardinality n from the top of the two resulting feature lists to obtain the accuracy for 5-NN.</p><p>Table <ref type="table" target="#tab_12">11</ref> also contains P values resulting from pair-wised comparisons of columns A, B, and C. P (A,B) values for columns A and B suggest that there are 5 pairs of average accuracy rates that are significantly different and the subsets selected by ReliefF with m ≈ 1 10 N cause accuracy decrease in all 5 cases. P (A,C) values for columns A and C show that there are 4 pairs of average accuracy rates that significantly different, and the subsets selected by ReliefS with m ≈ 1  10 N result in accuracy increase for 2 data sets and decrease for 2 data sets among the 4 cases. We then further compare the 5-NN results using the feature subsets of ReliefS and ReliefF with m ≈ 1 10 N directly. P (B,C) values for columns B and C indicate that there are 6 pairs of average accuracy rates that are significantly different. It is clear that the subsets selected by ReliefS with m ≈ 1 10 N result in better accuracy than the subsets selected by ReliefF with m ≈ 1 10 N for 5 data sets and worse accuracy for only one data set among the 6 cases. According to the above results, we conclude that with the same number of sampled instances, ReliefS in general achieves better performance than ReliefF in terms of learning accuracy and hence selective sampling is an effective approach for active feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Further Work</head><p>In this paper, we present a case for active feature selection using a formalism of selective sampling. We choose an efficient feature selection algorithm ReliefF in our case study to evaluate whether selective sampling has consistent advantages over random sampling. In particular, we use the kd-tree to partition data and select instances from the partitions. We conduct extensive experiments to evaluate the performance of active feature selection. Significant time savings are observed using the Raw Distance performance measure. Improvement of learning accuracy is reported for the nearest neighbor classifier on numeric data.</p><p>Although the experimental study demonstrates the effectiveness of active feature selection, we plan future work along the following lines: (1) to investigate why ReliefS still works in cases where data is not purely numeric and explore different methods of handling nominal features; (2) to automatically determine the cost-effective percentage of instances for selective sampling and investigate its performance on large data sets (e.g., Web data); (3) to investigate other means of exploiting data characteristics for selective sampling; and (4) to apply selective sampling to the vast body of feature selection and other data preprocessing algorithms <ref type="bibr" target="#b37">[38]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A typical kd-tree example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A modified use of kd-tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relief with selective sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 :</head><label>45</label><figDesc>Figure 5: The difference between ReliefF and ReliefS: The top and bottom flows show how ReliefF and ReliefS work, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example for the importance of feature order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Three illustrative numeric data sets: Average results of 30 runs in three performance measures (P, D, RD). 22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Three illustrative non-numeric data sets: Average results of 30 runs in three performance measures (P, D, RD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of performance measures.</figDesc><table><row><cell></cell><cell cols="3">Precision Distance Raw Distance</cell></row><row><cell>Complexity</cell><cell>O(n 2 )</cell><cell>O(nk)</cell><cell>O(k 2 )</cell></row><row><cell>Upper bound</cell><cell>1</cell><cell>1</cell><cell>None</cell></row><row><cell>Lower bound</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Ordering</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>γ setting</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell cols="2">list R. We name it Raw Distance (RD):</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Features of the test data adapted from Agrawal et al.<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Value</cell></row><row><cell>salary</cell><cell>salary</cell><cell>uniformly distributed from 20,000 to 150,000</cell></row><row><cell cols="2">commission commission</cell><cell>if salary ≥ 75000 → commission = 0</cell></row><row><cell></cell><cell></cell><cell>else uniformly distributed from 10000 to 75000.</cell></row><row><cell>age</cell><cell>age</cell><cell>uniformly distributed from 20 to 80.</cell></row><row><cell>elevel</cell><cell>education level</cell><cell>uniformly distributed from [0, 1, . . . , 4].</cell></row><row><cell>car</cell><cell>make of the car</cell><cell>uniformly distributed from [1, 2, . . . 20].</cell></row><row><cell>zipcode</cell><cell cols="2">zip code of the town uniformly chosen from 9 available zipcodes.</cell></row><row><cell>hvalue</cell><cell>value of the house</cell><cell>uniformly distributed from 0.5k10000 to 1.5k1000000</cell></row><row><cell></cell><cell></cell><cell>where k ∈ {0 . . . 9} depends on zipcode.</cell></row><row><cell>hyears</cell><cell>years house owned</cell><cell>uniformly distributed from [1, 2, . . . , 30].</cell></row><row><cell>loan</cell><cell cols="2">total amount of loan uniformly distributed from 1 to 500000.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For each P i , run ReliefF 30 times and calculate Precision, Distance, and Raw Distance each time, and obtain their average values after 30 runs.</figDesc><table /><note><p>2. Run ReliefS with bucket sizes t i (2 ≤ i ≤ 6). At each t i , run ReliefS 30 times with different seeds and calculate values of Precision, Distance, and Raw Distance for each iteration to obtain average values of these performance measures to eliminate any idiosyncrasy in a single run. A curve is plotted for each measure for comparison. Some of the curves are shown in Figures 7 and 8. 3. Run ReliefF with each P i (2 ≤ i ≤ 6) determined by corresponding t i in step 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of benchmark data sets: N -number of instances, Num -numeric features, Nom -nominal features, #C -number of classes.</figDesc><table><row><cell>Title</cell><cell>N</cell><cell cols="3">Num Nom #C</cell></row><row><cell>Iris</cell><cell>150</cell><cell>4</cell><cell>0</cell><cell>3</cell></row><row><cell>Glass</cell><cell>214</cell><cell>9</cell><cell>0</cell><cell>7</cell></row><row><cell>WDBC</cell><cell>569</cell><cell>30</cell><cell>0</cell><cell>2</cell></row><row><cell>Balance</cell><cell>625</cell><cell>4</cell><cell>0</cell><cell>3</cell></row><row><cell>Pima-Indian</cell><cell>768</cell><cell>8</cell><cell>0</cell><cell>2</cell></row><row><cell>Vehicle</cell><cell>846</cell><cell>18</cell><cell>0</cell><cell>4</cell></row><row><cell>German</cell><cell>1000</cell><cell>24</cell><cell>0</cell><cell>2</cell></row><row><cell>Segment</cell><cell>2310</cell><cell>19</cell><cell>0</cell><cell>7</cell></row><row><cell>Abalone</cell><cell>4177</cell><cell>8</cell><cell>0</cell><cell>3</cell></row><row><cell>Satimage</cell><cell>4435</cell><cell>36</cell><cell>0</cell><cell>6</cell></row><row><cell>Waveform</cell><cell>5000</cell><cell>40</cell><cell>0</cell><cell>3</cell></row><row><cell>Page-Blocks</cell><cell>5473</cell><cell>10</cell><cell>0</cell><cell>5</cell></row><row><cell>CoIL2000</cell><cell>5822</cell><cell>85</cell><cell>0</cell><cell>2</cell></row><row><cell>Shuttle</cell><cell>14500</cell><cell>8</cell><cell>0</cell><cell>7</cell></row><row><cell>Breast-cancer</cell><cell>286</cell><cell>0</cell><cell>9</cell><cell>2</cell></row><row><cell>Primary-tumor</cell><cell>339</cell><cell>0</cell><cell>17</cell><cell>22</cell></row><row><cell>KRKPA7</cell><cell>3196</cell><cell>0</cell><cell>36</cell><cell>2</cell></row><row><cell>Mushroom</cell><cell>8124</cell><cell>0</cell><cell>22</cell><cell>2</cell></row><row><cell>Zoo</cell><cell>101</cell><cell>1</cell><cell>16</cell><cell>7</cell></row><row><cell>Autos</cell><cell>205</cell><cell>15</cell><cell>10</cell><cell>7</cell></row><row><cell>Colic</cell><cell>368</cell><cell>7</cell><cell>15</cell><cell>2</cell></row><row><cell>Vowel</cell><cell>990</cell><cell>10</cell><cell>3</cell><cell>11</cell></row><row><cell>Hypothyroid</cell><cell>3772</cell><cell>7</cell><cell>22</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of ReliefF on synthetic data. The order of features in a relevant feature set has no significance based on function definitions.</figDesc><table><row><cell></cell><cell>Relevant Feature Set</cell><cell>Result from ReliefF (m = N )</cell></row><row><cell>Function 1</cell><cell>{A3}</cell><cell>A3, |A5, A6, A8, A4, A7, A9, A2, A1</cell></row><row><cell>Function 2</cell><cell>{A1, A3}</cell><cell>A1, A3, |A2, A9, A7, A4, A8, A6, A5</cell></row><row><cell>Function 3</cell><cell>{A3, A4}</cell><cell>A4, A3, |A7, A1, A9, A2, A6, A8, A5</cell></row><row><cell>Function 4</cell><cell>{A1, A3, A4}</cell><cell>A1, A4, A2, |A3, A7, A6, A5, A9, A8</cell></row><row><cell>Function 5</cell><cell>{A1, A3, A9}</cell><cell>A9, A3, A1, |A2, A7, A4, A8, A6, A5</cell></row><row><cell>Function 6</cell><cell>{A1, A2, A3}</cell><cell>A1, A3, A2, |A8, A4, A5, A7, A9, A6</cell></row><row><cell>Function 7</cell><cell>{A1, A2, A9}</cell><cell>A9, A1, A2, |A8, A5, A7, A6, A3, A4</cell></row><row><cell>Function 8</cell><cell>{A1, A2, A4}</cell><cell>A1, A4, A2, |A5, A7, A9, A6, A3, A8</cell></row><row><cell>Function 9</cell><cell>{A1, A2, A4, A9}</cell><cell>A9, A1, A4, A2, |A5, A3, A7, A6, A8</cell></row><row><cell cols="3">Function 10 {A1, A2, A4, A7, A8, A9} A4, A1, A2, A8, A3, A9, |A7, A6, A5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average values of three different measures using ReliefF and ReliefS on synthetic data.</figDesc><table><row><cell></cell><cell cols="2">Precision</cell><cell cols="2">Distance</cell><cell cols="2">Raw Distance</cell></row><row><cell></cell><cell cols="6">ReliefF ReliefS ReliefF ReliefS ReliefF ReliefS</cell></row><row><cell>Function 1</cell><cell>1.0</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.049</cell><cell>0.041</cell></row><row><cell>Function 2</cell><cell>0.997</cell><cell>1.0</cell><cell>0.017</cell><cell>0.0</cell><cell>0.047</cell><cell>0.044</cell></row><row><cell>Function 3</cell><cell>1.0</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.046</cell><cell>0.044</cell></row><row><cell>Function 4</cell><cell>0.930</cell><cell>0.932</cell><cell>0.075</cell><cell>0.061</cell><cell>0.049</cell><cell>0.043</cell></row><row><cell>Function 5</cell><cell>0.860</cell><cell>0.886</cell><cell>0.182</cell><cell>0.169</cell><cell>0.045</cell><cell>0.044</cell></row><row><cell>Function 6</cell><cell>0.891</cell><cell>0.927</cell><cell>0.280</cell><cell>0.225</cell><cell>0.047</cell><cell>0.026</cell></row><row><cell>Function 7</cell><cell>0.852</cell><cell>0.881</cell><cell>0.157</cell><cell>0.137</cell><cell>0.047</cell><cell>0.041</cell></row><row><cell>Function 8</cell><cell>1.0</cell><cell>1.0</cell><cell>0.039</cell><cell>0.007</cell><cell>0.053</cell><cell>0.042</cell></row><row><cell>Function 9</cell><cell>0.915</cell><cell>0.935</cell><cell>0.135</cell><cell>0.109</cell><cell>0.050</cell><cell>0.040</cell></row><row><cell>Function 10</cell><cell>0.900</cell><cell>0.910</cell><cell>0.079</cell><cell>0.072</cell><cell>0.061</cell><cell>0.046</cell></row><row><cell>W/L/T</cell><cell cols="2">7/0/3</cell><cell cols="2">8/0/2</cell><cell cols="2">10/0/0</cell></row></table><note><p>row (W/L/T) in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average values of three different measures using ReliefF and ReliefS on numeric data.</figDesc><table><row><cell></cell><cell cols="2">Precision</cell><cell cols="2">Distance</cell><cell cols="2">Raw Distance</cell></row><row><cell></cell><cell cols="6">ReliefF ReliefS ReliefF ReliefS ReliefF ReliefS</cell></row><row><cell>Iris</cell><cell>1.0</cell><cell>1.0</cell><cell>0.020</cell><cell>0.013</cell><cell>0.054</cell><cell>0.019</cell></row><row><cell>Glass</cell><cell>0.988</cell><cell>0.992</cell><cell>0.141</cell><cell>0.090</cell><cell>0.069</cell><cell>0.046</cell></row><row><cell>WDBC</cell><cell>0.991</cell><cell>0.994</cell><cell>0.139</cell><cell>0.103</cell><cell>0.111</cell><cell>0.068</cell></row><row><cell>Balance</cell><cell>0.864</cell><cell>0.940</cell><cell>0.468</cell><cell>0.247</cell><cell>0.037</cell><cell>0.018</cell></row><row><cell>Pima-Indian</cell><cell>0.906</cell><cell>0.930</cell><cell>0.248</cell><cell>0.209</cell><cell>0.019</cell><cell>0.016</cell></row><row><cell>Vehicle</cell><cell>0.996</cell><cell>1.0</cell><cell>0.206</cell><cell>0.105</cell><cell>0.052</cell><cell>0.026</cell></row><row><cell>German</cell><cell>0.898</cell><cell>0.920</cell><cell>0.360</cell><cell>0.309</cell><cell>0.154</cell><cell>0.125</cell></row><row><cell>Segment</cell><cell>1.0</cell><cell>1.0</cell><cell>0.074</cell><cell>0.029</cell><cell>0.054</cell><cell>0.020</cell></row><row><cell>Abalone</cell><cell>0.947</cell><cell>0.971</cell><cell>0.257</cell><cell>0.176</cell><cell>0.003</cell><cell>0.001</cell></row><row><cell>Satimage</cell><cell>0.998</cell><cell>1.0</cell><cell>0.088</cell><cell>0.047</cell><cell>0.065</cell><cell>0.022</cell></row><row><cell>Waveform</cell><cell>1.0</cell><cell>1.0</cell><cell>0.080</cell><cell>0.056</cell><cell>0.047</cell><cell>0.036</cell></row><row><cell>Page-Blocks</cell><cell>1.0</cell><cell>1.0</cell><cell>0.202</cell><cell>0.043</cell><cell>0.006</cell><cell>0.003</cell></row><row><cell>CoIL2000</cell><cell>1.0</cell><cell>1.0</cell><cell>0.060</cell><cell>0.041</cell><cell>0.110</cell><cell>0.074</cell></row><row><cell>Shuttle</cell><cell>1.0</cell><cell>1.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.003</cell><cell>0.001</cell></row><row><cell>W/L/T</cell><cell cols="2">8/0/6</cell><cell cols="2">13/0/1</cell><cell cols="2">14/0/0</cell></row></table><note><p>we notice that both ReliefF and ReliefS perform equally well in Precision but differently in Distance and Raw Distance. Precision being 1 indicates</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Time savings by ReliefS w.r.t. ReliefF for numeric data. m Relief S and m Relief F are numbers of instances used by ReliefS and ReliefF to achieve the same performance in raw distance. Relief S for ReliefS. Table 7 records the running times T kd-tree , T Relief S and T Relief F as well as m Relief S and m Relief F . We can observe that: 1. The time savings are consistent with the time complexity analysis of Reli-efF: it is usually O(mkN ), or linear in N if m and k are fixed. Now, k and N are fixed, its time complexity is O(m). That is, the reduction of m results in direct time savings.</figDesc><table><row><cell></cell><cell></cell><cell>ReliefS</cell><cell></cell><cell cols="2">ReliefF</cell></row><row><cell></cell><cell cols="5">T kd-tree T Relief S m Relief S T Relief F m Relief F</cell></row><row><cell></cell><cell>(ms)</cell><cell>(ms)</cell><cell></cell><cell>(ms)</cell><cell></cell></row><row><cell>Iris</cell><cell>20</cell><cell>14</cell><cell>18</cell><cell>60</cell><cell>87</cell></row><row><cell>Glass</cell><cell>61</cell><cell>140</cell><cell>62</cell><cell>259</cell><cell>126</cell></row><row><cell>WDBC</cell><cell>455</cell><cell>1414</cell><cell>125</cell><cell>3282</cell><cell>313</cell></row><row><cell>Balance</cell><cell>136</cell><cell>223</cell><cell>88</cell><cell>743</cell><cell>313</cell></row><row><cell>Pima</cell><cell>246</cell><cell>484</cell><cell>77</cell><cell>2684</cell><cell>476</cell></row><row><cell>Vehicle</cell><cell>530</cell><cell>2874</cell><cell>195</cell><cell>6790</cell><cell>499</cell></row><row><cell>German</cell><cell>697</cell><cell>6274</cell><cell>420</cell><cell>8310</cell><cell>590</cell></row><row><cell>Segment</cell><cell>1520</cell><cell>10870</cell><cell>277</cell><cell>53300</cell><cell>1317</cell></row><row><cell>Abalone</cell><cell>1359</cell><cell>23879</cell><cell>961</cell><cell>58228</cell><cell>2423</cell></row><row><cell>Satimage</cell><cell>4630</cell><cell>63901</cell><cell>577</cell><cell>255716</cell><cell>2572</cell></row><row><cell>Waveform</cell><cell>7170</cell><cell>359840</cell><cell>1900</cell><cell>529860</cell><cell>2950</cell></row><row><cell>Page</cell><cell>1858</cell><cell>43774</cell><cell>1095</cell><cell>125709</cell><cell>3284</cell></row><row><cell>CoIL</cell><cell cols="2">14117 612203</cell><cell>1979</cell><cell>983601</cell><cell>3377</cell></row><row><cell>Shuttle</cell><cell>6058</cell><cell>187156</cell><cell>1885</cell><cell>802001</cell><cell>8700</cell></row><row><cell cols="6">is around 50% of the whole data set) and use the performance measured by Raw</cell></row><row><cell cols="3">Distance to find corresponding m</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Average values of three different measures using ReliefF and ReliefS on non-numeric data.</figDesc><table><row><cell></cell><cell cols="2">Precision</cell><cell cols="2">Distance</cell><cell cols="2">Raw Distance</cell></row><row><cell></cell><cell cols="6">ReliefF ReliefS ReliefF ReliefS ReliefF ReliefS</cell></row><row><cell>Breast-cancer</cell><cell>0.765</cell><cell>0.821</cell><cell>0.695</cell><cell>0.601</cell><cell>0.203</cell><cell>0.162</cell></row><row><cell>Primary-tumor</cell><cell>0.957</cell><cell>0.965</cell><cell>0.259</cell><cell>0.207</cell><cell>0.296</cell><cell>0.246</cell></row><row><cell>KRKPA7</cell><cell>0.970</cell><cell>0.977</cell><cell>0.103</cell><cell>0.070</cell><cell>0.152</cell><cell>0.101</cell></row><row><cell>Mushroom</cell><cell>1.0</cell><cell>1.0</cell><cell>0.065</cell><cell>0.032</cell><cell>0.141</cell><cell>0.063</cell></row><row><cell>Zoo</cell><cell>0.986</cell><cell>0.994</cell><cell>0.136</cell><cell>0.119</cell><cell>0.633</cell><cell>0.539</cell></row><row><cell>Autos</cell><cell>0.9</cell><cell>0.928</cell><cell>0.308</cell><cell>0.259</cell><cell>0.380</cell><cell>0.298</cell></row><row><cell>Colic</cell><cell>0.854</cell><cell>0.867</cell><cell>0.351</cell><cell>0.316</cell><cell>0.382</cell><cell>0.334</cell></row><row><cell>Vowel</cell><cell>1.0</cell><cell>1.0</cell><cell>0.056</cell><cell>0.023</cell><cell>0.043</cell><cell>0.021</cell></row><row><cell>Hypothyroid</cell><cell>0.965</cell><cell>0.981</cell><cell>0.110</cell><cell>0.085</cell><cell>0.065</cell><cell>0.050</cell></row><row><cell>W/L/T</cell><cell cols="2">7/0/2</cell><cell cols="2">9/0/0</cell><cell cols="2">9/0/0</cell></row><row><cell>6.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>2 Results on non-numeric data</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Time savings by ReliefS w.r.t. ReliefF for non-numeric data. m Relief S and m Relief F are numbers of instances used by ReliefS and ReliefF to achieve the same performance in raw distance.</figDesc><table><row><cell></cell><cell></cell><cell>ReliefS</cell><cell></cell><cell cols="2">ReliefF</cell></row><row><cell></cell><cell cols="5">T kd-tree T Relief S m Relief S T Relief F m Relief F</cell></row><row><cell></cell><cell>(ms)</cell><cell>(ms)</cell><cell></cell><cell>(ms)</cell><cell></cell></row><row><cell>Breast-cancer</cell><cell>90</cell><cell>234</cell><cell>123</cell><cell>300</cell><cell>166</cell></row><row><cell>Primary-tumor</cell><cell>175</cell><cell>546</cell><cell>112</cell><cell>836</cell><cell>176</cell></row><row><cell>KRKPA7</cell><cell>4559</cell><cell>84880</cell><cell>1055</cell><cell>145520</cell><cell>1950</cell></row><row><cell>Mushroom</cell><cell>7668</cell><cell>248336</cell><cell>1950</cell><cell>491665</cell><cell>4062</cell></row><row><cell>Zoo</cell><cell>50</cell><cell>73</cell><cell>40</cell><cell>105</cell><cell>65</cell></row><row><cell>Autos</cell><cell>161</cell><cell>417</cell><cell>82</cell><cell>568</cell><cell>123</cell></row><row><cell>Colic</cell><cell>303</cell><cell>1047</cell><cell>151</cell><cell>1518</cell><cell>221</cell></row><row><cell>Vowel</cell><cell>484</cell><cell>2633</cell><cell>218</cell><cell>6570</cell><cell>574</cell></row><row><cell>Hypothyroid</cell><cell>4950</cell><cell>98094</cell><cell>1094</cell><cell>173950</cell><cell>2112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Effectiveness of ReliefF on benchmark data: 10-fold cross validation accuracy (%) of 5-NN on original features (full sets without feature selection) and target sets of features chosen by ReliefF (with m = N ). P reports the probability associated with a Student's paired two-tailed t-Test.</figDesc><table><row><cell></cell><cell cols="2">Full set Target Set</cell><cell>P</cell></row><row><cell></cell><cell></cell><cell>(m = N )</cell><cell></cell></row><row><cell>Iris</cell><cell>96.67</cell><cell>97.33</cell><cell>0.32</cell></row><row><cell>Glass</cell><cell>65.80</cell><cell>71.95</cell><cell>0.07</cell></row><row><cell>WDBC</cell><cell>96.67</cell><cell>95.43</cell><cell>0.15</cell></row><row><cell>Balance</cell><cell>88.00</cell><cell>77.42</cell><cell>0.00</cell></row><row><cell cols="2">Pima-Indian 74.49</cell><cell>73.84</cell><cell>0.69</cell></row><row><cell>Vehicle</cell><cell>69.97</cell><cell>69.27</cell><cell>0.66</cell></row><row><cell>German</cell><cell>71.00</cell><cell>72.50</cell><cell>0.19</cell></row><row><cell>Segment</cell><cell>95.80</cell><cell>95.76</cell><cell>0.94</cell></row><row><cell>Abalone</cell><cell>53.72</cell><cell>52.79</cell><cell>0.17</cell></row><row><cell>Satimage</cell><cell>90.73</cell><cell>86.16</cell><cell>0.00</cell></row><row><cell>Waveform</cell><cell>79.20</cell><cell>83.74</cell><cell>0.00</cell></row><row><cell cols="2">Page-Blocks 95.87</cell><cell>93.92</cell><cell>0.00</cell></row><row><cell>CoIL2000</cell><cell>93.63</cell><cell>93.89</cell><cell>0.07</cell></row><row><cell>Shuttle</cell><cell>99.71</cell><cell>99.60</cell><cell>0.01</cell></row><row><cell>W/L/T</cell><cell></cell><cell>3/4/7</cell><cell></cell></row><row><cell cols="4">first set of comparisons is shown in Table 10 and the second set of comparisons is</cell></row><row><cell>shown in Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison of ReliefF and ReliefS on benchmark data: 10-fold cross validation accuracy (%) of 5-NN on target sets of features chosen by ReliefF (with m = N ), subsets of features chosen by ReliefF (with m ≈ 1 10 N ), and subsets of features chosen by ReliefS (with m ≈ 1 10 N ). P reports the probability associated with a Student's paired two-tailed t-Test.</figDesc><table><row><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Target Set</cell><cell>ReliefF</cell><cell>ReliefS</cell><cell></cell><cell>P</cell><cell></cell></row><row><cell></cell><cell cols="4">(m = N ) (m ≈ 1 10 N ) (m ≈ 1 10 N ) (A,B)</cell><cell>(A,C)</cell><cell>(B,C)</cell></row><row><cell>Iris</cell><cell>97.33</cell><cell>97.33</cell><cell>97.33</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Glass</cell><cell>71.95</cell><cell>65.84</cell><cell>67.23</cell><cell>0.01</cell><cell>0.04</cell><cell>0.40</cell></row><row><cell>WDBC</cell><cell>95.43</cell><cell>95.43</cell><cell>96.71</cell><cell>0.99</cell><cell>0.09</cell><cell>0.08</cell></row><row><cell>Balance</cell><cell>77.42</cell><cell>76.97</cell><cell>77.42</cell><cell>0.79</cell><cell>1</cell><cell>0.79</cell></row><row><cell>Pima-Indian</cell><cell>73.84</cell><cell>65.61</cell><cell>72.27</cell><cell>0.02</cell><cell>0.21</cell><cell>0.04</cell></row><row><cell>Vehicle</cell><cell>69.27</cell><cell>66.01</cell><cell>69.16</cell><cell>0.04</cell><cell>0.92</cell><cell>0.09</cell></row><row><cell>German</cell><cell>72.50</cell><cell>70.60</cell><cell>72.60</cell><cell>0.06</cell><cell>0.94</cell><cell>0.06</cell></row><row><cell>Segment</cell><cell>95.76</cell><cell>95.84</cell><cell>96.19</cell><cell>0.44</cell><cell>0.25</cell><cell>0.35</cell></row><row><cell>Abalone</cell><cell>52.79</cell><cell>52.79</cell><cell>54.51</cell><cell>1</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell>Satimage</cell><cell>86.16</cell><cell>85.75</cell><cell>86.11</cell><cell>0.18</cell><cell>0.84</cell><cell>0.27</cell></row><row><cell>Waveform</cell><cell>83.74</cell><cell>83.48</cell><cell>83.38</cell><cell>0.49</cell><cell>0.36</cell><cell>0.83</cell></row><row><cell>Page-Blocks</cell><cell>93.92</cell><cell>91.17</cell><cell>89.73</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>CoIL2000</cell><cell>93.89</cell><cell>93.77</cell><cell>93.77</cell><cell>0.15</cell><cell>0.15</cell><cell>1</cell></row><row><cell>Shuttle</cell><cell>99.60</cell><cell>99.60</cell><cell>99.60</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>W/L/T</cell><cell></cell><cell></cell><cell></cell><cell>0/5</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>/9 2/2/10 5/1/8</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Every node at each level of the tree only records indexes of the instances this node contains.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Numeric values should be normalized into the range between 0 and 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>One heuristic suggested in<ref type="bibr" target="#b24">[25]</ref> is to choose m = logN in an algorithm similar to Relief.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Doing so increases the complexity of the algorithm to O(kN 2 ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For these two functions, one relevant feature is just outside the selected set. This could be due to the inability of ReliefF to differentiate redundant features with strong correlation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>The larger the data set, the more savings in time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_6"><p>The ratio of T kd-tree /T Relief S decreases when data size increases. In other words, the time spent on kd-tree building becomes immaterial when T Relief S increases. This is consistent with earlier theoretical analysis.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Bret Ehlert, Feifang Hu, Manoranjan Dash, Hongjun Lu, and Lance Parsons for their contributions to this work. We are grateful to the anonymous reviewers who have provided many helpful and constructive suggestions on an earlier version of this paper. An earlier short version of this work was published in the proceedings of the 19th International Conference on Machine learning, 2002. This work is in part based on the project supported by National Science Foundation under Grant No. IIS-0127815 for H. Liu, and on Grant-in-Aid for Scientific Research on Priority Areas (B), No. 759: Active Mining Project by Ministry of Education, Culture, Sports, Science and Technology of Japan for H. Motoda.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database mining: A performance perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="914" to="925" />
			<date type="published" when="1993-12">December 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Very Large Data Bases</title>
		<meeting>Int. Conf. Very Large Data Bases<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09">September 1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Bay</surname></persName>
		</author>
		<ptr target="http://kdd.ics.uci.edu" />
		<title level="m">The UCI KDD archive</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI Repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/∼mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selection of relevant features and examples in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="245" to="271" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling clustering algorithms to large databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the Fourth International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI PRESS</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Cochran</surname></persName>
		</author>
		<title level="m">Sampling Techniques</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving generalization with active learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Filters, wrappers and a boosting-based hybrid for feature selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An International Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature selection for clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Pacific Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Fourth Pacific Asia Conference on Knowledge Discovery and Data Mining<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
	<note>PAKDD-2000</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Consistency based feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Pacific Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Fourth Pacific Asia Conference on Knowledge Discovery and Data Mining<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
	<note>PAKDD-2000</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of unsupervised data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth IEEE International Conference on Tools with AI (ICTAI&apos;97)</title>
		<meeting>the Ninth IEEE International Conference on Tools with AI (ICTAI&apos;97)<address><addrLine>Newport Beach, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997-11">November, 1997. 1997</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<title level="m">Machine learning research: Four current directions. AI Magazine</title>
		<meeting><address><addrLine>Winter</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="97" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature subset selection and order identification for unsupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualization and interactive feature selection for unsupervised data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="360" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selective sampling using the query by committee algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="133" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for finding best matches in logarithmic expected time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multidimensional access methods</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gaede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Günther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="231" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Sampling: Knowing Whole from Its Part</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="21" to="38" />
		</imprint>
	</monogr>
	<note>Liu and Motoda</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Correlation Based Feature Selection for Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Waikato, Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correlation-based feature selection for discrete and numeric class machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning (ICML-00)</title>
		<meeting>the Seventeenth International Conference on Machine Learning (ICML-00)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Use of contextual information for feature ranking and discretization</title>
		<author>
			<persName><forename type="first">June</forename><surname>Se</surname></persName>
		</author>
		<author>
			<persName><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="718" to="730" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 10th European Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Nedellec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Rouveirol</surname></persName>
		</editor>
		<meeting>10th European Conference on Machine Learning<address><addrLine>Chemnitz, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature selection for unsupervised learning via evolutionary search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="365" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The feature selection problem: Traditional methods and a new algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth National Conference on Artificial Intelligence</title>
		<meeting>the Tenth National Conference on Artificial Intelligence<address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press/The MIT Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="129" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical approach to feature selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Machine Learning (ICML-92)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Sleeman</surname></persName>
		</editor>
		<editor>
			<persName><surname>Edwards</surname></persName>
		</editor>
		<meeting>the Ninth International Conference on Machine Learning (ICML-92)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The power of sampling in knowledge discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD/PODS&apos; 94</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating attributes : Analysis and extension of RELIEF</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bergadano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</editor>
		<meeting>the European Conference on Machine Learning<address><addrLine>Catania, Italy; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994-08">April 6-8. 1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overcoming the myopia of inductive learning algorithms with RELIEFF</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selection of relevant features in machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fall Symposium on Relevance</title>
		<meeting>the AAAI Fall Symposium on Relevance</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines. how to represent texts in input space?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Leopold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="423" to="444" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Annual ACM-SIGR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Seventeenth Annual ACM-SIGR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<title level="m">Instance Selection and Construction for Data Mining</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature selection with selective sampling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning</title>
		<meeting>the Nineteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="395" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An introductory tutorial on kd-trees</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<idno>No. 209</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge, Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Extract from Ph.D. Thesis Tech Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On feature selection: learning with exponentially many irrelevant features as training examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="404" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Customer retention via data mining</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="569" to="590" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An adaptation of relief for attribute estimation in regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourteenth International Conference on Machine Learning</title>
		<meeting>Fourteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attribute dependencies, understandability and split selection in tree based models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sixteenth International Conference on Machine Learning</title>
		<meeting>Sixteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comprehensible interpretation of relief&apos;s estimates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighteenth International Conference on Machine Learning</title>
		<meeting>Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Theoretical and empirical analysis of Relief and ReliefF</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="23" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Toward optimal active learning through sampling estimation of error reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Less is more: Active learning with support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Speeding up Relief algorithms with k-d trees</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Electrotechnical and Computer Science Conference ERK&apos;98</title>
		<meeting>the Electrotechnical and Computer Science Conference ERK&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A study of support vectors on model independent example selection</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD, International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</editor>
		<meeting>ACM SIGKDD, International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="272" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature selection as a preprocessing step for hierarchical clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Talavera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Internationl Conference on Machine Learning (ICML&apos;99)</title>
		<meeting>Internationl Conference on Machine Learning (ICML&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="389" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Active learning for natural language parsing and information extraction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Data Mining -Pracitcal Machine Learning Tools and Techniques with JAVA Implementations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional genomic microarray data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: a fast correlation-based filter solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth International Conference on Machine Learning</title>
		<meeting>the twentieth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
