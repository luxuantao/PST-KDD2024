<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SQL-on-Hadoop: Full Circle Back to Shared-Nothing Database Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Avrilia</forename><surname>Floratou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Umar</forename><forename type="middle">Farooq</forename><surname>Minhas</surname></persName>
							<email>ufminhas@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fatma</forename><surname>Özcan</surname></persName>
							<email>fozcan@us.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SQL-on-Hadoop: Full Circle Back to Shared-Nothing Database Architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B88D52080F01D012D1266060D75803C5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SQL query processing for analytics over Hadoop data has recently gained significant traction. Among many systems providing some SQL support over Hadoop, Hive is the first native Hadoop system that uses an underlying framework such as MapReduce or Tez to process SQL-like statements. Impala, on the other hand, represents the new emerging class of SQL-on-Hadoop systems that exploit a shared-nothing parallel database architecture over Hadoop. Both systems optimize their data ingestion via columnar storage, and promote different file formats: ORC and Parquet. In this paper, we compare the performance of these two systems by conducting a set of cluster experiments using a TPC-H like benchmark and two TPC-DS inspired workloads. We also closely study the I/O efficiency of their columnar formats using a set of micro-benchmarks. Our results show that Impala is 3.3X to 4.4X faster than Hive on MapReduce and 2.1X to 2.8X than Hive on Tez for the overall TPC-H experiments. Impala is also 8.2X to 10X faster than Hive on MapReduce and about 4.3X faster than Hive on Tez for the TPC-DS inspired experiments. Through detailed analysis of experimental results, we identify the reasons for this performance gap and examine the strengths and limitations of each system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Enterprises are using Hadoop as a central data repository for all their data coming from various sources, including operational systems, social media and the web, sensors and smart devices, as well as their applications. Various Hadoop frameworks are used to manage and run deep analytics in order to gain actionable insights from the data, including text analytics on unstructured text, log analysis over semi-structured data, as well as relational-like SQL processing over semi-structured and structured data.</p><p>SQL processing in particular has gained significant traction, as many enterprise data management tools rely on SQL, and many enterprise users are familiar and comfortable with it. As a result, the number of SQL-on-Hadoop systems have increased significantly. We can classify these systems into two general categories: Native Hadoop-based systems, and database-Hadoop hybrids. In the first category, Hive <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">21]</ref> is the first SQL-like system over Hadoop, which uses another framework such as MapReduce or Tez to process SQL-like queries, leveraging its task scheduling and load balancing features. Shark <ref type="bibr" target="#b0">[7,</ref><ref type="bibr" target="#b23">24]</ref> is somewhat similar to Hive in that it uses another framework, Spark <ref type="bibr" target="#b7">[8]</ref> as its runtime. In this category, Impala <ref type="bibr" target="#b9">[10]</ref> moved away from MapReduce to a sharednothing parallel database architecture. Impala runs queries using its own long-running daemons running on every HDFS DataNode, and instead of materializing intermediate results, pipelines them between computation stages. Similar to Impala, LinkedIn Tajo <ref type="bibr" target="#b19">[20]</ref>, Facebook Presto <ref type="bibr" target="#b16">[17]</ref>, and MapR Drill <ref type="bibr" target="#b4">[4]</ref>, also resemble parallel databases and use long-running custom-built processes to execute SQL queries in a distributed fashion.</p><p>In the second category, Hadapt <ref type="bibr" target="#b2">[2]</ref> also exploits Hadoop scheduling and fault-tolerance, but uses a relational database (PostgreSQL) to execute query fragments. Microsoft PolyBase <ref type="bibr" target="#b10">[11]</ref> and Pivotal HAWQ <ref type="bibr" target="#b8">[9]</ref>, on the other hand, use database query optimization and planning to schedule query fragments, and directly read HDFS data into database workers for processing. Overall, we observe a big convergence to shared-nothing database architectures among the SQL-on-Hadoop systems.</p><p>In this paper, we focus on the first category of native SQL-on-Hadoop systems, and investigate the performance of Hive and Impala, highlighting their different design trade-offs through detailed experiments and analysis. We picked these two systems due to their popularity as well as their architectural differences. Impala and Hive are the SQL offerings from two major Hadoop distribution vendors, Cloudera and Hortonworks. As a result, they are widely used in the enterprise. There are other SQL-on-Hadoop systems, such as Presto and Tajo, but these systems are mainly used by companies that created them, and are not as widely used. Impala is a good representative of emerging SQL-on-Hadoop systems, such as Presto, and Tajo, which follow a shared-nothing database like architecture with custom-built daemons and custom data communication protocols. Hive, on the other hand, represents another architectural design where a run-time framework, such as MapReduce or Tez, is used for scheduling, data movement, and parallelization.</p><p>Hive and Impala are used for analytical queries. Columnar data organization has been shown to improve the performance of such queries in the context of relational databases <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b17">18]</ref>. As a result, columnar storage organization is also utilized for analytical queries over Hadoop data. RCFile <ref type="bibr" target="#b13">[14]</ref> and Trevni <ref type="bibr" target="#b22">[23]</ref> are early columnar file formats for HDFS data. While relational databases store each column data in a separate file, in Hadoop and HDFS, all columns are packed into a single file, similar to the PAX <ref type="bibr" target="#b3">[3]</ref> data layout. Floratou et.al. <ref type="bibr" target="#b11">[12]</ref> pointed out to several performance problems in RCFile. To address these shortcomings, Hortonworks and Microsoft propose the ORC file format, which is the next generation of RCFile. Impala, on the other hand, is promoting the Parquet file format, which is a columnar format inspired by Dremel's <ref type="bibr" target="#b15">[16]</ref> storage format. These two formats compete to become the defacto standard for Hadoop data storage for analytical queries.</p><p>In this paper, we delve into the details of these two storage formats to gain insights into their I/O characteristics. We conduct a detailed analysis of Impala and Hive using TPC-H data, stored as text, Parquet and ORC file formats. We also conduct experiments using a TPC-DS inspired workload. We also investigate the columnar storage formats in conjunction with the system promoting them.</p><p>Our experimental results show that: • Impala's database-like architecture provides significant performance gains, compared to Hive's MapReduce or Tez based runtime. Hive on MapReduce is also impacted by the startup and scheduling overheads of the MapReduce framework, and pays the cost of writing intermediate results into HDFS. Impala, on the other hand, streams the data between stages of the query computation, resulting in significant performance improvements. But, Impala cannot recover from mid-query failures yet, as it needs to re-run the whole query in case of a failure. Hive on Tez eliminates the startup and materialization overheads of MapReduce, improving Hive's overall performance. However, it does not avoid the Java deserialization overheads during scan operations and thus it is still slower than Impala.</p><p>• While Impala's I/O subsystem provides much faster ingestion rates, the single threaded execution of joins and aggregations impedes the performance of complex queries.</p><p>• The Parquet format skips data more efficiently than the ORC file which tends to prefetch unnecessary data especially when a table contains a large number of columns. However, the built-in index in ORC file mitigates that problem when the data is sorted.</p><p>Our results reaffirm the clear advantages of a shared-nothing database architecture for analytical SQL queries over a MapReducebased runtime. It is interesting to see the community to come full circle back to parallel database architectures, after the extensive comparisons and discussions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The rest of the paper is organized as follows: in Section 2, we first review the Hive and Impala architectures, as well as the ORC and the Parquet file formats. In Section 3, we present our analysis on the cluster experiments. We investigate the I/O rates and effect of the ORC file's index using a set of micro-benchmarks in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Apache Hive</head><p>Hive <ref type="bibr" target="#b6">[6]</ref> is the first SQL-on-Hadoop system built on top of Apache Hadoop <ref type="bibr" target="#b5">[5]</ref> and thus exploits all the advantages of Hadoop including: its ability to scale out to thousands or tens of thousands of nodes, fault tolerance and high availability.</p><p>Hive implements a SQL-like query language namely HiveQL. HiveQL statements submitted to Hive are parsed, compiled, and optimized to produce a physical query execution plan. The plan is a Directed Acyclic Graph (DAG) of MapReduce tasks which is executed through the MapReduce framework or through the Tez framework in the latest Hive version. A HiveQL query is executed in a single Tez job when the Tez framework is used and is typically split into multiple MapReduce jobs when the MapReduce framework is used. Tez pipelines data through execution stages instead of creating temporary intermediate files. Thus, Tez avoids the startup, scheduling and materialization overheads of MapReduce.</p><p>The choice of join method and the order of joins significantly impact the performance of analytical queries. Hive supports two types of join methods. The most generic form of join is called a repartitioned join, in which multiple map tasks read the input tables, emitting the join (key, value) pair, where the key is the join column. These pairs are shuffled and distributed (over-the-network) to the reduce tasks, which perform the actual join operation. A second, more optimized, form of join supported by Hive is called map-side join. Its main goal is to eliminate the shuffle and reduce phases of the repartitioned join. Map-side join is applicable when one of the table is small. All the map tasks read the small table and perform the join processing in a map only job. Hive also supports predicate pushdown and partition elimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cloudera Impala</head><p>Cloudera Impala <ref type="bibr" target="#b9">[10]</ref> is another open-source project that implements SQL processing capabilities over Hadoop data, and has been inspired by Google's Dremel <ref type="bibr" target="#b15">[16]</ref>. Impala supports a SQL-like query language which is a subset of SQL as well.</p><p>As opposed to Hive, instead of relying on another framework, Impala provides its own long running daemons on each node of the cluster and has a shared-nothing parallel database architecture. The main daemon process impalad comprises the query planner, query coordinator, and the query execution engine. Impala supports two types of join algorithms: partitioned and broadcast, which are similar to Hive's repartitioned join and map-side join, respectively. Impala compiles all queries into a pipelined execution plan.</p><p>Impala has a highly efficient I/O layer to read data stored in HDFS which spawns one I/O thread per disk on each node. The Impala I/O layer decouples the requests to read data (which is asynchronous), and the actual reading of the data (which is synchronous), keeping disks and CPUs highly utilized at all times. Impala also exploits the Streaming SIMD Extension (SSE) instructions, available on the latest generation of Intel processors to parse and process textual data efficiently.</p><p>Impala still requires the working set of a query to fit in the aggregate physical memory of the cluster it runs on. This is a major limitation, which imposes strict constraints on the kind of datasets one is able to process with Impala.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Columnar File Formats</head><p>Columnar data organizations reduce disk I/O, and enable better compression and encoding schemes that significantly benefit analytical queries <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">1]</ref>. Both Hive and Impala have implemented their own columnar storage formats, namely Hive's Optimized Row Columnar (ORC) format and Impala's Parquet columnar format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Optimized Row Columnar (ORC) File Format</head><p>This file format is an optimized version of the previously published Row Columnar (RC) file format <ref type="bibr" target="#b13">[14]</ref>. The ORC file format provides storage efficiency by providing data encoding and block-level compression as well as better I/O efficiency through a lightweight built-in index that allows skipping row groups that do not pass a filtering predicate. The ORC file packs columns of a table in a PAX <ref type="bibr" target="#b3">[3]</ref> like organization within an HDFS block. More specifically, an ORC file stores multiple groups of row data as stripes. Each ORC file has a file footer that contains a list of stripes in the file, the number of rows stored in each stripe, the data type of each column, and column-level aggregates such as count, sum, min, and max. The stripe size is a configurable parameter and is typically set to 256 MB. Hive automatically sets the HDFS block size to min(2 * stripe size, 1.5GB), in order to ensure that the I/O requests operate on large chunks of local data.</p><p>Internally, each stripe is divided into index data, row data, and stripe footer, in that order. The data in each column consists of multiple streams. For example, integer columns consist of two streams: the present bit stream which denotes whether the value is null and the actual data stream. The stripe footer contains a directory of the stream locations of each column. As part of the index data, min and max values for each column are stored, as well as the row positions within each column. Using the built-in index, row groups (default of 10, 000 rows) can be skipped if the query predicate does not fall within the minimum and maximum values of the row group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Parquet File Format</head><p>The Parquet columnar format has been designed to exploit efficient compression and encoding schemes and to support nested data. It stores data grouped together in logical horizontal partitions called row groups. Every row group contains a column chunk for each column in the table. A column chunk consists of multiple pages and is guaranteed to be stored contiguously on disk. Parquet's compression and encoding schemes work at a page level. Metadata is stored at all the levels in the hierarchy i.e., file, column chunk, and page. The Parquet readers first parse the metadata to filter out the column chunks that must be accessed for a particular query and then read each column chunk sequentially.</p><p>Impala heavily makes use of the Parquet storage format but does not support nested columns yet. In Impala, all the data for each row group are kept in a separate Parquet data file stored on HDFS, to make sure that all the columns of a given row are stored on the same node. Impala automatically sets the HDFS block size and the Parquet file size to a maximum of 1 GB. In this way, I/O and network requests apply to a large chunk of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CLUSTER EXPERIMENTS</head><p>In this section, we explore the performance of Hive and Impala using a TPC-H like workload and two TPC-DS inspired workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hardware Configuration</head><p>For the experiments presented in this section, we use a 21 node cluster. One of the nodes hosts the HDFS NameNode and secondary namenode, the Impala StateStore, and Catalog processes and the Hive Metastore ("control" node). The remaining 20 nodes are designated as "compute" nodes. Every node in the cluster has 2x Intel Xeon CPUs @ 2.20GHz, with 6x physical cores each (12 physical cores total), 11x SATA disks (2TB, 7k RPM), 1x 10 Gigabit Ethernet card, and 96GB of RAM. Out of the 11 SATA disks, we use one for the operating system, while the remaining 10 disks are used for HDFS. Each node runs 64-bit Ubuntu Linux 12.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Software Configuration</head><p>For our experiments, we use Hive version 0.12 (Hive-MR) and Impala version 1.2.2 on top of Hadoop 2.0.0-cdh4.5.0. We also use Hive version 0.13 (Hive-Tez) on top of Tez 0.3.0 and Apache Hadoop 2.3.0. <ref type="foot" target="#foot_0">1</ref> We configured Hadoop to run 12 containers per node (1 per core). The HDFS replication factor is set to 3 and the maximum JVM heap size is set to 7.5 GB per task. We configured Impala to use MySQL as the metastore. We run one impalad process on each compute node and assign 90 GB of memory to it. We enable short-circuit reads and block location tracking as per the Impala instructions. Short-circuit reads allow reading local data directly from the filesystem. For this reason, this parameter is also enabled for the Hive experiments. Runtime code generation in Impala is enabled in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TPC-H like Workload</head><p>For our experiments, we use a TPC-H<ref type="foot" target="#foot_1">2</ref> like workload, which has been used by both the Hive and Impala engineers to evaluate their systems. The workload scripts for Hive and Impala have been published by the developers of these systems 3, 4 .</p><p>The workload contains the 22 read-only TPC-H queries but not the refresh streams. We use a TPC-H database with a scale factor of 1000 GB. We were not able to scale to larger TPC-H datasets because of Impala's limitation to require the workload's working set to fit in the cluster's aggregate memory. However, as we will show in our analysis, this dataset can provide significant insights into the strengths and limitations of each system.</p><p>Although the workload scripts for both Hive and Impala are available online, these do not take into account some of the new features of these systems. Hive has now support for nested sub-queries. In order to exploit this feature, we re-wrote 11 TPC-H queries. The remaining queries could not be further optimized since they already consist of a single query. Impala was able to execute these rewritten queries and produce correct results, except for Query 18, which produced incorrect results when nested and had to be split into two queries.</p><p>In Hive-MR, we enabled various optimizations including: a) optimization of correlated queries, b) predicate push-down and index filtering when using the ORC file format, and c) map-side join and aggregation. When a query is split into multiple sub-queries, the intermediate results between sub-queries were materialized into temporary tables in HDFS using the same file format/compression codec with the base tables. Hive, when used out of the box, typically generates a large number of reduce tasks that tend to negatively affect performance. We experimented with various numbers of reduce tasks for the workload and found that in our environment 120 reducers produce the best performance since all the reduce tasks can complete in one reduce round. In Hive-Tez, we additionally enabled the vectorized execution engine.</p><p>For Impala, we computed table and column statistics for each table using the COMPUTE STATS statement. These statistics are used to optimize the join order and choose the join methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Preparation and Load Times</head><p>In this section, we describe the loading process in Hive and Impala. We used the ORC file format in Hive and the Parquet file format in Impala which are the popular columnar formats that each system advertises. We also experimented with the text file format (TXT) in order to examine the performance of each system when the data arrives in native uncompressed text format and needs to be processed right away. We experimented with all the compression algorithms that are available for each columnar layout, namely snappy and zlib for the ORC file format and snappy and gzip for the Parquet format. Our results show that snappy compression provides slightly better query performance than zlib and gzip, for the TPC-H workload, on both file formats. For this reason, we only report results with snappy compression in both Hive and Impala.</p><p>We generated the data files using the TPC-H generator at 1TB scale factor and copied them into HDFS as plain text. We use a Hive query to convert the data from the text format to the ORC format. Similarly, we used an Impala query to load the data into the Parquet tables.</p><p>Table <ref type="table" target="#tab_0">2</ref> presents the time to load all the TPC-H tables in each system, for each format that we used. It also shows the aggre-  gate size of the tables. <ref type="foot" target="#foot_4">5</ref> In this table and in the following sections, we will use term ORC or Parquet to refer to uncompressed ORC and Parquet files respectively. When compression is applicable the compression codec will be specified. As a side note, the total table size for the zlib compressed ORC file tables is approximately 237 GB in Hive-MR and 212 GB in Hive-Tez and that of the gzip compressed Parquet tables is 222 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Overall Performance: Hive vs. Impala</head><p>In this experiment, we run the 22 TPC-H queries, one after the other, for both Hive and Impala and measure the execution time for each query. Before each full TPC-H run, we flush the file cache at all the compute nodes. We performed three full runs for each file format that we tested. For each query, we report the average response time across the three runs.</p><p>Table <ref type="table" target="#tab_1">1</ref> presents the running time of the queries in Hive and Impala for each TPC-H query and for each file format that we examined. The last column of the table presents the ratio between Hive-Tez's response time over Impala's response time for the snappy compressed ORC file and the snappy compressed Parquet format respectively. The table also contains the arithmetic (AM) and geometric (GM) mean of the response times for all file formats. The values of AM-Q{9,21} and GM-Q{9,21} correspond to the arithmetic and geometric mean of all the queries but Query 9 and Query 21. Query 9 did not complete in Impala due to lack of sufficient memory and Query 21 did not complete in Hive-Tez when the ORC file format was used since a container was automatically killed by the system after the query ran for about 2000 seconds.</p><p>Figures <ref type="figure">1</ref> and<ref type="figure">2</ref> present an overview of the performance results for the TPC-H queries for the various file formats that we used, for both Hive and Impala. Figure <ref type="figure">1</ref> shows the normalized arithmetic mean of the response times for the TPC-H queries, and Figure <ref type="figure">2</ref> shows the normalized geometric mean of the response times; the numbers plotted in Figures <ref type="figure">1</ref> and<ref type="figure">2</ref> are normalized to the response times of Impala when the snappy-compressed Parquet file format is used. These numbers were computed based on the AM-Q{9,21} and GM-Q{9,21} values.</p><p>Figures <ref type="figure">1</ref> and<ref type="figure">2</ref> show that Impala outperforms both Hive-MR and Hive-Tez for all the file formats, with or without compression. Impala (using the compressed Parquet format) is about 5X, 3.5X, and 3.3X faster than Hive-MR and 3.1X, 2.3X and 2, 1X faster than Hive-Tez using TXT, uncompressed and snappy compressed ORC file formats, respectively, when the arithmetic mean is the metric used to compare the two systems. As shown in Table <ref type="table" target="#tab_1">1</ref>, the performance gains obtained in Impala vary from 1.5X to 13.5X.</p><p>An interesting observation is that Impala's performance does not significantly improve when moving from the TXT format to the Parquet columnar format. Moreover, snappy compression does not further improve Impala's performance when the Parquet file format is used. This observation suggests that I/O is not Impala's bottleneck for the majority of the TPC-H queries. On the other hand, Hive's performance improves by approximately 30% when the ORC file is used, instead of the simpler TXT file format.</p><p>From the results presented in this section, we conclude that Impala is able to handle TPC-H queries much more efficiently than Hive, when the workload's working set fits in memory. As we will show in the following section, this behavior is attributed to the following reasons: (a) Impala has a far more efficient I/O subsystem than Hive-MR and Hive-Tez, (b) Impala relies on its own long running, daemon processes on each node for query execution and thus does not pay the overhead of the job initialization and scheduling introduced by the MapReduce framework in Hive-MR, and (c) Impala's query execution is pipelined as opposed to Hive-MR's MapReduce-based query execution which enforces data materialization at each step.</p><p>We now analyze four TPC-H queries in both Hive-MR and Impala, to gather some insights into the performance differences. We also point out how query behavior changes for Hive-Tez.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Query 1</head><p>The TPC-H Query 1 is a query that scans the lineitem table, applies an inequality predicate, projects a few columns, performs an aggregation and sorts the final result. This query can reveal how efficiently the I/O layer is engineered in each system and also demonstrates the impact of each each file format.</p><p>In Hive-MR, the query consists of two MapReduce jobs, MR1 and MR2. During the map phase, MR1 scans the lineitem table, applies the filtering predicate and performs a partial aggregation. In the reduce phase, it performs a global aggregation and stores Impala breaks the query into three fragments, namely F0, F1 and F2. F2 scans the lineitem table, applies the predicate, and performs a partial aggregation. This fragment runs on all the compute nodes. The output of each node is shuffled across all the nodes based on the values of the grouping columns. Since there are 4 distinct value pairs for the grouping columns, only 4 compute nodes receive data. Fragment F1 merges the partial aggregates on each node and generates 1 row of data at each of the 4 compute nodes. Then, it streams the data to a single node that executes Fragment F0. This fragment merges all the partial aggregates and sorts the final result. Table <ref type="table" target="#tab_3">4</ref> shows the time spent in each fragment. Fragment F0 is not included in the table, since its response time is in the order of a few milliseconds. As shown in the table, the use of the Parquet file improves F2's running time by 3.4X. As shown in tables 3 and 4, Impala can provide significantly faster I/O read rates compare to Hive-MR. One main reason for this behavior, is that Hive-MR is based on the MapReduce framework for query execution, which in turn means that it pays the overhead of starting multiple map tasks when reading a large file, even when a small portion of the file needs to be actually accessed. For example, in Query 1, when the uncompressed ORC file format is used, 1872 map tasks (16 map task rounds) were launched to read a total of 196 GB. Hive-Tez avoids the startup overheads of the MapReduce framework and thus provides a significant performance boost. However, we observed that during the scan operation, both Hive-MR and Hive-Tez are CPU-bound. All the cores are fully utilized at all the machines and the disk utilization is less than 20%. This observation is consistent with previous work <ref type="bibr" target="#b11">[12]</ref>. The Java deserialization overheads become the bottleneck during the scan of the input files. However, switching to a columnar format helps reduce the amount of data that is deserialized and thus reduces the overall scan time.</p><p>Impala runs one impalad daemon on each node. As a result it does not pay the overhead of starting and scheduling multiple tasks per node. Impala also launches a set of scanner and reader threads on each node in order to efficiently fetch and process the bytes read from disk. The reader threads read the bytes from the underlying storage system and forward them to the scanner threads which parse them and materialize the tuples. This multi-threaded execution model is much more efficient than Hive-MR's MapReduce-based data reading. Another reason, for Impala's superior read performance over Hive-MR is that the Parquet files are generally smaller than the corresponding ORC files. For example, the size of the Parquet snappy compressed lineitem table is 196 GB, whereas the snappy compressed ORC file is 223 GB on disk. During the scan, Impala is disk-bound with each disk being at least 85% utilized. During this experiment, code generation is enabled in Impala. The impact of code generation for the aggregation operation is significant. Without code generation, the query becomes CPUbound (one core per machine is fully utilized due to the singlethreaded execution of aggregations) and the disks are under-utilized (&lt; 20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Query 17</head><p>The TPC-H query 17 is about 2X faster in Impala than Hive-MR. Compared to other workload queries, like Query 22, this query does not get significant benefits in Impala.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the operations in Query 17. The query first applies a selection predicate on the part table and performs a join between the resulting table and the lineitem table (JOIN1). The lineitem table is aggregated on the l partkey attribute (AGG1) and the output is joined with the output of JOIN1 again on the l partkey attribute (JOIN2). Finally, a selection predicate is applied, the output is aggregated (AGG2) and the final result is produced. Note that in this query one join operation (JOIN1) and one aggregation (AGG1) take as input the lineitem table. Moreover, these operations as well as the final join operation (JOIN2) have the same partitioning key, namely l partkey. Hive-MR's "correlation" optimization feature is able to identify (a) the common input of the AGGR1 and JOIN1 operations and (b) the common partitioning key between AGG1, JOIN1 and JOIN2. Thus instead of launching a single MapReduce job for each of these operations, Hive-MR generates an optimized query plan that consists of two MapReduce jobs only, namely MR1 and MR2. In the map phase of MR1, the lineitem and part tables are scanned, the selection predicates are applied and a partial aggregation is performed on the lineitem table on the l partkey attribute. Note that because both JOIN1 and AGG1 get as input the lineitem table, they share a single scan of the table. provides a significant performance boost, since the lineitem table is the largest table in the workload, requiring multiple map task rounds when scanned. In the reduce phase, a global aggregation on the lineitem table is performed, the two join operations are executed and a partial aggregation of the output is performed. This is feasible because all the join operations share the same partitioning key. The second MapReduce job, MR2, reads the output of MR1 and performs a global aggregation. Table <ref type="table" target="#tab_4">5</ref> presents the time breakdown for Query 17, for the various file formats that we used. We observed that Hive is CPU-bound during the execution of this query. All the cores are fully utilized (95% -100%), irrespective of the file format used. The disk utilization is less than 5%.</p><p>Impala is not able to identify that the AGG1 and JOIN1 operations share a common input, and thus scans the lineitem table twice. However, this does not have a significant impact in Impala's overall execution time since during the second scan, the lineitem is cached in the OS file cache. Moreover, Impala avoids the data materialization overhead that occurs between the map and reduce tasks of Hive-MR's MR1. Impala breaks the query into 5 fragments (F0, F1, F2, F3 and F4). Fragment F4 scans the part table and broadcasts it to all the compute nodes. Fragment F3 scans the lineitem table, performs a partial aggregation on each node, and shuffles the results to all the nodes using the l partkey attribute as the partitioning key. Fragment F2 merges the partial aggregates on each node (AGG1) and broadcasts the results to all the nodes. Fragment F1 scans the lineitem table and performs the join between this table and the part table (JOIN1). Then, the output of this operation is joined with the aggregated lineitem table (JOIN2). The result of this join is partially aggregated and streamed to the node that executes Fragment F0. Fragment F0 merges the partial aggregates (AGG2) and produces the final result. Impala, executes both join operations using a broadcast algorithm, as opposed to Hive where both joins are executed in the reduce phase of MR1.</p><p>Table <ref type="table" target="#tab_5">6</ref> presents the time spent on each fragment in Impala. As shown in the table, the bulk of the time is spent executing fragments F2 and F3. The output of the nmon 6 performance tool shows that during the execution of these fragments, only one CPU core is used and is 100% utilized. This happens because Impala uses a single thread per node to perform aggregations. The disks are approximately 10% utilized. We also observed that one core is fully utilized on each node, during the JOIN1 and JOIN2 operations. However, the time spent on the join operations (fragment F1) is low compared to the time spent in the AGG1 operation and thus the major bottleneck in this query is the single-threaded aggregation operation. For this reason, using the Parquet file format instead of the TXT format does not provide significant benefit.</p><p>In summary, the analysis of this query shows that Hive-MR's "correlation" optimization helps shrinking the gap between Hive-MR and Impala on this particular query, since it avoids redundant scans on the data and also merges multiple costly operations into a single MapReduce job. This query does not benefit much from the Tez framework -the performance of Hive-Tez is comparable to that of Hive-MR. Although Hive-Tez avoids the task startup, scheduling and materialization overheads of MapReduce, it scans the lineitem table twice. Thus, Tez pipelining and container reuse do not to a significant performance boost in this particular query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Query 19</head><p>As shown in Table <ref type="table" target="#tab_1">1</ref>, Query 19 is slightly faster in Hive than Impala. This query performs a join between the lineitem and part tables and then applies a predicate that touches both tables and consists of a set of equalities, inequalities and regular expressions connected with AND and OR operators. After this predicate is applied, an aggregation is performed. Hive-MR splits the query into two MapReduce jobs, namely MR1 and MR2. MR1 scans the lineitem and part tables in the map phase. In the reduce phase, it performs the join between the two tables, applies the filtering predicate, performs a partial aggregation and writes the results back to HDFS. MR2 reads the output of the previous job and performs the global aggregation.</p><p>Impala breaks the query into 3 fragments F0, F1, F2. Each node in the cluster executes fragments F1 and F2. Fragment F0 is executed on one node only. Fragment F2 scans the part table and broadcasts it to all the nodes. Fragment F1 first builds a hash table on the part table. Then the lineitem table is scanned and a join is performed between the two tables. During the join the complex query predicate is evaluated. The qualified rows are then partially aggregated. The output of fragment F1 is transmitted to the node that executes fragment F0. This node performs the global aggregation and produces the final result.</p><p>Tables <ref type="table" target="#tab_6">7</ref> and<ref type="table" target="#tab_7">8</ref> show the time breakdown for Query 19 in Hive-MR and Impala respectively, for the various file formats that we tested.</p><p>As shown in Table <ref type="table" target="#tab_7">8</ref>, fragment F2 which performs the scan of the part table is slower when the data is stored in text format. The scan is about 1.2X times faster using the Parquet file format. During the execution of Fragment F2, all the cores are approximately 20% -30% utilized, and there is increased network activity due to the broadcast operation. The bulk of the query time is spent executing fragment F1. Using the nmon performance tool, we observed that during the execution of the join operation only one CPU core is used on each node and runs at full capacity (100%). This is consistent with Impala's limitation of executing join operations in a single thread. The output of Impala's PROFILE statement shows that the bulk of the time during the join is spent on probing the hash table using the rows from the lineitem and applying complex query predicate.</p><p>As shown in Table <ref type="table" target="#tab_6">7</ref>, Hive-MR is much slower in reading the data from HDFS. As in Query 17, both Hive-MR and Hive-Tez are CPU-bound during the execution of this query. All the cores in each node are more than 90% utilized and the disks are underutilized (&lt; 10%). The use of the ORC file has a positive effect since it can improve the scan time by 1.7X. However, in this particular query, Hive cannot benefit from predicate pushdown in the ORC file, because the filtering condition is a join predicate which has to be applied in the reduce phase of MR1. Hive-MR uses 120 reduce tasks to perform the join operation as well as the predicate evaluation, as opposed to Impala which uses 20 threads (one per compute node). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Query 22</head><p>Query 22 is the query with the most significant performance gain in Impala compared to Hive-MR (15X when using the snappycompressed Parquet format).</p><p>Figure <ref type="figure" target="#fig_2">5</ref> shows the operations in Query 22. The query consists of 3 sub-queries in both Hive and Impala. The first sub-query (S1) scans the customer table, applies a predicate and materializes  Then, an outer join between the result of AGG2 and table TMP1 is performed and a projection of the qualified rows is produced. In the following step, a cross-product between the TMP2 table and the result of the outer join is performed, followed by an aggregation (AGG3) and a sort. The time breakdown for Query 22 in Hive-MR is shown in Table 9. The response time of sub-queries S1 and S2 slightly increases when the ORC file is used, especially if it is compressed. These queries materialize their output in temporary tables. The output size is small (e.g., 803 MB in text format for sub-query S1). The overhead of creating and compressing an ORC file for such a small output has a negative impact on the overall query time. The bulk of the time in sub-query S3 is spent executing the outer join using a full MapReduce job. The cross-product, on the other hand, is executed as a map-side join in Hive. Our analysis shows that the time difference between the TXT format and the snappycompressed ORC file is due to increased scan time of the two tables participating in the outer join operation because of decompression overheads. In fact, for this particular operation the scan time of the tables stored in compressed ORC file format increases by 28% compared to the scan time of tables in uncompressed ORC file. Hive-MR uses 7 MapReduce jobs for this query. All had similar behavior: when a core is used during the query execution, it is fully utilized whereas the disks are underutilized (&lt; 15%). Hive-Tez avoids the startup and materialization overheads of the MapReduce jobs and thus improves the running time by about 3.5X.</p><p>The time breakdown for Query 22 in Impala is shown in Table 10. As shown in the table Impala is faster than Hive-MR in each sub-query, with speedups ranging from 7X to 20.5X depending on the file format. This performance gap is mainly attributed to the fact that Impala has a more efficient I/O subsystem. This behavior is observed in sub-queries S1 and S2 which only include scan operations. The disks are fully utilized (&gt; 90%) during the scan operation as well as during the materializion of the results. The CPU utilization was less than 20% on each node. Note that the startup cost of sub-query S3 and the time spent in code generation constitute about 30% of the query time, which is a significant overhead. This is because these queries scan and materialize very small amounts of data. Impala breaks sub-query S3 into 7 fragments. The Impala PROFILE statement shows that I/O was very efficient for that sub-query too. For example, Impala spent 13 seconds to read and partially aggregate the orders table (AGG2) when the TXT format was used corresponding to 70M B/sec per disk on each node. During the scan operation all the CPU cores were 10% utilized. The cross-product and outer join operations were executed using a single core (100% utilized). However, the amount of data involved in these operations is very small and thus the single-threaded execution did not become a major bottleneck for this sub-query. Both Impala and Hive picked the same algorithms for these operations, namely a partitioned-based algorithm for the outer join and a broadcast algorithm for the cross-product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Effectiveness of Runtime Code Generation in Impala</head><p>One of the much touted features of Impala is its ability to generate code at runtime to improve CPU efficiency and query execution times. Impala uses runtime code generation for eliminating various overheads associated with virtual function calls, inefficient instruction branching due to large switch statements, etc. In this section, we conduct experiments to evaluate the effectiveness of runtime code generation for the TPC-H queries. We again use the 21 TPC-H queries (Query 9 fails), execute them sequentially, with and without runtime code generation and measure the overall execution time. The results are presented in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>With runtime code generation enabled, the query execution time improves by about 1.3 for all the queries combined. As Figure <ref type="figure" target="#fig_3">4</ref> shows, runtime code generation has different effects for different queries. TPC-H Query 1 benefits the most from code generation, improving by 5.5X. The remaining queries see benefits up to 1.75X.</p><p>discussed in Section 3.5, Query 1 scans the lineitem table and then performs a hash-based aggregation. Impala's output of the PROFILE statement shows that the time spent in the aggregation increases by almost 6.5X when runtime code generation is not enabled. During the hash-based aggregation, for each tuple the grouping columns (l returnflags and l linestatus) are evaluated and then hashed. Then a hash table lookup is performed and all the expressions used in the aggregation (sum, avg and count) are evaluated. Runtime code generation can help sig-  nificantly improve the performance of this operation as all the logic for evaluating batches of rows for aggregation is compiled into a single inlined loop. Code generation does not practically impose any performance overhead. For example, the total code generation time for Query 1 is approximately 400 ms.</p><p>From the results presented in Figure <ref type="figure" target="#fig_3">4</ref>, we conclude that runtime code generation generally improves overall performance. However, the benefit of code generation varies for different queries. In that respect, our findings are inline with what has been reported<ref type="foot" target="#foot_6">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">TPC-DS Inspired Workloads</head><p>this section, we present cluster experiments using a workload inspired by the TPC-DS benchmark 8 . This is the workload published by Impala developers 9 where they compare Hive-MR and Impala on the cluster setup described in <ref type="bibr" target="#b21">[22]</ref>. Our goal is to reproduce this experiment on our hardware and examine if the performance trends are similar to what is reported in <ref type="bibr" target="#b21">[22]</ref>. We also performed additional experiments using Hive-Tez. Note that in our experimental setup we use 20 compute nodes connected through a 10Gbit Ethernet switch as opposed to the setup used by the Impala developers which consists of 5 compute nodes connected through a 1Gbit Ethernet switch. Our nodes have similar memory and I/O configurations with the node configuration described in <ref type="bibr" target="#b21">[22]</ref> but contain more physical cores (12 cores instead of 8 cores).</p><p>The workload (TPC-DS inspired workload 1) consists of 20 queries which access a single fact table and six dimension tables. Thus, it significantly differs from the original TPC-DS benchmark that consists of 99 queries that access 7 fact tables and multiple dimension tables. Since Impala does not currently support windowing functions and rollup, the TPC-DS queries have been modified by the Cloudera developers to exclude these features. An explicit partitioning predicate is also added, in order to restrict the portions of the fact table that need to be accessed. In the official TPC-DS benchmark, the values of the filtering predicates in each query change with the scale factor. Although Cloudera's workload operates on 3 TB TPC-DS data, the predicate values do not match the ones described in the TPC-DS specification for this scale factor. For completeness, we also performed a second experiment which uses the same workload but removes the explicit partitioning predicate and also uses the correct predicate values (TPC-DS inspired workload 2).</p><p>As per the instructions in <ref type="bibr" target="#b21">[22]</ref> we stored the data in Parquet and ORC snappy-compressed file formats. We flushed the file cache before running each workload. We repeated each experiment three times and report the average execution time for each query. As in the previous experiments, we computed statistics on the Impala tables. The Hive-MR configuration used by the Impala developers is not described in <ref type="bibr" target="#b21">[22]</ref> so we performed our own Hive-MR tuning. More specifically we enabled map-side joins, predicate pushdown and the "correlation" optimization in Hive-MR and also the vectorized execution in Hive-Tez. Figures <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_7">7</ref> present the execution times for the queries of TPC-DS inspired workload 1 and the TPC-DS inspired workload 2, respectively.</p><p>Our results show that Impala is on average 8.2X faster than Hive-MR and 4.3X faster than Hive-Tez on the TPC-DS inspired workload 1 and 10X faster than Hive-MR and 4.4X faster than Hive-Tez on the TPC-DS inspired workload 2. These numbers show the advantage of Impala for workloads that access one fact table and multiple small dimension tables. The query execution plan for these queries has the same pattern: Impala broadcasts the small dimension tables where the fact table data is and joins them in a pipelined fashion. The performance of Impala is also attributed to its efficient I/O subsystem. In our experiments we observed that Hive-MR has significant overheads when ingesting small tables, and all these queries involve many small dimension tables. When Hive-MR merges multiple operations into a single MapReduce job the performance gap between the two systems shrinks. For example, in Query 43, Hive-MR performs three joins and an aggregation using one MapReduce job. However, it starts multiple map tasks to scan the fact table (one per partition) which are CPU-bound. The overhead of instantiating multiple task rounds for a small amount of data makes Hive-MR almost 4X slower than Impala on this query in the TPC-DS inspired workload 1. Hive-MR is 13X slower than Impala for Query 43 of TPC-DS inspired workload 2. Since the explicit partitioning predicate is now removed, Hive-MR scans all the partitions of the fact table using one map task per partition, resulting in a very large number of map tasks. When Hive-MR generates multiple MapReduce jobs (e.g., in Query 79), the performance gap between Hive-MR and Impala increases. The performance difference between Hive and Impala decreases when Hive-Tez is used. This is because Impala and Hive-Tez avoid Hive-MR's scheduling and materialization overheads.  It is worth noting that Hive has support for windowing functions and rollup. As a result 5 queries of this workload (Q27, Q53, Q63, Q89, Q98) can be executed using the official versions of the TPC-DS queries in Hive but not in Impala. We executed these 5 official TPC-DS queries in Hive and observed that the execution time increases at most by 10% compared to the queries in workload 2.</p><p>To summarize, we observe similar trends in both TPC-H and TPC-DS inspired workloads. Impala performs better for TPC-DS inspired workloads than the TPC-H like workload because of its optimizations for the query pattern in the workloads (joining one large table with a set of small tables).</p><p>The individual query speedups that we observe (4.8X to 23X) are not as high as the ones reported in <ref type="bibr" target="#b21">[22]</ref> (6X to 69X) but our hardware configurations differ and it is not clear whether we used the same Hive-MR configuration when running the experiments and whether Hive-MR was properly tuned in <ref type="bibr" target="#b21">[22]</ref>, since these details are not clearly described in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MICRO-BENCHMARK RESULTS</head><p>In this section, we study the I/O characteristics of Hive-MR and Impala over their columnar formats, using a set of micro-benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Environment</head><p>generated three data files of 200 GB each (in text format). Each dataset contains a different number of integer columns (10, 100 and 1000 columns). We used the same data type for each column in order to eliminate the impact of diverse data types on scan performance. Each column is generated according to the TPC-H specification for the partkey attribute. We preferred to use the TPC-H generator over a random data generator because both the Parquet and the ORC file formats use run-length encoding (RLE) to encode integer values in each column. Thus, datasets with arbitrary random values would not be able to exploit this feature.</p><p>For our micro-benchmarks, we use one "control" node that runs all the main processes used by Hive, Hadoop and Impala, and a second "'compute" node to run the actual experiments. For the Hive-MR experiments, we use the default ORC file stripe size (256 MB) unless mentioned otherwise. During the load phase, we use only 1 map task to convert the dataset from the text format to the ORC file format in order to avoid generating many small files which can negatively affect the scan performance. In all our experiments, datasets are uncompressed, since our synthetic datasets cannot significantly benefit from block compression codecs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scan Performance in Hive and Impala</head><p>In this experiment, we measure the I/O throughput while scanning data in Hive-MR and Impala in a variety of settings. More specifically, we vary: (a) the number of columns in the file, (b)the number of columns accessed, and (c) the column access pattern.</p><p>In each experiment, we scan a fixed percentage of the columns and record the read throughput, measured as the amount of data processed by each system, divided by the scan time. We experimented with two column access patterns, namely sequential(S) and random(R). Given N columns to be accessed, in the sequential access pattern we scan the columns starting from the 1 st up to the N th one. In the random access pattern, we randomly pick N columns from the file and scan those. As a result, in the random access pattern, data skipping may occur between column accesses. To scan columns C1,...,CN we issue a "select max(c1), ..., max(cN ) from table" in both Impala and Hive-MR. We preferred to use max() over count(), since count() queries without a filtering predicate are answered using metadata only in Impala.</p><p>Figures <ref type="figure">8</ref> and<ref type="figure">9</ref> present the I/O throughput in Impala and Hive-MR respectively, as the percentage of accessed columns varies from 10% to 100%, for both the sequential(S) and the random(R) access patterns using our three datasets. Our calculations do not include the MapReduce job startup time in Hive-MR. When using the 1000-column file and accessing more than 60% of the columns, Impala pays a 20-30 seconds compilation cost. This time is also not included in our calculations.</p><p>As shown in Figure <ref type="figure">8</ref>, the Impala read throughput for the datasets that consist of 100 and 1000 integer columns, remains almost constant as we vary the percentage of columns accessed using the sequential access pattern. There are two interesting observations on the sequential access pattern: (a) the read throughput the 1000-column dataset is lower than that of the 100-column dataset, and (b) the read throughput on the 10-column dataset increases as the number of accessed columns increases.</p><p>Regarding the first observation, we noticed that the average CPU utilization when scanning a fixed percentage of columns is higher when scanning the 1000-column file than when scanning the 100column file. For example, when scanning 60% of the columns in the dataset, the CPU utilization is 94% during the experiment with the 1000-column file and 67% during the experiment with the 100column file. Using the Impala PROFILE statement, we observed that the ratio of the time spent in parsing the bytes to the time spent in actually reading the bytes from HDFS is higher for the 1000column file than the 100-column file. Although the amount of data processed is similar in both cases, processing a 1000-column record is more expensive than processing a 100-column record. Our 3 datasets have the same size in text format but different sizes (depending on the number of columns in the file). The fact that the 10-column file contains more records than the other files, is the reason why the read throughput for this file increases as the number of accessed columns increases. More specifically, tuple materialization becomes a bottleneck when scanning data from this file, especially when the amount of data accessed is small. When a few columns are accessed, the cost of tuple materialization dominates the running time. To verify this, we repeated the experiment but added a predicate of the form "c1 &lt; 0 OR c2 &lt; 0 OR ... OR cN &lt; 0" in the query. This predicate has 0% selectivity but forces a scan of the necessary columns. Now, the read throughput in the sequential access pattern becomes almost constant as the number of accessed columns varies.</p><p>As shown in Figure <ref type="figure">8</ref>, when the access pattern is random, the read throughput increases as the number of accessed columns increases and it is typically lower then the corresponding throughput of the sequential access pattern. This observation is consistent with the HDFS design. Scanning sequentially large amounts of data from HDFS is a "cheap" operation. For this reason, as the percentage of columns accessed increases, the read throughput approaches the throughput observed in the sequential access pattern.</p><p>Regarding our Hive-MR experiments, Figure <ref type="figure">9</ref> shows that the behavior of the system is independent of the access pattern (sequential or random). The read throughput always increases as the number of accessed columns increases. This behavior is attributed to the high overhead of starting and running short map tasks. As an example, scanning only one column from the 1000-column file takes about 120 seconds. Scanning sequentially 100 columns from the same file takes about 170 seconds. In our case, each map task processes 512 MB of data (aka, two ORC stripes). Since our datasets have sizes of about 80-90 GB, each file scan translates into multiple map rounds. Moreover, the average CPU utilization during the Hive-MR experiments is 87% and the disks are underutilized (&lt; 30%). This behavior is closely related to the overheads introduced by object deserialization in java <ref type="bibr" target="#b11">[12]</ref>. The CPU overheads introduced when scanning a large number of records (10column file) increase each map task's time by 23%.</p><p>The last property of the file formats that we tested is efficient skipping of data when the access pattern is random. To measure the amount of data read from disk, we used the nmon performance tool. We observed that Hive-MR and Impala read almost as much data as needed during the experiments with the 10-column and 100column files. Efficient data skipping is more challenging in the case of the 1000-column dataset. In the worst case, Impala reads twice as much data as needed on this dataset. Hive-MR, is not able to efficiently skip columns even when accessing only 10% of the columns and ends up reading about 92% of the file. Note that the unnecessary prefetched bytes are not deserialized. Thus the impact of prefetching on the overall execution time is not significant. Since packing 1000 columns within a 256 MB stripe results in unnecessary data prefetching, we increased the ORC stripe size to 750 MB and repeated the experiment. In this experiment, Hive-MR reads as much data as needed. In this case, each column is allocated more bytes in the stripe, and as a result skipping very small chunks of data is avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Indexing in ORC File</head><p>When a selection predicate on a column is pushed down to the ORC file, the built-in index can help skip chunks of data within a stripe based on the min and max values of the column within the chunk. A column chunk can either be skipped or read as a whole. Thus, the index operates on a coarse granularity as opposed to finegrained indices, such as B-trees. In this experiment, we examine the effectiveness of the lightweight built-in index in ORC file.</p><p>We use two variants of the 10-column dataset, namely sorted and unsorted. More specifically, we picked one column C and sorted the dataset on this column to create the sorted variant. For both datasets, we execute the following query: "select max(C) from table where C &lt; s". The parameter s determines the selectivity of the predicate. We enable predicate pushdown and index filtering and use the default index row group size of 10,000 rows.</p><p>Figure <ref type="figure">10</ref> shows the response time of the query in Hive-MR as the predicate becomes less selective, for the sorted and the unsorted datasets. Figure <ref type="figure">11</ref> shows the amount of data processed for each selectivity factor over the amount of data processed at the 100% selectivity factor, for both the unsorted and the sorted datasets. At the 100% selectivity point, Hive-MR processes approximately 8 GB of data when the dataset is unsorted, which corresponds to one full column. In the case of the sorted dataset, 344 MB of data is processed. This is because, in the sorted case, run-length encoding can further compress the column's data.</p><p>As shown in the figures, the built-in index in ORC file is helpful only when the selectivity of the predicate is 0% for the unsorted dataset. In this case, only 11.7 MB of data out of the 8 GB is actually processed by Hive-MR. This is expected since all the row groups of the file can immediately be skipped. However, when the predicate is less selective (even just 1%) the whole column is actually read. We repeated the same experiment but reduced the row group size to the minimum allowed value of  in this case, the index is not helpful for predicate selectivities above 0% for the unsorted dataset. Note that the running time of the query increases as the predicate becomes less selective. This is expected, since more records need to be On the other hand, the built-in index is more helpful when the column is sorted. In this case, the range of values in each row group is more distinct, and thus the probability of a query predicate falling into a smaller number of row groups increases. The amount of data processed and the response time of the query gradually increase as the predicate becomes less selective. As shown in Figure <ref type="figure">11</ref>, the full column is read only at the 100% selectivity factor.</p><p>Our results suggest that the built-in index in ORC file is more useful when the data is sorted in advance or has a natural order that would enable efficient skipping of row groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>Increased interest in SQL-on-Hadoop fueled the development of many SQL engines for Hadoop data. In this paper, we conduct an experimental evaluation of Impala and Hive, two representatives among SQL-on-Hadoop systems. We find that Impala provides a significant performance advantage over when the workload's working set fits in memory. The performance difference is attributed to Impala's very efficient I/O sub-system and to its pipelined query execution which resembles that of a sharednothing parallel database. Hive on MapReduce pays the overhead of scheduling and data materialization that the MapReduce framework imposes whereas Hive on Tez avoids these overheads. However, both Hive-MR and Hive-Tez are CPU-bound during scan operations, which negatively affects their performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Hive-Impala Overall Comparison (Arithmetic Mean) Figure 2: Hive-Impala Overall Comparison (Geometric Mean)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Query 17</figDesc><graphic coords="6,56.19,53.55,234.32,192.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Query 22</figDesc><graphic coords="7,319.20,53.90,234.52,198.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effectiveness of Impala Code Generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: TPC-DS inspired Workload 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: TPC-DS inspired Workload 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Impala/Parquet Read Throughput Figure 9: Hive/ORC Read Throughput</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Predicate Selectivity vs. Scan Time Figure 11: Predicate Selectivity vs. Data Processed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 : Data Loading in Hive and Impala</head><label>2</label><figDesc></figDesc><table><row><cell>System</cell><cell>File Format</cell><cell>Time</cell><cell>Size</cell></row><row><cell></cell><cell></cell><cell cols="2">(secs) (GB)</cell></row><row><cell>Hive-MR</cell><cell>ORC ORC-Snappy</cell><cell>3099 5191</cell><cell>803 337</cell></row><row><cell>Hive-Tez</cell><cell>ORC ORC-Snappy</cell><cell>1583 1606</cell><cell>629 290</cell></row><row><cell></cell><cell>Parquet</cell><cell>784</cell><cell>623</cell></row><row><cell>Impala</cell><cell>Parquet-Snappy</cell><cell>822</cell><cell>316</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Hive vs. Impala Execution Time</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Hive -MR</cell><cell></cell><cell></cell><cell>Hive -Tez</cell><cell></cell><cell></cell><cell>Impala</cell><cell></cell><cell></cell></row><row><cell>TPC-H</cell><cell></cell><cell></cell><cell>ORC</cell><cell></cell><cell></cell><cell>ORC</cell><cell></cell><cell></cell><cell cols="2">Parquet ORC-Snappy (Hive-Tez)</cell></row><row><cell>Query</cell><cell cols="4">TXT ORC Snappy TXT</cell><cell cols="2">ORC Snappy</cell><cell>TXT</cell><cell cols="2">Parquet Snappy</cell><cell>over</cell></row><row><cell></cell><cell cols="2">(secs) (secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell><cell>Parquet-Snappy (Impala)</cell></row><row><cell>Q1</cell><cell>535</cell><cell>266</cell><cell>228</cell><cell>232</cell><cell>154</cell><cell>172</cell><cell>73</cell><cell>25</cell><cell>24</cell><cell>7.2</cell></row><row><cell>Q2</cell><cell>293</cell><cell>299</cell><cell>294</cell><cell>130</cell><cell>132</cell><cell>111</cell><cell>36</cell><cell>29</cell><cell>29</cell><cell>3.8</cell></row><row><cell>Q3</cell><cell>761</cell><cell>543</cell><cell>462</cell><cell>524</cell><cell>286</cell><cell>280</cell><cell>149</cell><cell>122</cell><cell>115</cell><cell>2.4</cell></row><row><cell>Q4</cell><cell>534</cell><cell>277</cell><cell>232</cell><cell>272</cell><cell>220</cell><cell>214</cell><cell>152</cell><cell>135</cell><cell>147</cell><cell>1.5</cell></row><row><cell>Q5</cell><cell>1078</cell><cell>833</cell><cell>772</cell><cell>831</cell><cell>415</cell><cell>409</cell><cell>200</cell><cell>189</cell><cell>190</cell><cell>2.2</cell></row><row><cell>Q6</cell><cell>357</cell><cell>109</cell><cell>85</cell><cell>114</cell><cell>88</cell><cell>81</cell><cell>15</cell><cell>6</cell><cell>6</cell><cell>13.5</cell></row><row><cell>Q7</cell><cell>1409</cell><cell>1096</cell><cell>1038</cell><cell>581</cell><cell>597</cell><cell>589</cell><cell>195</cell><cell>179</cell><cell>182</cell><cell>3.2</cell></row><row><cell>Q8</cell><cell>1085</cell><cell>822</cell><cell>770</cell><cell>743</cell><cell>392</cell><cell>369</cell><cell>155</cell><cell>140</cell><cell>148</cell><cell>2.5</cell></row><row><cell>Q9</cell><cell>3371</cell><cell>2883</cell><cell>2848</cell><cell>2102</cell><cell>1682</cell><cell cols="4">1692 FAILED FAILED FAILED</cell><cell>-</cell></row><row><cell>Q10</cell><cell>778</cell><cell>526</cell><cell>502</cell><cell>336</cell><cell>236</cell><cell>224</cell><cell>117</cell><cell>59</cell><cell>50</cell><cell>4.5</cell></row><row><cell>Q11</cell><cell>235</cell><cell>234</cell><cell>216</cell><cell>158</cell><cell>141</cell><cell>134</cell><cell>30</cell><cell>22</cell><cell>22</cell><cell>6.1</cell></row><row><cell>Q12</cell><cell>555</cell><cell>308</cell><cell>241</cell><cell>226</cell><cell>186</cell><cell>184</cell><cell>37</cell><cell>38</cell><cell>40</cell><cell>4.6</cell></row><row><cell>Q13</cell><cell>255</cell><cell>266</cell><cell>228</cell><cell>176</cell><cell>155</cell><cell>156</cell><cell>119</cell><cell>123</cell><cell>127</cell><cell>1.2</cell></row><row><cell>Q14</cell><cell>417</cell><cell>175</cell><cell>155</cell><cell>124</cell><cell>123</cell><cell>120</cell><cell>17</cell><cell>13</cell><cell>14</cell><cell>8.6</cell></row><row><cell>Q15</cell><cell>619</cell><cell>272</cell><cell>233</cell><cell>166</cell><cell>159</cell><cell>156</cell><cell>27</cell><cell>21</cell><cell>22</cell><cell>7.1</cell></row><row><cell>Q16</cell><cell>247</cell><cell>235</cell><cell>239</cell><cell>152</cell><cell>143</cell><cell>133</cell><cell>39</cell><cell>44</cell><cell>43</cell><cell>3.1</cell></row><row><cell>Q17</cell><cell>1041</cell><cell>779</cell><cell>813</cell><cell>1205</cell><cell>945</cell><cell>724</cell><cell>401</cell><cell>385</cell><cell>395</cell><cell>1.8</cell></row><row><cell>Q18</cell><cell>1183</cell><cell>925</cell><cell>859</cell><cell>1128</cell><cell>723</cell><cell>672</cell><cell>259</cell><cell>185</cell><cell>189</cell><cell>3.6</cell></row><row><cell>Q19</cell><cell>907</cell><cell>677</cell><cell>656</cell><cell>919</cell><cell>734</cell><cell>569</cell><cell>742</cell><cell>703</cell><cell>710</cell><cell>0.8</cell></row><row><cell>Q20</cell><cell>663</cell><cell>379</cell><cell>332</cell><cell>236</cell><cell>179</cell><cell>175</cell><cell>196</cell><cell>195</cell><cell>191</cell><cell>0.9</cell></row><row><cell>Q21</cell><cell>2230</cell><cell>1605</cell><cell>1495</cell><cell cols="3">1688 FAILED FAILED</cell><cell>879</cell><cell>854</cell><cell>857</cell><cell>-</cell></row><row><cell>Q22</cell><cell>363</cell><cell>385</cell><cell>452</cell><cell>123</cell><cell>132</cell><cell>130</cell><cell>50</cell><cell>29</cell><cell>30</cell><cell>4.3</cell></row><row><cell>AM</cell><cell>860</cell><cell>632</cell><cell>598</cell><cell>553</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GM</cell><cell>665</cell><cell>459</cell><cell>422</cell><cell>358</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AM-Q{9,21}</cell><cell>666</cell><cell>470</cell><cell>440</cell><cell>419</cell><cell>307</cell><cell>280</cell><cell>150</cell><cell>132</cell><cell>134</cell><cell>2.1</cell></row><row><cell>GM-Q{9,21}</cell><cell>577</cell><cell>393</cell><cell>360</cell><cell>303</cell><cell>238</cell><cell>225</cell><cell>90</cell><cell>70</cell><cell>70</cell><cell>3.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Time Breakdown for Query 1 in Hive-MR</head><label>3</label><figDesc>Table 3 presents the time spent in each MapReduce job, for the different file formats that we tested. As shown in the table, replacing the text format with the ORC file format, provides a 2.3X speedup. An additional speedup of 1.2X is gained by compressing the ORC file using the snappy compression algorithm.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ORC</cell></row><row><cell></cell><cell cols="3">TXT ORC Snappy</cell></row><row><cell></cell><cell cols="2">(secs) (secs)</cell><cell>(secs)</cell></row><row><cell>MR1-map phase</cell><cell>485</cell><cell>215</cell><cell>176</cell></row><row><cell>MR1-reduce phase</cell><cell>5</cell><cell>7</cell><cell>4</cell></row><row><cell>MR2</cell><cell>10</cell><cell>11</cell><cell>11</cell></row><row><cell cols="4">the result in HDFS. MR2 reads the output produced by MR1 and</cell></row><row><cell>sorts it.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 : Time Breakdown for Query 1 in Impala</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Parquet</cell></row><row><cell></cell><cell cols="3">TXT Parquet Snappy</cell></row><row><cell></cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell></row><row><cell>F1</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell>F2</cell><cell>71</cell><cell>22</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 : Time Breakdown for Query 17 in Hive-MR</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ORC</cell></row><row><cell></cell><cell cols="3">TXT ORC Snappy</cell></row><row><cell></cell><cell cols="2">(secs) (secs)</cell><cell>(secs)</cell></row><row><cell>MR1-map phase</cell><cell>720</cell><cell>534</cell><cell>590</cell></row><row><cell>MR1-reduce phase</cell><cell>278</cell><cell>204</cell><cell></cell></row><row><cell>MR2</cell><cell>12</cell><cell>11</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 : Time Breakdown for Query 17 in Impala</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Parquet</cell></row><row><cell></cell><cell cols="3">TXT Parquet Snappy</cell></row><row><cell></cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell></row><row><cell>F0</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>F1</cell><cell>33</cell><cell>24</cell><cell>24</cell></row><row><cell>F2</cell><cell>109</cell><cell>113</cell><cell>120</cell></row><row><cell>F3</cell><cell>249</cell><cell>240</cell><cell>248</cell></row><row><cell>F4</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 : Time Breakdown for Query 19 in Hive-MR ORC TXT ORC Snappy (secs) (secs) (secs) MR1-map phase</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell>554</cell><cell>359</cell><cell>332</cell></row><row><cell>MR1-reduce phase</cell><cell>320</cell><cell>300</cell><cell>300</cell></row><row><cell>MR2</cell><cell>12</cell><cell>10</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 : Time Breakdown for Query 19 in Impala</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Parquet</cell></row><row><cell></cell><cell cols="3">TXT Parquet Snappy</cell></row><row><cell></cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell></row><row><cell>F0</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>F1</cell><cell>610</cell><cell>590</cell><cell>600</cell></row><row><cell>F2</cell><cell>120</cell><cell>95</cell><cell>97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 : Time Breakdown for Query 22 in Hive-MR</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ORC</cell></row><row><cell></cell><cell cols="3">TXT ORC Snappy</cell></row><row><cell></cell><cell cols="2">(secs) (secs)</cell><cell>(secs)</cell></row><row><cell>S1</cell><cell>70</cell><cell>74</cell><cell>92</cell></row><row><cell>S2</cell><cell>31</cell><cell>34</cell><cell>41</cell></row><row><cell>S3</cell><cell>252</cell><cell>268</cell><cell>308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 : Time Breakdown for Query 22 in Impala</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Parquet</cell></row><row><cell></cell><cell cols="3">TXT Parquet Snappy</cell></row><row><cell></cell><cell>(secs)</cell><cell>(secs)</cell><cell>(secs)</cell></row><row><cell>S1</cell><cell>10</cell><cell>6</cell><cell>6</cell></row><row><cell>S2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>S3</cell><cell>31</cell><cell>14</cell><cell>15</cell></row></table><note><p>the result in a temporary table (TMP1). The second sub-query (S2) applies an inequality predicate on the TMP1 table, performs an aggregation (AGG1) and materializes the result in a temporary table (TMP2). The third sub-query (S3) performs an aggregation on the orders table (AGG2).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the following sections, when the term Hive is used, this applies to both Hive-MR and Hive-Tez.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.tpc.org/tpch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https: //issues.apache.org/jira/browse/HIVE-600</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/kj-ki/tpc-h-impala</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that Hive-Tez includes some ORC file changes and hence results in file size differences.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://nmon.sourceforge.net/pmwiki.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://blog.cloudera.com/blog/2013/02/inside-cloudera-impalaruntime-code-generation/ http://www.tpc.org/tpcds/ 9 https://github.com/cloudera/impala-tpcds-kit</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>We thank Simon Harris and Abhayan Sundararajan for providing the TPC-H and the TPC-DS workload generators to us.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Column oriented Database Systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1664" to="1665" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abouzeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bajda-Pawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="922" to="933" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data Page Layouts for Relational Databases on Deep Memory Hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Drill</surname></persName>
		</author>
		<ptr target="http://www.mapr.com/resources/community-resources/apache-drill" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Hive</surname></persName>
		</author>
		<ptr target="http://hive.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Spark</surname></persName>
		</author>
		<ptr target="https://spark.incubator.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HAWQ: A Massively Parallel Processing SQL Engine in Hadoop</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1223" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cloudera</forename><surname>Impala</surname></persName>
		</author>
		<ptr target="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Split Query Processing in Polybase</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilar-Saborit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flasza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gramling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1255" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Column-oriented Storage Techniques for MapReduce</title>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="419" to="429" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Can the Elephants Handle the NoSQL Onslaught?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Teletia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1712" to="1723" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RCFile: A Fast and Space-efficient Data Placement Structure in MapReduce-based Warehouse Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">YSmart: Yet Another SQL-to-MapReduce Translator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDCS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dremel: Interactive Analysis of Web-scale Datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gubarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vassilakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="330" to="339" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://prestodb.io/" />
		<title level="m">Presto</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">C-Store: A Column-oriented DBMS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PVLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<title level="m">MapReduce and parallel DBMSs: Friends or Foes? CACM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://tajo.incubator.apache.org/" />
		<title level="m">Tajo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hive -a Petabyte Scale Data Warehouse using Hadoop</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://blog.cloudera.com/blog/2014/01/impala-performance-dbms-class-speed/" />
		<title level="m">TPC-DS like Workload on Impala</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="http://avro.apache.org/docs/1.7.6/trevni/spec.html" />
		<title level="m">Trevni Columnar Format</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shark: SQL and Rich Analytics at Scale</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
