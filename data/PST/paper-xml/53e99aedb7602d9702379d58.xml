<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Rectified Gaussian Distribution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Socci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<addrLine>Lucent Technologies Murray Hill</addrLine>
									<postCode>07974</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<addrLine>Lucent Technologies Murray Hill</addrLine>
									<postCode>07974</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<addrLine>Lucent Technologies Murray Hill</addrLine>
									<postCode>07974</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Rectified Gaussian Distribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA40E168DBABBB85726C8B7B7846488E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A simple but powerful modification of the standard Gaussian distribution is studied. The variables of the rectified Gaussian are constrained to be nonnegative, enabling the use of nonconvex energy functions. Two multimodal examples, the competitive and cooperative distributions, illustrate the representational power of the rectified Gaussian. Since the cooperative distribution can represent the translations of a pattern, it demonstrates the potential of the rectified Gaussian for modeling pattern manifolds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The rectified Gaussian distribution is a modification of the standard Gaussian in which the variables are constrained to be nonnegative. This simple modification brings increased representational power, as illustrated by two multimodal examples of the rectified Gaussian, the competitive and the cooperative distributions. The modes of the competitive distribution are well-separated by regions of low probability. The modes of the cooperative distribution are closely spaced along a nonlinear continuous manifold. Neither distribution can be accurately approximated by a single standard Gaussian. In short, the rectified Gaussian is able to represent both discrete and continuous variability in a way that a standard Gaussian cannot. This increased representational power comes at the price of increased complexity. While finding the mode of a standard Gaussian involves solution of linear equations, finding the modes of a rectified Gaussian is a quadratic programming problem. Sampling from a standard Gaussian can be done by generating one dimensional normal deviates, followed by a linear transformation. Sampling from a rectified Gaussian requires Monte Carlo methods. Mode-finding and sampling algorithms are basic tools that are important in probabilistic modeling.</p><p>Like the Boltzmann machine[l], the rectified Gaussian is an undirected graphical model. The rectified Gaussian is a better representation for probabilistic modeling A different version of the rectified Gaussian was recently introduced by Hinton and Ghahramani <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Their version is for a single variable, and has a singularity at the origin designed to produce sparse activity in directed graphical models. Our version lacks this singularity, and is only interesting in the case of more than one variable, for it relies on undirected interactions between variables to produce the multimodal behavior that is of interest here.</p><p>The present work is inspired by biological neural network models that use continuous dynamical attractors <ref type="bibr" target="#b3">[4]</ref>. In particular, the energy function of the cooperative distribution was previously studied in models of the visual cortex <ref type="bibr" target="#b4">[5]</ref>, motor cortex <ref type="bibr" target="#b5">[6]</ref>, and head direction system <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ENERGY FUNCTIONS: BOWL, TROUGH, AND SADDLE</head><p>The standard Gaussian distribution P(x) is defined as</p><formula xml:id="formula_0">P(x) E(x) = Z -l -{3E(;r:) e , 1 _xT Ax -bTx 2 .<label>(1)</label></formula><p>(</p><p>The symmetric matrix A and vector b define the quadratic energy function E(x).</p><p>The parameter (3 = lIT is an inverse temperature. Lowering the temperature concentrates the distribution at the minimum of the energy function. The prefactor Z normalizes the integral of P(x) to unity.</p><p>Depending on the matrix A, the quadratic energy function E(x) can have different types of curvature. The energy function shown in Figure l(a) is convex. The minimum of the energy corresponds to the peak of the distribution. Such a distribution is often used in pattern recognition applications, when patterns are well-modeled as a single prototype corrupted by random noise.</p><p>The energy function shown in Figure <ref type="figure" target="#fig_0">1</ref> (b) is flattened in one direction. Patterns generated by such a distribution come with roughly equal1ikelihood from anywhere along the trough. So the direction of the trough corresponds to the invariances of the pattern. Principal component analysis can be thought of as a procedure for learning distributions of this form.</p><p>The energy function shown in Figure <ref type="figure" target="#fig_0">1</ref> (c) is saddle-shaped. It cannot be used in a Gaussian distribution, because the energy decreases without limit down the sides of the saddle, leading to a non-normalizable distribution. However, certain saddle-shaped energy functions can be used in the rectified Gaussian distribution, which is defined over vectors x whose components are all nonnegative. The class of energy functions that can be used are those where the matrix A has the property</p><p>x T Ax &gt; 0 for all x &gt; 0, a condition known as copositivity. Note that this set of matrices is larger than the set of positive definite matrices that can be used with a standard Gaussian. The nonnegativity constraints block the directions in which the energy diverges to negative infinity. Some concrete examples will be discussed shortly. The energy functions for these examples will have multiple minima, and the corresponding distribution will be multimodal, which is not possible with a standard Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODE-FINDING</head><p>Before defining some example distributions, we must introduce some tools for analyzing them. The modes of a rectified Gaussian are the minima of the energy function ( <ref type="formula" target="#formula_1">2</ref>), subject to nonnegativity constraints. At low temperatures, the modes of the distribution characterize much of its behavior.</p><p>Finding the modes of a rectified Gaussian is a problem in quadratic programming. Algorithms for quadratic programming are particularly simple for the case of nonnegativity constraints. Perhaps the simplest algorithm is the projected gradient method, a discrete time dynamics consisting of a gradient step followed by a rectification</p><p>The rectification [x]+ = max(x, 0) keeps x within the nonnegative orthant (x ~ 0). If the step size 7J is chosen correctly, this algorithm can provably be shown to converge to a stationary point of the energy function <ref type="bibr" target="#b7">[8]</ref>. In practice, this stationary point is generally a local minimum.</p><p>Neural networks can also solve quadratic programming problems. We define the synaptic weight matrix W = I -A, and a continuous time dynamics</p><formula xml:id="formula_3">x+x = [b+ Wx]+ (4)</formula><p>For any initial condition in the nonnegative orthant, the dynamics remains in the nonnegative orthant, and the quadratic function ( <ref type="formula" target="#formula_1">2</ref>) is a Lyapunov function of the dynamics.</p><p>Both of these methods converge to a stationary point of the energy. The gradient of the energy is given by 9 = Ax -b. According to the Kiihn-Tucker conditions, a stationary point must satisfy the conditions that for all i, either gi = 0 and Xi &gt; 0, or gi &gt; 0 and Xi = O. The intuitive explanation is that in the interior of the constraint region, the gradient must vanish, while at the boundary, the gradient must point toward the interior. For a stationary point to be a local minimum, the Kiihn-Tucker conditions must be augmented by the condition that the Hessian of the nonzero variables be positive definite.</p><p>Both methods are guaranteed to find a global minimum only in the case where A is positive definite, so that the energy function (2) is convex. This is because a convex energy function has a unique minimum. Convex quadratic programming is solvable in polynomial time. In contrast, for a nonconvex energy function (indefinite A), it is not generally possible to find the global minimum in polynomial time, because of the possible presence of local minima. In many practical situations, however, it is not too difficult to find a reasonable solution. The rectified Gaussian distribution has two peaks.</p><p>The rectified Gaussian happens to be most interesting in the nonconvex case, precisely because of the possibility of multiple minima. The consequence of multiple minima is a multimodal distribution, which cannot be well-approximated by a standard Gaussian. We now consider two examples of a multimodal rectified Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPETITIVE DISTRIBUTION</head><p>The competitive distribution is defined by</p><formula xml:id="formula_4">Aij -dij + 2 (5) bi = 1;<label>(6)</label></formula><p>We first consider the simple case N = 2. Then the energy function given by X2 +y2 E(x,y)=-2 +(x+y)2_(x+y) <ref type="bibr" target="#b6">(7)</ref> has two constrained minima at (1,0) and (0,1) and is shown in figure <ref type="figure" target="#fig_1">2(a)</ref>. It does not lead to a normalizable distribution unless the nonnegativity constraints are imposed. The two constrained minima of this nonconvex energy function correspond to two peaks in the distribution (fig 2 <ref type="figure">(b)</ref>). While such a bimodal distribution could be approximated by a mixture of two standard Gaussians, a single Gaussian distribution cannot approximate such a distribution. In particular, the reduced probability density between the two peaks would not be representable at all with a single Gaussian.</p><p>The competitive distribution gets its name because its energy function is similar to the ones that govern winner-take-all networks <ref type="bibr" target="#b8">[9]</ref>. When N becomes large, the N global minima of the energy function are singleton vectors (fig <ref type="figure" target="#fig_3">3</ref>), with one component equal to unity, and the rest zero. This is due to a competitive interaction between the components. The mean of the zero temperature distribution is given by <ref type="bibr" target="#b7">(8)</ref> The eigenvalues of the covariance 1 1  all equal to 1/ N, except for a single zero mode. The zero mode is 1, the vector of all ones, and the other eigenvectors span the N -1 dimensional space perpendicular to 1. Figure <ref type="figure" target="#fig_3">3</ref> shows two samples: one (b) drawn at finite temperature from the competitive distribution, and the other (c) drawn from a standard Gaussian distribution with the same mean and covariance. Even if the sample from the standard Gaussian is cut so negative values are set to zero the sample does not look at all like the original distribution. Most importantly a standard Gaussian will never be able to capture the strongly competitive character of this distribution.</p><formula xml:id="formula_5">(XiXj) -(Xi)(Xj) = N dij -N2<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COOPERATIVE DISTRIBUTION</head><p>To define the cooperative distribution on N variables, an angle fh = 27ri/N is associated with each variable Xi, so that the variables can be regarded as sitting on a ring. The energy function is defined by</p><formula xml:id="formula_6">1 4 Aij 6ij + N -N COS(Oi -OJ) (10) bi = 1;<label>(11)</label></formula><p>The coupling Aij between Xi and X j depends only on the separation Oi -0 3 . between them on the ring.</p><p>The minima, or ground states, of the energy function can be found numerically by the methods described earlier. An analytic calculation of the ground states in the large N limit is also possible <ref type="bibr" target="#b4">[5]</ref>. As shown in Figure <ref type="figure" target="#fig_4">4</ref>(a), each ground state is a lump of activity centered at some angle on the ring. This delocalized pattern of activity is different from the singleton modes of the competitive distribution, and arises from the cooperative interactions between neurons on the ring. Because the distribution is invariant to rotations of the ring (cyclic permutations of the variables xd, there are N ground states, each with the lump at a different angle.</p><p>The mean and the covariance of the cooperative distribution are given by</p><formula xml:id="formula_7">(Xi) = const (XiXj) -(Xi}(Xj) = C(Oi -OJ) (12)<label>(13)</label></formula><p>A given sample of x, shown in Figure <ref type="figure" target="#fig_4">4</ref> These deviations from standard Gaussian behavior reflect fundamental differences in the underlying energy function. Here the energy function has N discrete minima arranged along a ring. In the limit of large N the barriers between these minima become quite small. A reasonable approximation is to regard the energy function as having a continuous line of minima with a ring geometry <ref type="bibr" target="#b4">[5]</ref> . In other words, the energy surface looks like a curved trough, similar to the bottom of a wine bottle. The mean is the centroid of the ring and is not close to any minimum.</p><p>The cooperative distribution is able to model the set of all translations of the lump pattern of activity. This suggests that the rectified Gaussian may be useful in invariant object recognition, in cases where a continuous manifold of instantiations of an object must be modeled. One such case is visual object recognition, where the images of an object from different viewpoints form a continuous manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SAMPLING</head><p>Figures <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_4">4</ref> depict samples drawn from the competitive and cooperative distribution. These samples were generated using the Metropolis Monte Carlo algorithm. Since full descriptions of this algorithm can be found elsewhere, we give only a brief description of the particular features used here. The basic procedure is to generate a new configuration of the system and calculate the change in energy (given by eq. 2). If the energy decreases, one accepts the new configuration unconditionally.</p><p>If it increases then the new configuration is accepted with probability e-{3AE.</p><p>In our sampling algorithm one variable is updated at a time (analogous to single spin flips). The acceptance ratio is much higher this way than if we update all the spins simultaneously. However, for some distributions the energy function may have approximately marginal directions; directions in which there is little or no barrier.</p><p>The cooperative distribution has this property. We can expect critical slowing down due to this and consequently some sort of collective update (analogous to multi-spin updates or cluster updates) might make sampling more efficient. However, the type of update will depend on the specifics of the energy function and is not easy to determine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>The competitive and cooperative distributions are examples of rectified Gaussians for which no good approximation by a standard Gaussian is possible. However, both distributions can be approximated by mixtures of standard Gaussians. The competitive distribution can be approximated by a mixture of N Gaussians, one for each singleton state. The cooperative distribution can also be approximated by a mixture of N Gaussians, one for each location of the lump on the ring. A more economical approximation would reduce the number of Gaussians in the mixture, but .make each one anisotropic <ref type="bibr">[IO]</ref>.</p><p>Whether the rectified Gaussian is superior to these mixture models is an empirical question that should be investigated empirically with specific real-world probabilistic modeling tasks. Our intuition is that the rectified Gaussian will turn out to be a good representation for nonlinear pattern manifolds, and the aim of this paper has been to make this intuition concrete.</p><p>To make the rectified Gaussian useful in practical applications, it is critical to find tractable learning algorithms. It is not yet clear whether learning will be more tractable for the rectified Gaussian than it was for the Boltzmann machine. Perhaps the continuous variables of the rectified Gaussian may be easier to work with than the binary variables of the Boltzmann machine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three types of quadratic energy functions. (a) Bowl (b) Trough (c) Saddle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The competitive distribution for two variables. (a) A non-convex energy function with two constrained minima on the x and y axes. Shown are contours of constant energy, and arrows that represent the negative gradient of the energy. (b) The rectified Gaussian distribution has two peaks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. . . , . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The competitive distribution for N = 10 variables. (a) One mode (zero temperature state) of the distribution. The strong competition between the variables results in only one variable on. There are N modes of this form, each with a different winner variable. (b) A sample at finite temperature (13 ~ 110) using Monte Carlo sampling. There is still a clear winner variable. (c) Sample from a standard Gaussian with matched mean and covariance. Even if we cut off the negative values this sample still bears little resemblance to the states shown in (a) and (b), since there is no clear winner variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The cooperative distribution for N = 25 variables. (a) Zero temperature state. A cooperative interaction between the variables leads to a delocalized pattern of activity that can sit at different locations on the ring. (b) A finite temperature (/3 = 50) sample. (c) A sample from a standard Gaussian with matched mean and covariance.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We would like to thank P. Mitra, L. Saul, B. Shraiman and H. Sompolinsky for helpful discussions. Work on this project was supported by Bell Laboratories, Lucent Technologies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for Boltzmann machines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative models for discovering sparse distributed representations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. Roy. Soc., B</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="1177" to="1190" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical non-linear factor analysis and topographic maps</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Info. Proc. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How the brain keeps the eyes still</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="13339" to="13344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theory of orientation tuning in visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ben-Yishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bar-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="3844" to="3848" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitive neurophysiology of the motor cortex</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lukashin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="47" to="52" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2112" to="2126" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Competition and cooperation in neural nets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arbib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems Neuroscience</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Metzler</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="119" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling the manifolds of images of handwritten digits</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Revow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
