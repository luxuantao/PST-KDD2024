<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Grasp Object Classification and Feature Extraction with Simple Robot Hands and Tactile Sensors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Spiers</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Minas</forename><forename type="middle">V</forename><surname>Liarokapis</surname></persName>
						</author>
						<author>
							<persName><roleName>Member IEEE</roleName><forename type="first">Berk</forename><surname>Calli</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
						</author>
						<title level="a" type="main">Single-Grasp Object Classification and Feature Extraction with Simple Robot Hands and Tactile Sensors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CC5DEC998B3C192A58718F758FC78999</idno>
					<idno type="DOI">10.1109/TOH.2016.2521378</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TOH.2016.2521378, IEEE Transactions on Haptics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tactile Sensing</term>
					<term>Object Classification</term>
					<term>Object Feature Extraction</term>
					<term>Underactuated Robot Hands</term>
					<term>Machine Learning</term>
					<term>Adaptive Grasping</term>
					<term>Robotics</term>
					<term>Haptics Applications</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classical robotic approaches to tactile object identification often involve rigid mechanical grippers, dense sensor arrays and exploratory procedures (EPs). Though EPs are a natural method for humans to acquire object information, evidence also exists for meaningful tactile property inference from brief, non-exploratory motions (a 'haptic glance'). In this work we implement tactile object identification and feature extraction techniques on data acquired during a single, unplanned grasp with a simple, underactuated robot hand equipped with inexpensive barometric pressure sensors. Our methodology utilizes two cooperating schemes based on an advanced machine learning technique (random forests) and parametric methods that estimate object properties. The available data is limited to actuator positions (one per two link finger) and force sensors values (8 per finger). The schemes are able to work both independently and collaboratively, depending on the task scenario. When collaborating, the results of each method contribute to the other, improving the overall result in a synergistic fashion. Unlike prior work, the proposed approach does not require object exploration, re-grasping, grasp release or force modulation and works for arbitrary object start positions and orientations. Due to these factors the technique may be integrated into practical robotic grasping scenarios without adding time or manipulation overheads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>HE extraction of object properties or class through both vision and haptic feedback is a natural sensory ability afforded to humans and other animals. In the field of robotics, both sensory modalities have been investigated. Though many properties of objects may be determined visually, common issues of occlusion and/or poor lighting conditions can limit the performance of vision based methods. Furthermore, other physical properties, such as stiffness, are difficult to visually ascertain, particularly without some kind of object manipulation. Regarding haptics, humans are known to make use of various 'exploratory procedures' (EPs) <ref type="bibr" target="#b0">[1]</ref>, in order to glean object properties through active manipulation of objects by one or both hands. While there have been several robotic approaches that have taken inspiration from this concept (e.g. <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b6">[7]</ref>), such methods tend to rely on timeconsuming palpatory motion sequences and robot hand and/or arm dexterity. In various real-world robotic applications (such as industrial pick and place) time or hardware limitations are critical factors that make the use of such EPs inappropriate. Evidence has demonstrated that meaningful knowledge of object properties is acquired by humans during minimal tactile object interactions. Klatzky and Lederman used the term 'haptic glance' to describe such interactions, which are far simpler than EPs <ref type="bibr" target="#b7">[8]</ref>. In turn, the ability of a robot to acquire meaningful haptic knowledge about an object during brief and functional actions may extend the practicality of active tactile sensing into scenarios with limitations on time and/or computational and hardware capabilities. By 'functional actions' we imply actions that serve a goal beyond sensing, such as grabbing an object for transport.</p><p>In a similar vein to the above objectives, adaptive under-actuated grippers (e.g. Fig. <ref type="figure" target="#fig_0">1</ref>) have constituted highly practical robot grasping solutions, with low complexity compared to more traditional approaches. Such systems rely on simple, mechanically adaptive designs (e.g. elas-</p><p>• A.J. Spiers, M.V. <ref type="bibr">Liarokapis</ref>  tomer flexure joints, tendons and under-actuation) to passively adapt to a wide variety of object shapes and sizes without prior object knowledge, hand modelling, grasp planning, actuator regulation or sensory feedback <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These benefits result in a low cost, easily implementable solution to grasping in unstructured scenarios.</p><p>In this work we seek to combine the benefits of simple adaptive robot grippers with methods of acquiring meaningful haptic object properties and identifying objects during a single functional grasp via low-cost, commercially available tactile sensors. The resulting method combines parametric estimation and classification (machine learning) techniques, unified via a hybrid collaborative framework (Fig. <ref type="figure" target="#fig_1">2</ref>). In previous work we presented some aspects of the classifier alone <ref type="bibr" target="#b10">[11]</ref>. Our current work compliments the original classifier via the inclusion of the parametric methods (based on kinematic and stiffness estimation) and a hybrid collaborative approach, where the component outputs support each other to increase overall accuracy in a synergistic fashion (via dynamic classifier retraining and object pose estimation).</p><p>Our gripper hardware consists of a simple two-finger underactuated hand equipped with TakkTile <ref type="bibr" target="#b11">[12]</ref> barometric pressure sensors (Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>). To maintain consistency with popular open source hand designs <ref type="bibr" target="#b8">[9]</ref>, the hand does not implement joint position sensing. Our proposed method achieves object classification and object feature extraction using the tactile sensor outputs and actuator positions, measured at three instances of the grasping process.</p><p>Taking inspiration from the haptic glance concept, our approach does not modify the typical open-loop actuator /finger behavior of adaptive underactuated hands during the grasping process. All necessary computations are also designed to be completed within a short time frame (&lt;100 ms), to allow achievement of the aforementioned goals during a normal grasping process. Overall, the presented process is designed to be executed during a single, typical, functional grasp with no temporal or motor overhead. Of course, these constraints (which include uncertain hand kinematics and a single grasp action) significantly reduce data breath, resolution and redundancy compared to more traditional EP based approaches with fully actuated robotic manipulators (e.g. <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>). As a result, state measurement under such conditions is accompanied by a level of uncertainty. However, we believe that some uncertainty is acceptable, given the minimal influence of this method on the fundamental grasping activities carried out daily by thousands of real-world robots. Nevertheless, classifier performance is excellent and parameter estimations are distinct. Though we focus on addressing the problems associated with simple robot hardware and control in this paper, we believe the methods are scalable to more 'complex' robot hands and sensors. For example, greater sensor resolution would only improve the outputs of the proposed algorithms.</p><p>An example application where this haptic glance philosophy may be useful is the inspection, sorting and packaging of objects (such as fruit) as part of a production line. In this scenario, the proposed methodology could permit the class of object (e.g. apple, pear), stiffness (ripeness), size and pose within the hand to be determined as the object is being lifted from the conveyor belt (a functional and necessary manipulation action).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>Due to the limited available data (as compared to EP approaches) we propose combining the strengths of two methods of tactile data interpretation via a hybrid approach, the structure of which is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. This methodology makes use of a random forests classifier (a machine learning technique) and parametric object property estimators. The classifier is capable of high level recognition (object class extraction) of different objects based on training data. Conversely, the parametric method ai ms to provide low-level outcomes related directly to physical object properties of size and stiffness. The two schemes therefore address different aspects of the tactile sensing problem space. Additional cross communication between these two approaches leads to additional parameter determination (object pose within the grasp) and improved classification accuracy. In the latter case, parametrically determined object dimensions are used for class decision validation and dynamic classifier retraining. This retraining process rejects the current class and improves subsequent classification accuracy.</p><p>The benefits of the hybrid approach applies to various use cases. For example, remote exploration or disaster response robots may encounter objects with unique and previously unseen characteristics, such as an unusual stone or a fragment of a larger object. While the parametric method may be able to 'measure' such an object, machine learning approaches may have limited classification success due to object novelty compared to training data. Conversely, the machine learning scheme can provide high level identification in structured or semi-structured environments, such as a production line, grocery store or warehouse. Here, encountered objects will always be part The remainder of the paper is structured as follows. First, related literature will be reviewed, focusing on biological then robotic systems. A detailed explanation of experimental conditions and hardware will be described in Section 4. Methods and algorithms of the proposed schemes will be presented in Section 5 with subsequent results in Section 6. Discussion, future work and conclusions will summarize the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Roboticists have applied tactile sensing to robot hands for many decades, inspired by nature's most versatile endeffector, the human hand. The hand has approximately 17,000 mechanoreceptive units that innervate its skin and provide a highly sophisticated system for understanding the environment <ref type="bibr" target="#b12">[13]</ref>. It has been noted that motion of the hand is crucial to fully exploiting its perceptual qualities during physical interaction <ref type="bibr" target="#b0">[1]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>. Such observations have been reflected in the active-touch approaches of many robot systems. A large proportion of such endeavors make use of complex, high-density sensor systems, such as the multi-modal BioTac sensor <ref type="bibr" target="#b14">[15]</ref> (used in <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b5">[6]</ref>). This expensive (&gt;$10,000) sensor is capable of providing thermal, vibratory and multiple pressure readings over an anthropomorphic finger pad.</p><p>Artificial tactile perception efforts generally focus on either deriving physical object properties or on the higher level discrimination of an object's class. In <ref type="bibr" target="#b12">[13]</ref>, it was considered that an object's properties contribute to manipulation actions while the object class enables the execution of object specific strategies or plans. Aspects of human tactile object perception will now be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Human Haptic Data Acquisition</head><p>The ability to characterize and identify objects without reliance on vision is beneficial in a number of scenarios. In humans, common tasks such as reaching for a computer mouse or cup while reading from a computer screen discretely employ complex tactile perceptual methods <ref type="bibr" target="#b7">[8]</ref>. Such methods facilitate object understanding (e.g. determining the object class and its pose relative to the hand) and subsequent motor action (orientating the hand to facilitate appropriate use) with limited physical interaction <ref type="bibr" target="#b15">[16]</ref>. Studies on more elaborate exploration of objects have demonstrated the ability of humans to identify a large number of objects and properties through touch alone. This is via the use of exploratory procedures (EPs) <ref type="bibr">[1][14]</ref>, stereotypical patterns of active hand motions that expose particular physical properties of objects. For example, rubbing permits textural perception, while squeezing exposes stiffness. In medicine, such interactions permit identification of tissue type and underlying structures <ref type="bibr" target="#b16">[17]</ref>. It was observed in two finger palpation by surgeons that EPs were often combined <ref type="bibr" target="#b17">[18]</ref>, permitting multiple feature extraction with increased efficiency.</p><p>Investigations have also been made into the capabilities of humans to extract meaningful haptic information with limited active finger/hand motion, which is akin to the single-grasp robotic approach taken in our work. In <ref type="bibr" target="#b7">[8]</ref>, perceptual accuracy was considered for a 'haptic glance', a brief and restrained contact between fingertips and an object. Similar investigations have been considered with reduced sensory and motor <ref type="bibr">[14][16]</ref>[19] capabilities. Lederman et al. noted that minimal haptic information is often informative enough to lead to object/feature identification and appropriate subsequent manipulation <ref type="bibr" target="#b15">[16]</ref>. The methods of this paper explore minimal active touch sensing and motor control in robotic haptic perception. This is realized using data acquired during a non-exploratory 'functional' grasp with adaptive fingers. In <ref type="bibr" target="#b18">[19]</ref> it was observed that adaptive 'molding' of the human hand around objects facilitates improved haptic identification. Such 'molding' is fundamental to adaptive grippers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Robot Hands and Tactile Sensing</head><p>As previously stated, adaptive underactuated grippers permit grasping of a wide variety of objects with little control or planning effort <ref type="bibr" target="#b19">[20]</ref>. This is demonstrated in Fig. <ref type="figure" target="#fig_2">3</ref>. In particular, compliant flexure joints permit inplane finger adaptation to various conditions while maintaining grasp stability <ref type="bibr" target="#b20">[21]</ref>. The transmission mechanisms employed in this particular class of hands <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have similarities to the mechanics and resulting adaptive behavior of the human finger <ref type="bibr" target="#b21">[22]</ref>.</p><p>Despite the benefits of adaptive grippers, there has been relatively limited use of such systems in haptic applications. This is likely to be due to kinematic uncertainty of finger behavior, after encountering unknown objects in arbitrary poses. The authors of <ref type="bibr" target="#b22">[23]</ref> determined contact with an underactuated grasper using motor current models. Grasp force regulation and some object shape distinction was achieved in <ref type="bibr" target="#b23">[24]</ref> using tactile sensors and closed loop control. The use of tactile contact sensing to further enhance grasping performance through individual finger control was proposed in <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b24">[25]</ref>, tactile sensing of finger contacts with an object during workspace exploration led to optimal object/hand positioning prior to grasping. Closer to feature extraction, in <ref type="bibr" target="#b25">[26]</ref> underactuated fingers equipped with joint sensors re-constructed the contours of immobile rigid objects based on finger positions, while physically exploring a workspace. and thermal properties <ref type="bibr" target="#b6">[7]</ref>. In some cases, a direct subset of human inspired exploratory procedures (EPs) were implemented <ref type="bibr" target="#b2">[3]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b31">[31]</ref>, a series of non-human EPs were executed with a parallel jaw gripper to improve force regulation during manipulation. In <ref type="bibr" target="#b5">[6]</ref>, human inspired EPs facilitated information gathering from a BioTac sensor mounted on an anthropomorphic robot finger.</p><p>Limited motor resolution of that finger hindered textural/vibration sensing, compared to precision positioning platforms. Four EPs were implemented in early work by Dario et al. <ref type="bibr" target="#b6">[7]</ref>. A series of six EPs (variations of pushing and sliding) permitted attribution to 34 'haptic adjectives' <ref type="bibr" target="#b2">[3]</ref>, interpreted by a classifier. Though EPs permit significant extension of the spatial and dynamic range of tactile sensors, the procedures are often associated with significant time overheads. The 6 motions in <ref type="bibr" target="#b2">[3]</ref> take over 85 seconds, while the reduced motions of <ref type="bibr" target="#b31">[31]</ref> lead to a 30 second grasping process. For industrial processes, this time demand seems excessive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Robotic Tactile Classification</head><p>Tactile data is often vast, interconnected and noisy. Machine learning approaches have been used to relate such complex data to object class. Following a training process, such systems aim to identify objects from new data. Machine learning approaches have been used both for highlevel object class distinction in addition to classifying specific feature properties. In <ref type="bibr" target="#b32">[32]</ref> pressure data acquired from gripping vegetables facilitated categorization into three classes of ripeness. Unfortunately, the gripping method destroyed the vegetables, via the combination of open loop control with a fully actuated gripper. In <ref type="bibr" target="#b30">[30]</ref>, a classifier determined local surface features (edge, face, empty space) in order to construct an overall spatial object model. Active sliding of a 6-axis force/torque sensor generated data for neural network based classification into various materials in <ref type="bibr" target="#b33">[33]</ref>. The popular vision based object recognition technique 'bag-of-features' was applied to tactile data in <ref type="bibr" target="#b34">[34]</ref>. This method constructed a 'vocabulary' of tactile images based on a several grasp locations. Tactile array 'images' gathered during object squeezing and releasing were used as the basis of a k nearest neighbors' classifier in <ref type="bibr" target="#b35">[35]</ref>, though only slight object pose variations were implemented. Unsupervised learning techniques have also been applied to this problem space. Incremental online learning was applied to tactile and joint sensor data in <ref type="bibr" target="#b36">[36]</ref> to improve classifier performance. In <ref type="bibr" target="#b37">[37]</ref> a sequence of five squeezing actions followed by releasing of an object led to spatial and temporal data for a variety of hands. An unsupervised hierarchical learning methodology was employed and a 1-vs-all classifier obtained.</p><p>Reinforcement learning techniques were utilized in <ref type="bibr" target="#b5">[6]</ref> to cluster BioTac data resulting from exploratory finger motion in order to report stiffness, texture and thermal properties of objects. The work of Chu et al. <ref type="bibr" target="#b2">[3]</ref> classified data resulting from 6 EPs into adjectives via machine learning approaches. Classification of the fullness of plastic bottles was achieved in <ref type="bibr" target="#b27">[28]</ref> based on a single grasp. Unlike our approach, the bottles had consistent size and orientation, with closed loop force and velocity control of the robot gripper employed to achieve 'safe' container grasping with a PR2 parallel gripper. Such concerns are avoided in our setup via the compliant adaptive gripper. Note that approaches such as <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b35">[35]</ref> only permit object classification once an object has been released. Presumably, any subsequent actions related to object classification (e.g. sorting) would then require re-grasping.</p><p>The review of existing work has demonstrated trends in tactile identification that favor dense sensory data and extended exploration of objects. It has also been illustrated that though humans make use of EPs, useful haptic object knowledge is also often extracted via minimal, nonexploratory, active haptic interaction <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In this work we strive for an equivalent robotic approach based on such minimal interaction. By negating overheads of motion, processing, hardware and time, we propose a solution that is practically implementable. Added to this, we distinguish ourselves from previous work via the robustness of the system to perturbations in object pose (position and orientation) within the grasp of the hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>The details of our experimental setup, including the specifics of the underactuated hand and the tactile sensors embedded in the fingers, are explained in this section. The properties of the object used in the experiments are also described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Underactuated Robot Hand</head><p>The robot hand used in this study (Fig. <ref type="figure" target="#fig_0">1</ref>) consists of two prototype fingers of the Reflex Hand (manufactured by Right Hand Robotics, Boston, USA) mounted on a modified model T42 base of the Yale OpenHand project <ref type="bibr" target="#b8">[9]</ref>. Each Reflex Hand finger (Fig. <ref type="figure" target="#fig_3">4</ref>) consists of two phalanges with a distal urethane flexure joint and a proximal pin joint with torsional spring. The benefits of this arrangement are described in <ref type="bibr" target="#b8">[9]</ref>. Each finger is actuated by a single tendon, attached to a Dynamixel MX-28 actuator via a pulley. Unlike in other models of the Reflex Hand, the fingers used in this work (like similar open source designs <ref type="bibr" target="#b8">[9]</ref>[10]) do not feature joint position sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tactile Sensors</head><p>A row of barometric TakkTile force sensors <ref type="bibr" target="#b11">[12]</ref> are embedded in the compliant, high-friction grip pads of each link of the robot finger. The grip pads are cast from 'VytaFlex 40' from Smooth-On Inc. The robust and inexpensive ($150 for 5 sensors) TakkTile sensors are based on urethane encased MEMS barometers, mounted on a PCB with an 8 mm separation. Each finger features 8 sensors mounted on two such strips; 3 sensors on the distal phalanx and 5 on the proximal phalanx. Each sensor outputs a single pressure value at 100Hz, with resolution of &lt;0.01 N <ref type="bibr" target="#b11">[12]</ref>. The embedded tactile sensors were calibrated using a series of weights (10 g to 110 g), illustrating linear responses. This allowed linear sensor equalization (note that calibrated sensor values are only necessary for the parametric methods; the classification method uses uncalibrated values). Sensor 3 of the left finger showed significantly reduced sensitivity and was negated from the parametric processes. Note that the sensor outputs were used only for object classification and parametric estimation and did not provide any type of feedback to the openloop actuator control scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Objects</head><p>The experiments were conducted with two sets of 'model objects' and a set of 'everyday' (i.e. household) objects (Fig. <ref type="figure">5</ref>). The model objects were custom fabricated to constrain parameter variation and validate the parametric estimation methods. The everyday objects were selected from the YCB object set <ref type="bibr" target="#b38">[38]</ref> to represent a diverse range of size, shape, stiffness and weight parameters. The set is a recent benchmarking standard for robotic manipulation that facilitates replication of test equipment and procedures between research groups. The model objects consisted of two sets, each of which contained circular and cubic objects. The first set were fabricated from 3D printed ABS (wall thickness 4 mm) to maintain stiffness with variations in size. The second set maintained the same size but varied stiffness, via various foam materials. The everyday objects were not constrained by shape, size, or stiffness. Indeed, 5 out of the 11 objects have irregular and varied shapes and profiles, as illustrated in Fig. <ref type="figure">6</ref>. Characteristics of the model and everyday object sets are presented in Tables 1, 2 and 3.</p><p>Stiffness in all cases was non-destructively measured via a Transducer Techniques LPO-500 load cell (10 mN accuracy) mounted on a Robotzone HDA4-50 linear actuator equipped with a Novotechnik TX2 LVDT for accurate displacement measurement (0.01 mm resolution). The load cell was placed against the secured object's surface and   Though the equipment and protocol may introduce some measurement error for stiffer objects, these measurements are only used for validation against stiffness classes and/or a non-linear stiffness scale. As such, coarse stiffness levels are sufficient for our current goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODS</head><p>In this section we present the details of the machine learning and parametric method algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Collection</head><p>Data was collected by grasping each object with the robot hand 20 times, in various positions and planar orientations, within the workspace of the gripper. An additional 7 empty grasps (with no object present) were also recorded. For the everyday objects, 20 grasps were recorded for each object in constrained orientations (±45 deg) and unconstrained orientations (±180 deg). As our method currently involves no post-grasp manipulation (e.g. lifting) the hand was mounted (via clamps) to a table. During each trial (grasp), the actuators were commanded in each case to move 270 deg over 3.25 seconds with a constant target velocity and no influence from sensor feedback (open loop control). The final target position was maintained for 250 ms at the end of the motion before the actuators returned to 0deg, releasing the object. Actuator target and actual positions, plus force sensor data were recorded at 100 Hz. Images from an overhead webcam were also logged for validation purposes. All logging and control was performed via ROS.</p><p>Example of object pose variation is illustrated in Fig. <ref type="figure" target="#fig_8">7</ref>, from logged webcam data. Note that the same object surface rested on the table in all cases and that objects were not constrained after placement. It was observed that objects placed in a pose with a horizontal offset would be 'pushed' into the center of the hand by the fingers during grasping (case 1 in Fig. <ref type="figure" target="#fig_8">7</ref>). An example of the actuator and force data resulting from a single grasp is demonstrated in the first three plots of Fig. <ref type="figure" target="#fig_6">8</ref>. Other features of this figure will be further described in subsequent sections.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Machine Learning Scheme</head><p>In this subsection we present the machine learning scheme, which aims to identify objects from the data acquired during a single grasp. A classifier based on the random forests technique was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Random Forests Classifier</head><p>Random forests (RF) were originally proposed by Ho <ref type="bibr" target="#b39">[39]</ref> and Breiman <ref type="bibr" target="#b40">[40]</ref> and are an ensemble classifier based on different decision trees. The output is the most popular class between the decisions of the individual classifiers.</p><p>The RF technique provides high classification accuracy and handles multiclass problems, such as distinguishing between multiple objects with different properties. Furthermore, the method is fast and efficient when dealing with large databases and has the capability to handle high numbers of input variables. A diagram of the RF classification procedure for n trees is presented in Fig. <ref type="figure" target="#fig_7">9</ref>. Each tree of the RF is constructed from a different out-of-bag (oob) sample set from the training data. This training data comprises two thirds of the recorded grasps data. The remaining data is used for validation. Comparison of RF technique to various state-of-the-art classifiers, will be presented as part of the results (Section 6.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Feature Selection</head><p>The feature space used for discriminating between the objects is defined by the actuator and force sensor data at two different time instances of the grasping process (an additional third time instance is sampled by the parametric method). The first instance (t 1 ) is taken when the sum of actuator target positions (A T ) exceeds the sum of actual actuator positions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Feature Importance Calculation</head><p>The RF technique has an inherent capability of computing the importance scores for all feature variables and comparing them. Such a calculation is useful for optimizing the hand designs, by minimizing the number of sensors required to achieve a certain level of classification accuracy. When fewer sensors are used, their locations on the fingers become more critical. Importance calculation is based on manipulations of a subset of the training data, which are called out-of-bag (oob) samples. These oob samples are given as an input to all decision trees and the numbers of correct votes are counted. Then, the oob samples values of a feature variable m are randomly permuted. The modified samples are once again "fed" to each of the n decision trees. Importance of the feature variable m is then calculated by the operation I m = V P -V U . Where Vp is the number of correct votes cast with the m-variable permuted oob data and V U is the same metric with the untouched oob data.</p><p>The overall/raw importance score (I m ) for each feature variable m, is the average of the importance scores computed for all trees of the RF. The process is described in Fig. <ref type="figure" target="#fig_12">10</ref>. In this work, we normalize importance scores to facilitate comparisons of the different feature variables, even for different classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parametric Method</head><p>The parametric method estimates physical object parameters of size and stiffness based on data acquired during  the grasp. Size is related to a contact polygon constructed from force and actuator data. A measure of grasp stability is also provided. The parametric method makes use of several processes, as illustrated in Fig. <ref type="figure" target="#fig_10">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Forward Kinematics Estimator</head><p>To estimate the size and shape of an object, the parametric method relies on knowledge of the kinematic position of the robot fingers once a secure grasp has been made. Predicting the kinematic behavior of mechanically compliant underactuated fingers is non-trivial. In addition to the complexities of modelling flexure joints <ref type="bibr" target="#b41">[41]</ref>, multiple joint position solutions exist for each actuator position. Actual finger kinematics result from finger and transmission dynamics, which are modulated during different stages of an adaptive grasp by interaction with unknown objects <ref type="bibr" target="#b19">[20]</ref>. In the case of the fingers used in this work, the inclusion of tactile sensors permits contact detection on each phalange. Based on this, a computationally efficient kinematics estimator (Fig. <ref type="figure" target="#fig_12">12</ref>) was constructed that uses force sensor data to switch between different grasping 'modes' (Fig. <ref type="figure" target="#fig_12">13</ref>). In each mode, a different set of transmission gains (G D1, G D2, G P ) converts actuator position (A P ) to motion of the proximal and distal joint (θP, θD). A P is also equivalent to tendon length from fingertip anchor to actuator. For simplicity, θD is considered as a pin joint in the kinematic structure of the finger. Mode selection is based on F P and F D , which are the sum of individual force sensor values on the proximal and distal phalanges respectively. The force value thresholds required to halt the motion of each joint are defined independently as T D and T P. These values were determined experimentally and are higher than the threshold used for contact detection, T C . This allows the method to deal with the common case of a single finger pushing an object into the center of the hand (e.g. case 1 of Fig. <ref type="figure" target="#fig_8">7</ref>), prior to a grasp being made. The different modes may be explained as follows:</p><p>•  <ref type="figure" target="#fig_12">13</ref> denotes a distal only grasp (no proximal contact). This is also recognized by F D &gt; T D (same as Mode 3) but without precedence by a proximal contact.</p><p>The forward kinematics method is iterative and as such, determination of joint angles for a specific instance necessitates calculation of all joint angles up until that instance. Due to a lack of typical kinematic matrix operations, this process has very little computational overhead. Kinematics estimation for all of the 3250 time intervals involved in a complete grasp (e.g. the data shown in Fig. <ref type="figure" target="#fig_6">8</ref>) takes less than 30 ms using Matlab on an Intel i7 3.6Ghz PC. Motion gains and threshold values were determined via a calibration process in which joint angles were visually observed from ten overhead video frames recorded during an empty grasp and a grasp of a rigid 70 mm cylinder. Joint angles were determined by locating the spatial centroid of small visible markers attached to the finger phalanges. These markers are visible in Fig. <ref type="figure" target="#fig_8">7</ref> and Fig. <ref type="figure" target="#fig_12">14</ref>. Interpolation of marker co-ordinates with synchronized actuator data led to linear transmission models and three gains. These gains (for left and right fingers) are proximal G P = <ref type="bibr">(3.58, 3.38)</ref>, distal during mode 1 G D1 = (0.26, 0.11) and distal during mode 2 G D2 = (5.29, 5.06). Re-orientating the hand with respect to gravity can modify these gains. When the hand is orientated in the direction of gravity, as opposed to the lateral case of our experimental setup, the proximal joints displace approximately 1 deg more for 90 deg of actuator rotation. It is likely that gain interpolation from several discrete observations would permit kinematic estimation for a range of hand orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Grasp Type, Location and Quality</head><p>The hand kinematics may be combined with force data to establish the spatial co-ordinates of contact points on the finger pads for any instance of grasp data. A suitable instance for further object analysis is the moment when maximum stability has been achieved for a grasp. In this work we associate grasp stability with the number of contacts of the hand with the object, which is also suggested in <ref type="bibr" target="#b42">[42]</ref>. Prior to this moment, the object is more likely to move within the grasp. Afterwards, the object may be compressed and deformed. Counting the number of sensors whose values exceed a given contact threshold (T C ) at each time instance (t) of the grasp produces the following 'sensors-in-contact' array SC(t):</p><formula xml:id="formula_0">ܵ‫ܥ‬ሺ‫ݐ‬ሻ = ‫ܥܮ‬ ‫)ݐ(‬ ୀ଼ ୀଵ + ‫ܥܴ‬ ‫)ݐ(‬ ୀ଼ ୀଵ<label>(1)</label></formula><formula xml:id="formula_1">‫݁ݎ݁‪ℎ‬ݓ‬ ‫ܥܮ‬ = ൜ 1, ‫ܮܨ‬ &gt; ܶ 0, ‫ܮܨ‬ ≤ ܶ , ‫ܥܴ‬ = ൜ 1, ‫ܴܨ‬ &gt; ܶ 0, ‫ܴܨ‬ ≤ ܶ<label>(2)</label></formula><p>Where LC and RC are a binary array of the sensors in contact, derived from the force values FL and FR. FL and FR are force sensors measurements on left and right fingers. The SC array is searched to locate the first instance of max(SC), the maximum sensors in contact. This instance is indicated on Fig. <ref type="figure" target="#fig_6">8</ref> as 'Maximum Contacts'. The process is highly efficient and may be computed at t 2 with no requirement to release the grasp.</p><p>The value of ݉ܽ‫(ݔ‬SC) may be used to give an indication of grasp stability (G S ), when combined with grasp type determination (proximal, distal or caging, as illustrated in Fig. <ref type="figure" target="#fig_12">13</ref>). Grasp type is determined by counting the number of proximal (C D ) and distal (C P ) contacts.</p><formula xml:id="formula_2">‫ܥ‬ = ‫ܥܮ‬ ሺ‫ݐ‬ሻ ୀହ ୀଵ + ‫ܥܴ‬ ሺ‫ݐ‬ሻ, ‫ܥ‬ = ‫ܥܮ‬ ‫)ݐ(‬ ୀ଼ ୀ + ‫ܥܴ‬ ሺ‫ݐ‬ሻ<label>(3)</label></formula><p>Each grasp type is represented by a gain ‫ܩ(‬ ் ) associated with the stability of that grasp. The number of sensors in contact SC, is scaled by ‫ܩ‬ ் to give the ‫ܩ‬ ௦ score.</p><formula xml:id="formula_3">‫ܩ‬ ௌ = ‫ܩ‬ ் ܵ‫ܥ‬ሺ‫ݐ‬ሻ, ‫݁ݎ݁‪ℎ‬ݓ‬ ‫ܩ‬ ் = ൝ 1, ‫ܥ‬ = 0 ⋀ ‫ܥ‬ &gt; 0 1.2, ‫ܥ‬ &gt; 0 ⋀ ‫ܥ‬ = 0 2, ‫ܥ‬ &gt; 0 ⋀ ‫ܥ‬ &gt; 0<label>(4)</label></formula><p>The highest gain (2) is associated with a caging grasp, where the object is secured by both proximal and distal links. The proximal only grasp is rated slightly higher (1.2) than a distal only grasp (1), due to a tendency for an object that slips from a distal grasp to become secured by a proximal grasp. These gains have been determined subjectively based on observations from a variety of underactuated robot hands when interacting with the extended YCB object set <ref type="bibr" target="#b38">[38]</ref>. Example ‫ܩ‬ ௦ scores will be provided for different example cases in Section 6.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Grasp Polygon Construction</head><p>The kinematics estimator also permits the location of contact points to be established at ‫.)ܥܵ(ݔܽ݉‬ At this point, the co-ordinates of in-contact sensors may be determined from finger kinematics (Section 5.3.1) and the arrays LC and RC. These co-ordinates construct convex grasp polygons, as will be later presented Section 6.2.1 and Fig. <ref type="figure" target="#fig_16">18</ref>.</p><p>The grasp polygon provides spatial aspects of the object and grasp. One easily extractable feature is the grasp aperture (polygon width), which may be used as a simple dimensional output. We use this metric for verification of the classifier results (further detailed in Section 6.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Stiffness Estimation</head><p>Estimation of object stiffness is achieved by observing the change in all measured forces between two intervals in the grasp. Due to the limits of available data, it is not possible to fulfill the stiffness equation ܵ = ‫ܨ∆‬ / ‫,ݔ∆‬ where stiffness, S, is measured by observing a change in force ‫ܨ∆‬ for a change in surface displacement ‫.ݔ∆‬ This is because the change in finger position after a grasp has been established cannot be accurately measured. However, we have found that a metric related to object stiffness (S M ) may be achieved by considering the average reaction force per tactile sensor at two instances of the grasping process. These intervals are the same as those used for feature selection (defined in Section 5.2.2), i.e. the actuator stall instance (t 1 ) and the steady state condition of A T (t 2 ). This capability results from the open-loop nature of the grasp, which maintains some consistency between conditions.</p><formula xml:id="formula_4">‫)ݐ(ܨ‬ = ‫ܥܮ‬ ‫ܮܨ)ݐ(‬ ‫)ݐ(‬ + ‫ܥܴ‬ ‫ܴܨ)ݐ(‬ ‫)ݐ(‬ ୀ଼ ୀଵ (5) ܵ ெ ሺ‫ݐ‬ሻ = ‫ݐ‪ሺ‬ܨ|‬ ଵ ሻ| ‫ݐ‪ሺ‬ܥܵ‬ ଵ ሻ + ‫ݐ‪ሺ‬ܨ|‬ ଶ ሻ| ‫ݐ‪ሺ‬ܥܵ‬ ଶ ሻ<label>(6)</label></formula><p>Where LC, RC and SC were defined in equations ( <ref type="formula" target="#formula_0">1</ref>) and <ref type="bibr" target="#b1">(2)</ref>. By combining (FL, FR) with (LC, RC), the values of non-contacting sensors are set to zero, reducing noise. Other methods of stiffness estimation that incorporate differences in actuator position error and time were evaluated, but led to large error margins with variations in grasp type and pose, particularly for stiffer objects. Stiffness estimation of object set 2 is presented in Section 6.2.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Pose Estimation</head><p>Pose estimation allows the location of objects within a grasp to be estimated based on object class and contact locations. As such, the technique is facilitated by a synergistic collaboration between the machine learning and parametric methods. Pose information is useful for determining subsequent manipulation of an object. For example, placing a grasped object precisely on a target location will require different hand positioning depending on the location of the object in the hand. The pose estimator functions by using the object class (C) to recall a simple polygon model of an object's cross section, based on size and shape. For example, the apple object polygon is a circle 75 mm diameter. The centroid of the object polygon is matched to the Y (distal) component of the centroid of the grasp polygon to align actual and recalled data. Currently this method functions only for circular objects, as non-round objects have multiple solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classification Results</head><p>For the training of the random forest classifiers, a 10-fold cross-validation procedure <ref type="bibr" target="#b43">[43]</ref> was used to assess efficiency and avoid overfitting. The classification accuracies are reported in Table <ref type="table" target="#tab_7">4</ref>.</p><p>These results were computed by averaging multiple rounds of the cross-validation method. For all objects, data from 12 of the acquired 20 grasps were used for training, with the remaining 8 grasps used for validation. Discussion of these results follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Comparison of Various Classifiers</head><p>To test the suitability of the RF machine learning approach to this problem, the classification of everyday objects in constrained orientations was repeated with a number of alternative state-of-the-art classifiers. The methods used were a Linear Discriminant Analysis (LDA), a Naïve Bayes classifier, a Neural Network (NN), a binary Support Vector Machines classifier (SVM) and the Random Forests (RF) technique. The SVM classifier was trained using different kernels (linear, RBF etc.) and the best results were acquired. The NN classifier was constructed using a single hidden layer with fifteen hidden units trained with the Levenberg-Marquardt backpropagation algorithm. RF forests were grown with ten trees for processing speed (two times faster) and one hundred trees for accuracy. All classifiers were compared for the task of discriminating between the everyday objects with constrained orientations (±45 deg) and arbitrary positions. The classification accuracies for the different techniques are reported in Table <ref type="table" target="#tab_5">5</ref>. Random forests outperform all other classification methods, but all methods provide high classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Model Objects Classification Results</head><p>The first classification problem involved discrimination between the model objects of set 1 and 2 (see Section 4.3). The trained classifier is slightly better at discriminating between objects with different shapes and sizes rather than objects of different shapes and stiffness. Classification results for this problem are presented in Table <ref type="table" target="#tab_7">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Everyday Objects Classification Results</head><p>The second classification problem involved discrimination between the various everyday objects. Two cases are analyzed; constrained orientation and free orientation (as described in Section 5.1). As expected the classification accuracy is higher for the constrained orientations case, though the free orientation result also demonstrates excellent accuracy. Results are presented in Table <ref type="table" target="#tab_7">4</ref>.</p><p>In Fig. <ref type="figure" target="#fig_14">15</ref> we present a confusion matrix for the case of classifying everyday objects with unconstrained orientations. The diagonal elements represent the correctly classified trials while the non-diagonal elements represent the misclassifications. It may be noticed that the classifier is very efficient in discriminating between the examined objects, with only a few objects appearing harder to distinguish (apples may be classified as apricots, the gelatin box may be identified as the sugar box, the bleach is sometimes confused with the Windex etc.). These misclassifications are caused by dimensional similarities between certain sides of dissimilar objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Sensor Placement Optimization</head><p>Feature variable importance was implemented to determine optimal force sensor locations for different quantities of force sensors. The feature variables importance scores for all 36 features (18 features at two time instances -Section 5.2.2) are presented in Fig. <ref type="figure" target="#fig_17">16</ref>, for the cases of everyday objects with constrained and free orientations. The height of the bars represents the importance scores of the features for 10 sets (a distinct random splitting of the training data). All scores are shown to be robust along all sets. To evaluate this approach, the most important feature variables were selected and the classifier was retrained after removal of the redundant features. Three different cases were examined, performing retraining for the 4, 6 and 8 most 'important' sensors of the hand (shown in Fig. <ref type="figure" target="#fig_15">17</ref>), based on these values, rather than using the initial 16 sensors. These results illustrate a significant redundancy in the    initial feature space, with respect to classification method. In this respect, future hands could be constructed with fewer sensors based on the generated designs (Fig. <ref type="figure" target="#fig_15">17</ref>). In all cases the designs indicate a preference for sensor placement on the proximal regions of the phalanxes. The asymmetrical nature of the results is likely to be due to sensitivity of particular tactors and/or tendon tension difference between the robot fingers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parametric Method Results</head><p>Parametric results of dimension, stiffness and in-grasp pose estimation will now be presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Dimension Estimation</head><p>Dimension estimation was performed for the 50mm, 70mm and 90mm diameter cylindrical model objects of set 1 (Fig. <ref type="figure">5</ref>) with unconstrained starting positions. Dimensions were based on grasp polygon width, derived from the estimated finger kinematics. Examples of determined grasp polygons for an everyday object (box of sugar) are shown in Fig. <ref type="figure" target="#fig_16">18</ref>. These examples also show the measure of grasp stability G S , as defined in Section 5.3.2. Fig. <ref type="figure" target="#fig_16">18</ref> illustrates some kinematic mismatch, which is largely due to simplistic modelling of the urethane flexure joint as a pin joint (thus ignoring dynamic properties and out-of-plane motion), in addition to occasional variation in contact detection. This impacts the accuracy of kinematic mode-selection (Fig. <ref type="figure" target="#fig_12">13</ref>). Fig. <ref type="figure" target="#fig_12">19</ref> provides a histogram of estimated dimensions for all grasps. Clear distinctions are illustrated between the different objects sizes, following a trend consistent with the actual diameters of the objects, though a linearly increasing offset between estimated and actual values may be observed. It may be seen that sizable error bounds are present, particularly for the smaller, 50mm diameter cylinder. Grouping the dimension estimations by grasp types in Fig. <ref type="figure" target="#fig_1">20</ref> illustrates persistent errors for the 50mm object with the caging grasp. Conversely, little variance is indicated overall for proximal grasps. The results reflect the possibility that kinematics estimator accuracy is dependent on the grasp type. Indeed, a distal grasp can lead to further object motion (e.g. pulling the object towards the palm). Of course, the grasp does not always occur at the widest point of the object, leading to a negative offset in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Stiffness Estimation</head><p>Parametric stiffness estimations for set 2 of the model objects (Fig. <ref type="figure">5</ref>) are illustrated in Fig. <ref type="figure" target="#fig_12">21</ref>. The outputs of the stiffness metric S M were scaled between values of 0-100. The results indicate excellent distinction between three different stiffness objects (in the range 156 to 2100 N/m) with some larger distribution of errors for the most rigid object (where sensor output would often saturate during the grasp). Inspection of S M distribution in Fig. <ref type="figure" target="#fig_1">22</ref> illustrates that the results do not overlap within each grasp type. Therefore, by determining the grasp type G S via equation ( <ref type="formula" target="#formula_3">4</ref>), it is possible to automatically categorize each result into an appropriate stiffness value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Collaborative Results</head><p>The following brief results are based on collaboration (data sharing) between the parametric method and classifier to improve performance and extract further data.   Dynamic classifier retraining uses the dimension estimated by the parametric method to improve classifier performance. By omitting objects whose dimensions are inconsistent with the measured parameter it is possible to validate the initial classifier decision and, if necessary, reject the decision and re-train the classifier with a new, dynamically reduced data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">On-Line Classifier Retraining</head><p>As a hypothetical example, say the classifier were to mistakenly classify a soup can as a coffee can, perhaps due to an extreme object starting position. The estimated object dimension (provided by parametric method) immediately indicates that the grasped object is too small to be a coffee can, given known dimensions from Table <ref type="table" target="#tab_3">3</ref>. The classification decision is therefore verified as false, and the classifier is dynamically retrained, excluding all the objects from the training set whose dimensions are significantly different from the estimated dimension. Essentially, this is an a-posteriori filtering and correction of the classification decisions that increases the machine learning approach's efficiency. The retraining and reclassification (which takes less than 65 ms) may be completed without re-grasping the object. The approach was tested on the failed cases of unconstrained everyday object presented in Table <ref type="table" target="#tab_7">4</ref>. In all cases, dynamic re-training led to correct identification of the object class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Pose Estimation</head><p>Collaboration also occurs by passing the classifier output to the parametric method (as in Section 5.3.5). This enables estimation of pose for round (orientation free) ob-jects. Such pose estimation can be useful for accurate placement of objects after grasping, e.g. for multiple part assembly or the placement of an item into packaging. Sample results of this approach are illustrated in Fig. <ref type="figure" target="#fig_2">23</ref> for proximal, distal and caging grasps of a 70mm rigid cylinder. Accurate kinematic estimation of finger pose is also shown in Fig. <ref type="figure" target="#fig_2">23</ref>. The geometric cylinder model was loaded into the parametric approach via the object class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper we have presented a hybrid methodology for performing tactile classification and feature extraction during a single grasp with a simple underactuated robot hand. Such hands constitute easily implementable practical robotic grasping solutions. Similarly, our work has aimed to provide a system with low computation and complexity overheads for haptic sensing applications in practical robotics. Despite this, we believe our approach is scalable to other robotic hands with alternative sensing and actuation, providing that adaptive grasping may be implemented. In many cases this may need to be via a motor control, rather than a mechanical approach.</p><p>Promising results have been presented, showing high classification accuracy and the ability to extract several object features (object dimension, stiffness and pose) within error bounds. While more accurate parameter identification has been carried out by other robotic approaches, these have tended to focus on a single parameter as part of an extensive tactile exploration process.  Though our classification and parametric methods can work independently, a novel collaborative scheme allows outcomes to be combined. This improves classification accuracy and facilitates estimation of object pose. The various aspects of the proposed methodology are highly suited to dynamic, semi-structured environments where the time or hardware necessary for detailed haptic object exploration is not available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>TFig. 1 :</head><label>1</label><figDesc>Fig. 1: The adaptive underactuated hand used in this work. Each finger has a proximal pin joint and distal flexor joint, both driven by a single tendon. TakkTile sensors are embedded in the grip pads of each finger.</figDesc><graphic coords="1,322.20,393.84,228.36,171.24" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The collaborative hybrid approach. Machine learning and parametric schemes operate simultaneously during a single, open-loop, object grasp. Outputs have rounded corners.</figDesc><graphic coords="2,325.80,64.68,206.76,200.76" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An OpenHand T42 securely grasps a variety of objects using open loop motor control with compliant adaptive fingers.</figDesc><graphic coords="3,340.56,63.60,189.36,148.92" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: An underactuated prototype Reflex Hand finger, equipped with barometric TakkTile sensors. There are no position sensors.</figDesc><graphic coords="4,100.56,64.08,152.52,161.04" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: The three sets of objects used in this work. Product logos have been obscured for copyright considerations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>then extended by 1 to 5 mm, depending on object properties (e.g. a 5 mm compression of the plastic fruit would break it). Change in load cell output was combined with displacement to give a measure of stiffness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Actuator position and force sensor data during a single object grasp. Triangular markers denote events identified by the methodology. The bottom plot shows the number of sensors in contact during a grasp. The stable grasp start may occur before or after actuator deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The random forests classifier constructed with n trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Examples of object pose variation and resulting grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(A P ) by a given threshold, (T Stall = 20 deg) via |A T -A P |&gt; T Stall . This deviation indicates a stall in actuator motion due to finger /object interaction. The second instance (t 2 ) occurs when actuator target positions (A T ) have reached the steady state (t=3.25s) at which time hand reconfiguration has stopped and the system is at an elastic equilibrium. Here, the object is being held with constant tendon exertion. These instances are indicated as 'Deviation' and 'Target Motion End' on Fig. 8. Actual actuator positions (2 values) and force sensor readings (16 values) are extracted at these two instances, giving a fea-ture space of 36 variables. This raw data is obtained without a-priori information regarding the robot model or actual joint angles, making the machine learning methodology model-free. Another beneficial characteristic of the classifier is that it does not require calibrated force values, as classification is based on the differentiation in input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Processes within the parametric estimator. Outputs are shaded boxes. Classifier dependent components (such as object class, C) have dotted lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :Fig. 12 :</head><label>1012</label><figDesc>Fig. 10: Random forests feature importance calculation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Mode 1 -</head><label>1</label><figDesc>Pre-contact: 'Free motion' of the finger prior to object contact. Actuator motion generates a large change in θP and small change in θD. • Mode 2 -Proximal Contact: If F P &gt; T P motion of the θP stops. Actuator motion is transferred to θD. • Mode 3 -Proximal &amp; Distal Contact: If F D &gt; T D motion of both joints stop. • Mode 3a -Distal Only Contact: Mode 3a in Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Kinematic mode progression based on finger adaptation to object contact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Confusion matrix for the case of classifying everyday objects with unconstrained orientations.</figDesc><graphic coords="10,327.96,63.60,207.84,172.92" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 :</head><label>17</label><figDesc>Fig. 17: Optimal sensor placement for 6 and 8 sensor setups based on features variables importance.</figDesc><graphic coords="11,336.24,63.60,201.36,108.12" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 :</head><label>18</label><figDesc>Fig. 18: Top row: grasp polygon reconstruction from finger kinematics estimation. GS shows grasp stability score. Middle row: corresponding video frame of the most stable grasp instance, based on max(SC). Bottom row: object starting pose.</figDesc><graphic coords="11,340.20,562.44,193.92,141.00" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Feature variables importance bar plots for discrimination of everyday objects with a) constrained and b) free orientations.</figDesc><graphic coords="11,64.08,69.12,221.16,251.76" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 :Fig. 20 :</head><label>1920</label><figDesc>Fig. 19: Histogram of parametrically estimated diameters of three stiff cylinder objects (from model set 1) and an empty grasp.</figDesc><graphic coords="12,56.88,39.84,228.84,172.08" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 21 :Fig. 22 :</head><label>2122</label><figDesc>Fig. 21: Histogram of parametrically estimated stiffness (scale 0-100) with for various stiffness cylinders (from model set 2)</figDesc><graphic coords="12,309.96,53.04,228.48,170.64" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc>SIZES (mm) OF MODEL SET 1 OBJECTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 .</head><label>2</label><figDesc>STIFFNESS (N/m) OF MODEL SET 2 OBJECTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 .</head><label>3</label><figDesc>CHARACTERISTICS OF THE EVERYDAY OBJECTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc></figDesc><table /><note><p>reports classification results for all cases.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 .</head><label>5</label><figDesc></figDesc><table /><note><p>COMPARISON OF DIFFERENT CLASSIFIER ACCURACY ON EVERYDAY OBJECTS WITH CONSTRAINED ORIENTATIONS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 .</head><label>6</label><figDesc>EFFECT OF SENSOR REDUCTION ON CLASSIFICATION ACCURACY FOR EVERYDAY OBJECTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 .</head><label>4</label><figDesc>CLASSIFICATION RESULTS FOR ALL EXPERIMENTS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>SPIERS ET AL.: SINGLE-GRASP OBJECT CLASSIFICATION AND FEATURE EXTRACTION WITH SIMPLE ROBOT HANDS AND TACTILE SENSORS.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Raymond R. Ma for his expertise in robot hand design. Kevin Gemmell is acknowledged for assistance in object stiffness measurements.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work has been partially supported by the NSF NRI grant IIS-1317976.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extracting object properties through haptic exploration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Haptic Exploration of Objects with Rolling and Sliding 3 . Exploratory Procedure Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Cutkosky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using robotic exploratory procedures to learn the meaning of haptic adjectives</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez-Tejada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nappo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kuchenbecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3048" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Use of tactile feedback to control exploratory movements to characterize object compliance</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>JULY</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A potential field approach to dexterous tactile exploration of unknown objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bierbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 8th IEEE-RAS International Conference on Humanoid Robots</title>
		<meeting><address><addrLine>Humanoids</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="360" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian exploration for intelligent identification of textures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Planning And Executing Tactile Exploratory Procedures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacalone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Allotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buttazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sabatini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1896" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying objects from a haptic glance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1111" to="1123" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A modular, opensource 3D printed underactuated hand</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Odhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Robotics and Automation</title>
		<meeting>-IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2737" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stable, open-loop precision manipulation with underactuated hands</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Odhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unplanned, Model-Free, Single Grasp Object Classification with Underactuated Hands and Force Sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liarokapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5073" to="5080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Feel of MEMS Barometers: Inexpensive and Easily Customized Tactile Array Sensors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Jentoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<biblScope unit="issue">c</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coding and use of tactile signals from the fingertips in object manipulation tasks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="345" to="359" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand movements: A window into haptic object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="368" />
			<date type="published" when="1987-07">Jul. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-modal synergistic tactile sensing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wettels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RAS International Conference on Humanoid Robotics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Haptic identification of common objects: effects of constraining the manual exploration process</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="618" to="628" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Haptic feedback for medical applications , a survey</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Vander Poorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lammertse</surname></persName>
		</author>
		<editor>Actuator</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experimentally driven design of a palpating gripper with minimally invasive surgery considerations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Persad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Haptics Symposium</title>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
			<biblScope unit="page" from="261" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Haptic identification of objects and their depictions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underactuated Robotic Hands</title>
		<author>
			<persName><forename type="first">L</forename><surname>Birglen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laliberté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Tracts in Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page">244</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Limits to Compliance and the Role of Tactile Sensing in Grasping</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Jentoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D H</forename><surname>Fellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6394" to="6399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Framework for Studying Underactuation in the Human Hand</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M D</forename><surname>Ravi Balasbramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Americal Society of Biomechanics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A compliant self-adaptive gripper with proprioceptive haptic feedback</title>
		<author>
			<persName><forename type="first">B</forename><surname>Belzile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birglen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robot hand with soft tactile sensors and underactuated control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tsutsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Murashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Honma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Akazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting>the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4148" to="4151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contact sensing and grasping performance of compliant hands</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Jentoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="75" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Determining object geometry with compliance and simple sensors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Jentoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3468" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object texture recognition by dynamic tactile sensing using active exploration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drimus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Borlum</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bilberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="277" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tactile object class and internal state recognition for mobile manipulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">23: Pose estimation of a cylinder. The object class polygon (circle) overlaps the contact polygon</title>
	</analytic>
	<monogr>
		<title level="m">GS scores are provided IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2342" to="2348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Feature Detection for Haptic Exploration with Robotic Fingers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Cutkosky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active contour following to explore object shape with robot touch</title>
		<author>
			<persName><forename type="first">U</forename><surname>Martinez-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Lepora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 World Haptics Conference</title>
		<meeting><address><addrLine>WHC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-inspired robotic grasp control with tactile sensing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kuchenbecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1067" to="1079" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tactile sensing based softness classification using machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bandyopadhyaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roychowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Advance Computing Conference (IACC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1231" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of neural-network-based classifiers for material classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ekong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="367" to="377" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Object identification with tactile sensors using bag-of-features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reisert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="243" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Design of a flexible tactile sensor for classification of rigid and deformable objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drimus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bilberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incrementally learning objects by touch: online discriminative and generative models for tactile-based recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on haptics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="512" to="525" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ST-HMP: Unsupervised Spatio-Temporal feature learning for tactile data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2262" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M D</forename><surname>Berk Calli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>arXiv:150203143</idno>
		<title level="m">Benchmarking in Manipulation Research: The YCB Object and Model Set and Benchmarking Protocols</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random decision forests</title>
		<author>
			<persName><forename type="first">Kam</forename><surname>Tin</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Conference on Document Analysis and Recognition</title>
		<meeting>3rd International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The smooth curvature model: An efficient representation of Euler-Bernoulli flexures as robot joints</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Odhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="772" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grasp quality measures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koutroumbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">5748</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
