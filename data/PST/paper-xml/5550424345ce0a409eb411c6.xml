<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset</title>
				<funder ref="#_Wb38Zvq #_nXeJWAM">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Houwei</forename><surname>Cao</surname></persName>
							<email>houwei.cao@uphs.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department at Ursinus College</orgName>
								<address>
									<addrLine>601 E. Main Street</addrLine>
									<postCode>19426</postCode>
									<settlement>Collegeville</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Univer-sity of Illinois at Chicago</orgName>
								<address>
									<addrLine>1007 West Harrison Street, M/C 285</addrLine>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Keutmann</surname></persName>
							<email>michaelk@alumni.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Neuropsychiatry section of the Psychiatry De-partment</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<addrLine>3400 Spruce Street, 10th Floor</addrLine>
									<region>Gates Bldg</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruben</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
							<email>gur@mail.med.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Philadelphia Veterans Administration Medical Center</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
							<email>nenkova@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<addrLine>3330 Walnut Street</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ragini</forename><surname>Verma</surname></persName>
							<email>ragini.verma@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">University of Penn-sylvania</orgName>
								<address>
									<addrLine>3600 Market Street, Suite 380</addrLine>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TAFFC.2014.2336244</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Emotional corpora</term>
					<term>facial expression</term>
					<term>multi-modal recognition</term>
					<term>voice expression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People convey their emotional state in their face and voice. We present an audio-visual data set uniquely suited for the study of multi-modal emotion expression and perception. The data set consists of facial and vocal emotional expressions in sentences spoken in a range of basic emotional states (happy, sad, anger, fear, disgust, and neutral). 7,442 clips of 91 actors with diverse ethnic backgrounds were rated by multiple raters in three modalities: audio, visual, and audio-visual. Categorical emotion labels and real-value intensity values for the perceived emotion were collected using crowd-sourcing from 2,443 raters. The human recognition of intended emotion for the audio-only, visual-only, and audio-visual data are 40.9%, 58.2% and 63.6% respectively. Recognition rates are highest for neutral, followed by happy, anger, disgust, fear, and sad. Average intensity levels of emotion are rated highest for visual-only perception. The accurate recognition of disgust and fear requires simultaneous audio-visual cues, while anger and happiness can be well recognized based on evidence from a single modality. The large dataset we introduce can be used to probe other questions concerning the audio-visual perception of emotion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>motion is conveyed by both facial and vocal expressions. Numerous prior studies followed Ekman's basic discrete emotion theory and concentrated on emotion perception from facial cues. They have established that prototypical basic emotions can be universally recognized by different groups of people based on the activation of specific facial expressions <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref>. Parallel research exists in vocal expression <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>. Many prosodic features, e.g., pitch, duration, loudness, voice quality, etc. contribute to the transmission of emotional content in voice. Although acoustic correlates are subject to large inter-speaker variability, it has been commonly found that pitch is the most important feature in emotion communication, followed by duration and loudness <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr">8]</ref>. In addition, recent computational emotion recognition systems use spectral features to improve recognition <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>. Research on bimodal perception of emotional expressions in faces and voices together has received considerable attention only in recent years. It is well-established that facial expressions convey more information about a subject's emotional state than changes in voice, which typically convey arousal <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. Some studies investigate how audio and video information is integrated and focus particularly on the issues arising when conflicting interpretations of the facial and vocal expression are possible <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>. However, we still do not have a full understanding of the interplay between the two modalities. Specifically, it is of interest to know how often and for which emotion the audio and visual modalities exhibit complementarity (i.e. when the combination of modalities create an impression different than either of the individual modalities create), dominance (when the two modalities create impressions of different emotions, one of which matches the impression from multi-modal perception) and redundancy (when the two separate modalities create impressions of the same emotion that match the impression from multi-modal perception).</p><p>Datasets consisting of emotion expressions are available that contain rated visual displays <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>, auditory stimuli <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref> and audio-visual clips <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">29]</ref>, but these are relatively small, with perceptual rating only from a few raters or do not contain independent ratings for each modality of the same recording. As a result, researchers have been limited in their ability to tease apart the contribution of visual and audio modalities to the perception of intended emotion.</p><p>We present the Crowd-sourced Emotional Multimodal Actors Dataset, CREMA-D, a labeled data set for the study of multimodal expression and perception of basic acted emotions. The large set of expressions was collected as part of an effort to generate standard emotional stimuli for neuroimaging studies, and these require varying degrees of length and intensity and separation of visual and auditory presentation modalities. Actors were used to generate these stimuli because they have training in expressing emotions clearly at varying levels of intensity. They were coached by professional theatre directors. CREMA-D includes 7,442 clips from 91 actors and actresses, with diverse age and ethnicity, expressing six of the seven "universal emotions" <ref type="bibr" target="#b31">[30]</ref>: happy, sad, anger, fear, disgust, and neutral (surprise was not considered by the acting directors to be sufficiently specific, as it could relate to any of the other emotions with rapid onset). The data set contains a collection of multiple perceptual ratings for each clip in three different modalities (audioonly, visual-only, or audio-visual) that were submitted for validation by 2,443 raters using crowd sourcing. Each clip from each modality received two types of emotion perception ratings--categorical emotion label and intensity. Multiple ratings for the same clip are summarized in group perceived emotion labels, i.e. the emotion selected most frequently by the raters; intensity for the group perceived emotion is the average intensity specified by the raters for that emotion. In addition, from each of the three perceptual perspectives, each clip is categorized into one of three subsets depending on the combination of intended and group perceived emotions. Matching clips are those where the most common label from the perceptual ratings is assigned by the majority of raters and it matches the intended emotion. Non-matching clips are those where the majority of raters identified an emotion that is different from the intended emotion. Finally, the ambiguous clips are those without a majority group perceptual agreement.</p><p>Our work offers the first data set to address the question of audio-visual perception, meeting the following 5 criteria: (1) over 50 actors (2) over 5,000 clips (3) 6 emotional categories (4) at least 6 ratings per clip (5) 3 modalities. At most three of these five criteria are satisfied for older datasets, as illustrated in Table <ref type="table" target="#tab_0">1</ref>. The GEMEP database <ref type="bibr" target="#b32">[31]</ref> includes ratings with 15 categories, with at least 23 raters per clip, using all three rating modalities. However, the clips come from only 10 actors with 1,260 rated clips. De Silva et al. <ref type="bibr" target="#b33">[32]</ref> investigated the interactions be- AV Integration data set combines silent video clips (the subject is not talking while expressing emotion on the face) and speech audio clips from separate sources in order to create the rated emotional displays as well as mismatched emotions. That dataset only compares fear and disgust <ref type="bibr" target="#b34">[33]</ref>. The AV Synthetic Character study uses animated video clips that are synchronized with human audio clips <ref type="bibr" target="#b35">[34]</ref> to make the audio-visual stimuli. Some audio-visual databases have a set of audio data with audio ratings and video data with video ratings, but no mixed audio-visual data with corresponding ratings <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref>. IEMOCAP <ref type="bibr" target="#b38">[37]</ref> and the Chen bimodal databases <ref type="bibr" target="#b39">[38]</ref> have an extensive collection protocol, but only include ratings when both the audio and visual information is present.</p><p>Similarly, the HUMAINE <ref type="bibr" target="#b40">[39]</ref> and RECOLA <ref type="bibr" target="#b41">[40]</ref> databases have many different types of labels to get at different aspects of audio-visual emotional expression, but all annotations are made for portions of the recording where both vocal and facial information is present. The CHAD database has over 5000 audio-visual clips with 7 emotional categories and 120 raters per clip, but only the audio is rated <ref type="bibr" target="#b42">[41]</ref>. The MAHNOB-HCI <ref type="bibr" target="#b43">[42]</ref> database is a recent audio-visual database of participants watching emotional videos that has self-reported emotion labels. GEMEP database includes 1,260 audio-visual expressions with 15 emotions expressed by 10 actors. Each expression is evaluated by 18 -20 raters <ref type="bibr" target="#b32">[31]</ref>. Further details on other datasets of emotion expression can be found in <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b44">43]</ref>.</p><p>Our dataset has a large number of actors and raters, which allows us to study the variation in successful emotion communication. It also provides crowd-sourced ratings in all three modalities, allowing for comparisons between single mode and multimodal perception.</p><p>Crowd sourcing gives us a large number of total raters, which increases the ecological validity of the ratings. In addition, crowd sourcing allows us to create tasks that do not require too many ratings from any single rater. This protects against bias to the judgments of specific raters. Crowd sourcing also allows us to collect a larger number of ratings per clip and enables us to study the variation of emotion perception of the intended emotions.</p><p>CREMA-D explicitly distinguished between matching, non-matching and ambiguous clips. The dataset will be released with classes of emotional expressions and levels of ambiguity. These characteristics permit the development of applications needing subtle and unprototypical emotion expressions. In some studies, such as the Berlin dataset <ref type="bibr">[8]</ref>, the matching clips are the only ones released as part of the dataset. Some datasets, such as IEMOCAP <ref type="bibr" target="#b38">[37]</ref> and FAU Aibo <ref type="bibr" target="#b24">[24]</ref>, do not characterize stimuli ambiguity at all. In our dataset however we may retrieve prototypical/non-prototypical emotion expressions by further analyzing the evaluation file distributed as part of the dataset. In most studies there is no mention of such a distinction.</p><p>The rest of the paper presents the full details of our data acquisition effort and a first analysis of the interplay between modalities in emotion expression. The aim of the presented analysis is to highlight the potential of the dataset and the various applications in which the corpus can be used. Stimuli preparation is described in Section 2; the acquisition of emotion expressions in Section 2.1, and the ratings protocol and crowd-sourcing method in Section 2.2. Section 3 presents the cleaning of spurious responses from the raw crowd sourcing data and a discussion of how we defined the group response for each emotion expression clip. In Section 4, we further investigate the variability of our dataset and discuss the advantages of the wide range of emotion expressions involved in our collected dataset. In Section 5, we explore how people perceive emotions in terms of audio, video, and audiovisual modalities and further discuss the interaction between modalities in the perception of different emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DATA PREPARATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Audio-Visual Stimuli</head><p>The stimuli for our study include video recordings of professional actors working under the supervision of professional theatre directors. The task for the actors was to convey that they are experiencing a target emotion while uttering a given sentence. The director guided the actors by describing scenarios specially designed to evoke the target emotion. An example scenario for happy is: "Ask the actor what their favorite travel destination is. Tell them they've just won an all-expenses paid 10-day trip to the destination." Some actors preferred to work with personal experiences rather than the scripted scenarios to evoke the emotion and were allowed to do so. The actors acted out a given sentence in a specific target emotion until the director approved the performance. Two directors were required due to the large number of participants who needed to be scheduled.</p><p>There are 91 actors, 48 male and 43 female (51 actors worked with one director, 40 with the another). The actors were between the ages of 20 and 74 with a mean age of 36. Table <ref type="table" target="#tab_3">2</ref> provides detailed age information. Several racial and ethnic backgrounds were represented in the actor group: Caucasian, African American, Hispanic, and Asian. Table <ref type="table" target="#tab_4">3</ref> provides a detailed breakdown of the racial and ethnic groups.</p><p>Recording sessions typically lasted about three hours and the full sessions were captured on video. Recording sessions took place in a sound attenuated environment with professional light boxes. Actors were seated against a green screen for ease of post-production editing. Videos were recorded on a Panasonic AG-HPX170 at a resolution of 960x720 in the DVCPRO HD format. A directional, farfield microphone is used to collect the audio signal. The final emotional video clips were manually extracted from the full recording of raw digital video files and converted to MP4 using H.264/MPEG-4 Part 10 compression for video and AAC compression at 48 kHz for audio. Videos were further converted to Adobe Flash video, cropped from widescreen to a full screen aspect ratio (4:3).</p><p>The target emotions were happy, sad, anger, fear, disgust, as well as neutral. There are 12 sentences, each rendered in all of the emotional states. The actors were directed to express the first sentence in three levels of intensity: low, medium, and high. For the remaining 11 sentences the intensity level was unspecified. It would have been prohibitively expensive to record all sentences for three levels of emotion. The expressions of the one sentence that we did collect can be used in pilot studies for feature analysis of features related to the expression of emotion or as tiered test data to quantify the change of detection capabilities at different intensities.</p><p>The semantic content of all 12 sentences was rated as emotionally neutral in a prior study <ref type="bibr" target="#b45">[44]</ref>. ? We'll stop in a couple of minutes. According to the data acquisition design, the data should consist of 7,462 clips, where a clip is the rendition from actor A of a target emotion E while speaking sentence S. However, technical issues prevented 20 clips from being extracted from the original videos. Three of the sentences were missing all clips from one actor, accounting for 18 of the missing clips. In addition, two sentences were missing one neutral clip from one actor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Perceptual Ratings From Crowd Sourcing</head><p>We collected perceptual ratings for three versions of the clips in the actors' database: the original audio-visual video clip, audio-only, and visual-only. Each version corresponds to a modality of perception. With 7,442 original clips, there are a total of 22,326 items for annotation. The goal was to collect 10 ratings for each item, for a total of 223,260 individual ratings.</p><p>We created a perceptual survey using Adobe Flash and utilized crowd sourcing to obtain ratings. We hired raters through Survey Sampling International (SSI), which specializes in providing support for survey research and recruits subjects for this purpose. SSI advertised the task to their pool of participants and recruited the 2,443 raters who completed our task.</p><p>The raters were between the ages of 18 and 89 with a mean age of 43. Of them 40.5% were male and 59.5% female. They were mostly Caucasian, but some African American, Hispanic, and Asian raters participated as well. Table <ref type="table" target="#tab_2">4</ref> shows the distribution of race and ethnicity for the raters next to the actors for comparison.</p><p>Each rater was allowed to do only a single session of annotation. At the beginning of the session, participants were asked to use headphones, and a two-part listening task was given before the rating started in order to make sure that quiet speech was audible and loud actor speech was tolerable while using headphones. The listening task consisted of samples of words recorded at low, medium, and high intensity. The low intensity approximated the quietest speech from the actors' clips. The high intensity approximated the level and quality of the loudest speech from the actors' clips. The medium intensity was recorded at a level in between the high and the low intensity.</p><p>First, raters were asked to complete a sound calibration task. Three word samples are used as examples of low, medium, and high volume clips as shown in Fig. <ref type="figure">1</ref> on the left. The participant is asked to listen to the word samples as many times as needed and to adjust the volume in order to hear the low volume sound without the high volume sound being too harsh. Once they are satisfied with their sound settings, each participant moves from the calibration task to the sound test. The sound test allows up to three trials to recognize three new word samples at low, medium, and high levels. Each trial starts with a single audio presentation of the three samples. Then raters have to select the words they heard from a list of twelve words displayed on a grid, shown on the right panel of Fig. <ref type="figure">1</ref>. When a participant correctly selects the three target words in the trial, they   pass the sound test and can move on to the perceptual rating task. Participants who fail three sound trials are not permitted to continue with the perceptual rating. 12 words were used as target words so that there would be no repetition of words in either the sound calibration task or any of the trials in the sound test. In addition, 27 foil words were chosen so that each trial would have 9 unique alternate choices on the multiplechoice grid. The target words and the foil words were chosen from WRAT4 reading lists <ref type="bibr" target="#b46">[45]</ref> so that all participants would know the words. For each rater, the 12 target words and the 27 foil words are pseudo-randomly ordered, and each word is used only once per rater.</p><p>Upon completion of the sound test, raters were given a description of the rating task, which is to specify two ratings of emotion expressed in each clip presented to them. The first rating is the emotional category. Raters can select an emotion from the target emotions (anger, disgust, fear, happy, sad) or select "No Emotion" if they do not perceive any specific emotions. The second rating is a continuous value, which corresponds to the intensity of the emotional state if an emotion is recognized or the confidence level that there is no perceivable emotion if "No Emotion" is selected.</p><p>Raters were then presented with three sections consisting of clips with a single modality: first audio-only, then visual-only, and finally audio-visual (the original clip). Each section began with a short description of the respective modality, followed by two practice questions so that the raters could get used to the form of the presentation and the interface for selecting emotions and intensities. Once the practice questions were completed, a screen would indicate the beginning of the real questions for the section. Each clip was presented, then the video display became black for visual stimuli, and raters were asked to select an emotion. Once an emotion was selected, an option for intensity rating is displayed below the selected emotion. This sequence is shown in Fig. <ref type="figure">2</ref>. The intensity rating is provided on a continuous sliding scale that is scored from 0 to 100. If someone wanted to change their selection, they could press the reset button to go back to the emotion selection dialog. After the 'Continue' button was pressed, the next clip would be presented. This would continue until the end of each section.</p><p>Each rater annotated one list of assignments with 105 items each. Each list of assignments had three parts, corresponding to each modality, which were always presented in the same order: 35 audio-only, 35 visualonly, and 35 audio-visual videos. In each modality, there were two practice items presented at the beginning, 30 items for the CREMA-D dataset, and three items that were repeated in order to test the consistency of ratings. The repeated items were randomly interspersed among those for the final dataset; among these, the first rating provided was used in the dataset, the second one was only used to check consistency. Across the modalities in each list of assignments, there were no items associated with the same original clip. The same actor is however occasionally seen in items from different modalities.</p><p>We created 2,490 assignment lists to guarantee that each item is rated at least 10 times. Each sentence was assigned a number from 1 to 7,442. Each rating session was assigned the next available list of 90 unique sentences (30 audio-only, 30 visual-only, and 30 audio-visual). Each of the three sub-lists for each section of the survey was pseudo-randomly permuted. Then 9 file numbers (3 for each section) were selected at pre-specified locations for duplication, and the duplicated values were inserted at pre-specified positions. These duplicated values are used for the consistency test. Some raters did not complete the session. They either stopped before rating all of their clips, had technical difficulties, or failed the sound test. Since incomplete sessions suggest lack of interest in the task, ratings from incomplete sessions were discarded and the assigned list was added back to the available lists. This required manual intervention and left us with 47 incomplete lists.</p><p>We lost about 1% of ratings (or 733 ratings per mode) in transit from the flash program to our server. Despite of this, only about 1,700 stimuli have fewer than 10 ratings in each mode. This is due a combination of the data lost in transit and the 47 lists that were not completed. With this data loss, there were still more than 95% of clips with 9 ratings or more before the data was cleaned, and 8 ratings or more after the data was cleaned. The data cleaning is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA CLEANING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Producing a Reliable Dataset</head><p>A concern in crowd-sourcing data acquisition is "cheating" or "poor" responses <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b52">51]</ref>.</p><p>Our study was designed to be less attractive to cheaters as suggested by Eickhoff and de Vries <ref type="bibr" target="#b49">[48]</ref>. Cheating on our task does not gain the participant too much time and each participant is given a unique order of presentation, so raters cannot copy from each other.</p><p>To eliminate poor responses due to distraction, we remove responses that exceed a threshold of 10 seconds for the initial response. The average response time for all responses is 3 seconds and median response time is 1.7 seconds. Taking the median and mean as reference points, it may be safe to assume that participants who took more than 10 seconds to respond were most likely distracted when answering the question. As a result, 7,687 responses are removed as distracted responses, accounting for 3.6% of the full set of 219,687 responses from the 2,443 participants. There was no good way to assess if questions were answered too quickly, since a good answer could be formulated during the presentation of the stimulus. The rater could click as soon as the stimulus stops if they position the mouse adeptly.</p><p>We chose to keep all other annotations. In traditional data annotation efforts it is considered essential to quantify annotator consistency and inter-annotator agreement. Assessing annotation reliability for our data set, however, poses a number of challenges. First, even before starting the annotation we expected that some of the stimuli are ambiguous, expressing nuanced or mixed emotions. Our aim was to identify the ambiguous stimuli as those where the annotators did not agree. This aspect of the problem we address complicates the study of interrater agreement because disagreement on these stimuli is not a sign of an annotation problem.</p><p>The other difficulty for the application of standard reliability measures is that there is very little overlap of raters between clips---most clips were annotated by different groups of raters. This was done by design, to increase the chance that each clip is rated by a set of good raters and minimize the possibility that a the same group of poor raters annotates multiple stimuli.</p><p>Despite these challenges we provide reliability analysis in two ways. First we analyze the reliability of individual raters in terms of real-valued annotation of emotion intensity. We investigate the reliability of raters in terms of the correlation between the individual rater's annotation and the "reference" label calculated as the averaged intensity rating of all raters. These calculations are per annotator, with correlations computed for all the stimuli rated by this particular annotator. Our raters show reasonable reliability. Most of them achieve moderate or high correlation with the reference labels. Specifically, when we consider the average correlation score of all 6 emotions, only 20 raters out of 2443 raters (~1% of the raters) show week correlation (correlation &lt; 0.4); 888 raters (~36% of the raters) achieve moderate correlation (0.4 &lt; correlation &lt; 0.7); and the rest of 1535 raters (~63% of the raters) obtain strong or high correlation (correlation &gt; 0.7). There is a stark difference in rater agreement with the reference dependeing on the modality of the stimuli. The highest average correlation of 0.75 with the reference label is on audio-visual clips, dropping to 0.72 on video-only clips. The lowest average correlation of 0.62 is on audio-only clips. We provide the detailed histogram of rater's correlation on each modality and emotion in the paper supplement.</p><p>Next we investigate the overall inter-annotator agreement. Here we analyze groups of annotators. The calculations are perfromed for each unique group of raters that annotated at least one clip together, on all clips for which at least two of the annotators of the group provided judgements. We analyze the agreement on the nominal emotion rating on the entire accepted dataset, as well as on the subset for which we find that 80% of the raters agree on the emotion label. We consider the latter subset of emotions to be clear depictions of emotional states. We consider the stimuli where the majority emotion label was given by fewer than 80% of the raters as examples of ambiguous portrayal of emotion as we discuss in later sections so for these agreement between the annotators in not expected. We also analyze the agreement on the scale intensity rating.</p><p>We use Krippendorff's alpha as our reliability metric <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b55">54]</ref> because it can handle categorical responses (selected emotion label) as well as ratio responses (where 0 is the lowest value and means absence, like our realvalue intensity rating). It can also handle missing responses which is common for our dataset as discussed at the beginning of this section. Our methods of analysis are fully defined in the supplement. Results are summarized in Table <ref type="table" target="#tab_5">5</ref>, listing the average number of clips over which the scores are computed, as well as the average number of raters from the group in these clips, along with the alpha values. The average alpha reliability score for the full set is 0.42. When assessing the reliability only for the subset of data for which at least 80% of the annotators identified the same emotion, the average alpha reliability score is 0.79. Given this group-wise analysis of inter-annotator agreement, each clip is characterized by the reliability of annotators that rated the clip. In studies with specific requirements on realiability researchers could select a subset of stimuli rated by raters with high agreement. We provide the detailed analysis of Krippendorff's alpha in the supplement.</p><p>In addition to reliability analysis, we measure rater self-consistency based on how raters annotate the duplicated clips. In our study, 9 clips were selected (3 for each modality) for duplication. The consistency of an annotator is defined as the fraction of all repeated clips that were assigned the same label among all repeated clips. The detailed consistency information of every individual rater will be provided in the CREMA-D dataset, such that the database users can decide how to use the data based on their own applications. In general, the consistency of individual raters is high, with overall consistency close to 70%. The expected accuracy of randomly selecting the same emotion of a clip as that given in the first rating is 16%. In addition, we observe that there is less consistency on the low intensity stimuli, and annotators are more likely to change their annotation on those clips. Specifically, for the clips with consistent rating, we notice the higher intensity (mean 65.5, std 23.6), while the intensity for the clips where annotators were not consistent are much lower (mean 54.5, std 24.9). We discuss the consistency of raters on the three modalities in section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grouping the Responses</head><p>After the data were cleaned, we tabulated the responses by sentence and modality. The tabulated responses include the number of times a clip has been recognized as a given emotion, the total number of responses, the mean intensity value for each emotion label, the mean confidence value for neutral responses, and normalized versions of intensity and confidence values. The intensity and confidence values are normalized per rater to span the full range of 0 to 100 matching the scale of the original ratings. Specifically, the mean intensities for audio-only are around 50 for all emotions, except for anger, which is nearly 60. The mean intensities for visual-only is around 60 for all emotions except for happy, which is almost 70. For audio-visual responses, the mean intensity is in between the visualonly and the audio-only intensity except for anger, where the intensity is slightly higher than visual-only.</p><p>We define unambiguous clips as those with a group perceived emotion identified by the majority of raters. Ambiguous clips are those with a group perceived emotion with no majority. Some unambiguous clips had rater agreement on the emotion, but the group perceived emotion did not match the intended emotion of the actor despite a director validating the emotion at the time of acquisition. We therefore further split the unambiguous clips into a matching subset, in which the group perceived emotion matches the intended emotion, and a non-matching subset, in which the group perceived emotion differs from the intended emotion.</p><p>The resulting three subsets are matching, nonmatching and ambiguous. The matching subset corresponds to group recognition and is typically treated as the gold-standard, however, there are a number of uses for clips where the intended emotion does not match the majority rating. We include all three subsets in the final release of the dataset so that researchers can utilize data that best fits their research objectives in terms of ambiguity and prototypical expressions.</p><p>Binomial majority is used to define majority recognition. Unlike traditional majority, which is defined as more than 50% of raters having selected the specific emotion, binomial majority is achieved when a binomial test would reject at the 95% confidence level the null hypothesis that the most commonly chosen label is selected randomly from the six possible labels.</p><p>Table <ref type="table" target="#tab_6">6</ref> shows the number of clips that have between 4 and 12 ratings for the cleaned data. More than 99.9% of the clips have more than 6 rateings. More than 95% of the clips have more than 8 ratings. These values are shaded in the table. We also list the minimum number of votes necessary to select an emotional label using the binomial majority. Compared with the strict majority, we need fewer votes with the binomial majority for the clips with 8 ratings and above. In such case we will put more clips in one of the unambiguous categories. Table <ref type="table">7</ref> gives the proportion of three subsets of matching, non-matching, and ambiguous, for each modality. The large number of On the other hand, we will also specify a subset of clips (MAIN set) with 10 or more rating. This MAIN set is expected can be reliably used in future perception studies from a statistical/psychological point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE VARIABILITY AND AMBIGUITY OF DATASET</head><p>Based on the perceptual surveys, we first explore the variability and ambiguity of the CREMA-D database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>Perception and Modality First of all, our proposed CREMA-D dataset shows its variability in terms of large collection of recording from various sources and separate human ratings on each corresponding source channels.</p><p>The multi-modal recordings in terms of audio-only, video-only and audio-visual emotion expression allows for future studies of emotion communications either focusing on any of the single channels or on the more complex problem of multi-modal expression. Unlike many other multi-modal datasets of emotion expression that contain only one overall label reflecting the impression created by the multi-modal stimulus, our collection provides separate human ratings in each modality. The availability of ratings in all three modalities allows us to further investigate the relationship and difference among single-and multi-modal perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Emotion expression with varying intensity</head><p>The stimuli in the CREMA-D dataset also demonstrate variability in terms of wide range of emotion expression intensity in all three modalities. Table <ref type="table" target="#tab_8">8</ref> summarizes the distribution of different intensity levels for each modality with corresponding recognition rates. As shown in the table, intensities were into three levels, where mild expressions correspond to the lower quartile of intensity ratings, extreme fall in the upper quartile, and medium is everything in between.</p><p>In order to better understand the relationship between expression intensity and corresponding recognition rate, we list the corresponding group perception rate in the same table. The group perception rate is the percentage of clips that the majority of the group labels with the same emotion that the actor expressed.</p><p>A large proportion of emotion expressions in our datasets are of moderate intensity, in all three modalities. This finding attests to the fact that in general the actors, guided by the directors, in many instances expressed subtle emotional states rather than extreme exaggerated expressions. The visual and audio-visual expressions had the largest proportion of portrayals with extreme intensity, these account for only about a third of all stimuli in a given modality. In contrast, for audio a large fraction of the acquired expressions are in fact perceived as mild, with less than 15% of the stimuli rated to be of extreme intensity.</p><p>Most importantly, each level of intensity for each modality contains at least 1,000 clips. Thus, if researchers want to use only a portion of the dataset, for example only the mild, more ambiguous expressions, or only the prototypical, easily recognizable expressions, the subset will still be larger than many of the currently existing corpora of emotion expression. Models built on the former subset may be more appropriate for application on natural conversational data, while those on the latter may be more useful for emotion generation applications.</p><p>The intensity of emotion expression is strongly correlated with human recognition rate. The recognition rates increase as intensity increases, in each modality. Moreover, simialr as Busso et. al <ref type="bibr" target="#b53">[52]</ref> found, the recognition rate at each intensity level is worst for audioonly and best for audio-visual. All of these observations illustrate that human can perceive emotion more correctly with more explicit information, e.g., higher intensity level and more complete channels of expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mixture of emotion expression</head><p>Natural emotion expression is often mixed or ambiguous. Accordingly, human raters are expected to have high agreement for clear and prototypical expressions but for ambiguous expressions, different raters may disagree when asked to pick a single emotion. We tabulate the distribution of raters' responses for each clip as an emotion profile. The emotion profile of a clip shows the mixture of emotions perceived by the raters. Fig. <ref type="figure">3</ref> depicts the average emotion profiles of stimuli in our dataset, showing the distributions of responses when anger, disgust, fear, happy, neutral, and sad are the primary perceived emotion in each modality.</p><p>The clarity of emotion expressions varies in terms of distinct expression channels on different emotions. In general, emotion expression is most ambiguous in terms of vocal expression and most clear based on multimodal audio-visual expressions. In terms of emotions, anger is the clearest emotion. Disgust is its clear secondary perceived emotion, in all three modalities. On the other hand, sad is the most ambiguous emotion with the lowest rate on the primary component and it does not show any clear preference for secondary emotion. We also notice that various modalities show advantages in representing different emotions. For example, facial expression conveys happiness quite unambiguously. About 90% of the raters agree with each other in perception of happy in terms of facial or audio-visual expressions.</p><p>Our inclusion of clips with ambiguous emotional profiles is motivated by applications where the emotions expressed may not be as clear. For example, sometimes ambiguous expressions may be more close to natural emotion expression in real life. Different from many emotional datasets which only include recording with clear and prototypical emotion expressions, the partitioning of three subgroups of matching, nonmatching, and ambiguous in our dataset can support a wider range of applications in future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HUMAN PERCEPTION OF EMOTION EXPRESSION</head><p>Our analysis so far has demonstrated the wide variety of expressions in our datasets. Here we are particularly interested in the difference and correlation among various modalities in emotion perception. We first discuss how people perceive emotion differently across the three modalities of audio-only, video-only, and audio-visual in section 5.1. Then in section 5.2 we further discuss the interaction of modalities in perception of different emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Emotion perception in various modalities</head><p>In order to better understand how people perceive emotion expressions in different modalities, here we examine the response time of perception, recognition rate and intensity, rater consistency, and individual rater difference, across different modalities. Fig. <ref type="figure">4</ref> compares three histograms, one for each modality, with the distribution of response time for the task of selecting an emotion category for each clip. The difference in the histogram shapes shows that people have different speed of emotion perception across modalities. People need longer time to perceive emotion via vocal expression in general, while they have similar faster response speed for facial and audio-visual expressions on average. There are more samples with very quick response of less than 1 second on audio-visual multimodal compared with the video-only modality. This suggests that involving of vocal expression may help to increase the speed of perception in some cases.</p><p>Next, we analyze how individual human raters differ across modalities in terms of 1) the accuracy with which the rater was able to recognize the intended emotions (i.e. the percentage of stimuli correctly recognized as the intended emotion), and 2) levels of intensity or confidence once an emotion is selected by the rater.</p><p>We find that there is a significant difference between recognition rates in each of the modalities. On average, audio-only, visual-only, and audio-visual ratings from individual raters matched the intended emotion at 40.9%, 58.2% and 63.6% respectively, with p &lt; 0.001 for ANOVA test for difference between the groups. This suggests that more emotion-related information is portrayed in the visual-only than in the audio-only expressions, and than the combined audio-visual presentation further improves  the recognition rate of individual raters. In addition, there is a significant difference between each emotion. The overall recognition rates, across modalities, from highest to lowest by emotion are neutral, happy, anger, disgust, fear, and sad. Fig. <ref type="figure">5</ref> shows the comparison of individual recognition rates between modalities and emotions.</p><p>It is evident that different modalities are better suited for expressing particular emotions. Video-only and audio-visual modalities show similar trend but they are markedly different from the audio-only modality. For example, facial expression conveys happy the clearest, while vocal expression conveys anger much better than other emotions.</p><p>We also test the effect of modality on mean emotional intensity and show the results in Fig. <ref type="figure">6</ref>. There is a significant difference between modalities for both emotional intensity and neutral confidence. The mean intensities per modality ranked from highest to lowest are visual-only, audio-visual, and audio-only. In contrast the highest confidence in assigning a neutral label was for audio-visual, then for visual-only. Confidence was lowest for audio-only stimuli.</p><p>We also examine the consistency of rater responses for testing clips when a stimulus is repeated. In our database, three stimuli were repeated per modality for each rater. Table <ref type="table" target="#tab_11">9</ref> shows the consistency of individual raters on three modalities. The overall consistency is quite high, nearly 70%, indicating that the same expressions trigger the same interpretation of emotional content. Moreover, the consistency is highest for audio-visual stimuli, 76%, and lowest for audio-only.</p><p>In addition to the analysis of general trends based on average recognition and intensity from individual raters, we were also interested in the variations across individual raters. Fig. <ref type="figure">8</ref> shows the boxplot of recognition rates across all human raters, in each modality. The central mark on the boxplot is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the most extreme data points not considered outliers, and outliers are plotted individually.</p><p>Overall, considerable variations are observed across different raters, in all three modalities. For example, in terms of vocal expression, the best recognition rate is higher than 80% for one rater, while the lowest performance is worse than 10% for another rater. Videoonly modality shows the smallest difference across all raters, followed by audio-visual one, while audio-only exhibit the largest variations.</p><p>The boxplot analysis shows that audio-visual expression more clearly conveys emotional states than facial and vocal expressions across all human raters. However this general trend does not always hold true for individual raters. In order to discount the impression of a potential learning effect by having a fixed order of presentation of audio, then visual, then audio-visual, two tests were run. The first test is to group the individual ratings by the modality of presentation, and check the ordering of recognition. Since there are three modalities, Fig. <ref type="figure">5</ref>. There are significant differences in recognition rates between each emotion, and between each modality. When looking at a particular emotion, all differences between modality are significant (p &lt; 0.05) except for Face vs. Audio-visual when either happy or sad is the intended emotion, and Voice vs. Face when either anger or neutral is the intended emotion.  there are three ordinals, best, middle, worst. These individual recognition rate orders are tabulated in Table <ref type="table" target="#tab_12">10</ref>, which summarizes the distribution individual rater recognition order by modality. (e.g. An individual whose recognition order from best to worst was Visual, Audio, Audio-visual would increase the count of Best Visual, Middle Audio, Worst Audio-visual.) The audio-visual multimodal expressions were ordered as Best at the highest rate, 58.7%. However, in order to state that there is a learning effect, audio-visual should be best, visual should be middle, and audio should be worst for almost all of the time. Instead, each bin has at leat 5% of the raters. In addition, audio is not always the modality with worst recognition when individual raters are considered: almost 20% of the raters show worse recognition rate on video or multimodal expressions. More evidence against the presence of a learning effect is that the recognition of the audio stimuli was in fact the highest for 5% of the raters. Similarly, 36% of the individual raters recognized the visual-only stimuli with the highest accuracy. These findings suggest that the overall trend that audio-visual stimuli are most accurately recognized while audio-only stimuli are least accurately recognized is not due to learning effects; individual raters do not follow the same trend in their individual performance. Some individuals are best at the clips that come first or second, and some are worst at the clips that come second or last.</p><p>To complete our analysis of individual rater recognition rates and possible learning effects due to presentation order, we examine the average recognition rate of all raters per stimulus number (stimuli number corresponds to the order in which raters saw them) and plot the results in Fig. <ref type="figure">9</ref>. There does not appear to be a direct relationship between recognition rates and presentation order of the stimuli. The recognition rates are comparable for different stimuli for the video-only modality, which was always presented second and audiovisual modality, which was always presented last. Audio stimuli were always presented first, and there we do observe steady improvement on the first 5 responses, however the performance tends to become stable for the remaining 30 responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interaction of various modalities</head><p>Now we turn to investigate the interaction among modalities in the perception of different emotions. We perform the analysis in terms of group perceived ratings.</p><p>First, we examine the confusion between intended emotion and the group perceived emotion that has binomial majority across all modalities together. Fig. <ref type="figure" target="#fig_3">10</ref> shows the confusion matrix between intended and group perceived emotion for all unambiguous clips. There is a clear bias of the raters to select neutral. Apart from the intended emotions in the diagonal, neutral was the most often perceived emotion, in all of the five emotion classes. Also more than 95% neutral utterances are correctly recognized as intended with only a handful of exceptions. The most common confusion between actual emotion classes were intended anger perceived as disgust (11%); intended disgust perceived respectively as sad (9%), fear (6%), and anger (5%) with decreasing rate. Intended fear is perceived as sad (9%) or vice versa (8%). Happy is mostly recognized correctly.</p><p>Next, in order to tease apart the contribution of audio and video modalities to the perception of intended emotion, we examine the agreement in perception based on individual modality (audio or visual) compared to the audio-visual ratings. In order to do this we count the number of clips that have the same group perceived label in each combination of modalities. We show these counts Fig. <ref type="figure">9</ref> The recognition rate over time for audio-only, videoonly, and audio-visual respectively. The first five clips of audio appear to have a slight learning effect, but after that no effect is present, and this initial effect does not explain the large jump in recognition rates between modalities. For each emotion there is a Venn diagram in Fig. <ref type="figure" target="#fig_4">11</ref>. The sum of the numbers inside the each circle is the total count of clips perceived as the emotion of the diagram's title for the specified mode. For example, happy is perceived as the expressed emotion by the audio, visual, and audio-visual modalities in 318, 952, and 1206 clips, respectively. In addition, of the 1235 perceived happy clips, merely 45 were identified in only one modality.</p><p>Based on the diagrams of Fig. <ref type="figure" target="#fig_4">11</ref>, we first notice that a visual signal is crucial for the accurate perception of a happy expression. Next, neutral and anger both have the majority of clips detected in all modalities (the center number of each diagram), suggesting that much of the information for perception of neutral and anger is redundant across modalities. On the other hand, disgust and fear benefit from the multimodal perception the most, since they have a much smaller proportion of clips detected in all modalities and relatively larger portion of clips detected via only audio-visual modality. Finally, each single modality is important for sad because sad has almost the same number of clips detected by more than one modality as are detected by a single mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION AND CONCLUSIONS</head><p>CREMA-D is a new data set for multimodal research on emotion. We have described the creation of this data set containing ratings of intended and perceived emotional expression. We used crowd sourcing as a way to collect the perceptual ratings. CREMA-D has over 7 ratings in each modality (audio, visual, and audio-visual), for more than 95% of 7,442 clips that cover 12 sentences spoken with 6 different emotion categories (happy, sad, anger, fear, disgust, and neutral). This makes CREMA-D an excellent resource when considering questions of audio-visual perception of emotions. Group perceived emotion is provided on each clip and we further created three subgroups (matching, non-matching, and ambiguous) by applying the binomial majority test and comparing the intended emotion to the group perceived emotion.</p><p>The dataset contains a wide range of emotion expression from ambiguous to prototypical emotion, subtle to extreme expression, in all three. We also provided a detailed perception analysis in terms of the individual ratings, the different modalities, and the group ratings.</p><p>The recognition of emotion in each modality was tested in multiple ways. Audio is the most difficult to recognize, visual is in the middle, and audio-visual is the easiest to recognize. This holds both when we considered the overall performance of all individual raters and group perceived emotion recognition. However, there are also some exceptions when we consider specific emotions or raters. For example, audio and visual show similar performance on anger, while visual and audio-visual are not significantly different happy and sad. On the other hand, in terms of emotion recognition performance on different raters, we also notice that about 20% of the raters had their best recognition rate for audio stimuli.</p><p>When looking at consistency of individual emotion perception, the same order of audio, visual, and audiovisual is seen with regards to the proportion of consistent responses. There also appears to be a relationship between recognition performance and perceived intensity of expression. Clips rated as expressing emotions of extreme intensity were recognized over 25% more accurately than those perceived to be of mild intensity in each modality. Similarly, the order of recognition (audio, visual, and audio-visual) holds in each intensity bin.</p><p>In terms of response time, we observed significantly faster response on audio-visual and video modalities, than on audio. Although video and audio-visual show comparable response speed on average, we notice a larger portion of very quick response (&lt;1s) on audio-visual perception. Finally, for the perception within each   modality, we do not observe significant difference depending on the order of presentation of the stimuli. We further investigate the interaction of various modalities. We observe that certain modalities best convey particular emotions. For happy, the group perception of emotion is dominated by the visual channel for over 70% clips, and the rest of the clips are primarily having redundant audio and visual information. For neutral, most of the clips have redundant information (the center number of the neutral diagram in Fig. <ref type="figure" target="#fig_4">11</ref>), and there are a small number of clips where either the visual modality dominates (the visual and audio-visual overlap) or the audio modality dominates (the audio and audiovisual overlap). For anger, disgust, fear, and sad, all modalities make some contributions and we do not observe obvious preferred channel of expression. There are cases of redundancy, visual dominance, audio dominance and complementarity shown to different degrees. A deeper examination of the data set, where individual clips are examined across modalities, could yield further insight into these questions. This is reserved for the future, when the analysis can be carried out as part of more emotion specific hypothesis driven studies.</p><p>Other highly relevant potential studies that can be carried out on the newly developed dataset can quantify the effects of gender, age, race, etc. for the production and perception of emotion expression. We performed some pilot studies and observed that women are better than man both in clearly expressing emotions and in accurately recognizing the intended emotion. This agrees with an earlier study examining gender differences in emotion detection using the voice channel <ref type="bibr" target="#b4">[4]</ref>. More detailed studies can be performed in the future.</p><p>In conclusion, we presented a sampling of the analysis that can be performed on CREMA-D, and found differences related to emotional expression in each modality. These differences allow a user of the dataset to separate the data as appropriate. Labels include both intended and group perceived emotions as well as group perceived emotional intensities. The variety of labels and modalities is intended to make this dataset accessible and useful for a variety of purposes. This analysis is by no means comprehensive and has been carried out with the aim of demonstrating the scope of the dataset and the plethora of analyses that can be based on it. We plan to release the database soon to the academic community for research purpose. The researchers are free to apply the best methods of their choice to the dataset based on the needs of the study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Left: Sound Calibration, Soft, Med, and Loud light up when the corresponding sound is played. Right: Sound Check, the 3 target words have been selected and are shown in green.</figDesc><graphic url="image-4.png" coords="5,53.25,62.25,446.00,135.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig.3. The distributions of response when anger, disgust, fear, happy, neutral, and sad are the primary perceived emotion in three modalities of audio-only, visual-only, and the audio-visual multimodal The percentage of emotion is shown per emotion in order of ranking, from most number of clips to least number of clips. A-anger, D-disgust, F-fear, H-happy, N-neutral, S-sad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .Fig. 8 .</head><label>68</label><figDesc>Fig. 6. Mean emotion intensity (left) and mean neutral confidence (right) are significantly different per modality (p &lt; 0.05).</figDesc><graphic url="image-6.png" coords="10,39.23,571.00,220.23,153.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The confusion matrix between intended emotion and group perceived emotion indicates that the group perception matches the intended emotion more than other emotions except for sad, which tends to be recognized as neutral.</figDesc><graphic url="image-9.png" coords="11,341.94,541.70,159.37,140.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Examination of overlapping group perceived emotional responses: The center number of each diagram is the number of clips that are recognized in all conditions. The outer numbers indicate the number of clips that are only recognized by one modality. If a diagram has a high outer top number, with lower combined outer bottom numbers (as seen in the fear Venn diagram with 173 vs. 18 + 73), this suggests that the bimodal perception is advantagious over single mode perception.</figDesc><graphic url="image-19.png" coords="12,139.08,201.61,65.82,57.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>17</head><label>17</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,49.50,333.15,463.60,386.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 COMPARISON</head><label>1</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing tween audio and visual perception with 6 categories, 18 raters per clip, and used all three modalities, however there were only 2 actors and 72 clips rated. Mower Provost et al. [15] discussed an audio-visual dataset created using the McGurk effect paradigm with matching and mismatching vocal and facial information. The dataset was collected by crowd sourcing 72 original and 216 resynthesized clips from only 1 actress. Collignon et al.'s</figDesc><table /><note><p>TO PRIOR WORK USING DATA SET CRITERIA -indicates the criterion is met. -indicates criterion is not met. Each highlighted cell indicates that the criterion for the column was met by the data set. 1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Race/Ethnicity Distribution</cell><cell></cell></row><row><cell>Race/Ethnicity</cell><cell>Raters</cell><cell>Actors</cell></row><row><cell>Caucasian</cell><cell>73.60%</cell><cell>58.24%</cell></row><row><cell>Hispanic</cell><cell>10.80%</cell><cell>10.99%</cell></row><row><cell>African American</cell><cell>8.10%</cell><cell>23.08%</cell></row><row><cell>Asian</cell><cell>4.50%</cell><cell>7.69%</cell></row><row><cell>Other/No Answer</cell><cell>3.00%</cell><cell>0.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Actors' Age Distribution</cell></row><row><cell>Age</cell><cell># actors</cell></row><row><cell>20-29 YRS</cell><cell>34</cell></row><row><cell>30-39 YRS</cell><cell>23</cell></row><row><cell>40-49 YRS</cell><cell>16</cell></row><row><cell>50-59 YRS</cell><cell>12</cell></row><row><cell>60-69 YRS</cell><cell>5</cell></row><row><cell>OVER 70 YRS</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">ACTORS' RACE/ETHNICITY BREAKDOWN</cell></row><row><cell cols="2">Ethnicity</cell><cell>Not Hispanic</cell><cell>Hispanic</cell><cell>Total</cell></row><row><cell>Race</cell><cell>.</cell><cell></cell><cell></cell></row><row><cell>Caucasian</cell><cell></cell><cell>53</cell><cell>8</cell><cell>61</cell></row><row><cell>African American</cell><cell></cell><cell>21</cell><cell>1</cell><cell>22</cell></row><row><cell>Asian</cell><cell></cell><cell>7</cell><cell>0</cell><cell>7</cell></row><row><cell>Unspecified</cell><cell></cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>Total</cell><cell></cell><cell>81</cell><cell>10</cell><cell>91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Krippendorff's alpha statistics on different groups</cell></row><row><cell></cell><cell>Average</cell><cell>Average</cell><cell>Average</cell></row><row><cell>Rating</cell><cell>Alpha</cell><cell>Num Clips</cell><cell>Num Raters</cell></row><row><cell></cell><cell>(min.,max.)</cell><cell>(min., max.)</cell><cell>(min., max.)</cell></row><row><cell>Emotion label</cell><cell>0.42 [0.25, 0.57]</cell><cell>134.3 [105,177]</cell><cell>9.5 [4, 12]</cell></row><row><cell>Emotion intensity</cell><cell>0.47 [0.18, 0.71]</cell><cell>134.3 [105,177]</cell><cell>9.5 [4, 12]</cell></row><row><cell>Emotion label</cell><cell>0.79</cell><cell>42.9</cell><cell>9.6</cell></row><row><cell>(agreement &gt; 0.8)</cell><cell>[0.58, 0.96]</cell><cell>[16, 78]</cell><cell>[5, 12]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">RATINGS DISTRIBUTION</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># Ratings 4 5 6 7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell cols="2">11 12</cell></row><row><cell># Clips</cell><cell cols="6">2 10 71 460 2,046 6,951 11,296 1,471 19</cell></row><row><cell>Min. Major-</cell><cell>3 3 4 4</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>ity Votes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>The highlighted area shows over 95% of the clips with 8 or more ratings. The number of votes needed is fewer for binomial majority than strict majority when there are</p>8  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>or more ratings. Average, min and max reliability score for emotion class and intensity for the entire dataset, as well as for emotion class on clips unambiguous clips</head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing</figDesc><table><row><cell>8</cell><cell>IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID</cell></row><row><cell>annotated clips in CREMA-D ensures that each of the 9</cell><cell></cell></row><row><cell>subsets has a sufficient number of clips to be useful. The</cell><cell></cell></row><row><cell>smallest group, ambiguous audio-visual, has over 590</cell><cell></cell></row><row><cell>rated clips.</cell><cell></cell></row></table><note><p>. 1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="5">EMOTION EXPRESSION AT DIFFERENT INTENSITIES</cell></row><row><cell></cell><cell></cell><cell>Mild (&lt;=48)</cell><cell>Medium</cell><cell>Extreme (&gt;65)</cell></row><row><cell>Audio</cell><cell>Rate Count</cell><cell>34.82% 2,685</cell><cell>44.35% 3,711</cell><cell>68.01% 1,016</cell></row><row><cell>Visual</cell><cell>Rate Count</cell><cell>55.31% 1,159</cell><cell>64% 3,653</cell><cell>80.38% 2,600</cell></row><row><cell>AV</cell><cell>Rate Count</cell><cell>61.04% 1,422</cell><cell>69.96% 3,545</cell><cell>87.61% 2,445</cell></row><row><cell>Recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>rate (the % of clips labeled as the intended emotion) in- creases with intensity for all modalities, and with modality for all in- tensities.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>TABLE 7</cell><cell></cell><cell></cell></row><row><cell cols="3">PROPORTION OF THE THREE SUBSETS</cell><cell></cell></row><row><cell></cell><cell cols="2">Three subsets</cell><cell></cell></row><row><cell></cell><cell>Matching</cell><cell>Non-</cell><cell>Ambiguous</cell></row><row><cell></cell><cell cols="2">matching</cell><cell></cell></row><row><cell>Audio-only</cell><cell>41%</cell><cell>46%</cell><cell>13%</cell></row><row><cell>Video-only</cell><cell>64%</cell><cell>25%</cell><cell>11%</cell></row><row><cell>Audio-visual</cell><cell>72%</cell><cell>21%</cell><cell>8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">MEAN PERCENT CONSISTENCY VS. MODALITY</cell><cell></cell></row><row><cell></cell><cell>Audio-only</cell><cell>Video-only</cell><cell>Audio-visual</cell><cell>Overall</cell></row><row><cell>Individual rater consistency</cell><cell>62.6%</cell><cell>69.1%</cell><cell>76.4%</cell><cell>69.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">RECOGNITION RATE ORDERING BY MODALITY</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Best</cell><cell>Middle</cell><cell>Worst</cell></row><row><cell>Audio</cell><cell>Rate Count</cell><cell>5.2% 126</cell><cell>13.7% 335</cell><cell>81.1% 1,981</cell></row><row><cell>Visual</cell><cell>Rate Count</cell><cell>36.1% 882</cell><cell>53.4% 1304</cell><cell>10.5% 257</cell></row><row><cell>Audio-</cell><cell>Rate</cell><cell>58.7%</cell><cell>32.9%</cell><cell>8.4%</cell></row><row><cell>visual</cell><cell>Count</cell><cell>1,435</cell><cell>804</cell><cell>205</cell></row><row><cell>Each</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>count bin represents the number of raters whose recogni- tion rate order was best, middle, or worst for the modalities of audio, visual and audio-visual. The rate is the percentage of raters for in the column or row.</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2014.2336244, IEEE Transactions on Affective Computing 10 IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENT</head><p>The authors wish to thank the directors, <rs type="person">Amy Dugas Brown</rs> and <rs type="person">David M. O'Connor</rs>, as well as <rs type="person">Brian Chaffinch</rs>, who assisted with the recording sessions and video editing. This work was supported in part by the following grants: <rs type="funder">NIH</rs> <rs type="grantNumber">R01-MH060722</rs> and <rs type="funder">NIH</rs> <rs type="grantNumber">R01 MH084856</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Wb38Zvq">
					<idno type="grant-number">R01-MH060722</idno>
				</org>
				<org type="funding" xml:id="_nXeJWAM">
					<idno type="grant-number">R01 MH084856</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Psychology in Psychiatry and the Director of the Brain Behavior Laboratory at the University of Pennsylvania. His research has been in the study of brain and behavior in healthy people and patients with brain disorders, with a special emphasis on exploiting neuroimaging as experimental probes. His work has documented sex differences, aging effects, and abnormalities in regional brain function associated with schizophrenia, affective disorders, stroke, epilepsy, movement disorders and dementia. His work has been supported by grants from the NSF, NIH, NIMH, NIA, NINDS, NSBRI, private foundations (Spencer, MacArthur, EJLB) and industry (Pfizer, AstraZeneca).</p><p>Ani Nenkova obtained her BS degree in computer science at Sofia University in 2000, and MS and PhD degrees from Columbia University in 2004 and 2006 respectively. Prior to joining Penn she was a postdoctoral fellow at Stanford University. Her main areas of research are summarisation, text quality and affffect recognition. Ani and her collaborators were recipients of the best student paper award at SIGDial in 2010 and best paper award at EMNLP in 2012. Ani was a member of the editorial board of Computational Linguistics (2009--2011) and has served as an area chair/senior program committee member for ACL, NAACL, AAAI and IJCAI.</p><p>Ragini Verma earned her M.S. in Mathematics and Computer Applications followed by a Ph.D. in computer vision and mathematics, from IIT Delhi (India). She did two years of postdoc at INRIA, Rhone-Alpes, with the MOVI project (currently LEARS and PERCEPTION). She then did two years of post doc in medical imaging at SBIA, prior to taking up her current position. She is currently an Associate Professor of Radiology in Section of Biomedical Image Analysis, Department of Radiology at the University of Pennsylvania. Ragini's research interests span the area of diffusion tensor imaging, multimodality statistics and facial expression analysis. She is actively involved in several clinical studies in schizophrenia, aging, tumors and multiple sclerosis as well as projects in animal imaging. Ragini works in the broad area of multi-parametric image analysis which aims at integrating several channels of information (MRI, genetic and clinical scores) to solve a clinical or biological problems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facial expressions of emotion: New findings, new questions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Psychological Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="38" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">An argument for basic emotions</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="169" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-cultural emotion recognition among canadian ethnic groups</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Beaupre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Journal of Cross-Cultural Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="370" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion inferences from vocal expression correlate across languages and cultures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Journal of Cross-cultural psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="92" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2408" to="2412" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of the glottal excitation of emotionally styled and stressed speech</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arnott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fundamental frequency of phonation and perceived emotional stress</title>
		<author>
			<persName><forename type="first">A</forename><surname>Protopapas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gmm supervector based svm with spectral features for speech emotion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? in Proc. ICASSP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Acoustic feature selection for automatic emotion recognition from speech,? Information processing &amp; management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="315" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotion recognition in spontaneous speech using gmms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Neiberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laskowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vocal expressions of emotion,? Handbook of emotions</title>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Bachorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Owren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="196" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Facial expressions of emotion,? Handbook of emotions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Shiota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perception of emotions by ear and by eye</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vroomen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using emotional noise to uncloud audio-visual emotion perceptual evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG 2000</title>
		<meeting>FG 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<title level="m">A video database of moving faces and people,? IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="812" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The belfast induced natural emotion database</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sneddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcrorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanratty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acoustic profiles in vocal emotion expression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Private emotions versus social interaction: a data-driven approach towards analysing emotion in speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Oth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="175" to="206" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Maclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">The ISL meeting corpus: The impact of meeting type on speech style,? in Proc. ICSLP 2002</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sendlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<title level="m">A database of German emotional speech,? in Proc. Interspeech</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic Classification of Emotion Related User States in Spontaneous Children&apos;s Speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Logos-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotional speech: Towards a new generation of databases</title>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Speech communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="60" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Klasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Kenworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Mathiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kircher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supramodal representation of emotions</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="13635" to="13643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multisensory emotions: perception, combination and underlying neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Reviews in the neurosciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="381" to="392" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Habel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Derntl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Turetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Eickhoff</surname></persName>
		</author>
		<title level="m">Incongruence effects in crossmodal emotional integration,? Neu-roImage</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="2257" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personal use is permitted, but republication/redistribution requires IEEE permission</title>
		<ptr target="http://www.ieee.org/publications_standards/publications/rights/index.html" />
	</analytic>
	<monogr>
		<title level="m">for more information</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1949" to="3045" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cieslik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Turetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Crossmodal interactions in audiovisual emotion processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The argument and evidence about universals in facial expressions of emotion,? Handbook of social psychophysiology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="342" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Introducing the geneva multimodal emotion portrayal (GEMEP) corpus,? Blueprint for affective computing: A sourcebook</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="271" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using multi-modal information</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICICS 1997</title>
		<meeting>ICICS 1997</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Collignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saint-Amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lassonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lepore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research</title>
		<imprint>
			<biblScope unit="volume">1242</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human perception of audiovisual synthetic character emotion expression in the presence of ambiguous and conflicting information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Trans. Multi</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="843" to="855" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cearreta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garay</surname></persName>
		</author>
		<title level="m">Validating a multilingual and multimodal affective database,? in Usability and Internationalization</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4560</biblScope>
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The vera am mittag german audiovisual emotional speech database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<title level="m">Iemocap: Interactive emotional dyadic motion capture database,? Language resources and evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sneddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcrorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abrilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<title level="m">The humaine database: Addressing the collection and annotation of naturalistic and induced emotional data,? in Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4738</biblScope>
			<biblScope unit="page" from="488" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG 2013</title>
		<meeting>FG 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A chinese affective database,? in Affective Computing and Intelligent Interaction, ser. Lecture Notes in Computer</title>
		<author>
			<persName><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename></persName>
		</author>
		<editor>Science, J. Tao, T. Tan, and R. W. Picard</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3784</biblScope>
			<biblScope unit="page" from="542" to="549" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond emotion archetypes: Databases for emotion modelling using neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="371" to="388" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Validation of affective and neutral sentence content for prosodic testing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Bilker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? Behav Res Methods</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="935" to="939" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wide range achievement test</title>
		<meeting><address><addrLine>Lutz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>wrat4</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Crowdsourcing preference tests, and how to detect cheating</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Latorre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH 2011</title>
		<meeting>INTERSPEECH 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Difallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demartini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cudre-Mauroux</surname></persName>
		</author>
		<title level="m">Mechanical cheat: Spamming schemes and adversarial techniques on crowdsourcing platforms,? in Proc. of the First International Workshop on Crowdsourcing Web Search</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Increasing cheat robustness of crowdsourcing tasks,? Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cheat-detection mechanisms for crowdsourcing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ho?feld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tran-Gia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">474</biblScope>
		</imprint>
		<respStmt>
			<orgName>? University of Wurzburg, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cost-optimal validation mechanisms and cheat-detection for crowdsourcing platforms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hossfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tran-Gia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IMIS</title>
		<meeting>IMIS</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An analysis of assessor behavior in crowdsourced preference judgments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serdar</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungbok</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Estimating the Reliability, Systematic Error and Random Error of Interval Data?</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">She is currently a Postdoctoral Fellow at the University of Pennsylvania. Her research interests are in speech and language processing, include multi-lingual and cross-lingual speech recognition, emotion and affect analysis and recognition</title>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">;</forename><surname>Krippendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004, the M.S. degree from University of Surrey, U.K., in 2005, and the Ph.D. degree in 2011 from The Chinese University of Hong</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="411" to="433" />
		</imprint>
	</monogr>
	<note>Howei Cao earned her B.E. degree from Shenzhen University, China</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">He held a position as postdoctoral researcher in the Section of Biomedical Image Analysis in the Radiology department of the University of Pennsylvania from 2011 to 2013. He is currently a lecturer at Ursinus College. His research interests include emotional and cognitive models of human interaction, visual and auditory emotion recognition</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Science from Carnegie Mellon in 2000, and M.S. and Ph.D. in Computer Science from the University of Massachusetts Amherst in 2009 and 2011 respectively</title>
		<imprint/>
	</monogr>
	<note>He worked at Lockheed Martin Advanced Technology Labs from 2000-2006. sensor integration for computer awareness, and biologically inspired computation</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">He is a former Research Coordinator in the Brain Behavior Laboratory at the University of Pennsylvania and is currently a graduate student</title>
		<author>
			<persName><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Counseling and Psychological Services</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</editor>
		<imprint>
			<publisher>Keutmann earned a B.A. in Biology</publisher>
			<date type="published" when="2005">2005. 2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>in Clinical Psychology at the University of Illinois at Chicago</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gur received his B.A</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology and Philosophy from the Hebrew University of Jerusalem, Israel, in 1970 and his M.A. and Ph.D. in Psychology (Clinical) from Michigan State University in 1971 and 1973, respectively. He did Postdoctoral training with E.R. Hilgard at Stanford University and began at the University of Pennsylvania as Assistant</title>
		<imprint/>
	</monogr>
	<note>Professor in 1974. He is currently a Professor of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
