<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLINICALGPT: LARGE LANGUAGE MODELS FINETUNED WITH DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-16">16 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Guangyu</forename><surname>Wang</surname></persName>
							<email>guangyu.wang24@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongxin</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longjun</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLINICALGPT: LARGE LANGUAGE MODELS FINETUNED WITH DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-16">16 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.09968v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>large language model</term>
					<term>medical knowledge</term>
					<term>electronic medical record</term>
					<term>text generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domainspecific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the paradigm of pre-training and fine-tuning large language models has brought about significant advancements in Natural Language Processing (NLP) domain. The earliest approaches like BERT <ref type="bibr" target="#b0">[1]</ref>, utilized optimized objectives like Masked Language Model (MLM) to pre-train on large text corpora such as BookCorpus <ref type="bibr" target="#b1">[2]</ref>, in an unsupervised manner to learn good representations. These representations can be fine-tuned and adapted to one or more specific downstream tasks to improve their performance. Further research aims to develop competent generalists, i.e. generalized systems that can perform multiple NLP tasks without the need for a manually labeled training dataset for each task. For instance, T5 <ref type="bibr" target="#b2">[3]</ref> treats multiple NLP tasks as text-to-text transformation tasks and leverages an encoderdecoder architecture, achieving promising results such as text classification, question answering, and summarization, though with a larger number of parameters. In contrast, GPT-3 <ref type="bibr" target="#b3">[4]</ref> uses large auto-regressive model for few-shot predictions, improving performance without parameter fine-tuning by incorporating few-shot demonstrations through text interaction with the model. PALM <ref type="bibr" target="#b4">[5]</ref> is Transformers-based and Pathways-enabled large-scale language model. Compared to other models, PALM is more resource-efficient in terms of computation and achieves state-of-the-art few-shot results across hundreds of natural language, code, and mathematical reasoning tasks.</p><p>With their substantial generalization capabilities in NLP tasks, large pre-trained models are increasingly utilized for various tasks and facilitating human interaction through dialogue models. LaMDA <ref type="bibr" target="#b5">[6]</ref>, a transformer-based model designed for dialogues, leverages annotated data and external knowledge to augment its helpfulness and role consistency. InstructGPT <ref type="bibr" target="#b6">[7]</ref> aligns with user intent across various tasks through fine-tuning and reinforcement learning with human feedback, resulting in improved truthfulness and reduced toxicity in output generation. ChatGPT can simulate human interaction, write abstracts or create movie scripts in response to prompts, driving the AI revolution. Large language models are also effective for writing assistance and generating efficient code for programmers.</p><p>As we know, medicine and health care still face many challenges, including aging population, lack of equitable access, rising costs, doctor and nurse burnout, and global pandemics. Information technology has the potential to transform modern medicine by offering new tools and insights for healthcare, with ChatGPT and GPT-4 promising to revolutionize clinical decision support, clinical trial recruitment, clinical data management, research support, patient education <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Google researchers developed FlanPaLM, an instruction-tuned variant of PaLM, showing improved task performance via natural language instructions. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy in MultiMedQA multiple-choice datasets, but remains outperformed by clinicians. Recent prospective suggests generalist medical AI (GMAI) using foundation models may disrupt task-specific paradigms, enabling versatile applications like interactive note-taking, bedside decision support, and patient chatbots <ref type="bibr" target="#b9">[10]</ref>.However, there are considerable challenges to overcome in applying generative language models to the medical field. The output of generative language models may have factual errors, logic inconsistencies, and problems with coherence, such as citing article references that do not exist <ref type="bibr" target="#b10">[11]</ref>. The models have limited reasoning abilities and lack grounding in real-world experience, leading to general and vague responses. ChatGPT has been found lacking in depth and insight <ref type="bibr" target="#b3">[4]</ref>, likely due to its alignment model used for reward-based training, which produces overly generalized answers that lack medical expertise. This evidence implies that employing these technologies in the medical field brings unique hurdles, such as the necessity for high accuracy, interpretability, and secure handling of sensitive health data.</p><p>In this study, we present ClinicalGPT, a large language model that is specifically designed for tasks across medical applications. To train the model, we leverage extensive and diverse datasets consisting of real-world medical records, allowing us to transform domain-specific knowledge to the model. In addition, we establish a comprehensive evaluation framework that includes medical knowledge question-answering, medical examinations, patient consultations, and medical record analysis. By utilizing parameter-efficient fine-tuning methods, we were able to further improve the performance of ClinicalGPT. The results demonstrate that ClinicalGPT outperform existing models in term of performance, thus confirming the effectiveness of our approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>In this study, we incorporated a large and diverse medical datasets including cMedQA2, cMedQA-KG, MD-EHR, MEDQA-MCMLE, and MedDialog, for the training and evaluation of our model. The cMedQA2 dataset <ref type="bibr" target="#b11">[12]</ref> is a Chinese medical question-and-answer dataset that consists of 120k questions and 226k answers. The data is aggregated from a Chinese medical question-and-answer online forum <ref type="foot" target="#foot_0">1</ref> . For training purposes, we followed the original dataset partition as proposed by the author, and then we randomly selected one answer per question. We annotated 10k questions from the training set for training reward models and used 4k questions from the validation set for reinforcement learning. We sampled questions from the testing set for evaluation.</p><p>The cMedQA-KG is a medical question-answer dataset which are curated based on knowledge graphs. It is established on three knowledge graphs: cMeKG<ref type="foot" target="#foot_1">2</ref> , xywy-KG<ref type="foot" target="#foot_2">3</ref> , and 39Health-KG <ref type="foot" target="#foot_3">4</ref> .These knowledge graphs cover comprehensive medical entities such as disease, medication, and symptom, and their relationships. Detailed descriptions of the knowledge graphs can be found in Appendix A. We have designed templates (see Appendix B) to transform each knowledge triplet into fine-tuning instruction data, i.e text-to-text pair for text generation, yielding 100k question-answer pairs. cMedQA-KG is used exclusively for training purposes.</p><p>The MEDQA-MCMLE dataset is a subset of the original MEDQA dataset <ref type="bibr" target="#b12">[13]</ref>, consisting of Chinese medical examination questions in a multiple-choice format. It includes 34k questions, each offering multiple choices, typically 4 or 5. We have followed the original author's division of the dataset into training, validation, and testing sets. As this dataset is derived from professional medical board examinations, it effectively evaluates applied knowledge, clinical reasoning, and patient-centric skills.</p><p>The MedDialog dataset <ref type="bibr" target="#b13">[14]</ref> is a data collection of multi-turn medical conversations obtained from an online platform <ref type="foot" target="#foot_4">5</ref> . MedDialog comprises 1.1 million dialogues and 4 million utterances. Due to the large volume of data, we have randomly sampled 100k, 1k, and 1k dialogues for the training, validation, and testing sets, respectively. These multi-turn dialogues closely resemble real interactions between doctors and patients, aiding the model in understanding the process of clinical inquiry and decision-making.</p><p>The MD-EHR dataset is comprised of electronic health records from multicenter, large-scale hospitals in China. This dataset contains 100k records covering a range of disease groups, including Respiratory, Digestive, Urinary, Psychiatry, Neurology, Gynecology, and Hematology.</p><p>Each record within the MD-EHR dataset provides a comprehensive overview of the patient's complaints, medical history, findings from physical examinations, ancillary test results, and the final diagnosis. We have divided the dataset into three sets: 2,000 records for the validation set, 2,000 records for the testing set, and the remaining entries for the training set. Following T5 <ref type="bibr" target="#b2">[3]</ref>, we transformed the medical records into a text generation task by concatenating the notes from the records as input and using the diagnosis as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Finetuning</head><p>We adopt the T5 model's <ref type="bibr" target="#b2">[3]</ref> strategy of utilizing text generation grounded in language models to complete all tasks in our study. Language models, pre-trained on extensive corpora, have demonstrated a remarkable ability to understand and generate human-like text <ref type="bibr" target="#b3">[4]</ref>. These models calculate the probability of a sequence of words in a text, T = (w 1 , w 2 , ..., w L ). Specifically, the casual language model calculates the probability of the text T that can be formulated as p(T ) = p(w 1 )p(w 2 |w 1 )...p(w L |w 1 , w 2 , ..., w L-1 ), where L represents the length of the text. Several large language models, such as BLOOM, GLM, and others, are available for public use.</p><p>To enhance the utility of large models for downstream tasks, we apply an instruction-tuning approach with supervised fine tuning (SFT). The language model p ? is trained to generate a response R = v 1:n for a given input prompt I = w 1:m , optimizing the likelihood p ? (R|I) = p ? (v 1:n |w 1:m ), where n and m represent the lengths of the response and input prompt, respectively. Thus, the loss function is 1 n m+n i=m+1 -log p ? (w i |w 1 , ..., w i-1 ). To incorporate domain-specific knowledge into LLMs, we turn to knowledge graphs (KGs) specific to the domain for constructing prompt-response pairs. KGs capture knowledge in the form of structured triples (s, r, o), where s denotes the subject, r the relationship, and o the object. An example of such a triple could be (Cough, SymptomOf, Pneumonia). We leverage a set of manually designed templates to transform these triples into question-answer pairs, rendering them suitable for instruction tuning. The manually designed templates can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reward model</head><p>Existing works have demonstrated that reinforcement learning can incorporate human feedback to enhance large language models. For instance, WebGPT <ref type="bibr" target="#b14">[15]</ref> is a browser-assisted question-answering system that utilizes human feedback for performance improvement. InstructGPT also <ref type="bibr" target="#b6">[7]</ref> to align with human feedback via reinforcement learning for helpful and safe response generation.</p><p>We follow the work of <ref type="bibr" target="#b6">[7]</ref>, constructing a reward model (RM) r ? to furnish the reward signal crucial for the reinforcement learning process. We employ rank-based training for the RM. Human labelers rank responses for a given input prompt I, generating a comparison pair for each prompt. For a comparison pair with a human-preferred response R w and a less preferred response R l , the loss is given by -log(?(r ? (I, R w ) -r ? (I, R l ))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reinforcement learning</head><p>We adopt the method proposed by Stiennon et al. <ref type="bibr" target="#b15">[16]</ref>, leveraging reinforcement learning to enhance the fine-tuned models with the objective of generating high-quality and helpful outputs, as well as improving the generation of medical texts, thereby aiding in the accurate description and treatment of patient conditions.</p><p>We utilize the trained reward model as the reward function. In order to prevent the model from deviating too far from its initial state, we employ Proximal Policy Optimization (PPO) as our optimization strategy. Specifically, we incorporate a penalty term in the reward function that penalizes the KL divergence between the learned reinforcement learning policy, denoted as ? RL ? , and the original supervised model, ? SF T . This is to ensure that the final model does not deviate excessively from the original supervised model. The complete reward function is defined as follows: R(x, y) = r ? (x, y) -? log(? RL ? (y|x)/? SF T (y|x)), where r ? (x, y) represents the output of the reward model and ? is the coefficient for KL divergence in the reward function. The loss function used in PPO optimization is given by: L = r ? ?t -?KL[? ? old , ? ? ], where r ? is the reward function, ?t is an estimator of the advantage function, ? old represents the parameters of the policy at the previous step, and ? ? is the current policy.</p><p>3 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implemented details</head><p>We chose BLOOM-7B <ref type="bibr" target="#b16">[17]</ref> as our base large language model, due to its open-source nature and multilingual support. For the supervised fine-tuning process, we set the learning rate to 5e-5, with a batch size of 128 and a maximum length of 1,024, training across 3 epochs. During the training of the reward model, we utilized the last feature vector of the final output sequence features as the text representation. Based on the fine-tuned model, we added a binary classification head to output the reward. We set the learning rate to 2e-5, with a batch size of 128, a maximum length of 1,024, and training over 3 epochs. For the reinforcement learning process, we applied a learning rate of 1e-5 and a maximum length of 1,024, training for 4000 steps. To efficiently train the large language model, we adopted LoRA (Low-Rank Approximated adapter) <ref type="bibr" target="#b17">[18]</ref>, a parameter efficient fine tuning method, with r of 8, alpha of 32, and dropout of 0.1. To decrease memory usage and improve training speed, we employed the ZeRO-2 <ref type="bibr" target="#b18">[19]</ref>, and made use of both TF32 (TensorFloat-32) and BF16 (Bfloat16). We selected several instruction fine-tuned models for comparison, including ChatGLM-6B <ref type="bibr" target="#b19">[20]</ref>, LLAMA-7B <ref type="bibr" target="#b20">[21]</ref> (fine-tuned on English and Chinese data), and BLOOM-7B <ref type="bibr" target="#b21">[22]</ref> (fined-tuned on crosslingual tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medical conversation</head><p>We conducted performance evaluation of the medical conversation on the test set of MedDialog. To address the challenge of multiple rounds of conversation within each medical dialogue, we randomly truncated the dialogue at a certain round, discarding the subsequent dialogue, and using the historical dialogue prior to this round as input. The sample response is shown in Table <ref type="table" target="#tab_0">1</ref>. We used three evaluation metrics: BLEU <ref type="bibr" target="#b22">[23]</ref>, ROUGE <ref type="bibr" target="#b23">[24]</ref>, and GLEU, to assess the quality of the conversations. BLEU is a commonly used metric that compares a candidate translation with one or more reference translations based on n-gram precision. GLEU calculates the average score of different n-grams, providing a more comprehensive evaluation of the generated text. ROUGE, on the other hand, is a particularly useful metric for evaluating automatic summarization and machine translation, as it focuses on the recall aspect of generated summaries by comparing them with references.</p><p>The experimental results are presented in Table <ref type="table" target="#tab_1">2</ref>. It demonstrates that ClinicalGPT achieves outstanding performance on BLEU-1 and all ROUGE scores. ClinicalGPT comes second only to BLOOM-7B in terms of BLEU-2, BLEU-3, and BLEU-4. The superior ROUGE scores achieved by ClinicalGPT indicate that the responses generated by the model cover the information provided by the reference text more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Medical examination</head><p>In this study, the medical examination assessment using the MEDQA-MCMLE dataset was evaluated with the categories which are the highest frequencies in the dataset. The selected categories included Medical ethics, Respiratory system, Digestive system, Urinary system, Hematologic diseases, Rheumatic immune Diseases, Pediatric diseases, Description of medical conditions and history ???????paget? (Disease: Suspected Paget's disease of the skin) ????????? (Duration of illness: more than six months.) ?????????????????? (Disease description: Has been treated as eczema for several years.) ??????????????????????????????????????????????? ???????????????????? (Help needed: Can the doctor provide me with some treatment suggestions? Does the current condition require surgery? Do I need to seek medical attention? What preparations should be made before seeking medical treatment? May I ask if the Third Hospital has ever received this disease? How to treat it?) Dialogue Assistant: ???????????????????????????????????????????? ????????????? (Assistant:Based on your condition, it is indeed necessary to undergo surgery to remove the lesion, but the specific surgical situation may require on-site judgment. It is recommended that you come for treatment and have a professional doctor develop a treatment plan for you.) User: ????????????? (User: What do you need to prepare before seeing a doctor?) Assistant: ???????????????????????????????????????????? ??????????????????????????? (Assistant: Before going to the clinic, you need to undergo a skin examination, including observing skin color, texture, rash size, etc. In addition, you also need to bring medical records, doctor's prescriptions, etc., so that the doctor can better diagnose and treat you.)  </p><formula xml:id="formula_0">SLE????????????????????????????????????????????? ?????????????????????????SLE???????E???SLE????????</formula><p>(SLE is an autoimmune disease, whose blood system changes include thrombocytopenia, autoimmune hemolytic anemia, positive Chromatophore anemia, etc. Leukemia like changes refer to the presence of a large number of immature cells in the bone marrow, which is not related to SLE. Therefore, option E does not meet the hematological changes of SLE.)</p><p>Table <ref type="table">3</ref>: A sample response in medical examination.</p><p>The experimental results, as shown in Table <ref type="table">4</ref>, reveal that ClinicalGPT outperformed other LLMs such as LLAMA-7B, ChatGLM-6B, and BLOOM-7B in all evaluated categories, boasting an average accuracy of 38.4. Specifically, ClinicalGPT achieved strong performance, exceeding the average scores of ChatGLM-6B, BLOOM-7B, and LLAMA-7B with 19.9, 25.7, and 27.2 respectively. Among all categories, ClinicalGPT achieved the best score in Rheumatic immune with an accuracy of 47.4. Conversely, it underperformed in Respiratory and Digestive diseases, with accuracies of 26.1 and 36.9, respectively. These findings suggest that while ClinicalGTP excels in understanding and generating responses related to rheumatic immune system, further refinement is required to improve its performance in Respiratory and Digestive diseases. Table <ref type="table">4</ref>: Comparisons on medical examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diagnosis</head><p>The diagnostic capabilities of LLMs (large language models) were evaluated on the testing set of MD-EHR. Disease groups were selected for evaluation, including Respiratory, Digestive, Urinary, Psychiatry, Neurology, Gynecology, and Hematology. The models were provided with concatenated notes from each medical record as input and generated text as output. The accuracy of the models was calculated by comparing the generated text with the diagnosis labels in the medical records. The sample response is shown in Table <ref type="table" target="#tab_4">5</ref>.</p><p>Prompt   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Medical question answering</head><p>For medical question-answering (QA) assessment, our model was benchmarked against several other models using a dataset of 388 questions sampled from cMedQA2. Automated evaluation metrics were used, with GPT-4 serving as the refrence model. Given the question, each model generated an answer independently. Then GPT-4 was used to assess these responses based on their accuracy, helpfulness, and safety. The GPT-4 assigned a judgment of Win, Tie, or Lose for each comparison. A "Win" indicates ClinicalGPT provided a superior response, a "Lose" indicates the competing model offered a better response, and a "Tie" means that no obvious difference between the responses was observed.  The results of the medical question-answering evaluation are presented in Table <ref type="table" target="#tab_7">7</ref>. According to the results, ClinicalGPT outperformed all of BLOOM-7B, LLAMA-7B, and ChatGLM-6B. In comparisons against BLOOM-7B and LLAMA-7B, our model won in 89.7% and 85.0% of the cases respectively. The percentage of tie cases were relatively small, at 1.8% against BLOOM-7B and 2.3% against LLAMA-7B. Meanwhile, ClinicalGPT wins against ChatGLM-6B at 67.2%. The tie rate increased to 10.9% and the loss rate to 22.0%. This performance suggests that while ChatGLM-6B has a commendable repository of medical knowledge and displays fluent textual expression, training with ClinicalGPT is beneficial for augmenting the capabilities in medical question answering, despite the extensive knowledge reserves of larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we introduced ClinicalGPT, a large language model tailored for medical and clinical applications.</p><p>Recognizing the limitations that generic large language models present in these specialized fields, we took steps to refine the model, assembling comprehensive datasets for its fine-tuning. These datasets incorporate real medical records, patient consultations, diverse medical knowledge, and exam data, all aimed at shaping the model's knowledge base and responsiveness. Our extensive experiments cover a range of critical tasks in the medical field, such as medical conversation, medical examination, diagnosis, and medical question answering. The empirical results highlight the superior capabilities of ClinicalGPT in understanding and generating medical and clinical-related responses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of ClinicalGPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A sample response in medical conversation.</figDesc><table><row><cell></cell><cell>BLEU-1</cell><cell>BLEU-2</cell><cell>BLEU-3</cell><cell>BLEU-4</cell><cell>GLEU</cell><cell>ROUGE-1</cell><cell>ROUGE-2</cell><cell>ROUGE-L</cell></row><row><cell>LLAMA-7B</cell><cell>10.8</cell><cell>2.9</cell><cell>1.5</cell><cell>0.9</cell><cell>0.6</cell><cell>22.4</cell><cell>5.1</cell><cell>17.3</cell></row><row><cell>ChatGLM-6B</cell><cell>6.6</cell><cell>1.6</cell><cell>0.9</cell><cell>0.5</cell><cell>0.3</cell><cell>23.6</cell><cell>5.0</cell><cell>16.2</cell></row><row><cell>BLOOM-7B</cell><cell>12.2</cell><cell>4.4</cell><cell>2.9</cell><cell>2.2</cell><cell>2.4</cell><cell>11.0</cell><cell>1.6</cell><cell>8.6</cell></row><row><cell>Ours</cell><cell>13.9</cell><cell>3.7</cell><cell>2.0</cell><cell>1.2</cell><cell>0.9</cell><cell>27.9</cell><cell>6.5</cell><cell>21.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on medical conversation. and Pharmacology. The models were fed with the form of questions and options as input, and the generated text was subsequently used to extract answers to compute accuracy. The sample response is shown in Table3.</figDesc><table><row><cell>Prompt:</cell></row><row><cell>????????SLE????????</cell></row><row><cell>(Which of the following blood system changes does not comply with SLE?)</cell></row><row><cell>?????</cell></row><row><cell>(Answer options:)</cell></row><row><cell>??A ?????</cell></row><row><cell>(Option A Thrombocytopenia)</cell></row><row><cell>??B ?????</cell></row><row><cell>(Option B: Self cell reduction)</cell></row><row><cell>??C ??????????</cell></row><row><cell>(Option C Autoimmune hemolytic anemia)</cell></row><row><cell>??D ????????</cell></row><row><cell>(Option D Positive Chromatophore anemia)</cell></row><row><cell>??E ???????</cell></row><row><cell>(Option E Leukemia like changes)</cell></row><row><cell>Response:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Current medical history and physical examination: Abdominal pain, no obvious pattern, vomiting, previous or current fever, no discomfort such as abdominal distension or abnormal crying. Urinary volume is acceptable, mental state is acceptable, and appetite is average. R 20 times/minute, P 110 times/minute, clear mind, good mental response, stable breathing, red lips, no congestion in the throat, thick respiratory sounds in both lungs, no rale heard, strong heart sounds, regular rhythm, no murmur heard in each valve area, soft abdomen, full abdominal tenderness, liver and spleen not reached, and no abnormalities were found in the nervous system examination.) ????????????????+???????????????????????????????? ?????????????? (Examination: high urine Amylase in the external hospital, influenza+, abdominal plain film: Bowel obstruction. The image shows that the shape and size of the liver section are normal, the capsule is smooth, and the parenchymal echo is uniform. (Before the operation, we will make some necessary preparations for you, including fasting and drinking water, as well as Blood test. After the surgery, we will closely observe and care for you to ensure your recovery is good.) ???????????????????????????????????????????????? ?? (At the same time, we also recommend that you pay attention to rest, avoid vigorous exercise and excessive fatigue, maintain good dietary habits and lifestyle, and promote physical recovery.) ??????????????????????????????????????????????? (If you have any questions or need further assistance, please feel free to contact us at any time. We will do our best to provide you with the best medical services.)</figDesc><table><row><cell>:</cell></row><row><cell>?????1??</cell></row><row><cell>(Complaint: Abdominal pain for 1 day.)</cell></row><row><cell>????????????????????????????????????????????????</cell></row><row><cell>??????????R 20?/??P 110?/??????????????????????????????</cell></row><row><cell>????????????????????????????????????????????????</cell></row><row><cell>?????</cell></row><row><cell>(No obvious abnormal echo is detected.)</cell></row><row><cell>Response:</cell></row><row><cell>????????????????????????????????????????????????</cell></row><row><cell>???????????????????????</cell></row><row><cell>(Hello, according to your symptoms and examination results, you are diagnosed as Appendicitis. Appendicitis is a common</cell></row><row><cell>acute abdominal pain disease, which usually requires surgical treatment. We suggest that you undergo surgical treatment as</cell></row><row><cell>soon as possible to avoid worsening the condition.)</cell></row><row><cell>????????????????????????????????????????????????</cell></row><row><cell>???????????????????????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>A sample response in diagnosis for an electronic medical record.The experimental results are demonstrated in Table6for each disease group. ClinicalGPT outperformed other language models, such as ChatGLM-6B, LLAMA-7B, and BLOOM-7B, across all disease groups. The average accuracy of ClinicalGPT across all disease groups was 80.9%, which is obviously higher than the 40.9% of ChatGLM-6B, 36.6% of LLAMA-7B, and 60.3% of BLOOM-7B. ClinicalGPT demonstrated particularly strong performance in the Digestive and Urinary departments, achieving accuracies of 90.1% and 89.9%, respectively. This indicates a robust capability for understanding and interpreting medical records across different disease groups. However, ClinicalGPT exhibited slightly lower, yet still impressive, performance in the Gynecology and Hematology departments, with accuracies of 78.6% and 80.7% respectively. This suggests that there may be room for improvement, specifically in the fields of Gynecology and Hematology, although ClinicalGPT still performed well overall across a range of medical specialties.</figDesc><table><row><cell></cell><cell>Respiratory</cell><cell>Digestive</cell><cell>Urinary</cell><cell>Psychiatry</cell><cell>Neurology</cell><cell>Gynecology</cell><cell>Hematology</cell><cell>Average</cell></row><row><cell>ChatGLM-6B</cell><cell>22.3</cell><cell>49.7</cell><cell>55.0</cell><cell>38.7</cell><cell>39.3</cell><cell>39.8</cell><cell>41.6</cell><cell>40.9</cell></row><row><cell>LLAMA-7B</cell><cell>24.2</cell><cell>43.7</cell><cell>40.9</cell><cell>34.9</cell><cell>32.8</cell><cell>40.8</cell><cell>39.2</cell><cell>36.6</cell></row><row><cell>BLOOM-7B</cell><cell>36.9</cell><cell>73.9</cell><cell>71.7</cell><cell>59.1</cell><cell>57.7</cell><cell>56.8</cell><cell>65.7</cell><cell>60.3</cell></row><row><cell>Ours</cell><cell>64.3</cell><cell>90.1</cell><cell>89.9</cell><cell>79.2</cell><cell>83.6</cell><cell>78.6</cell><cell>80.7</cell><cell>80.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparisons on diagnosis.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Medical question-answering on automatic evaluation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.xywy.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://cmekg.pcl.ac.cn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/baiyang2464/chatbot-base-on-Knowledge-Graph</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/zhihao-chen/QASystemOnMedicalGraph</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.haodf.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://www.xywy.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://www.39.net</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Parts of the experiments are conducted in the InforSuperBahn Testbed. The authors appreciate <rs type="affiliation">Nanjing Institute of InforSuperBahn</rs> for providing the test and evaluation platform.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Medical knowledge graphs</head><p>The CMeKG (Chinese Medical Knowledge Graph) is a Chinese medical knowledge graph created by human-AI collaboration, using natural language processing and text mining techniques. It's built upon international standards such as ICD, ATC, SNOMED, and MeSH, and integrates clinical guidelines, industry standards, and medical wiki websites as diverse sources. The CMeKG contains 62k entities and 374k relationship triplets, representing nine types of medical entities and their 23 different relationships. Entities include diseases <ref type="bibr" target="#b14">(15,</ref><ref type="bibr">962)</ref>, manifestations <ref type="bibr" target="#b11">(12,</ref><ref type="bibr">271)</ref>, body parts <ref type="bibr" target="#b16">(17,</ref><ref type="bibr">706)</ref>, equipment (900), procedures <ref type="bibr" target="#b5">(6,</ref><ref type="bibr">418)</ref>, microorganisms <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">934)</ref>, medical departments (356), tests <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">605)</ref>, and medications <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">935)</ref>. Relationships cover diverse medical aspects, with the most prominent being common symptoms (94,657) and side effects (62,339).</p><p>The xywy-KG is a medical knowledge graph generated using data sourced from a Chinese online medical consultation website 6 . These entities are categorized into seven groups: diseases (11,013), manifestations <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">998)</ref>, procedures (554), departments (54), examination items <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">353)</ref>, medications <ref type="bibr" target="#b21">(22,</ref><ref type="bibr">359)</ref>, and foods <ref type="bibr" target="#b3">(4,</ref><ref type="bibr">993)</ref>. The relationships are sorted into nine types, most notably examinations (39,531) and recommended medications (59,467), totally comprising 44k entities and 294k relationships.</p><p>The 39Health-KG is a medical knowledge graph built from data collected from 39-health, a website dedicated to health consultation and registration 7 . This graph integrates seven types of medical entities and eight types of relationships among them. It comprises 37k entities and 210k entity relationships. The entity types are diseases <ref type="bibr" target="#b13">(14,</ref><ref type="bibr">337)</ref>, body parts (82), departments (83), examination items (3,074), clinical manifestations (5,927), treatment methods (1,493), and medications (4,966). The relationships majorly revolve around related symptoms (48,757) and examination items (31,577).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Prompt templates</head><p>We designed prompt templates, transforming knowledge triplets into question-answer data for training language models. Examples of prompt templates are shown in Table <ref type="table">8</ref>.</p><p>What diseases are related to {disease}? {Disease} may be related to {disease}. What other diseases may be associated with {disease}?</p><p>{Disease} may be associated with {disease}. What are the common symptoms of {disease}?</p><p>The common symptoms of {disease} include {clinical manifestations}. What symptoms might a patient with {disease} exhibit?</p><p>Patients with {disease} may exhibit symptoms such as {clinical manifestations}. What are the typical {clinical manifestations} of {disease}?</p><p>The {Drug} interacts with {drug}. What drug interactions should be considered when using {drug}?</p><p>When using {drug}, interactions with {drug} should be considered. What symptoms can {drug} primarily treat? {Drug} is primarily used to treat symptoms such as {clinical manifestations}. What is the main therapeutic action of {drug}?</p><p>The main therapeutic action of {drug} is to treat {clinical manifestations}. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The potential impact of chatgpt in clinical and translational medicine</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical and translational medicine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The ai revolution in medicine: Gpt-4 and beyond</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Cowen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foundation models for generalist medical artificial intelligence</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oishi</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Shakeri Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harlan</forename><forename type="middle">M</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Krumholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="issue">7956</biblScope>
			<biblScope unit="page" from="259" to="265" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</title>
		<meeting>the 2021 ACM conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-scale attentive interaction networks for chinese medical question answer selection</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="74061" to="74071" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03329</idno>
		<title level="m">Two large-scale medical dialogue datasets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Browser-assisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Bigscience</forename><surname>Workshop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">:</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom</surname></persName>
		</author>
		<title level="m">A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagn?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gall?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
