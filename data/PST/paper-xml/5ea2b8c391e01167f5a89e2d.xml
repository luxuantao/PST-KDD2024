<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-23">23 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
							<email>prannayk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
							<email>pteterwak@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
							<email>wangch@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
							<email>sarna@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><forename type="middle">Isola</forename><surname>Mit</surname></persName>
							<email>phillipi@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
							<email>amaschinot@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
							<email>dilipkay@google.com</email>
						</author>
						<title level="a" type="main">Supervised Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-23">23 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.11362v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Figure <ref type="figure">1</ref>: Our supervised contrastive loss outperforms the cross entropy loss with standard data augmentations such as AutoAugment <ref type="bibr" target="#b8">[9]</ref> and RandAugment <ref type="bibr" target="#b9">[10]</ref>; we also compare to CutMix <ref type="bibr" target="#b54">[55]</ref>). We show results on ResNet-50, ResNet-101 and ResNet-200, and compare against the same ResNet architectures for other techniques (except CutMix models for which we compare against ResNeXt-101).</p><p>The cross-entropy loss is the most widely used loss function for supervised learning. It is naturally defined as the KL-divergence between two discrete distributions: the label distribution (a discrete distribution of 1-hot vectors) and the empirical distribution of the logits. A number of works have explored shortcomings with this loss, such as lack of robustness to noisy labels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref> and the possibility of poor margins <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>, leading to reduced generalization performance. In practice, however, most proposed alternatives do not seem to have worked better for large-scale datasets, such as ImageNet <ref type="bibr" target="#b10">[11]</ref>, as evidenced by the continued use of cross-entropy to achieve state of the art results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Many proposed improvements to regular crossentropy in fact involve a loosening of the definition of the loss, specifically that the reference distribution is axisaligned. These improvements are often motivated in dif-Figure <ref type="figure">2</ref>: Supervised vs. self-supervised contrastive losses: In the supervised contrastive loss considered in this paper (left), positives from one class are contrasted with negatives from other classes (since labels are provided); images from the same class are mapped to nearby points in a low-dimensional hypersphere. In self-supervised contrastive loss (right), labels are not provided. Hence positives are generated as data augmentations of a given sample (crops, flips, color changes etc.), and negatives are randomly sampled from the mini-batch. This can result in false negatives (shown in bottom right), which may not be mapped correctly, resulting in a worse representation. ferent ways. Label smoothing <ref type="bibr" target="#b44">[45]</ref> makes a fuzzy distinction between correct and incorrect labels by moving off-axis, which provides a small but significant boost in many applications <ref type="bibr" target="#b32">[33]</ref>. In self-distillation <ref type="bibr" target="#b23">[24]</ref>, multiple rounds of cross-entropy training are performed by using the "soft" labels from previous rounds as reference class distributions. Mixup <ref type="bibr" target="#b55">[56]</ref> and related data augmentation strategies create explicit new training examples, often by linear interpolation, and then apply the same linear interpolation to the target label distribution, akin to a softening of the original cross entropy loss. Models trained with these modifications show improved generalization, robustness, and calibration.</p><p>In this work, we propose a new loss for supervised training which completely does away with a reference distribution; instead we simply impose that normalized embeddings from the same class are closer together than embeddings from different classes. Our loss is directly inspired by the family of contrastive objective functions, which have achieved excellent performance in self-supervised learning in recent years in the image and video domains <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">43]</ref> and have connections to the large literature on metric learning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>As the name suggests, contrastive losses consist of two "opposing forces": for a given anchor point, the first force pulls the anchor closer in representation space to other points, and the second force pushes the anchor farther away from other points. The former set is known as positives, and the latter as negatives. Our key technical novelty in this work is to consider many positives per anchor in addition to many negatives (as opposed to the convention in self-supervised contrastive learning which uses only a single positive). We use provided labels to select the positives and negatives. Fig. <ref type="figure">2</ref> and Fig. <ref type="figure">3</ref> provide a visual explanation of our proposed loss.</p><p>The resulting loss is stable to train, as our empirical results show. It achieves very good top-1 accuracy on the ImageNet dataset on the ResNet-50 and ResNet-200 architectures <ref type="bibr" target="#b19">[20]</ref>. On ResNet-50 with Auto-Augment <ref type="bibr" target="#b8">[9]</ref>, we achieve a top-1 accuracy of 78.8%, which is a 1.6% improvement over the cross-entropy loss with the same data augmentation and architecture (see Fig. <ref type="figure">1</ref>). The gain in top-1 accuracy is also accompanied by increased robustness as measured on the ImageNet-C dataset <ref type="bibr" target="#b21">[22]</ref>. Our main contributions are summarized below: Figure <ref type="figure">3</ref>: Cross entropy, self-supervised contrastive loss and supervised contrastive loss: The cross entropy loss (left) uses labels and a softmax loss to train a model; the self-supervised contrastive loss (middle) uses a contrastive loss and data augmentations to learn representations about classes; the supervised contrastive loss (right) proposed in this paper has two stages; in the first stage we use labels to choose the images for a contrastive loss. In the second stage, we freeze the learned representations and then learn a classifier on a linear layer using a softmax loss: thus combining the benefits of using labels and contrastive losses.</p><p>3. Our loss is less sensitive to a range of hyperparameters than cross-entropy. This is an important practical consideration.</p><p>We believe that this is due to the more natural formulation of our loss that pulls representations of samples from the same class to be pulled closer together, rather than forcing them to be pulled towards a specific target as done in cross-entropy.</p><p>4. We show analytically that the gradient of our loss function encourages learning from hard positives and hard negatives. We also show that triplet loss <ref type="bibr" target="#b47">[48]</ref> is a special case of our loss when only a single positive and negative are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work draws on existing literature in self-supervised representation learning, metric learning and supervised learning. Due to the large amount of literature, we focus on the most relevant papers. The cross-entropy loss was introduced as a powerful loss function to train deep networks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28]</ref>. The key idea is simple and intuitive: each class is assigned a target (usually 1-hot) vector and the logits at the last layer of the network, after a softmax transformation, are gradually transformed towards the target vector. However, it is unclear why these target labels should be the optimal ones; some work has been done into identifying better target labels vectors e.g. <ref type="bibr" target="#b51">[52]</ref>.</p><p>In addition, a number of papers have studied other drawbacks of the cross-entropy loss, such as sensitivity to noisy labels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref>, adversarial examples <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, and poor margins <ref type="bibr" target="#b3">[4]</ref>. Alternative losses have been proposed; however, the more popular and effective ideas in practice have been approaches that change the reference label distribution, such as label smoothing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>, data augmentations such as Mixup <ref type="bibr" target="#b55">[56]</ref> and CutMix <ref type="bibr" target="#b54">[55]</ref>, and knowledge distillation <ref type="bibr" target="#b23">[24]</ref>.</p><p>Recent years have seen significantly more powerful self-supervised representation learning approaches based on deep learning models, and exploiting structure in the data. In the language domain, state of the art models learn pre-trained embeddings by predicting masked out tokens in a sentence or paragraph e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b30">31]</ref>. Downstream fine-tuning is then used to achieve excellent results on tasks such as sentiment classification and question answering. Due to the very general nature of pre-training, a huge amount of unlabeled data can be utilized, along with very large architectures.</p><p>In the image domain, predictive approaches have also been used to learn embeddings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b34">35]</ref>: the typical setup is that some part of the signal is left out and we try to predict that portion from other parts of the signal. To do this effectively requires the network to learn some semantic information about the signal, especially when passed through a bottleneck of lower dimension. However, the accurate prediction of high dimensional signals such as images is very difficult. A more powerful approach has been to replace a dense per-pixel predictive loss in input space with a loss in lower-dimensional representation space. A powerful family of models for self-supervised representation learning are collected under the umbrella of contrastive learning <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b5">6]</ref>. In these works, the losses are inspired by noise contrastive estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> or N-pair losses <ref type="bibr" target="#b42">[43]</ref>. Typically, the loss is applied at the last layer of a deep network. At test time, the embeddings from a previous layer are utilized for downstream transfer tasks, fine tuning or direct retrieval tasks.</p><p>Closely related to contrastive learning are metric learning and triplet losses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b39">40]</ref>. These losses have been used to learn powerful representations, often in supervised settings, where labels are used to guide the choice of positive and negative pairs. The key distinction between triplet losses and contrastive losses is the number of positive and negative pairs per data point. The triplet loss uses just one positive and one negative pair. In the supervised metric learning setting, the positive pair is chosen from the same class or category and the negative pair is chosen from other classes, often using hard-negative mining <ref type="bibr" target="#b39">[40]</ref>. Self-supervised contrastive losses similarly use just one positive pair, selected using either co-occurence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref> or using data augmentation <ref type="bibr" target="#b5">[6]</ref>. The major difference is that many negative pairs are used for each data-point. These are usually chosen uniformly at random using some form of weak knowledge, such as patches from other images, or frames from other randomly chosen videos, relying on the assumption that this approach yields a very low probability of false negatives.</p><p>Most similar to our supervised contrastive is the soft-nearest neighbors loss introduced in <ref type="bibr" target="#b38">[39]</ref> and used in <ref type="bibr" target="#b48">[49]</ref>. Similar to <ref type="bibr" target="#b48">[49]</ref>, we improve upon <ref type="bibr" target="#b38">[39]</ref> by normalizing the embeddings and replacing euclidean distance with inner products. We further improve on <ref type="bibr" target="#b48">[49]</ref> by the increased use of data augmentation, a disposable contrastive head and two-stage training (contrastive followed by cross-entropy). These distinctions help us achieve state-of-the-art top-1 accuracies for ImageNet on the ResNet-50 and ResNet-200 architectures <ref type="bibr" target="#b19">[20]</ref>. In <ref type="bibr" target="#b48">[49]</ref> introduces the approximation of only backpropagating through part of the loss, and also the approximation of using stale representations in the form of memory bank. By only contrasting against samples in the current mini-batch, we are able to remove these approximations. Furthermore, our specific loss formulation makes our learning gradient efficient (see Section 3.2.3 for more details). The work in <ref type="bibr" target="#b14">[15]</ref> also uses a similar loss formulation to ours; however it is used to entangle classes at intermediate layers by maximizing, instead of disentangling classes at the final layer as is done in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we start by reviewing the contrastive learning loss for self-supervised representation learning, as used in recent papers that achieve state of the art results <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6]</ref>. Then we show how we can modify this loss to be suitable for fully supervised learning, while simultaneously preserving properties important to the self-supervised approach. A natural transition between self-supervision and full supervision is semi-supervision, but we do not consider that paradigm in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation Learning Framework</head><p>Our representation learning framework is structurally similar to that used in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b5">6]</ref> for self-supervised contrastive learning and consists of the following components (see Fig. <ref type="figure">2</ref> and Fig. <ref type="figure">3</ref> for an illustration of the difference between the supervised and self-supervised scenarios).</p><p>• A data augmentation module, A(•), which transforms an input image, x, into a randomly augmented image, x. For each input image, we generate two randomly augmented images, each of which represents a different view of the data and thus contains some subset of the information in the original input image. The first stage of augmentation is applying a random crop to the image and then resizing that back to the image's native resolution. In light of the findings of <ref type="bibr" target="#b5">[6]</ref> that self-supervised contrastive loss requires significantly different data augmentation than cross-entropy loss, for the second stage we evaluate three different options:</p><p>-AutoAugment: <ref type="bibr" target="#b8">[9]</ref> -RandAugment: <ref type="bibr" target="#b9">[10]</ref> -SimAugment: A variant of the strategy of <ref type="bibr" target="#b5">[6]</ref> to sequentially apply random color distortion and Gaussian blurring, where we probabilistically add an additional sparse image warp to the end of the sequence.</p><p>• An encoder network, E(•), which maps an augmented image x to a representation vector, r = E(x) ∈ R D E . In our framework, both augmented images for each input image are separately input to the same encoder, resulting in a pair of representation vectors. We experiment with two commonly used encoder architectures, ResNet-50 and ResNet-200 <ref type="bibr" target="#b19">[20]</ref>, where the activations of the final pooling layer (D E = 2048) are used as the representation vector. This representation layer is always normalized to the unit hypersphere in R D E . We find from experiments that this normalization always improves performance, consistent with other papers that have used metric losses e.g. <ref type="bibr" target="#b39">[40]</ref>. We also find that the new supervised loss is able to train both of these architectures to a high accuracy with no special hyperparameter tuning. In fact, as reported in Sec. 4, we found that the supervised contrastive loss was less sensitive to small changes in hyperparameters, such as choice of optimizer or data augmentation.</p><p>• A projection network, P(•), which maps the normalized representation vector r into a vector z = P(r) ∈ R D P suitable for computation of the contrastive loss. For our projection network, we use a multi-layer perceptron <ref type="bibr" target="#b17">[18]</ref> with a single hidden layer of size 2048 and output vector of size D P = 128. We again normalize this vector to lie on the unit hypersphere, which enables using an inner product to measure distances in the projection space. The projection network is only used for training the supervised contrastive loss. After the training is completed, we discard this network and replace it with a single linear layer (for more details see Sec. 4). Similar to the results for self-supervised contrastive learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b5">6]</ref>, we found representations from the encoder to give improved performance on downstream tasks than those from the projection network. Thus our inference-time models contain exactly the same number of parameters as their cross-entropy equivalents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Losses: Self-Supervised and Supervised</head><p>We seek to develop a contrastive loss function that allows for an impactful incorporation of labeled data while at the same time preserves the beneficial properties of contrastive losses which have been paramount to the success of self-supervised representation learning. Similar to self-supervised contrastive learning, we generate minibatches by randomly sampling the data. For a set of N randomly sampled image/label pairs, {x k , y k } k=1...N , the corresponding minibatch used for training consists of 2N pairs, {x k , ỹk } k=1...2N , where, x2k and x2k−1 are two random augmentations of x k (k = 1...N ) and ỹ2k−1 = ỹ2k = y k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Self-Supervised Contrastive Loss</head><p>Within a minibatch, let i ∈ {1...2N } be the index of an arbitrary augmented image, and let j(i) be the index of the other augmented image originating from the same source image. In self-supervised contrastive learning (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>), the loss takes the following form.</p><formula xml:id="formula_0">L self = 2N i=1 L self i (1)</formula><formula xml:id="formula_1">L self i = − log exp z i • z j(i) /τ 2N k=1 1 i =k • exp (z i • z k /τ )<label>(2)</label></formula><p>where z = P(E(x )) (remember that P(•) and E(•) refer to the projection and encoder networks), 1 B ∈ {0, 1} is an indicator function that returns 1 iff B evaluates as true, and τ &gt; 0 is a scalar temperature parameter. Within the context of Eq. 2, index i is called the anchor, index j(i) is called the positive, and the other 2(N − 1) indices (k = 1...2N, k / ∈ {i, j}) are called the negatives. z i • z j(i) computes an inner (dot) product between the normalized vectors z i and z j(i) in 128-dimensional space. Note that for each anchor i, there is 1 positive pair and 2N − 2 negative pairs. The denominator has a total of 2N − 1 terms (the positive and negatives).</p><p>It is insightful to consider the effects on the encoder due to minimizing Eq. 1. During training, for any i, the encoder is tuned to maximize the numerator of the log argument in Eq. 2 while simultaneously minimizing its denominator. The constraint that the term exp z i • z j(i) is present in both the numerator and the denominator ensures that the log argument goes no higher than 1, and since Eq. 1 sums over all pairs of indices ((i, j) and (j, i)), the encoder is restricted from minimizing the denominator or maximizing the numerator without doing the other as well. As a result, the encoder learns to map similar views to neighboring representations while mapping dissimilar ones to non-neighboring ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Supervised Contrastive Loss</head><p>For supervised learning, the contrastive loss in Eq. 2 is incapable of handling the case where more than one sample is known with certainty to belong to the same class. To generalize the loss to handle arbitrary numbers of positives belonging to the same class, we propose the following novel loss function:</p><formula xml:id="formula_2">L sup = 2N i=1 L sup i (3) L sup i = −1 2N ỹi − 1 2N j=1 1 i =j • 1 ỹi = ỹj • log exp (z i • z j /τ ) 2N k=1 1 i =k • exp (z i • z k /τ ) (4)</formula><p>where N ỹi is the total number of images in the minibatch that have the same label, ỹi , as the anchor, i. This loss has important properties well suited for supervised learning:</p><p>• Generalization to an arbitrary number of positives. The major structural change of Eq. 4 over Eq. 2 is that now, for any anchor, all positives in a minibatch (i.e., the augmentation-based one as well as any of the remaining 2(N − 1) entries that are from the same class) contribute to the numerator. For minibatch sizes that are large with respect to the number of classes, multiple additional terms will be present (on average, N Li = N/C, where C is the number of classes). The loss encourages the encoder to give closely aligned representations to all entries from the same class in each instance of Eq. 4, resulting in a more robust clustering of the representation space that that generated from Eq. 2, as will be supported by our experiments in Sec. 4.</p><p>• Contrastive power increases with more negatives. The general form of the self-supervised contrastive loss (Eq. 4) is largely motivated by noise contrastive estimation and N-pair losses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>, wherein the ability to discriminate between signal and noise (negatives) is improved by adding more examples of negatives. This property has been shown to be important to representation learning via self-supervised contrastive learning, with many studies showing increased performance with increasing number of negatives <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6]</ref>. The supervised contrastive loss in Eq. 4 preserves this structure: adding larger numbers of negatives to the denominator provides increased contrast for the positives.</p><p>By using many positives and many negatives, we are able to better model both intra-class and inter-class variability. As we expect intuitively, and supported by our experiments in Sec. 4, this translates to representations that are provide improved generalization, since they better capture the representation for a particular class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Supervised Contrastive Loss Gradient Properties</head><p>We now provide further motivation for the form of the supervised contrastive loss in Eq. 4 by showing that its gradient has a structure that naturally causes learning to focus more on hard positives and negatives (i.e., ones against which continuing to contrast the anchor greatly benefits the encoder) rather than on weak ones (i.e., ones against which continuing to contrast the anchor only weakly benefits the encoder). The loss can thus be seen to be efficient in its training. Other contrastive losses, such as triplet loss <ref type="bibr" target="#b47">[48]</ref>, often use the computationally expensive technique of hard negative mining to increase training efficacy <ref type="bibr" target="#b39">[40]</ref>. As a byproduct of this analysis, we motivate the addition of a normalization layer at the end of the projection network, since its presence allows the gradient to have this structure.</p><p>As shown in the supplementary (see Sec. 10), if we let w denote the projection network output immediately prior to normalization (i.e., z = w/ w ), then the gradients of Eq. 4 with respect to w has the form:</p><formula xml:id="formula_3">∂L sup i ∂w i = ∂L sup i ∂w i pos + ∂L sup i ∂w i neg<label>(5)</label></formula><p>where:</p><formula xml:id="formula_4">∂L sup i ∂w i pos ∝ 2N j=1 1 i =j • 1 ỹi = ỹj • ((z i • z j ) • z i − z j ) • (1 − P ij )<label>(6)</label></formula><formula xml:id="formula_5">∂L sup i ∂w i neg ∝ 2N j=1 1 i =j • 1 ỹi = ỹj • 2N k=1 1 k / ∈{i,j} • (z k − (z i • z k ) • z i ) • P ik<label>(7)</label></formula><p>where:</p><formula xml:id="formula_6">P i = exp (z i • z /τ ) 2N k=1 1 i =k • exp (z k • z /τ ) , i, ∈ {1...2N } , i =<label>(8)</label></formula><p>is the 'th component of the temperature-scaled softmax distribution of inner products of representations with respect to anchor i and is thus interpretable as a probability. Eq. 6 generally includes contributions from the positives in the minibatch, while Eq. 7 includes those for negatives. We now show that easy positives and negatives have small gradient contributions while hard positives and negatives have large ones. For an easy positive, z i • z j ≈ 1 and thus P ij is large. Thus (see Eq. 6):</p><formula xml:id="formula_7">((z i • z j ) • z i − z j ) • (1 − P ij ) = 1 − (z i • z j ) 2 • (1 − P ij ) ≈ 0<label>(9)</label></formula><p>However, for a hard positive, z i • z j ≈ 0 and P ij is moderate, so:</p><formula xml:id="formula_8">((z i • z j ) • z i − z j ) • (1 − P ij ) = 1 − (z i • z j ) 2 • (1 − P ij ) &gt; 0<label>(10)</label></formula><p>Thus, for weak positives, where further contrastive efforts are of diminishing returns, the contribution to ∇ zi L sup i,pos is small, while for hard positives, where further contrastive efforts are still needed, the contribution is large. For a weak negative (z i • z k ≈ −1) and a hard negative (z i • z k ≈ 0), analogous calculations of (z k − (z i • z k ) • z i ) • P ik from Eq. 7 give similar conclusions: the gradient contribution is large for hard negatives and small for weak ones. As shown in the supplementary, the general ((z i • z ) • z − z ) structure, which plays a key role in ensuring the gradients are large for hard positives and negatives, appears only if a normalization layer is added to the end of the projection network, thereby justifying the use of a normalization in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connections to Triplet Loss</head><p>Contrastive learning is closely related to the triplet loss <ref type="bibr" target="#b47">[48]</ref>, which is one of the widely-used alternatives to cross-entropy for supervised representation learning. As discussed in Sec 2, the triplet loss has been used to generate robust representations via supervised settings where hard negative mining leads to efficient contrastive learning <ref type="bibr" target="#b39">[40]</ref>. The triplet loss, which can only handle one positive and negative at a time, can be shown to be a special case of the contrastive loss when the number of positives and negatives are each one. Assuming the representation of the anchor and the positive are more aligned than that of the anchor and negative (z a • z p z a • z n ), we have:</p><formula xml:id="formula_9">L con = −log exp (z a • z p /τ ) exp (z a • z p /τ ) + exp (z a • z n /τ ) = log (1 + exp ((z a • z n − z a • z p ) /τ )) ≈ exp ((z a • z n − z a • z p ) /τ ) (Taylor expansion of log) ≈ 1 + 1 τ • (z a • z n − z a • z p ) = 1 − 1 2τ • z a − z n 2 − z a − z p 2 ∝ z a − z p 2 − z a − z n 2 + 2τ</formula><p>which has the same form as a triplet loss with margin α = 2τ . This result is consistent with empirical results <ref type="bibr" target="#b5">[6]</ref> which show that contrastive loss performs better in general than triplet loss on representation tasks. Additionally, whereas triplet loss in practice requires computationally expensive hard negative mining (e.g., <ref type="bibr" target="#b39">[40]</ref>), the discussion in the previous section shows that the gradients of the supervised contrastive loss naturally impose a measure of hard negative reinforcement during training. This of course comes at the cost of requiring large batch sizes to allow for the inclusion of many positives and negatives, some of which will be hard in expectation as training proceeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our supervised contrastive loss by measuring classification accuracy on ImageNet and robustness to common image corruptions <ref type="bibr" target="#b21">[22]</ref>. After training the embedding network with supervised contrastive loss on ImageNet <ref type="bibr" target="#b10">[11]</ref>, we replace the projection head of the network with a a new randomly initialized linear dense (fully connected) layer. This linear layer is trained with standard cross entropy while the parameters of the embedding network are kept unchanged.  <ref type="bibr" target="#b21">[22]</ref> (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification Accuracy</head><p>Using the linear evaluation protocol as described above, we find that networks trained using our supervised contrastive loss give state-of-the-art results on ImageNet. Table <ref type="table" target="#tab_0">1</ref> shows results for ResNet-50 and ResNet-200 (we use ResNet-v1 <ref type="bibr" target="#b19">[20]</ref>). The supervised contrastive loss performs better than cross entropy for both architectures that we considered by over 1%. We achieve a new state of the art accuracy of 78.8% on ResNet-50 with AutoAugment (for comparison, a number of the other top-performing methods are shown in Table <ref type="table" target="#tab_0">1</ref>). Note that we also achieve a slight improvement over CutMix <ref type="bibr" target="#b54">[55]</ref>, which is considered to be a state of the art data augmentation strategy. Incorporating data augmentation strategies such as CutMix <ref type="bibr" target="#b54">[55]</ref> and MixUp <ref type="bibr" target="#b55">[56]</ref> into supervised contrastive learning could potentially improve results further. However, mixing labels blurs the interpretation of our loss, so we leave such experiments for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robustness to Image Corruptions and Calibration</head><p>Deep neural networks often lack robustness to out of distribution data or natural corruptions. This has been shown not only with adversarially constructed examples <ref type="bibr" target="#b15">[16]</ref>, but also with naturally occurring variations such as noise, blur and JPEG compression <ref type="bibr" target="#b21">[22]</ref>. To this end, <ref type="bibr" target="#b21">[22]</ref> made a benchmark dataset, ImageNet-C, which applies common naturally occuring perturbations such as noise, blur and contrast changes to the ImageNet dataset. In Table <ref type="table" target="#tab_1">2</ref>, we compare the supervised contrastive models to cross entropy using the mean Corruption Error (mCE) and relative mean Corruption Error (rel. mCE) metrics <ref type="bibr" target="#b21">[22]</ref>.</p><p>We see that the supervised contrastive models have lower mCE values across different corruptions, thus showing their increased robustness. We believe this increased robustness reflects the more powerful representations that are learnt by the contrastive loss function. In Fig. <ref type="figure" target="#fig_0">5</ref>, it is seen that the supervised contrastive methods retain high accuracy at high corruption severities while having low expected calibration errors. More details are provided in the supplementary material.</p><p>Figure <ref type="figure">4</ref>: Comparison of top-1 accuracy variability of cross entropy and supervised contrastive loss to changes in hyperparameters. We compare three augmentations (RandAugment <ref type="bibr" target="#b9">[10]</ref>, AutoAugment <ref type="bibr" target="#b8">[9]</ref> and SimAugment) (left plot); three optimizers (LARS, SGD with Momentum and RMSProp); and 3 learning rates that vary from the optimal rate by a factor of 10 smaller or larger. The supervised contrastive loss is more stable to changes in hyperparameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyperparameter Stability</head><p>Deep network training is well known to be sensitive to hyper-parameters and a large body of literature is devoted to finding efficient ways to perform hyperparameter tuning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>. We find that the contrastive supervised loss is more stable to changes in hyperparameters. In Figure <ref type="figure">4</ref>, we compare the top-1 accuracy of our loss against cross-entropy for different optimizers, data augmentations, and learning rates. We see significantly lower variance in the output of the contrastive loss when changing optimizer and data augmentation. While we do not have a theoretical explanation of this behaviour, we conjecture that this is due to the smoother geometry of the hypersphere compared to labels which are the endpoints of the n-dimensional simplex (as cross-entropy requires). Note that the minibatch sizes for cross entropy and supervised contrastive are the same, thus ruling out effects related to batch size. We experiment with hyperparameter stability by changing augmentations, optimizers and learning rates one at a time from the best combination for each of the methodologies. Table <ref type="table">3</ref>: Comparison of Top-1 accuracy variability as a function of the number of positives N ỹi in Eq. 4 varies from 1 to 5. Adding more positives benefits the final Top-1 accuracy. We compare against previous state of the art self-supervised work <ref type="bibr" target="#b5">[6]</ref> which has used one positive which is another data augmentation of the same sample; see text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effect of Number of Positives</head><p>We run ablations to test the effect of the number of positives in Eq. 4. Table <ref type="table">3</ref> shows the steady benefit of adding more positives for a ResNet-50 model trained on ImageNet with supervised contrastive loss. The trade-off is that more positives corresponds to a higher computational cost at training time. However, this is a highly parallelizable computation. Note that for each experiment, the number of positives always contains one positive which is the same sample but with a different data augmentation; and the remainder of the positives are different samples from the same class. Under this definition, self-supervised learning is considered as having 1 positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training Details</head><p>The supervised contrastive loss was trained for up to 700 epochs during the pretraining stage. We found that using half the number of epochs (350) only dropped the top-1 accuracy by a small amount. Each training step is about 50% slower than cross-entropy. This is due to the need to compute cross-products between every element of the minibatch and every other element (Eq. 4). The supervised contrastive loss needs an (optional) additional step of training a final linear classifier to compute top-1 accuracy. However, this is not needed if the purpose is to use representations for transfer learning tasks or retrieval.</p><p>We trained our models with batch sizes of up to 8192, although batch sizes of 2048 suffice for most purposes for both supervised contrastive and cross entropy losses. We report metrics for experiments with batch size 8192 for ResNet-50 and batch size 2048 for ResNet-200 (due to the larger network size, a smaller batch size is necessary). We observed that for a fixed batch size it was possible to train with supervised contrastive loss using larger learning rates and for a smaller number of epochs than what was required by cross entropy to achieve similar performance. Additionally, we observe that a small number of steps suffice to train the dense layers on top of the frozen embedding network, and we see minimal degradation in performance by training for as few as 10 epochs. All our results used a temperature of τ = 0.07 and note that smaller temperature benefit training more than higher ones. But, lower temperatures can be sometimes harder to train due to numerical stability issues. We also find that AutoAugment <ref type="bibr" target="#b8">[9]</ref> gives the best results for both Supervised Contrastive and Cross Entropy and we report ablations with other augmentations in the supplementary. We experimented with standard optimizers such as LARS <ref type="bibr" target="#b53">[54]</ref>, RMSProp <ref type="bibr" target="#b22">[23]</ref> and SGD with momentum <ref type="bibr" target="#b36">[37]</ref> in different permutations for the initial pre-training step and training of the dense layer. While the momentum optimizer works best for training ResNets with cross entropy, we get the best performance for supervised contrastive loss by using LARS for pre-training and RMSProp for training the dense layer on the top of the frozen network. We give detailed results for combination of optimizers in the supplementary section of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have presented a novel loss, inspired by contrastive learning, that outperforms cross entropy on classification accuracy and robustness benchmarks. Furthermore, our experiments show that this loss is less sensitive to hyperparameter changes, which could be a useful practical consideration. The loss function provides a natural connection between fully unsupervised training on the one end, and fully supervised training on the other. This opens the possibility of applications in semisupervised learning which can leverage the benefits of a single loss that can smoothly shift behavior based on the availability of labeled data. that models trained with contrastive learning do not see degradation of calibration and show a lower degradation of top-1 accuracy as the corruption severity increases. This shows that the model learns better representations which are robust to corruptions.  <ref type="bibr" target="#b21">[22]</ref> for a given level of severity (lower is better); Bottom: Average Top-1 Accuracy over all the corruptions for a given level of severity (higher is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Comparison with Cross Entropy</head><p>We also compared against using models trained with cross-entropy loss for representation learning. We do this by first training the model with cross entropy and then re-initializing the final layer of the network. In this second stage of training we again train with cross entropy but keep the weights of the network fixed. Table <ref type="table" target="#tab_3">5</ref> shows that the representations learnt by cross-entropy for a ResNet-50 network are not robust and just the re-initialization of the last layer leads to large drop in accuracy and a mixed result on robustness compared to a single-stage cross-entropy training. Both methods of training cross-entropy are inferior to supervised contrastive loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Training Details</head><p>In this section we provide the details of the experiments we ran to find the best set of optimizers and data augmentations to train supervised contrastive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Optimizer</head><p>We experiment with various optimizers for the contrastive learning and training the linear classifier in various combinations. We present our results in Table <ref type="table" target="#tab_4">6</ref>. The LARS optimizer <ref type="bibr" target="#b53">[54]</ref> gives us the best results to train the embedding network, confirming what has been reported by previous work <ref type="bibr" target="#b5">[6]</ref>. With LARS we use a cosine learning rate decay. On the other hand we find that the RMSProp optimizer <ref type="bibr" target="#b46">[47]</ref> works best for training the linear classifier. For RMSProp we use an exponential decay for the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Data Augmentation</head><p>We experimented with various data augmentations and found that AutoAugment <ref type="bibr" target="#b28">[29]</ref> gave us the best results in terms of downstream performance. We also note that AutoAugment is faster to implement than other augmentation schemes such as RandAugment <ref type="bibr" target="#b9">[10]</ref> or the data augmentations proposed in <ref type="bibr" target="#b5">[6]</ref>, which we denote SimAugment. As we show in Table <ref type="table">7</ref>, using the same data augmentation (AutoAugment) for both pre-training and training the linear classifier is optimal. We leave experimenting with MixUp <ref type="bibr" target="#b55">[56]</ref> or CutMix <ref type="bibr" target="#b54">[55]</ref>   Further we experiment with varying levels of augmentation magnitude for RandAugment since that has shown to affect performance when training models with cross entropy loss <ref type="bibr" target="#b9">[10]</ref>. Fig. <ref type="figure" target="#fig_2">7</ref> show that supervised contrastive methods consistently outperform cross entropy training independent of augmentation magnitude. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Derivation of Supervised Contrastive Learning Gradient</head><p>In Sec 2 in the main paper, we presented motivation based on the functional form of the gradient of the supervised contrastive loss, L sup i (z i ) (Eq. 4 in the paper), that the supervised contrastive loss intrinsically causes learning to focus on hard positives and negatives, where the encoder can greatly benefit, instead of easy ones, where the encoder can only minimally benefit. In this section, we derive the mathematical expression for the gradient:</p><formula xml:id="formula_10">∂L sup i (z i ) ∂w i = ∂z i ∂w i • ∂L sup i (z i ) ∂z i<label>(11)</label></formula><p>where w i is the projection network output prior to normalization, i.e., z i = w i / w i . As we will show, normalizing the representations provides structure to the gradient that causes learning to focus on hard positives and negatives instead of easy ones.</p><p>The supervised contrastive loss can be rewritten as:</p><formula xml:id="formula_11">L sup i (z i ) = −1 2N ỹi − 1 2N j=1 1 i =j • 1 ỹi = ỹj • log exp (z i • z j /τ ) 2N k=1 1 i =k • exp (z i • z k /τ ) = −1 2N ỹi − 1 2N j=1 1 i =j • 1 ỹi = ỹj • z i • z j τ − log 2N k=1 1 i =k • exp z i • z k τ</formula><p>Thus:</p><formula xml:id="formula_12">∂L sup i (z i ) ∂z i = −1 (2N ỹi − 1) • τ • 2N j=1 1 i =j • 1 ỹi = ỹj • z j − 2N k=1 1 i =k • z k • exp (z i • z k /τ ) 2N m=1 1 i =m • exp (z i • z m /τ ) = −1 (2N ỹi − 1) • τ • 2N j=1 1 i =j • 1 ỹi = ỹj • z j − 2N k=1 1 i =k • z k • P ik = −1 (2N ỹi − 1) • τ • 2N j=1 1 i =j • 1 ỹi = ỹj • (1 − P ij ) • z j − 2N k=1 1 k / ∈{i,j} • z k • P ik<label>(12)</label></formula><p>where we have defined P i as follows:</p><formula xml:id="formula_13">P i = exp (z i • z /τ ) 2N k=1 1 i =k • exp (z i • z /τ )</formula><p>, i, ∈ {1...2N } , i = P i is the 'th component of the temperature-scaled softmax distribution of inner products of representations with respect to anchor i and is thus interpretable as a probability. Note that, were we not to normalize the projection network output representations, then Eq. 12 would effectively be the gradient used for learning (simply let z i denote a non-normalized vector). It will be insightful to contrast Eq. 12 with the form for the projection network gradient (derived below) in which output representations are normalized.</p><p>The fact that we are using normalized projection network output representations introduces an additional term in Eq. 11, namely ∂z i /∂w i , which has the following form:</p><formula xml:id="formula_14">∂z i ∂w i = ∂ ∂w i w i w i = 1 w i • I − w i • ∂ (1/ w i ) ∂w i T = 1 w i I − w i • w T i w i 2 = 1 w i I − z i • z T i<label>(13)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Expected Calibration Error and mean top-1 accuracy at different corruption severities on ImageNet-C, on the ResNet-50 architecture (top) and ResNet-200 architecture (bottom). The contrastive loss maintains a higher accuracy over the range of corruption severities, and does not suffer from increasing calibration error, unlike the cross entropy loss.</figDesc><graphic url="image-7.png" coords="9,73.62,455.77,222.74,136.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 7 :</head><label>7</label><figDesc>Combinations of different data augmentations for ResNet-50 trained with optimal set of hyper-parameters and optimizers. We observe the best performance when the same data augmentation is used for both pre-training and training the linear classifier on top of the frozen embedding network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top-1 Accuracy vs RandAugment magnitude for ResNet-50 (left) and ResNet-200 (right). We see that supervised contrastive methods consistently outperform cross entropy for varying strength of augmentation.</figDesc><graphic url="image-10.png" coords="16,73.62,501.14,222.75,127.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="2,50.11,72.00,495.02,214.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="3,50.11,72.00,495.00,260.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="14,99.61,352.26,396.00,224.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Top-1/Top-5 accuracy results on ImageNet on ResNet-50 and ResNet-200 with AutoAugment<ref type="bibr" target="#b8">[9]</ref> being used as the augmentation for Supervised Contrastive learning. Achieving 78.8% on ResNet-50, we outperform all of the top methods whose performance is shown above. Baseline numbers are taken from the referenced papers and we also additionally reimplement cross-entropy ourselves for fair comparison.</figDesc><table><row><cell>Loss</cell><cell>Architecture</cell><cell></cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Cross Entropy</cell><cell>AlexNet [27]</cell><cell></cell><cell>56.5</cell><cell>84.6</cell></row><row><cell>(baselines)</cell><cell>VGG-19+BN [42]</cell><cell></cell><cell>74.5</cell><cell>92.0</cell></row><row><cell></cell><cell>ResNet-18 [20]</cell><cell></cell><cell>72.1</cell><cell>90.6</cell></row><row><cell></cell><cell cols="2">MixUp ResNet-50 [56]</cell><cell>77.4</cell><cell>93.6</cell></row><row><cell></cell><cell cols="2">CutMix ResNet-50 [55]</cell><cell>78.6</cell><cell>94.1</cell></row><row><cell></cell><cell cols="2">Fast AA ResNet-50 [9]</cell><cell>77.6</cell><cell>95.3</cell></row><row><cell></cell><cell cols="2">Fast AA ResNet-200 [9]</cell><cell>80.6</cell><cell>95.3</cell></row><row><cell>Cross Entropy</cell><cell>ResNet-50</cell><cell></cell><cell>77.0</cell><cell>92.9</cell></row><row><cell>(our implementation)</cell><cell>ResNet-200</cell><cell></cell><cell>78.0</cell><cell>93.3</cell></row><row><cell>Supervised Contrastive</cell><cell>ResNet-50</cell><cell></cell><cell>78.8</cell><cell>93.9</cell></row><row><cell></cell><cell>ResNet-200</cell><cell></cell><cell>80.8</cell><cell>95.6</cell></row><row><cell>Loss</cell><cell>Architecture</cell><cell cols="3">rel. mCE mCE</cell></row><row><cell>Cross Entropy</cell><cell>AlexNet [27]</cell><cell></cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>(baselines)</cell><cell>VGG-19+BN [42]</cell><cell></cell><cell>122.9</cell><cell>81.6</cell></row><row><cell></cell><cell>ResNet-18 [20]</cell><cell></cell><cell>103.9</cell><cell>84.7</cell></row><row><cell>Cross Entropy</cell><cell>ResNet-50</cell><cell></cell><cell>103.7</cell><cell>68.4</cell></row><row><cell>(our implementation)</cell><cell>ResNet-200</cell><cell></cell><cell>96.6</cell><cell>69.4</cell></row><row><cell>Supervised Contrastive</cell><cell>ResNet-50</cell><cell></cell><cell>87.5</cell><cell>64.4</cell></row><row><cell></cell><cell>ResNet-200</cell><cell></cell><cell>77.1</cell><cell>57.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Training with Supervised Contrastive Loss makes models more robust to corruptions in images, as measured by Mean Corruption Error (mCE) and relative mCE over the ImageNet-C dataset</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Top: Average Expected Calibration Error (ECE) over all the corruptions in ImageNet-C</figDesc><table><row><cell>Test</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison between representations learnt using SupCon and representations learnt using Cross Entropy loss with either 1 stage of training or 2 stages (representation learning followed by linear classifier).</figDesc><table><row><cell></cell><cell cols="3">Accuracy mCE rel. mCE</cell></row><row><cell>Supervised Contrastive</cell><cell>78.8</cell><cell>64.4</cell><cell>87.5</cell></row><row><cell>Cross Entropy (1 stage)</cell><cell>77.1</cell><cell>68.4</cell><cell>103.7</cell></row><row><cell>Cross Entropy (2 stage)</cell><cell>73.7</cell><cell>73.3</cell><cell>92.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Results of training the ResNet-50 architecture with AutoAugment data augmentation policy for 350 epochs and then training the linear classifier for another 350 epochs. Learning rates were optimized for every optimizer while all other hyper-parameters were kept the same.</figDesc><table><row><cell cols="3">Contrastive Optimizer Linear Optimizer Top-1 Accuracy</cell></row><row><cell>LARS</cell><cell>RMSProp</cell><cell>78.6</cell></row><row><cell>LARS</cell><cell>LARS</cell><cell>77.9</cell></row><row><cell>RMSProp</cell><cell>RMSProp</cell><cell>77.8</cell></row><row><cell>LARS</cell><cell>Momentum</cell><cell>77.6</cell></row><row><cell>Momentum</cell><cell>RMSProp</cell><cell>73.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>as future work.</figDesc><table><row><cell cols="3">Contrastive Augmentation Linear classifier Augmentation Accuracy</cell></row><row><cell>AutoAugment</cell><cell>AutoAugment</cell><cell>78.6</cell></row><row><cell>AutoAugment</cell><cell>RandAugment</cell><cell>78.1</cell></row><row><cell>AutoAugment</cell><cell>SimAugment</cell><cell>75.4</cell></row><row><cell>SimAugment</cell><cell>AutoAugment</cell><cell>76.1</cell></row><row><cell>SimAugment</cell><cell>RandAugment</cell><cell>75.9</cell></row><row><cell>SimAugment</cell><cell>SimAugment</cell><cell>77.9</cell></row><row><cell>RandAugment</cell><cell>AutoAugment</cell><cell>78.3</cell></row><row><cell>RandAugment</cell><cell>RandAugment</cell><cell>78.4</cell></row><row><cell>RandAugment</cell><cell>SimAugment</cell><cell>76.3</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary 6. Effect of Temperature in Loss Function</head><p>Similar to previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46]</ref>, we find that the temperature used in the loss function (for the softmax) has an important role to play in supervised contrastive learning and that the model trained with the optimal temperature can outperform ablations up to 3%. Two competing effects that changing the temperature has on training the model are:</p><p>1. Smoothness: The distances in the representation space used for training the model have gradients with smaller norm (||∇L|| ∝ 1 τ ); see Section 10. Smaller magnitude gradients make the optimization problem simpler by allowing for larger learning rates. In Section 3.3 of the paper, it is shown that in the case of a single positive and negative, the contrastive loss is equivalent to a triplet loss with margin ∝ τ . Therefore, in these cases, a larger temperature makes the optimization easier, and classes more separated.</p><p>2. Hard negatives: On the other hand, hard negatives have shown to improve classification accuracy when models are trained with the triplet loss <ref type="bibr" target="#b39">[40]</ref>. Low temperatures are equivalent to optimizing for hard negatives: for a given batch of samples and a specific anchor, lowering the temperature increases the value of P ik (see Eq. 8) for samples which have larger inner product with the anchor, and reduces it for samples which have smaller inner product. Further the magnitude of gradient coming from a given sample k belonging to a different class than the anchor is proportional to the probability P ik . Therefore the model derives a large amount of training signal from samples which belong to a different class but it finds hard to separate from the given anchor, which is by definition a hard negative.</p><p>Based on numerical experiments, we found a temperature of 0.07 to be optimal for top-1 accuracy on ResNet-50; results on various temperatures are shown in Fig. <ref type="figure">6</ref>. We use the same temperature for all experiments on ResNet-200, as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Robustness</head><p>Along with measuring the mean Corruption Error (mCE) and mean relative Corruption Error <ref type="bibr" target="#b21">[22]</ref> on the ImageNet-C dataset (see paper, Section 4.2 and Table <ref type="table">2</ref>), we also measure the Expected Calibration Error and the mean accuracy of our models on different severities of the corruption. The aim is to understand how performance and calibration degrades as the data shifts farther from the training distribution and becomes harder to classify. Table <ref type="table">4</ref> shows the results. We clearly observe where I is the identity matrix. Substituting Eqs. 12 and 13 into Eq. 11 gives the following expression for the gradient wrt the normalized projection network output representations:</p><p>where:</p><p>As discussed in detail in the main paper, for hard positives (i.e., z i</p><p>i /∂w i | pos results in large gradients while for easy positives (z i • z j ≈ 1), it results in small gradients. Analogously, for hard negatives (z i • z j ≈ 0), the (z k − (z i • z k ) • z k ) • P ik structure present in ∂L sup i /∂w i | neg results in large gradients while for easy negatives (z i • z j ≈ −1), it results in small gradients. Comparing ∂L sup i /∂w i | pos and ∂L sup i /∂w i | neg to that of Eq. 12, one can see that normalizing the projection network output representations served to introduce the general ((z i • z ) • z − z ) structure, which plays a key role in ensuring the gradients are large for hard positives and negatives.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised learning of probability distributions by neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><surname>Wilczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
				<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1565" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2, 4, 5, 6, 7, 10, 14</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Marc</forename><surname>Claesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02127</idno>
		<title level="m">Hyperparameter search in machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009">2019. 1, 2, 4, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Practical data augmentation with no separate search</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2009">2019. 1, 4, 9</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large margin deep networks for classification</title>
		<author>
			<persName><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2012" to="2020" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Series in Statistics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2001">2001</date>
			<publisher>Springer New York Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008">2016. 2, 4, 5, 8</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2006">2019. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019. 2, 7, 8, 14</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno>2012. 10</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Cited on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerated learning in layered neural networks</title>
		<author>
			<persName><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fleisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="625" to="640" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00397</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Fast autoaugment. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cross-entropy loss and low-rank features have responsibility for adversarial examples</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Nar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Ocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Shankar Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Ramchandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08360</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015. 4, 5, 6, 7, 14</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Timecontrastive networks: Self-supervised learning from video</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2006">2016. 2, 4, 6</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019. 2, 4, 5, 6, 14</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural networks for machine learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2007">Feb. 2009. 2, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep representation learning with target coding</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2019. 1, 3</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2003">2017. 2, 3</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
