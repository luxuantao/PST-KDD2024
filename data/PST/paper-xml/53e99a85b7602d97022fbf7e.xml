<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low Density Lattice Codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Naftali</forename><surname>Sommer</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Meir</forename><surname>Feder</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ofir</forename><surname>Shalvi</surname></persName>
						</author>
						<title level="a" type="main">Low Density Lattice Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A0415E75572ED20FCA956524AF37B86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lattices</term>
					<term>lattice codes</term>
					<term>iterative decoding</term>
					<term>LDPC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low density lattice codes (LDLC) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white Gaussian noise (AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional Euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = Gb, where H = G -1 is restricted to be sparse. The fact that H is sparse is utilized to develop a lineartime iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ∼ 0.5dB from capacity at block length of n = 100, 000 symbols. The paper also discusses convergence results and implementation considerations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>If we take a look at the evolution of codes for binary or finite alphabet channels, it was first shown <ref type="bibr" target="#b0">[1]</ref> that channel capacity can be achieved with long random codewords. Then, it was found out <ref type="bibr" target="#b1">[2]</ref> that capacity can be achieved via a simpler structure of linear codes. Then, specific families of linear codes were found that are practical and have good minimum Hamming distance (e.g. convolutional codes, cyclic block codes, specific cyclic codes such as BCH and Reed-Solomon codes <ref type="bibr" target="#b3">[4]</ref>). Later, capacity achieving schemes were found, which have special structures that allow efficient iterative decoding, such as low-density parity-check (LDPC) codes <ref type="bibr" target="#b4">[5]</ref> or turbo codes <ref type="bibr" target="#b5">[6]</ref>.</p><p>If we now take a similar look at continuous alphabet codes for the additive white Gaussian noise (AWGN) channel, it was first shown <ref type="bibr" target="#b2">[3]</ref> that codes with long random Gaussian codewords can achieve capacity. Later, it was shown that lattice codes can also achieve capacity ( <ref type="bibr" target="#b6">[7]</ref> - <ref type="bibr" target="#b11">[12]</ref>). Lattice codes are clearly the Euclidean space analogue of linear codes. Similarly to binary codes, we could expect that specific practical lattice codes will then be developed. However, there was almost no further progress from that point. Specific lattice codes that were found were based on fixed dimensional classical lattices <ref type="bibr" target="#b18">[19]</ref> or based on algebraic error correcting codes <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref>, but no significant effort was made in designing lattice codes directly in the Euclidean space or in finding specific capacity achieving lattice codes. Practical coding schemes for the AWGN channel were based on finite alphabet codes.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, "signal codes" were introduced. These are lattice codes, designed directly in the Euclidean space, where the information sequence of integers i n , n = 1, 2, ... is encoded by convolving it with a fixed signal pattern g n , n = 1, 2, ...d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal codes are clearly analogous to convolutional codes, and</head><p>The material in this paper was presented in part in the IEEE International Symposium on Information Theory, Seattle, July 2006, and in part in the Inauguration of the UCSD Information Theory and Applications Center, San Diego, <ref type="bibr">Feb. 2006.</ref> in particular can work at the AWGN channel cutoff rate with simple sequential decoders. In <ref type="bibr" target="#b15">[16]</ref> it is also demonstrated that signal codes can work near the AWGN channel capacity with more elaborated bi-directional decoders. Thus, signal codes provided the first step toward finding effective lattice codes with practical decoders.</p><p>Inspired by LDPC codes and in the quest of finding practical capacity achieving lattice codes, we propose in this work "Low Density Lattice Codes" (LDLC). We show that these codes can approach the AWGN channel capacity with iterative decoders whose complexity is linear in block length. In recent years several schemes were proposed for using LDPC over continuous valued channels by either multilevel coding <ref type="bibr" target="#b17">[18]</ref> or by non-binary alphabet (e.g. <ref type="bibr" target="#b16">[17]</ref>). Unlike these LDPC based schemes, in LDLC both the encoder and the channel use the same real algebra which is natural for the continuous-valued AWGN channel. This feature also simplifies the convergence analysis of the iterative decoder.</p><p>The outline of this paper is as follows. Low density lattice codes are first defined in Section II. The iterative decoder is then presented in Section III, followed by convergence analysis of the decoder in Section IV. Then, Section V describes how to choose the LDLC code parameters, and Section VI discusses implementation considerations. The computational complexity of the decoder is then discussed in Section VII, followed by a brief description of encoding and shaping in Section VIII. Simulation results are finally presented in Section IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BASIC DEFINITIONS AND PROPERTIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lattices and Lattice Codes</head><p>An n dimensional lattice in R m is defined as the set of all linear combinations of a given basis of n linearly independent vectors in R m with integer coefficients. The matrix G, whose columns are the basis vectors, is called a generator matrix of the lattice. Every lattice point is therefore of the form x = Gb, where b is an n-dimensional vector of integers. The Voronoi cell of a lattice point is defined as the set of all points that are closer to this point than to any other lattice point. The Voronoi cells of all lattice points are congruent, and for square G the volume of the Voronoi cell is equal to det(G). In the sequel G will be used to denote both the lattice and its generator matrix.</p><p>A lattice code of dimension n is defined by a (possibly shifted) lattice G in R m and a shaping region B ⊂ R m , where the codewords are all the lattice points that lie within the shaping region B. Denote the number of these codewords by N . The average transmitted power (per channel use, or per symbol) is the average energy of all codewords, divided by the codeword length m. The information rate (in bits/symbol) is log 2 (N )/m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:0704.1317v1 [cs.IT] 11 Apr 2007</head><p>When using a lattice code for the AWGN channel with power limit P and noise variance σ 2 , the maximal information rate is limited by the capacity 1  2 log 2 (1 + P σ 2 ). Poltyrev <ref type="bibr" target="#b19">[20]</ref> considered the AWGN channel without restrictions. If there is no power restriction, code rate is a meaningless measure, since it can be increased without limit. Instead, it was suggested in <ref type="bibr" target="#b19">[20]</ref> to use the measure of constellation density, leading to a generalized definition of the capacity as the maximal possible codeword density that can be recovered reliably. When applied to lattices, the generalized capacity implies that there exists a lattice G of high enough dimension n that enables transmission with arbitrary small error probability, if and only if</p><formula xml:id="formula_0">σ 2 &lt; n √ |det(G)| 2 2πe</formula><p>. A lattice that achieves the generalized capacity of the AWGN channel without restrictions, also achieves the channel capacity of the power constrained AWGN channel, with a properly chosen spherical shaping region (see also <ref type="bibr" target="#b11">[12]</ref>).</p><p>In the rest of this work we shall concentrate on the lattice design and the lattice decoding algorithm, and not on the shaping region or shaping algorithms. We shall use lattices with det(G) = 1, where analysis and simulations will be carried for the AWGN channel without restrictions. A capacity achieving lattice will have small error probability for noise variance σ 2 which is close to the theoretical limit 1 2πe .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Syndrome and Parity Check Matrix for Lattice Codes</head><p>A binary (n, k) error correcting code is defined by its n × k binary generator matrix G. A binary information vector b with dimension k is encoded by x = Gb, where calculations are performed in the finite field GF <ref type="bibr" target="#b1">(2)</ref>. The parity check matrix H is an (n -k) × n matrix such that x is a codeword if and only if Hx = 0. The input to the decoder is the noisy codeword y = x + e, where e is the error sequence and addition is done in the finite field. The decoder typically starts by calculating the syndrome s = Hy = H(x+e) = He which depends only on the noise sequence and not on the transmitted codeword.</p><p>We would now like to extend the definitions of the parity check matrix and the syndrome to lattice codes. An ndimensional lattice code is defined by its n×n lattice generator matrix G (throughout this paper we assume that G is square, but the results are easily extended to the non-square case). Every codeword is of the form x = Gb, where b is a vector of integers. Therefore, G -1 x is a vector of integers for every codeword x. We define the parity check matrix for the lattice code as H ∆ = G -1 . Given a noisy codeword y = x + w (where w is the additive noise vector, e.g. AWGN, added by real arithmetic), we can then define the syndrome as s ∆ = f rac{Hy}, where f rac{x} is the fractional part of x, defined as f rac{x} = x -x , where x denotes the nearest integer to x. The syndrome s will be zero if and only if y is a lattice point, since Hy will then be a vector of integers with zero fractional part. For a noisy codeword, the syndrome will equal s = f rac{Hy} = f rac{H(x + w)} = f rac{Hw} and therefore will depend only on the noise sequence and not on the transmitted codeword, as desired.</p><p>Note that the above definitions of the syndrome and parity check matrix for lattice codes are consistent with the definitions of the dual lattice and the dual code <ref type="bibr" target="#b18">[19]</ref>: the dual lattice of a lattice G is defined as the lattice with generator matrix H = G -1 , where for binary codes, the dual code of G is defined as the code whose generator matrix is H, the parity check matrix of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low Density Lattice Codes</head><p>We shall now turn to the definition of the codes proposed in this paper -low density lattice codes (LDLC).</p><p>Definition 1 (LDLC): An n dimensional LDLC is an ndimensional lattice code with a non-singular lattice generator matrix G satisfying |det(G)| = 1, for which the parity check matrix H = G -1 is sparse. The i'th row degree r i , i = 1, 2, ...n is defined as the number of nonzero elements in row i of H, and the i'th column degree c i , i = 1, 2, ...n is defined as the number of nonzero elements in column i of H.</p><p>Note that in binary LDPC codes, the code is completely defined by the locations of the nonzero elements of H. In LDLC there is another degree of freedom since we also have to choose the values of the nonzero elements of H.</p><p>Definition 2 (regular LDLC): An n dimensional LDLC is regular if all the row degrees and column degrees of the parity check matrix are equal to a common degree d.</p><p>Definition 3 (magic square LDLC): An n dimensional regular LDLC with degree d is called "magic square LDLC" if every row and column of the parity check matrix H has the same d nonzero values, except for a possible change of order and random signs. The sorted sequence of these d values h 1 ≥ h 2 ≥ ... ≥ h d &gt; 0 will be referred to as the generating sequence of the magic square LDLC.</p><p>For example, the matrix The bipartite graph of an LDLC is defined similarly to LDPC codes: it is a graph with variable nodes at one side and check nodes at the other side. Each variable node corresponds to a single element of the codeword x = Gb. Each check node corresponds to a check equation (a row of H). A check equation is of the form k h k x i k = integer, where i k denotes the locations of the nonzero elements at the appropriate row of H, h k are the values of H at these locations and the integer at the right hand side is unknown. An edge connects check node i and variable node j if and only if H i,j = 0. This edge is assigned the value H i,j . Figure <ref type="figure" target="#fig_9">1</ref> illustrates the bi-partite graph of a magic square LDLC with degree 3. In the figure, every variable node x k is also associated with its noisy channel observation y k .</p><formula xml:id="formula_1">H =         0 -0.8 0 -0.</formula><p>Finally, a k-loop is defined as a loop in the bipartite graph that consists of k edges. A bipartite graph, in general, can only</p><formula xml:id="formula_2">1 h 3 h 2 h 1 h 2 h 3 h 1 x k x n x 1 y n y k y</formula><p>Fig. <ref type="figure" target="#fig_9">1</ref>. The bi-partite graph of an LDLC contain loops with even length. Also, a 2-loop, which consists of two parallel edges that originate from the same variable node to the same check node, is not possible by the definition of the graph. However, longer loops are certainly possible. For example, a 4-loop exists when two variable nodes are both connected to the same pair of check nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ITERATIVE DECODING FOR THE AWGN CHANNEL</head><p>Assume that the codeword x = Gb was transmitted, where b is a vector of integers. We observe the noisy codeword y = x + w, where w is a vector of i.i.d Gaussian noise samples with common variance σ 2 , and we need to estimate the integer valued vector b. The maximum likelihood (ML) estimator is then b = arg min b ||y -Gb|| 2 . Our decoder will not estimate directly the integer vector b. Instead, it will estimate the probability density function (PDF) of the codeword vector x. Furthermore, instead of calculating the n-dimensional PDF of the whole vector x, we shall calculate the n one-dimensional PDF's for each of the components x k of this vector (conditioned on the whole observation vector y). In appendix I it is shown that f x k |y (x k |y) is a weighted sum of Dirac delta functions:</p><formula xml:id="formula_3">f x k |y (x k |y) = C • l∈G∩B δ(x k -l k ) • e -d 2 (l,y)/2σ 2 (1)</formula><p>where l is a lattice point (vector), l k is its k-th component, C is a constant independent of x k and d(l, y) is the Euclidean distance between l and y. Direct evaluation of ( <ref type="formula" target="#formula_109">1</ref>) is not Our decoder will decode to the infinite lattice, thus ignoring the shaping region boundaries. This approximate decoding method is no longer exact maximum likelihood decoding, and is usually denoted "lattice decoding" <ref type="bibr" target="#b11">[12]</ref>.</p><p>The calculation of f x k |y (x k |y) is involved since the components x k are not independent random variables (RV's), because x is restricted to be a lattice point. Following <ref type="bibr" target="#b4">[5]</ref> we use a "trick" -we assume that the x k 's are independent, but add a condition that assures that x is a lattice point. Specifically, define s ∆ = H • x. Restricting x to be a lattice point is equivalent to restricting s ∈ Z n . Therefore, instead of calculating f x k |y (x k |y) under the assumption that x is a lattice point, we can calculate f x k |y (x k |y, s ∈ Z n ) and assume that the x k are independent and identically distributed (i.i.d) with a continuous PDF (that does not include Dirac delta functions). It still remains to set f x k (x k ), the PDF of x k . Under the i.i.d assumption, the PDF of the codeword x is</p><formula xml:id="formula_4">f x (x) = n k=1 f x k (x k ).</formula><p>As shown in Appendix II, the value of f x (x) is not important at values of x which are not lattice points, but at a lattice point it should be proportional to the probability of using this lattice point. Since we assume that all lattice points are used equally likely, f x (x) must have the same value at all lattice points. A reasonable choice for f x k (x k ) is then to use a uniform distribution such that x will be uniformly distributed in an n-dimensional cube. For an exact ML decoder (that takes into account the boundaries of the shaping region), it is enough to choose the range of f x k (x k ) such that this cube will contain the shaping region. For our decoder, that performs lattice decoding, we should set the range of f x k (x k ) large enough such that the resulting cube will include all the lattice points which are likely to be decoded. The derivation of the iterative decoder shows that this range can be set as large as needed without affecting the complexity of the decoder.</p><p>The derivation in <ref type="bibr" target="#b4">[5]</ref> further imposed the tree assumption. In order to understand the tree assumption, it is useful to define the tier diagram, which is shown in Figure <ref type="figure" target="#fig_6">2</ref> for a regular LDLC with degree 3. Each vertical line corresponds to a check equation. The tier 1 nodes of x 1 are all the elements x k that take place in a check equation with x 1 . The tier 2 nodes of x 1 are all the elements that take place in check equations with the tier 1 elements of x 1 , and so on. The tree assumption assumes that all the tree elements are distinct (i.e. no element appears in different tiers or twice in the same tier). This assumption simplifies the derivation, but in general, does not hold in practice, so our iterative algorithm is not guaranteed to converge to the exact solution (1) (see Section IV).</p><p>The detailed derivation of the iterative decoder (using the above "trick" and the tree assumption) is given in Appendix III. In Section III-A below we present the final resulting algorithm. This iterative algorithm can also be explained by intuitive arguments, described after the algorithm specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Iterative Decoding Algorithm</head><p>The iterative algorithm is most conveniently represented by using a message passing scheme over the bipartite graph of the code, similarly to LDPC codes. The basic difference is that in LDPC codes the messages are scalar values (e.g. the log likelihood ratio of a bit), where for LDLC the messages are real functions over the interval (-∞, ∞). As in LDPC, in each iteration the check nodes send messages to the variable nodes along the edges of the bipartite graph and vice versa. The messages sent by the check nodes are periodic extensions of PDF's. The messages sent by the variable nodes are PDF's.</p><p>LDLC iterative decoding algorithm: Denote the variable nodes by x 1 , x 2 , ..., x n and the check nodes by c 1 , c 2 , ...c n .</p><p>• Initialization: each variable node x k sends to all its check nodes the message f</p><formula xml:id="formula_5">(0) k (x) = 1 √ 2πσ 2 e -(y k -x) 2 2σ 2</formula><p>.</p><p>• Basic iteration -check node message: Each check node sends a (different) message to each of the variable nodes that are connected to it. For a specific check node denote (without loss of generality) the appropriate check equation by r l=1 h l x m l = integer, where x m l , l = 1, 2...r are the variable nodes that are connected to this check node (and r is the appropriate row degree of H). Denote by f l (x), l = 1, 2...r, the message that was sent to this check node by variable node x m l in the previous halfiteration. The message that the check node transmits back to variable node x mj is calculated in three basic steps.</p><p>1) The convolution step -all messages, except f j (x), are convolved (after expanding each f l (x) by h l ):</p><formula xml:id="formula_6">pj (x) = f 1 x h 1 • • • f j-1 x h j-1 f j+1 x h j+1 • • • • • • f r x h r<label>(2)</label></formula><p>2) The stretching step -The result is stretched by (-h j ) to p j (x) = pj (-h j x) 3) The periodic extension step -The result is extended to a periodic function with period 1/|h j |:</p><formula xml:id="formula_7">Q j (x) = ∞ i=-∞ p j x - i h j<label>(3)</label></formula><p>The function Q j (x) is the message that is finally sent to variable node x mj . 1) The product step:</p><formula xml:id="formula_8">fj (x) = e -(y k -x) 2 2σ 2 e l=1 l =j Q l (x)</formula><p>2) The normalization step:</p><formula xml:id="formula_9">f j (x) = fj (x) R ∞ -∞ fj (x)dx</formula><p>This basic iteration is repeated for the desired number of iterations.</p><p>• Final decision: After finishing the iterations, we want to estimate the integer information vector b. First, we estimate the final PDF's of the codeword elements x k , k = 1, 2, ...n, by calculating the variable node messages at the last iteration without omitting any check node message in the product step:</p><formula xml:id="formula_10">f (k) f inal (x) = e -(y k -x) 2</formula><p>2σ 2 e l=1 Q l (x). Then, we estimate each x k by finding the peak of its PDF:</p><formula xml:id="formula_11">xk = arg max x f (k) f inal (x). Finally, we estimate b as b = H x .</formula><p>The operation of the iterative algorithm can be intuitively explained as follows. The check node operation is equivalent to calculating the PDF of x mj from the PDF's of x mi , i = 1, 2, ..., j -1, j + 1, ...r, given that r l=1 h l x m l = integer, and assuming that x mi are independent. Extracting x mj from the check equation, we get x mj = 1</p><p>hj (integer-</p><formula xml:id="formula_12">r l=1 l =j h l x m l ).</formula><p>Since the PDF of a sum of independent random variables is the convolution of the corresponding PDF's, equation <ref type="bibr" target="#b1">(2)</ref> and the stretching step that follows it simply calculate the PDF of x mj , assuming that the integer at the right hand side of the check equation is zero. The result is then periodically extended such that a properly shifted copy exists for every possible value of this (unknown) integer. The variable node gets such a message from all the check equations that involve the corresponding variable. The check node messages and the channel PDF are treated as independent sources of information on the variable, so they are multiplied all together.</p><p>Note that the periodic extension step at the check nodes is equivalent to a convolution with an infinite impulse train. With this observation, the operation of the variable nodes is completely analogous to that of the check nodes: the variable nodes multiply the incoming messages by the channel PDF, where the check nodes convolve the incoming messages with an impulse train, which can be regarded as a generalized "integer PDF".</p><p>In the above formulation, the integer information vector b is recovered from the PDF's of the codeword elements x k . An alternative approach is to calculate the PDF of each integer element b m directly as the PDF of the left hand side of the appropriate check equation. Using the tree assumption, this can be done by simply calculating the convolution p(x) as in <ref type="bibr" target="#b1">(2)</ref>, but this time without omitting any PDF, i.e. all the received variable node messages are convolved. Then, the integer b m is determined by bm = arg max j∈Z p(j). Figure <ref type="figure">3</ref> shows an example for a regular LDLC with degree d = 5. The figure shows all the signals that are involved in generating a variable node message for a certain variable node. The top signal is the channel Gaussian, centered around the noisy observation of the variable. The next 4 signals are the periodically extended PDF's that arrived from the check nodes, and the bottom signal is the product of all the 5 signals. It can be seen that each periodic signal has a different period, according to the relevant coefficient of H. Also, the signals with larger period have larger variance. This diversity resolves all the ambiguities such that the multiplication result (bottom plot) remains with a single peak. We expect the iterative algorithm to converge to a solution where a single peak will remain at each PDF, located at the desired value and narrow enough to estimate the information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONVERGENCE A. The Gaussian Mixture Model</head><p>Interestingly, for LDLC we can come up with a convergence analysis that in many respects is more specific than the similar analysis for LDPC.</p><p>We start by introducing basic claims about Gaussian PDF's.</p><formula xml:id="formula_13">Denote G m,V (x) = 1 √ 2πV e -(x-m) 2 2V .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 1 (convolution of Gaussians):</head><p>The convolution of n Gaussians with mean values m 1 , m 2 , ..., m n and variances</p><formula xml:id="formula_14">V 1 , V 2 , ..., V n , respectively, is a Gaussian with mean m 1 + m 2 + ... + m n and variance V 1 + V 2 + ... + V n . Proof: See [21]. Claim 2 (product of n Gaussians): Let G m1,V1<label>(x)</label></formula><p>, G m2,V2 (x),...,G mn,Vn (x) be n Gaussians with mean values m 1 , m 2 , ..., m n and variances V 1 , V 2 , ..., V n respectively. Then, the product of these Gaussians is a scaled Gaussian:</p><formula xml:id="formula_15">n i=1 G mi,Vi (x) = Â • G m, V (x), where 1 V = n i=1 1 Vi , m = P n i=1 miV -1 i P n i=1 V -1 i</formula><p>, and</p><formula xml:id="formula_16">Â = 1 √ (2π) n-1 V -1 Q n k=1 V k • e -V 2 P n i=1 P n j=i+1 (m i -m j ) 2 V i •V j</formula><p>.</p><p>Proof: By straightforward mathematical manipulations.</p><p>The reason that we are interested in the properties of Gaussian PDF's lies in the following lemma.</p><p>Lemma 1: Each message that is exchanged between the check nodes and variable nodes in the LDLC decoding algorithm (i.e. Q j (x) and f j (x)), at every iteration, can be expressed as a Gaussian mixture of the form M (x) = ∞ j=1 A j G mj ,Vj (x).</p><p>Proof: By induction: The initial messages are Gaussians, and the basic operations of the iterative decoder preserve the Gaussian mixture nature of Gaussian mixture inputs (convolution and multiplication preserve the Gaussian nature according to claims 1 and 2, stretching, expanding and shifting preserve it by the definition of a Gaussian, and periodic extension transforms a single Gaussian to a mixture and a mixture to a mixture). Convergence analysis should therefore analyze the convergence of the variances, mean values and amplitudes of the Gaussians in each mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence of the Variances</head><p>We shall now analyze the behavior of the variances, and start with the following lemma.</p><p>Lemma 2: For both variable node messages and check node messages, all the Gaussians that take place in the same mixture have the same variance.</p><p>Proof: By induction. The initial variable node messages are single element mixtures so the claim obviously holds. Assume now that all the variable node messages at iteration t are mixtures where all the Gaussians that take place in the same mixture have the same variance. In the convolution step (2), each variable node message is first expanded. All Gaussians in the expanded mixture will still have the same variance, since the whole mixture is expanded together. Then, d -1 expanded Gaussian mixtures are convolved. In the resulting mixture, each Gaussian will be the result of convolving d -1 single Gaussians, one from each mixture. According to claim 1, all the Gaussians in the convolution result will have the same variance, which will equal the sum of the d-1 variances of the expanded messages. The stretching and periodic extension (3) do not change the equal variance property, so it holds for the final check node messages. The variable nodes multiply d -1 check node messages. Each Gaussian in the resulting mixture is a product of d -1 single Gaussians, one from each mixture, and the channel noise Gaussian. According to claim 2, they will all have the same variance. The final normalization step does not change the variances so the equal variance property is kept for the final variable node messages at iteration t + 1.</p><p>Until this point we did not impose any restrictions on the LDLC. From now on, we shall restrict ourselves to magic square regular LDLC (see Definition 3). The basic iterative equations that relate the variances at iteration t + 1 to the variances at iteration t are summarized in the following two lemmas.</p><p>Lemma 3: For magic square LDLC, variable node messages that are sent at the same iteration along edges with the same absolute value have the same variance.</p><p>Proof: See Appendix IV. Lemma 4: For magic square LDLC with degree d, denote the variance of the messages that are sent at iteration t along edges with weight ±h l by</p><formula xml:id="formula_17">V (t) l . The variance values V (t) 1 , V (t) 2 , ..., V (t) d</formula><p>obey the following recursion:</p><formula xml:id="formula_18">1 V (t+1) i = 1 σ 2 + d m=1 m =i h 2 m d j=1 j =m h 2 j V (t) j<label>(4)</label></formula><p>for i = 1, 2, ...d, with initial conditions</p><formula xml:id="formula_19">V (0) 1 = V (0) 2 = ... = V (0) d = σ 2 . Proof: See Appendix IV.</formula><p>For illustration, the recursion for the case d = 3 is:</p><formula xml:id="formula_20">1 V (t+1) 1 = h 2 2 h 2 1 V (t) 1 + h 2 3 V (t) 3 + h 2 3 h 2 1 V (t) 1 + h 2 2 V (t) 2 + 1 σ 2 (5) 1 V (t+1) 2 = h 2 1 h 2 2 V (t) 2 + h 2 3 V (t) 3 + h 2 3 h 2 1 V (t) 1 + h 2 2 V (t) 2 + 1 σ 2 1 V (t+1) 3 = h 2 1 h 2 2 V (t) 2 + h 2 3 V (t) 3 + h 2 2 h 2 1 V (t) 1 + h 2 3 V (t) 3 + 1 σ 2</formula><p>The lemmas above are used to prove the following theorem regarding the convergence of the variances.</p><p>Theorem 1: For a magic square LDLC with degree d and generating sequence</p><formula xml:id="formula_21">h 1 ≥ h 2 ≥ ... ≥ h d &gt; 0, define α ∆ = P d i=2 h 2 i h<label>2 1</label></formula><p>. Assume that α &lt; 1. Then:</p><p>1) The first variance approaches a constant value of σ 2 (1α), where σ 2 is the channel noise variance:</p><formula xml:id="formula_22">V (∞) 1 ∆ = lim t→∞ V (t) 1 = σ 2 (1 -α).</formula><p>2) The other variances approach zero:</p><formula xml:id="formula_23">V (∞) i ∆ = lim t→∞ V (t) i = 0 for i = 2, 3..d.</formula><p>3) The asymptotic convergence rate of all variances is exponential: <ref type="bibr" target="#b3">4)</ref> The zero approaching variances are upper bounded by the decaying exponential σ 2 α t :</p><formula xml:id="formula_24">0 &lt; lim t→∞ V (t) i -V (∞) i α t &lt; ∞ for i = 1, 2..d.</formula><formula xml:id="formula_25">V (t) i ≤ σ 2 α t</formula><p>for i = 2, 3..d and t ≥ 0. Proof: See Appendix IV. If α ≥ 1, the variances may still converge, but convergence rate may be as slow as o(1/t), as illustrated in Appendix IV.</p><p>Convergence of the variances to zero implies that the Gaussians approach impulses. This is a desired property of the decoder, since the exact PDF that we want to calculate is indeed a weighted sum of impulses (see <ref type="bibr" target="#b0">(1)</ref>). It can be seen that by designing a code with α &lt; 1, i.e. h 2 1 &gt; d i=2 h 2 i , one variance approaches a constant (and not zero). However, all the other variances approach zero, where all variances converge in an exponential rate. This will be the preferred mode because the information can be recovered even if a single variance does not decay to zero, where exponential convergence is certainly preferred over slow 1/t convergence. Therefore, from now on we shall restrict our analysis to magic square LDLC with α &lt; 1.</p><p>Theorem 1 shows that every iteration, each variable node will generate d-1 messages with variances that approach zero, and a single message with variance that approaches a constant. The message with nonzero variance will be transmitted along the edge with largest weight (i.e. h 1 ). However, from the derivation of Appendix IV it can be seen that the opposite happens for the check nodes: each check node will generate d -1 messages with variances that approach a constant, and a single message with variance that approaches zero. The check node message with zero approaching variance will be transmitted along the edge with largest weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence of the Mean Values</head><p>The reason that the messages are mixtures and not single Gaussians lies in the periodic extension step (3) at the check nodes, and every Gaussian at the output of this step can be related to a single index of the infinite sum. Therefore, we can label each Gaussian at iteration t with a list of all the indices that were used in (3) during its creation process in iterations 1, 2, ...t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4 (label of a Gaussian):</head><p>The label of a Gaussian consists of a sequence of triplets of the form {t, c, i}, where t is an iteration index, c is a check node index and i is an integer. The labels are initialized to the empty sequence. Then, the labels are updated along each iteration according to the following update rules:</p><p>1) In the periodic extension step (3), each Gaussian in the output periodic mixture is assigned the label of the specific Gaussian of p j (x) that generated it, concatenated with a single triplet {t, c, i}, where t is the current iteration index, c is the check node index and i is the index in the infinite sum of (3) that corresponds to this Gaussian. 2) In the convolution step and the product step, each Gaussian in the output mixture is assigned a label that equals the concatenation of all the labels of the specific Gaussians in the input messages that formed this Gaussian.</p><p>3) The stretching and normalization steps do not alter the label of each Gaussian: Each Gaussian in the stretched/normalized mixture inherits the label of the appropriate Gaussian in the original mixture. Definition 5 (a consistent Gaussian): A Gaussian in a mixture is called "[t a , t b ] consistent" if its label contains no contradictions for iterations t a to t b , i.e. for every pair of triplets {t</p><formula xml:id="formula_26">1 , c 1 , i 1 }, {t 2 , c 2 , i 2 } such that t a ≤ t 1 , t 2 ≤ t b , if c 1 = c 2 then i 1 = i 2 . A [0, ∞] consistent Gaussian will be simply called a consistent Gaussian.</formula><p>We can relate every consistent Gaussian to a unique integer vector b ∈ Z n , which holds the n integers used in the n check nodes. Since in the periodic extension step (3) the sum is taken over all integers, a consistent Gaussian exists in each variable node message for every possible integer valued vector b ∈ Z n . We shall see later that this consistent Gaussian corresponds to the lattice point Gb.</p><p>According to Theorem 1, if we choose the nonzero values of H such that α &lt; 1, every variable node generates d -1 messages with variances approaching zero and a single message with variance that approaches a constant. We shall refer to these messages as "narrow" messages and "wide" messages, respectively. For a given integer valued vector b, we shall concentrate on the consistent Gaussians that relate to b in all the nd variable node messages that are generated in each iteration (a single Gaussian in each message). The following lemmas summarize the asymptotic behavior of the mean values of these consistent Gaussians for the narrow messages.</p><p>Lemma 5: For a magic square LDLC with degree d and α &lt; 1, consider the d -1 narrow messages that are sent from a specific variable node. Consider further a single Gaussian in each message, which is the consistent Gaussian that relates to a given integer vector b. Asymptotically, the mean values of these d -1 Gaussians become equal.</p><p>Proof: See Appendix V. Lemma 6: For a magic square LDLC with dimension n, degree d and α &lt; 1, consider only consistent Gaussians that relate to a given integer vector b and belong to narrow messages. Denote the common mean value of the d -1 such Gaussians that are sent from variable node i at iteration t by m (t) i , and arrange all these mean values in a column vector m (t) of dimension n. Define the error vector e (t) ∆ = m (t) -x, where x = Gb is the lattice point that corresponds to b. Then, for large t, e (t) satisfies:</p><formula xml:id="formula_27">e (t+1) ≈ -H • e (t)<label>(6)</label></formula><p>where H is derived from H by permuting the rows such that the ±h 1 elements will be placed on the diagonal, dividing each row by the appropriate diagonal element (h 1 or -h 1 ), and then nullifying the diagonal. Proof: See Appendix V. We can now state the following theorem, which describes the conditions for convergence and the steady state value of the mean values of the consistent Gaussians of the narrow variable node messages.</p><p>Theorem 2: For a magic square LDLC with α &lt; 1, the mean values of the consistent Gaussians of the narrow variable node messages that relate to a given integer vector b are assured to converge if and only if all the eigenvalues of H have magnitude less than 1, where H is defined in Lemma 6. When this condition is fulfilled, the mean values converge to the coordinates of the appropriate lattice point:</p><formula xml:id="formula_28">m (∞) = G • b.</formula><p>Proof: Immediate from Lemma 6. Note that without adding random signs to the LDLC nonzero values, the all-ones vector will be an eigenvector of H with eigenvalue</p><formula xml:id="formula_29">P d i=2 hi h1</formula><p>, which may exceed 1. Interestingly, recursion ( <ref type="formula" target="#formula_27">6</ref>) is also obeyed by the error of the Jacobi method for solving systems of sparse linear equations <ref type="bibr" target="#b21">[22]</ref> (see also Section VIII-A), when it is used to solve Hm = b (with solution m = Gb). Therefore, the LDLC decoder can be viewed as a superposition of Jacobi solvers, one for each possible value of the integer valued vector b.</p><p>We shall now turn to the convergence of the mean values of the wide messages. The asymptotic behavior is summarized in the following lemma.</p><p>Lemma 7: For a magic square LDLC with dimension n and α &lt; 1, consider only consistent Gaussians that relate to a given integer vector b and belong to wide messages. Denote the mean value of such a Gaussian that is sent from variable node i at iteration t by m (t) i , and arrange all these mean values in a column vector m (t) of dimension n. Define the error vector e (t) ∆ = m (t) -Gb. Then, for large t, e (t) satisfies:</p><formula xml:id="formula_30">e (t+1) ≈ -F • e (t) + (1 -α)(y -Gb) (<label>7</label></formula><formula xml:id="formula_31">)</formula><p>where y is the noisy codeword and F is an n × n matrix defined by:</p><formula xml:id="formula_32">F k,l =    H r,k H r,l</formula><p>if k = l and there exist a row r of H for which |H r,l | = h 1 and H r,k = 0 0 otherwise (8) Proof: See Appendix V, where an alternative way to construct F from H is also presented.</p><p>The conditions for convergence and steady state solution for the wide messages are described in the following theorem.</p><p>Theorem 3: For a magic square LDLC with α &lt; 1, the mean values of the consistent Gaussians of the wide variable node messages that relate to a given integer vector b are assured to converge if and only if all the eigenvalues of F have magnitude less than 1, where F is defined in Lemma 7. When this condition is fulfilled, the steady state solution is</p><formula xml:id="formula_33">m (∞) = G • b + (1 -α)(I + F ) -1 (y -G • b).</formula><p>Proof: Immediate from Lemma 7. Unlike the narrow messages, the mean values of the wide messages do not converge to the appropriate lattice point coordinates. The steady state error depends on the difference between the noisy observation and the lattice point, as well as on α, and it decreases to zero as α → 1. Note that the final PDF of a variable is generated by multiplying all the d check node messages that arrive to the appropriate variable node. d -1 of these messages are wide, and therefore their mean values have a steady state error. One message is narrow, so it converges to an impulse at the lattice point coordinate. Therefore, the final product will be an impulse at the correct location, where the wide messages will only affect the magnitude of this impulse. As long as the mean values errors are not too large (relative to the width of the wide messages), this should not cause an impulse that corresponds to a wrong lattice point to have larger amplitude than the correct one. However, for large noise, these steady state errors may cause the decoder to deviate from the ML solution (As explained in Section IV-D).</p><p>To summarize the results for the mean values, we considered the mean values of all the consistent Gaussians that correspond to a given integer vector b. A single Gaussian of this form exists in each of the nd variable node messages that are generated in each iteration. For each variable node, d -1 messages are narrow (have variance that approaches zero) and a single message is wide (variance approaches a constant). Under certain conditions on H, the mean values of all the narrow messages converge to the appropriate coordinate of the lattice point Gb. Under additional conditions on H, the mean values of the wide messages converge, but the steady state values contain an error term.</p><p>We analyzed the behavior of consistent Gaussian. It should be noted that there are many more non-consistent Gaussians. Furthermore non-consistent Gaussians are generated in each iteration for any existing consistent Gaussian. We conjecture that unless a Gaussian is consistent, or becomes consistent along the iterations, it fades out, at least at noise conditions where the algorithm converges. The reason is that nonconsistency in the integer values leads to mismatch in the corresponding PDF's, and so the amplitude of that Gaussian is attenuated.</p><p>We considered consistent Gaussians which correspond to a specific integer vector b, but such a set of Gaussians exists for every possible choice of b, i.e. for every lattice point. Therefore, the narrow messages will converge to a solution that has an impulse at the appropriate coordinate of every lattice point. This resembles the exact solution (1), so the key for proper convergence lies in the amplitudes: we would like the consistent Gaussians of the ML lattice point to have the largest amplitude for each message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convergence of the Amplitudes</head><p>We shall now analyze the behavior of the amplitudes of consistent Gaussians (as discussed later, this is not enough for complete convergence analysis, but it certainly gives insight to the nature of the convergence process and its properties). The behavior of the amplitudes of consistent Gaussians is summarized in the following lemma.</p><p>Lemma 8: For a magic square LDLC with dimension n, degree d and α &lt; 1, consider the nd consistent Gaussians that relate to a given integer vector b in the variable node messages that are sent at iteration t (one consistent Gaussian per message). Denote the amplitudes of these Gaussians by p (t) i , i = 1, 2, ...nd, and define the log-amplitude as l</p><formula xml:id="formula_34">(t) i = log p (t)</formula><p>i . Arrange these nd log-amplitudes in a column vector l (t) , such that element (k -1)d + i corresponds to the message that is sent from variable node k along an edge with weight ±h i . Assume further that the bipartite graph of the LDLC contains no 4-loops. Then, the log-amplitudes satisfy the following recursion:</p><formula xml:id="formula_35">l (t+1) = A • l (t) -a (t) -c (t)<label>(9)</label></formula><p>with initialization l (0) = 0. A is an nd × nd matrix which is all zeros except exactly (d -1) 2 '1's in each row and each column. The element of the excitation vector a (t) at location (k -1)d + i (where k = 1, 2, ...n and i = 1, 2, ...d) equals:</p><formula xml:id="formula_36">a (t) (k-1)d+i = (10) = V (t) k,i 2     d l=1 l =i d j=l+1 j =i m(t) k,l - m(t) k,j<label>2</label></formula><formula xml:id="formula_37">Ṽ (t) k,l • Ṽ (t) k,j + d l=1 l =i m(t) k,l -y k 2 σ 2 • Ṽ (t) k,l    </formula><p>where m(t) k,l and Ṽ (t) k,l denote the mean value and variance of the consistent Gaussian that relates to the integer vector b in the check node message that arrives to variable node k at iteration t along an edge with weight ±h l . y k is the noisy channel observation of variable node k, and</p><formula xml:id="formula_38">V (t) k,i ∆ = 1 σ 2 + d l=1 l =i 1 Ṽ (t) k,l -1</formula><p>. Finally, c (t) is a constant excitation term that is independent of the integer vector b (i.e. is the same for all consistent Gaussians). Note that an iteration is defined as sending variable node messages, followed by sending check node messages. The first iteration (where the variable nodes send the initialization PDF) is regarded as iteration 0.</p><p>Proof: At the check node, the amplitude of a Gaussian at the convolution output is the product of the amplitudes of the corresponding Gaussians in the appropriate variable node messages. At the variable node, the amplitude of a Gaussian at the product output is the product of the amplitudes of the corresponding Gaussians in the appropriate check node messages, multiplied by the Gaussian scaling term, according to claim 2. Since we assume that the bipartite graph of the LDLC contains no 4-loops, an amplitude of a variable node message at iteration t will therefore equal the product of (d -1) 2 amplitudes of Gaussians of variable node messages from iteration t -1, multiplied by the Gaussian scaling term. This proves <ref type="bibr" target="#b8">(9)</ref> and shows that A has (d-1) 2 '1's in every row. However, since each variable node message affects exactly (d -1) 2 variable node messages of the next iteration, A must also have (d -1) 2 '1's in every column. The total excitation term -a (t) -c (t) corresponds to the logarithm of the Gaussian scaling term. Each element of this scaling term results from the product of d -1 check node Gaussians and the channel Gaussian, according to claim 2. This scaling term sums over all the pairs of Gaussians, and in <ref type="bibr" target="#b9">(10)</ref> the sum is separated to pairs that include the channel Gaussian and pairs that do not. The total excitation is divided between <ref type="bibr" target="#b9">(10)</ref>, which depends on the choice of the integer vector b, and c (t) , which includes all the constant terms that are independent on b (including the normalization operation which is performed at the variable node).</p><p>Since there are exactly (d -1) 2 '1's in each column of the matrix A, it is easy to see that the all-ones vector is an eigenvector of A, with eigenvalue (d -1) 2 . If d &gt; 2, this eigenvalue is larger than 1, meaning that the recursion ( <ref type="formula" target="#formula_35">9</ref>) is non-stable.</p><p>It can be seen that the excitation term a (t) has two components. The first term sums the squared differences between the mean values of all the possible pairs of received check node messages (weighted by the inverse product of the appropriate variances). It therefore measures the mismatch between the incoming messages. This mismatch will be small if the mean values of the consistent Gaussians converge to the coordinates of a lattice point (any lattice point). The second term sums the squared differences between the mean values of the incoming messages and the noisy channel output y k . This term measures the mismatch between the incoming messages and the channel measurement. It will be smallest if the mean values of the consistent Gaussians converge to the coordinates of the ML lattice point.</p><p>The following lemma summarizes some properties of the excitation term a (t) .</p><p>Lemma 9: For a magic square LDLC with dimension n, degree d, α &lt; 1 and no 4-loops, consider the consistent Gaussians that correspond to a given integer vector b. According to Lemma 8, their amplitudes satisfy recursion <ref type="bibr" target="#b8">(9)</ref>. The excitation term a (t) of ( <ref type="formula" target="#formula_35">9</ref>), which is defined by <ref type="bibr" target="#b9">(10)</ref>, satisfies the following properties:</p><formula xml:id="formula_39">1) a (t)</formula><p>i , the i'th element of a (t) , is non-negative, finite and bounded for every i and every t. Moreover, a (t) i converges to a finite non-negative steady state value as</p><formula xml:id="formula_40">t → ∞. 2) lim t→∞ nd i=1 a (t) i = 1 2σ 2 (Gb -y) T W (Gb -y)</formula><p>, where y is the noisy received codeword and W is a positive definite matrix defined by:</p><formula xml:id="formula_41">W ∆ = (d + 1 -α)I -2(1 -α)(I + F ) -1 + (11) +(1 -α)(I + F ) -1 T (d -1) 2 I -F T F (I + F ) -1</formula><p>where F is defined in Lemma 7. 3) For an LDLC with degree d &gt; 2, the weighted infinite sum</p><formula xml:id="formula_42">∞ j=0 P nd i=1 a (j) i (d-1) 2j+2 converges to a finite value. Proof: See Appendix VI.</formula><p>The following theorem addresses the question of which consistent Gaussian will have the maximal asymptotic amplitude. We shall first consider the case of an LDLC with degree d &gt; 2, and then consider the special case of d = 2 in a separate theorem.</p><p>Theorem 4: For a magic square LDLC with dimension n, degree d &gt; 2, α &lt; 1 and no 4-loops, consider the nd consistent Gaussians that relate to a given integer vector b in the variable node messages that are sent at iteration t (one consistent Gaussian per message). Denote the amplitudes of these Gaussians by p (t) i , i = 1, 2, ...nd, and define the product-of-amplitudes as</p><formula xml:id="formula_43">P (t) ∆ = nd i=1 p (t) i . Define further S = ∞ j=0 P nd i=1 a (j) i (d-1) 2j+2</formula><p>, where a (j) i is defined by (10) (S is well defined according to <ref type="bibr">Lemma 9)</ref>. Then:</p><p>1) The integer vector b for which the consistent Gaussians will have the largest asymptotic product-of-amplitudes lim t→∞ P (t) is the one for which S is minimized.</p><p>2) The product-of-amplitudes for the consistent Gaussians that correspond to all other integer vectors will decay to zero in a super-exponential rate. Proof: As in Lemma 8, define the log-amplitudes l</p><formula xml:id="formula_44">(t) i ∆ = log p (t) i . Define further s (t) ∆ = nd i=1 l (t)</formula><p>i . Taking the elementwise sum of (9), we get:</p><formula xml:id="formula_45">s (t+1) = (d -1) 2 s (t) - nd i=1 a (t) i (12)</formula><p>with initialization s (0) = 0. Note that we ignored the term nd i=1 c (t) i . As shown below, we are looking for the vector b that maximizes s (t) . Since ( <ref type="formula">12</ref>) is a linear difference equation, and the term</p><formula xml:id="formula_46">nd i=1 c (t) i is independent of b, its effect on s (t)</formula><p>is common to all b and is therefore not interesting.</p><p>Define now s(t) ∆ = s (t) (d-1) 2t . Substituting in <ref type="bibr" target="#b11">(12)</ref>, we get:</p><formula xml:id="formula_47">s(t+1) = s(t) - 1 (d -1) 2t+2 nd i=1 a (t) i (13)</formula><p>with initialization s(0) = 0, which can be solved to get:</p><formula xml:id="formula_48">s(t) = - t-1 j=0 nd i=1 a (j) i (d -1) 2j+2<label>(14)</label></formula><p>We would now like to compare the amplitudes of consistent Gaussians with various values of the corresponding integer vector b in order to find the lattice point whose consistent Gaussians will have largest product-of-amplitudes. From the definitions of s (t) and s(t) we then have:</p><formula xml:id="formula_49">P (t) = e s (t) = e (d-1) 2t •s (t)<label>(15)</label></formula><p>Consider two integer vectors b that relate to two lattice points. Denote the corresponding product-of-amplitudes by P (t) 0 and P (t) 1 , respectively, and assume that for these two vectors S converges to the values S 0 and S 1 , respectively. Then, taking into account that lim t→∞ s(t) = -S, the asymptotic ratio of the product-of-amplitudes for these lattice points will be:</p><formula xml:id="formula_50">lim t→∞ P (t) 1 P (t) 0 = e -(d-1) 2t •S1 e -(d-1) 2t •S0 = e (d-1) 2t •(S0-S1)<label>(16)</label></formula><p>It can be seen that if S 0 &lt; S 1 , the ratio decreases to zero in a super exponential rate. This shows that the lattice point for which S is minimized will have the largest product-ofamplitudes, where for all other lattice points, the productof-amplitudes will decay to zero in a super-exponential rate (recall that the normalization operation at the variable node keeps the sum of all amplitudes in a message to be 1). This completes the proof of the theorem. We now have to find which integer valued vector b minimizes S. The analysis is difficult because the weighting factor inside the sum of ( <ref type="formula" target="#formula_48">14</ref>) performs exponential weighting of the excitation terms, where the dominant terms are those of the first iterations. Therefore, we can not use the asymptotic results of Lemma 9, but have to analyze the transient behavior. However, the analysis is simpler for the case of an LDLC with row and column degree of d = 2, so we shall first turn to this simple case (note that for this case, both the convolution in the check nodes and the product at the variable nodes involve only a single message).</p><p>Theorem 5: For a magic square LDLC with dimension n, degree d = 2, α &lt; 1 and no 4-loops, consider the 2n consistent Gaussians that relate to a given integer vector b in the variable node messages that are sent at iteration t (one consistent Gaussian per message). Denote the amplitudes of these Gaussians by p (t) i , i = 1, 2, ...2n, and define the productof-amplitudes as</p><formula xml:id="formula_51">P (t) ∆ = 2n i=1 p (t)</formula><p>i . Then: 1) The integer vector b for which the consistent Gaussians will have the largest asymptotic product-of-amplitudes lim t→∞ P (t) is the one for which (Gb-y) T W (Gb-y) is minimized, where W is defined by <ref type="bibr" target="#b10">(11)</ref> and y is the noisy received codeword.</p><p>2) The product-of-amplitudes for the consistent Gaussians that correspond to all other integer vectors will decay to zero in an exponential rate. Proof: For d = 2 (12) becomes:</p><formula xml:id="formula_52">s (t+1) = s (t) - 2n i=1 a (t) i (17)</formula><p>With solution:</p><formula xml:id="formula_53">s (t) = - t-1 j=0 2n i=1 a (j) i (18) Denote S a = lim j→∞ 2n i=1 a (j)</formula><p>i . S a is well defined according to Lemma 9. For large t, we then have s (t) ≈ -t • S a . Therefore, for two lattice points with excitation sum terms which approach S a0 , S a1 , respectively, the ratio of the corresponding product-of-amplitudes will approach</p><formula xml:id="formula_54">lim t→∞ P (t) 1 P (t) 0 = e -Sa1•t e -Sa0•t = e (Sa0-Sa1)•t<label>(19)</label></formula><p>If S a0 &lt; S a1 , the ratio decreases to zero exponentially (unlike the case of d &gt; 2 where the rate was super-exponential, as in ( <ref type="formula" target="#formula_50">16</ref>)). This shows that the lattice point for which S a is minimized will have the largest product-of-amplitudes, where for all other lattice points, the product-of-amplitudes will decay to zero in an exponential rate (recall that the normalization operation at the variable node keeps the sum of all amplitudes in a message to be 1). This completes the proof of the second part of the theorem. We still have to find the vector b that minimizes S a . The basic difference between the case of d = 2 and the case of d &gt; 2 is that for d &gt; 2 we need to analyze the transient behavior of the excitation terms, where for d = 2 we only need to analyze the asymptotic behavior, which is much easier to handle.</p><p>According to Lemma 9, we have:</p><formula xml:id="formula_55">S a ∆ = lim j→∞ 2n i=1 a (j) i = 1 2σ 2 (Gb -y) T W (Gb -y) (<label>20</label></formula><formula xml:id="formula_56">)</formula><p>where W is defined by <ref type="bibr" target="#b10">(11)</ref> and y is the noisy received codeword. Therefore, for d = 2, the lattice points whose consistent Gaussians will have largest product-of-amplitudes is the point for which (Gb -y) T W (Gb -y) is minimized. This completes the proof of the theorem. For d = 2 we could find an explicit expression for the "winning" lattice point. As discussed above, we could not find an explicit expression for d &gt; 2, since the result depends on the transient behavior of the excitation sum term, and not only on the steady state value. However, a reasonable conjecture is to assume that b that maximizes the steady state excitation will also maximize the term that depends on the transient behavior. This means that a reasonable conjecture is to assume that the "winning" lattice point for d &gt; 2 will also minimize an expression of the form <ref type="bibr" target="#b19">(20)</ref>.</p><p>Note that for d &gt; 2 we can still show that for "weak" noise, the ML point will have the minimal S. To see that, it comes out from (10) that for zero noise, the ML lattice point will have a (t) i = 0 for every t and i, where all other lattice points will have a (t) i &gt; 0 for at least some i and t. Therefore, the ML point will have a minimal excitation term along the transient behavior so it will surely have the minimal S and the best product-of-amplitudes. As the noise increases, it is difficult to analyze the transient behavior of a (t) i , as discussed above. Note that the ML solution minimizes (Gb -y) T (Gby), where the above analysis yields minimization of (Gby) T W (Gb -y). Obviously, for zero noise (i.e. y = G • b) both minimizations will give the correct solution with zero score. As the noise increases, the solutions may deviate from one another. Therefore, both minimizations will give the same solution for "weak" noise but may give different solutions for "strong" noise.</p><p>An example for another decoder that performs this form of minimization is the linear detector, which calculates b = H • y (where x denotes the nearest integer to x). This is equivalent to minimizing</p><formula xml:id="formula_57">(Gb -y) T W (Gb -y) with W = H T H = G -1 T G -1 .</formula><p>The linear detector fails to yield the ML solution if the noise is too strong, due to its inherent noise amplification.</p><p>For the LDLC iterative decoder, we would like that the deviation from the ML decoder due to the W matrix would be negligible in the expected range of noise variance. Experimental results (see Section IX) show that the iterative decoder indeed converges to the ML solution for noise variance values that approach channel capacity. However, for quantization or shaping applications (see Section VIII-B), where the effective noise is uniformly distributed along the Voronoi cell of the lattice (and is much stronger than the noise variance at channel capacity) the iterative decoder fails, and this can be explained by the influence of the W matrix on the minimization, as described above. Note from (11) that as α → 1, W approaches a scaled identity matrix, which means that the minimization criterion approaches the ML criterion. However, the variances converge as α t , so as α → 1 convergence time approaches infinity.</p><p>Until this point, we concentrated only on consistent Gaussians, and checked what lattice point maximizes the productof-amplitudes of all the corresponding consistent Gaussians. However, this approach does not necessarily lead to the lattice point that will be finally chosen by the decoder, due to 3 main reasons:</p><p>1) It comes out experimentally that the strongest Gaussian in each message is not necessarily a consistent Gaussian, but a Gaussian that started as non-consistent and became consistent at a certain iteration. Such a Gaussian will finally converge to the appropriate lattice point, since the convergence of the mean values is independent of initial conditions. The non-consistency at the first several iterations, where the mean values are still very noisy, allows these Gaussians to accumulate stronger amplitudes than the consistent Gaussians (recall that the exponential weighting in <ref type="bibr" target="#b13">(14)</ref> for d &gt; 2 results in strong dependency on the behavior at the first iterations). 2) There is an exponential number of Gaussians that start as non-consistent and become consistent (with the same integer vector b) at a certain iteration, and the final amplitude of the Gaussians at the lattice point coordinates will be determined by the sum of all these Gaussians. 3) We ignored non-consistent Gaussians that endlessly remain non-consistent. We have not shown it analytically, but it is reasonable to assume that the excitation terms for such Gaussians will be weaker than for Gaussians that become consistent at some point, so their amplitude will fade away to zero. However, non-consistent Gaussians are born every iteration, even at steady state. The "newly-born" non-consistent Gaussians may appear as sidelobes to the main impulse, since it may take several iterations until they are attenuated. Proper choice of the coefficients of H may minimize this effect, as discussed in Sections III-A and V-A. However, these Gaussians may be a problem for small d (e.g. d = 2) where the product step at the variable node does not include enough messages to suppress them.</p><p>Note that the first two issues are not a problem for d = 2, where the winning lattice point depends only on the asymptotic behavior. The amplitude of a sum of Gaussians that converged to the same coordinates will still be governed by <ref type="bibr" target="#b17">(18)</ref> and the winning lattice point will still minimize <ref type="bibr" target="#b19">(20)</ref>. The third issue is a problem for small d, but less problematic for large d, as described above.</p><p>As a result, we can not regard the convergence analysis of the consistent Gaussians' amplitudes as a complete convergence analysis. However, it can certainly be used as a qualitative analysis that gives certain insights to the convergence process. Two main observations are:</p><p>1) The narrow variable node messages tend to converge to single impulses at the coordinates of a single lattice point. This results from ( <ref type="formula" target="#formula_50">16</ref>), <ref type="bibr" target="#b18">(19)</ref>, which show that the "non-winning" consistent Gaussians will have amplitudes that decrease to zero relative to the amplitude of the "winning" consistent Gaussian. This result remains valid for the sum of non-consistent Gaussians that became consistent at a certain point, because it results from the non-stable nature of the recursion <ref type="bibr" target="#b8">(9)</ref>, which makes strong Gaussians stronger in an exponential manner.</p><p>The single impulse might be accompanied by weak "sidelobes" due to newly-born non-consistent Gaussians.</p><p>Interestingly, this form of solution is different from the exact solution <ref type="bibr" target="#b0">(1)</ref>, where every lattice point is represented by an impulse at the appropriate coordinate, with amplitude that depends on the Euclidean distance of the lattice point from the observation. The iterative decoder's solution has a single impulse that corresponds to a single lattice point, where all other impulses have amplitudes that decay to zero. This should not be a problem, as long as the ML point is the remaining point (see discussion above). 2) We have shown that for d = 2 the strongest consistent Gaussians relate to b that minimizes an expression of the form (Gb -y) T W (Gb -y). We proposed a conjecture that this is also true for d &gt; 2. We can further widen the conjecture to say that the finally decoded b (and not only the b that relates to strongest consistent Gaussians) minimizes such an expression. Such a conjecture can explain why the iterative decoder works well for decoding near channel capacity, but fails for quantization or shaping, where the effective noise variance is much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Summary of Convergence Results</head><p>To summarize the convergence analysis, it was first shown that the variable node messages are Gaussian mixtures. Therefore, it is sufficient to analyze the sequences of variances, mean values and relative amplitudes of the Gaussians in each mixture. Starting with the variances, it was shown that with proper choice of the magic square LDLC generating sequence, each variable node generates d -1 "narrow" messages, whose variance decreases exponentially to zero, and a single "wide" message, whose variance reaches a finite value. Consistent Gaussians were then defined as Gaussians that their generation process always involved the same integer at the same check node. Consistent Gaussians can then be related to an integer vector b or equivalently to the lattice point Gb. It was then shown that under appropriate conditions on H, the mean values of consistent Gaussians that belong to narrow messages converge to the coordinates of the appropriate lattice point. The mean values of wide messages also converge to these coordinates, but with a steady state error. Then, the amplitudes of consistent Gaussians were analyzed. For d = 2 it was shown that the consistent Gaussians with maximal productof-amplitudes (over all messages) are those that correspond to an integer vector b than minimizes (Gb -y) T W (Gb -y), where W is a positive definite matrix that depends only on H. The product-of-amplitudes for all other consistent Gaussians decays to zero. For d &gt; 2 the analysis is complex and depends on the transient behavior of the mean values and variances (and not only on their steady state values), but a reasonable conjecture is to assume that a same form of criterion is also minimized for d &gt; 2. The result is different from the ML lattice point, which minimizes</p><formula xml:id="formula_58">G • b -y 2 ,</formula><p>where both criteria give the same point for weak noise but may give different solutions for strong noise. This may explain the experiments where the iterative decoder is successful in decoding the ML point for the AWGN channel near channel capacity, but fails in quantization or shaping applications where the effective noise is much stronger. These results also show that the iterative decoder converges to impulses at the coordinates of a single lattice point. It was then explained that analyzing the amplitudes of consistent Gaussians is not sufficient, so these results can not be regarded as a complete convergence analysis. However, the analysis gave a set of necessary conditions on H, and also led to useful insights to the convergence process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CODE DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Choosing the Generating Sequence</head><p>We shall concentrate on magic square LDLC, since they have inherent diversity of the nonzero elements in each row and column, which was shown above to be beneficial. It still remains to choose the LDLC generating sequence h 1 , h 2 , ...h d . Assume that the algorithm converged, and each PDF has a peak at the desired value. When the periodic functions are multiplied at a variable node, the correct peaks will then align. We would like that all the other peaks will be strongly attenuated, i.e. there will be no other point where the peaks align. This resembles the definition of the least common multiple (LCM) of integers: if the periods were integers, we would like to have their LCM as large as possible. This argument suggests the sequence {1/2, 1/3, 1/5, 1/7, 1/11, 1/13, 1/17, ...}, i.e. the reciprocals of the smallest d prime numbers. Since the periods are 1/h 1 , 1/h 2 , ...1/h d , we will get the desired property. Simulations have shown that increasing d beyond 7 with this choice gave negligible improvement. Also, performance was improved by adding some "dither" to the sequence, resulting in {1/2.31, 1/3.17, 1/5.11, 1/7.33, 1/11.71, 1/13.11, 1/17.55}. For d &lt; 7, the first d elements are used.</p><p>An alternative approach is a sequence of the form {1, , , ..., }, where &lt;&lt; 1. For this case, every variable node will receive a single message with period 1 and d -1 messages with period 1/ . For small , the period of these d -1 messages will be large and multiplication by the channel Gaussian will attenuate all the unwanted replicas. The single remaining replica will attenuate all the unwanted replicas of the message with period 1. A convenient choice is = 1 √ d , which ensures that α = d-1 d &lt; 1, as required by Theorem 1. As an example, for d = 7 the sequence will be {1,</p><formula xml:id="formula_59">1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Necessary Conditions on H</head><p>The magic square LDLC definition and convergence analysis imply four necessary conditions on H: </p><formula xml:id="formula_60">1) |det(H)| = 1.</formula><formula xml:id="formula_61">= P d i=2 h 2 i h 2 1</formula><p>. This guarantees exponential convergence rate for the variances (Theorem 1). Choosing a smaller α results in faster convergence, but we should not take α too small since the steady state variance of the wide variable node messages, as well as the steady state error of the mean values of these messages, increases when α decreases, as discussed in Section IV-C. This may result in deviation of the decoded codeword from the ML codeword, as discussed in Section IV-D. For the first LDLC generating sequence of the previous subsection, we have α = 0.92 and 0.87 for d = 7 and 5, respectively, which is a reasonable trade off. For the second sequence type we have α = d-1 d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) All the eigenvalues of H must have magnitude less than</head><p>1, where H is defined in Theorem 2. This is a necessary condition for convergence of the mean values of the narrow messages. Note that adding random signs to the nonzero H elements is essential to fulfill this necessary condition, as explained in Section IV-C. 4) All the eigenvalues of F must have magnitude less than 1, where F is defined in Theorem 3. This is a necessary condition for convergence of the mean values of the wide messages. Interestingly, it comes out experimentally that for large codeword length n and relatively small degree d (e.g. n ≥ 1000 and d ≤ 10), a magic square LDLC with generating sequence that satisfies h 1 = 1 and α &lt; 1 results in H that satisfies all these four conditions: H is nonsingular without any need to omit rows and columns, n |det(H)| ≈ 1 without any need for normalization, and all eigenvalues of H and F have magnitude less than 1 (typically, the largest eigenvalue of H or F has magnitude of 0.94 -0.97, almost independently of n and the choice of nonzero H locations). Therefore, by simply dividing the first generating sequence of the previous subsection by its first element, the constructed H meets all the necessary conditions, where the second type of sequence meets the conditions without any need for modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Construction of H for Magic Square LDLC</head><p>We shall now present a simple algorithm for constructing a parity check matrix for a magic square LDLC. If we look at the bipartite graph, each variable node and each check node has d edges connected to it, one with every possible weight h 1 , h 2 , ...h d . All the edges that have the same weight h j form a permutation from the variable nodes to the check nodes (or vice versa). The proposed algorithm generates d random permutations and then searches sequentially and cyclically for 2-loops (two parallel edges from a variable node to a check node) and 4-loops (two variable nodes that both are connected to a pair of check nodes). When such a loop is found, a pair is swapped in one of the permutations such that the loop is removed. A detailed pseudo-code for this algorithm is given in Appendix VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DECODER IMPLEMENTATION</head><p>Each PDF should be approximated with a discrete vector with resolution ∆ and finite range. According to the Gaussian Q-function, choosing a range of, say, 6σ to both sides of the noisy channel observation will ensure that the error probability due to PDF truncation will be ≈ 10 -9 . Near capacity, σ 2 ≈ 1 2πe , so 12σ ≈ 3. Simulation showed that resolution errors became negligible for ∆ = 1/64. Each PDF was then stored in a L = 256 elements vector, corresponding to a range of size 4.</p><p>At the check node, the PDF f j (x) that arrives from variable node j is first expanded by h j (the appropriate coefficient of H) to get f j (x/h j ). In a discrete implementation with resolution ∆ the PDF is a vector of values f j (k∆), k ∈ Z. As described in Section V, we shall usually use h j ≤ 1 so the expanded PDF will be shorter than the original PDF. If the expand factor 1/|h j | was an integer, we could simply decimate f j (k∆) by 1/|h j |. However, in general it is not an integer so we should use some kind of interpolation. The PDF f j (x) is certainly not band limited, and as the iterations go on it approaches an impulse, so simple interpolation methods (e.g. linear) are not suitable. Suppose that we need to calculate f j ((k + )∆), where -0.5 ≤ ≤ 0.5. A simple interpolation method which showed to be effective is to average f j (x) around the desired point, where the averaging window length l w is chosen to ensure that every sample of f j (x) is used in the interpolation of at least one output point. This ensures that an impulse can not be missed. The interpolation result is then</p><formula xml:id="formula_62">1 2lw+1 lw i=-lw f j ((k -i)∆), where l w = 1/|hj | 2</formula><p>. The most computationally extensive step at the check nodes is the calculation the convolution of d -1 expanded PDF's. An efficient method is to calculate the fast Fourier transforms (FFTs) of all the PDF's, multiply the results and then perform inverse FFT (IFFT). The resolution of the FFT should be larger than the expected convolution length, which is roughly</p><formula xml:id="formula_63">L out ≈ L• d i=1 h i ,</formula><p>where L denotes the original PDF length. Appendix VIII shows a way to use FFTs of size 1/∆, where ∆ is the resolution of the PDF. Usually 1/∆ &lt;&lt; L out so FFT complexity is significantly reduced. Practical values are L = 256 and ∆ = 1/64, which give an improvement factor of at least 4 in complexity.</p><p>Each variable node receives d check node messages. The output variable node message is calculated by generating the product of d -1 input messages and the channel Gaussian. As the iterations go on, the messages get narrow and may become impulses, with only a single nonzero sample. Quantization effects may cause impulses in two messages to be shifted by one sample. This will result in a zero output (instead of an impulse). Therefore, it was found useful to widen each check node message Q(k) prior to multiplication, such that</p><formula xml:id="formula_64">Q w (k) = 1 i=-1 Q(k + i), i.</formula><p>e. the message is added to its right shifted and left shifted (by one sample) versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. COMPUTATIONAL COMPLEXITY AND STORAGE REQUIREMENTS</head><p>Most of the computational effort is invested in the d FFT's and d IFFT's (of length 1/∆) that each check node performs each iteration. The total number of multiplications for t iterations is o n</p><formula xml:id="formula_65">• d • t • 1 ∆ • log 2 ( 1 ∆ ) .</formula><p>As in binary LDPC codes, the computational complexity has the attractive property of being linear with block length. However, the constant that precedes the linear term is significantly higher, mainly due to the FFT operations.</p><p>The memory requirements are governed by the storage of the nd check node and variable node messages, with total memory of o(n • d • L). Compared to binary LDPC, the factor of L significantly increases the required memory. For example, for n = 10, 000, d = 7 and L = 256, the number of storage elements is of the order of 10 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ENCODING AND SHAPING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoding</head><p>The LDLC encoder has to calculate x = G•b, where b is an integer message vector. Note that unlike H, G = H -1 is not sparse, in general, so the calculation requires computational complexity and storage of o(n 2 ). This is not a desirable property because the decoder's computational complexity is only o(n). A possible solution is to use the Jacobi method <ref type="bibr" target="#b21">[22]</ref> to solve H • x = b, which is a system of sparse linear equations. Using this method, a magic square LDLC encoder calculates several iterations of the form:</p><formula xml:id="formula_66">x (t) = b -H • x (t-1)<label>(21)</label></formula><p>with initialization x (0) = 0. The matrix H is defined in Lemma 6 of Section IV-C. The vector b is a permuted and scaled version of the integer vector b, such that the i'th element of b equals the element of b for which the appropriate row of H has its largest magnitude value at the i'th location. This element is further divided by this largest magnitude element.</p><p>A necessary and sufficient condition for convergence to x = G • b is that all the eigenvalues of H have magnitude less than 1 <ref type="bibr" target="#b21">[22]</ref>. However, it was shown that this is also a necessary condition for convergence of the LDLC iterative decoder (see Sections IV-C, V-B), so it is guaranteed to be fulfilled for a properly designed magic square LDLC. Since H is sparse, this is an o(n) algorithm, both in complexity and storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shaping</head><p>For practical use with the power constrained AWGN channel, the encoding operation must be accompanied by shaping, in order to prevent the transmitted codeword's power from being too large. Therefore, instead of mapping the information vector b to the lattice point x = G • b, it should be mapped to some other lattice point x = G • b , such that the lattice points that are used as codewords belong to a shaping region (e.g. an n-dimensional sphere). The shaping operation is the mapping of the integer vector b to the integer vector b .</p><p>As explained in Section II-A, this work concentrates on the lattice design and the lattice decoding algorithm, and not on the shaping region or shaping algorithms. Therefore, this section will only highlight some basic shaping principles and ideas.</p><p>A natural shaping scheme for lattice codes is nested lattice coding <ref type="bibr" target="#b11">[12]</ref>. In this scheme, shaping is done by quantizing the lattice point G • b onto a coarse lattice G , where the transmitted codeword is the quantization error, which is uniformly distributed along the Voronoi cell of the coarse lattice. If the second moment of this Voronoi cell is close to that of an n-dimensional sphere, the scheme will attain close-tooptimal shaping gain. Specifically, assume that the information vector b assumes integer values in the range 0, 1, ...M -1 for some constant integer M . Then, we can choose the coarse lattice to be G = M G. The volume of the Voronoi cell for this lattice is M n , since we assume det(G) = 1 (see Section II-A). If the shape of the Voronoi cell resembles an ndimensional sphere (as expected from a capacity approaching lattice code), it will attain optimal shaping gain (compared to uncoded transmission of the original integer sequence b).</p><p>The shaping operation will find the coarse lattice point M Gk, k ∈ Z n , which is closest to the fine lattice point x = G • b. The transmitted codeword will be:</p><formula xml:id="formula_67">x = x -M Gk = G(b -M k) = Gb</formula><p>where b ∆ = b -M k (note that the "inverse shaping" at the decoder, i.e. transforming from b to b, is a simple modulo calculation: b = b mod M ). Finding the closest coarse lattice point M Gk to x is equivalent to finding the closest fine lattice point G • k to the vector x/M . This is exactly the operation of the iterative LDLC decoder, so we could expect that is could be used for shaping. However, simulations show that the iterative decoding finds a vector k with poor shaping gain. The reason is that for shaping, the effective noise is much stronger than for decoding, and the iterative decoder fails to find the nearest lattice point if the noise is too large (see Section IV-D).</p><p>Therefore, an alternative algorithm has to be used for finding the nearest coarse lattice point. The complexity of finding the nearest lattice point grows exponentially with the lattice dimension n and is not feasible for large dimensions <ref type="bibr" target="#b22">[23]</ref>. However, unlike decoding, for shaping applications it is not critical to find the exact nearest lattice point, and approximate algorithms may be considered (see <ref type="bibr" target="#b14">[15]</ref>). A possible method <ref type="bibr" target="#b23">[24]</ref> is to perform QR decomposition on G in order to transform to a lattice with upper triangular generator matrix, and then use sequential decoding algorithms (such as the Fano algorithm) to search the resulting tree. The main disadvantage of this approach is computational complexity and storage of at least o(n 2 ). Finding an efficient shaping scheme for LDLC is certainly a topic for further research.  <ref type="figure">B</ref>). The H matrices were generated using the algorithm of Section V-C. PDF resolution was set to ∆ = 1/256 with a total range of 4, i.e. each PDF was represented by a vector of L = 1024 elements. High resolution was used since our main target is to prove the LDLC concept and eliminate degradation due to implementation considerations. For this reason, the decoder was used with 200 iterations (though most of the time, a much smaller number was sufficient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. SIMULATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magic</head><p>In all simulations the all-zero codeword was used. Approaching channel capacity is equivalent to σ 2 → 1 2πe (see Section II-A), so performance is measured in symbol error rate (SER), vs. the distance of the noise variance σ 2 from capacity (in dB). The results are shown in Figure <ref type="figure" target="#fig_1">4</ref>. At SER of 10 -5 , for n = 100000, 10000, 1000, 100 we can work as close as 0.6dB, 0.8dB, 1.5dB and 3.7dB from capacity, respectively.</p><p>Similar results were obtained for d = 7 with the second type of generating sequence of Section V-A, i.e. {1,</p><formula xml:id="formula_68">1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 , 1 √ 7 }.</formula><p>Results were slightly worse than for the first generating sequence (by less than 0.1 dB). Increasing d did not give any visible improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION</head><p>Low density lattice codes (LDLC) were introduced. LDLC are novel lattice codes that can approach capacity and be decoded efficiently. Good error performance within ∼ 0.5dB from capacity at block length of 100,000 symbols was demonstrated. Convergence analysis was presented for the iterative decoder, which is not complete, but yields necessary conditions on H and significant insight to the convergence process. Code parameters were chosen from intuitive arguments, so it is reasonable to assume that when the code structure will be more understood, better parameters could be found, and channel capacity could be approached even closer.</p><p>Multi-input, multi-output (MIMO) communication systems have become popular in recent years. Lattice codes have been proposed in this context as space-time codes (LAST) <ref type="bibr" target="#b24">[25]</ref>. The concatenation of the lattice encoder and the MIMO channel generates a lattice. If LDLC are used as lattice codes and the MIMO configuration is small, the inverse generator matrix of this concatenated lattice can be assumed to be sparse. Therefore, the MIMO channel and the LDLC can be jointly decoded using an LDLC-like decoder. However, even if a magic square LDLC is used as the lattice code, the concatenated lattice is not guaranteed to be equivalent to a magic square LDLC, and the necessary conditions for convergence are not guaranteed to be fulfilled. Therefore, the usage of LDLC for MIMO systems is a topic for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I EXACT PDF CALCULATIONS</head><p>Given the n-dimensional noisy observation y = x + w of the transmitted codeword x = Gb, we would like to calculate the probability density function (PDF) f x k |y (x k |y). We shall start by calculating f x|y (x|y) = fx(x)f y|x (y|x) fy(y)</p><p>. Denote the shaping region by B (G will be used to denote both the lattice and its generator matrix). f x (x) is a sum of |G ∩ B| n-dimensional Dirac delta functions, since x has nonzero probability only for the lattice points that lie inside the shaping region. Assuming further that all codewords are used with equal probability, all these delta functions have equal weight of 1  |G∩B| . The expression for f y|x (y|x) is simply the PDF of the i.i.d Gaussian noise vector. We therefore get:</p><formula xml:id="formula_69">f x|y (x|y) = f x (x)f y|x (y|x) f y (y) = (<label>22</label></formula><formula xml:id="formula_70">)</formula><formula xml:id="formula_71">1 |G∩B| l∈G∩B δ(x -l) • (2πσ 2 ) -n/2 e - P n i=1 (yi-xi) 2 /2σ 2 f y (y) = = C • l∈G∩B δ(x -l) • e -d 2 (l,y)/2σ 2</formula><p>Where C is not a function of x and d 2 (l, y) is the squared Euclidean distance between the vectors l and y in R n . It can be seen that the conditional PDF of x has a delta function for each lattice point, located at this lattice point with weight that is proportional to the exponent of the negated squared Euclidean distance of this lattice point from the noisy observation. The ML point corresponds to the delta function with largest weight. As the next step, instead of calculating the n-dimensional PDF of the whole vector x, we shall calculate the n onedimensional PDF's for each of the components x k of the vector x (conditioned on the whole observation vector y):</p><formula xml:id="formula_72">f x k |y (x k |y) =<label>(23)</label></formula><p>xi,i =k</p><formula xml:id="formula_73">• • • f x|y (x|y)dx 1 dx 2 • • • dx k-1 dx k+1 • • • dx n = = C • l∈G∩B δ(x k -l k ) • e -d 2 (l,y)/2σ 2</formula><p>This finishes the proof of (1). It can be seen that the conditional PDF of x k has a delta function for each lattice point, located at the projection of this lattice point on the coordinate x k , with weight that is proportional to the exponent of the negated squared Euclidean distance of this lattice point from the noisy observation. The ML point will therefore correspond to the delta function with largest weight in each coordinate. Note, however, that if several lattice points have the same projection on a specific coordinate, the weights of the corresponding delta functions will add and may exceed the weight of the ML point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II EXTENDING GALLAGER'S TECHNIQUE TO THE CONTINUOUS CASE</head><p>In <ref type="bibr" target="#b4">[5]</ref>, the derivation of the LDPC iterative decoder was simplified using the following technique: the codeword elements x k were assumed i.i.d. and a condition was added to all the probability calculations, such that only valid codewords were actually considered. The question is then how to choose the marginal PDF of the codeword elements. In <ref type="bibr" target="#b4">[5]</ref>, binary codewords were considered, and the i.i.d distribution assumed the values '0' and '1' with equal probability. Since we extend the technique to the continuous case, we have to set the continuous marginal distribution f x k (x k ). It should be set such that f x (x), assuming that x is a lattice point, is the same as f (x|s ∈ Z n ), assuming that x k are i.i.d with marginal PDF f x k (x k ), where s ∆ = H • x. This f x (x) equals a weighted sum of Dirac delta functions at all lattice points, where the weight at each lattice point equals the probability to use this point as a codeword.</p><p>Before proceeding, we need the following property of conditional probabilities. For any two continuous valued RV's u, v we have:</p><formula xml:id="formula_74">f (u|v ∈ {v 1 , v 2 , ..., v N }) = N k=1 f u,v (u, v k ) N k=1 f v (v k )<label>(24)</label></formula><p>(This property can be easily proved by following the lines of <ref type="bibr" target="#b20">[21]</ref>, pp. 159-160, and can also be extended to the infinite sum case).</p><p>Using <ref type="bibr" target="#b23">(24)</ref>, we now have:</p><formula xml:id="formula_75">f (x|s ∈ Z n ) = i∈Z n f x,s (x, s = i) i∈Z n f s (i) = = C i∈Z n f (x)f (s = i|x) = C i∈Z n f (x)δ(x -Gi)<label>(25)</label></formula><p>where C, C are independent of x.</p><p>The result is a weighted sum of Dirac delta functions at all lattice points, as desired. Now, the weight at each lattice point should equal the probability to use this point as a codeword. Therefore, f x k (x k ) should be chosen such that at each lattice point, the resulting vector distribution</p><formula xml:id="formula_76">f x (x) = n k=1 f x k (x k )</formula><p>will have a value that is proportional to the probability to use this lattice point. At x which is not a lattice point, the value of f x (x) is not important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX III DERIVATION OF THE ITERATIVE DECODER</head><p>In this appendix we shall derive the LDLC iterative decoder for a code with dimension n, using the tree assumption and Gallager's trick.</p><p>Referring to figure <ref type="figure" target="#fig_6">2</ref>, assume that there are only 2 tiers. Using Gallager's trick we assume that the x k 's are i.i.d. We would like to calculate f (x 1 |(y, s ∈ Z n ), where s</p><formula xml:id="formula_77">∆ = H • x.</formula><p>Due to the tree assumption, we can do it in two steps:</p><p>1. calculate the conditional PDF of the tier 1 variables of x 1 , conditioned only on the check equations that relate the tier 1 and tier 2 variables.</p><p>2. calculate the conditional PDF of x 1 itself, conditioned only on the check equations that relate x 1 and its first tier variables, but using the results of step 1 as the PDF's for the tier 1 variables. Hence, the results will be equivalent to conditioning on all the check equations.</p><p>There is a basic difference between the calculation in step 1 and step 2: the condition in step 2 involves all the check equations that are related to x 1 , where in step 1 a single check equation is always omitted (the one that relates the relevant tier 1 element with x 1 itself).</p><p>Assume now that there are many tiers, where each tier contains distinct elements of x (i.e. each element appears only once in the resulting tree). We can then start at the farthest tier and start moving toward x 1 . We do it by repeatedly calculating step 1. After reaching tier 1, we use step 2 to finally calculate the desired conditional PDF for x 1 .</p><p>This approach suggests an iterative algorithm for the calculation of f (x k |(y, s ∈ Z n ) for k=1, 2..n. In this approach we assume that the resulting tier diagram for each x k contains distinct elements for several tiers (larger or equal to the number of required iterations). We then repeat step 1 several times, where the results of the previous iteration are used as initial PDF's for the next iteration. Finally, we perform step 2 to calculate the final results.</p><p>Note that by conditioning only on part of the check equations in each iteration, we can not restrict the result to the shaping region. This is the reason that the decoder performs lattice decoding and not exact ML decoding, as described in Section III.</p><p>We shall now turn to derive the basic iteration of the algorithm. For simplicity, we shall start with the final step of the algorithm (denoted step 2 above). We would like to perform t iterations, so assume that for each x k there are t tiers with a total of N c check equations. For every</p><formula xml:id="formula_78">x k we need to calculate f (x k |s ∈ Z Nc , y) = f (x k |s (tier1) ∈ Z c k , s (tier2:tiert) ∈ Z Nc-c k , y)</formula><p>, where c k is the number of check equations that involve x k . s (tier1) ∆ = H (tier1) •x denotes the value of the left hand side of these check equations when x is substituted (H (tier1) is a submatrix of H that contains only the rows that relate to these check equations), and s (tier2:tiert) relates in the same manner to all the other check equations. For simplicity of notations, denote the event s (tier2:tiert) ∈ Z Nc-c k by A. As explained above, in all the calculations we assume that all the x k 's are independent.</p><p>Using <ref type="bibr" target="#b23">(24)</ref>, we get:</p><formula xml:id="formula_79">f (x k |s (tier1) ∈ Z c k , A, y) = = i∈Z c k f (x k , s (tier1) = i|A, y) i∈Z c k f (s (tier1) = i|A, y)<label>(26)</label></formula><p>Evaluating the term inside the sum of the nominator, we get:</p><formula xml:id="formula_80">f (x k , s (tier1) = i|A, y) = = f (x k |A, y) • f (s (tier1) = i|x k , A, y)<label>(27)</label></formula><p>Evaluating the left term, we get:</p><formula xml:id="formula_81">f (x k |A, y) = f (x k |y k ) = f (x k )f (y k |x k ) f (y k ) = = f (x k ) f (y k ) • 1 √ 2πσ 2 e -(y k -x k ) 2 2σ 2<label>(28)</label></formula><p>where f (x k |y) = f (x k |y k ) due to the i.i.d assumption.</p><p>Evaluating now the right term of (27), we get:</p><formula xml:id="formula_82">f (s (tier1) = i|x k , A, y) = = c k m=1 f (s (tier1) m = i m |x k , A, y)<label>(29)</label></formula><p>where s</p><formula xml:id="formula_83">(tier1) m</formula><p>denotes the m'th component of s (tier1) and i m denotes the m'th component of i. Note that each element of s (tier1) is a linear combination of several elements of x. Due to the tree assumption, two such linear combinations have no common elements, except for x k itself, which appears in all linear combinations. However, x k is given, so the i.i.d assumption implies that all these linear combinations are independent, so (29) is justified. The condition A (i.e. s (tier2:tiert) ∈ Z Nc-c k ) does not impact the independence due to the tree assumption.</p><p>Substituting ( <ref type="formula" target="#formula_80">27</ref>), ( <ref type="formula" target="#formula_81">28</ref>), (29) back in (26), we get:</p><formula xml:id="formula_84">f (x k |s (tier1) ∈ Z c k , A, y) = (30) = C • f (x k ) • e -(y k -x k ) 2 2σ 2 i∈Z c k c k m=1 f (s (tier1) m = i m |x k , A, y) = = C • f (x k ) • e -(y k -x k ) 2 2σ 2 i1∈Z i2∈Z • • • • • • ic k ∈Z c k m=1 f (s (tier1) m = i m |x k , A, y) = = C • f (x k ) • e -(y k -x k ) 2 2σ 2 c k m=1 im∈Z f (s (tier1) m = i m |x k , A, y)</formula><p>where C is independent of x k .</p><p>We shall now examine the term inside the sum: f (s</p><formula xml:id="formula_85">(tier1) m = i m |x k , A, y). Denote the linear combination that s (tier1) m</formula><p>represents by:</p><formula xml:id="formula_86">s (tier1) m = h m,1 x k + rm l=2 h m,l x j l<label>(31)</label></formula><p>where {h m,l }, l = 1, 2...r m is the set of nonzero coefficients of the appropriate parity check equation, and j l is the set of indices of the appropriate x elements (note that the set j l depends on m but we omit the "m" index for clarity of notations). Without loss of generality, h m,1 is assumed to be the coefficient of</p><formula xml:id="formula_87">x k . Define z m ∆ = rm l=2 h m,l x j l , such that s (tier1) m = h m,1 x k + z m .</formula><p>We then have:</p><formula xml:id="formula_88">f (s (tier1) m = i m |x k , A, y) = (32) = f zm|x k ,A,y (z m = i m -h m,1 x k |x k , A, y)</formula><p>Now, since we assume that the elements of x are independent, the PDF of the linear combination z m equals the convolution of the PDF's of its components:</p><formula xml:id="formula_89">f zm|x k ,A,y (z m |x k , A, y) = = 1 |h m,2 | f xj 2 |A,y z m h m,2 |A, y 1 |h m,3 | f xj 3 |A,y z m h m,3 |A, y • • • 1 |h m,rm | f xj rm |A,y z m h m,rm |A, y<label>(33)</label></formula><p>Note that the functions f xj i |y x ji |A, y are simply the output PDF's of the previous iteration. Define now</p><formula xml:id="formula_90">p m (x k ) ∆ = f zm|x k ,A,y (z m = -h m,1 x k |x k , A, y)<label>(34)</label></formula><p>Substituting ( <ref type="formula">32</ref>), (34) in (30), we finally get:</p><formula xml:id="formula_91">f (x k |s (tier1) ∈ Z c k , A, y) = (35) = C • f (x k ) • e -(y k -x k ) 2 2σ 2 c k m=1 im∈Z p m (x k - i m h m,1 )</formula><p>This result can be summarized as follows. For each of the c k check equations that involve x k , the PDF's (previous iteration results) of the active equation elements, except for x k itself, are expanded and convolved, according to (33). The convolution result is scaled by (-h m,1 ), the negated coefficient of x k in this check equation, according to (34), to yield p m (x k ). Then, a periodic function with period 1/|h m,1 | is generated by adding an infinite number of shifted versions of the scaled convolution result, according to the sum term in (35). After repeating this process for all the c k check equations that involve x k , we get c k periodic functions, with possibly different periods. We then multiply all these functions. The multiplication result is further multiplied by the channel Gaussian PDF term e -(y k -x k ) 2 2σ 2 and finally by f (x k ), the marginal PDF of x k under the i.i.d assumption. As discussed in Section III, we assume that f (x k ) is a uniform distribution with large enough range. This means that f (x k ) is constant over the valid range of x k , and can therefore be omitted from (35) and absorbed in the constant C.</p><p>As noted above, this result is for the final step (equivalent to step 2 above), where we determine the PDF of x k according to the PDF's of all its tier 1 elements. However, the repeated iteration step is equivalent to step 1 above. In this step ,we assume that x k is a tier 1 element of another element, say x l , and derive the PDF of x k that should be used as input to step 2 of x l (see figure <ref type="figure" target="#fig_6">2</ref>). It can be seen that the only difference between step 2 and step 1 is that in step 2 all the check equations that involve x k are used, where in step 1 the check equation that involves both x k and x l is ignored (there must be such an equation since x k is one of the tier 1 elements of x l ). Therefore, the step1 iteration is identical to (35), except that the product does not contain the term that corresponds to the check equation that combines both x k and x l . Denote</p><formula xml:id="formula_92">f kl (x k ) ∆ = f (x k |s (tier1 except l) ∈ Z c k -1 , A, y)<label>(36)</label></formula><p>We then get:</p><formula xml:id="formula_93">f kl (x k ) = C • e -(y k -x k ) 2 2σ 2 c k m=1 m =m l im∈Z p m (x k - i m h m,1 )<label>(37)</label></formula><p>where m l is the index of the check equation that combines both</p><p>x k and x l . In principle, a different f kl (x k ) should be calculated for each x l for which x k is a tier 1 element. However, the calculation is the same for all x l that share the same check equation. Therefore, we should calculate f kl (x k ) once for each check equation that involves x k . l can be regarded as the index of the check equation within the set of check equations that involve x k .</p><p>We can now formulate the iterative decoder. The decoder state variables are PDF's of the form f (t) kl (x k ), where k = 1, 2, ...n. For each k, l assumes the values 1, 2, ...c k , where c k is the number of check equations that involve x k . t denotes the iteration index. For a regular LDLC with degree d there will be nd PDF's. The PDF's are initialized by assuming that x k is a leaf of the tier diagram. Such a leaf has no tier 1 elements, so</p><formula xml:id="formula_94">f kl (x k ) = f (x k ) • f (y k |x k ).</formula><p>As explained above for equation (35), we shall omit the term f (x k ), resulting in initialization with the channel noise Gaussian around the noisy observation y k . Then, the PDF's are updated in each iteration according to (37). The variable node messages should be further normalized in order to get actual PDF's, such that ∞ -∞ f kl (x k )dx k = 1 (this will compensate for the constant C). The final PDF's for x k , k = 1, 2, ...n are then calculated according to <ref type="bibr">(35)</ref>.</p><p>Finally, we have to estimate the integer valued information vector b. This can be done by first estimating the codeword vector x from the peaks of the PDF's: xk = arg max x k f (x k |s (tier1) ∈ Z c k , A, y). Finally, we can estimate b as b = H x .</p><p>We have finished developing the iterative algorithm. It can be easily seen that the message passing formulation of Section III-A actually implements this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX IV ASYMPTOTIC BEHAVIOR OF THE VARIANCES RECURSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Lemma 3 and Lemma 4</head><p>We shall now derive the basic iterative equations that relate the variances at iteration t + 1 to the variances at iteration t for a magic square LDLC with dimension n, degree d and generating sequence</p><formula xml:id="formula_95">h 1 ≥ h 2 ≥ ... ≥ h d &gt; 0.</formula><p>Each iteration, every check node generates d output messages, one for each variable node that is connected to it, where the weights of these d connections are ±h 1 , ±h 2 , ..., ±h d . For each such output message, the check node convolves d -1 expanded variable node PDF messages, and then stretches and periodically extends the result. For a specific check node, denote the variance of the variable node message that arrives along an edge with weight ±h j by V (t) j , j = 1, 2, ...d. Denote the variance of the message that is sent back to a variable node along an edge with weight ±h j by Ṽ (t) j . From ( <ref type="formula" target="#formula_6">2</ref>), (3), we get:</p><formula xml:id="formula_96">Ṽ (t) j = 1 h 2 j d i=1 i =j h 2 i V (t) i<label>(38)</label></formula><p>Then, each variable node generates d messages, one for each check node that is connected to it, where the weights of these d connections are ±h 1 , ±h 2 , ..., ±h d . For each such output message, the variable node generates the product of d -1 check node messages and the channel noise PDF. For a specific variable node, denote the variance of the message that is sent back to a check node along an edge with weight ±h j by V (t+1) j</p><p>(this is the final variance of the iteration). From claim 2, we then get:</p><formula xml:id="formula_97">1 V (t+1) j = d i=1 i =j 1 Ṽ (t) i + 1 σ 2<label>(39)</label></formula><p>From symmetry considerations, it can be seen that all messages that are sent along edges with the same absolute value of their weight will have the same variance, since the same variance update occurs for all these messages (both for check node messages and variable node messages). Therefore, the d variance values</p><formula xml:id="formula_98">V (t) 1 , V<label>(t)</label></formula><p>2 , ..., V</p><p>d are the same for all variable nodes, where V (t) l is the variance of the message that is sent along an edge with weight ±h l . This completes the proof of Lemma 3.</p><p>Using this symmetry, we can now derive the recursive update of the variance values</p><formula xml:id="formula_100">V (t) 1 , V (t) 2 , ..., V<label>(t)</label></formula><p>d . Substituting (38) in (39), we get:</p><formula xml:id="formula_101">1 V (t+1) i = 1 σ 2 + d m=1 m =i h 2 m d j=1 j =m h 2 j V (t) j<label>(40)</label></formula><p>for i = 1, 2, ...d, which completes the proof of Lemma 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 1</head><p>We would like to analyze the convergence of the nonlinear recursion (4) for the variances</p><formula xml:id="formula_102">V (t) 1 , V (t) 2 , ..., V<label>(t)</label></formula><p>d . This recursion is illustrated in <ref type="bibr" target="#b4">(5)</ref> for the case d = 3. It is assumed that α &lt; 1, where α =</p><formula xml:id="formula_103">P d i=2 h 2 i h 2 1</formula><p>. Define another set of variables</p><formula xml:id="formula_104">U (t) 1 , U (t) 2 , ..., U<label>(t)</label></formula><p>d , which obey the following recursion. The recursion for the first variable is:</p><formula xml:id="formula_105">1 U (t+1) 1 = 1 σ 2 + d m=2 h 2 m h 2 1 U (t) 1<label>(41)</label></formula><p>where for i = 2, 3, ...d the recursion is:</p><formula xml:id="formula_106">1 U (t+1) i = h 2 1 d j=2 h 2 j U (t) j with initial conditions U (0) 1 = U (0) 2 = ... = U (0) d = σ 2 .</formula><p>It can be seen that ( <ref type="formula" target="#formula_105">41</ref>) can be regarded as the approximation of (4) under the assumptions that</p><formula xml:id="formula_107">V (t) i &lt;&lt; V (t) 1 and V (t) i &lt;&lt; σ 2 for i = 2, 3, ...d.</formula><p>For illustration, the new recursion for the case d = 3 is:</p><formula xml:id="formula_108">1 U (t+1) 1 = h 2 2 h 2 1 U (t) 1 + h 2 3 h 2 1 U (t) 1 + 1 σ 2 (42) 1 U (t+1) 2 = h 2 1 h 2 2 U (t) 2 + h 2 3 U (t) 3 1 U (t+1) 3 = h 2 1 h 2 2 U (t) 2 + h 2 3 U (t) 3</formula><p>It can be seen that in the new recursion, U</p><p>obeys a recursion that is independent of the other variables. From (41), this recursion can be written as</p><formula xml:id="formula_110">1 U (t+1) 1 = 1 σ 2 + α U (t) 1 , with initial condition U (0) 1 = σ 2 . Since α &lt; 1, this is a stable linear recursion for 1 U (t) 1</formula><p>, which can be solved to get 1  1-α t+1 . For the other variables, it can be seen that all have the same right hand side in the recursion (41). Since all are initialized with the same value, it follows that (i.e. ( <ref type="formula" target="#formula_18">4</ref>) to (41)), then the right hand side for</p><formula xml:id="formula_111">U (t) 1 = σ 2 (1 -α)</formula><formula xml:id="formula_112">U (t) 2 = U (t) 3 = ... = U (t</formula><formula xml:id="formula_113">1 V (t+1) 1</formula><p>is smaller, because it has additional positive terms in the denominators, where the common terms in the denominators are larger according to the induction assumption. Therefore,</p><formula xml:id="formula_114">V (t+1) 1 ≥ U (t+1) 1</formula><p>, as required. In the same manner, if we compare the right hand side of the update recursion for 1</p><formula xml:id="formula_115">V (t+1) i to that of 1 U (t+1) i for i ≥ 2, then the right hand side for 1 V (t+1) i</formula><p>is larger, because it has additional positive terms, where the common terms are also larger since their denominators are smaller due to the induction assumption. Therefore, V i , we now have:</p><formula xml:id="formula_116">V (t) 1 ≥ U (t) 1 = σ 2 (1 -α) 1 1 -α t+1 ≥ σ 2 (1 -α)<label>(43)</label></formula><p>where for i = 2, 3, ...d we have:</p><formula xml:id="formula_117">V (t) i ≤ U (t) i = σ 2 α t<label>(44)</label></formula><p>We have shown that the first variance is lower bounded by a positive nonzero constant where the other variances are upper bounded by a term that decays exponentially to zero. Therefore, for large t we have</p><formula xml:id="formula_118">V (t) i &lt;&lt; V (t) 1 and V (t) i</formula><p>&lt;&lt; σ 2 for i = 2, 3, ...d. It then follows that for large t the variances approximately obey the recursion (41), which was built from the actual variance recursion (4) under these assumptions. Therefore, for i = 2, 3, ...d the variances are not only upper bounded by an exponentially decaying term, but actually approach such a term, where the first variance actually approaches the constant σ 2 (1 -α) in an exponential rate. This completes the proof of Theorem 1.</p><p>Note that the above analysis only applies if α &lt; 1. To illustrate the behavior for α ≥ 1, consider the simple case of h 1 = h 2 = ... = h d . From (4), <ref type="bibr" target="#b4">(5)</ref> it can be seen that for this case, if V (0) i is independent of i, then V (t) i is independent of i for every t &gt; 0, since all the elements will follow the same recursive equations. Substituting this result in the first equation, we get the single variable recursion</p><formula xml:id="formula_119">1 V (t+1) i = 1 V (t) i + 1 σ 2 with initialization V (0) i = σ 2 . This recursion is easily solved to get 1 V (t) i = t+1 σ 2 or V (t) i = σ 2 t+1</formula><p>. It can be seen that all the variances converge to zero, but with slow convergence rate of o(1/t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX V ASYMPTOTIC BEHAVIOR OF THE MEAN VALUES RECURSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Lemma 5 and Lemma 6 (Mean of Narrow Messages)</head><p>Assume a magic square LDLC with dimension n and degree d. We shall now examine the effect of the calculations in the check nodes and variable nodes on the mean values and derive the resulting recursion. Every iteration, each check node generates d output messages, one for each variable node that connects to it, where the weights of these d connections are ±h 1 , ±h 2 , ..., ±h d . For each such output message, the check node convolves d -1 expanded variable node PDF messages, and then stretches and periodically extends the result. We shall concentrate on the nd consistent Gaussians that relate to the same integer vector b (one Gaussian in each message), and analyze them jointly. For convenience, we shall refer to the mean value of the relevant consistent Gaussian as the mean of the message.</p><p>Consider now a specific check node. Denote the mean value of the variable node message that arrives at iteration t along the edge with weight ±h j by m (t) j , j = 1, 2, ...d. Denote the mean value of the message that is sent back to a variable node along an edge with weight ±h j by m(t) j . From (2), (3) and claim 1, we get:</p><formula xml:id="formula_120">m(t) j = 1 h j   bk - d i=1 i =j h i m (t) i   <label>(45)</label></formula><p>where b k is the appropriate element of b that is related to this specific check equation, which is the only relevant index in the infinite sum of the periodic extension step (3). Note that the check node operation is equivalent to extracting the value of m j from the check equation</p><formula xml:id="formula_121">d i=1 h i m i = b k , assuming</formula><p>all the other m i are known. Note also that the coefficients h j should have a random sign. To keep notations simple, we assume that h j already includes the random sign. Later, when several equations will be combined together, we should take it into account. Then, each variable node generates d messages, one for each check node that is connected to it, where the weights of these d connections are ±h 1 , ±h 2 , ..., ±h d . For each such output message, the variable node generates the product of d -1 check node messages and the channel noise PDF. For a specific variable node, denote the mean value of the message that arrives from a check node along an edge with weight ±h j by m(t) j , and the appropriate variance by Ṽ (t) j . The mean value of the message that is sent back to a check node along an edge with weight ±h j is m (t+1) j , the final mean value of the iteration. From claim 2, we then get:</p><formula xml:id="formula_122">m (t+1) j = y k /σ 2 + d i=1 i =j m(t) i / Ṽ (t) i 1/σ 2 + d i=1 i =j 1/ Ṽ (t) i (<label>46</label></formula><formula xml:id="formula_123">)</formula><p>where y k is the channel observation for the variable node and σ 2 is the noise variance. Note that m(t) i , i = 1, 2, ..., d in (46) are the mean values of check node messages that arrive to the same variable node from different check nodes, where in (45) they define the mean values of check node messages that leave the same check node. However, it is beneficial to keep the notations simple, and we shall take special care when (46) and (45) are combined.</p><p>It can be seen that the convergence of the mean values is coupled to the convergence of the variances (unlike the recursion of the variances which was autonomous). However, as the iterations go on, this coupling disappears. To see that, recall from Theorem 1 that for each check node, the variance of the variable node message that comes along an edge with weight ±h 1 approaches a finite value, where the variance of all the other messages approaches zero exponentially. According to (38), the variance of the check node message is a weighted sum of the variances of the incoming variable node messages. Therefore, the variance of the check node message that goes along an edge with weight ±h 1 will approach zero, since the weighted sum involves only zero-approaching variances. All the other messages will have finite variance, since the weighted sum involves the non zero-approaching variance. To summarize, each variable node sends (and each check node receives) d -1 "narrow" messages and a single "wide" message. Each check node sends (and each variable node receives) d -1 "wide" messages and a single "narrow" message, where the narrow message is sent along the edge from which the wide message was received (the edge with weight ±h 1 ).</p><p>We shall now concentrate on the case where the variable node generates a narrow message. Then, the sum in the nominator of (46) has a single term for which Ṽ (t) i → 0, which corresponds to i = 1. The same is true for the sum in the denominator. Therefore, for large t, all the other terms will become negligible and we get:</p><formula xml:id="formula_124">m (t+1) j ≈ m(t) 1<label>(47)</label></formula><p>where m(t) 1 is the mean of the message that comes from the edge with weight h 1 , i.e. the narrow check node message. As discussed above, d -1 of the d variable node messages that leave the same variable node are narrow. From (47) it comes out that for large t, all these d -1 narrow messages will have the same mean value. This completes the proof of Lemma 5. Now, combining (45) and (47) (where the indices are arranged again, as discussed above), we get:</p><formula xml:id="formula_125">m (t+1) l1 ≈ 1 h 1 b k - d i=2 h i m (t) li<label>(48)</label></formula><p>where l i , i = 1, 2..., d are the variable nodes that take place in the check equation for which variable node l 1 appears with coefficient ±h 1 . b k is the element of b that is related to this check equation. m (t+1) l1</p><p>denotes the mean value of the d -1 narrow messages that leave variable node l 1 at iteration t + 1. m (t) li is the mean value of the narrow messages that were generated at variable node l i at iteration t. Only narrow messages are involved in (48), because the right hand side of (47) is the mean value of the narrow check node message that arrived to variable node l 1 , which results from the convolution of d-1 narrow variable node messages. Therefore, for large t, the mean values of the narrow messages are decoupled from the mean values of the wide messages (and also from the variances), and they obey an autonomous recursion.</p><p>The mean values of the narrow messages at iteration t can be arranged in an n-element column vector m (t) (one mean value for each variable node). We would like to show that the mean values converge to the coordinates of the lattice point x = Gb. Therefore, it is useful to define the error vector e (t) ∆ = m (t) -x. Since Hx = b, we can write (using the same notations as (48)):</p><formula xml:id="formula_126">x l1 = 1 h 1 b k - d i=2 h i x li<label>(49)</label></formula><p>Subtracting (49) from (48), we get:</p><formula xml:id="formula_127">e (t+1) l1 ≈ - 1 h 1 d i=2 h i e (t) li<label>(50)</label></formula><p>Or, in vector notation:</p><formula xml:id="formula_128">e (t+1) ≈ -H • e (t)<label>(51)</label></formula><p>where H is derived from H by permuting the rows such that the ±h 1 elements will be placed on the diagonal, dividing each row by the appropriate diagonal element (h 1 or -h 1 ), and then nullifying the diagonal. Note that in order to simplify the notations, we embedded the sign of ±h j in h j and did not write it implicitly. However, the definition of H solves this ambiguity. This completes the proof of Lemma 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Lemma 7 (Mean of Wide Messages)</head><p>Recall that each check node receives d-1 narrow messages and a single wide message. The wide message comes along the edge with weight ±h 1 . Denote the appropriate lattice point by x = Gb, and assume that the Gaussians of the narrow variable node messages have already converged to impulses at the corresponding lattice point coordinates (Theorem 2). We can then substitute in (45) m (t) i = x i for i ≥ 2. The mean value of the (wide) message that is returned along the edge with weight ±h j (j = 1) is:</p><formula xml:id="formula_129">m(t) j = 1 h j   bk - d i=2 i =j h i x i -h 1 m (t) 1    = (52) = 1 h j h 1 x 1 + h j x j -h 1 m (t) 1 = x j + h 1 h j x 1 -m (t)<label>1</label></formula><p>As in the previous section, for convenience of notations we embed the sign of ±h j in h j itself. The sign ambiguity will be resolved later. The meaning of ( <ref type="formula">52</ref>) is that the returned mean value is the desired lattice coordinate plus an error term that is proportional to the error in the incoming wide message. From (38), assuming that the variance of the incoming wide message has already converged to its steady state value σ 2 (1 -α) and the variance of the incoming narrow messages has already converged to zero, the variance of this check node message will be:</p><formula xml:id="formula_130">Ṽ (t) j = h 2 1 h 2 j σ 2 (1 -α)<label>(53)</label></formula><p>where α =</p><formula xml:id="formula_131">P d i=2 h 2 i h 2 1</formula><p>. Now, each variable node receives d -1 wide messages and a single narrow message. The mean values of the wide messages are according to (52) and the variances are according to (53). The single wide message that this variable node generates results from the d -1 input wide messages and it is sent along the edge with weight ±h 1 . From (46), the wide mean value generated at variable node k will then be:</p><formula xml:id="formula_132">m (t+1) k = (54) = y k /σ 2 + d j=2 x k + h1 hj (x p(k,j) -m<label>(t)</label></formula><p>p(k,j) )</p><formula xml:id="formula_133">h 2 j h 2 1 σ 2 (1-α) 1/σ 2 + d j=2 h 2 j h 2 1 σ 2 (1-α)</formula><p>Note that the x 1 and m 1 terms of (52) were replaced by x p(k,j) and m p(k,j) , respectively, since for convenience of notations we denoted by m 1 the mean of the message that came to a check node along the edge with weight ±h 1 . For substitution in (46) we need to know the exact variable node index that this edge came from. Therefore, p(k, j) denotes the index of the variable node that takes place with coefficient ±h 1 in the check equation where x k takes place with coefficient ±h j .</p><p>Rearranging terms, we then get: k -x k (where x = Gb is the lattice point that corresponds to b). Denote by q the difference vector between x and the noisy observation y, i.e. q ∆ = y -x. Note that if b corresponds to the correct lattice point that was transmitted, q equals the channel noise vector w. Subtracting x k from both sides of (55), we finally get:</p><formula xml:id="formula_134">e (t+1) k = q k (1 -α) - 1 h 1 d j=2 h j e (t) p(k,j)<label>(56)</label></formula><p>If we now arrange all the errors in a single column vector e, we can write:</p><formula xml:id="formula_135">e (t+1) = -F • e (t) + (1 -α)q (<label>57</label></formula><formula xml:id="formula_136">)</formula><p>where F is an n × n matrix defined by:</p><formula xml:id="formula_137">F k,l =    H r,k H r,l</formula><p>if k = l and there exist a row r of H for which |H r,l | = h 1 and H r,k = 0 0 otherwise (58)</p><p>F is well defined, since for a given l there can be at most a single row of H for which |H r,l | = h 1 (note that α &lt; 1 implies that h 1 is different from all the other elements of the generating sequence).</p><p>As discussed above, we embedded the sign in h i for convenience of notations, but when several equations are combined the correct signs should be used. It can be seen that using the notations of (57) resolves the correct signs of the h i elements. This completes the proof of Lemma 7.</p><p>An alternative way to construct F from H is as follows. To construct the k'th row of F , denote by r i , i = 1, 2, ...d, the index of the element in the k'th column of H with value h i (i.e. |H ri,k | = h i ). Denote by l i , i = 1, 2, ...d, the index of the element in the r i 'th row of H with value h 1 (i.e. |H ri,li | = h 1 ). The k'th row of F will be all zeros except for the d -1 elements l i , i = 2, 3...d, where F k,li = H r i ,k H r i ,l i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VI ASYMPTOTIC BEHAVIOR OF THE AMPLITUDES RECURSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Lemma 9</head><p>From (10), a (t) i is clearly non-negative. From Sections IV-B, IV-C (and the appropriate appendices) it comes out that for consistent Gaussians, the mean values and variances of the messages have a finite bounded value and converge to a finite steady state value. The excitation term a (t) i depends on these mean values and variances according to <ref type="bibr" target="#b9">(10)</ref>, so it is also finite and bounded, and it converges to a steady state value, where caution should be taken for the case of a zero approaching variance. Note that at most a single variance in <ref type="bibr" target="#b9">(10)</ref> may approach zero (as explained in Section IV-B, a single narrow check node message is used for the generation of narrow variable node messages, and only wide check node messages are used for the generation of wide variable node messages). The zero approaching variance corresponds to the message that arrives along an edge with weight ±h 1 , so assume that</p><formula xml:id="formula_138">Ṽ (t) k,1</formula><p>approaches zero and all other variances approach a non-zero value. Then, V (t) k,i also approaches zero and we have to show that the term</p><formula xml:id="formula_139">V (t) k,i Ṽ (t) k,1</formula><p>, which is a quotient of zero approaching terms, approaches a finite value. Substituting for V (t) k,i , we get: lim</p><formula xml:id="formula_140">Ṽ (t) k,1 →0 V (t) k,i Ṽ (t) k,1</formula><p>= lim</p><formula xml:id="formula_141">Ṽ (t) k,1 →0 1 Ṽ (t) k,1     1 σ 2 + d j=1 j =i 1 Ṽ (t) k,j     -1 = = lim Ṽ (t) k,1 →0     Ṽ (t)</formula><p>k,1</p><formula xml:id="formula_142">σ 2 + 1 + d j=2 j =i Ṽ (t)</formula><p>k,1</p><formula xml:id="formula_143">Ṽ (t) k,j     -1 = 1<label>(59)</label></formula><p>Therefore, a (t) i converges to a finite steady state value, and has a finite value for every i and t. This completes the first part of the proof.</p><p>We would now like to show that lim t→∞ nd i=1 a (t) i can be expressed in the form 1 2σ 2 (Gb -y) T W (Gb -y). Every variable node sends d -1 narrow messages and a single wide message. We shall start by calculating a (t) i that corresponds to a narrow message. For this case, d -1 check node messages take place in the sums of <ref type="bibr" target="#b9">(10)</ref>, from which a single message is narrow and d -2 are wide. The narrow message arrives along the edge with weight ±h 1 , and has variance Ṽ (t) k,1 → 0. Substituting in <ref type="bibr" target="#b9">(10)</ref>, and using (59), we get:</p><formula xml:id="formula_144">a (t) (k-1)d+i → 1 2     d j=2 j =i m(t) k,1 - m(t) k,j<label>2</label></formula><formula xml:id="formula_145">Ṽ (t) k,j + m(t) k,1 -y k 2 σ 2    <label>(60)</label></formula><p>Denote x = Gb. The mean values of the narrow check node messages converge to the appropriate lattice point coordinates, i.e. m(t) k,1 → x k . From Theorem 3, the mean value of the wide variable node message that originates from variable node k converges to x k +e k , where e denotes the vector of error terms. The mean value of a wide check node message that arrives to node k along an edge with weight ±h j can be seen to approach m(t) k,j = x k -h1 hj e p(k,j) , where p(k, j) denotes the index of the variable node that takes place with coefficient ±h 1 in the check equation where x k takes place with coefficient ±h j . For convenience of notations, we shall assume that h j already includes the sign (this sign ambiguity will be resolved later). The variance of the wide variable node messages converges to σ 2 (1 -α), so the variance of the wide check node message that arrives to node k along an edge with weight ±h j can be seen to approach</p><formula xml:id="formula_146">Ṽ (t) k,j → h 2 1 h 2 j σ 2 (1 -α).</formula><p>Substituting in (60),</p><p>and denoting q = y -x, we get: i that corresponds to a wide message. Substituting m(t) k,j → x k -h1 hj e p(k,j) , <ref type="bibr" target="#b9">(10)</ref>, we get: where F is defined in Theorem 3 and (F • e) k denotes the k'th element of the vector (F • e). Note that using F solves the sign ambiguity that results from embedding the sign of ±h j in h j for convenience of notations, as discussed above.</p><formula xml:id="formula_147">Ṽ (t) k,j → h 2 1 h 2 j σ 2 (1 -α), V (t) k,1 → σ 2 (1 -α) in</formula><p>Turning now to the second term of (63): </p><formula xml:id="formula_148">+ (2 -α)q 2 k -2q k e k</formula><p>where we have substituted F e → (1 -α)q -e, as comes out from Lemma 7. Again, using F resolves the sign ambiguity of h j , as discussed above. Substituting (64) and (65) back in (63), summing the result with (62), and rearranging terms, the total contribution of variable node k to the asymptotic excitation sum term is: </p><formula xml:id="formula_149">+ d + 1 -α 2σ 2 q 2 k - 1 2σ 2 (1 -α) (F e) 2 k - 1 σ 2 q k e k</formula><p>Summing over all the variable nodes, the total asymptotic excitation sum term is: </p><formula xml:id="formula_150">+ d + 1 -α 2σ 2 q 2 - 1 2σ 2 (1 -α) F e 2 - 1 σ 2 q T e</formula><p>Substituting e = (1 -α)(I + F ) -1 q (see Theorem 3), we finally get: </p><p>From <ref type="bibr" target="#b9">(10)</ref> it can be seen that nd i=1 a (t) i is positive for every nonzero q. Therefore, W is positive definite. This completes the second part of the proof. expanded PDF's. Then, d -1 such results are multiplied, and an IFFT (of length 1/∆) gives a single period of Qj (x).</p><p>With this method, instead of calculating d FFT's and d IFFT's of size larger than L, we calculate d FFT's and d IFFT's of size L/D = 1/∆.</p><p>In order to generate the final check node message, we should stretch Qj (x) to Q j (x) = Qj (-h j x). This can be done by interpolating a single period of Qj (x) using interpolation methods similar to those that were used in Section VI for expanding the variable node PDF's.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>check matrix of a magic square LDLC with lattice dimension n = 6, degree d = 3 and generating sequence {1, 0.8, 0.5}. This H should be further normalized by the constant n |det(H)| in order to have |det(H)| = |det(G)| = 1, as required by Definition 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Simulation results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>i for i = 2, 3, ...d, as required. Using claim 3 and the analytic results for U (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>=</head><label></label><figDesc>y k (1 -α) + x k • α + d j=2 hj h1 x p(k,j) -m (t) p(k,j) (1 -α) + α = = y k + α(x k -y k )Denote now the wide message mean value error by e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the narrow messages that leave variable node k, we get:To complete the calculation of the contribution of node k to the excitation term, we still have to calculate a (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>p(k,l) -h1 hj e p(k,j) ,j) -(F • e) 2 k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>αq 2 k</head><label>2</label><figDesc>+ 2q k [(1 -α)q k -e k ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>= ( 1</head><label>1</label><figDesc>-α)(I + F ) -1 T (d -1) 2 I -F T F (I + F ) -1 + +(d + 1 -α)I -2(1 -α)(I + F ) -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>our decoder will try to estimate f x k |y (x k |y) (or at least approximate it) in an iterative manner.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X11</cell><cell>X10</cell><cell>X9</cell><cell>X8</cell></row><row><cell></cell><cell></cell><cell cols="2">Tier 2 (24 nodes)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tier1 (6 nodes)</cell></row><row><cell>X7</cell><cell>X6</cell><cell>X5</cell><cell>X4</cell><cell>X3</cell><cell>X2</cell></row><row><cell></cell><cell></cell><cell>X1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 2. Tier diagram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>practical, so</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>{1/2.31, 1/3.17, 1/5.11, 1/7.33, 1/11.71, 1/13.11, 1/17.55}) were simulated for the AWGN channel at various block lengths. The degree was d = 5 for n = 100 and d = 7 for all other n. For n = 100 the matrix H was further normalized to get n det(H) = 1. For all other n, normalizing the generating sequence such that the largest element has magnitude 1 also gave the desired determinant normalization (see Section V-</figDesc><table><row><cell></cell><cell>square</cell><cell>LDLC</cell><cell>with</cell><cell>the</cell><cell>first</cell><cell>gen-</cell></row><row><cell>erating</cell><cell>sequence</cell><cell>of</cell><cell cols="2">Section</cell><cell>V-A</cell><cell>(i.e.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) d for all t ≥ 0. Substituting back in (41), we get the recursion U For every t ≥ 0, the first variables of the two sets are related by V By induction: the initialization of the two sets of variables obviously satisfies the required relations. Assume now that the relations are satisfied for iteration t, i.e. V</figDesc><table><row><cell cols="3">(t+1) 2 1, this is a stable linear recursion for U = αU (t) 2 , with initial condition U 2 (0) (t) 2 , which can be solved = σ 2 . Since α &lt; to get U (t) 2 = σ 2 α t . We found an analytic solution for the variables U (t) i . How-ever, we are interested in the variances V (t) i . The following</cell></row><row><cell cols="3">claim relates the two sets of variables.</cell></row><row><cell>V</cell><cell>Claim 3: (t) 1 (t) i ≤ U (t) i .</cell><cell>≥ U 1 , where for i = 2, 3, ...d we have (t)</cell></row><row><cell cols="3">Proof: (t) 1 1 and for i = 2, 3, ...d, V U (t) (t) i ≤ U (t) i . If we now compare the ≥ right hand side of the update recursion for 1 (t+1) to that of V 1 1</cell></row><row><cell cols="2">(t+1) 1 U</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Support and interesting discussions with Ehud Weinstein are gratefully acknowledged.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since a</head><p>(t) i is finite and bounded, there exists m a such that |a (t) i | ≤ m a for all 1 ≤ i ≤ nd and t &gt; 0. We then have:</p><p>Therefore, for d &gt; 2 the infinite sum will have a finite steady state value. This completes the proof of Lemma 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VII GENERATION OF A PARITY CHECK MATRIX FOR LDLC</head><p>In the following pseudo-code description, the i, j element of a matrix P is denoted by P i,j and the k'th column of a matrix P is denoted by P :,k . FFT calculation can be made simpler by using the fact that the convolution is followed by the following steps: the convolution result pj (x) is stretched to p j (x) = pj (-h j x) and then periodically extended to <ref type="formula">3</ref>)). It can be seen that the stretching and periodic extension steps can be exchanged, and the convolution result pj (x) can be first periodically extended with period 1 to Qj (x) = ∞ i=-∞ pj (x + i) and then stretched to Q j (x) = Qj (-h j x). Now, the infinite sum can be written as a convolution with a sequence of Dirac impulses:</p><p>Therefore, the Fourier transform of Qj (x) will equal the Fourier transform of pj (x) multiplied by the Fourier transform of the impulse sequence, which is itself an impulse sequence. The FFT of Qj (x) will therefore have several nonzero values, separated by sequences of zeros. These nonzero values will equal the FFT of pj (x) after decimation. To ensure an integer decimation rate, we should choose the PDF resolution ∆ such that an interval with range 1 (the period of Qj (x)) will contain an integer number of samples, i.e. 1/∆ should be an integer. Also, we should choose L (the number of samples in Qj (x)) to correspond to a range which equals an integer, i.e. D = L • ∆ should be an integer. Then, we can calculate the (size L) FFT of pj (x) and then decimate by D. The result will give 1/∆ samples which correspond to a single period (with range 1) of Qj (x).</p><p>However, instead of calculating an FFT of length L and immediately decimating, we can directly calculate the decimated FFT. Denote the expanded PDF at the convolution input by fk , k = 1, 2, ...L (where the expanded PDF is zero padded to length L). To generate directly the decimated result, we can first calculate the (size D) FFT of each group of D samples which are generated by decimating fk by L/D = 1/∆. Then, the desired decimated result is the FFT (of size 1/∆) of the sequence of first samples of each FFT of size D. However, The first sample of an FFT is simply the sum of its inputs. Therefore, we should only calculate the sequence (of length 1/∆) g i = D-1 k=0 fi+k/∆ , i = 1, 2, ...1/∆ and then calculate the FFT (of length 1/∆) of the result. This is done for all the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948-10">July and Oct. 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coding for noisy channels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Elias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Conv. Rec</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1955-03">Mar. 1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probability of error for optimal codes in a Gaussian channel</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="611" to="656" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Theory and Practice of Error Control Codes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Blahut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Low-Density Parity-Check Codes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Near Shannon limit errorcorrecting coding and decoding: Turbo codes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glavieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thitimajshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Communications</title>
		<meeting>IEEE Int. Conf. Communications</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1064" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The upper error bound of a new near-optimal code</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Buda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="441" to="445" />
			<date type="published" when="1975-07">July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Some optimal codes have structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Buda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="893" to="899" />
			<date type="published" when="1989-08">Aug. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Corrected proof of de Buda&apos;s Theorem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ch</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1737" />
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Averaging bounds for lattices and linear codes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1767" to="1773" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lattice codes can achieve capacity on the AWGN channel</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rimoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Achieving 1/2 log(1 + SNR) on the AWGN channel with lattice encoding and decoding</title>
		<author>
			<persName><forename type="first">U</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2293" to="2314" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">New trellis codes based on lattices and cosets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="177" to="195" />
			<date type="published" when="1987-03">Mar. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coset codes-Part I: Introduction and geometrical classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="1123" to="1151" />
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Signal Codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Information theory Workshop</title>
		<meeting>the Information theory Workshop</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="332" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Signal Codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design and analysis of nonbinary LDPC codes for arbitrary discrete-memoryless channels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bennatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="549" to="583" />
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Capacity approaching bandwidth efficient coded modulation schemes based on low density parity check codes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="2141" to="2155" />
			<date type="published" when="2003-09">Sept. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Sloane</surname></persName>
		</author>
		<title level="m">Sphere Packings, Lattices and Groups</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On coding without restrictions for the AWGN channel</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poltyrev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="409" to="417" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Papoulis</surname></persName>
		</author>
		<title level="m">Probability, Random variables and Stochastic Processes</title>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative Methods for Sparse Linear Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematic</title>
		<imprint>
			<biblScope unit="issue">SIAM</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Closest point search in lattices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2201" to="2214" />
			<date type="published" when="2002-08">Aug. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Closest point search in lattices using sequential decoding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shalvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the International Symposium on Information Theory (ISIT)</title>
		<meeting>the International Symposium on Information Theory (ISIT)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1053" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lattice coding and decoding achieve the optimal diversity-multiplexing tradeoff of MIMO channels</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="968" to="985" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
