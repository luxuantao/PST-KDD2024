<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Keyword Search on Large RDF Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wangchao</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anastasios</forename><surname>Kementsietsidis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Songyun</forename><surname>Duan</surname></persName>
						</author>
						<title level="a" type="main">Scalable Keyword Search on Large RDF Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CD1AA7E3084B0E9FD136584F010A995</idno>
					<idno type="DOI">10.1109/TKDE.2014.2302294</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2014.2302294, IEEE Transactions on Knowledge and Data Engineering This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2014.2302294, IEEE Transactions on Knowledge and Data Engineering</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keyword search is a useful tool for exploring large RDF datasets. Existing techniques either rely on constructing a distance matrix for pruning the search space or building summaries from the RDF graphs for query processing. In this work, we show that existing techniques have serious limitations in dealing with realistic, large RDF data with tens of millions of triples. Furthermore, the existing summarization techniques may lead to incorrect/incomplete results. To address these issues, we propose an effective summarization algorithm to summarize the RDF data. Given a keyword query, the summaries lend significant pruning powers to exploratory keyword search and result in much better efficiency compared to previous works. Unlike existing techniques, our search algorithms always return correct results. Besides, the summaries we built can be updated incrementally and efficiently. Experiments on both benchmark and large real RDF data sets show that our techniques are scalable and efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The RDF (Resource Description Framework) is the de-facto standard for data representation on the Web. So, it is no surprise that we are inundated with large amounts of rapidly growing RDF data from disparate domains. For instance, the Linked Ope n Data (LOD) initiative integrates billions of entities from hundreds of sources. Just one of these sources, the DBpedia dataset, describes more than 3.64 million things using more than 1 billion RDF triples; and it contains numerous keywords, as shown in Figure <ref type="figure">1</ref>.</p><p>Keyword search is an important tool for exploring and searching large data corpuses whose structure is either unknown, or constantly changing. So, keyword search has already been studied in the context of relational databases <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, XML documents <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and more recently over graphs <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> and RDF data <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, existing solutions for RDF data have limitations. Most notably, these solutions suffer from: (i) returning incorrect answers, i.e., the keyword search returns answers that do not correspond to real subgraphs or misses valid matches from the underlying RDF data; (ii) inability to scale to handle typical RDF datasets with tens of millions of triples. Consider the results from two representative solutions <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, as shown in Figures <ref type="figure">2</ref> and<ref type="figure" target="#fig_1">3</ref>. Figure <ref type="figure">2</ref> shows the query results on three different datasets using the solution specifically designed for RDF in <ref type="bibr" target="#b23">[24]</ref>, the Schema method. While this solution may perform well on datasets that have regular topological structure (e.g., DBLP), it returns incorrect answers for others (e.g., LUBM <ref type="bibr" target="#b12">[13]</ref> etc.) when compared to a naive, but Exact method. On the other hand, classical techniques <ref type="bibr" target="#b13">[14]</ref> proposed for general graphs can be used for RDF data, but they assume a distance matrix built on the data, which makes it prohibitively expensive to apply to large RDF dataset as shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>Motivated by these observations, we present a comprehensive study to address the keyword search problem over big RDF data. Our goal is to design a scalable and exact solution Wangchao Le and Feifei Li are with the School of Computing, University of Utah. E-mail: {lew,lifeifei}@cs.utah.edu. Anastasios Kementsietsidis and Songyun Duan are with IBM Thomas J. Watson Research Center. E-mail: {akement, sduan}@us.ibm.com.   <ref type="bibr" target="#b13">[14]</ref>.</p><p>that handles realistic RDF datasets with tens of millions of triples. To address the scalability issues, our solution builds a new, succinct and effective summary from the underlying RDF graph based on its types. Given a keyword search query, we use the summary to prune the search space, leading to much better efficiency compared to a baseline solution. To summarize, our contributions are:</p><p>• We identify and address limitations in the existing, stateof-the-art methods for keyword search in RDF data <ref type="bibr" target="#b23">[24]</ref>. We show that these limitations could lead to incomplete and incorrect answers in real RDF datasets. We propose a new, correct baseline solution based on the backward search idea.</p><p>• We develop efficient algorithms to summarize the structure of RDF data, based on the types in RDF graphs, and use it to speed up the search. Compared to previous works that also build summary <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>, our technique uses different intuitions, which is more scalable and lends significant pruning power without sacrificing the soundness of the result. Further, our summary is light-weight and updatable.</p><p>• Our experiments on both benchmark and large real RDF datasets show that our techniques are much more scalable and efficient in correctly answering keyword search queries for realistic RDF workloads than the existing methods.</p><p>In what follows, we formulate the keyword search problem on RDF data in Section 2, survey related work in Section 3, present our solutions in Sections 4 to 7, show experimental results in Section 8, and conclude in Section 9. Table <ref type="table" target="#tab_1">1</ref> list the frequently used symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>An RDF dataset is a graph (RDF graph) composed by triples, where a triple is formed by subject, predicate and object in that order. When such ordering is important semantically, a triple is regarded as a directed edge (the predicate) connecting two vertices (from subject to object). Thus, an RDF dataset can be alternatively viewed as a directed graph, as shown by the arrows in Figure <ref type="figure">1</ref>. W3C has provided a set of unified vocabularies (as part of the RDF standard) to encode the rich semantics. From these, the rdfs:type predicate (or type for short) is particularly useful to our problem (see <ref type="bibr">Section 5)</ref>, since it provides a classification of vertices of an RDF graph into different groups. For instance in Figure <ref type="figure">1</ref>, the entity URI 3 has type SpaceMission. Formally, we view an RDF dataset as an RDF graph G = (V, E) where</p><p>• V is the union of disjoint sets, V E , V T and V W , where V E is the set of entity vertices (i.e.,URIs), V T is the set of type vertices, and V W is a set of keyword vertices.</p><p>• E is the union of disjoint sets, E R , E A , and E T where E R is the set of entity-entity edges (i.e., connecting two vertices in V E ), E A is the set of entity-keyword edges (i.e., connecting an entity to a keyword), and E T is the set entity-type edges (i.e., connecting an entity to a type).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rocket booster SpaceMission</head><p>SpaceMission URI 1 URI 5 URI 3 "Saturn-V" "Apollo 1" "Apollo 11" booster Fig. <ref type="figure" target="#fig_10">4</ref>. Condensed view: combining vertices.</p><p>For example in Figure <ref type="figure">1</ref>, all gray vertices are type vertices while entity vertices are in white. Each entity vertex also has associated keyword vertices (in cyan). The division on vertices results in a corresponding division on the RDF predicates, which leads to the classification of the edge set E discussed earlier. Clearly, the main structure of an RDF graph is captured by the entity-entity edges represented by the set E R . As such, an alternative view is to treat an entity vertex and its associated type and keyword vertices as one vertex. For example, the entity vertices URI 5 , URI 1 and URI 3 from Figure <ref type="figure">1</ref>, with their types and keywords, can be viewed as the structure in Figure <ref type="figure" target="#fig_10">4</ref>.</p><p>In general, for an RDF graph G = {V, E}, we will refer this as the condensed view of G, denoted as</p><formula xml:id="formula_0">G c = {V E , E R }. While |V E | ≡ |V E |, every vertex v ∈ V E contains</formula><p>not only the entity value of a corresponding vertex v ∈ V E , but also the associated keyword(s) and type(s) of v. For the ease of presentation, hereafter we associate a single keyword and a single type to each entity. Our techniques can be efficiently extended to handle the general cases. Also for simplicity,  We assume that readers are familiar with the SPARQL query lanaguge; a brief review of SPARQL is also provided in the online appendix, in Section 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem statement</head><p>Intuitively, a keyword query against an RDF graph looks for (smallest) subgraphs that contain all the keywords. Given an RDF graph G = {V, E}, for any vertex v ∈ V , denote the keyword stored in v as w(v). For the ease of presentation, we assume each vertex contains a single keyword. However the solutions we have developed can be seamlessly applied to general cases where a vertex has multiple keywords or no keywords. Formally, a keyword search query q against an RDF data set G = {V, E} is defined by m unique keywords {w 1 , w 2 , . . . , w m }. A set of vertices {r, v 1 , . . . , v m } from V is a qualified candidate when:</p><p>• r ∈ V is called a root answer node which is reachable by</p><formula xml:id="formula_1">v i ∈ V for i ∈ [1, m] • w(v i ) = w i .</formula><p>If we define the answer for q as A(q) and the set of all qualified candidates in G with respect to q as C(q), then</p><formula xml:id="formula_2">A(q) = arg min g∈C(q) s(g), and s(g) = r, vi ∈ g, i = 1..m d(r, v i ) (1)</formula><p>where d(r, v i ) is the graph distance between vertices r and v i (when treating G as an undirected graph). Intuitively, this definition looks for a subgraph in an RDF graph that has minimum length to connect all query keywords from a root node r. In prior works concerning keyword search in RDF data, the graph distance of d(v 1 , v 2 ) is simply the shortest path between v 1 and v 2 in G, where each edge is assigned a weight of 1 (in the case of general graph <ref type="bibr" target="#b13">[14]</ref>, the weight of each edge could be different). Note that if v 1 and v 2 belong to disconnected parts of G, then d(v 1 , v 2 ) = +∞. Also note that this metric (i.e., eq. 1) is proposed by <ref type="bibr" target="#b13">[14]</ref> and has been used by prior work on keyword search in RDF data <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>This definition has a top-k version, where the query asks for the top k qualified candidates from C(q). Let the score of a qualified candidate g ∈ C(q) defined as s(g) in (1), then we can rank all qualified candidates in C(q) in an ascending order of their scores, and refer to the ith ranked qualified candidate as A(q, i). The answer to a top-k keyword search query q is an ordered set A(q, k) = {A(q, 1), . . . , A(q, k)}. A(q) is a special case when k = 1, and A(q) = A(q, 1). Lastly, we adopt the same assumption as in the prior works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref> that the answer roots in A are distinct.</p><p>Unless otherwise specified, all proofs in this paper appear in the online appendix, in Section 11. A complete version is also available online in our technical report <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>For keyword search on generic graphs, many techniques <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> assume that graphs fit in memory, an assumption that breaks for big RDF graphs. For instance, the approaches in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref> maintain a distance matrix for all vertex pairs, and clearly do not scale for graphs with millions of vertices. Furthermore, these works do not consider how to handle updates. A typical approach used here for keyword-search is backward search. Backward search when used to find a Steiner tree in the data graph is NP-hard. He et al <ref type="bibr" target="#b13">[14]</ref> proposed a tractable problem that does not aim to find a Steiner tree and can be answered by using backward search. In this work we extend this problem to large RDF graphs with rigorous analysis, and without depending on the distance matrix.</p><p>Techniques for summarizing large graph data to support keyword search were also studied <ref type="bibr" target="#b8">[9]</ref>. The graph data are first partitioned into small subgraphs by heuristics. In this version of the problem, the authors assumed edges across the boundaries of the partitions are weighted. A partition is treated as a supernode and edges crossing partitions are superedges. The supernodes and superedges form a new graph, which is considered as a summary the underlying graph data. By recursively performing partitioning and building summaries, a large graph can be eventually summarized with a small summary and fit into memory for query processing. During query evaluation, the correspondent supernodes containing the keywords being queried are unfolded and the respective portion of graph are fetched from external memory for query processing. This approach is proposed for generic graphs, and cannot be extended for RDF data, as edges in RDF data are predicates and are not weighed. Furthermore, the portion of the graph that does not contain any keyword being queried is still useful in query evaluation, therefore, this approach cannot be applied to address our problem. A summary built in this manner is not updatable.</p><p>Keyword search for RDF data has been recently studied in <ref type="bibr" target="#b23">[24]</ref>, which adopted the popular problem defintion from <ref type="bibr" target="#b13">[14]</ref> as we do in this paper. In this approach, a schema to represent the relations among entities of distinct types is summarized from the RDF data set. Backward search is first applied on the schema/summary of the data to identify promising relations which could have all the keywords being queried. Then, by translating these relations into search patterns in SPARQL expansion step queries and executing them against the RDF data, the actual subgraphs are retrived.</p><formula xml:id="formula_3">1 2 3 v 1 v 2 v 6 v 3 v 3 v 5 v 1 v 4 v 7 v 4 v 5 v 5 v 3 v 1 v 2 v 3 v 4 v 5 v 6 w 1 w 2 w 3 4 v 6 v 6 v 1 v 2 v 7 v 8 v 10 v 11 v 12 v 13 v 14 w 1 w 2 w 3 v 9 v 7 w 4 v 7 v 2 v 4 v 7 v 3 v 1 v 2 v 4 v 5 v 6 v 15 w 4 (a) (b) (c)</formula><p>The proposed summarization process in <ref type="bibr" target="#b23">[24]</ref> has a limitation: it bundles all the entities of the same type into one node in its summary, which loses too much information in data as to how one type of entities are connected to other types of entities. As a result, this approach could generates erroneous results (both false positives and false negatives). We have given one example in Figure <ref type="figure">2</ref>. As another example, consider Figure <ref type="figure">1</ref>. In this approach, all vertices of the type SpaceMission are represented by one node named SpaceMission in the summary. Then, summarizing the predicate previousMission connecting URI 3 and URI 8 results in a self-loop over the node SpaceMission in the summary, which is incorrect as such a loop does not exist in the data. To be more concrete, when a user asks for all the space missions together with their previous missions, the search pattern in SPARQL would be {?x PreviousMission ?x. ?x type SpaceMission.}, which is resultless in DBpedia. Furthermore, such a summary does not support updates. While we also built our summarization using type information, our summarization process uses different intuitions, which guarantees (a) the soundness of the results; and (b) the support of efficient updates.</p><p>There are other works related to keyword search on graphs. In <ref type="bibr" target="#b18">[19]</ref>, a 3-in-1 method is proposed to answer keyword search on structured, semi-structured and unstructured data. The idea is to encode the heterogeneous relations as a graph. Similar to <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, it also needs to maintain a distance matrix. An orthogonal problem to keyword search on graph is the study of different ranking functions. This problem is studied in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In this work, we adopt the standard scoring function in previous work in RDF <ref type="bibr" target="#b23">[24]</ref> and generic graphs <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE BASELINE METHOD</head><p>A baseline solution is based on the "backward search" heuristic. Intuitively, the "backward search" starts simultaneously from each vertex in the graph G that corresponds to a query keyword, and expands to its neighboring nodes recursively until a candidate answer is generated. A termination condition is used to determine whether the search is complete.</p><p>The state-of-the-art keyword search method on RDF graphs <ref type="bibr" target="#b23">[24]</ref> has applied the backward search idea. Their termination condition is to stop the search whenever the expansions originating from m vertices {v 1 , . . . , v m } (each corresponding to a distinct query keyword) meet at a node r for the first time, where {r, v 1 , . . . , v m } is returned as the answer. Unfortunately, this termination condition is incorrect.</p><p>Counter example. Consider the graph in Figure <ref type="figure" target="#fig_2">5</ref>(a) and a top-1 query q = {w 1 , w 2 , w 3 , w 4 }. The steps for the four backward expansions performed on Figure <ref type="figure" target="#fig_2">5</ref>(a) are shown in Figure <ref type="figure" target="#fig_2">5</ref>(b). Using the above termination condition, the backward expansions from the four vertices {v 1 , v 2 , v 6 , v 7 } covering the query keywords {w 1 , w 2 , w 3 , w 4 } meet for the first time in the second iteration, so the candidate answer g = {r=v 4 , v 1 , v 2 , v 6 , v 7 } is returned and s(g) = 8. However, if we continue to the next iteration, the four expansions will meet again at v 3 , with g = {r=v 3 , v 1 , v 2 , v 6 , v 7 } and s(g ) = 6, which is the best answer. One may argue that the graph covering the query keywords is still correctly identified. However, it will be problematic if we also consider the graph in Figure <ref type="figure" target="#fig_2">5</ref>(c) as input for the search. There, the best possible answer would be g = {r=v 12 , v 8 , v 10 , v 14 , v 15 } and s(g ) = 7 &lt; s(g). Hence, g will be declared as the top-1 answer for q instead of g , which is clearly incorrect. Furthermore, later we will explain that even if we fix this error in the terminating condition, their method <ref type="bibr" target="#b23">[24]</ref> may still return incorrect results due to the limitations in the summary it builds, as shown by the results in Figure <ref type="figure">2</ref>.</p><p>The correct termination. Next, we show the correct termination condition for the backward search on RDF data. The the complete algorithm appears in Algorithm 1.  pairs. A (vertex, distance) pair in the j-th entry of M [v] indicates a (shortest) path from vertex that reaches v in distance hops and it is the shortest possible path starting from any instance of w j (recall that there could have multiple copies of w j in G). Next, we also use</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: BACKWARD</head><formula xml:id="formula_4">Input: q = {w 1 , w 2 , . . . , w m }, G = {V, E} Output: top-k answer A(q) 1 Initialize {W 1 , ..W m }</formula><formula xml:id="formula_5">M [v][j] to indicate the j-th pair in M [v]. For instance in Figure 5(a), consider an element M [v 3 ] = {(v 1 , 1), (v 2 , 1), nil, (v 7 , 1)} in M .</formula><p>The entry indicates that v 3 has been reached by three expansions from vertices v 1 , v 2 and v 7 , containing keywords w 1 , w 2 and w 4 respectively -each can reach v 3 in one hop. However, v 3 has not been reached by any expansion from any vertex containing w 3 yet. The algorithm. With the structures in place, the algorithm proceeds in iterations. In the first iteration (lines 3-7), for each vertex v from W i and every neighbor u of v (including v itself), we add an entry (v, p ← {v, u}, d(p)) to the priority queue a i (entries are sorted in the ascending order of d(p) where p stands for a path and d(p) represents its length).</p><p>Next, we look for the newly expanded node</p><formula xml:id="formula_6">u in M . If u ∈ M , we simply replace M [u][i] with (v, d(p)) (line 7).</formula><p>Otherwise, we initialize an empty element for M [u] and set</p><formula xml:id="formula_7">M [u][i] = (v, d(p)) (line<label>6</label></formula><p>). We repeat this process for all W i 's for i = 1..m.</p><p>In the j-th (j &gt; 1) iteration of our algorithm (lines 8-12), we pop the smallest top entry of {a 1 ..a m } (line 9), say an entry (v, p = {v, . . . , u}, d(p)) from the queue a i . For each neighboring node u of u in G such that u is not in p yet (i.e., not generating a cycle), we push an entry (v, p∪{u }, d(p)+1) back to the queue a i (line 11). We also update M with u similarly as above (line 12). This concludes the j-th iteration.</p><p>In any step, if an entry M [u] for a node u has no nil pairs in its list of m (vertex, distance) pairs, this entry identifies a candidate answer and u is a candidate root. Due to the property of the priority queue and the the fact that all edges have a unit weight, the paths in M [u] are the shortest paths to u from m distinct query keywords. Denote the graph concatenated by the list of shortest paths in M [u] as g. We have:</p><formula xml:id="formula_8">Lemma 1 g = {r=u, v 1 , . . . , v m } is a candidate answer with s(g) = m i=1 d(u, v i ).</formula><p>A node v is not fully explored if it has not been reached by at least one of the query keywords. Denote the set of vertices that are not fully explored as V t , and the top entries from the m expansion queues (i.e., min-heaps) a 1 ..a m as (v 1 , p 1 , d(p 1 )), . . . , (v m , p m , d(p m )). Consider two cases: (i) an unseen vertex, i.e., v / ∈ M , will become the answer root; (ii) a seen but not fully expanded vertex v ∈ M will become the answer root. The next two lemmas bound the optimal costs for these two cases respectively. For the first case, Lemma 2 provides a lower bound for the best potential cost. Lemma 2 Denote the best possible candidate answer as g 1 , and a vertex v / ∈ M as the answer root of g 1 . Then it must have s(g</p><formula xml:id="formula_9">1 ) &gt; m i=1 d(p i ). For the second case, it is clearly that v ∈ V t . Assume the list stored in M [v] is (v b1 , d 1 ), . . . , (v bm , d m ).</formula><p>Lemma 3 shows a lower bound for this case. Lemma 3 Suppose the best possible candidate answer using such an v (v ∈ M and v ∈ V t ) as the answer root is g 2 , then</p><formula xml:id="formula_10">s(g 2 ) &gt; m i=1 f (v bi )d i + (1 -f (v bi ))d(p i ),<label>(2)</label></formula><p>where f</p><formula xml:id="formula_11">(v bi ) = 1 if M [v][b i ] =nil, and f (v bi ) = 0 otherwise. Notice that in Lemma 3, if M [v][b i ] = nil, then d(p i ) ≥ d i due to the fact that a i is a min-heap. It follows s(g 2 ) ≤ s(g 1 ).</formula><p>The termination condition. These v's represent all nodes that have not been fully explored. For case (i), we simply let s(g 1 ) = m i=1 d(p i ); for case (ii), we find a vertex with the smallest possible s(g 2 ) value w.r.t. the RHS of (2), and simply denote its best possible score as s(g 2 ).</p><p>Denote the kth smallest candidate answer identified in the algorithm as g, our search can safely terminate when s(g) ≤ min(s(g 1 ), s(g 2 )) = s(g 2 ). We denote this algorithm as BACKWARD. By Lemmas 1, 2, 3, we have Theorem 1:</p><p>Theorem 1 The BACKWARD method finds the top-k answers A(q, k) for any top-k keyword query q on an RDF graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TYPE-BASED SUMMARIZATION</head><p>The BACKWARD method is clearly not scalable on large RDF graphs. For instance, the keyword "Armstrong" appears 269 times in our experimental DBpedia dataset, but only one is close to the keyword "Apollo 11", as in Figure <ref type="figure">1</ref>. If we are interested in the smallest subgraphs that connect these two keywords, the BACKWARD method will initiate many random accesses to the data on disk, and has to construct numerous search paths in order to complete the search. However, the majority of them will not lead to any answers. Intuitively, we would like to reduce the input size to BACKWARD and apply BACKWARD only on the most promising subgraphs. We approach this problem by proposing a type-based summarization approach on the RDF data. The idea is that, by operating our keyword search initially on the summary (which is typically much smaller than the data), we can navigate and prune large portions of the graph that are irrelevant to the query, and only apply BACKWARD method on the smaller subgraphs that guarantee to find the optimal answers. The intuition. The idea is to first induce partitions over the RDF graph G. Keywords being queried will be first concatenated by partitions. The challenge lies on how to safely prune connections (of partitions) that will not result in any top-k answer. To this end, we need to calibrate the length of a path in the backward expansion that crosses a partition. However, maintaining the exact distance for every possible path is expensive, especially when the data is constantly changing. Therefore, we aim to distill an updatable summary from the distinct structures in the partitions such that any path length in backward expansion can be effectively estimated.</p><p>The key observation is that neighborhoods in close proximity surrounding vertices of the same type often share similar structures in how they connect to vertices of other types. Example 1. Consider the condensed view of Figure <ref type="figure">1</ref>. The graph in Figure <ref type="figure" target="#fig_4">6</ref>(a) is common for the 1-hop neighborhoods of URI 3 and URI 5 with the type SpaceMission.</p><p>This observation motivates us to study how to build a typebased summary for RDF graphs. A similar effort can be seen in <ref type="bibr" target="#b23">[24]</ref>, where a single schema is built for all the types of entities in the data. However, this is too restrictive as RDF data is known to be schemaless <ref type="bibr" target="#b9">[10]</ref>, e.g., entities of the same type do not have a unified property conformance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Outline and preliminaries</head><p>Our approach starts by splitting the RDF graph into multiple, smaller partitions. Then, it defines a minimal set of common type-based structures that summarizes the partitions.</p><p>Intuitively, the summary bookkeeps the distinct structures from all the partitions. In general, the keyword search can benefit from the summary in two perspectives. With the summary,</p><p>• we can obtain the upper and lower bounds for the distance traversed in any backward expansion without constructing the actual path (Section 6); and</p><p>• we can efficiently retrieve every partition from the data by collaboratively using SPARQL query and any RDF store without explicitly storing the partition (Section 14).</p><p>We first introduce two notions from graph theory: graph homomorphism and core. Homomorphism across partitions. As in Figure <ref type="figure" target="#fig_4">6</ref>(a), type vertices at close proximity are a good source to generate induced partitions of the data graph. However, if we were to look for such induced partitions that are exactly the same across the whole graph, we would get a large number of them. Consider another type-based structure in Figure <ref type="figure" target="#fig_4">6</ref> We consider discovering such embeddings between the induced partitions, so that one template can be reused to bookkeep multiple structures.</p><formula xml:id="formula_12">Definition 1 A graph homomorphism f from a graph G = {V, E} to a graph G = {V , E }, written as f : G → G , is a mapping function f : V → V such that (i) f (x) = x indicates that x and f (x) have the same type; and (ii) (u, v) ∈ E implies (f (u), f (v)</formula><p>) ∈ E and they have the same label. When such an f exists, we say G is homomorphic to G .</p><p>Intuitively, embedding G to G not only reduces the number of structures we need to keep but also preserve any path from G in G , as shown by Figure <ref type="figure" target="#fig_4">6</ref> (more expositions in Section 6). Finally, notice that homomorphism is transitive, i.e., G → G and G → G imply that G → G . Cores for individual partitions. A core is a graph that is only homomorphic to itself, but not to any one of its proper subgraphs (i.e., there is no homomorphism from a core to any of its proper subgraphs).</p><p>Definition 2 A core c of a graph G is a graph with the following properties: there exists a homomorphism from c to G; there exists a homomorphism from G to c; and c is minimal (in the number of vertices) with these properties. Intuitively, a core succinctly captures how different types of entities are connected, e.g., the partition in Figure <ref type="figure" target="#fig_6">7</ref>(b) is converted to its core in Figure <ref type="figure" target="#fig_6">7</ref>(a) by eliminating one branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Partition</head><p>The summarization process starts with splitting the data graph into smaller but semantically similar and edge-disjoint subgraphs. Given our observation that nodes with the same type often share similar type-neighborhoods, we induce a distinct set of partitions for G based on the types in G, using small subgraphs surrounding vertices of the same type. Our partitioning algorithm treats an input RDF dataset as a directed graph G concerning only the type information, i.e., we use the condensed view of an RDF graph. For any vertex that does not have a type specified by the underlying dataset, we assign an universal type NA to them. Notice that graph partitioning is a well studied problem in the literature, here we do not propose any new technique in that respect but rather focus on how to build semantically similar partitions for our purpose. The partitioning algorithm is shown in Algorithm 2.</p><p>Algorithm 2: Partition Input: G = {V, E}, α Output: A set of partitions in P 1 Let T = {T 1 , . . . , T n } be the distinct types in V ; 2 P ← ∅;</p><formula xml:id="formula_13">3 for T i ∈ T do 4 for v ∈ V i do 5 identify h(v, α) -the α neighborhood of v; 6 E ← E -{triples in h(v, α)} and P ← P ∪ h(v, α); 7 return P;</formula><p>In Algorithm 2, suppose G has n distinct number of types {T 1 , . . . , T n }, and we use the set V i to represent the vertices from V that have a type T i (line 4). We define the αneighborhood surrounding a vertex, where α is a parameter used to produce a set of edge disjoint partitions P over G. Formally, for any vertex v ∈ V and a constant α, the α-neighborhood of v is the subgraph from G obtained by expanding v with α hops in a breadth-first manner, denoted as h(v, α) (line 5), but subject to the constraint that the expansion only uses edges which have not been included by any partition in P yet. We define the i-hop neighboring nodes of v as the set of vertices in G that can be connected to v through a directed path with exactly i directed edges. Note that since we are using directed edges, it is possible the i-hop neighboring nodes of v is an empty set. Clearly the nodes in h(v, α) are a subset of the α-hop neighboring nodes of v (since some may have already been included in another partition).</p><p>To produce a partition P, we initialize P to be an empty set (line 2) and then iterate all distinct types (line 3). For a type T i and for each vertex v ∈ V i , we find its α-neighborhood h(v, α) and add h(v, α) as a new partition into P. The following lemma summarizes the properties of our construction: Lemma 4 Partitions in P are edge disjoint and the union of all partitions in P cover the entire graph G.</p><p>It is worth pointing out that Algorithm 2 takes an ad hoc order to partition the RDF data, i.e., visiting the set of entities from type 1 to type n in order. A different order to partition the data could lead to different performance in evaluating a keyword query. However, finding an optimal order to partition the RDF data set is beyond the scope the paper, therefore we decide not to expand the discussion on this issue.  Note that the order in which we iterate through different types may affect the final partitions P we build. But no matter which order we choose, vertices in the same type always induce a set of partitions based on their α-neighborhoods. For example, the partitions P of Figure <ref type="figure">1</ref> (as condensed in Figure <ref type="figure" target="#fig_10">4</ref>) are always the ones shown in Figure <ref type="figure" target="#fig_7">8</ref>, using α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summarization</head><p>The intuition of summarization technique is as follows. The algorithm identifies a set of templates from the set of partitions P. Such templates serve as a summary for the partitions. In addition, the summarization algorithm guarantees that every partition in P is homomorphic to one of the templates in the summary. As we will show in section 6, this property allows the query optimizer to (i) efficiently estimate any path length in the backward expansion without frequently accessing the RDF data being queried; and (ii) efficiently reconstruct the partitions of interest by querying the RDF data without explicitly storing and indexing the partitions.</p><p>We first outline our approach to summarize the distinct structures in a partition P. Then, we discuss how to make it more practical by proposing our optimizations. Finally, we discuss the related indices in Section 5.4. The general framework of our approach is shown in Algorithm 3.</p><p>Given a partition P, Algorithm 3 retrieves all the distinct structures and stores them in a set S. Algorithm 3 begins with processing partitions in P in a loop (line 2). For a partition h i , we use its core c to succinctly represent the connections between different types in h i (line 3). Once a core c is constructed for a partition, we scan the existing summary structures in S to check (a) if c is homomorphic to any existing structure s i in S; or (b) if any existing structure s i in S is homomorphic to c. In the former case, we terminate the scan and S remains intact (without adding c), as in lines 5-6; in the latter case, we remove s i from S and continue the scan, as in lines 7-8. When S is empty or c is not homomorphic to any of the structures in S after a complete scan on S, we add c into S. We repeat the procedure for all partitions in P. Improving efficiency and reducing |S|. There are two practical problems in Algorithm 3. First, the algorithm requires testing subgraph isomorphism for two graphs in lines 3, 5 and 7, which is NP-hard. Second, we want to reduce |S| as much as possible so that it can be cached in memory for query processing. The latter point is particularly important for RDF datasets that are known to be irregular, e.g., DBpedia.</p><p>The optimization is as follows. Before line 3 of Algorithm 3, consider each partition h(v, α) in P, which visits the αneighborhood of v in a breadth-first manner. We redo this traversal on h(v, α) and construct a covering tree for the edges in h(v, α), denoted as h t (v, α). In more detail, for each visited vertex in h(v, α), we extract its type and create a new node in h t (v, α) (even if a node for this type already exists). By doing so, we build a tree h t (v, α) which represents all the distinct type-paths in h(v, α). In the rest of the algorithm (lines 3-10), we simply replace h(v, α) with h t (v, α). Example 2. As in Figure <ref type="figure">9</ref>, a tree h t (v 1 , 2) is built for the partition h(v 1 , 2). Notice that the vertex v 4 is visited three times in the traversal (across three different paths), leading to three distinct nodes with type T 4 created in h t (v 1 , 2). In the same figure, a tree h t (v 5 , 2) is built from the partition h(v 5 , 2) and isomorphic to h t (v 1 , 2).</p><p>There are two motivations behind this move. First, using the covering tree instead of the exact partition potentially reduces the size of the summary S. As seen in Figure <ref type="figure">9</ref>, two partitions with distinct structures at the data level (e.g., h(v 1 , 2) and h(v 5 , 2)) could share an identical structure at the type level. Taking advantage of such overlaps is the easiest way to reduce the number of distinct structures in S. The second reason is efficiency. Whereas testing subgraph isomorphism is computationally hard for generic graphs, there are polynomial time solutions if we can restrict the testing on trees <ref type="bibr" target="#b21">[22]</ref> leading to better efficiency. For instance, to find the core of</p><formula xml:id="formula_14">vertex id type v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 9 v 8 T 1 T 2 T 3 T 4 T 1 T 2 T 3 T 4 T 4 T 4 T 1 T 3 T 2 T 4</formula><p>T </p><formula xml:id="formula_15">P 3 P 3 h(v 1 , 2) h(v 5 , 2) h t (v 1 , 2)/h t (v 5 , 2)</formula><p>Fig. <ref type="figure">9</ref>. A tree structure for two partitions.</p><p>a covering tree h t , it simply amounts to a bottom-up and recursive procedure to merge the homomorphic branches under the same parent node in the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Auxiliary indexing structures</head><p>To facilitate the keyword search, along with the summary S, we maintain three auxiliary (inverted) indexes.</p><p>A portal node is a data node that is included in more than one partitions (remember that partitions are edge-disjoint, not node disjoint). Intuitively, a portal node joins different partitions. A partition may have multiple portals but usually much less than the total number of nodes in the partition. Portal nodes allow us to concatenate different partitions. In the first index, dabbed portal index, for each partition h(v, α), we assign it a unique id, and associate it with the list of portals in the partition. In practice, since the partition root node v is unique in each partition, we can simply use it to denote the partition h(v, α) when the context is clear.</p><p>Recall that we use h t (v, α) to represent h(v, α), where a vertex in h(v, α) could correspond to more than one vertex in h t (v, α). σ(v i ) registers the fact that there are more than one v i in h t (v, α) and σ(v i ) also denotes the set of all v i s in h t (v, α). Without loss of generality, let Σ = {σ(v 1 ), σ(v 2 ), . . .} denote all the one-to-many mappings in a partition. For instance, consider h(v 1 , 2) and h t (v 1 , 2) in Figure <ref type="figure">9</ref>, Σ ← {σ(v 4 ) = {T 4 }}. The second index, dabbed partition index, is to map the partition root v of h(v, α) to its Σ. Intuitively, this index helps rebuild from h t (v, α) a graph structure that is similar to h(v, α) (more rigorous discussion in section 6).</p><p>The third index, dabbed summary index, maps data nodes in partitions to summary nodes in S. In particular, we assign a unique id sid to each summary in S and denote each node in S with a unique id nid. For any node u in a partition h(v, α), this index maps the node u to an entry that stores the partition root v, the id sid of the summary and the id nid of the summary node that u corresponds to. Notice that since h t (v, α) is built in a breadth-first traversal, we can easily compute the shortest path from v to any node in h t (v, α) using this index.</p><p>To obtain the homomorphic mappings from each h t (v, α) to a summary in S, one needs to maintain a log for all the homomorphisms found during the construction of S, as in lines 6 and 8. Once S is finalized, we trace the mappings in this log to find the mappings from data to summaries. As each partition (represented by its core) is either in the final S or is homomorphic to one other partition, the size of the log is linear to the size of G. An example for the log is in Figure <ref type="figure">10 (h i</ref> t is the covering tree for the i-th partition). It shows sets of trees (and their homomorphic mappings); each set is associated with a summary in S, that all trees in that set are homomorphic to.</p><p>To find the final mappings, we scan each set of trees in the log and map the homomorphisms of each entry in a set to the corresponding entry in S, i.e., the blue arrows in Figure <ref type="figure">10</ref>. We remove the log once all the mappings to S are found. </p><formula xml:id="formula_16">h</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">KEYWORD SEARCH WITH SUMMARY</head><p>Next, we present a scalable and exact search algorithm. It performs a two-level backward search: one backward search at the summary-level, and one at the data-level. Only for identified connected partitions that are found to contain all the distinct keywords at the summary-level and whose score could enter the top-k answers, do we initiate a backward search at the data-level on the selected partitions. Remember that path-length computation is at the heart of backward search and pruning. While working at the summary-level, exact path lengths are not available. Therefore, we first show how to estimate the path length of the actual data represented by our summary. Then, we proceed to describe the algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bound the shortest path length</head><p>At the summary-level, any shortest path in the underlying RDF graph must go through a number of partitions, and for each intermediate partition the path connects two of its portals, i.e., an entrance and an exit node. By construction, the shortest distance from the partition root v of a partition to any vertex u in the same partition can be computed with the summary index. By triangle inequality, the shortest distance d(v 1 , v 2 ) for any two vertices v 1 and v 2 in a partition with a partition root v can be upper bounded by</p><formula xml:id="formula_17">d(v 1 , v 2 ) ≤ d(v, v 1 )+d(v, v 2 )</formula><p>, and lower bounded by</p><formula xml:id="formula_18">d(v 1 , v 2 ) ≥ |d(v, v 1 ) -d(v, v 2 )</formula><p>|. Yet, a possibly tighter lower bound can be found by using the correspondent summary of the partition that is rooted at v and Lemma 5.</p><p>Lemma 5 Given two graphs g and h, if f : g → h, then ∀v 1 , v 2 ∈ g and their homomorphic mappings</p><formula xml:id="formula_19">f (v 1 ), f (v 2 ) ∈ h, d(v 1 , v 2 ) ≥ d(f (v 1 ), f (v 2 )).</formula><p>The homomorphic mappings between a partition h, its covering tree h t , and its summary s in S are shown in Figure <ref type="figure">11(a)</ref>. Notice that due to the optimization we employ in Section 5.3, there is no homomorphism from h to s, so that we can not apply Lemma 5 directly. To obtain a lower bound for the distance of any two vertices in h, we need to rebuild a homomorphic structure for h from its summary s and h t .</p><p>To do so, we first define a mapping function Join, which takes as input a graph g, a list of disjoint sets of vertexes {V t1 , V t2 , . . .} and outputs a new graph g , written as   <ref type="figure">11</ref>. Homomorphic mappings g = Join(g(V, E), {V t1 , V t2 , . . .}). In particular, V ti ⊆ V and vertexes in V ti are all of type t i . The function Join constructs g as follows:</p><formula xml:id="formula_20">h f 1 ←-h t f 2 -→ s (a) h f 1 ←-h t f 2 -→ s (b) Join(h t , Σ) → s ′ = Join(s, f 2 (Σ)) ↑ ↑</formula><p>1. Initialize g as g; 2. For the vertexes in V ti of g , merge them into a single node v i of type t i , and all edges incident to the vertexes in V ti are now incident to v i ; 3. Repeat step 2 for all is.</p><p>Notice that the function Join itself is a homomorphic mapping, which constructs a homomorphism from g to g . Also recall σ(x) of a partition h registers the fact that a type node x in h has more than one replicas in h t and hence σ(x) is a one-tomany mapping from x in h to the set of all vertexes of type x in h t and Σ of h t is the set of all such mappings. Example 3. Use the examples in Figure <ref type="figure">9</ref> for illustration. Join(h t (v 1 , 2), {σ(T 4 )}) rebuilds h(v 1 , 2) in Figure <ref type="figure">9</ref> and hence there is a homomorphism from h(v 1 , 2) to Join(h t (v 1 , 2), {σ(T 4 )}), i.e., isomorphism. On the other hand, Join(h t (v 5 , 2), {σ(T 4 )}) equals h(v 1 , 2). Although it does not reconstruct h(v 5 , 2) this time, the Join function in this case still produces a homomorphic mapping from h(v 5 , 2) to Join(h t (v 5 , 2), {σ(T 4 )}) since it is not hard to see that h(v 5 , 2) is homomorphic to h(v 1 , 2) in Figure <ref type="figure">9</ref>. More formally, we have: Lemma 6 For a partition h and its covering tree h t , there is a homomorphism from h to Join(h t , Σ).</p><p>In what follows, we show how to build a homomorphism from Join(h t , Σ) to a graph s derived from the summary s of h t . With this and by the transitivity of homomorphism and Lemma 6, it follows h is homomorphic to s and hence Lemma 5 can be applied (as shown in Figure <ref type="figure">11(b))</ref>.</p><p>By Algorithm 2, every h t from a partition h is homomorphic to a summary s in S (see the relations in Figure <ref type="figure">11(a)</ref>). Assume the homomorphism is</p><formula xml:id="formula_21">f 2 : h t → s. Given the Σ of a partition h, define f 2 (Σ) = {f 2 (σ(v)) | σ(v) ∈ Σ} where f 2 (σ(v)) = {f 2 (u) | u ∈ σ(v) ∧ u ∈ h t ∧ f 2 (u) ∈ s}, i.e.</formula><p>, the set of the mapped vertexes of σ(v) in s by the homomorphism f 2 . Further, we have the following result: Lemma 7 For a partition h, its covering tree h t and its summary s that has f 2 : h t → s, there is a homomorphism from Join(h t , Σ) to Join(s, f 2 (Σ)).</p><p>By Lemmas 6, 7 and the transitivity of homomorphism, a partition h is homomorphic to Join(s, f 2 (Σ)), as shown in Figure <ref type="figure">11(b)</ref>. Notice f 2 is a part of our summary index, which maps a vertex in data to a vertex in summary. Finally, given any two vertices in a partition h, their shortest distance can be (lower) bounded by combining Lemmas 5, 6, 7 and using any shortest path algorithm, e.g., Dijkstra's algorithm, to find the shortest distance between the correspondent mappings on Join(s, f 2 (Σ)). In practice, we use the larger lower bound from either the summary or the triangle inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The algorithm</head><p>The algorithm is in Algorithm 4, dabbed the SUMM method.  Data structures. Like the BACKWARD method in section 4, we define {W 1 , W 2 , . . . , W m }, where W i is the set of vertexes containing the keyword w i . We also initialize m priority queues {a 1 , . . . , a m } and maintain a set M of entries, one for each visited partition. Each entry in M stores a unique partition root followed by m lists. The i-th list records all the reachable vertexes containing keyword w i and through which other partitions they connect to the current partition in the search. An entry of M is in the form of quadruples -each can be represented as (u, S, d l , d u ). Here, the node u is the first vertex in the backward expansion and contains the keyword w i . The expansion reaches the current partition by routing through a sequence of the portals from some partitions, stored in S as a sequence of (portal, partition root) pairs. A sequence S defines a path (of partitions) that starts at u. Denote the lower and upper bounds for the path in S as d l and d u .</p><formula xml:id="formula_22">6 a i ⇐ (v, t); // enqueue 7 if v ∈ M then M [v] ← {nil, ...,</formula><p>Example 4. A sub-sequence {( , v a ), ( , v b )} of S indicates that the path first enters the partition rooted at v a and exits the partition from one of its portals . From the portal , this path then enters a partition rooted at v b and leaves the partition from the portal . We are interested in (lower and upper) bounding the shortest distance that connects two adjacent portals in S, e.g., d( , ) in the partition rooted at v b .</p><p>Example 5. In Figure <ref type="figure" target="#fig_0">12</ref>, assume m = 2 (i.e., the query has two keywords) and an entry in M for a partition rooted at v is shown as below. The entry records that there is a path (of</p><formula xml:id="formula_23">w1 w2 t1=(va, {( 2, v0)}, 5, 7) t2=(v b , {( 1, v4), ( 0, v5)}, 3, 5) t3=(vc, {( 3, v2)}, 5, 6)</formula><p>Fig.</p><p>12. An entry in M for the partition rooted at v partitions) from w 1 that reaches the current partition rooted at v. This path starts at v a , enters the concerned partition from portal 2 and has a length of at least 5 hops and at most 7 hops.</p><p>To reach the partition rooted at v, the path has already passed through a partition rooted at v 0 . Same for w 2 , the concerned partition is reachable from two paths starting at v b and v c respectively, both contain the keyword w 2 .</p><p>The algorithm. With the data structures in place, the algorithm proceeds in iterations.</p><p>• In the first iteration. For each vertex u from W i , we retrieve the partition root v that u corresponds to, from the summary index. Next, if there is an entry for v in M , we append a quadruple t=(u, {∅}, 0, 0) to the i-th list of the entry; otherwise we initialize a new entry for v in M (with m empty lists) and update the i-th list with t, as in lines 7-8. We also add an entry (v, t) to the priority queue a i (entries in the priority queue are sorted in ascending order by their lower bound distances in t's). We repeat this process for all W i 's for i = 1, . . . , m, which completes the first iteration (lines 3-8).</p><p>• In the j-th iteration. We pop the smallest entry from all a i 's, say (v, (u, S, d l , d u )) (line 10). We denote the partition rooted at v as the current partition. Denote the last pair in S as ( , v ), which indicates that the path leaves the partition rooted at v and enters the current partition using portal . Next, for the current partition, we find its portals L = { 1 , 2 , . . .} from the portal index. For each in L, we compute the lower and upper bounds for d( , ) (or d(u, ) if =nil) in the current partition using the approach discussed in section 6.1, denoted as d l and d u (line 13). A portal can connect the current partition to a set P of neighboring partitions. For each partition in P , denoted by its partition root v r , we construct a quadruple t=(u, S ∪ ( , v r ), d l + d l , d u + d u ) as in line 14. We also search for the entry of v r in M and update its i-th list with t in the same way as in the first iteration. However, if either of the following cases is satisfied, we stop updating the entry for v r in M : (i) adding to S generates a cycle; and (ii) d l + d l is greater than the k-th largest upper bound in the i-th list. Otherwise, we also push (v r , t) to the queue a i .</p><p>At any iteration, if a new quadruple t has been appended to the i-th list of an entry indexed by v in M , and all of its other m -1 lists are non-empty, then the partition rooted at v contains potential answer roots for the keyword query. To connect the partitions containing all the keywords being queried, we find all the possible combinations of the quadruples from the (m -1) lists, and combine them with t.</p><p>Each combination of the m quadruples denotes a connected subgraph having all the keywords being queried. Example 6. In Figure <ref type="figure" target="#fig_0">12</ref>, denote the new quadruple just inserted to the first list of an entry in M as t 1 . Since both of its lists are now non-empty, two combinations can be found, i.e., (t 1 , t 2 ) and (t 1 , t 3 ), which leads to two conjunctive subgraphs. Using the partition information in the quadruples, we can easily locate the correspondent partitions.</p><p>We study how to access the instance data for a partition in Section 7. Once the instance data from the selected partitions are ready, we proceed to the second-level backward search by applying the BACKWARD method to find the top-k answers on the subgraph concatenated by these partitions (line 20). In any phase of the algorithm, we track the top-k answers discovered in a priority queue.</p><p>• Termination condition. The following Lemmas provide a correct termination condition for the SUMM method.</p><p>Lemma 8 Denote an entry in the priority queue as (v, (u, S, d l , d u )), then for any v in the partition rooted at v and the length of any path starting from u and using the portals in S is d(u, v ) ≥ d l .</p><p>Lemma 9 Denote the top entry in the priority queue a i as (v, (u, S, d l , d u )), then for any explored path p from w i in the queue a i , the length of p, written as d(p), has d(p) ≥ d l .</p><p>We denote the set of all unexplored partitions in P as P t . For a partition h rooted at v that has not been included in M , clearly, h ∈ P t . The best possible score for an answer root in h is to sum the d l 's from all the top entries of the m expansion queues, i.e., a 1 , . . . , a m . Denote these m top entries as (v1, (u1,</p><formula xml:id="formula_24">S 1 , d 1 l , d 1 u )), . . ., (vm, (um, S m , d m l , d m u</formula><p>)), respectively. Then, Lemma 10 Let g 1 be a possible unexplored candidate answer rooted at a vertex in a partition h, with h ∈ P t ,</p><formula xml:id="formula_25">s(g 1 ) &gt; m i=1 d i l .<label>(3)</label></formula><p>Next, consider the set of partitions that have been included in M , i.e., the set P -P t . For a partition h ∈ P -P t , let the first quadruple from each of the m lists for its entry in M be:</p><formula xml:id="formula_26">t1 = (û1, Ŝ1, d1 l , d1 u ), . . . , tm = (ûm, Ŝm, dm l , dm</formula><p>u ) (note that due to the order of insertion, each list has been implicitly sorted by the lower bound distance dl in ascending order), where t j = nil if the j-th list is empty. Then, we have: Lemma 11 Denote the best possible unexplored candidate answer as g 2 , which is rooted at a vertex in the partition h, where h ∈ P -P t , then</p><formula xml:id="formula_27">s(g 2 ) &gt; m i=1 f (t i ) di l + (1 -f (t i ))d i l ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_28">f (ti)=1 if ti =nil otherwise f (ti)=0.</formula><p>Finally, we can derive the termination condition for the search. The termination condition. We denote the score of the best possible answer in an unexplored partition as s(g 1 ), as defined by the RHS of (3); and the score of the best possible answer in all explored partitions as s(g 2 ), as defined by the RHS of (4). Denote the candidate answer with the k-th smallest score during any phase of the algorithm as g. Then, the backward expansion on the summary level can safely terminate when s(g) ≤ min(s(g 1 ), s(g 2 )). By Lemmas 10 and 11, we have:</p><p>Theorem 2 SUMM finds the top-k answers A(q, k) for any top-k keyword search query q on an RDF graph.</p><p>Sections 12 and 13 in the online appendix discuss the complexity of SUMM and further elaborate its correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACCESSING DATA AND UPDATE</head><p>The SUMM algorithm uses the summary of the RDF data to reduce the amount of data accessed in the BACKWARD method. For the algorithm to be effective, we should be able to efficiently identify and retrieve the instance data from selected partitions. One option is to store the triples by partitions and index on their partition ids, i.e., adding another index to the algorithm. But then whenever an update on the partition happens, we need to update the index. Furthermore, the approach enforces a storage organization that is particular to our methods (i.e., not general). In what follows, we propose an alternative efficient approach that has no update overhead and requires no special storage organization. Our approach stores the RDF data in an RDF store and works by dynamically identifying the data of a partition using appropriately constructed SPARQL queries that retrieve only the data for that partition.</p><p>Since graph homomorphism is a special case of homomorphism on relational structure (i.e., binary relations) <ref type="bibr" target="#b15">[16]</ref> and the fact that relational algebra <ref type="bibr" target="#b20">[21]</ref> is the foundation of SPARQL, we can use the Homomorphism Theorem <ref type="bibr" target="#b0">[1]</ref> to characterize the results of two conjunctive SPARQL queries.</p><p>Theorem 3 Homomorphism Theorem <ref type="bibr" target="#b0">[1]</ref>. Let q and q be relational queries over the same data D. Then q (D) ⊆ q(D) iff there exists a homomorphism mapping f : q → q .</p><p>Recall that f 1 : h t → h (see Figure <ref type="figure">11(a)</ref>) and for each h t , we extract a core c from h t . By definition, c is homomorphic to h t , thus c is homomorphic to h (transitivity). Using c as a SPARQL query pattern can extract h due to Theorem 3. SELECT * WHERE{URI 5 name "A1". URI 5 type S. OPTIONAL{URI 5 launchPad ?x. ?x type B.} OPTIONAL{URI 5 booster ?y. ?y type R} OPTIONAL{URI 5 crew ?z. ?z type C} . OPTIONAL{URI 5 previousmission ?m. ?m type S} . } Fig. <ref type="figure" target="#fig_1">13</ref>. A query to retrieve the targeted partition.</p><p>There are two practical issues the need our attention. First, there is usually a many-to-one mapping from a set of h t 's to the same core c -leading to a low selectivity by using c as the query pattern. To address this issue, we can bind constants from the targeted partition to the respective variables in query pattern. These constants include the root and the portals of the targeted partition which are retrievable from the indexes. The second issue is that in our construction of S, we do not explicitly keep every c. Instead, a core c is embedded (by homomorphism) to a summary s ∈ S, where c is a subtree of s. To construct a SPARQL query from s, we first need to find a mapping for the partition root in s, then the triple patterns corresponding to the subtree in s are expressed in (nested) OPTIONALs from the root to the leaves. For example, the SPARQL query for the partition rooted at URI 5 in Figure <ref type="figure" target="#fig_7">8</ref> can be constructed by using the summary in Figure <ref type="figure" target="#fig_6">7</ref>(a). Notice that URI 5 is bound to the root to increase selectivity.</p><p>Our approach also supports efficient updates, which is addressed in Section 14 in the online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTS</head><p>We implemented the BACKWARD and SUMM methods in C++. We also implemented two existing approaches proposed in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b13">[14]</ref>. We denote them as SCHEMA and BLINKS respectively. All experiments were conducted on a 64-bit Linux machine with 6GB of memory. Datasets: We used large sythetic and real RDF data sets for experiments. Sythetic data sets are generated by popular RDF benchmarks. The first is the Lehigh University Benchmark (LUBM) <ref type="bibr" target="#b12">[13]</ref>, which models universities with students, departments, etc. With its generator, we created a default dataset of 5 million triples and varied its size up to 28 million triples. The second is the Berlin SPARQL Benchmark (BSBM) <ref type="bibr" target="#b4">[5]</ref>, which models relationships of products and their reviews. Unlike LUBM where data are generated from a fixed set of templates, BSBM provides interfaces to scale the data on both size and complexity of its structure. In particular, it provides means for adding new types in the data. The rest of the data sets are popular real RDF data sets, i.e., Wordnet, Barton and DBpedia Infobox. The number of distinct types for the data sets are shown in Figure <ref type="figure" target="#fig_10">14</ref> and their sizes are reported in Figure <ref type="figure" target="#fig_2">15</ref>.</p><p>Notice DBpedia and BSBM are very irregular in structure, i.e., both have more than 1, 000 distinct types. BSBM is also large in size (70 million triples by default).</p><p>Implementation and setup: We used the disk-based B + -tree implementation from the TPIE library to build a Hexstorelike <ref type="bibr" target="#b24">[25]</ref> index on the RDF datasets. For subgraph isomorphism test, we used the VFLib. We assume each entity in the data has one type. For an entity that has multiple types, we bind the entity to its most popular type. To store and query RDF data with SPARQL, we use Sesame <ref type="bibr" target="#b5">[6]</ref>. In all experiments, if not otherwise noted, we built the summary for 3-hop neighbors, i.e., α = 3, and set k = 5 for top-k queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Evaluating summarization algorithms</head><p>Time on the summarization process. We start with a set of experiments to report the time (in log scale) in building a summary. For LUBM, we vary the size of the data from 100 thousand triples to 28 million triples. In Figure <ref type="figure" target="#fig_13">16</ref>(a), we plot the total time for building the summary, which     includes: the time spent to find homomorphic mappings (i.e., performing subgraph isomorphism tests) and the time spent for the rest of operations, e.g., partitioning the graph, and constructing the inverted indexes. The latter cost dominates the summarization process for all the cases in LUBM datasets.</p><p>The same trend can also be observed in all the other datasets, as shown in Figure <ref type="figure" target="#fig_13">16</ref>(b). Notice that the summary used by SUMM is built once and thereafter incrementally updatable whenever the data get updated. For comparison, we study the summarization performance for the SCHEMA method. The comparisons are shown in Figure <ref type="figure" target="#fig_15">17</ref>. Regardless of the quality of the summary, the SCHEMA method in general performs an order of magnitude faster than our SUMM method across all the data sets we experimented. However, as it will become more clearly shortly, while the summary built by SCHEMA might be useful in some settings, it does not yield correct results in all our experimental data sets.</p><p>Size of the summary. As the SCHEMA method generates one (type) node in the summary for all the nodes in the data that have the same type, the size of summary (in terms of number of nodes) is equal to the number of distinct types from the data, as shown in Figure <ref type="figure" target="#fig_10">14</ref>. For our summarization technique, we plot the number of partitions and the number of summaries in Figures <ref type="figure" target="#fig_16">18(a</ref>) and 18(b). In Figure <ref type="figure" target="#fig_16">18</ref>(a) for LUBM, the summarization technique results in at least two orders less distinct structures comparing to the number of partitions. Even  in the extreme case where the dataset is partitioned into about a million subgraphs, the number of distinct summaries is still less than 100. In fact, it remains almost a constant after we increase the size of the data set to 1 million triples. This is because LUBM data is highly structured <ref type="bibr" target="#b9">[10]</ref>. Not all RDF data sets are as structured as LUBM. Some RDF datasets like BSBM and DBpedia are known to have a high variance in their structuredness <ref type="bibr" target="#b9">[10]</ref>. In Figure <ref type="figure" target="#fig_16">18(b)</ref>, we plot the number of distinct summaries for other datasets after applying the SUMM method. For Wordnet and Barton, SUMM distills a set of summaries that has at least three orders less distinct structures than the respective set of partitions. Even in the case of DBpedia and BSBM, our technique still achieves at least one order less distinct structures than the number of the partitions, as shown in Figure <ref type="figure" target="#fig_16">18(b)</ref>.</p><p>In Figures <ref type="figure" target="#fig_18">19(a</ref>) and 19(b), we compare the number of triples stored in the partitions and in the summary. Clearly, the results show that the distinct structures in the data partitions can be compressed with orders-of-magnitude less triples in the summary, e.g., at least one order less for DBpedia Infobox, and at least three orders less for LUBM, Wordnet, Barton and BSBM. Since the summaries are all small, this suggests that we can keep the summaries in main memory to process keyword query. Therefore, the first level of backward search for SUMM method can be mostly computed in memory.     types of entities in an RDF data set implicitly adds more variances to the data and hence makes the summarization process more challenging. To see the impact on SUMM, we leverage the BSBM data generator. In particular, we generate four BSBM data sets with the number of distinct types ranged from 100 to 1, 500. The results from SUMM on these data sets are shown in Figures <ref type="figure" target="#fig_19">20(a</ref> Impact of α. Since trends are similar, we only report the impact of α on two representative data sets, i.e., LUBM and DBpedia. In Figures <ref type="figure" target="#fig_22">21(a</ref>) and 21(b), we report the impact of α (a parameter on the max number of hops in each partition, see Section 5.2) on the size of summary. Intuitively, the smaller α is, the more similar the α-neighborhoods are, leading to a smaller size of summary after performing summarization. This is indeed the case when we vary α for all the data sets. The smallest summary is achieved when α = 1 in both Figures <ref type="figure" target="#fig_22">21(a</ref>) and 21(b). Notice that there is a trade-off between the size of the summary and the size of the auxiliary indexes. A smaller partition implies that more nodes become portals, which increases the size of the auxiliary indexes. On the other hand, increasing α leads to larger partitions in general, which adds more variances in the structure of the partitions and inevitably leads to a bigger summary. However in practice, since the partitions are constructed by directed traversals on the data, we observed that most of the directed traversals terminate after a few hops. For instance, in LUBM, most partitions stop growing when α &gt; 3. A similar trend is visible in Figures <ref type="figure" target="#fig_22">21(b</ref>). When we increase α, the number of distinct structures increases moderately.</p><p>Overheads of auxiliary indexes. In Figures <ref type="figure">22(a</ref>) and 22(b), we study the size of the auxiliary indexes. Figure <ref type="figure">22(a)</ref> shows that for LUBM, the size of the auxiliary indexes is one order less  than the respective data when we vary its size up to 28 million triples. Similar trends can be observed in Figure <ref type="figure">22</ref>(b) for the other datasets. This is because that for all indexes, we do not explicitly store the edges of the RDF data that usually dominate the cost in storing large graphs. In Figure <ref type="figure" target="#fig_1">23</ref>, we report the breakdown of the inverted indexes for all the datasets. The most costly part is to store the mappings in the summary index (i.e., SUMMIDX in the figure) and the other indexes are all comparably small in size. Thus, to efficiently process query, we can keep all but the summary index in main memory.</p><p>We also compare in Figure <ref type="figure" target="#fig_10">24</ref> the indexing overhead of different methods as we vary the data size for LUBM. Notice that BLINKS is the most costly method in terms of storage (i.e., it demands at least one order of magnitude more space), as its distance matrix leads to a quadratic blowup in indexing size. BLINKS is no doubt the faster method for small data, but it clearly does not scale with large RDF datasets. Therefore, we do not report BLINKS in the evaluation of query performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Query performance</head><p>In this section, we study the performance of top-k keyword search using SUMM and BACKWARD. In particular, we compare the proposed methods with SCHEMA, which is the only existing method that can apply to large RDF datasets. To this end, we design a query workload that has various characteristics. Figure <ref type="figure" target="#fig_25">25</ref> lists 12 typical queries, together with the number of keyword occurrences in the datasets. For LUBM, all the keywords are selected from the first university in the data set, except for keywords with respect to publications 17 and 18. For the two indicated keywords, we select one copy of each publication from the first university and pick the rest of them randomly from other universities. This is to simulate the cases in real data sets where not all the keywords in a query are close to each other. Notice that in LUBM, keywords such as publications 17 or professor 9 are associated with multiple entities. For the other data sets, we pick two representative queries for each of them. The queries are shown in Figure <ref type="figure" target="#fig_25">25</ref>. In particular, the second column # nodes shows the number of occurrences for the keywords being queried in the respective data set. For long running queries, we terminate the executions after 1000 seconds. The response times are in log scale.</p><p>We first use SCHEMA to answer the queries in Figure <ref type="figure" target="#fig_25">25</ref>. SCHEMA generates a set of k SPARQL queries for each    keyword query. Evaluating these queries are supposed to return k answer roots ranked by the scores. However, even if we have fixed the incorrect termination condition in SCHEMA (as discussed in Section 4), our observation is that SCHEMA still returns incorrect results for all of the queries, as we have indicated in Figure <ref type="figure">2</ref>. This can be explained by the way it summarizes the data, as all nodes of the same type are indistinguishably mapped to the same type node in the summary. For instance, every FullProfessor has a publication in LUBM, whereas this does not mean that FullProfessor9 has a Publication17 for Q 2 . Nevertheless, we report the response time of the SCHEMA method together with other methods for comparison. In what follows, we focus on discussing the query performance for the BACKWARD and SUMM methods, both of which have provable guarantees on the correctness of the query results. Their performance on the sampled query workload are plotted on Figures <ref type="figure" target="#fig_27">26(a</ref>) and 26(b) respectively. The efficiency of a keyword search is determined by a collection of factors <ref type="bibr" target="#b13">[14]</ref>, with no single factor being the most deterministic one. In particular, we observe that for selective keywords (i.e., keywords that have fewer occurrences in the data) and especially those that are close to each others, e.g., Q 1 , Q 5 , both BACKWARD and SUMM answer the queries efficiently. In some cases, e.g., Q 5 , BACKWARD outperforms SUMM since SUMM uses a lower bound to decide when to terminate. This inevitably requires SUMM to access more data than what is necessary to correctly answer the query.</p><p>However, being selective alone does not necessarily lead to better query performance, especially if the keyword being queried corresponds to a hub node that has a large degree, e.g., the Department0 in Q 4 . On the other hand, as the keywords being queried become non-selective, e.g., Q 3 or Q 11 , or the keywords are far away from one another, e.g., the solo dance and the baseball team in Q 6 , the SUMM approach generally performs much better than the BACKWARD method. This is because only when the connected partitions are found to contain all the keywords, the SUMM needs to access the whole subgraph on disk. This leads to savings in dealing with keywords that are resultless, e.g., most of the occurrences for publications 17 and 18 in Q 2 -Q 4 fall into this category. In addition, at the partition level, the backward expansion in the SUMM approach can be done almost completely in memory as the major indexes for expansion are lightweight (as shown in Section 8.1) and therefore can be cached in memory for query evaluation. In such cases, i.e., Q 2 -Q 4 and Q 6 -Q 12 , we observe that the SUMM approach performs much better.</p><p>In Figures <ref type="figure" target="#fig_29">27 (a</ref>) and (b), we investigate the query performance while varying the size of the LUBM data set. As shown in the figure, for both SUMM and BACKWARD, the cost of query evaluation generally becomes more expensive as the size of the data increases. In contrast to BACKWARD, the difference in time for the SUMM method is relatively moderate as the size of the data changes. This is because for SUMM, keywords that are far apart can be pruned in the first level of the backward search. Unlike SUMM, BACKWARD has to pay equal effort in dealing with each copy of every keyword being queried, leading to a much slower convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We studied the problem of scalable keyword search on big RDF data and proposed a new summary-based solution: (i) we construct a concise summary at the type level from RDF data; (ii) during query evaluation, we leverage the summary to prune away a significant portion of RDF data from the search space, and formulate SPARQL queries for efficiently accessing data. Furthermore, the proposed summary can be incrementally updated as the data get updated. Experiments on both RDF benchmark and real RDF datasets showed that our solution is efficient, scalable, and portable across RDF engines. An interesting future direction is to leverage the summary for optimizing generic SPARQL queries on large RDF datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Keywords in a sample from DBpedia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Distance matrix<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Backward search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and m min-heaps {a 1 , ..a m }; 2 M ← ∅; // for tracking potential C(q) 3 for v ∈ W i and i = 1..m do 4 for ∀u ∈ V and d(v, u) ≤ 1 do 5 a i ⇐ (v, p ← {v, u}, d(p) ← 1) ; // enqueue 6 if u ∈ M then M [u] ← {nil, ..(v, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Graph homomorphism across summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b), which is extracted from 1-hop neighbors around the vertex URI 3 in Figure 1. Notice the two graphs are different, however Figure 6(a) is a subgraph of Figure 6(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Build a core (a) from (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Partitions P of the RDF data in Figure 1, α = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 3 : 3 c 8 S 9 S</head><label>3389</label><figDesc>Summarize structures in P Input:P = {h(v 1 , α), h(v 2 , α), . . .} Output: A set of summaries in S 1 S ← ∅; 2 for h i ∈ P, i = 1, . . . , |P| do ← core(h i ); //see discussion on optimization4 for s j ∈ S, j = 1, . . . , |S| do 5 if f : c → s j then 6 goto line 2; // also bookkeep f : c → sj 7 else if f : s j → c then ← S -{s j }; //also bookkeep f : sj → c ← S ∪ {c}; 10 return S;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.</head><label></label><figDesc>Fig.11. Homomorphic mappings g = Join(g(V, E), {V t1 , V t2 , . . .}). In particular, V ti ⊆ V and vertexes in V ti are all of type t i . The function Join constructs g as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 4 :</head><label>4</label><figDesc>SUMM Input: q = {w 1 , w 2 , . . . , w m }, G = {V, E} Output: top-k answer A 1 initialize {W 1 , ..W m } and m min-heaps {a 1 , ..a m }; 2 M ← ∅; // for tracking partitions 3 for u ∈ W i and i = 1..m do 4 if u ∈ h(v, α) then 5 t ← (u, {∅}, 0, 0);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Number of distinct types in the datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Time for the summary construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Time for the summarization: SUMM vs. SCHEMA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. # subgraphs: partitions vs. summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. # triples: partitions vs. summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Vary the number of types in BSBM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>α</head><label></label><figDesc>Number of summaries SUMM (b) DBPedia Infobox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Impact of α to the size of summary. Impact from the distinct number of types. Adding new</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 22 .Fig. 24 .</head><label>2224</label><figDesc>Fig. 22. Size of the auxiliary indexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>) and (b). As we are injecting more randomness into the data by adding more types, the size of the summary increases moderately. Consistent to our observations for DBpedia in Figures18(b) and 19(b), SUMM efficiently summarizes the BSBM data sets with orders less triples, even when the data set has more than a thousand distinct types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 25 .</head><label>25</label><figDesc>Fig. 25. Sample query workload</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 26 .</head><label>26</label><figDesc>Fig. 26. Query performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 27 .</head><label>27</label><figDesc>Fig. 27. Query performance vs. data size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Frequently used notations.</figDesc><table /><note><p>hereafter, we use G = {V, E} to represent the condensed view of an RDF graph.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>., nil};</figDesc><table><row><cell>7</cell><cell>else M [u][i] ← (v, 1);</cell><cell>↑the i-th entry</cell></row><row><cell cols="3">8 while not terminated and A not found do 9 (v, p, d(p)) ← pop(arg min m i=1 {top(a i )}); for ∀u ∈ V and d(v, u) = 1 and u ∈ p do a i ⇐ (u, p ∪ {u}, d(p) + 1); update M the same way as in lines 6 and 7;</cell></row><row><cell></cell><cell>return A (if found) or nil (if not);</cell><cell></cell></row><row><cell cols="3">Data structures. Given q = {w 1 , . . . , w m } and a (condensed) RDF graph G = {V, E}, we use W i to denote the set of vertices in V containing the keyword w i (line 1). We initialize</cell></row><row><cell cols="3">m empty priority queues (e.g., min-heaps) {a 1 , ..a m }, one for each query keyword (line 1). We also maintain a set</cell></row><row><cell cols="3">M of elements (line 2), one for each distinct node we have</cell></row><row><cell cols="3">explored so far in the backward expansion to track the state</cell></row><row><cell cols="3">of the node, i.e., what keywords are reachable to the node</cell></row><row><cell cols="3">and their best known distances. In what follows, we use M [v]</cell></row><row><cell cols="3">to indicate the bookkeeping for the node v. Specifically, in</cell></row><row><cell cols="3">each element of M , we store a list of m (vertex, distance)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>↑</figDesc><table><row><cell></cell><cell>A summary</cell><cell></cell><cell></cell></row><row><cell cols="2">to build s, not kept intermediate data</cell><cell></cell><cell>Apply Lemma 5</cell></row><row><cell>h: partition</cell><cell>h t : tree</cell><cell>s: summary</cell><cell>→: homomorphism</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>t, ..., nil};</figDesc><table /><note><p>8 else M [v][i] ← t; ↑the i-th entry 9 while not terminated and A not found do (v, (u, S, d l , d u )) ← pop(arg min m i=1 {top(a i )}); Denote the last entry in S as ( , v ) and L = { 1 , 2 , . . .} be the portals in the partition rooted at v; for ∀ ∈ L do compute d l and d u for d( , ) or d(u, ); let t ← (u, S ∪ ( , v r ), d l + d l , d u + d u ); update M [v r ] with t; // see discussions if M [v r ] is updated and nil ∈ M [v r ] then a i ⇐ (v r , t); // enqueue for each new subgraph g incurred by t do retrieve g from data; apply BACKWARD on g and update A; return A (if found) or nil (if not);</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>[Research 5 , FullProf 9 , Pub 17 ] (9,4,83) L Q 3 [FullProf 9 , Grad 0 , Pub 18 , Lec 6 ] (4,15,40,5) L Q 4 [Dep 0 , Grad 1 , Pub 18 , AssocProf 0 ] (1,15,40,15) L Q 5 [Afghan, Afghanistan, al-Qaeda, al-Qa'ida] (6,3,3,2) W Q 6 [3 rd base, 1 st base, baseball team, solo dance] (14,13,17,4) W</figDesc><table><row><cell>Query</cell><cell># nodes</cell><cell>Dataset</cell></row><row><cell>Q 1 [Pub 19 , Lec 6 ]</cell><cell>(20,13)</cell><cell>L</cell></row><row><cell>Q 2 Q 7 [Knuth, Addison-Wesley, Number theory]</cell><cell>(1,1,35)</cell><cell>B</cell></row><row><cell>Q 8 [Data Mining, SIGMOD, Database Mgmt.]</cell><cell>(166,1,4)</cell><cell>B</cell></row><row><cell>Q 9 [Bloomberg, New York City, Manhattan]</cell><cell>(1,7,108)</cell><cell>D</cell></row><row><cell>Q 10 [Bush, Hussein, Iraq]</cell><cell>(1,1,48)</cell><cell>D</cell></row><row><cell>Q 11 [deflation, railroaders]</cell><cell>(32,70)</cell><cell>S</cell></row><row><cell>Q 12 [ignitor, microprocessor, lawmaker]</cell><cell>(3,110,43)</cell><cell>S</cell></row><row><cell cols="3">L:LUBM W:Wordnet B:Barton D:DBpedia Infobox S:BSBM</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wangchao Le holds a PhD degree from the University of Utah, specialized in data management and analysis. Wangchao's research interests lie in database systems, in particular, temporal and multi-version data, query processing and optimization for large graph data, and semantic web. His dissertation is about supporting scalable data analytics on large linked data. Wangchao is currently working at Microsoft for its big data and search infrastructure. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Foundations Of Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vianu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The input/output complexity of sorting and related problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1116" to="1127" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DBXplorer: enabling keyword search over relational databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keyword searching and browsing in databases using banks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhalotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hulgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nakhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sudarshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The berlin SPARQL benchmark</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal On Semantic Web and Information Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sesame: A generic architecture for storing and querying RDF and RDF schema</title>
		<author>
			<persName><forename type="first">J</forename><surname>Broekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keyword-based search and exploration on databases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keyword search on structured and semi-structured data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keyword search on external memory data graphs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sudarshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Apples and oranges: a comparison of RDF benchmarks and real RDF datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kementsietsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effectively interpreting keyword queries on rdf databases with a rear view</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anyanwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Keyword proximity search in complex data graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Golenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sagiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LUBM: A benchmark for OWL knowledge base systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blinks: ranked keyword searches on graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient IR-style keyword search over relational databases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the algebraic structure of combinatorial problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jeavons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Keyword search in graphs: Finding r-cliques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kargar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scalable Keyword Search on Big RDF Data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kementsietsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<ptr target="http://www.cs.utah.edu/∼lifeifei/papers/rdfkeyword-r2.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ease: Efficient and adaptive keyword search on unstructured, semi-structured and structured data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SPARK: A keyword search engine on relational databases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">From SPARQL to rules (and back)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Polleres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster subtree isomorphism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiway slca-based keyword search in xml data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Goenka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Top-k exploration of query candidates for efficient keyword search on graph-shaped (RDF) data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hexastore: sextuple indexing for semantic web data management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
