<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELLE: Efficient Lifelong Pre-training for Emerging Data</title>
				<funder>
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_9DQPf2j">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Pattern Recognition Center, WeChat AI, Tencent Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-12">12 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ELLE: Efficient Lifelong Pre-training for Emerging Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-12">12 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.06311v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pretraining for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pretrained domain prompts, which disentangle the versatile knowledge learned during pretraining and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ ELLE. * Indicates equal contribution. ? Corresponding author. ? Part of the work was done while Peng Li was working at Tencent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (PLM) have broken the glass ceiling for various natural language processing (NLP) tasks <ref type="bibr" target="#b33">(Radford et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b14">Han et al., 2021)</ref>. However, most of the existing PLMs are typically trained with a static snapshot of the web information, ignoring that in real-world scenarios, streaming data from various sources may continuously grow, e.g., the gatherings of literary works <ref type="bibr" target="#b50">(Zhu et al., 2015)</ref>, news articles <ref type="bibr" target="#b48">(Zellers et al., 2019)</ref> and science papers <ref type="bibr" target="#b13">(Lo et al., 2020)</ref>. In addition, the distribution of incoming data may also vary over time. This requires PLMs to continually integrate the information from all the sources to grasp the versatile structural and semantic knowledge comprehensively, so that PLMs could utilize the proper knowledge to boost the performance in various downstream tasks.</p><p>A simple yet effective way to integrate all the information is to pre-train PLMs on all the existing data exhaustively. However, such a process is computationally expensive <ref type="bibr" target="#b37">(Schwartz et al., 2019)</ref>, especially under the information explosion era when tremendous data is continually collected. This leaves us an important question: with limited computational resources, how can we efficiently adapt PLMs in a lifelong manner? We formulate it as the efficient lifelong pre-training problem. Similar to conventional lifelong learning, PLMs are expected to continually abosrb knowledge from emerging data, and in the meantime, mitigate the catastrophic forgetting <ref type="bibr" target="#b29">(McCloskey and Cohen, 1989</ref>) on previously learned knowledge.</p><p>In addition, efficient lifelong pre-training poses two new challenges: (1) efficient knowledge growth. When the overall data scale accumulates to a certain magnitude, packing more knowledge into a fixed-sized PLM becomes increasingly hard, which significantly impacts the efficiency of PLM's knowledge growth. This is because larger PLMs show superior sample efficiency and training efficiency over their smaller counterparts <ref type="bibr" target="#b20">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b25">Li et al., 2020)</ref> due to overparameterization <ref type="bibr" target="#b2">(Arora et al., 2018)</ref>. That is, larger PLMs learn knowledge in a more efficient way. Therefore, timely model expansions are essential for efficient knowledge growth; (2) proper knowledge stimulation. During pre-training, various knowledge from all domains is packed into PLMs hastily. However, a certain downstream task may largely require the knowledge from a specific domain. Thus it is essential for PLMs to disentangle different kinds of knowledge and properly stimulate the needed knowledge for each task.</p><p>In this paper, we propose ELLE, targeting at Efficient LifeLong pre-training for Emerging data. Specifically, (1) to facilitate the efficiency of knowledge growth, we propose the function preserved model expansion to flexibly expand an existing PLM's width and depth. In this way, we increase PLM's model size and thus improve its training efficiency. Before being adapted to a new domain, the expanded PLM performs a function recovering warmup to regain the functionality of the original PLM; (2) for proper knowledge stimulation, we pre-implant domain prompts during pre-training to prime the PLM which kind of knowledge it is learning. Therefore, versatile knowledge from multiple sources can be disentangled. During downstream fine-tuning, we could further utilize these implanted prompts and manipulate the PLM to stimulate the proper knowledge for a specific task.</p><p>To demonstrate the effectiveness of ELLE, we simulate the scenario where streaming data from 5 domains sequentially comes. We pre-train two typical PLMs (BERT and GPT) and expand their model sizes each time when the new data is available. We experiment when the number of parameters is sequentially grown from both 30M to 125M and 125M to 355M. The experimental results show the superiority of ELLE over multiple lifelong learning baselines in both pre-training efficiency and downstream task performances. In addition, we conduct sufficient experiments to verify the effectiveness of each component of ELLE. In general, we provide a promising research direction and hope this work could inspire more future attempts towards efficient lifelong pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lifelong Learning for PLMs. Lifelong learning aims at incrementally acquiring new knowledge, and in the meantime, mitigating the catastrophic forgetting issue. Numerous efforts have been spent towards this goal, including (1) memory-based methods <ref type="bibr" target="#b34">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b35">Rolnick et al., 2019)</ref>, which perform experience replay with authentic data (de <ref type="bibr" target="#b7">Masson d'Autume et al., 2019)</ref>, automatically generated data <ref type="bibr" target="#b42">(Sun et al., 2020)</ref>, or previously computed gradients <ref type="bibr" target="#b27">(Lopez-Paz and Ranzato, 2017)</ref> conserved in the memory, (2) consolidationbased methods <ref type="bibr" target="#b23">(Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b1">Aljundi et al., 2018)</ref>, which introduce additional regularization terms to consolidate the model parameters that are important to previous tasks, and (3) dynamic architecture methods <ref type="bibr" target="#b36">(Rusu et al., 2016;</ref><ref type="bibr" target="#b46">Yoon et al., 2018)</ref>, which fix trained network architectures in old tasks and dynamically grow branches for new tasks. Lifelong learning is also a hot topic for PLMs. Some target at domain adaptation through continual pre-training <ref type="bibr" target="#b13">(Gururangan et al., 2020)</ref>, parameter-efficient adapters <ref type="bibr" target="#b15">(He et al., 2021)</ref> and sparse expert models <ref type="bibr" target="#b12">(Gururangan et al., 2021)</ref>. Others focus on the incremental acquisition of factual knowledge that changes over time <ref type="bibr" target="#b9">(Dhingra et al., 2021;</ref><ref type="bibr" target="#b17">Jang et al., 2021)</ref>. However, the existing works seldom consider our lifelong learning setting where streaming data from multiple sources is sequentially gathered. Recently, researchers have also conducted a series of empirical studies on the continual learning of PLMs <ref type="bibr" target="#b45">(Wu et al., 2021;</ref><ref type="bibr" target="#b18">Jin et al., 2021)</ref>.</p><p>Efficient Pre-training in NLP. Many attempts have been made towards improving the efficiency of pre-training, such as designing novel pretraining tasks <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>, model architectures <ref type="bibr" target="#b49">(Zhang and He, 2020)</ref>, optimization algorithms <ref type="bibr" target="#b47">(You et al., 2020)</ref> and parallel architectures <ref type="bibr" target="#b40">(Shoeybi et al., 2019;</ref><ref type="bibr" target="#b39">Shazeer et al., 2018)</ref>. Until recently, researchers propose to "back distill" the knowledge from existing PLMs to accelerate large PLMs' pre-training <ref type="bibr">(Qin et al., 2021a)</ref>. Another line of work proposes progressive training to dynamically expand an existing PLM's size through parameter recycling <ref type="bibr" target="#b10">(Gong et al., 2019;</ref><ref type="bibr" target="#b11">Gu et al., 2021;</ref><ref type="bibr" target="#b11">Chen et al., 2021)</ref>. However, these methods typically focus on training PLMs on one static corpus, and thus cannot be directly applied to our lifelong pre-training setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Background for PLM. A PLM M generally consists of an embedding layer and L Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> layers. Given an input x consisting of a series of tokens, i.e., x = {w 1 , . . . , w |x| }, M first converts the input into embeddings {h 0 1 , . . . , h 0 |x| }, which are sequentially processed by each Transformer layer into contextualized hidden representations H l = {h l 1 , . . . , h l |x| }, where 1 ? l ? L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Width Expansion</head><formula xml:id="formula_0">? ! ? ! ? " ? ! ? " ? " ? ! ? ! ? " ? ? ? ? ?/2 ?/2 ?/2 ?/2 ????????? Depth Expansion 4 3 2 4 3 2 1 3 1 1 4 3 2 1 4 3 2 1 ? #$! M #$! %&amp;' ! ? # Previous Data ? ! ? #$!</formula><p>Pre-trained Domain Prompts</p><formula xml:id="formula_1">? ! ? " = ? ? ? ? ? ! ? " = ( " ( "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>) "</p><p>) "  </p><formula xml:id="formula_2">? ? ! ? ! ? " Prompts ! !</formula><formula xml:id="formula_3">? # ? ! ? #$! M #$! %&amp; Function Recovering Warmup ! ? ! ? #$! Task 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELLE Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Width + Depth Expansion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Preserved Model Expansion</head><formula xml:id="formula_4">D i = {x j i } |D i |</formula><p>j=1 . The whole training process can be partitioned into several stages. Initially, we have a PLM M 1 , which has been well trained on D 1 , and for the i-th stage (i &gt; 1), we obtain a new collection of data D i . Assume in this stage, we only have limited computational resources R i , our goal is to continually pretrain the existing PLM M i-1 to learn new knowledge on D i , and obtain a new PLM M i . Meanwhile, we expect the adapted PLM M i should not forget the previously learned knowledge of D i-1 .</p><p>Overall Framework. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, starting from M i-1 , which is trained on previous data D i-1 , we first expand M i-1 's width and depth and construct an enlarged PLM M WD i-1 to improve its training efficiency. Then we perform function recovering warmup and train M WD i-1 to inherit the knowledge of M i-1 to obtain M WD+ i-1 . The above procedures are dubbed as function preserved model expansion ( ? 3.2). After that, we continually pre-train M WD+ i-1 to gain new knowledge on D i . To mitigate the catastrophic forgetting on the previously learned knowledge, we employ data-based memory replay on a subset of previously gathered data</p><formula xml:id="formula_5">D sub i-1 = {D sub 1 , . . . , D sub i-1 } conserved in the memory, where D sub k = {x 1 k , . . . , x B k } ? D k (1 ? k ? i -1</formula><p>) and B is the constrained memory size for each domain. To help PLMs disentangle the knowledge during pre-training and also stimulate the needed knowledge for each downstream task, we implant domain prompts into PLMs during the whole training process ( ? 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Function Preserved Model Expansion</head><p>To accumulate knowledge more efficiently, each time when a new corpus D i comes, we expand both M i-1 's width and depth to attain the superior sample efficiency and fast convergence brought by larger model capacity <ref type="bibr" target="#b25">(Li et al., 2020)</ref>.</p><p>Width Expansion. For width expansion, we borrow the function preserving initialization (FPI) from <ref type="bibr" target="#b11">Chen et al. (2021)</ref>. For a brief introduction, FPI expands the matrices of all modules of a Transformer layer to arbitrary larger sizes and constructs an enlarged PLM M W i-1 . M W i-1 is initialized using the corresponding matrices of the original M i-1 through parameter replication. For example, as visualized in Figure <ref type="figure" target="#fig_0">1</ref>, the core principle of FPI is to divide the product of o?x 1 into multiple partitions, e.g.</p><formula xml:id="formula_6">o 2 ? x 1 + o 2 ? x 1 . Formally, FPI expands a ma- trix W ? R h 1 ?h 2 of M i-1 to an enlarged matrix W ? R (h 1 +? h 1 )?h 2 of M W</formula><p>i-1 as follows:</p><formula xml:id="formula_7">m(i) = i i ? [1, h1] U ({1, . . . , h1}) i ? (h1, h1 + ? h 1 ], Ci = h 1 +? h 1 i =1 I(m(i ) = m(i)), W (i, * ) = 1 Ci ? W (m(i), * ) + I(Ci &gt; 1) ? ?i,<label>(1)</label></formula><p>where U (?) denotes a uniform sampling function, m(?) denotes the mapping function between two matrices, I(?) is an indicator function, C i counts how many partitions a specific neuron is splitted and ? i ? R h 2 is a random gaussian noise. FPI ensures that both M W i-1 and M i-1 have approximately the same functionality, i.e., both models have almost the same output given the same input. Besides function preservation, the initialized model could serve as a good starting point for further optimization. We refer readers to <ref type="bibr" target="#b11">Chen et al. (2021)</ref> for more details about width expansion. Different from <ref type="bibr" target="#b11">Chen et al. (2021)</ref>, we additionally introduce random noises ? i into the newly copied parameters of W during initialization. These slight noises would break the symmetry after the replication and accelerate later pre-training.</p><p>Depth Expansion. For depth expansion, previous works generally resort to stacking all the original PLM layers into 2? layers through parameter replication <ref type="bibr" target="#b10">(Gong et al., 2019)</ref>. Such initialization is demonstrated to improve training efficiency.</p><p>However, the above layer stacking method restricts the number of layers of the enlarged PLM M D i-1 to be integer multiples of that of the original PLM M i-1 , which is not flexible for practical uses. To improve the expansion flexibility so that M i-1 could be expanded with arbitrary number of layers, we propose a novel layer insertion method to construct a new PLM M D i-1 with L + L layers, where 1 ? L ? L. Specifically, we randomly select L layers from M i-1 , copy each layer's parameters and insert the replication layer right before / after the original layer. We found empirically that inserting the copied layer into other positions would cause a performance drop, and the reason is that it will violate the processing order of the original layer sequence and break the PLM's original functionality. At each expansion stage when new data comes, since different layers have different functionalities, we always choose those layers that have not been copied before to help PLMs develop in an all-around way, instead of just developing a certain kind of functionality. Since both width expansion and depth expansion are compatible with each other, we simultaneously expand both of them to construct an enlarged model M WD i-1 , which inherits M i-1 's knowledge contained in the parameters.</p><p>Function Recovering Warmup. Since the above model expansion cannot ensure exact function preservation and inevitably results in functionality loss and performance drops, we pre-train the initialized PLM M WD i-1 on the previous corpora D sub i-1 conserved in the memory to recover the language abilities lost during model expansion, which is dubbed as function recovering warmup (FRW). After the warmup, we obtain M WD+ i-1 , which successfully inherits the knowledge from M i-1 and is also well-prepared for the next training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-trained Domain Prompt</head><p>Instead of training a separate model for each domain, we expect a single compact PLM to integrate the knowledge from all the sources. When confronted with a downstream task from a specific domain, the PLM needs to expose the proper knowledge learned during pre-training. To facilitate both knowledge acquisition during pre-training and knowledge exposure during fine-tuning, we resort to prompts as domain indicators and condition the PLM's behavior on these prompts. Soft prompts have been demonstrated excellent task indicators <ref type="bibr">(Qin et al., 2021b)</ref> and have non-trivial transferability among tasks <ref type="bibr" target="#b41">(Su et al., 2021)</ref>.</p><p>Specifically, during pre-training, to disentangle the knowledge from different sources, we implant a soft prompt token into the input to prime the PLM which kind of knowledge it is learning. The prompt of domain i is a tunable vector p i . We prepend p i before the original token embeddings</p><formula xml:id="formula_8">H 0 = {h 0 1 , . . . , h 0 |x| } for an input x ? D i , resulting in the modified input H 0 * = {p i ; h 0 1 , . . . , h 0 |x| },</formula><p>which is then processed by all the Transformer layers. Each p i is optimized together with other parameters of the PLM during pre-training. During fine-tuning, when applying the PLM on a similar domain of data seen before, we could leverage the trained domain prompt and prepend it before the input of downstream data. In this way, we manually manipulate the PLM to stimulate the most relevant knowledge learned during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Data Streams. We simulate the scenario where streaming data from 5 domains is gathered sequentially, i.e., the concatenation of WIKIPEDIA and BOOKCORPUS (WB) <ref type="bibr" target="#b50">(Zhu et al., 2015)</ref>, NEWS ARTICLES (NS) <ref type="bibr" target="#b48">(Zellers et al., 2019)</ref>, AMA-ZON REVIEWS (REV) <ref type="bibr" target="#b16">(He and McAuley, 2016)</ref>, BIOMEDICAL PAPERS (BIO) <ref type="bibr" target="#b13">(Lo et al., 2020)</ref> and COMPUTER SCIENCE PAPERS (CS) <ref type="bibr" target="#b13">(Lo et al., 2020)</ref>. For each corpus D i , we roughly sample 3, 400M tokens, and the quantity for each D i is comparable to the pre-training data of BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. In addition, considering that in practice, the expense of storage is far cheaper than the computational resources for pre-training, we maintain a relatively large memory compared with conventional lifelong learning settings by randomly sampling 200M tokens (D sub i ) for each corpus D i .</p><p>Evaluated Models. We mainly follow the model architectures of BERT and GPT <ref type="bibr" target="#b33">(Radford et al., 2018)</ref>. We use byte-level BPE vocabulary to ensure there are few unknown tokens in each corpus.</p><p>We experiment with the initial PLM M 1 of 6 layers and hidden size of 384 (around 30M parameters, denoted as BERT L6_D384 / GPT L6_D384 ), and linearly enlarge the PLM's number of parameters for 4 times, to the final PLM M 5 of 12 layers and hidden size of 768 (around 125M parameters, denoted as BERT L12_D768 / GPT L12_D768 ). We also experiment on a larger model size, i.e., growing the PLM from BERT L12_D768 (125M) to BERT L24_D1024 (355M). Details of each M i 's architecture are listed in appendix B. We also discuss the effect of expanded model size at each stage in appendix A.</p><p>Training Details. We train our model for 62, 500 steps for the first corpus. For the following domain i (i &gt; 1), after the model expansion, we perform function recovering warmup for 5, 000 steps, then train the resulting PLM for 20, 000 steps on the new data together with memory replay. Following <ref type="bibr">Chaudhry et al. (2019b)</ref>, we jointly train PLMs on a mixture samples from both D i and D sub i-1 in each batch, and the sampling ratio of D i and D sub i-1 is set to 9 : 1 in every batch. <ref type="bibr">Adam (Kingma and Ba, 2015)</ref> is chosen as the optimizer. All the experiments are conducted under the same environment of 8 V100 GPUs with a batch size of 2, 048.</p><p>More training details of pre-training are left in appendix B. We also experiment with fewer computational budgets and memory budgets in appendix G, and find that within a reasonable range, both of the two factors would not significantly influence the performance of ELLE.</p><p>Evaluation Metrics. We deem one algorithm to be more efficient if it could achieve the same performance with other methods utilizing fewer computations. For PLM, this is equivalent to achieving better performance using the same computations since pre-training with more computations almost always results in better performance <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>. We evaluate the PLM's performance during both pre-training and downstream fine-tuning.</p><p>Specifically, for pre-training, we propose two metrics to evaluate how PLMs perform on the learned domains following <ref type="bibr">Chaudhry et al. (2019a)</ref>: (1) average perplexity (AP) and ( <ref type="formula" target="#formula_11">2</ref>) average increased perplexity (AP + ). We record the train wall time <ref type="bibr" target="#b25">(Li et al., 2020)</ref> during pre-training. For a model checkpoint at time step T when learning the j-th domain, we measure the checkpoint's perplexity PPL T,i on the validation set of each domain i. Let PPL f i,i be the perplexity on the i-th domain when the PLM finishes training on the i-th domain, the above metrics are calculated as follows:</p><formula xml:id="formula_9">:% 1HZV 5HYLHZ %LR &amp;6 7UDLQ:DOO7LPH $3 1DLYH (:&amp; 0$6 *(0 (5 /RJLW.' 311 2XUV</formula><formula xml:id="formula_10">AP = exp 1 j j i=1</formula><p>log PPLT,i ,</p><formula xml:id="formula_11">AP + = 1 j -1 j-1 i=1 (PPLT,i -PPL f i,i ),<label>(2)</label></formula><p>where AP measures the average performance on all the seen data {D 1 , . . . , D j }. Lower AP indicates the PLM generally learns more knowledge from existing domains; AP + measures the influence of current data D j on previous data D j-1 . Lower AP + means PLMs forget less knowledge learned before.</p><p>To evaluate PLMs' performance in downstream tasks, for each domain, we select a representative task that is relatively stable, i.e., MNLI <ref type="bibr" target="#b44">(Williams et al., 2018)</ref>, HYPERPARTISAN <ref type="bibr" target="#b21">(Kiesel et al., 2019)</ref>, <ref type="bibr">HELPFULLNESS (McAuley et al., 2015)</ref>, CHEMPROT <ref type="bibr" target="#b24">(Kringelum et al., 2016)</ref> and ACL-ARC <ref type="bibr" target="#b19">(Jurgens et al., 2018)</ref> for WB, NS, REV, BIO and CS, respectively. Training details for finetuning are left in appendix C.</p><p>Baselines. Keeping most of the experimental settings the same, we choose the following baselines for comparison: (1) Naive, which is a naive extension of <ref type="bibr" target="#b13">Gururangan et al. (2020)</ref>  . ELLE is based on ER and additionally introduces the model expansion and pre-trained domain prompts. For ER, we set the sampling ratio of D i and D sub i-1 to be 9 : 1 in every batch same as ELLE;</p><p>(5) A-GEM <ref type="bibr">(Chaudhry et al., 2019a)</ref>, which constrains the new parameter gradients to make sure that optimization directions do not conflict with gradients on old domains; (6) Logit-KD, which prevents forgetting by distilling knowledge from the previous model M i-1 using the old data in the memory; (7) PNN <ref type="bibr" target="#b36">(Rusu et al., 2016)</ref>, which fixes the old PLM M i-1 to completely avoid knowledge forgetting and grows new branches for learning new knowledge. For a fair comparison, we control the total train wall time of ELLE and all the baselines to be the same at each training stage, so that each method consumes the same computational costs.  (3) PNN achieves significantly lower AP than non-progressive baselines, and is immune to knowledge forgetting (AP + = 0). It also performs better on the downstream tasks than other baselines. This indicates that enlarging the network is an effective way for lifelong pre-training and also benefits downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we conduct analyses to investigate the effect of ELLE's components. We follow the setting in ? 4 by choosing BERT L6_D384 as the initial model and continually growing it to BERT L12_D768 . Specifically, we investigate the effect of (1) width expansion (WE), (2) depth expansion (DE), (3) function recovering warmup (FRW), (4) the random noises added into the newly constructed parameters during model expansion (? N ) and ( <ref type="formula">5</ref>) the pre-trained domain prompts (PT). We test ELLE under different combinations of the above components and compare the results. The experimental results of pre-training and downstream tasks are summarized in Table <ref type="table" target="#tab_3">3</ref> and<ref type="table">Table 4</ref>, respectively. Detailed trend curves for AP and AP + are illustrated in appendix D.</p><p>Effect of Width / Depth Expansion. First, we compare the differences of conducting only width expansion (WE+FRW), only depth expansion (DE+FRW) and expansion on both width and depth (WE+DE+FRW) before function preserving warmup. For a fair comparison, we keep the total number of M i 's increased parameters for the above three strategies almost the same at each stage i. The specific model architectures are listed in appendix F. The results show that: (1) compared with the non-expanding baseline, all these three strategies achieve better pre-training and downstream performance, showing that with the growth of model size, the sample efficiency and training efficiency are extensively increased. Therefore, PLMs could gain more knowledge with limited computational resources and perform better in downstream tasks;</p><p>(2) compared with expanding only width or depth, expanding both of them is more efficient and can also achieve better downstream performance on almost all domains, except the NS domain. This is also aligned with previ- ous findings that PLM's growth favors compound scaling <ref type="bibr" target="#b11">(Gu et al., 2021)</ref>. We also conclude from the trend curves in appendix D that only expanding depth will make the training process unstable. To rigorously investigate how domain prompts stimulate the knowledge during fine-tuning, for a PLM pre-implanted with prompts during pretraining, we test its downstream performance when (1) no prompt is prepended in the input (i.e., ELLE-PT fine-tune ) during fine-tuning and (2) a prompt from a random wrong domain is prepended in the input (i.e., ELLE + ?PT fine-tune ). The results in Table <ref type="table">5</ref> show that both of the above strategies have lower downstream performance than prepending the right prompt (ELLE). We hypothesize the reasons are two-fold: (1) firstly, for ELLE-PT fine-tune , there exists a great gap between the formats of input during pre-training and fine-tuning, and such a gap would hinder the successful knowledge transfer; (2) secondly, for ELLE + ?PT fine-tune , although the above gap disappears, the PLM is primed with a wrong domain prompt, and thus cannot properly stimulate the knowledge that is most relevant to the downstream task. Although manually deciding the most relevant domain prompt for a specific downstream task is relatively easy and fast, such a process can also be automated by training a domain discriminator, which is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of</head><p>Attention Pattern Visualization of a Stream of PLMs. Through the function preserved model expansion, PLMs inherit the knowledge of their "ancestors" contained in the parameters. Intuitively, the descendant PLM (the expanded larger PLM) should have similar functionalities to the ancestor PLM (the original PLM before model expansion). We thus investigate such functionality similarity through the lens of attention patterns of each attention head in the Transformer layer. Specifically, we visualize the attention patterns of a stream of PLMs ({M 1 , . . . , M 5 }) trained by ELLE when growing from BERT L6_D384 to BERT L12_D768 . We checkpoint each PLM M i when it finishes training on the emerging data D i . We input the same data into these checkpoints to derive the attention patterns. The results are illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, from which we observe that the attention patterns of a head in a descendant PLM are surprisingly similar to those of its "ancestors", even if the descendant PLM is further trained on the new data and enlarged many times. This indicates that the expanded PLM by ELLE successfully inherits the knowledge from its "ancestor", and thus exhibits similar functionality to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present the efficient lifelong pretraining problem, which requires PLMs to continually integrate the information from emerging data efficiently. To achieve our goal, we propose ELLE and progressively expand PLMs to acquire knowledge efficiently and mitigate the knowledge forgetting. We also pre-implant domain prompts during pre-training and use them to stimulate the needed knowledge for downstream tasks. The experimental results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Additional Analysis on Function Preserved Model Expansion</head><p>In addition to the analyses of function preserved model expansion conducted in our main paper, in this section, we further analyze the effect of (1) the expanded model size at each training stage and (2) the choice of copied layer during depth expansion. We experiment on the combination of WE+DE+FRW as mentioned in ? 5 and choose BERT L6_D384 as the initial PLM M 1 . Other settings are kept the same as ? 5.</p><p>Effect of Expanded Model Size. In our main experiments, we assume that the data size of each emerging corpus is the same and linearly enlarge the model size when conducting model expansion.</p><p>In this section, we explore the effect of expanded model size given limited computational resources.</p><p>We conduct experiments on a stream of data from 3 domains, i.e., WB, NS and REV domain. We start from the initial PLM BERT L6_D384 and continually adapt it to new corpora. Under the same training environment, we control the computational costs (train wall time) of each domain to be 7200 seconds. We compare the performances when the PLM expands 0, 2, 4, and 6 layers and heads for each domain, respectively. Note the PLMs expanded with a larger size would be trained with fewer steps to control the train wall time.</p><p>The results are shown in Table <ref type="table" target="#tab_7">6</ref>, from which we can conclude that the best performance is obtained when the model expands 2 layers and heads at each expansion stage, and expanding more or fewer parameters leads to a performance drop. The reasons are two-fold: (1) firstly, as mentioned before, expanding the model size improves the sample efficiency <ref type="bibr" target="#b20">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b25">Li et al., 2020)</ref>, which is beneficial for PLMs' knowledge acquisition; (2) secondly, when increasing the expanded model size, the benefits from inheriting the knowledge of a small PLM would become less and less evident. To sum up, expanding with an intermediate size strikes the best trade-off between the above two reasons, and there may exist an optimal expanded size when performing model expansion.</p><p>Intuitively, the optimal expanded model size may be influenced by many factors, e.g., the computational budgets, the amount of emerging data, the PLM's model architecture, etc. And systematically analyzing the effects of all these factors is beyond the scope of this paper, thus we expect future works to design algorithms to accurately estimate the optimal expanded size for model expansion.</p><p>Choice of Copied Layer. As mentioned in ? 3.2, each time when we conduct width expansion, we choose those layers that have not been copied before. To demonstrate the benefit of this trick, we compare three expansion strategies: (1) always replicating those layers that have not been copied before (WE+DE+FRW); (2) always replicating the first layer (WE+DE first +FRW) and ( <ref type="formula" target="#formula_12">3</ref>) always replicating the last layer (WE+DE last +FRW).</p><p>The results in Figure <ref type="figure">4</ref> show that AP and AP + descend the fastest when we always replicate those layers that have not been copied before (i.e., WE+DE+FRW). This demonstrates that, since different layers have different functionalities, choosing those layers that have not been expanded before would help PLMs develop in an all-around way, instead of just developing a certain kind of functionality. Furthermore, we find empirically that when pre-training PLMs continually on multiple domains, if we always choose those layers that have not been expanded before at each depth expansion stage, then the final performance is not sensitive to choosing which layers to expand first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pre-training Hyper-parameters</head><p>In Table <ref type="table">7</ref>, we list the architectures and the hyperparameters for the PLMs we pre-trained with ELLE in this paper, including the total number of trainable parameters (n params ), the number of layers (n layers ), the number of units in each bottleneck layer (d model ), the number of attention heads (n heads ), the inner hidden size of FFN layer (d FFN ), the learning rate (lr), the training steps of FRW (SF), the training steps of adaptation after FRW (STF) when learning the new corpus, the ratio of learning rate warmup (RW), and the total train wall time (TWT). We set the dropout rate for each model to 0.1, weight decay to 0.01 and use linear learning rate decay for BERT and inverse square root decay for GPT. We adopt Adam <ref type="bibr" target="#b22">(Kingma and Ba, 2015)</ref> as the optimizer. The hyper-parameters for the optimizer is set to 1 ? 10 -6 , 0.9, 0.98 for , ? 1 , ? 2 , respectively. We reset the optimizer and the learning rate scheduler each time when the PLM finishes FRW or the training on new corpus. All experiments are conducted under the same computation environment with 8 NVIDIA 32GB V100 GPUs.  Table <ref type="table">7</ref>: Model architectures, learning rate (lr), steps of FRW (SF), steps of training after FRW (STF), the ratio of steps for learning rate warmup (for both FRW and pre-training) (RW), and train wall time (TWT) for all the models pre-trained with ELLE in this paper. We list the details when growing BERT L6_D384 to BERT L12_D768 , BERT L12_D768 to BERT L24_D1024 and GPT L6_D384 to GPT L12_D768 , respectively. The total train wall time consumed by the above three settings is 2.57 ? 10 5 seconds, 7.79 ? 10 5 seconds, and 3.08 ? 10 5 seconds, respectively.</p><p>fairseq 1 <ref type="bibr" target="#b30">(Ott et al., 2019)</ref> (MIT-license).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HyperParam</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNLI HYPERPARTISAN HELPFULNESS CHEMPROT ACL-ARC</head><p>Learning Rate 1 ? 10 -5 2 ? 10 -5 2 ? 10 -5 2 ? 10 -5 2 ? 10 Table <ref type="table">8</ref>: Hyper-parameters for fine-tuning on downstream tasks of each domain. As mentioned in the main paper, for each domain, we select a representative task that is relatively stable, i.e., MNLI <ref type="bibr" target="#b44">(Williams et al., 2018)</ref>, HY-PERPARTISAN <ref type="bibr" target="#b21">(Kiesel et al., 2019)</ref>, <ref type="bibr">HELPFULLNESS (McAuley et al., 2015)</ref>, CHEMPROT <ref type="bibr" target="#b24">(Kringelum et al., 2016)</ref> and ACL-ARC <ref type="bibr" target="#b19">(Jurgens et al., 2018)</ref> for WB, NS, REV, BIO and CS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details and</head><p>Additional Experiments for Downstream Fine-tuning Implementation Details. </p><formula xml:id="formula_12">F1 i avg = 1 N N j=1 F1 j M i<label>(3)</label></formula><p>where F1 j M i is the F1 score of M i evaluated on the downstream task of the j-th domain. We also list the detailed numerical results for each task in Table <ref type="table" target="#tab_11">9</ref>, covering all PLMs trained by each lifelong learning method.</p><p>The results show that ELLE outperforms all the lifelong learning baselines after finishing training on each domain, demonstrating that ELLE could properly stimulate the learned knowledge during pre-training and boost the performance in downstream tasks.</p><p>1 https://github.com/pytorch/fairseq 2 https://github.com/pytorch/fairseq 3 https://github.com/allenai/ dont-stop-pretraining D Trend Curves for AP and AP + For the experiments in ? 4, the trend curves of average perplexity (AP) and average increased perplexity (AP + ) w.r.t train wall time are shown in Figure <ref type="figure">7</ref> (growing from BERT L6_D384 to BERT L12_D768 ), Figure <ref type="figure">8</ref> (growing from BERT L12_D768 to BERT L24_D1024 ), and Figure <ref type="figure">9</ref> (growing from GPT L6_D384 to GPT L12_D768 ). Each figure illustrates the performance of different lifelong learning methods. The above results reflect that, compared with all the baselines, AP and AP + of ELLE descend with the fastest speed, demonstrating that ELLE could acquire knowledge and mitigate the knowledge forgetting on previous domains more efficiently. Thus given limited computational resources, PLMs trained by ELLE could integrate more information from different domains.</p><p>For the analysis in ? 5, we visualize the trend curves of AP and AP + when choosing different combinations of strategies. Specifically, we investigate (1) the effect of width / depth expansion in Figure <ref type="figure" target="#fig_0">10</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Representational Similarity of a Stream of PLMs</head><p>We investigate the representational similarity <ref type="bibr" target="#b0">(Abnar et al., 2019)</ref>   p i k and p j k output by the LM head of M i and M j , respectively for each [MASK] token k, where 1 ? k ? n. We calculate the average representational similarity (ARS) between M j and all its ancestors {M 1 , ? ? ? , M j-1 } as follows:  where KL denotes the Kullback-Leibler divergence between two probability distributions. Higher ARS j means the representations of M j and its ancestors are more similar. To some extent, ARS j could reflect how much knowledge / functionality of the ancestors is preserved by M j . We compare ARS of PLMs trained by Naive, MAS, ER, Logit-KD and ELLE and illustrate the results in Figure <ref type="figure">6</ref>, from which we observe that Logit-KD has the highest ARS. This is because the training objective of knowledge distillation in Logit-KD is highly correlated with ARS. In addition, ELLE takes second place. We also find that, with PLMs continually absorbing new knowledge, the ASR generally decreases.</p><formula xml:id="formula_13">ARSj = -1 (j -1) ? n j-1 i=1 n k=1 KL(p i k , p j k ),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Model Architectures for the Analysis of Model Expansion</head><p>In Table <ref type="table" target="#tab_14">12</ref>, we list the model architectures of all the investigated PLMs when conducting analysis of model expansion in ? 5. Specifically, three strategies are investigated, including WE+FRW, DE+FRW and WE+DE+FRW. As mentioned in our main paper, for a fair comparison, we keep the total number of M i 's increased parameters for the above three strategies almost the same at each stage i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Performance of ELLE with Fewer Computational Budgets and Storage Budgets</head><p>To investigate the performance of ELLE under limited (1) computational budgets and (2) storage budgets, in this section, we take an initial step to investigate the effect of (1) training resources (train The experimental results for the above two settings are listed in Table <ref type="table" target="#tab_13">10</ref> (pre-training) and Table 11 (fine-tuning), respectively. We also illustrate the trend curves of AP and AP + in Figure <ref type="figure" target="#fig_2">13</ref> and Figure <ref type="figure" target="#fig_0">14</ref>. From the above results, we find that: (1) when given fewer computational budgets and storage budgets, ELLE still outperforms all the lifelong learning baselines in both pre-training and downstream performance, which demonstrates the superiority of ELLE; (2) for ELLE, when PLMs are trained with fewer computational budgets, we observe significant performance drops in both pre-training (higher AP and AP + ) and downstream tasks (lower average F1). This shows that pre-training with fewer computations would harm PLMs' knowledge acquisition; (3) for ELLE, when there are fewer memory budgets, although we also observe slight performance drops in pretraining (higher AP and AP + ), the performance in downstream tasks is generally not influenced, with the average F1 score keeping almost the same <ref type="bibr">(77.8)</ref>. This shows the data-efficiency of PLMs, i.e., PLMs could easily recall the learned knowledge by reviewing small-scale data conserved in the memory (as few as 1%). As mentioned before, considering that for pre-training, the expense of storage (e.g., hard disks) is far cheaper than the computational resources (e.g., GPUs), the storage space problem for memory seldom needs to be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Details of Baselines</head><p>We tried different hyper-parameters for baselines, including the regularization parameter ? for EWC and MAS, and the memory size for A-GEM, to derive and report their best performance. Their AP and AP + curves are shown in Figure <ref type="figure" target="#fig_0">15</ref>, 16 and 17. From the results we can see that none of these hyperparameters works well. For EWC and MAS, when the regularization parameter ? is small, the pre-training performance is not obviously better than that of naive method. However, if we slightly increase ?, the performance would become worse than baseline. For A-GEM, the case with bigger memory also doesn't clearly outperform cases with smaller memory and naive case. Specially, we observed that during A-GEM pre-training, 99.9% of the inter-products of current gradient and replay gradient are positive, implying that pre-training on different domains is similar to each other to a large extent. This might indicate that EWC, MAS, and A-GEM cannot deal with the subtle difference of various domains. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of ELLE when adapting an existing PLM M i-1 trained on previous data D i-1 to a new corpus D i . We also visualize the mechanism of width / depth expansion and pre-trained domain prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average perplexity (AP) of different lifelong learning methods with BERT L6_D384 as the initial PLM. The trend curves for AP + and other PLMs are left in appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The visualization of the attention patterns of different attention heads in M 1 (BERT L6_D384 ), M 2 (BERT L8_D512 ), M 3 (BERT L10_D640 ), M 4 (BERT L11_D708 ) and M 5 (BERT L12_D768 ) after finishing training on the new corpus D i . Note that in this figure, all the attention heads of a PLM M i are expanded from all its ancestors {M 1 , . . . , M i-1 } in the same column. We observe similar attention patterns between the descendant PLM and the ancestor PLM, demonstrating the descendant PLM successfully preserves the functionality of its ancestors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(comparing WE+FRW, DE+FRW and WE+DE+FRW); (2) the effect of function recovering warmup in Figure 11 (comparing WE+DE and WE+DE+FRW); (3) the effect of random noises added into the newly initialized parameters during model expansion in Figure 11 (comparing WE+DE+FRW and WE+DE+FRW+? N ) and (4) the effect of pre-trained domain prompts in Figure 12 (comparing ELLE and ELLE-PT). All of the above results again demonstrate the effectiveness of ELLE's each component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Figure 16: AP and AP + of MAS with BERT L6_D384 as the initial model and with different regularization parameter ? w.r.t train wall time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>to continually adapt PLMs for each domain and can be seen Average perplexity (AP) and average increased perplexity (AP + ) of PLMs trained by different lifelong learning methods with the same train wall time. PLMs are trained with streaming data from WB, NS, REV, BIO and CS domain sequentially. We evaluate the performance each time when PLMs finish training on one domain.</figDesc><table><row><cell>Domain</cell><cell>WB</cell><cell></cell><cell></cell><cell>NS</cell><cell></cell><cell>REV</cell><cell></cell><cell>BIO</cell><cell>CS</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="2">AP AP+</cell><cell cols="2">AP AP+</cell><cell>AP</cell><cell>AP+</cell><cell>AP</cell><cell>AP+</cell><cell cols="2">AP AP+</cell></row><row><cell cols="4">Growing from BERTL6_D384 to BERTL12_D768</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Naive (Lower Bound)</cell><cell>7.96</cell><cell>-</cell><cell>8.03</cell><cell cols="2">5.54 13.52</cell><cell cols="2">21.42 13.86</cell><cell>17.67</cell><cell>9.93</cell><cell>9.81</cell></row><row><cell>EWC</cell><cell>7.96</cell><cell>-</cell><cell>8.09</cell><cell cols="2">5.65 13.40</cell><cell cols="2">20.98 13.92</cell><cell>17.75</cell><cell>9.94</cell><cell>9.82</cell></row><row><cell>MAS</cell><cell>7.96</cell><cell>-</cell><cell>8.08</cell><cell cols="2">5.65 13.44</cell><cell cols="2">21.17 13.87</cell><cell>17.67</cell><cell>9.91</cell><cell>9.75</cell></row><row><cell>A-GEM</cell><cell>7.96</cell><cell>-</cell><cell>8.82</cell><cell cols="2">6.72 13.31</cell><cell cols="2">20.06 14.73</cell><cell cols="3">18.89 10.56 10.58</cell></row><row><cell>ER</cell><cell>7.96</cell><cell>-</cell><cell>6.85</cell><cell>1.59</cell><cell>6.99</cell><cell>4.09</cell><cell>6.66</cell><cell>3.62</cell><cell>6.39</cell><cell>3.16</cell></row><row><cell>Logit-KD</cell><cell>7.96</cell><cell>-</cell><cell>7.60</cell><cell>0.99</cell><cell>7.19</cell><cell>1.95</cell><cell>7.08</cell><cell>2.02</cell><cell>6.92</cell><cell>1.92</cell></row><row><cell>PNN</cell><cell>7.96</cell><cell>-</cell><cell>6.52</cell><cell>0.00</cell><cell>5.29</cell><cell>0.00</cell><cell>4.84</cell><cell>0.00</cell><cell>4.76</cell><cell>0.00</cell></row><row><cell>ELLE (ours)</cell><cell>7.92</cell><cell>-</cell><cell cols="2">5.62 -0.20</cell><cell>4.81</cell><cell>0.64</cell><cell>4.41</cell><cell>0.64</cell><cell>4.06</cell><cell>0.44</cell></row><row><cell cols="4">Growing from BERTL12_D768 to BERTL24_D1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ER</cell><cell>4.54</cell><cell>-</cell><cell>4.33</cell><cell>1.31</cell><cell>4.02</cell><cell>1.46</cell><cell>3.73</cell><cell>1.15</cell><cell>3.82</cell><cell>1.28</cell></row><row><cell>ELLE (ours)</cell><cell>4.52</cell><cell>-</cell><cell>3.89</cell><cell>0.47</cell><cell>3.61</cell><cell>0.75</cell><cell>3.66</cell><cell>0.97</cell><cell>3.29</cell><cell>0.54</cell></row><row><cell cols="4">Growing from GPTL6_D384 to GPTL12_D768</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Naive (Lower Bound) 46.54</cell><cell>-</cell><cell cols="8">52.91 37.96 81.28 177.22 94.44 160.51 60.64 80.48</cell></row><row><cell>MAS</cell><cell>46.54</cell><cell>-</cell><cell cols="8">53.12 38.44 81.23 177.20 93.21 157.93 60.62 80.28</cell></row><row><cell>ER</cell><cell>46.54</cell><cell>-</cell><cell cols="3">44.49 12.42 35.46</cell><cell cols="2">21.78 33.24</cell><cell cols="3">23.38 31.94 19.83</cell></row><row><cell>Logit-KD</cell><cell>46.54</cell><cell>-</cell><cell>48.93</cell><cell cols="2">5.41 37.60</cell><cell cols="2">9.97 34.60</cell><cell cols="3">11.74 33.67 11.19</cell></row><row><cell>PNN</cell><cell>46.54</cell><cell>-</cell><cell>39.90</cell><cell cols="2">0.00 26.84</cell><cell cols="2">0.00 22.19</cell><cell cols="2">0.00 21.43</cell><cell>0.00</cell></row><row><cell>ELLE (ours)</cell><cell>46.50</cell><cell>-</cell><cell>36.84</cell><cell cols="2">2.25 25.60</cell><cell cols="2">4.38 22.29</cell><cell cols="2">5.88 20.49</cell><cell>4.31</cell></row></table><note><p><p><p><p><p><p><p>as the lower bound; (2) EWC</p><ref type="bibr" target="#b38">(Schwarz et al., 2018)</ref></p>, which adopts elastic weight consolidation to add L 2 regularization on parameter changes; (3) MAS</p><ref type="bibr" target="#b1">(Aljundi et al., 2018)</ref></p>, which estimates parameter importance via the gradients of the model outputs; (4) ER</p>(Chaudhry et al., 2019b)</p>, which alleviates forgetting by jointly training models on a mixture samples from new data D i and the memory D sub i-1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 1 summarizes the pre-training performance each time when the PLM finishes training on a specific domain. Figure 2 depicts the trend of AP for</figDesc><table><row><cell>Domain</cell><cell>WB</cell><cell></cell><cell>NS</cell><cell></cell><cell></cell><cell>REV</cell><cell>BIO</cell><cell>CS</cell></row><row><cell>WE DE FRW ?N PT</cell><cell cols="2">AP AP+</cell><cell>AP</cell><cell>AP+</cell><cell cols="2">AP AP+</cell><cell>AP AP+</cell><cell>AP AP+</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell>6.85</cell><cell cols="4">1.59 6.99 4.09 6.66 3.62 6.39 3.16</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell>6.23</cell><cell cols="4">0.78 5.34 1.42 4.98 1.20 4.48 0.89</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell>5.81</cell><cell cols="4">0.03 5.49 1.43 5.16 1.32 4.79 0.94</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell>5.78</cell><cell cols="4">0.02 4.91 0.76 4.49 0.73 4.13 0.52</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell>5.79</cell><cell cols="4">0.09 5.09 1.13 4.58 0.88 4.22 0.65</cell></row><row><cell></cell><cell>7.96</cell><cell>-</cell><cell cols="5">5.69 -0.13 4.85 0.67 4.45 0.69 4.09 0.47</cell></row><row><cell></cell><cell>7.92</cell><cell>-</cell><cell>5.62</cell><cell cols="4">-0.20 4.81 0.64 4.41 0.64 4.06 0.44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Domain</cell><cell cols="2">WB NS REV BIO CS AVG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Growing from BERTL6_D384 to BERTL12_D768</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Naive</cell><cell></cell><cell cols="2">77.2 72.8 60.6 77.1 64.8 70.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EWC</cell><cell></cell><cell cols="2">77.4 72.8 61.6 77.5 59.6 69.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAS</cell><cell></cell><cell cols="2">77.1 73.7 60.7 77.5 68.2 71.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">A-GEM</cell><cell cols="2">76.6 71.4 61.5 76.9 67.5 70.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ER</cell><cell></cell><cell cols="2">77.6 72.2 61.9 78.3 63.5 70.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Logit-KD 77.2 69.5 63.9 76.8 58.9 69.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PNN</cell><cell></cell><cell cols="2">76.0 76.3 68.0 79.5 65.2 73.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ELLE</cell><cell></cell><cell cols="2">83.2 81.8 68.5 82.9 72.7 77.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Growing from BERTL12_D768 to BERTL24_D1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ER</cell><cell></cell><cell cols="2">84.7 83.3 68.0 82.7 71.4 78.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ELLE</cell><cell></cell><cell cols="2">86.3 90.4 70.5 84.2 73.8 81.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Table 2: Final downstream performance (F1) of BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">on each domain after finishing pre-training on all do-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">mains. Experiments of NS domain are repeated for 10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">times with different seeds and others are repeated for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">5 times. More detailed results at different pre-training</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">stages are illustrated in appendix C.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">BERT w.r.t. train wall time, other trend curves are</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">illustrated in appendix D. We also report the final</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">downstream performance for discriminative PLMs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(BERT) on each domain after finishing the whole</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">pre-training in Table 2. The intermediate down-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">stream performance each time when the PLM fin-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">ishes training on one domain is left in appendix C.</cell></row></table><note><p><p>Superiority of ELLE.</p>(1) From the results in Table 1, we observe that, compared with all the baselines, ELLE achieves the lowest AP and satisfying AP + after finishing training on each domain. This demonstrates that, given limited computational re-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>AP and AP + of different combinations of strategies when growing BERT L6_D384 to BERT L12_D768 . sources, ELLE could acquire more knowledge and in the meantime, mitigate the knowledge forgetting problem. (2) We also observe from Figure2that the AP of ELLE descends the fastest, showing the superior training efficiency of ELLE over all baselines. (3) Besides, ELLE performs the best on all downstream tasks, indicating that the knowledge learned during pre-training could be properly stimulated and leveraged for each downstream task. (4) The superiority of ELLE is consistently observed on the larger model size, i.e., BERT L24_D1024 and other model architectures, i.e., GPT L12_D768 . This shows that ELLE is agnostic to both the model size and the specific PLM model architecture chosen. We expect future work to apply ELLE on other PLM architectures and extremely large PLMs.</figDesc><table><row><cell>Comparisons with Baselines. (1) First of all,</cell></row><row><cell>consolidation-based methods (EWC and MAS) per-</cell></row><row><cell>form almost comparable with the naive baseline</cell></row><row><cell>in either pre-training or downstream tasks. This</cell></row><row><cell>means that parameter regularization may not be</cell></row><row><cell>beneficial for PLMs' knowledge acquisition. (2)</cell></row><row><cell>Among memory-based methods, gradient-based</cell></row><row><cell>reaply (A-GEM) exhibits poorer performance in</cell></row><row><cell>pre-training, on the contrary, data-based replay (ER</cell></row><row><cell>and Logit-KD) achieve lower AP and AP + than</cell></row><row><cell>the naive baseline, demonstrating that replaying</cell></row><row><cell>real data points could more efficiently mitigate the</cell></row><row><cell>knowledge forgetting problem. Meanwhile, all of</cell></row><row><cell>the memory-based methods perform comparable</cell></row><row><cell>or worse than the naive baseline in downstream</cell></row><row><cell>performance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>All the pre-training implementations are based on</figDesc><table><row><cell>Domain</cell><cell>WB</cell><cell></cell><cell cols="2">NEWS</cell><cell>REVIEW</cell></row><row><cell>Metrics</cell><cell cols="2">AP AP +</cell><cell>AP</cell><cell>AP +</cell><cell>AP AP +</cell></row><row><cell cols="2">Expand 0 layers and heads per domain 13.09</cell><cell>-</cell><cell cols="3">8.99 -0.49 8.24 2.80</cell></row><row><cell cols="2">Expand 2 layers and heads per domain 13.09</cell><cell>-</cell><cell>8.28</cell><cell cols="2">-1.44 7.25</cell><cell>1.11</cell></row><row><cell cols="2">Expand 4 layers and heads per domain 13.09</cell><cell>-</cell><cell cols="3">8.62 -0.95 7.53 1.30</cell></row><row><cell cols="2">Expand 6 layers and heads per domain 13.09</cell><cell>-</cell><cell cols="3">9.08 -0.24 7.92 1.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>AP and AP + of PLMs trained with ELLE that expands 0, 2, 4 and 6 layers and heads during model expansion, respectively. AP and AP + are evaluated when each PLM finishes training on each domain. Figure 4: and AP + of PLMs trained by ELLE using different depth expansion strategies: WE+DE+FRW, WE+DE first +FRW and WE+DE last +FRW w.r.t train wall time.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">WE + DE + FRW WE + DEfirst + FRW WE + DElast + FRW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>$3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>$3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WE + DE + FRW WE + DEfirst + FRW WE + DElast + FRW</cell></row><row><cell></cell><cell cols="3">:% 1HZV 5HYLHZ 7UDLQ:DOO7LPH %LR</cell><cell>&amp;6</cell><cell></cell><cell cols="4">:% 1HZV 5HYLHZ 7UDLQ:DOO7LPH %LR</cell><cell>&amp;6</cell></row><row><cell cols="5">Model nparams nlayers dmodel nheads</cell><cell>dFFN</cell><cell>lr</cell><cell>SF</cell><cell>STF</cell><cell>RW</cell><cell>TWT(s)</cell></row><row><cell cols="5">Growing from BERTL6_D384 to BERTL12_D768</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>30.3M</cell><cell>6</cell><cell>384</cell><cell>6</cell><cell cols="2">1536 5.0 ? 10 -4</cell><cell>-</cell><cell>62.5k</cell><cell>8%</cell><cell>6.0 ? 10 4</cell></row><row><cell>M2</cell><cell>51.5M</cell><cell>8</cell><cell>512</cell><cell>8</cell><cell cols="3">2048 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell>8%</cell><cell>2.4 ? 10 4</cell></row><row><cell>M3</cell><cell>82.2M</cell><cell>10</cell><cell>640</cell><cell>10</cell><cell cols="3">2560 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell>8%</cell><cell>5.0 ? 10 4</cell></row><row><cell>M4</cell><cell>102M</cell><cell>11</cell><cell>704</cell><cell>11</cell><cell cols="3">2816 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell>8%</cell><cell>5.8 ? 10 4</cell></row><row><cell>M5</cell><cell>125M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell cols="3">3072 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell>8%</cell><cell>6.8 ? 10 4</cell></row><row><cell cols="5">Growing from BERTL12_D768 to BERTL24_D1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>125M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 5.0 ? 10 -4</cell><cell>-</cell><cell>62.5k</cell><cell>8%</cell><cell>1.9 ? 10 5</cell></row><row><cell>M2</cell><cell>216M</cell><cell>15</cell><cell>960</cell><cell>15</cell><cell cols="3">3840 2.5 ? 10 -4 1k</cell><cell>20k</cell><cell cols="2">20% 6.5 ? 10 4</cell></row><row><cell>M3</cell><cell>280M</cell><cell>18</cell><cell>1024</cell><cell>16</cell><cell cols="3">4096 2.5 ? 10 -4 1k</cell><cell>20k</cell><cell cols="2">20% 1.4 ? 10 5</cell></row><row><cell>M4</cell><cell>318M</cell><cell>21</cell><cell>1024</cell><cell>16</cell><cell cols="3">4096 2.5 ? 10 -4 1k</cell><cell>20k</cell><cell cols="2">20% 1.7 ? 10 5</cell></row><row><cell>M5</cell><cell>355M</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell cols="3">4096 2.5 ? 10 -4 1k</cell><cell>20k</cell><cell cols="2">20% 2.2 ? 10 5</cell></row><row><cell cols="5">Growing from GPTL6_D384 to GPTL12_D768</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>29.9M</cell><cell>6</cell><cell>384</cell><cell>6</cell><cell cols="2">1536 5.0 ? 10 -4</cell><cell>-</cell><cell cols="3">62.5k 16% 6.7 ? 10 4</cell></row><row><cell>M2</cell><cell>51.0M</cell><cell>8</cell><cell>512</cell><cell>8</cell><cell cols="3">2048 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell cols="2">16% 3.9 ? 10 4</cell></row><row><cell>M3</cell><cell>81.4M</cell><cell>10</cell><cell>640</cell><cell>10</cell><cell cols="3">2560 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell cols="2">16% 5.6 ? 10 4</cell></row><row><cell>M4</cell><cell>101M</cell><cell>11</cell><cell>704</cell><cell>11</cell><cell cols="3">2816 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell cols="2">16% 6.8 ? 10 4</cell></row><row><cell>M5</cell><cell>124M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell cols="3">3072 5.0 ? 10 -4 5k</cell><cell>20k</cell><cell cols="2">16% 7.8 ? 10 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table 8 describes the hyper-parameters for fine-tuning PLMs on downstream tasks of each domain. The implementations of MNLI are based on fairseq 2 (Ott et al., 2019) (MIT-license). The implementations of HY-PERPARTISAN, HELPFULNESS CHEMPROT, and ACL-ARC are based on<ref type="bibr" target="#b13">(Gururangan et al., 2020)</ref> 3 .</figDesc><table><row><cell>Additional Experiments. Figure 5 visualizes</cell></row><row><cell>the specific F1 on each downstream tasks and the</cell></row><row><cell>average F1 of PLMs trained with Naive, A-GEM,</cell></row><row><cell>EWC, MAS, ER, Logit-KD, PNN and ELLE after</cell></row><row><cell>finishing training on each domain when we choose</cell></row><row><cell>BERT L6_D384 as the initial PLM M 1 . The average</cell></row><row><cell>F1 when finishing training on the i-th domain is</cell></row><row><cell>calculated as follows:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>experiment on a stream of PLMs when growing BERT L6_D384 to BERT L12_D768 . For a model M j and its ancestor M i (1 ? i ? j -1), we randomly sample n [MASK] tokens from the raw corpus D j , and get the probability distributions</figDesc><table><row><cell>1DLYH (5</cell><cell>$*(0 0$6</cell><cell cols="2">(:&amp; /RJLW.'</cell><cell>311 (//(</cell><cell cols="2">Domain WB Naive</cell><cell>NS</cell><cell>REV</cell><cell>BIO</cell><cell>CS</cell><cell>AVG</cell></row><row><cell></cell><cell cols="2">0/1,:%</cell><cell></cell><cell></cell><cell>M1 M2</cell><cell cols="5">77.11 76.29 62.85 76.49 63.07 71.16 78.17 80.21 61.54 75.95 61.64 71.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M3</cell><cell cols="5">77.70 73.00 64.46 73.39 53.41 68.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell><cell cols="5">75.60 68.33 61.32 80.32 59.49 69.01</cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5 A-GEM</cell><cell cols="5">77.18 72.84 60.63 77.12 64.82 70.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M1</cell><cell cols="5">77.11 76.29 62.85 76.49 63.07 71.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2</cell><cell cols="5">77.99 76.80 61.99 75.53 59.65 71.50</cell></row><row><cell></cell><cell cols="2">+&lt;31HZV</cell><cell></cell><cell></cell><cell>M3 M4</cell><cell cols="5">77.71 72.96 63.92 73.39 53.66 68.39 74.76 71.80 61.41 79.70 62.00 69.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5</cell><cell cols="5">76.55 71.37 61.53 76.85 64.82 70.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M1 M2</cell><cell cols="5">77.11 76.29 62.85 76.49 63.07 71.16 78.13 76.75 61.68 75.12 62.69 70.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M3</cell><cell cols="5">76.60 73.79 64.04 72.11 53.95 70.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell><cell cols="5">76.09 71.90 61.83 80.62 64.26 70.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5</cell><cell cols="5">77.14 73.70 60.69 77.53 68.23 71.46</cell></row><row><cell></cell><cell cols="2">+HOSIXO5HYLHZ</cell><cell></cell><cell></cell><cell>MAS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M1</cell><cell cols="5">77.11 76.29 62.85 76.49 63.07 71.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2</cell><cell cols="5">78.30 80.15 61.18 75.87 59.96 71.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M3</cell><cell cols="5">77, 11 72.26 64.41 72.37 52.07 67.64</cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell><cell cols="5">76.21 73.21 61.34 80.81 62.33 70.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5</cell><cell cols="5">77.41 72.79 61.62 77.49 59.62 69.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ER</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M1</cell><cell cols="5">77.11 76.29 62.85 76.49 63.07 71.16</cell></row><row><cell></cell><cell cols="2">&amp;KHP3URW%LR</cell><cell></cell><cell></cell><cell>M2 M3</cell><cell cols="5">78.40 79.13 61.41 76.25 67.41 72.52 78.18 78.04 63.98 75.57 57.53 70.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell><cell cols="5">77.47 72.40 62.19 80.44 59.89 73.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5</cell><cell cols="5">77.57 72.15 61.92 78.25 63.49 70.68</cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Logit-KD M1</cell><cell cols="5">77.11 79.29 62.85 76.49 64.07 71.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2</cell><cell cols="5">76.33 69.77 63.14 75.21 59.19 68.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M3</cell><cell cols="5">76.63 71.32 64.97 74.46 55.91 68.66</cell></row><row><cell></cell><cell cols="2">$&amp;/$5&amp;&amp;6</cell><cell></cell><cell></cell><cell>M4 M5</cell><cell cols="5">76.84 69.12 64.30 76.96 59.11 69.27 77.21 69.48 63.86 76.82 58.87 69.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M1</cell><cell cols="5">76.04 74.11 62.31 75.09 59.57 69.42</cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2 M3</cell><cell cols="5">76.04 76.30 64.74 75.65 59.19 70.24 76.04 76.30 68.01 75.51 55.91 71.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4</cell><cell cols="5">76.04 76.30 68.01 79.46 59.11 72.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M5</cell><cell cols="5">76.04 76.30 68.01 79.46 58.87 73.01</cell></row><row><cell></cell><cell></cell><cell>$YHUDJH</cell><cell></cell><cell></cell><cell>ELLE M1</cell><cell cols="5">77.12 78.85 64.05 76.81 65.67 72.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M2</cell><cell cols="5">79.67 78.48 67.93 76.38 65.84 73.66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M3</cell><cell cols="5">81.99 86.75 69.32 78.14 62.63 75.77</cell></row><row><cell>$YJ)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M4 M5</cell><cell cols="5">82.55 81.18 69.19 83.27 69.03 77.04 83.17 81.83 68.47 82.87 72.69 77.81</cell></row><row><cell>:%</cell><cell>1HZV</cell><cell>5HYLHZ 'RPDLQ</cell><cell>%LR</cell><cell>&amp;6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 5: Specific and average F1 on downstream tasks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">of all domains of different lifelong learning methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">The initial PLM is chosen as BERT L6_D384 . The score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">is evaluated after each model finishes training on each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>domain.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">of a descendant PLM and its an-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">cestors. Representational similarity measures how</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">similar two PLMs represent the data. Specifically,</cell></row></table><note><p>we</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Specific and average F1 scores on downstream tasks from each domain after the PLM finishes training on each domain. We evaluate PLMs trained with different lifelong learning methods that choose BERT ?6_D384 as the initial model M 1 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Average perplexity (AP) and average increased perplexity (AP + ) of PLMs trained by different lifelong learning methods with half train wall time on Ns, Rev, Bio, CS domains and smaller memory containing 34M tokens for each domain. We evaluate the performance each time when PLMs finish training on one domain.</figDesc><table><row><cell>$56</cell><cell>1DLYH 0$6 5HSOD\ /RJLW.' 2XUV</cell><cell></cell><cell></cell></row><row><cell cols="2">:% 1HZV</cell><cell>5HYLHZ 'RPDLQ</cell><cell>%LR</cell><cell>&amp;6</cell></row><row><cell cols="5">Figure 6: Average representational similarity (ARS) of</cell></row><row><cell cols="5">a stream of PLMs comparing different lifelong learning</cell></row><row><cell cols="5">algorithms. We choose BERT L6_D384 as the initial PLM</cell></row><row><cell>M 1 .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Domain</cell><cell cols="3">WB NS REV BIO CS</cell><cell>AVG</cell></row><row><cell cols="2">Half train wall time</cell><cell></cell><cell></cell></row><row><cell>MAS</cell><cell cols="3">76.7 72.3 61.6 77.4 64.3</cell><cell>70.5</cell></row><row><cell>ER</cell><cell cols="3">78.0 71.0 61.1 77.4 65.8</cell><cell>70.7</cell></row><row><cell cols="4">Logit-KD 77.0 72.6 63.8 76.2 58.4</cell><cell>69.6</cell></row><row><cell>PNN</cell><cell cols="3">76.0 55.9 62.6 53.1 28.0</cell><cell>55.1</cell></row><row><cell>ELLE</cell><cell cols="3">82.0 78.4 68.7 81.7 74.0</cell><cell>77.0</cell></row><row><cell cols="2">Smaller memory</cell><cell></cell><cell></cell></row><row><cell>MAS</cell><cell cols="3">77.1 73.7 60.7 77.5 68.2</cell><cell>71.5</cell></row><row><cell>ER</cell><cell cols="3">77.9 72.0 61.5 76.3 63.6</cell><cell>70.3</cell></row><row><cell cols="4">Logit-KD 77.0 73.1 63.3 75.9 57.4</cell><cell>69.3</cell></row><row><cell>PNN</cell><cell cols="3">76.0 64.9 64.2 55.1 30.5</cell><cell>58.1</cell></row><row><cell>ELLE</cell><cell cols="3">82.9 80.5 68.9 82.6 74.2</cell><cell>77.8</cell></row><row><cell cols="5">Full train wall time &amp; memory (the main results in  ? 4)</cell></row><row><cell>ELLE</cell><cell cols="3">83.2 81.8 68.5 82.9 72.7</cell><cell>77.8</cell></row><row><cell cols="5">Table 11: Final downstream performance (F1) of BERT</cell></row><row><cell cols="5">on each domain after finishing pre-training on all do-</cell></row><row><cell cols="5">mains with half train wall time on Ns, Rev, Bio, CS</cell></row><row><cell cols="5">domains and smaller memory containing 34M tokens</cell></row><row><cell cols="5">for each domain. Experiments of NS domain are re-</cell></row><row><cell cols="5">peated for 10 times with different seeds and others are</cell></row><row><cell cols="2">repeated for 5 times.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Model architectures the investigated PLMs of WE+FRW, DE+FRW, WE+DE+FRW. We keep the total number of M i 's increased parameters for the above three strategies almost the same at each stage i.wall time) and (2) memory size for ELLE. Following the experimental setting in ? 4, we continually grow BERT L6_D384 to BERT L12_D768 on a stream of data from 5 domains. We test the performance of ELLE and a series of lifelong learning baselines (MAS, ER, Logit-KD and PNN), by (1) reducing the train wall time by half (for NS, REV, BIO and CS domain) and (2) randomly sample only 34M tokens (1% of the full corpus) as the memory D sub</figDesc><table><row><cell cols="5">Model nparams nlayers dmodel nheads</cell><cell>dFFN</cell><cell>lr</cell></row><row><cell cols="2">WE + FRW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>30.3M</cell><cell>6</cell><cell>384</cell><cell>6</cell><cell cols="2">1536 5.0 ? 10 -4</cell></row><row><cell>M2</cell><cell>53.6M</cell><cell>6</cell><cell>576</cell><cell>9</cell><cell cols="2">2304 5.0 ? 10 -4</cell></row><row><cell>M3</cell><cell>82.2M</cell><cell>6</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 5.0 ? 10 -4</cell></row><row><cell>M4</cell><cell>104M</cell><cell>6</cell><cell>896</cell><cell>14</cell><cell cols="2">3584 5.0 ? 10 -4</cell></row><row><cell>M5</cell><cell>129M</cell><cell>6</cell><cell>1024</cell><cell>16</cell><cell cols="2">4096 5.0 ? 10 -4</cell></row><row><cell cols="2">DE + FRW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>30.3M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 5.0 ? 10 -4</cell></row><row><cell>M2</cell><cell>51.6M</cell><cell>18</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 2.5 ? 10 -4</cell></row><row><cell>M3</cell><cell>83.6M</cell><cell>36</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 2.5 ? 10 -4</cell></row><row><cell>M4</cell><cell>105M</cell><cell>48</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 2.5 ? 10 -4</cell></row><row><cell>M5</cell><cell>126M</cell><cell>60</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 2.5 ? 10 -4</cell></row><row><cell cols="2">WE + DE + FRW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M1</cell><cell>30.3M</cell><cell>6</cell><cell>384</cell><cell>6</cell><cell cols="2">1536 5.0 ? 10 -4</cell></row><row><cell>M2</cell><cell>51.5M</cell><cell>8</cell><cell>512</cell><cell>8</cell><cell cols="2">2048 5.0 ? 10 -4</cell></row><row><cell>M3</cell><cell>82.2M</cell><cell>10</cell><cell>640</cell><cell>10</cell><cell cols="2">2560 5.0 ? 10 -4</cell></row><row><cell>M4</cell><cell>102M</cell><cell>11</cell><cell>704</cell><cell>11</cell><cell cols="2">2816 5.0 ? 10 -4</cell></row><row><cell>M5</cell><cell>125M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell cols="2">3072 5.0 ? 10 -4</cell></row></table><note><p>i for each corpus i, compared with the memory size 200M in ? 4.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2020AAA0106502</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs>. This work is also supported by the <rs type="funder">Pattern Recognition Center, WeChat AI, Tencent Inc.</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9DQPf2j">
					<idno type="grant-number">2020AAA0106502</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Beinborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rochelle</forename><surname>Choenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01539</idno>
		<title level="m">Blackbox meets blackbox: Representational similarity and stability analysis of neural language models and brains</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Memory aware synapses: Learning what (not) to forget</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2019a. Efficient lifelong learning with A-GEM</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On tiny episodic memories in continual learning</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1902.10486</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">2021. bert2bert: Towards reusable pretrained language models</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2110.07143</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Episodic memory in lifelong language learning</title>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13122" to="13131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Time-aware language models as temporal knowledge bases</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>abs/2106.15110</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient training of BERT by progressively stacking</title>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 2019, 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the transformer growth for progressive BERT training</title>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.406</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5174" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Demix layers: Disentangling domains for modular language modeling</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/2108.05036</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<idno>abs/2106.07139</idno>
		<imprint>
			<date type="published" when="2021-06">Jun Zhu. 2021</date>
			<publisher>Wayne Xin Zhao</publisher>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the effectiveness of adapterbased tuning for pretrained language model adaptation</title>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.172</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2208" to="2222" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-04-11">2016. 2016. April 11 -15, 2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards continual knowledge learning of language models</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonghyeon</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joongbo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janghoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeonghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">Jungkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno>abs/2110.03215</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lifelong pretraining: Continually adapting language models to emerging corpora</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Shang-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2110.08534</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring the evolution of a scientific field through citation frames</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mc-Farland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00028</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 4: Hyperpartisan news detection</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payam</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Chemprot-3.0: a global chemical biology diseases mapping</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Kringelum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonny</forename><surname>Kim Kjaerulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Taboureau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Train big, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5958" to="5968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767755</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-08-09">2015. August 9-13, 2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Maosong Sun, et al. 2021a. Knowledge inheritance for pre-trained language models</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2105.13880</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maosong Sun, et al. 2021b. Exploring lowdimensional intrinsic task subspace via prompt tuning</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07867</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.587</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21">2017. 2017. July 21-26, 2017</date>
			<biblScope unit="page" from="5533" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Experience replay for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="348" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Progressive neural networks</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1606.04671</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>abs/1907.10597</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan, Stockholm, Sweden</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm?ssan, Stockholm, Sweden</meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4535" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="10435" to="10444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06719</idno>
		<title level="m">On transferability of prompt tuning for natural language understanding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LAMOL: language modeling for lifelong language learning</title>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pretrained language model in continual learning: A comparative study</title>
		<author>
			<persName><forename type="first">Tongtong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="9051" to="9062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Accelerating training of transformer-based language models with progressive layer dropping</title>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2010.13369</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
