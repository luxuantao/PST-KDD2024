<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binxing</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIMLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose SIMLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA [6], to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SIMLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. Remarkably, SIMLM even outperforms multi-vector approaches such as ColBERTv2 [35]   which incurs significantly more storage cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Passage retrieval is an important component in applications like ad-hoc information retrieval, opendomain question answering <ref type="bibr" target="#b16">[17]</ref>, retrieval-augmented generation <ref type="bibr" target="#b19">[20]</ref> and fact verification <ref type="bibr" target="#b38">[38]</ref>. Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM) <ref type="bibr" target="#b9">[10]</ref>, increasing attention is being paid to neural dense retrieval methods <ref type="bibr" target="#b42">[42]</ref>. Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors.</p><p>Table <ref type="table">1</ref>: Inconsistent performance trends between different models on retrieval task and NLU tasks. We report MRR@10 on the dev set of MS-MARCO passage ranking dataset and test set results on GLUE benchmark. Details are available in the Appendix A. Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pre-training does not solve all the problems. As shown in Table <ref type="table">1</ref>, improved pre-training techniques that are verified by benchmarks like GLUE <ref type="bibr" target="#b39">[39]</ref> do not result in consistent performance gain for retrieval tasks. Similar observations are also made by Lu et al. <ref type="bibr" target="#b23">[24]</ref>. We hypothesize that to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence Preprint.</p><p>arXiv:2207.02578v1 [cs.IR] 6 Jul 2022 prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa <ref type="bibr" target="#b21">[22]</ref> and ELECTRA do not have such sequence-level tasks.</p><p>In this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ an ELECTRA-style generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help training biencoder-based<ref type="foot" target="#foot_0">1</ref> dense retrievers.</p><p>Compared to existing pre-training approaches such as Condenser <ref type="bibr" target="#b11">[12]</ref> or coCondenser <ref type="bibr" target="#b12">[13]</ref>, our method has several advantages. First, it does not have any extra skip connection between the encoder and decoder, thus reducing the bypassing effects and simplifying the architecture design. Second, similar to ELECTRA pre-training, our replaced language modeling objective can back-propagate gradients at all positions and does not have <ref type="bibr">[MASK]</ref> tokens in the inputs during pre-training. Such a design increases sample efficiency and decreases the input distribution mismatch between pre-training and fine-tuning.</p><p>To verify the effectiveness of our method, we conduct experiments on several large-scale web search and open-domain QA datasets: MS-MARCO passage ranking <ref type="bibr" target="#b1">[2]</ref>, TREC Deep Learning Track datasets, and the Natural Questions (NQ) dataset <ref type="bibr" target="#b18">[19]</ref>. Results show substantial gains over other competitive methods using BM25 hard negatives only. When combined with mined hard negatives and cross-encoder based re-ranker distillation, we can achieve new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dense Retrieval The field of information retrieval (IR) <ref type="bibr" target="#b27">[28]</ref> aims to find the relevant information given an ad-hoc query and has played a key role in the success of modern search engines. In recent years, IR has witnessed a paradigm shift from traditional BM25-based inverted index retrieval to neural dense retrieval <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b16">17]</ref>. BM25-based retrieval, though efficient and interpretable, suffers from the issue of lexical mismatch between the query and passages. Methods like document expansion <ref type="bibr" target="#b29">[30]</ref> or query expansion <ref type="bibr" target="#b0">[1]</ref> are proposed to help mitigate this issue. In contrast, neural dense retrievers first map the query and passages to a low-dimensional vector space, and then perform semantic matching. Popular methods include DSSM <ref type="bibr" target="#b15">[16]</ref>, C-DSSM <ref type="bibr" target="#b37">[37]</ref>, and DPR <ref type="bibr" target="#b16">[17]</ref> etc. Inference can be done efficiently with approximate nearest neighbor (ANN) search algorithms such as HNSW <ref type="bibr" target="#b26">[27]</ref>.</p><p>Some recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> show that neural dense retrievers may fail to capture some exact lexical match information. To mitigate this issue, Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposes to use BM25 as a complementary teacher model, ColBERT <ref type="bibr" target="#b17">[18]</ref> instead replaces simple dot-product matching with a more complex token-level MaxSim interaction, while COIL <ref type="bibr" target="#b13">[14]</ref> incorporates lexical match information into the scoring component of neural retrievers. Our proposed pre-training method aims to adapt the underlying text encoders for retrieval tasks, and can be easily integrated with existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training for Dense Retrieval</head><p>With the development of large-scale language model pre-training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref>, Transformer-based models such as BERT <ref type="bibr" target="#b9">[10]</ref> have become the de facto backbone architecture for learning text representations. However, most pre-training tasks are designed without any prior knowledge of downstream applications. Chang et al. <ref type="bibr" target="#b2">[3]</ref> presents three heuristically constructed pre-training tasks tailored for text retrieval: inverse cloze task (ICT), body first selection (BFS), and wiki link prediction (WLP). These tasks exploit the document structure of Wikipedia pages to automatically generate contrastive pairs. Other related pre-training tasks include representative words prediction <ref type="bibr" target="#b24">[25]</ref> and contrastive span prediction <ref type="bibr" target="#b25">[26]</ref> etc.</p><p>Another line of research builds upon the intuition that the [CLS] vector should encode all the important information in the given text for robust matching, which is also one major motivation for this paper. Such methods include Condenser <ref type="bibr" target="#b11">[12]</ref>, coCondenser <ref type="bibr" target="#b12">[13]</ref>, SEED <ref type="bibr" target="#b23">[24]</ref>, DiffCSE <ref type="bibr" target="#b4">[5]</ref>, and RetroMAE <ref type="bibr" target="#b22">[23]</ref> etc. Compared with Condenser and coCondenser, our pre-training architecture does not have skip connections between the encoder and decoder, and therefore forces the [CLS] vector to encode as much information as possible. RetroMAE is a concurrent work by Liu and Shao <ref type="bibr" target="#b22">[23]</ref> that combines a bottleneck architecture and the masked auto-encoding objective.</p><p>3 SimLM , where x denotes a single passage. Since our motivation is to have a general pre-training method, we do not assume access to any query or human-labeled data.</p><p>The overall pre-training architecture is shown in Figure <ref type="figure">1</ref>. Given a text sequence x, its tokens are randomly replaced with probability p by two sequential operations: random masking with probability p denoted as x = Mask(x, p), and then sampling with an ELECTRA-style generator g denoted as Sample(g, x ). Due to the randomness of sampling, a replaced token can be the same as the original one. The above operations are performed twice with potentially different replace probabilities p enc and p dec to get the encoder input x enc and decoder input x dec .</p><p>x enc = Sample(g, Mask(x, p enc ))</p><formula xml:id="formula_0">x dec = Sample(g, Mask(x, p dec ))<label>(1)</label></formula><p>We also make sure that any replaced token in x enc is also replaced in x dec to increase the difficulty of the pre-training task.</p><p>The encoder is a deep multi-layer Transformer that can be initialized with pre-trained models like BERT <ref type="bibr" target="#b9">[10]</ref>. It takes x enc as input and outputs the last layer [CLS] vector h cls as a representation bottleneck. The decoder is a 2-layer shallow Transformer with a language modeling head and takes x dec and h cls as inputs. Unlike the decoder component in autoregressive sequence-to-sequence models, the self-attention in our decoder is bi-directional. The pre-training task is replaced language modeling for both the encoder and decoder, which predicts the tokens before replacement at all positions. The loss function is the token-level cross-entropy. The encoder loss L enc is shown as follows:</p><formula xml:id="formula_1">min L enc = - 1 |x| |x| i=1 log p(x[i] | x enc )<label>(2)</label></formula><p>Similarly for the decoder loss L dec . The final pre-training loss is their simple sum:</p><formula xml:id="formula_2">L pt = L enc + L dec .</formula><p>We do not fine-tune the parameters of the generator as our preliminary experiments do not show any performance gain.</p><p>It is often reasonable to assume access to the target retrieval corpus before seeing any query. Therefore, we directly pre-train on the target corpus similar to coCondenser <ref type="bibr" target="#b12">[13]</ref>. After the pre-training finishes, we throw away the decoder and only keep the encoder for supervised fine-tuning.</p><p>Since the decoder has very limited modeling capacity, it needs to rely on the representation bottleneck to perform well on the pre-training task. For the encoder, it should learn to compress all the semantic information and pass it to the decoder through the bottleneck.  Compared to training text classification or generation models, training state-of-the-art dense retrieval models requires a relatively complicated procedure. In Figure <ref type="figure" target="#fig_0">2</ref>, we show our supervised fine-tuning pipeline. In contrast to previous approaches, our proposed pipeline is relatively straightforward and does not require joint training <ref type="bibr" target="#b33">[34]</ref> or re-building index periodically <ref type="bibr" target="#b41">[41]</ref>. Each stage takes the outputs from the previous stage as inputs and can be trained in a standalone fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning</head><p>Retriever 1 Given a labeled query-passage pair (q + , d + ), we take the last-layer [CLS] vector of the pre-trained encoder as their representations (h q + , h d + ). Both the in-batch negatives and BM25 hard negatives are used to compute the contrastive loss L cont :</p><formula xml:id="formula_3">min L cont = -log ?(q + , d + ) ?(q + , d + ) + ni?N (?(q + , n i ) + ?(d + , n i ))<label>(3)</label></formula><p>Where N denotes all the negatives, and ?(q, d) is a function to compute the matching score between query q and passage d. In this paper, we use temperature-scaled cosine similarity function:</p><formula xml:id="formula_4">?(q, d) = exp( 1 ? cos(h q , h d ))</formula><p>. ? is a temperature hyper-parameter and set to a constant 0.02 in our experiments.</p><p>Retriever 2 It is trained in the same way as Retriever 1 except that the hard negatives are mined based on a well-trained Retriever 1 checkpoint.</p><p>Re-ranker is a cross-encoder that re-ranks the top-k results of Retriever 2 . It takes the concatenation of query q and passage d as input and outputs a real-valued score ?(q, d). Given a labeled positive pair (q + , d + ) and n -1 hard negative passages randomly sampled from top-k predictions of Retriever 2 , we adopt a listwise loss to train the re-ranker:</p><formula xml:id="formula_5">-log exp(?(q + , d + )) exp(?(q + , d + )) + n-1 i=1 exp(?(q + , d - i ))<label>(4)</label></formula><p>The cross-encoder architecture can model the full interaction between the query and the passage, making it suitable to be a teacher model for knowledge distillation.</p><p>Retriever distill Although cross-encoder based re-ranker is powerful, it is not scalable enough for first-stage retrieval. To combine the scalability of biencoder and the effectiveness of cross-encoder, we can train a biencoder-based retriever by distilling the knowledge from the re-ranker. The re-ranker from the previous stage is employed to compute scores for both positive pairs and mined negatives from Retriever 2 . These scores are then used as training data for knowledge distillation. With n -1 mined hard negatives, we use KL (Kullback-Leibler) divergence L kl as the loss function for distilling the soft labels:</p><formula xml:id="formula_6">L kl = n i=1 p i ranker log p i ranker p i ret (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where p ranker and p ret are normalized probabilities from the re-ranker teacher and Retriever distill student.</p><p>For training with the hard labels, we use the contrastive loss L cont as defined in Equation <ref type="formula" target="#formula_3">3</ref>. The final loss is their linear interpolation: L = L kl + ?L cont .</p><p>Our pre-trained SimLM model is used to initialize all three biencoder-based retrievers but not the cross-encoder re-ranker. Since our pre-training method only affects model initialization, it can be easily integrated into other more effective training pipelines. For evaluation metrics, we use MRR@10, Recall@50, and Recall@1000 for MS-MARCO, nDCG@10 for TREC DL, and Recall@20, Recall@100 for the NQ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Implementation Details For pre-training, we initialize the encoder with BERT base (uncased version).</p><p>The decoder is a two-layer Transformer whose parameters are initialized with the last two layers of BERT base . The generator is borrowed from the ELECTRA base generator, and its parameters are frozen during pre-training. We pre-train for 80k steps for MS-MARCO corpus and 200k steps for NQ corpus, which roughly correspond to 20 epochs. Pre-training is based on 8 V100 GPUs. With automatic mixed-precision training, it takes about 1.5 days and 3 days for the MS-MARCO and NQ corpus respectively.</p><p>For fine-tuning on the MS-MARCO dataset, we train for 3 epochs with a peak learning rate 2 ? 10 -5 . Each batch consists of 16 queries, each query has 1 positive passage and 15 randomly sampled hard negatives. One shared encoder is used to encode both the query and passages. We start with the official BM25 hard negatives in the first training round and then change to mined hard negatives. During inference, given a query, we use brute force search to rank all the passages for a fair comparison with previous works.</p><p>For more implementation details, please check out the Appendix section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Table <ref type="table">3</ref>: Results on the test set of Natural Questions (NQ) dataset. Listed results of SimLM are based on Retriever 2 and do not use knowledge distillation.</p><p>Model NQ R@20 R@100 BM25 59.1 73.7 DPR single <ref type="bibr" target="#b16">[17]</ref> 78.4 85.4 ANCE <ref type="bibr" target="#b41">[41]</ref> 81.9 87.5 RocketQA <ref type="bibr" target="#b30">[31]</ref> 82.7 88.5 Condenser <ref type="bibr" target="#b11">[12]</ref> 83.2 88.4 PAIR <ref type="bibr" target="#b32">[33]</ref> 83.5 89.1 RocketQAv2 <ref type="bibr" target="#b33">[34]</ref> 83.7 89.0 coCondenser <ref type="bibr" target="#b12">[13]</ref> 84 We list the main results in Table <ref type="table" target="#tab_3">2</ref> and<ref type="table">3</ref>. For the MS-MARCO passage ranking dataset, the numbers are based on the Retriever distill in Figure <ref type="figure" target="#fig_0">2</ref>. Our method establishes new state-of-the-art with MRR@10 41.1, even outperforming multi-vector methods like ColBERTv2. As shown in Table <ref type="table">4</ref>, ColBERTv2 has a 6x storage cost as it stores one vector per token instead of one vector per passage. It also requires a customized two-stage index search algorithm during inference, while our method can utilize readily available vector search libraries.</p><p>The TREC DL datasets have more fine-grained human annotations, but also much fewer queries (less than 100 labeled queries). We find that using different random seeds could have a 1%-2% difference in terms of nDCG@10. Though our model performs slightly worse on the 2019 split compared to coCondenser, we do not consider such difference as significant.</p><p>For passage retrieval in the open-domain QA setting, a passage is considered relevant if it contains the correct answer for a given question. In Table <ref type="table">3</ref>, our model achieves R@20 84.3 and R@100 89.3 on the NQ dataset, which are comparable to or better than other methods. For end-to-end evaluation of question answering accuracy, we will leave it as future work. Though SimLM achieves substantial gain for biencoder-based retrieval, its success for re-ranking is not as remarkable. In Table <ref type="table" target="#tab_5">5</ref>, when used as initialization for re-ranker training, SimLM outperforms BERT base by 0.6% but still lags behind ELECTRA base . Next, we zoom in on the impact of each stage in our training pipeline. In Table <ref type="table" target="#tab_6">6</ref>, we mainly compare with coCondenser <ref type="bibr" target="#b12">[13]</ref>. With BM25 hard negatives only, we can achieve MRR@10 38.0, which already matches the performance of many strong models like RocketQA <ref type="bibr" target="#b30">[31]</ref>. Model-based hard negative mining and re-ranker distillation can bring further gains. This is consistent with many previous works <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b33">34]</ref>. We also tried an additional round of mining hard negatives but did not observe any meaningful improvement.</p><p>Based on the results of Table <ref type="table" target="#tab_6">6</ref>, there are many interesting research directions to pursue. For example, how to simplify the training pipeline of dense retrieval systems while still maintaining competitive performance? And how to further close the gap between biencoder-based retriever and cross-encoder based re-ranker?</p><p>5 Analysis Besides our proposed replaced language modeling objective, we also tried several other pre-training objectives as listed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Variants of Pre-training Objectives</head><p>Enc-Dec MLM uses the same encoder-decoder architecture as in Figure <ref type="figure">1</ref> but without the generator. The inputs are randomly masked texts and the pre-training objective is masked language modeling (MLM) over the masked tokens only. The mask rate is the same as our method for a fair comparison, which is 30% for the encoder and 50% for the decoder. In contrast, RetroMAE <ref type="bibr" target="#b22">[23]</ref> uses a specialized decoding mechanism to derive supervision signal from all tokens in the decoder side.</p><p>Condenser is a pre-training architecture proposed by Gao and Callan <ref type="bibr" target="#b11">[12]</ref>. Here we pre-train Condenser with a 30% mask rate on the target corpus.</p><p>MLM is the same as the original BERT pre-training objective with a 30% mask rate.</p><p>Enc-Dec RTD is the same as our method in Figure <ref type="figure">1</ref> except that we use replaced token detection (RTD) <ref type="bibr" target="#b5">[6]</ref> as pre-training task for both the encoder and decoder. This variant shares some similarity with DiffCSE <ref type="bibr" target="#b4">[5]</ref>. The main difference is that the input for DiffCSE encoder is the original text, making it a much easier task. Our preliminary experiments with DiffCSE pre-training do not result in any improvement.</p><p>AutoEncoder attempts to reconstruct the inputs based on the bottleneck representation. The encoder input is the original text without any mask, and the decoder input only consists of [MASK] tokens and [CLS] vector from the encoder.</p><p>BERT base just uses off-the-shelf checkpoint published by Devlin et al. <ref type="bibr" target="#b9">[10]</ref>. It serves as a baseline to compare against various pre-training objectives.</p><p>The results are summarized in Table <ref type="table" target="#tab_7">7</ref>. Naive auto-encoding only requires memorizing the inputs and does not need to learn any contextualized features. As a result, it becomes the only pre-training objective that underperforms BERT base . Condenser is only slightly better than simple MLM pretraining, which is possibly due to the bypassing effects of the skip connections in Condenser. Enc-Dec MLM substantially outperforms Enc-Dec RTD, showing that MLM is a better pre-training task than RTD for retrieval tasks. This is consistent with the results in Table <ref type="table">1</ref>. Considering the superior performance of RTD pre-trained models on benchmarks like GLUE, we believe further research efforts are needed to investigate the reason behind this phenomenon. In the experiments, we use fairly large replace rates (30% for the encoder and 50% for the decoder). This is in stark contrast to the mainstream choice of 15%. In Table <ref type="table" target="#tab_8">8</ref>, we show the results of pre-training with different replace rates. Our model is quite robust to a wide range of values with 30%-40% encoder replace rate performing slightly better. Similar findings are also made by Wettig et al. <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Replace Rate</head><p>One interesting extreme scenario is a 100% replace rate on the decoder side. In such a case, the decoder has no access to any meaningful context. It needs to predict the original texts solely based on the representation bottleneck. This task may be too difficult and has negative impacts on the encoder.  Since pre-training can be costly in terms of both time and carbon emission, it is preferred to have an objective that converges fast. Our proposed method shares two advantages of ELECTRA <ref type="bibr" target="#b5">[6]</ref>. First, the loss is computed over all input tokens instead of a small percentage of masked ones. Second, the issue of input distribution mismatch is less severe than MLM, where the [MASK] token is seen during pre-training but not for supervised fine-tuning. In Figure <ref type="figure" target="#fig_1">3</ref>, our method achieves competitive results with only 10k training steps and converges at 60k, while MLM still slowly improves with more steps.  For a typical retrieval task, the number of candidate passages is much larger than the number of labeled queries, and many passages are never seen during training. Take the NQ dataset as an example, it has 21M candidate passages but only less than 80k question-answer pairs for training. In the experiments, we directly pre-train on the target corpus. Such pre-training can be regarded as implicit memorization of the target corpus in a query-agnostic way. One evidence to support this argument is that, as shown in Table <ref type="table" target="#tab_7">7</ref>, simple MLM pre-training on target corpus can have large performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of</head><p>An important research question to ask is: will there be any benefits of our method when pre-training on non-target corpus? In Table <ref type="table" target="#tab_10">9</ref>, the largest performance gains are obtained when the corpus matches between pre-training and fine-tuning. If we pre-train on the MS-MARCO corpus and fine-tune on the labeled NQ dataset or the other way around, there are still considerable improvements over the baseline. We hypothesize that this is due to the model's ability to compress information into a representation bottleneck. Such ability is beneficial for training robust biencoder-based retrievers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes SIMLM, a novel pre-training method for dense passage retrieval. It follows an encoder-decoder architecture with a representation bottleneck in between. The encoder learns to compress all the semantic information into a dense vector and passes it to the decoder to perform well on the replaced language modeling task. When used as initialization in a dense retriever training pipeline, our model achieves competitive results on several large-scale passage retrieval datasets. We also provide detailed ablation analyses to show the key ingredients behind its success.</p><p>For future work, we would like to increase the model size and the corpus size to examine the scaling effects. It is also interesting to explore other pre-training mechanisms to support unsupervised dense retrieval and multilingual retrieval.</p><p>negatives only. For BERT and RoBERTa, we use the same hyperparameters as discussed in Section 4.1. For ELECTRA, we train for 6 epochs with a peak learning rate 4 ? 10 -5 since it converges much slower. The hyper-parameters for our proposed pre-training and fine-tuning are listed in Table <ref type="table" target="#tab_13">11</ref> and 13, respectively. The generator is initialized with the released one by ELECTRA authors <ref type="foot" target="#foot_2">3</ref> , and its parameters are frozen during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>For fine-tuning on the NQ dataset, we reuse most hyper-parameters values from MS-MARCO training.</p><p>A few exceptions are listed below. We fine-tune for 20k steps with learning rate 5 ? 10 -6 . The maximum length for passage is 192. The mined hard negatives come from top-100 predictions that do not contain any correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Variants of Generators</head><p>In the ELECTRA pre-training, the generator plays a critical role. Using either a too strong or too weak generator hurts the learnability and generalization of the discriminator. We also tried several variants of generators. In Table <ref type="table" target="#tab_14">12</ref>, "frozen generator" keeps the generator parameters unchanged during our pre-training, "joint train" also fine-tunes the generator parameters, and "joint train w/ random init" uses randomly initialized generator parameters. We do not observe any significant performance difference between these variants. In our experiments, we simply use "frozen generator" as it has faster training speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our supervised fine-tuning pipeline. Note that we only use SimLM to initialize the biencoder-based retrievers. For cross-encoder based re-ranker, we use off-the-shelf pre-trained models such as ELECTRA base .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our pre-training objective converges faster and consistently outperforms vanilla masked language model pre-training. The y-axis shows the MRR@10 on the dev set of MS-MARCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 4</head><label>4</label><figDesc>On the Choice of Pre-training Corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results on MS-MARCO passage ranking and TREC datasets. Results with * are from our reproduction with public checkpoints. ?: from Pyserini [21]. MARCO dataset is based on Bing search results and consists of about 500k labeled queries and 8.8M passages. Since the test set labels are not publicly available, we report results on the development set with 6980 queries. The NQ dataset is targeted for open-domain QA with about 80k question-answer pairs in the training set and 21M passages based on Wikipedia.</figDesc><table><row><cell>Model</cell><cell>+distill</cell><cell>vector? single</cell><cell cols="2">MS MARCO dev</cell><cell></cell><cell>TREC DL 19 TREC DL 20</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MRR@10 R@50 R@1k</cell><cell>nDCG@10</cell><cell>nDCG@10</cell></row><row><cell>Sparse retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell></cell><cell></cell><cell>18.5</cell><cell>58.5</cell><cell>85.7</cell><cell>51.2  *</cell><cell>47.7  *</cell></row><row><cell>DeepCT [9]</cell><cell></cell><cell></cell><cell>24.3</cell><cell>69.0</cell><cell>91.0</cell><cell>57.2</cell><cell>-</cell></row><row><cell>docT5query [29]</cell><cell></cell><cell></cell><cell>27.7</cell><cell>75.6</cell><cell>94.7</cell><cell>64.2</cell><cell>-</cell></row><row><cell>Dense retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANCE [41]</cell><cell></cell><cell></cell><cell>33.0</cell><cell>-</cell><cell>95.9</cell><cell>64.5  ?</cell><cell>64.6  ?</cell></row><row><cell>SEED [24]</cell><cell></cell><cell></cell><cell>33.9</cell><cell>-</cell><cell>96.1</cell><cell>-</cell><cell>-</cell></row><row><cell>TAS-B [15]</cell><cell></cell><cell></cell><cell>34.0</cell><cell>-</cell><cell>97.5</cell><cell>71.2</cell><cell>69.3</cell></row><row><cell>RetroMAE [23]</cell><cell></cell><cell></cell><cell>35.0</cell><cell>-</cell><cell>97.6</cell><cell>-</cell><cell>-</cell></row><row><cell>COIL [14]</cell><cell></cell><cell></cell><cell>35.5</cell><cell>-</cell><cell>96.3</cell><cell>70.4</cell><cell>-</cell></row><row><cell>ColBERT [18]</cell><cell></cell><cell></cell><cell>36.0</cell><cell>82.9</cell><cell>96.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Condenser [12]</cell><cell></cell><cell></cell><cell>36.6</cell><cell>-</cell><cell>97.4</cell><cell>69.8</cell><cell>-</cell></row><row><cell>RocketQA [31]</cell><cell></cell><cell></cell><cell>37.0</cell><cell>85.5</cell><cell>97.9</cell><cell>-</cell><cell>-</cell></row><row><cell>PAIR [33]</cell><cell></cell><cell></cell><cell>37.9</cell><cell>86.4</cell><cell>98.2</cell><cell>-</cell><cell>-</cell></row><row><cell>coCondenser [13]</cell><cell></cell><cell></cell><cell>38.2</cell><cell>86.5  *</cell><cell>98.4</cell><cell>71.7  *</cell><cell>68.4  *</cell></row><row><cell>RocketQAv2 [34]</cell><cell></cell><cell></cell><cell>38.8</cell><cell>86.2</cell><cell>98.1</cell><cell>-</cell><cell>-</cell></row><row><cell>AR2 [43]</cell><cell></cell><cell></cell><cell>39.5</cell><cell>87.8</cell><cell>98.6</cell><cell>-</cell><cell>-</cell></row><row><cell>ColBERTv2 [35]</cell><cell></cell><cell></cell><cell>39.7</cell><cell>86.8</cell><cell>98.4</cell><cell>-</cell><cell>-</cell></row><row><cell>SIMLM</cell><cell></cell><cell></cell><cell>41.1</cell><cell>87.8</cell><cell>98.7</cell><cell>71.2</cell><cell>69.7</cell></row><row><cell>4.1 Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Datasets and Evaluation We use MS-MARCO passage ranking [2], TREC Deep Learning (DL)</cell></row><row><cell cols="7">Track 2019 [7] and 2020 [8], Natural Questions (NQ) [19, 17] datasets for training and evaluation.</cell></row><row><cell>The MS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Re-ranker performance w/ different pre-trained models on the dev set of MS-MARCO passage ranking dataset.</figDesc><table><row><cell>Model</cell><cell>MRR@10</cell></row><row><cell>BERT base</cell><cell>42.3</cell></row><row><cell>ELECTRA base</cell><cell>43.7</cell></row><row><cell>SIMLM</cell><cell>42.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art dense retriever coCondenser under various settings on the dev set of MS-MARCO passage ranking dataset. Results with * are from our reproduction.</figDesc><table><row><cell></cell><cell cols="2">MRR@10 R@1k</cell></row><row><cell>coCondenser</cell><cell></cell><cell></cell></row><row><cell>BM25 negatives</cell><cell>35.7</cell><cell>97.8</cell></row><row><cell>+ mined negatives</cell><cell>38.2</cell><cell>98.4</cell></row><row><cell>+ distillation</cell><cell>40.2  *</cell><cell>98.3  *</cell></row><row><cell>SIMLM</cell><cell></cell><cell></cell></row><row><cell>BM25 negatives (Retriever 1 )</cell><cell>38.0</cell><cell>98.3</cell></row><row><cell>+ mined negatives (Retriever 2 )</cell><cell>39.1</cell><cell>98.6</cell></row><row><cell>+ (Retriever distill )</cell><cell>41.1</cell><cell>98.7</cell></row><row><cell>Cross-encoder re-ranker</cell><cell>43.7</cell><cell>98.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Different pre-training objectives. Reported numbers are MRR@10 on the dev set of MS-MARCO passage ranking. We finetune the pre-trained models with official BM25 hard negatives.</figDesc><table><row><cell></cell><cell cols="7">SIMLM Enc-Dec MLM Condenser MLM Enc-Dec RTD AutoEncoder BERT base</cell></row><row><cell>MRR@10</cell><cell>38.0</cell><cell>37.7</cell><cell>36.9</cell><cell>36.7</cell><cell>36.2</cell><cell>32.8</cell><cell>33.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>MS-MARCO passage ranking performance w.r.t different token replace rates. Here the replace rate is the percentage of masked tokens fed to the generator.</figDesc><table><row><cell cols="3">encoder decoder MRR@10</cell></row><row><cell>15%</cell><cell>15%</cell><cell>37.6</cell></row><row><cell>15%</cell><cell>30%</cell><cell>37.5</cell></row><row><cell>30%</cell><cell>30%</cell><cell>37.9</cell></row><row><cell>30%</cell><cell>50%</cell><cell>38.0</cell></row><row><cell>40%</cell><cell>60%</cell><cell>38.0</cell></row><row><cell>30%</cell><cell>100%</cell><cell>36.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Fine-tuning performance w.r.t different pre-training corpora. We use BM25 negatives for MS-MARCO and mined negatives for NQ. "Wikipedia" refers to the corpus with 21M 100-word Wikipedia passages released by Karpukhin et al.<ref type="bibr" target="#b16">[17]</ref>. "none" use BERT base as the foundation model.</figDesc><table><row><cell>Corpus</cell><cell cols="4">MS-MARCO MRR@10 R@1k R@20 R@100 NQ</cell></row><row><cell>none</cell><cell>33.7</cell><cell>95.9</cell><cell>82.9</cell><cell>88.0</cell></row><row><cell>MS-MARCO</cell><cell>38.0</cell><cell>98.3</cell><cell>83.3</cell><cell>88.6</cell></row><row><cell>Wikipedia</cell><cell>36.3</cell><cell>97.4</cell><cell>84.3</cell><cell>89.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Some (cherry-picked) examples from the dev set of MS-MARCO passage ranking dataset. We show the query, top retrieved passages, and their binary relevance labels. Relevant text snippets are shown in italic. The little boy who talks to the animals in the Winnie-the-Pooh stories is called Christopher Robin, which is the name of A. A. Milne's real-life son, who was born in 1920. On August 21, 1921, the real-life Christopher Robin Milne received a stuffed bear from Harrods for his first birthday . . . So, it looks like we were lied to our entire childhood! Winnie the Pooh is not a boy. SHE is a girl and she's from Canada, not England. Really! In a new picture book called Finding Winnie: The True Story of the World's Most Famous Bear, we learn that Winnie is actually named after . . .</figDesc><table><row><cell>query</cell><cell>was winnie the pooh a boy</cell></row><row><cell></cell><cell>Rank: 1, Relevant:</cell></row><row><cell cols="2">BERTbase Passage: SIMLM Rank: 1, Relevant: Passage: query colorado routing number loveland colorado</cell></row><row><cell></cell><cell>Rank: 1, Relevant:</cell></row><row><cell>BERTbase</cell><cell>Passage: Loveland, CO is currently served by one area code which is area code 970. In addition to Loveland,</cell></row><row><cell></cell><cell>CO area code information read more about area code 970 details and Colorado area codes. . . .</cell></row><row><cell></cell><cell>Rank: 2, Relevant:</cell></row><row><cell>SIMLM</cell><cell>Passage: 107006787 Routing Transit Number (RTN) for Advantage Bank Main Office located at Loveland, Colorado, CO, 80538, United States, Street Address 1475 NORTH DENVER AVENUE,</cell></row><row><cell></cell><cell>Telephone Number 970-613-1982 . . .</cell></row></table><note><p>To qualitatively understand the gains brought by pre-training, we show several examples in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>The BERT base retriever can return passages with high lexical overlap while missing some subtle but key semantic information. In the first example, the retrieved passage by BERT base contains keywords like "boy", "Winnie the Pooh", but does not answer the question. In the second example, there is no routing number in the BERT base retrieved passage, which is the key intent of the query. Our proposed pre-training can help to learn better semantics to answer such queries. For more examples, please check out Table14in the Appendix.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters for pre-training. The Wikipedia corpus comes from DPR<ref type="bibr" target="#b16">[17]</ref> instead of the original one used for BERT pre-training.</figDesc><table><row><cell></cell><cell cols="2">MS-MARCO Wikipedia</cell></row><row><cell># of passages</cell><cell>8.8M</cell><cell>21M</cell></row><row><cell>PLM</cell><cell>BERT base</cell><cell>BERT base</cell></row><row><cell>batch size</cell><cell>2048</cell><cell>2048</cell></row><row><cell>text length</cell><cell>144</cell><cell>144</cell></row><row><cell>learning rate</cell><cell>3 ? 10 -4</cell><cell>3 ? 10 -4</cell></row><row><cell>warmup steps</cell><cell>4000</cell><cell>4000</cell></row><row><cell>train steps</cell><cell>80k</cell><cell>200k</cell></row><row><cell>encoder replace rate</cell><cell>30%</cell><cell>30%</cell></row><row><cell>decoder replace rate</cell><cell>50%</cell><cell>50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Variants of generators for SimLM pre-training. Performances are reported on the dev set of MS-MARCO with BM25 negatives only.</figDesc><table><row><cell>generator</cell><cell cols="2">MRR@10 R@1k</cell></row><row><cell>frozen generator</cell><cell>38.0</cell><cell>98.3</cell></row><row><cell>joint train</cell><cell>38.0</cell><cell>98.4</cell></row><row><cell>joint train w/ random init</cell><cell>37.8</cell><cell>98.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Also called dual-encoder / two-tower encoder / Siamese networks in different contexts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://gluebenchmark.com/leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/google/electra-base-generator</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Details on Table <ref type="table">1</ref> The numbers for the GLUE benchmark are from the official leaderboard 2 . Note that the leaderboard submission from BERT does not use ensemble, so the comparison is not entirely fair. However, this does not change our conclusion that BERT generally performs worse than RoBERTa and ELECTRA on NLP tasks. For the MS-MARCO dataset, we fine-tune all the pre-trained models with BM25 hard Passage: What is process control? Process control is an algorithm that is used in the during the manufacturing process in the industries for the active changing process based on the output of process monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMLM</head><p>Rank: 1, Relevant: Passage: Process equipment is equipment used in chemical and materials processing, in facilities like refineries, chemical plants, and wastewater treatment plants. This equipment is usually designed with a specific process or family of processes in mind and can be customized for a particular facility in some cases.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query expansion techniques for information retrieval: a survey</title>
		<author>
			<persName><surname>Dr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Hiteshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1698" to="1735" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fernando Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<idno>ArXiv, abs/1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkg-mA4FDr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one?</title>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yih</forename></persName>
		</author>
		<idno>ArXiv, abs/2110.06918</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Diffcse: Differencebased contrastive learning for sentence embeddings</title>
		<author>
			<persName><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumen</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marin</forename><surname>Soljavci'c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.10298</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ELECTRA: pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>abs/2003.07820</idno>
		<ptr target="https://arxiv.org/abs/2003.07820" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Overview of the trec 2020 deep learning track</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fernando Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.07820</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context-aware sentence/passage term importance estimation for first stage retrieval</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10687</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
	<note>c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Condenser: a pre-training architecture for dense retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.75</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.75" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="981" to="993" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised corpus aware language model pre-training for dense passage retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">COIL: Revisit exact lexical match in information retrieval with contextualized inverted list</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.241</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.241" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3030" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505665</idno>
		<ptr target="https://doi.org/10.1145/2505515.2505665" />
	</analytic>
	<monogr>
		<title level="m">22nd ACM International Conference on Information and Knowledge Management, CIKM&apos;13</title>
		<editor>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arun</forename><surname>Iyengar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajeev</forename><surname>Rastogi</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-11-01">October 27 -November 1, 2013. 2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Colbert</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">July 25-30, 2020. 2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>SIGIR 2020, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://aclanthology.org/Q19-1026" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>b493230205f780e1bc26945df7481e5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations</title>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Retromae: Pre-training retrieval-oriented transformers via masked auto-encoder</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.12035</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder</title>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.220</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2780" to="2791" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prop: Pretraining with representative words prediction for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pre-train a discriminative text encoder for dense retrieval via contrastive span prediction</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.10641</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">From doc2query to doctttttquery</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>ArXiv, abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.466</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.466" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The curse of dense low-dimensional information retrieval for large index sizes</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.77</idno>
		<ptr target="https://aclanthology.org/2021.acl-short.77" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="605" to="611" />
		</imprint>
	</monogr>
	<note>Online, 2021. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PAIR: Leveraging passage-centric similarity relation for improving dense passage retrieval</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.191</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.191" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2173" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.224</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.224" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2825" to="2835" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Keshav Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><forename type="middle">A</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.01488</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple entity-centric questions challenge dense retrievers</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sciavolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="1/2021.emnlp-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6138" to="6148" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://aclanthology.org/2021.emnlp-main.496" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The fact extraction and VERification (FEVER) shared task</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5501</idno>
		<ptr target="https://aclanthology.org/W18-5501" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
	<note>id= rJ4km2R5t7</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Should you mask 15% in masked language modeling?</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.08005</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=zeFrfgyZln" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pretrained transformers for text ranking: BERT and beyond</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-tutorials.1</idno>
		<ptr target="https://aclanthology.org/2021.naacl-tutorials.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">1-4, Online, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03611</idno>
		<title level="m">Adversarial retriever-ranker for dense text retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
