<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tagommenders: Connecting Users to Items through Tags</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shilad</forename><surname>Sen</surname></persName>
							<email>ssen@macalester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Macalester College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
							<email>jvig@cs.umn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Grouplens Research</orgName>
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
							<email>riedl@cs.umn.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Grouplens Research</orgName>
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tagommenders: Connecting Users to Items through Tags</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CF5D7A63B0E7768EC535377C449710F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information Filtering; H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-Collaborative computing Algorithms</term>
					<term>Experimentation tagging</term>
					<term>recommender systems</term>
					<term>collaborative filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tagging has emerged as a powerful mechanism that enables users to find, organize, and understand online entities. Recommender systems similarly enable users to efficiently navigate vast collections of items. Algorithms combining tags with recommenders may deliver both the automation inherent in recommenders, and the flexibility and conceptual comprehensibility inherent in tagging systems. In this paper we explore tagommenders, recommender algorithms that predict users' preferences for items based on their inferred preferences for tags. We describe tag preference inference algorithms based on users' interactions with tags and movies, and evaluate these algorithms based on tag preference ratings collected from 995 MovieLens users. We design and evaluate algorithms that predict users' ratings for movies based on their inferred tag preferences. Our tag-based algorithms generate better recommendation rankings than state-of-the-art algorithms, and they may lead to flexible recommender systems that leverage the characteristics of items users find most important.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recommender systems enable users to navigate vast collections of items. Amazon suggests products users may like based on their ratings, clicked items, and purchased items <ref type="bibr" target="#b17">[17]</ref>. Users of Digg receive news articles based on other articles they find interesting <ref type="bibr" target="#b25">[25]</ref>. Members of Netflix receive movie recommendations based on their movie ratings <ref type="bibr" target="#b3">[3]</ref>. In each of these scenarios, recommender systems choose a few items a user will like most from among thousands, or even millions, of possibilities. This task, which we call recommend, is one of two tasks supported by nearly all recommender systems <ref type="bibr" target="#b26">[26]</ref>. For the second common task, predict, recommender systems predict which rating a user will assign to a particular item. For example, a user of "Rate Your Music"<ref type="foot" target="#foot_0">1</ref> might receive a predicted rating of 4.2 out of 5 stars for the album "White Blood Cells" by the White Stripes based on a five star rating for "In Rainbows" by Radiohead. For both the recommend and predict tasks, recommender systems help users understand an unknown relationship between themselves and an item by comparing a user's behavior (e.g. album clicks and ratings) to patterns of behavior in other users.</p><p>Tagging systems offer users an alternate way to address the recommend and predict tasks. Shirky suggests that since tags are created by users, they represent concepts meaningful to them <ref type="bibr" target="#b31">[31]</ref>. Because tags are easily comprehended by users, tags serve as a bridge enabling users to better understand an unknown relationship between an item and themselves. In previous work, we validated this relationship by finding that certain types of tags help users to find and make decisions about items <ref type="bibr" target="#b29">[29]</ref>. For example, Alice<ref type="foot" target="#foot_1">2</ref> is a real user in the MovieLens movie recommendation community we study. She enjoys animated movies, and has assigned five star ratings to "Shrek," "Pinnochio," and "Toy Story". If Alice visits the web page for the movie "Ratatouille" she would see that 5 users have applied the tag animated to it. Based on these tags, she might decide she would enjoy the movie (the predict task). Alice might then click on the tag pixar to discover the related movie "The Incredibles" (the recommend task).</p><p>Recommender algorithms that incorporate tagging information promise to combine the best elements of both types of systems. Lamere refers to these tag-based recommendations as "tagomendations" <ref type="bibr" target="#b16">[16]</ref>. We similary refer to tag-based recommender systems as tagommenders. Tagommenders offer the automation of traditional recommender systems, but retain the flexibility of tagging systems. Schafer et al. found that users enjoy specifying feedback about items along a variety of dimensions <ref type="bibr" target="#b27">[27]</ref>. Tagommenders enable recommenders to use the dimensions of items that users consider most important.</p><p>In this paper, we design tagommenders inspired by the way in which humans use tags to evaluate items. Figure <ref type="figure" target="#fig_0">1</ref> presents the model we explore in the movie recommendation domain in this paper. The bottom of the figure shows that traditional recommender systems infer users' preferences for movies based on their movie ratings. The top half of the figure describes the two main components of tagommender algorithms. First, we infer users' preferences for tags based on their interactions with tags and movies. Second, we infer users' preferences for movies based on their preferences for tags.</p><p>We define a user's preference for a tag as the user's level of interest in movies exhibiting the concept represented by the tag. For example, Alice indicated that she likes animated movies and swashbucklers, but dislikes movies about serial killers. <ref type="foot" target="#foot_2">3</ref> Her (dis)interest in these concepts may have influenced her 4.5 star rating for "The Mask of Zorro," and her 1.5 star rating for "Hannibal." In the first half of our paper we explore algorithms that infer users' preferences for tags: RQ1: Can we infer users' preferences for tags?</p><p>We consider tag preference inference algorithms that analyze signals of a user's interest in a tag or movie (Figure <ref type="figure" target="#fig_0">1</ref>, upper left). For instance, Alice's application of the tag shipwrecked may suggest that she is interested in swashbucklers (a signal of tag interest). In addition, Alice's rating of 4.5 stars for "The Mask of Zorro" and her click on a hyperlink leading the movie "The Pirates of Penzance" may also have been a result of her liking for swashbucklers (signals of movie interest).</p><p>One other signal of tag preference we consider is a tag's quality. We define a tag as high quality if it helps the community understand an important aspect of an item. For instance, Alice considers the tags serial killer and animated as high quality tags that capture important movie concepts but she considers sure thing as a low quality tag. 4 Since high quality tags capture important movie concepts, we evaluate an algorithm that infers users' preference for a tag based on the tag's quality.</p><p>We evaluate eleven tag preference inference algorithms using 118,017 tag preference ratings collected in a user survey on the MovieLens movie recommender website.</p><p>In the second half of this paper we analyze algorithms that predict users' ratings for movies based on their preferences for tags (upper right of Figure <ref type="figure" target="#fig_0">1</ref>). We separate our algorithms by the type of signals they use: implicit only, or both implicit and explicit. Implicit signals such as clicks and searches occur during users' natural interactions with tags and items. Tagommenders for sites that do scribe in Section 3 4 We learned Alice's views about tag quality through the thumbbased quality ratings we describe in Section 3 not support item ratings, such as the online bookmarking site Delicious <ref type="foot" target="#foot_3">5</ref> , must generate recommendations based on these implicit signals. Other systems with tags, such as Amazon, support explicit signals of interest in the form of item ratings. Our last two research questions explore the performance of tagommenders in both types of systems.</p><p>RQ2: How well do tagommenders perform in systems without ratings? RQ3: How well do tagommenders perform in systems with ratings?</p><p>We evaluate RQ2 and RQ3 using movie ratings and tags created by MovieLens users. Our work offers three contributions to researchers and practitioners:</p><p>• We develop and evaluate algorithms that infer users' preferences for tags.</p><p>• We develop tag-based recommendation algorithms that infer users' preferences for movies based on their inferred preferences for tags.</p><p>• We evaluate the end-to-end predictive performance of tagommender algorithms that combine tag preference inference algorithms with tag-based recommenders.</p><p>We believe this work to be important for two reasons. First, we hope that sites with an abundance of tagging activity, such as Delicious or flickr <ref type="foot" target="#foot_4">6</ref> , can use our algorithms to improve item recommendations. Second, we believe that tagommenders offer a flexible and comprehensible alternative to traditional recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Many first generation recommenders such as the GroupLens <ref type="bibr" target="#b24">[24]</ref> Usenet recommender employed user-based algorithms: given a particular user, recommend movies that similar users like. Sarwar et al.'s item-based algorithm transposed this model: predict users' ratings for an item based on their ratings for similar items. During the Netflix Prize, two trends emerged in recommender systems research <ref type="bibr" target="#b3">[3]</ref>. First, researchers adopted Simon Funk's<ref type="foot" target="#foot_5">7</ref> singular value decomposition algorithm (SVD) due to its accuracy, efficiency, and ease of implementation <ref type="bibr">[8]</ref>. Second, researchers such as Bell et al. combined the output of multiple recommender algorithms to improve performance <ref type="bibr" target="#b2">[2]</ref> . Our research differs from these collaborative filtering algorithms by using tags as intermediary entities.</p><p>Collaborative filtering (CF) algorithms such as the user-based, item-based and SVD algorithms rely on patterns between user ratings, but do not use data about items. They do not, for instance, know that "Toy Story" is an animated movie. Balabanovic et al. were among the first researchers who investigated content-based systems that make use of the data about an item such as a movie's genre. Other researchers have studied methods for combining collaborative filtering with content-based systems <ref type="bibr" target="#b22">[22]</ref> . Our research extends existing techniques for content-based recommendation in two ways. First, since tags are maintained by community members instead of expert editors their quality varies <ref type="bibr" target="#b29">[29]</ref>. We explore how estimates of tag quality improve recommender performance. Second, unlike earlier content-based algorithms, we automatically learn relationships between tags and movies based on inferred tag preferences and movie ratings.</p><p>Our algorithms that translate signals of item interest to signals of tag interest build on existing work in user profile extraction for content-based recommenders <ref type="bibr" target="#b1">[1]</ref> and web-based systems <ref type="bibr" target="#b21">[21]</ref>. However, our algorithms face challenges not present in other domains. While other systems' data are created by domain experts, tags are created by ordinary users. <ref type="bibr">Koutrika et al.</ref> suggest that this transfer of control allows tagging systems to be "threatened" by both "malicious" and "lousy" taggers <ref type="bibr" target="#b15">[15]</ref>. We differ from previous work on profile extraction by designing algorithms that are robust to differences in tag quality.</p><p>Although public bookmarking systems such as Fab <ref type="bibr" target="#b1">[1]</ref>, and Pharos <ref type="bibr" target="#b4">[4]</ref> have been available since the 1990's, Millen et al. point to tagging as a key reason current social bookmarking systems have enjoyed greater success <ref type="bibr" target="#b19">[19]</ref>. In early academic research on tagging communities, MacGregor and McCullogh <ref type="bibr" target="#b18">[18]</ref> explore the relative merits of controlled versus evolved vocabularies, arguing that evolved ontologies engage users but lack the precision of their controlled counterparts.</p><p>In earlier work, we show that the tags a user sees influence the tags they create themselves <ref type="bibr" target="#b29">[29]</ref>. We also classify tags as generally factual, subjective, or personal (intended for the tag creator themselves), and find that users generally prefer factual tags over subjective tags and strongly dislike personal tags. Our work furthers these studies of tagging communities by analyzing how tags can be incorporated into recommender systems.</p><p>In earlier work we explore user interfaces that help systems determine a tag's qulity <ref type="bibr" target="#b28">[28]</ref>. In offline results we find that thumb rating feedback significantly improves a system's ability to identify good tags compared to simple implicit signals of tag quality. We verify our results using an online study in the MovieLens movie recommender system <ref type="bibr" target="#b30">[30]</ref>. We extend this work by using tag quality (among other signals) to infer users' preferences for tags.</p><p>Several researchers have explored algorithms that recommend tags for an item <ref type="bibr">[29] [13]</ref>. Hayes et al. examine how tags can be used to cluster bloggers and posts and suggest that tags can be used as a gold standard for cluster coherency <ref type="bibr" target="#b10">[10]</ref>. Brooks et al. propose hybrid algorithms drawing on both blog tags and blog text to accurately cluster blogs <ref type="bibr" target="#b5">[5]</ref>. We build on this work by providing an end-to-end algorithm that infer preferences for tags and generates movie recommendations based on those preferences.</p><p>Three researchers have directly studied tag-based recommenders. In a blog post, Lamere suggests a metric for identifying similar sets of items in tagging sites by measuring the cosine similarity of the tags applied to items <ref type="bibr" target="#b16">[16]</ref>. Diederich et al. describe an exploratory study in which users create tag profiles corresponding to their interests and receive recommendations based on those tag profiles <ref type="bibr" target="#b7">[7]</ref>. Niwa et al. propose a cluster-based algorithm for recommending web-pages based on the pages users have tagged, and the tags applied to web pages <ref type="bibr" target="#b20">[20]</ref>. All three researchers base their recommendations on the similarity of TF-IDF tag profile vectors. We extend this existing research in three main ways. First, we investigate 11 different signals of a user's interest in tags, including tag searches, and item ratings. Second, we explore five different algorithms for calculating item preferences based on tag preferences. Third, we conduct an empirical evaluation using 118,017 star-ratings of tag preference and 1,720,390 star-ratings of item preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL DATASETS</head><p>We conduct our analyses using data collected from the Movie-Lens website. MovieLens primarily serves as a movie recommender system. Users receive movie recommendations in exchange for rating movies on a five star scale. MovieLens was created in 1997 and maintains an active base of approximately 1,200 users per week. We conduct our analyses using five sets of data from MovieLens  described in Table <ref type="table">1</ref>. We now describe the data contained in each dataset along with details not specified in the table.</p><p>Movie Ratings: MovieLens users rate movies on a one to five star scale.</p><p>Movie Clicks: We logged clicks on links to detailed information about a particular movie for approximately 17 months starting in December 2006.</p><p>Tag Applications: MovieLens members can tag movies, and use tags contributed by others in the community to find and evaluate movies. MovieLens users most commonly interact with tags through the search results screen (Figure <ref type="figure" target="#fig_1">2</ref>) and movie details screens (Figure <ref type="figure" target="#fig_2">3</ref>). The movie details screen displays movie information including up to 30 of a movie's tags. Since we introduced tagging features to MovieLens in January 2006, MovieLens members have created 84,155 tag applications resulting in 13,558 distinct tags (a tag is a particular word or phrase, a tag application is a three way relationship between a user, tag, and item). Further details of MovieLens and the MovieLens tagging system can be found in <ref type="bibr" target="#b29">[29]</ref>.</p><p>Tag Searches: Tag searches are textual searches for tags, or clicks on tag hyperlinks. 1,000 users have searched for at least five distinct tags. 107 users have searched for at least 50 distinct tags.</p><p>Tag Preference Ratings: In our model for tag-based recommendation, we first infer users' preferences for tags. In order to evaluate our tag preference inference algorithms, we conducted a survey of tag preferences for MovieLens users. We emailed invitations to 8,361 active users. 995 users responded (11.9% response rate).</p><p>In the survey, we showed each user a collection of tags, and asked them to "estimate how much" they "would like movies with each tag using a one to five star scale, or unsure." We asked each user to complete at least 60 tag ratings, but gave them the option to complete more if they wished. In total, users supplied 118,017 ratings for 9,889 distinct tags (mean 117 tags per user, median 78). 800 users completed the requested 60 ratings, while seven provided more than 1,000 ratings.</p><p>The breakdown of the survey responses by star ratings is as follows: 6% (7,641) were 5 stars, 17% (20,597) were 4 stars, 22% (26,499) were 3 stars, 13% (15,135) were 2 stars, 13% (15,155) were 1 star, and 28% (32,990) were unsure. The average tag rating was 2.89. The unsure rating was used differently by different users. Among the 25% of users who used the unsure rating most often, unsure ratings accounted for 43% of ratings. Among the 50% of users who used the unsure rating least often, unsure ratings accounted for only 18.3% of ratings.</p><p>Pruning: Although we draw on all data when analyzing tag preference inference algorithms in Section 4, we pruned the data sets Table <ref type="table">1</ref>: Size of different datasets we use in this paper. Count is the number of the entities the dataset contains. Num-users is the number of users that generated those entities. For example, the first two columns in the third row indicate that 84,155 tags have been applied by 3,582 users. The last two columns indicate the same numbers after the pruning we apply for our analyses in the second half of this paper. Fig. <ref type="figure">4</ref>: Inferring a user's preference for a tag based on her direct interactions with a tag such as her searches for a tag and her applications of a tag.</p><p>for our analyses of tag-based recommendations in section 5. In order to reduce the computational requirements of our analyses, we focused on a set of movies with a minimum threshold of tags, and a set of users with a rich profile of MovieLens behavior.</p><p>We began pruning the tag-recommendation dataset by selecting movies that had been tagged with at least five distinct tags. We wanted to focus on tags that represented concepts applicable to multiple movies, so we required that each tag be applied to at least five movies. We iteratively repeated this pruning until we reached a stable set of movies and tags.</p><p>After movie ratings, movie clicks are the most abundant source of behavioral information we have for MovieLens users. Since we wanted to explore the effectiveness of tag-based recommendations for domains without tag ratings, we only included users that had clicked five or more movies.</p><p>After pruning, 1,720,390 ratings remained from 5,637 users for 2,636 movies. Since applying a tag may indicate interest in a tag, we also track tag applications created by users in the pruned set. In total, 1,315 users in the pruned set applied 50,060 tags (mean of 38 tags per user, median of 2). More statistics are shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INFERRING TAG PREFERENCE</head><p>In this section we address RQ1: RQ1: Can systems infer users' preferences for tags?</p><p>We consider two approaches to inferring tag preference. First, algorithms can directly infer a user's preference for a tag based Fig. <ref type="figure">5</ref>: Inferring a user's preference for a tag indirectly based on her interactions with items having a tag such as her rating of items with the tag.</p><p>on her direct interactions with the tag (Figure <ref type="figure">4</ref>). For example, if Alice searches for animation, she is probably interested in it. Second, an algorithm may indirectly infer a user's preference for a tag based on her interactions with items having the tag (Figure <ref type="figure">5</ref>). For example, Alice has assigned five-star ratings to three movies tagged with animation: "Shrek", "Pinnocchio", and "Toy Story". Based on these movie ratings, we may infer that she would enjoy other movies tagged with animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inferring Preference using Tag Signals</head><p>We consider three algorithms based on direct signals of a user's interest in a tag (Figure <ref type="figure">4</ref>). Users may be more interested in tags they themselves apply. Tag-applied infers higher preference for those tags a user has applied. Users may also be interested in the tags for which they have searched. Tag-searched infers higher preference for tags for which a user has searched. Both tag-applied and tag-searched use a simple 0 or 1 numeric coding.</p><p>We also use a third implicit tag signal: a tag's quality (tagquality). As we mentioned in the introduction, a user's preference towards a tag may be correlated with the tag's quality. In order to examine this relationship, we include the best performing tag quality prediction algorithm from our previous research <ref type="bibr" target="#b30">[30]</ref>. <ref type="foot" target="#foot_6">8</ref> The algorithm draws on many signals of tag quality including the number of users who apply a tag, and the number of users who search for a tag. In order to make our results more generalizable to other sites, we do not draw on the tag quality thumb ratings unique to MovieLens.</p><p>All tag preference algorithms translate between a score (i.e. 0 or 1 for tag-applied) and a one-to-five star inferred tag preference according to a simple linear relationship. This relationship is estimated by performing a least-squares regression between the algorithm scores and users' actual preference for tags as reported in the survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inferring Preference using Item Signals</head><p>In this section, we explore algorithms that calculate a user's preference for a tag based on her interactions with movies related to the tag (Figure <ref type="figure">5</ref>). We found that our inference algorithms performed better when they took into account the relevance of a tag to a movie. For example, if we wish to infer a user's preference for the tag cars we might treat her interactions with each of 46 movies tagged with cars as equally important. However, cars may be more relevant for certain movies than others. For instance, cars accounts for 9 of the 38 tag applications for "Gone in 60 Seconds," but only 1 of the 36 applications for "Cast Away."</p><p>To account for differences in a tag's relevance, each inference algorithm in this section includes a weighting quantifying the relevance of a tag to a movie similarly to Vig et al. <ref type="bibr" target="#b32">[32]</ref>. As a measure of a tag's relevance to a movie, we use the tag quality measure we discussed in the previous section <ref type="bibr" target="#b30">[30]</ref>. We found that applying a sigmoid transformation improved the performance of weighting by tag quality. If w(m, t) represents the relevance weighting between a movie and tag:<ref type="foot" target="#foot_7">9</ref> w(m, t) = 1 e -tag-quality (m,t)  .</p><p>We normalize w(m, t) so that the weights for each movie's tags sum to 1.0. We explore six different algorithms for calculating a user's preference for a tag based on her interactions with items having the tag. The six algorithms can be grouped according to the type of signal of movie interest that they use. The first two algorithms (movie-clicks, movie-log-odds-clicks) use clicks on movie hyperlinks as a signal of a user's interest in a movie. The third and fourth algorithms (movie-r-clicks, movie-r-log-odds-clicks), analyze the specific movies a user chooses to rate. The last two algorithms (movie-ratings, movie-bayes) draw on a user's numeric ratings for movies.</p><p>Movie-clicks: The movie clicks algorithm is based on the hypothesis that users click on movies with tags they like more often. This algorithm estimates a user's preference for a tag based on the fraction of clicked movies that have the tag. Instead of weighting each movie equally, we weight movies according to the relevance weighting based on quality we described above: If clicked(u) is the set of movies clicked by user u, then:</p><formula xml:id="formula_0">movie-clicks(u, t) = X m∈clicked(u) w(m, t) |clicked(u)| .</formula><p>Movie-log-odds-clicks: Similar to movie-clicks, movie-log-oddsclicks assumes that users click movies with tags they like more often, but it adjusts for overall tag popularity. Movie-log-odds-click uses the log odds metric to compare the movie-specific tag frequency to the overall tag frequency. If M is the set of all movies, and Mt is the set of all movies with tag t,</p><formula xml:id="formula_1">logit(p) = log " p 1 -p « .</formula><p>movie-log-odds-clicks(u, t) = logit(movie-clicks(u, t))logit(</p><formula xml:id="formula_2">P m∈M t w(m, t) |M | ).</formula><p>Movie-r-clicks: Users' movie viewing decisions may correlate with their tag preferences. For instance, a user may choose to watch a movie because it contains "violence." We assume that users have watched the movies they have rated. Based on this, we consider a version of the movie-clicks algorithm that substitutes the movies a user has rated for the movies they have clicked.</p><p>Movie-r-log-odds-clicks: Similar to movie-r-clicks, movie-rlog-odds-clicks uses the movie-log-odds-click algorithm, but substitutes the movies a user has rated for the movies they have clicked.</p><p>Movie-ratings: Perhaps users rate movies with a particular tag consistently. For example, Alice consistently rated three animated movies five stars. Movie-ratings draws on this signal by predicting that a user's preference for a tag is the user's average rating for movies with the tag. As with the previous inference algorithms, we draw on tag quality for the tag relevance weighting w. If ru,m is user u's rating for movie m: movie-ratings(u, t) =</p><formula xml:id="formula_3">P m∈M t w(m, t) • ru,m P m∈M t w(m, t)</formula><p>.</p><p>The sums in both the numerator and denominator ignore movies the user has not rated. Movie-bayes: Movie-bayes is a bayesian generative model for how users rate movies with a particular tag <ref type="bibr" target="#b12">[12]</ref>. Figure <ref type="figure">6</ref> describes the model. For every user u and tag t we select a user-tag-specific distribution N (µt,u, σt,u) from hyper-distributions. For each rating ru,m by user u for movie m with tag t, the tag may be a relevant Fig. <ref type="figure">6</ref>: Movie-bayes is a generative model for how users rate movies with a particular tag. For every user u and tag t we select a user-tag-specific distribution N (µt,u, σt,u) from hyperdistributions. For each rating ru,m by u for movie m with tag t, the tag may be relevant, or irrelevant for the movie. If t is relevant, the rating is chosen from the user-tag-specific distribution. If t was not relevant, the rating is chosen from the user's background ratings distribution N (µu, σu). We estimate the hyperparameters for hyperdistributions N (μ, σ) and Γ( k, θ) using the empirical bayes methodology. We calculate the expected parameters for a particular user and tag, µt,u and σt,u, using MCMC.</p><p>tag for the movie, or an irrelevant for the movie. If t is a relevant tag, the rating is chosen from the user-tag-specific distribution. If t is not relevant, the rating is chosen from the user's background ratings distribution N (µu, σu).</p><p>We adopt the bayesian paradigm of considering all possible usertag-specific normal distributions <ref type="bibr" target="#b9">[9]</ref>. For each distribution, we calculate the probability that that distribution generated the user's ratings. We then take the expectation of the mean for a particular tag and user (µt,u) over all possible distributions by applying bayes rule based on the user's ratings. In the following formulas Ru,t is the set of ratings by user u for movies tagged with t, and E(X) denotes the expectation of random variable X. Γ( k, θ) and N (μ, σ) specify the gamma and normal hyperdistributions for the standard deviation and mean respectively. movie-bayes(u, t) = E(µt,u|Ru,t).</p><formula xml:id="formula_4">= Z µ Z σ µ • p " µ, σ|Ru,t, N (μ, σ), Γ( k, θ) " . = Z µ Z σ µ • p(Ru,t|µ, σ) • p " µ, σ|N (μ, σ), Γ( k, θ) " . = Z µ Z σ µ • p(Ru,t|µ, σ) • p " µ|N (μ, σ) " • p " σ|Γ( k, θ) " .<label>(1)</label></formula><p>In equation 1 the second term, p(Ru,t|µ, σ), is the probability of the user's ratings for movies with a tag based on a user-tag-specific ratings distribution. To calculate this probability, we treat the ratings as independent events. As described earlier, each rating may be the result of the user's background ratings distribution, or a rating may be the result of the user's tag specific distribution. The user's background distribution, N (µu, σu), is fit to all of the user's ratings. The user-tag-specific distribution is chosen with probability equal to the relevance weighting w(m, t):</p><formula xml:id="formula_5">p(Ru,t,|µ, σ) = Y r∈R u,t p(r|µ, σ) = Y r∈R u,t h p(r|N (µ, σ))w(m, t) + p(r|N (µu,σu))(1 -w(m, t)) i</formula><p>The third term in Equation <ref type="formula" target="#formula_4">1</ref>, p(µ|N (μ, σ)), is the prior probability of a mean for a particular user-tag-specific normal distribution. We assume the user-tag-specific mean is drawn from a normal distribution with hyper-parameters chosen using the empirical bayes methodology <ref type="bibr" target="#b9">[9]</ref>:</p><formula xml:id="formula_6">p(µ | N (μ, σ)) = p(µ|N (2.885, 1.0)).</formula><p>The fourth term in equation 1, p " σ|Γ( k, θ) " , is the prior probability a of a standard deviation for the user-tag-specific normal distribution. We assume the deviation is drawn from a gamma distribution with hyper-parameters chosen using the empirical bayes methodology:</p><formula xml:id="formula_7">p(σ|Γ( k, θ)) = p(σ|Γ(2.0, 1.0)).</formula><p>The choice of a gamma and normal distributions as hyper-distributions for a normal distribution is common in the Bayesian literature <ref type="bibr" target="#b9">[9]</ref>. We evaluate the complete integral using a Markov Chain Monte Carlo estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tag Preference Inference Results</head><p>As an evaluation metric, we calculate the Pearson correlation between each one to five star survey preference response and the inferred value from a particular algorithm (e.g. log-odds-clicks).</p><p>Figure <ref type="figure" target="#fig_3">7</ref> shows pearson correlations for all tag preference inference algorithms. 95% confidence intervals are displayed on the graph. All pairwise differences are significant. The two algorithms based on explicit item rating signals (movie-ratings, moviebayes) outperformed all implicit measures. Both the click-based and ratings-based movie-log-odds-click algorithms performed poorly. The two tag-based algorithms (tag-searched and tag-applied) did not perform as well as the movie-clicks algorithm. We suspect that this is due to the relatively small amount of tagging activity on MovieLens. For example, only 2.8% of the survey rating responses were associated with a tag the user had applied. Similarly, only 0.6% of the survey responses were associated with a tag the user had searched for. Tag quality performed best among the algorithms using tag signals. Based on its relatively strong correlation (0.17), we conclude that the quality of a tag affects a user's preference towards the tag.</p><p>We also evaluated linear combinations of tag preference algorithms. We combined algorithms using a least square regression between tag preference responses and algorithm outputs. <ref type="foot" target="#foot_8">10</ref> Allimplicit is the best combination of all tag preference inference algorithms that do not use movie ratings. All is the best combination of all tag preference algorithms. "All-implicit" outperformed each individual implicit feature. "All" outperformed all other algorithms.</p><p>In the following section we use the tag preference algorithms in tag-based recommendation algorithms. We either use "all", or "allimplicit" depending on whether the tag-based algorithm is designed for systems with or without ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TAG-BASED RECOMMENDERS</head><p>In the previous sections we evaluated methods for inferring users' preferences for tags based on signals of interest in tags and items (Figure <ref type="figure" target="#fig_0">1</ref>, upper left). We now shift our focus to using those inferred tag preferences to predict ratings for movies (Figure <ref type="figure" target="#fig_0">1</ref>, upper right). We present five tag-based recommendation algorithms -two based on implicit data, and three based on explicit data. We then describe our methodology including our evaluation metrics and baseline recommender algorithms. Finally, we present results for all algorithms as they relate to our research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implicit Tag-Based Algorithms</head><p>We first consider two tag-based recommendation algorithms that use implicit data in order to support sites without item ratings. As input, these algorithms use tag preferences inferred by all-implicit, the top performing implicit tag preference algorithm. As output, these algorithms produce a score suitable for ranking items in a recommendation list. Recommender systems that do not collect ratings generally do not predict ratings; therefore, the values output by the implicit tag recommendation algorithms are suitable for the recommend task but not the predict task.</p><p>In the previous section we saw that the average tag preference differed by tag. For example, users generally preferred a highquality tag to a low-quality one. During our analyses of tag-based recommenders, we found that these per-tag differences skewed results. We accounted for these per-tag differences by normalizing each tag's inferred preference to have mean 0 and standard deviation 1. In addition, we found that more active users generally had higher tag preferences than less active ones. To neutralize this effect, we subtracted the average tag preference for each user. We use these normalized tag preference values throughout this section.</p><p>We now describe the two implicit tag-based recommender algorithms:</p><p>Implicit-tag: The implicit-tag algorithm is inspired by algorithms from information retrieval that calculate the similarity between a user's profile vector and a document's term vector <ref type="bibr" target="#b23">[23]</ref>. In information retrieval, the columns in each vector correspond to words. In the implicit-tag algorithm, the columns correspond to tags. To generate a prediction for a movie m, implicit-tag calculates the dot product between users' preferences for movie m's tags and the weighting w(t, m) between tag t and movie m. We use probinformed as a weighting based on its strong performance in section 4.4. If ntp(u, t) is user u's normalized inferred tag preference for tag t, then user u's predicted score for movie m is:</p><formula xml:id="formula_8">implicit-tag(u, m) = X t∈Tm ntp(u, t) • w(m, t).</formula><p>Implicit-tag-pop: Implicit-tag ignores the overall popularity of a particular movie, an important signal of a users's liking for a movie. We next consider implicit-tag-pop, a version of the algorithm that adds pop(m), a term estimating a movie's popularity:</p><p>implicit-tag-pop(u, m) = implicit-tag(u, m) + pop(m). 11   We experimented with a variety of signals of popularity for a movie based on the number of clicks, tags, clickers, and taggers for a movie. For each possible signal, we fit a function between the signal value for each movie (e.g. num clicks) and the average rating for the movie. We evaluated each signal of popularity based on how well implicit-tag-pop performed using the signal. Although we omit the detailed results due to space, we found that tags outperformed clicks, counting users (clickers) outperformed counting events (clicks), and log transforming signals improved results. The best overall estimate was a linear estimate based on the log of the number of users who tagged a movie. If users(Am) is the set of users who applied a tag to movie m, then: pop(m) = 0.31 • log(|users(Am)|) + 3.16. 12   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Explicit Tag-Based Algorithms</head><p>The final three tag-based algorithms are intended for sites with item ratings; as a result, they rely on both implicit and explicit data. As input these algorithms use tag preferences inferred by all, the top performing tag preference algorithm using both implicit and explicit signals. Since these algorithms output a value between 1.0 and 5.0 corresponding to a star rating for a movie, they support both the predict and recommend tasks. We choose three algorithms that model increasingly complex relationships between tag preferences and movie ratings.</p><p>Cosine-tag: The success of the traditional item-based rating models that use cosine similarities inspired us to create a similar model based on tags. Cosine-tag predicts that a user's rating for a movie is a weighted average of the user's preferences for the movie's tags. Cosine-tag weights a particular tag according to the adjusted cosine similarity between ratings for a movie and inferred preferences for a tag. We refer to user u's mean movie rating as ru, and Um is the collection of users who rated movie m. The adjusted cosine similarity between movie m and tag t is:</p><formula xml:id="formula_9">sim(m, t) = X u∈Um (rm,u -ru) • ntp(t, u) s X u∈Um (rm,u -ru) 2 s X u∈Um ntp(u) 2 .</formula><p>Note that ntp(t, u) is already average adjusted, so there is no additional adjusting performed. Given this definition of similarity, cosine-tag constructs a prediction for a movie as the average of the user's preferences for its tags, weighted by the tags similarities to 11 We experimented with different weightings of the pop term, but found 1.0 to perform optimally. 12 Recall that we use the implicit tag-based algorithms for recommendation but not for prediction. Although we report the the intercept value <ref type="bibr">(3.16)</ref>, the choice of intercept does not affect the relative ordering. Therefore, the intercept is unnecessary -we could simply use 0.0. the movie. If Tm is the collection of all tags applied to movie m:</p><formula xml:id="formula_10">cosine-tag(u, m) = X t∈Tm sim(m, t) • ntp(t, u) X t∈Tm sim(m, t) + ru.</formula><p>One choice in this algorithms is the tags over which the average should be calculated. We found that the algorithm performed best when averaging over the 10 most similar tags.</p><p>Linear-tag: Since cosine-tag predicts a weighted average of a user's inferred tag preferences, the variability of movie predictions it outputs depend on the variability of its inputs (inferred tag preferences). Linear-tag models a more complex relationship between an inferred tag preference and a predicted movie rating. For each tag t applied to movie m, linear tag estimates a least-squares fit yt,m(u) between users' inferred tag preferences for t and their ratings for m:</p><p>y t,m (u) = αt,mntp(t, u) + βt,m + t,m. In the linear equation above, αt,m is the coefficient between tag t and movie m, βt,m is the intercept, and t,m is the residual error term.</p><p>Linear-tag generates user u's prediction for movie m by averaging the values predicted by each of the linear fits yt,m(u). We found that weighting by inverse residual improved performance because it gave greater importance to more accurate fits. If Am is the set of all tag applications for movie m, then:</p><formula xml:id="formula_11">linear-tag(u, m) = X t∈tags(Am) " yt,m<label>(u)/ t,m " X t∈tags(Am)</label></formula><p>" 1.0/ t,m " + ru.</p><p>As with cosine-tag, we experimented with averaging over different sets of tags. Averaging over the 5 tags with smallest residual performed best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regress-tag:</head><p>The linear-tag model treats each linear fit between a tag and a movie as independent. This may not be optimal. For example, both animated and animation have been applied to "Toy Story." It seems plausible that users' inferred preferences for these two tags would correlate. Algorithms aware of relationships between tags may perform better than those that do not.</p><p>Regress-tag constructs a linear equation for each movie m. The input variables are all users' inferred tag preferences for tags applied to m. The output is each user's rating for m. If m has tags t1, . . . , tn then: regress-tag(u, m) = h0 + h1ntp(u, t1) + . . . + hnntp(u, tn).</p><p>We experimented with three methods for choosing the coefficients hi: simple least-squares multiple regression, regularized multiple regression, and regression support vector machines. We found that the least-squares and regularized multiple regressions overfit movies with few ratings. For example, several movies had the same number of tags and ratings applied to them. In this case, multiple regression can build an equation that perfectly fits the input data. These fits often lead to large values hi that seemed intuitively incorrect and performed poorly. SVMs performed best due to their robustness to overfitting. We used the libsvm library based on its java implementation and efficient performance for linear kernels <ref type="bibr" target="#b6">[6]</ref>. We found that libsvm performed best when c, the tradeoff between the margin and error penalty, was set to 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Tagommenders Methodology</head><p>We compare the tag-based recommendation algorithms to three naive baselines:</p><p>Overall-avg: Overall-avg generates a prediction equal to the overall average rating (3.55): overall-avg(m, u) = 3.55.</p><p>In the recommend task, movies are ordered by predicted rating. Since overall-avg returns the same value for every movie, we randomly order recommendation lists.</p><p>User-avg: User-avg predicts a user's average for all of his or her movies. If Ru is the set of all movie ratings by user u: As with overall-avg, given a particular user, user-avg returns the same value for every movie. Thus, we randomly order recommendation lists.</p><p>User-movie-avg: User-movie-avg begins by average adjusting all of a user's ratings. The prediction for a movie is the average of all users' adjusted ratings for the movie. If Um is the collection of users who rated movie m, then: While these three naive baselines provide insight into algorithm performance, we ultimately compare our tag-based algorithms to top-performing traditional CF algorithms. We consider three traditional algorithms:</p><p>Explicit-item: We include the item-based algorithm introduced by Sarwar et al. based on its accuracy and popularity in real-world systems such as Amazon <ref type="bibr" target="#b17">[17]</ref>. The explicit item-based model calculates similarities between the ratings for each pair of movies. In order to predict for a particular movie m, the item model constructs a weighted average of the user's ratings for the movies most similar to m. The rating weights used for the weighted average are based on the similarities to m.</p><p>Implicit-item: We compare our implicit tag-based algorithms to Karypis et al.'s item-based algorithm for unary data (such as click and transaction data) <ref type="bibr" target="#b14">[14]</ref>. We selected this algorithm based on its accuracy and popularity. The item-based model calculates similarities between each pair of movies based on the number of times movies co-occur in user baskets. In order to predict for a particular movie m, the movie model sums the similarities between m and the movies in the user's basket that are most similar to m.</p><p>Funk-svd: We include Simon Funk's Singular Value Decomposition algorithm due to its strong performance in the Netflix competition <ref type="bibr">[8]</ref>. The Funk SVD approximates the full users × movies rating matrix using a matrix of lower dimension, and uses regularization to manage the sparsity of the ratings matrix.</p><p>We used five-fold cross validation in our analyses. For each each of the five test / train splits, we hide 30% of user ratings in the test set, and evaluate the performance of an algorithm by comparing the ordering of a recommendation list to the hidden ratings. Herlocker et al. find two important classes of evaluation metrics: those that evaluate an algorithm's performance on the predict task, and those that focus on the recommend task <ref type="bibr" target="#b11">[11]</ref>. We choose one evaluation metric from each class:</p><p>Top-5: As an evaluation metric for the recommendation task, we use top-5, the fraction of the top five recommended movies for a user that are rated four stars or higher by the user. <ref type="foot" target="#foot_9">13</ref> We only consider elements the user has rated when selecting the top 5 movies. The 95% confidence intervals for top-5 was ±0.57% (n = 28, 185).</p><p>MAE: In addition to top-5 we report mean absolute error (MAE), the average absolute difference between the value predicted by a recommender system and the user's actual rating value. MAE reflects an algorithm's performance on the predict task. We examined the distribution of MAE values produced in our analyses and found the 95% confidence intervals for MAE was ±0.001 (µ = 0.577, σ = 0.491, n = 516, 441). As we discussed in Section 5.1, the implicit algorithms support only the recommend task. Therefore, we only report MAE for explicit algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Tagommenders Results and Discussion</head><p>Figure <ref type="figure" target="#fig_6">8</ref> shows the top-5 precision for the five tag-based algorithms, the three naive baselines, and the three collaborative filtering (CF) baselines. Higher top-5 values correspond to better performance. The traditional CF algorithms are displayed in solid bars and the tag-based algorithms are displayed in striped bars. We also include hybrid, a simple linear combination of the best performing tag-based algorithm (regress-tag) and traditional algorithm (funksvd).<ref type="foot" target="#foot_10">14</ref> 95% confidence intervals are displayed for each algorithm.</p><p>Implicit-tag, user-tag, and overall-tag all achieve a top-5 of 53%, the same as randomly ordering a recommendation list. Differences between other pairs are significant (p ≤ 0.05) except for those between user-movie-avg and implicit-tag-pop, and between regresstag and hybrid. The tag-based algorithms generally perform well. Implicit-tag-pop, the best implicit algorithm, achieves a top-5 of 77%. Regress-tag, the best performing explicit algorithm achieves a top-5 of 83%.</p><p>Figure <ref type="figure" target="#fig_7">9</ref> shows the MAE for all explicit algorithms. The traditional CF algorithms are displayed in solid bars. Lower MAE values correspond to better performance. The tag-based algorithms are displayed in striped bars. All pairwise differences are significant (p ≤ 0.05) except hybrid and funk-svd. As discussed in Section 5, the implicit algorithms only support the recommend task. Therefore, we only report MAE for explicit algorithms. In general, the tag-based algorithms outperformed the naive baselines, and the traditional CF algorithms outperformed the tag-based algorithms. Regress-tag performed best among the tag algorithms, achieving an mae of 0.584. As with top-5, cosine-tag performed poorly, achieving an mae of 0.639. Among the CF algorithms, funk-svd performs best, achieving an mae of 0.555.</p><p>Given these results, we return to our second research question:</p><p>RQ2: How well do tagommenders perform in systems without ratings?</p><p>As shown in Figure <ref type="figure" target="#fig_6">8</ref>, implicit-tag-pop (77%) performs significantly better than the popular implicit-item algorithm (69%) according to top-5. We wondered if the strong performance of implicittag-pop compared to implicit-item was due to its inclusion of popularity. To test this possibility, we experimented with different methods for building popularity into the implicit-item algorithm. None of them significantly improved its performance. We conclude that the tagommender algorithm performs better than traditional CF algorithms in our evaluation without ratings.</p><p>Finally, we address our last research question:</p><p>RQ3: How well do tagommenders perform in systems with ratings?</p><p>Among the explicit tag algorithms, regress-tag performs best in both top-5 (83%) and MAE (0.584). Among the traditional CF algorithms, funk-svd performs best in both top-5 (80%) and MAE (0.555). Both these differences are significant (p &lt; 0.05). We conclude that tagommenders appear to perform better than traditional CF algorithms for the recommend task, but worse for the predict task. However, the recommend task seems to be more prevalent in real world systems. Among all popular recommender systems we investigated, only three (Netflix, MovieLens, Rate Your Music) offer predicted ratings. Most recommender systems now follow Amazon's model. Amazon does support the recommend task. However, instead of supporting the predict task through CF, they offer users rich product data, user reviews, and average user ratings. Thus, tagommenders perform better than traditional CF algorithms in the task most important to real world recommender systems.</p><p>We conclude by noting that hybrid, the linear combination of funk-svd and regress-tag, offers the best of both tag-based and CF algorithms. Hybrid achieves a top-5 of 83%, equal to that of the top performing tag-based algorithm, and significantly better than traditional CF algorithms. In addition, hybrid's MAE equals that of funk-svd, the best performing traditional CF algorithm. Thus, a simple hybrid algorithm performs better than any CF algorithm on the recommend task and it matches the best CF algorithm on the predict task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we introduced and evaluated tagommenders, recommender algorithms that make use of tags. We evaluated a wide variety of tag preference inference algorithms and found that algorithms combining a variety of signals performed best. We constructed implicit and explicit tag-based recommendation algorithms based on users' inferred tag preferences. These tagommenders outperformed existing CF algorithms in the recommend task most critical to real world recommender systems. Finally, we showed that a hybrid tag and CF algorithm combines the strong predict performance of CF algorithms with the strong recommend performance of tag based algorithms.</p><p>We believe that tagommenders may lead to novel interfaces for recommender systems. Since tagommenders use tags as an intermediary entity, their recommendations can be explained based on users' preferences for tags. MovieLens users often ask for the opportunity to rate movies on a more diverse set of dimensions. Tagommenders might prove a way to meet that desire.</p><p>Relationships between ratings and tags may also be used to infer the tags that should be applied to a movie. Although previous researchers have investigated the tag inference problem <ref type="bibr" target="#b13">[13]</ref>, they have not made use of patterns in users' ratings of items. For each movie, the cosine tag recommender calculates the similarity between between users inferred preferences for each tag and their ratings for the movie. For example the most similar tags for the movie "Last of the Mohicans" starring Daniel Day-Lewis are the tags tribal, sword fight, cavalry charge, historical, and stirring. The only one of these tags that users actually applied to the movie is tribal. Figure <ref type="figure">6</ref> lists 10 &lt;tag, movie&gt; pairs with highest similarity scores.</p><p>One important question related to our findings is how tagommenders will perform in domains other than MovieLens. While we cannot be certain, the high tag density of a system such as Delicious might lead to more accurate recommendations.</p><p>Finally, we believe there are a number of fundamental issues surrounding the relationship between preference and quality. It may be challenging to design interfaces that collect ratings along both the quality and preference dimension without confusing users. Perhaps because of this, most systems such as YouTube and Netflix only collect ratings along the preference dimension. Future research might explore interfaces for differentiating between quality and preference and examine the role quality and preference play in different domains. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Our model of movie tagommenders. Traditional recommender systems (bottom) generate predictions for movies based on movie ratings or clicks. Tagommenders (top) first infer users' preferences for tags (upper left). Based on these inferred preferences for tags, tagommenders generate movie recommendations (upper right). Users' preference for tags can be inferred based on signals of interest in tags (tag applications, searches), or signals of interest in items (movie ratings, clicks). In order to use item signals to infer users' preferences for tags, they must be translated to tag signals (upper left).</figDesc><graphic coords="2,53.80,53.80,259.20,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Tags as they appear on the MovieLens search results screen.</figDesc><graphic coords="3,330.34,53.80,212.04,50.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Tags as they appear on the MovieLens movie details screen.</figDesc><graphic coords="3,350.56,129.30,171.60,55.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Pearson correlation between inferred tag preference, and actual tag preference. All pairwise differences are significant at the 0.05 level according to a two-tailed t-test. Algorithms based on explicit item signals performed best, followed by those based on implicit item signals, followed by those based on tag signals. All, a linear combination of all algorithms, performed best.</figDesc><graphic coords="6,53.80,53.80,244.99,207.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>user-avg(m, u) = P r∈Ru rm,u |Ru| .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>user-movie-avg(m, u) = X u ∈Um " r m,uuser-avg(u ) " |Um| +user-avg(u).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Top-5 precision for recommender algorithms. 95% confidence intervals are displayed for each algorithm. Higher top-5 values correspond to better performance. CF algorithms are displayed in solid bars and tag-based algorithms are displayed in striped bars. The best of the tag-based algorithms perform better than the best CF algorithms.</figDesc><graphic coords="8,319.02,53.80,234.68,209.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: MAE for explicit algorithms. We do not include implicit algorithms since they do not support the predict task. 95% confidence intervals are displayed for each algorithm. Lower MAE values correspond to better performance. CF algorithms are displayed in solid bars and tag-based algorithms are displayed in striped bars.In general, tag-based algorithms perform better than naive baselines but worse than their CF counterparts.</figDesc><graphic coords="9,322.78,53.80,227.17,175.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top 10 inferred tags not applied to movies based on the similarity between tag preferences and movie ratings. We do not include movies in a series (e.g. trilogies), and only include the top entry for tags or movies that appear more than once.</figDesc><table><row><cell>movie</cell><cell>tag</cell><cell>cosine sim</cell></row><row><cell>Pearl Harbor (2001)</cell><cell>disaster</cell><cell>0.47</cell></row><row><cell>Runaway Bride (1999)</cell><cell>girlie movie</cell><cell>0.45</cell></row><row><cell>Beauty and the Beast (1991)</cell><cell>talking animals</cell><cell>0.42</cell></row><row><cell>Armageddon (1998)</cell><cell>will smith</cell><cell>0.41</cell></row><row><cell>Cinderella (1950)</cell><cell>cartoon</cell><cell>0.40</cell></row><row><cell>Inconvenient Truth (2006)</cell><cell>documentary</cell><cell>0.40</cell></row><row><cell>The Little Mermaid (1989)</cell><cell>musical</cell><cell>0.40</cell></row><row><cell>Gone in 60 Seconds (2000)</cell><cell>exciting</cell><cell>0.39</cell></row><row><cell>My Best Friend's Wedding (1997)</cell><cell>chick flick</cell><cell>0.39</cell></row><row><cell>Billy Madison (1995)</cell><cell>very funny</cell><cell>0.39</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.rateyourmusic.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Alice is a pseudonym to protect the user's privacy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We learned Alice's preferences for tags through a survey we de-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://del.icio.us</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://www.flickr.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Simon Funk is a pseudonym for Brandyn Webb. Since researchers have consistently referred to him by his pseudoname, so do we.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We use the all-implicit tag quality inference algorithm from<ref type="bibr" target="#b30">[30]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>We explored five other weightings (such as TF-IDF), and found that the tag quality-based weighting performed within 3% of the best novel weighting algorithm (a graphical bayesian network). The tag quality-based weighting also outperformed TF-IDF. For simplicity, we choose to build on our existing research instead of introducing a new weighting scheme.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>We tested support vector machines as well, but they yielded no significant improvement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>We choose 4 stars as the cutoff for a "good" rating since it is the first star rating higher than the overall average rating of 3.55 stars. We experimented with other values for n in top-n: 1, 3, 5, 10, 20. We found the results to generally be consistent regardless of choice of n.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"><p>We experimented with all mixtures between 0 and 100 in increments of ten, and found that a 50-50 average performed best</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The authors thank Arindam Banerjee, F. Maxwell Harper, Andrew Sheppard, Melissa Skeans, Katy Sen, and the entirety of Grou-pLens for their feedback on the ideas presented in this paper. We also thank the MovieLens community for their ratings, feedback and suggestions. This paper is funded in part by National Science Foundation grants IS 03-24851 and IIS 05-34420.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content-based, collaborative recommendation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balabanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66" to="72" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lessons from the Netflix prize challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Netflix Prize</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Cup and Workshop</title>
		<meeting>KDD Cup and Workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pharos, a collaborative infrastructure for web knowledge sharing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bouthors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dedieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECDL</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved Annotation of the Blogosphere Via Autotagging and Hierarchical Clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Montanez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on World Wide Web</title>
		<meeting>the 15th International Conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding Communities of Practice from User Profiles Based On Folksonomies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iofciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Worskhop on Building Technology Learning Solutions for Communities of Practice</title>
		<meeting>the 1st International Worskhop on Building Technology Learning Solutions for Communities of Practice</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Netflix Update: Try This At Home</title>
		<author>
			<persName><forename type="first">S</forename><surname>Funk</surname></persName>
		</author>
		<ptr target=".sifter.org/simon/journal/20061211.html" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of the use of tags in a blog recommender system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<ptr target="http://sra.itc.it/people/hayes/pubs/06/hayes-ijcai07-tech-report.pdf" />
	</analytic>
	<monogr>
		<title level="m">ITC-IRST Technical Report</title>
		<imprint>
			<date type="published" when="2006-06">June, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="53" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Empirical Bayes for Learning to Learn</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tag Recommendations in Folksonomies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes In Computer Science</title>
		<imprint>
			<biblScope unit="volume">4702</biblScope>
			<biblScope unit="page">506</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of Item-Based Top-N Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combating spam in tagging systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koutrika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Effendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gyöngyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international workshop on Adversarial information retrieval on the web</title>
		<meeting>the 3rd international workshop on Adversarial information retrieval on the web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tagomendations -making recommendations transparent</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<ptr target="http://blogs.sun.com/plamere/entry/tagomendations_making_recommedations_transparent" />
		<imprint>
			<date type="published" when="2007">2007. 2008</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Amazon.com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative tagging as a knowledge organisation and resource discovery tool</title>
		<author>
			<persName><forename type="first">G</forename><surname>Macgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Library View</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social bookmarking in the enterprise</title>
		<author>
			<persName><forename type="first">D</forename><surname>Millen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Queue</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hon'iden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Page Recommender System Based on Folksonomy Mining. Transactions of Information Processing Society of Japan</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1382" to="1392" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning and Revising User Profiles: The Identification of Interesting Web Sites</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="331" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A framework for collaborative, content-based and demographic filtering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="393" to="408" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A critical analysis of vector space model for information retrieval</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="279" to="287" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GroupLens: An open architecture for collaborative filtering of netnews</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW &apos;94: Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work</title>
		<meeting><address><addrLine>Chapel Hill, North Carolina, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Digg: Recommendation engine rolling out this week</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
		<ptr target="http://http://blog.digg.com/?p=127" />
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborative Filtering Recommender Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes In Computer Science</title>
		<imprint>
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">291</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-recommendation systems: User-controlled integration of diverse recommendations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Information and Knowledge Management</title>
		<meeting>the ACM Conference on Information and Knowledge Management<address><addrLine>MacLean, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The quest for quality tags</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GROUP &apos;07: Proceedings of the 2007 international ACM conference on Supporting group work</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">tagging, communities, vocabulary, evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Osterhouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM 2006 Conference on CSCW</title>
		<meeting>the ACM 2006 Conference on CSCW<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Recognize Quality Tags</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IUI</title>
		<meeting>IUI</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Shirky</surname></persName>
		</author>
		<ptr target="http://www.shirky.com/writings/ontology_overrated.html" />
		<title level="m">Ontology is overrated</title>
		<imprint>
			<date type="published" when="2005-05-26">2005. May 26, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tagsplanations: Explaining Recommendations Using Tags</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent User Interfaces</title>
		<meeting>the International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
