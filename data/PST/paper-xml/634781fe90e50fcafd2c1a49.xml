<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning</title>
				<funder ref="#_y47snHf">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Foundation of Key Laboratory of Machine Intelligence and Advanced Computing of the Ministry of Education</orgName>
				</funder>
				<funder ref="#_4CPUZpu">
					<orgName type="full">Program for Guangdong Introducing Innovative and Entrepreneurial Teams</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-12">12 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>2 Meta AI 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinghao</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>2 Meta AI 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>2 Meta AI 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>2 Meta AI 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shaoliang</forename><surname>Nie</surname></persName>
							<email>snie@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>2 Meta AI 1</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-12">12 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.05883v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available. While dropout proves to be an effective antidote by randomly dropping a proportion of units, existing research has not examined its effect on the self-attention mechanism. In this paper, we investigate this problem through self-attention attribution and find that dropping attention positions with low attribution scores can accelerate training and increase the risk of overfitting. Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on lowattribution positions to reduce overfitting. We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to avoid dropping high-attribution positions excessively. Extensive experiments on various benchmarks show that AD-DROP yields consistent improvements over baselines. Analysis further confirms that AD-DROP serves as a strategic regularizer to prevent overfitting during fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-training large language models (PrLMs) on massive unlabeled corpora and fine-tuning them on downstream tasks has become a new paradigm <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Their success can be partly attributed to the self-attention mechanism <ref type="bibr" target="#b3">[4]</ref>, yet these self-attention networks are often redundant <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and tend to cause overfitting when fine-tuned on downstream tasks due to the mismatch between their overparameterization and the limited annotated data <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. To address this issue, various regularization techniques such as data augmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, adversarial training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>), and dropout-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> have been developed. Among them, dropout-based methods are widely adopted for their simplicity and effectiveness. Dropout <ref type="bibr" target="#b18">[19]</ref>, which randomly discards a proportion of units, is at the core of dropout-based methods. Recently, several variants of dropout have been proposed, such as Concrete Dropout <ref type="bibr" target="#b19">[20]</ref>, DropBlock <ref type="bibr" target="#b20">[21]</ref>, and AutoDropout <ref type="bibr" target="#b21">[22]</ref>. However, these variants generally follow the vanilla dropout to randomly drop units during training and pay little attention to the effect of dropout on self-attention. In this paper, we seek to fill this gap from the perspective of self-attention attribution <ref type="bibr" target="#b22">[23]</ref> and aim to reduce overfitting when fine-tuning PrLMs.</p><p>Attribution <ref type="bibr" target="#b23">[24]</ref> is an interpretability method that attributes model predictions to input features via saliency measures such as gradient <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. It is also used to explain the influence patterns of self-attention in recent literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Our prior experiment of self-attention attribution (Section 2.2) reveals that attention positions are not equally important in preventing overfitting, and dropping low-attribution positions is more likely to cause overfitting than discarding high-attribution positions. This observation suggests that attention positions should not be treated the same in dropout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivated by the above, we propose</head><p>Attribution-Driven Dropout (AD-DROP) to better fine-tune PrLMs based on selfattention attribution. The general idea of AD-DROP is to drop a set of self-attention positions with high attribution scores. We illustrate the difference between vanilla dropout and AD-DROP by their attention maps in Figure <ref type="figure" target="#fig_0">1</ref>. When fine-tuning a PrLM on a batch of training samples, AD-DROP involves four steps. First, predictions are made through a forward computation without dropping any attention position. Second, we compute the attribution score of each attention position by gradient <ref type="bibr" target="#b24">[25]</ref> or integrated gradient <ref type="bibr" target="#b25">[26]</ref> attribution methods. Third, we sample a set of positions with high attribution scores and generate a mask matrix for each attention map. Finally, the mask matrices are applied to the next forward computation to make predictions for backpropagation. AD-DROP can be regarded as a strategic dropout regularizer that forces the model to make predictions by relying more on low-attribution positions to reduce overfitting. Nevertheless, excessive neglect of high-attribution positions would leave insufficient information for training. Hence, we further propose a cross-tuning strategy that performs fine-tuning and AD-DROP alternately to improve the training stability.</p><p>To verify the effectiveness of AD-DROP, we conduct extensive experiments with different PrLMs (i.e., BERT <ref type="bibr" target="#b0">[1]</ref>, RoBERTa <ref type="bibr" target="#b1">[2]</ref>, ELECTRA <ref type="bibr" target="#b28">[29]</ref>, and OPUS-MT <ref type="bibr" target="#b29">[30]</ref>) on various datasets (i.e., GLUE <ref type="bibr" target="#b30">[31]</ref>, CoNLL-2003 <ref type="bibr" target="#b31">[32]</ref>, WMT 2016 EN-RO and TR-EN <ref type="bibr" target="#b32">[33]</ref>, HANS <ref type="bibr" target="#b33">[34]</ref>, and PAWS-X <ref type="bibr" target="#b34">[35]</ref>). Experimental results show that the models tuned with AD-DROP obtain remarkable improvements over that tuned with the original fine-tuning approach. For example, on the GLUE benchmark, BERT achieves an average improvement of 1.98/0.87 points on the dev/test sets while RoBERTa achieves an average improvement of 1.29/0.62 points. Moreover, ablation studies and analysis demonstrate that gradient-based attribution <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> is a more suitable saliency measure for implementing AD-DROP than directly using attention weights or simple random sampling. Moreover, they also demonstrate that the cross-tuning strategy plays a crucial role in improving training stability.</p><p>To sum up, this work reveals that self-attention positions are not equally important for dropout when fine-tuning PrLMs. Arguably, low-attribution positions are more difficult to optimize than high-attribution positions, and dropping these positions tends not to relieve but accelerate overfitting. This leads to a novel dropout regularizer, AD-DROP, driven by self-attention attribution. Although proposed for self-attention units, AD-DROP can be potentially extended to other units as dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Since Transformers <ref type="bibr" target="#b3">[4]</ref> are the backbone of PrLMs, we first review the details of self-attention in Transformers and self-attention attribution <ref type="bibr" target="#b22">[23]</ref>. Let X ? R n?d be the input of a Transformer block, where n is the sequence length and d is the embedding size. Self-attention in this block first maps X into three matrices Q h , K h and V h via linear projections as query, key, and value respectively for the h-th head. Then, the attention output of this head is calculated as:</p><formula xml:id="formula_0">Attention (Q h , K h , V h ) = A h V h = softmax Q h K h T ? d k + M h V h ,<label>(1)</label></formula><p>where ? d k is a scaling factor. M h is the mask matrix to apply dropout in self-attention, and elements in M h will be -? if the corresponding positions in attention maps are masked and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Based on the attention maps</head><formula xml:id="formula_1">A = [A 1 , A 2 , ? ? ? , A H ]</formula><p>for H attention heads, gradient attribution <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref> directly produces an attribution matrix B h by computing the following partial derivative:</p><formula xml:id="formula_2">B h = ?F c (A) ?A h ,<label>(2)</label></formula><p>where F c (?) denotes the logit output of the Transformer for class c.</p><p>To provide a theoretically more sound attribution method, Sundararajan et al. <ref type="bibr" target="#b25">[26]</ref> propose integrated gradient, which is employed by Hao et al. <ref type="bibr" target="#b22">[23]</ref> as a saliency measure for self-attention attribution. Specifically, Hao et al. <ref type="bibr" target="#b22">[23]</ref> compute the attribution matrix B h as:</p><formula xml:id="formula_3">B h = A h m m k=1 ?F c k m A ?A h , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where m is the number of steps for approximating the integration in integrated gradient, and is the element-wise multiplication operator. Despite its theoretical advantage over gradient attribution, integrated gradient requires m times more computational effort, which is especially expensive when it is applied to all the attention heads in Transformers. Moreover, our experiments in Section 3.4 show that gradient attribution achieves comparable performance with integrated gradient but requires much less computational cost, suggesting that gradient attribution is more desirable for AD-DROP. To better motivate our work, we first conduct a prior experiment on MRPC <ref type="bibr" target="#b36">[37]</ref> to investigate how different positions in self-attention maps affect fine-tuning performance based on attribution results. RoBERTa_base <ref type="bibr" target="#b1">[2]</ref> is used as the base model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Prior Attribution Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attribution-Driven Dropout</head><p>Inspired by the observations in Section 2.2, we propose a novel regularizer, AD-DROP, to better prevent overfitting when adapting PrLMs to downstream tasks. The motivation of AD-DROP is to minimize the over-reliance of these models on particular features which may affect their generalization.</p><p>Formally, given a training set D = {(x i , y i )} N i=1 of N samples, where x i is the i-th sample and y i is its label, the goal of AD-DROP is to fine-tune a PrLM F (?) of L layers on D. Same as the vanilla dropout <ref type="bibr" target="#b18">[19]</ref>, AD-DROP is only applied in the training phase. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the idea of AD-DROP can be described in four steps. First, we conduct a forward computation of the model to obtain the label with the highest probability as the pseudo label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Maps</head><p>The reason we adopt pseudo labels rather than gold labels for attribution will be explained shortly. Specifically, for the input x i with n tokens, we apply F (?) to encode it and obtain its pseudo label c:</p><formula xml:id="formula_5">c = arg max c (P F (c|x i )) ,<label>(4)</label></formula><p>where P F (c|x i ) is the probability of class c for x i . After the forward computation, we also obtain a set of attention maps</p><formula xml:id="formula_6">A = [A 1 , A 2 , ? ? ? , A H ]</formula><p>for each layer according to Eq. (1).</p><p>Second, we compute the attribution matrices</p><formula xml:id="formula_7">B = [B 1 , B 2 , ? ? ? , B H ]</formula><p>for H heads according to Eq. ( <ref type="formula" target="#formula_2">2</ref>). Specifically, the attribution matrix B h for the h-th head is computed as:</p><formula xml:id="formula_8">B h = ?F c (A) ?A h ,<label>(5)</label></formula><p>where F c (A) is the logit output of pseudo label c before softmax. <ref type="foot" target="#foot_1">3</ref>Third, we generate a mask matrix M h based on B h . To this end, we first sort each row of B h in ascending order and obtain a sorted attribution matrix B h . Then, we define a candidate discard region S h , in which each element s i,j is defined as:</p><formula xml:id="formula_9">s i,j = 1, b i,j &lt; b i,int(n(1-p)) 0, otherwise<label>(6)</label></formula><p>where b i,j and b i,j are elements of B h and B h , respectively, int(?) is an integer function, and p ? (0, 1) is used to control the size of the candidate discard region. Next, we apply dropout in the region to produce the mask matrix M h as:</p><formula xml:id="formula_10">m i,j = -?, (s i,j +u i,j ) = 0 0, otherwise<label>(7)</label></formula><p>where u i,j ? Bernoulli(1 -q) is an element of matrix U h ? R n?n , and q is the dropout rate.</p><p>Finally, M h is fed into self-attention of Eq. ( <ref type="formula" target="#formula_0">1</ref>) for the second forward computation, and the final output is used to calculate the loss for backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The reasons that AD-DROP uses pseudo labels for attribution are twofold. First, adopting gold labels will divulge label information and lead to inconsistency between training and inference. Second, for misclassified samples in the first forward computation, AD-DROP with gold labels tends to continue to make incorrect predictions because high-attribution attention positions derived from gold labels may be located in low-attribution regions derived from pseudo labels. Therefore, dropping these positions does not help the model correct wrong predictions, while AD-DROP with pseudo labels urges the model to rely on important features in the current pass and may correct the wrong predictions. The attribution with gold labels will be investigated in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cross-Tuning Algorithm</head><p>We further design a cross-tuning algorithm to avoid dropping high-attribution positions excessively when applying AD-DROP. The idea of cross-tuning is to execute the original fine-tuning and AD-DROP alternatively. Specifically, it performs the original fine-tuning at odd epochs and AD-DROP at even epochs. The overall process of cross-tuning is described in Algorithm 1, where Lines 3-5 are the original fine-tuning operations and Lines 7-9 describe the process of AD-DROP.</p><p>Algorithm 1 Cross-tuning</p><formula xml:id="formula_11">Input: shuffled training samples D = {(x i , y i )} N i=1 , PrLM F with parameters W Output: updated parameters W 1: Initialize F with W, epoch = 1 2: while not converged do 3:</formula><p>Calculate the prediction P F (y i |x i ) and loss via forward computation. Backpropagate the loss to update model parameters W. Perform AD-DROP by Eq. ( <ref type="formula" target="#formula_5">4</ref>)-( <ref type="formula" target="#formula_10">7</ref>) to obtain mask matrices</p><formula xml:id="formula_12">M = [M 1 , M 2 , ? ? ? , M H ]. 8:</formula><p>Calculate the new prediction P F (y i |x i ) and new loss by feeding M into Eq. ( <ref type="formula" target="#formula_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Backpropagate the new loss to update model parameters W. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct our main experiments on eight tasks of the GLUE benchmark <ref type="bibr" target="#b30">[31]</ref>, including SST-2 <ref type="bibr" target="#b37">[38]</ref>, MNLI <ref type="bibr" target="#b38">[39]</ref>, QNLI <ref type="bibr" target="#b39">[40]</ref>, QQP <ref type="bibr" target="#b40">[41]</ref>, CoLA <ref type="bibr" target="#b41">[42]</ref>, STS-B <ref type="bibr" target="#b42">[43]</ref>, MRPC <ref type="bibr" target="#b36">[37]</ref>, and RTE <ref type="bibr" target="#b43">[44]</ref>. The evaluation metrics are Matthew's Corrcoef (Mcc) <ref type="bibr" target="#b44">[45]</ref> for CoLA, Pearson Corrcoef (Pcc) <ref type="bibr" target="#b45">[46]</ref> for STS-B, and Accuracy (Acc) for the others. To demonstrate that AD-DROP applies to token-level tasks as well, we conduct experiments on Named Entity Recognition (CoNLL-2003 <ref type="bibr" target="#b31">[32]</ref>) and Machine Translation (WMT 2016 <ref type="bibr" target="#b32">[33]</ref>) datasets, the results of which are shown in Appendix A.2. Besides, we also evaluate AD-DROP on two out-of-distribution (OOD) datasets, including HANS <ref type="bibr" target="#b33">[34]</ref> and PAWS-X <ref type="bibr" target="#b34">[35]</ref>. The details of these datasets are introduced in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implement our AD-DROP in Pytorch with the Transformers package <ref type="bibr" target="#b46">[47]</ref>. We train the selected PrLMs on GeForce RTX 3090 GPUs. We tune the learning rate in {1e-5, 2e-5, 3e-5} and the batch size in {16, 32, 64}. Following Miao et al. <ref type="bibr" target="#b16">[17]</ref>, we perform early stopping to choose the number of training epochs on GLUE. The two critical hyperparameters p and q are searched within [0.1, 0.9] with step size 0.1. For integrated gradient in Eq. (3), we follow Hao et al. <ref type="bibr" target="#b22">[23]</ref> and set m to 20. We apply AD-DROP only in the first layer for the datasets of SST-2, MNLI, QNLI, QQP, and STS-B since the fine-tuning on these datasets is stable and less likely to cause overfitting. For the rest datasets, we apply AD-DROP in all layers. We provide the detailed hyperparameter settings on each dataset in Appendix C.2. Our code is available at https://github.com/TaoYang225/AD-DROP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Results</head><p>We report the overall results of the fine-tuned models in Table <ref type="table" target="#tab_1">1</ref>. We first compare AD-DROP with existing regularization methods on the development sets, including the original fine-tuning, SCAL <ref type="bibr" target="#b16">[17]</ref>, SuperT <ref type="bibr" target="#b47">[48]</ref>, R-Drop <ref type="bibr" target="#b17">[18]</ref>, and HiddenCut <ref type="bibr" target="#b14">[15]</ref>. We observe that AD-DROP surpasses the baselines on most of the datasets. Specifically, AD-DROP yields an average improvement of 1.98 and 1.29 points on BERT base and RoBERTa base , respectively. We then discuss the performance of AD-DROP on the test sets. Results in Table <ref type="table" target="#tab_1">1</ref> show that AD-DROP achieves consistent improvement, boosting the average scores of BERT base and RoBERTa base by 0.87 and 0.62, respectively. Besides, compared with large datasets, AD-DROP achieves more gains on small datasets, which are more likely to cause overfitting, illustrating that AD-DROP is more suitable for small data scenarios. We conduct ablation experiments on four small datasets to investigate the impact of different components. Due to the limited number of submissions imposed by the GLUE server for evaluation, the results here are reported on the development sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Attribution methods AD-DROP can be implemented with different attribution methods to generate the mask matrix in Eq. ( <ref type="formula" target="#formula_0">1</ref>), such as integrated gradient attribution (IGA) introduced Eq. ( <ref type="formula" target="#formula_3">3</ref>), attention weights for attribution (AA), and randomly generating the discard region (RD) in Eq. ( <ref type="formula" target="#formula_9">6</ref>). We replace the gradient attribution (GA) in Eq. ( <ref type="formula" target="#formula_8">5</ref>)-( <ref type="formula" target="#formula_9">6</ref>) with these methods.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, we can make three observations. First, AD-DROP with gradientbased attribution methods (GA and IGA) surpasses that with the other methods (AA or RD) on most of the datasets, illustrating that gradientbased methods are better at finding features that are likely to cause overfitting. Second, IGA outperforms GA in some cases. Although IGA provides better theoretical justification than GA for attribution, it requires prohibitively more computational cost than GA (see Section 4.7 for efficiency analysis), making GA a more desirable choice for AD-DROP. Third, AD-DROP improves the original BERT base and RoBERTa base with any of the masking strategies, demonstrating the robustness of AD-DROP to overfitting when fine-tuning these models.</p><p>Pseudo labels vs gold labels In Section 2.3, we discuss the motivation of using pseudo labels for attribution in AD-DROP. To verify the reasonability, we conduct an experiment with gold labels for attribution. As the results show in Table <ref type="table" target="#tab_2">2</ref>, using gold labels for attribution deteriorates the performance, illustrating that AD-DROP with pseudo labels for attribution is preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZRFURVVWXQLQJ ZLWKFURVVWXQLQJ $FFXUDF\</head><p>Figure <ref type="figure">4</ref>: Results of AD-DROP with and without cross-tuning when enumerating p and q in [0.1, 0.9]. RoBERTa is chosen as the base model. Results show that "with cross-tuning" leads to much lower variance and higher performance.</p><p>Cross-tuning To verify the effectiveness of the cross-tuning strategy, we ablate it and apply only AD-DROP in all training epochs. As shown in Table <ref type="table" target="#tab_2">2</ref>, removing cross-tuning causes noticeable performance degradation on most of the datasets. This can be explained by the intuition that AD-DROP without cross-tuning tends to discard high-attribution positions excessively and make the model difficult to converge normally. To vividly demonstrate the effect of AD-DROP with or without cross-tuning, we visualize the distributions of the performance on the RTE <ref type="foot" target="#foot_2">4</ref> development set when enumerating the parameters p and q in the range of [0.1, 0.9].</p><p>The results are plotted in Figure <ref type="figure">4</ref>, where each blue/orange point denotes the accuracy with a pair of p and q values. We observe from the figure that AD-DROP without cross-tuning cannot be trained properly under some parameter settings. However, it works well for most parameter settings when cross-tuning is applied, demonstrating that cross-tuning is vital for improving training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>In this section, we further conduct several experiments for more thorough analysis. To reduce the influence of randomness, we conduct repeated experiments on four small datasets (i.e., CoLA, STS-B, MRPC and RTE). We repeat the training of each model with five random seeds and report the average score and standard deviation on the development sets. From Table <ref type="table" target="#tab_3">3</ref>, we observe that AD-DROP outperforms the original fine-tuning on all the datasets. In addition, AD-DROP results in lower standard deviations on most of the datasets, showing that AD-DROP is more robust in fine-tuning PrLMs than the original approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Repeated Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Data Size</head><p>To study the impact of data size, we compare AD-DROP with the original fine-tuning (FT) approach on QNLI and QQP, <ref type="foot" target="#foot_3">5</ref> two relatively large datasets, and report their performance when the number of training samples changes. RoBERTa is chosen as the base model. Figure <ref type="figure" target="#fig_7">5</ref> shows that AD-DROP  outperforms FT consistently on QNLI. Moreover, AD-DROP improves the efficiency of data use as training AD-DROP with 60% training samples produces comparable performance to FT with full data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Sensitivity</head><p>AD-DROP involves two hyperparameters p and q to control the number of discarded attention positions. To investigate the sensitivity of AD-DROP to them, we show the results of different p and q combinations on CoLA and RTE in Figure <ref type="figure" target="#fig_8">6</ref>, in which we apply MaxAbsScaler<ref type="foot" target="#foot_4">6</ref> to project the difference between the results of AD-DROP and FT into the interval of [-1.0, 1.0]. We observe that BERT with AD-DROP is not hyperparameter-sensitive as it outperforms the baseline under most settings. In contrast, RoBERTa with AD-DROP is more sensitive and requires a careful search for optimal hyperparameter settings. The possible reason is that RoBERTa is pre-trained with more data and more effective tasks than BERT, making it less prone to overfitting than BERT.  To verify the scalability of AD-DROP for a larger model size, we evaluate AD-DROP with RoBERTa large on the RTE and MRPC datasets. Table <ref type="table" target="#tab_4">4</ref> shows the average scores and standard deviations of five random seeds. There are two main observations. First, AD-DROP achieves consistent improvements over the larger RoBERTa model, illustrating that AD-DROP is scalable to large models. Second, compared with RoBERTa base on RTE in Table <ref type="table" target="#tab_3">3</ref>, the larger model significantly reduces the deviation (from 1.7 to 0.86), suggesting that a larger model size indeed helps to improve the stability. AD-DROP further improves the performance and reduces the deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Larger Model Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Few-shot Scenario</head><p>In this subsection, we test the performance of AD-DROP under few-shot scenarios. Specifically, we carry out 16-, 64-, and 256-shot experiments on SST-2 and CoLA with RoBERTa base as the base model and the baseline. We report the average scores and standard deviations of five random seeds in Table <ref type="table" target="#tab_5">5</ref>. We observe that RoBERTa with AD-DROP consistently outperforms the original fine-tuning approach. Besides, AD-DROP tends to bring more benefits when fewer samples are available. To further demonstrate AD-DROP is beneficial to reducing overfitting, we test AD-DROP with RoBERTa base on two out-of-distribution (OOD) datasets, i.e., HANS and PAWS-X. For HANS, we use the checkpoints trained on MNLI and test their performance on the validation set (the test set is not supplied). For PAWS-X, we use the checkpoints trained on QQP and examine its performance on the test set. The evaluation metric is accuracy. From Table <ref type="table" target="#tab_6">6</ref>, we can see that RoBERTa with AD-DROP achieves better generalization, where AD-DROP boosts the performance by 0.66 on HANS and 3.35 on PAWS-X, illustrating that the model trained with AD-DROP generalizes better to OOD data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Computational Efficiency</head><p>To analyze the computational efficiency, we quantitatively study the computational cost of AD-DROP with different dropping strategies (GA, IGA, AA, and RD) relative to the original fine-tuning on CoLA, STS-B, MRPC, and RTE. BERT is chosen as the base model for this experiment. As shown in Table <ref type="table" target="#tab_7">7</ref>, although IGA achieves more favorable performance on one of the datasets, it requires higher computational costs than its counterparts, especially when applied in all the layers. In contrast, AD-DROP with GA is more competitive in both performance and computational cost.  <ref type="bibr" target="#b18">[19]</ref> randomly selects neurons with a predefined probability and sets their values to zeros during training. By doing so, the neurons cannot co-adapt and the trained networks can lead to better generalization. In recent years, many variants of dropout have emerged. One body of research aims to adopt different strategies to drop units in neural networks. For example, DropConnect <ref type="bibr" target="#b48">[49]</ref> randomly selects connections between neurons to discard. DropBlock <ref type="bibr" target="#b20">[21]</ref> defines a structured dropout that randomly drops the units in a specific contiguous region of a feature map. AutoDropout <ref type="bibr" target="#b21">[22]</ref> aims to improve the dropout pattern of DropBlock by introducing an automatic method to design dropout structures. HiddenCut <ref type="bibr" target="#b14">[15]</ref> drops contiguous spans within the hidden space, in which the attention weights are utilized to select the dropped spans strategically.</p><p>Another body of research devotes to addressing the inconsistency between training and inference when dropout is applied. For instance, mixout <ref type="bibr" target="#b10">[11]</ref> randomly replaces selected parameters with original pre-trained weights rather than setting them to zeros. CHILD-TUNING <ref type="bibr" target="#b12">[13]</ref> selects a child network and masks out the gradients of the non-child network during the backward step, only updating weights in the child network. R-Drop <ref type="bibr" target="#b17">[18]</ref> performs dropout twice in the forward steps to produce two sub-models and then applies KL-divergence for their output distributions, forcing the two sub-models to be consistent with each other. However, most of these methods follow the random sampling strategy of dropout and pay little attention to the different importance of self-attention positions in PrLMs.</p><p>Attribution Numerous studies have been undertaken to interpret the behaviors of deep neural networks (DNNs). As a theory for interpretability, attribution aims to evaluate the impact of input features on predictions <ref type="bibr" target="#b23">[24]</ref>. Generally, attribution methods can be divided into perturbation-based <ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>, gradient-related <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b25">26]</ref>, and attention-based <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref> methods. We focus on reviewing the gradient-related methods as they are more relevant to our work. Specifically, earlier works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> try to explain model decisions via gradients since gradients indicate the direction and rate that changes the loss the fastest. However, Sundararajan et al. <ref type="bibr" target="#b25">[26]</ref> point out that gradient attribution violates the sensitivity axiom in some cases that the gradients will be zero for the function in saturated areas, and propose integrated gradient as a theoretically more sound attribution method.</p><p>Other efforts have been devoted to revealing the behavior patterns of PrLMs. Kovaleva et al. <ref type="bibr" target="#b7">[8]</ref> and Clark et al. <ref type="bibr" target="#b8">[9]</ref> use attention weights for attribution to investigate what specific knowledge BERT <ref type="bibr" target="#b0">[1]</ref> learns. Jain and Wallace <ref type="bibr" target="#b26">[27]</ref> and Brunner et al. <ref type="bibr" target="#b58">[59]</ref> investigate the identifiability of attention weights and conclude that attention weights are not a faithful explanation for model predictions. Hao et al. <ref type="bibr" target="#b22">[23]</ref> apply integrated gradient <ref type="bibr" target="#b25">[26]</ref> as a saliency measure for self-attention attribution in BERT, and use the attribution result to interpret information interactions inside Transformers. Similarly, Lu et al. <ref type="bibr" target="#b27">[28]</ref> develop influence patterns based on integrated gradient to explain information flow in BERT. Unlike these works, we aim to examine the effect of dropout on self-attention through self-attention attribution and to reduce overfitting when fine-tuning PrLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel dropout regularizer, AD-DROP, to mitigate overfitting when fine-tuning PrLMs on downstream tasks. Unlike previous dropout-based methods that generally adopt the random sampling strategy to discard units, AD-DROP draws inspiration from self-attention attribution which reveals that attention positions are not equally important in reducing overfitting and that dropping inappropriate positions may exacerbate the problem. Therefore, AD-DROP focuses on discarding high-attribution attention positions to prevent the model from relying heavily on these positions to make predictions. Besides, we propose a cross-tuning strategy that performs the original fine-tuning and our AD-DROP alternately to stabilize the fine-tuning process. Extensive experiments and analysis on the GLUE benchmark demonstrate the effectiveness of AD-DROP. Although originally proposed and evaluated based on self-attention attribution, AD-DROP can be potentially extended to other neural network units as vanilla dropout, which deserves further research efforts. Note that AD-DROP is naturally suitable for classification tasks since we can obtain one single attribution matrix with respect to the only logit output for each attention map. For token-level tasks (e.g., NER and text generation), as we have several logit outputs to produce the corresponding attribution matrices for each attention map, applying AD-DROP has the challenge of how to fuse these attribution matrices. We provide a simple alternative to calculate the attribution matrix in Eq. ( <ref type="formula" target="#formula_8">5</ref>) as:</p><formula xml:id="formula_13">B h = - ?L ?A h , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where L is the pseudo loss in terms of the pseudo labels. Given a sequence x with n input tokens, we represent each pseudo label as a one-hot vector of C elements and compute L as:</p><formula xml:id="formula_15">L = n i=1 L i = - n i=1 C c=1 y i,c logP F (c|x, i) = - n i=1 y i,c logP F (c|x, i),<label>(9)</label></formula><p>where y i,c is the c-th element in the one-hot vector for token i, P F (c|x, i) is the softmax output of class c for token i, and c is the pseudo label. Then, Eq. ( <ref type="formula" target="#formula_13">8</ref>) can be updated as:</p><formula xml:id="formula_16">B h = - ?L ?A h = - n i=1 ?L i ?F i,c (A) ? ?F i,c (A) ?A h = n i=1 (y i,c -P F (c|x, i)) B i,h .<label>(10)</label></formula><p>Therefore, we can use Eq. ( <ref type="formula" target="#formula_16">10</ref>) to compute a single attribution matrix for each attention map when applying AD-DROP in token-level tasks. Besides, as regression tasks (e.g., STS-B) cannot infer pseudo labels, we directly use the actual loss instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Token-Level Experiments</head><p>We conduct additional experiments of AD-DROP on NER (CoNLL-2003) and Machine Translation (WMT 2016) tasks. <ref type="foot" target="#foot_5">7</ref> The results on the test sets are reported in Table <ref type="table" target="#tab_8">8</ref> and Table <ref type="table" target="#tab_9">9</ref>. Moreover, to verify that AD-DROP can be adapted to other pre-trained models, for CoNLL-2003 NER, we choose ELECTRA as the base model. For WMT 2016, OPUS-MT is chosen. The results show that AD-DROP consistently improves the baselines on both NER and Machine Translation tasks.  Our observations in Figure <ref type="figure" target="#fig_1">2</ref>(a) show that dropping low-attribution positions makes the model fit the training data rapidly, while dropping high-attribution positions reduces the fitting speed. To further probe the effect of dropping low-or high-attention positions, we fine-tune a RoBERTa on the training set and evaluate its performance on the development set by applying the two dropping strategies. The results on MRPC, SST-2, and QNLI are plotted in Figure <ref type="figure" target="#fig_11">7</ref>. Similar phenomena can be observed that the model rapidly fits the data while dropping only a small proportion of low-attribution positions. As the dropping rate increases, the accuracy remains stable until discarding too many low-attribution positions. When dropping high-attribution positions, we observe an opposite trend that the performance deteriorates sharply. These results further confirm the observations in Section 2.2 that attention positions should not be treated equally important in dropout.</p><p>Note that we only drop positions in the first layer of RoBERTa for the above experiments to exclude the impact of different layers. We also conduct experiments in the other layers on SST-2, and the overall results are shown in Figure <ref type="figure" target="#fig_12">8</ref>. We note that similar results are obtained in the first few layers, while the trend becomes less obvious as the number of layers increases. It could be caused by the over-smoothing <ref type="bibr" target="#b59">[60]</ref> issue that the representations of all tokens are similar in the last few layers.   The details of the used datasets are introduced as follows.</p><p>(1) Stanford Sentiment Treebank (SST-2) <ref type="bibr" target="#b37">[38]</ref> is a sentence sentiment prediction task. (2) Multi-Genre Natural Language Inference (MNLI) <ref type="bibr" target="#b38">[39]</ref> is a pairwise sentence classification task that aims to predict whether the relationship between two sentences is entailment, contradiction, or neutral. (3) Question Natural Language Inference (QNLI) <ref type="bibr" target="#b39">[40]</ref> is a binary sentence classification task that aims to predict whether the sentence in a question-sentence pair contains the correct answer to the question. (4) Quora Question Pairs (QQP) <ref type="bibr" target="#b40">[41]</ref> is a binary pairwise sentence classification task that aims to predict whether two questions are semantically equivalent. <ref type="bibr" target="#b4">(5)</ref> The Corpus of Linguistic Acceptability (CoLA) <ref type="bibr" target="#b41">[42]</ref> aims to predict whether a single English sentence conforms to linguistics. <ref type="bibr" target="#b5">(6)</ref> The goal of Semantic Textual Similarity Benchmark (STS-B) <ref type="bibr" target="#b42">[43]</ref> is to predict how two given sentences are semantically similar. (7) Microsoft Research Paraphrase Corpus (MRPC) <ref type="bibr" target="#b36">[37]</ref> aims to predict if two sentences are semantically equivalent. (8) Recognizing Textual Entailment (RTE) <ref type="bibr" target="#b43">[44]</ref> is similar to MNLI but has binary labels. (9) CoNLL-2003 <ref type="bibr" target="#b31">[32]</ref> is to recognize the named entities in a sentence, which contains four types of named entities. ( <ref type="formula" target="#formula_16">10</ref>) WMT 2016 <ref type="bibr" target="#b32">[33]</ref> is a multilingual translation database. In this study, we choose English-Romanian (EN-RO) and Turkish-English (TR-EN) for the experiment. <ref type="bibr" target="#b10">(11)</ref> Heuristic Analysis for NLI Systems (HANS) <ref type="bibr" target="#b33">[34]</ref> aims to evaluate whether NLI models adopt syntactic heuristics. ( <ref type="formula">12</ref>) PAWS-X <ref type="bibr" target="#b34">[35]</ref> is a cross-lingual adversarial dataset for paraphrase identification. HANS and PAWS-X are typically used for the OOD generalization test. The statistics of these datasets are shown in Table <ref type="table" target="#tab_10">10</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameter Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ablation of Cross-Tuning</head><p>We further report the results of removing cross-tuning in AD-DROP when enumerating p and q in the range of [0.1, 0.9] on the CoLA and MRPC datasets. We observe consistent performance degradation in Figure <ref type="figure" target="#fig_13">9</ref> after removing the cross-tuning strategy from AD-DROP.  Figure <ref type="figure" target="#fig_5">10</ref> shows a comparison between AD-DROP and the original fine-tuning (FT) as the size of training examples changes. We observe from the figure that AD-DROP performs consistently better than original FT with different sizes of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Appendix: Limitations</head><p>We discuss potential limitations of AD-DROP as follows. First, as reported in Section 4.7, training with AD-DROP requires more computational cost than the original fine-tuning approach especially when integrated gradient is applied for attribution in all the attention heads. Therefore, we propose to use gradient for attribution in AD-DROP as it achieves competitive performance with acceptable computational cost. Second, AD-DROP introduces additional hyperparameters (p and q) and requires more effort to search for the best hyperparameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention maps of vanilla dropout and our AD-DROP. Darker attention positions indicate higher attribution scores, and crossed circles mean dropped attention positions. Red-dotted boxes refer to candidate discard regions with high attribution scores. Unlike vanilla dropout which randomly discards attention positions, AD-DROP focuses on dropping highattribution positions in candidate discard regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of training and validation losses when finetuning RoBERTa with different dropping strategies on MRPC. The dropping rate is set to 0.3 if it applies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of AD-DROP in four steps. (1) Conduct the first forward computation to obtain pseudo label c. (2) Generate attribution matrices B via computing the gradient of logit output F c (A) with respect to each attention head. (3) Sort B and strategically drop some positions to produce mask matrices M. (4) Feed M into the next forward computation to compute the final loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>10 :</head><label>10</label><figDesc>epoch = epoch + 1 11: return W = W 3 Experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of AD-DROP and FT as the number of training samples changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of sensitivity study on CoLA and RTE. Rows correspond to p and columns refer to q. Blue blocks indicate the results of AD-DROP below the baseline (FT), and red blocks mean the results of AD-DROP above the baseline. Darker colors mean greater gaps with the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] Our contributions and scope are accurately reflected in the abstract and introduction. (b) Did you describe the limitations of your work? [Yes] We discuss the potential limitations of our work in Appendix E. (c) Did you discuss any potential negative societal impacts of your work? [No] Our work is a general training method and will not bring any negative societal impact. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the guidelines and ensured our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We have submitted our code and data in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We describe our training details in Section 3.2 and Appendix C. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We conduct repeated experiments in Section 4.1. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We describe the type of GPUs used in this work in Section 3.2. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We provide appropriate citations for the used data and tools. (b) Did you mention the license of the assets? [No] All used datasets are publicly available. (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] A Appendix: AD-DROP for Token-Level Tasks A.1 Attribution Matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of fine-tuned RoBERTa on development sets, where two dropping strategies (i.e., drop low-/high-attribution positions) are applied. Gold labels are used for the attribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Results dropping self-attention positions in different layers of RoBERTa on SST-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Results of AD-DROP with and without cross-tuning when enumerating p and q in the range of [0.1, 0.9] on the CoLA and MRPC datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Results of comparison between AD-DROP and original FT as the size of training data changes on QQP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overall results of fine-tuned models on the GLUE benchmark. The symbol ? denotes results directly taken from the original papers. The best average results are shown in bold.</figDesc><table><row><cell>Methods</cell><cell cols="9">SST-2 MNLI QNLI QQP CoLA STS-B MRPC RTE Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Development</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT base</cell><cell>92.3</cell><cell>84.6</cell><cell>91.5</cell><cell>91.3</cell><cell>60.3</cell><cell>89.9</cell><cell>85.1</cell><cell>70.8</cell><cell>83.23</cell></row><row><cell>+SCAL  ? [17]</cell><cell>92.8</cell><cell>84.1</cell><cell>90.9</cell><cell>91.4</cell><cell>61.7</cell><cell>-</cell><cell>-</cell><cell>69.7</cell><cell>-</cell></row><row><cell>+SuperT  ? [48]</cell><cell>93.4</cell><cell>84.5</cell><cell>91.3</cell><cell>91.3</cell><cell>58.8</cell><cell>89.8</cell><cell>87.5</cell><cell>72.5</cell><cell>83.64</cell></row><row><cell>+R-Drop  ? [18]</cell><cell>93.0</cell><cell>85.5</cell><cell>92.0</cell><cell>91.4</cell><cell>62.6</cell><cell>89.6</cell><cell>87.3</cell><cell>71.1</cell><cell>84.06</cell></row><row><cell>+AD-DROP</cell><cell>93.9</cell><cell>85.1</cell><cell>92.3</cell><cell>91.8</cell><cell>64.6</cell><cell>90.4</cell><cell>88.5</cell><cell>75.1</cell><cell>85.21</cell></row><row><cell>RoBERTa base</cell><cell>95.3</cell><cell>87.6</cell><cell>92.9</cell><cell>91.9</cell><cell>64.8</cell><cell>90.9</cell><cell>90.7</cell><cell>79.4</cell><cell>86.69</cell></row><row><cell>+R-Drop [18]</cell><cell>95.2</cell><cell>87.8</cell><cell>93.2</cell><cell>91.7</cell><cell>64.7</cell><cell>91.2</cell><cell>90.5</cell><cell>80.5</cell><cell>86.85</cell></row><row><cell>+HiddenCut  ? [15]</cell><cell>95.8</cell><cell>88.2</cell><cell>93.7</cell><cell>92.0</cell><cell>66.2</cell><cell>91.3</cell><cell>92.0</cell><cell>83.4</cell><cell>87.83</cell></row><row><cell>+AD-DROP</cell><cell>95.8</cell><cell>88.0</cell><cell>93.5</cell><cell>92.0</cell><cell>66.8</cell><cell>91.4</cell><cell>92.2</cell><cell>84.1</cell><cell>87.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT base</cell><cell>93.6</cell><cell>84.7</cell><cell>90.4</cell><cell>89.3</cell><cell>52.8</cell><cell>85.6</cell><cell>81.4</cell><cell>68.4</cell><cell>80.78</cell></row><row><cell>+AD-DROP</cell><cell>94.3</cell><cell>85.2</cell><cell>91.6</cell><cell>89.4</cell><cell>53.3</cell><cell>86.6</cell><cell>84.1</cell><cell>68.7</cell><cell>81.65</cell></row><row><cell>RoBERTa base</cell><cell>94.8</cell><cell>87.5</cell><cell>92.8</cell><cell>89.6</cell><cell>58.3</cell><cell>88.7</cell><cell>86.3</cell><cell>75.1</cell><cell>84.14</cell></row><row><cell>+AD-DROP</cell><cell>95.9</cell><cell>87.6</cell><cell>93.4</cell><cell>89.5</cell><cell>58.5</cell><cell>89.3</cell><cell>87.9</cell><cell>76.0</cell><cell>84.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="4">CoLA STS-B MRPC RTE</cell></row><row><cell>BERT base</cell><cell>60.3</cell><cell>89.9</cell><cell>85.1</cell><cell>70.8</cell></row><row><cell>+AD-DROP (GA)</cell><cell>64.6</cell><cell>90.4</cell><cell>88.5</cell><cell>75.1</cell></row><row><cell>r/w IGA</cell><cell>63.8</cell><cell>90.7</cell><cell>88.5</cell><cell>74.4</cell></row><row><cell>r/w AA</cell><cell>63.6</cell><cell>90.0</cell><cell>88.0</cell><cell>74.7</cell></row><row><cell>r/w RD</cell><cell>62.1</cell><cell>90.2</cell><cell>87.8</cell><cell>74.7</cell></row><row><cell>r/w gold labels</cell><cell>63.2</cell><cell>-</cell><cell>88.0</cell><cell>74.4</cell></row><row><cell>w/o cross-tuning</cell><cell>62.1</cell><cell>90.4</cell><cell>87.3</cell><cell>71.5</cell></row><row><cell>RoBERTa base</cell><cell>64.8</cell><cell>90.9</cell><cell>90.7</cell><cell>79.4</cell></row><row><cell>+AD-DROP (GA)</cell><cell>66.8</cell><cell>91.4</cell><cell>92.2</cell><cell>84.1</cell></row><row><cell>r/w IGA</cell><cell>68.1</cell><cell>91.6</cell><cell>91.4</cell><cell>82.7</cell></row><row><cell>r/w AA</cell><cell>66.3</cell><cell>91.5</cell><cell>91.2</cell><cell>82.3</cell></row><row><cell>r/w RD</cell><cell>66.5</cell><cell>91.5</cell><cell>92.2</cell><cell>82.0</cell></row><row><cell>r/w gold labels</cell><cell>66.4</cell><cell>-</cell><cell>91.2</cell><cell>82.0</cell></row><row><cell>w/o cross-tuning</cell><cell>67.3</cell><cell>91.3</cell><cell>90.4</cell><cell>80.5</cell></row></table><note><p>Results of ablation studies, in which r/w means "replace with" and w/o means "without".</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of repeated experiments. Each score is the average of five runs with a standard deviation.RoBERTa base 64.3?0.9 91.0?0.2 89.8?0.8 79.1?1.7 +AD-DROP 66.4?0.9 91.2?0.1 91.3?0.7 82.5?0.9</figDesc><table><row><cell>Methods</cell><cell>CoLA</cell><cell>STS-B</cell><cell>MRPC</cell><cell>RTE</cell></row><row><cell>BERT base</cell><cell cols="4">61.8?1.9 89.4?0.5 85.2?1.3 71.2?1.2</cell></row><row><cell cols="5">+AD-DROP 63.4?0.4 90.1?0.5 87.4?0.9 73.9?1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Testing AD-DROP on a larger model.</figDesc><table><row><cell>Methods</cell><cell>MRPC</cell><cell>RTE</cell></row><row><cell cols="3">RoBERTa large 90.83 ?0.75 85.99 ?0.86</cell></row><row><cell>+AD-DROP</cell><cell cols="2">91.62 ?0.53 88.01 ?0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Testing AD-DROP in few-shot settings. RoBERTa with AD-DROP achieves higher performance and lower deviations than that with the original fine-tuning approach. DROP 80.16 ?1.51 91.61 ?0.52 92.61 ?0.<ref type="bibr" target="#b12">13</ref> 26.70 ?4.96 46.41 ?1.98 52.47 ?1.16</figDesc><table><row><cell>Methods</cell><cell>16-shot</cell><cell>SST-2 64-shot</cell><cell>256-shot</cell><cell>16-shot</cell><cell>CoLA 64-shot</cell><cell>256-shot</cell></row><row><cell cols="2">RoBERTa base 74.50 ?3.03</cell><cell>89.06 ?0.83</cell><cell>91.44 ?0.17</cell><cell>23.18 ?6.38</cell><cell>39.70 ?4.68</cell><cell>51.11 ?1.64</cell></row><row><cell cols="3">+AD-4.6 Out-of-Distribution Generalization</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Testing AD-DROP on OOD datasets.</figDesc><table><row><cell>Methods</cell><cell cols="2">HANS PAWS-X</cell></row><row><cell cols="2">RoBERTa base 69.83</cell><cell>47.90</cell></row><row><cell>+AD-DROP</cell><cell>70.49</cell><cell>51.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results of performance and computational cost of AD-DROP with different masking strategies (GA, IGA, AA, and RD) relative to the original fine-tuning. The symbol ? means AD-DROP is only applied in the first layer. BERT is chosen as the base model.</figDesc><table><row><cell>Methods</cell><cell>Mcc</cell><cell>CoLA Time</cell><cell cols="2">STS-B  ? Pcc Time</cell><cell>Acc</cell><cell>MRPC Time</cell><cell>Acc</cell><cell>RTE Time</cell></row><row><cell>RD</cell><cell>+1.8</cell><cell>?1.42</cell><cell>+0.3</cell><cell>?1.38</cell><cell>+2.7</cell><cell>?1.31</cell><cell>+3.9</cell><cell>?1.42</cell></row><row><cell>AA</cell><cell>+3.3</cell><cell>?1.42</cell><cell>+0.1</cell><cell>?1.48</cell><cell>+2.9</cell><cell>?1.94</cell><cell>+3.9</cell><cell>?1.58</cell></row><row><cell>GA</cell><cell>+4.3</cell><cell>?3.58</cell><cell>+0.5</cell><cell>?1.95</cell><cell>+3.4</cell><cell>?4.13</cell><cell>+4.3</cell><cell>?4.50</cell></row><row><cell>IGA</cell><cell cols="8">+3.5 ?99.61 +0.8 ?15.00 +3.4 ?110.12 +3.6 ?125.67</cell></row><row><cell cols="2">5 Related Work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Dropout Dropout is a widely used regularizer to mitigate overfitting when training deep neural</cell></row><row><cell cols="3">networks. Vanilla dropout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Test results of AD-DROP on the CoNLL-2003 NER dataset.</figDesc><table><row><cell>Methods</cell><cell>Accuracy</cell><cell>F1</cell></row><row><cell>ELECTRA base</cell><cell>97.83</cell><cell>91.23</cell></row><row><cell>+AD-DROP</cell><cell>97.95</cell><cell>91.77</cell></row><row><cell cols="3">B Appendix: More Prior Experiments</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Test results of AD-DROP on Translation datasets. The evaluation metric is BLEU.</figDesc><table><row><cell>Methods</cell><cell cols="2">EN-RO TR-EN</cell></row><row><cell>OPUS-MT</cell><cell>26.11</cell><cell>23.88</cell></row><row><cell>+AD-DROP</cell><cell>26.43</cell><cell>23.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Statistics of the used datasets.</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>SST-2</cell><cell>67349</cell><cell>872</cell><cell>1821</cell></row><row><cell>MNLI</cell><cell cols="2">392702 9815</cell><cell>9796</cell></row><row><cell>QNLI</cell><cell cols="2">104743 5463</cell><cell>5463</cell></row><row><cell>QQP</cell><cell cols="3">363846 40430 390965</cell></row><row><cell>CoLA</cell><cell>8551</cell><cell>1043</cell><cell>1063</cell></row><row><cell>STS-B</cell><cell>5749</cell><cell>1500</cell><cell>1378</cell></row><row><cell>MRPC</cell><cell>3668</cell><cell>408</cell><cell>1725</cell></row><row><cell>RTE</cell><cell>2490</cell><cell>277</cell><cell>3000</cell></row><row><cell cols="2">CoNLL-2003 14041</cell><cell>3250</cell><cell>3453</cell></row><row><cell>EN-RO</cell><cell cols="2">610320 1999</cell><cell>1999</cell></row><row><cell>TR-EN</cell><cell cols="2">205756 1001</cell><cell>3000</cell></row><row><cell>HANS</cell><cell cols="2">30000 30000</cell><cell>-</cell></row><row><cell>PAWS-X</cell><cell>49401</cell><cell>2000</cell><cell>2000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Table 11 presents the final hyperparameter settings of AD-DROP for BERT/RoBERTa base . The setting with only one value means the parameter is shared by BERT and RoBERTa. Hyperparameter settings of AD-DROP for BERT and RoBERTa.</figDesc><table><row><cell cols="4">Dataset Learning rate Batch size Length</cell><cell>p</cell><cell>q</cell></row><row><cell>SST-2</cell><cell>1e-5</cell><cell>16/64</cell><cell>120</cell><cell cols="2">0.6/0.3 0.8/0.7</cell></row><row><cell>MNLI</cell><cell>1e-5</cell><cell>16/32</cell><cell>128</cell><cell cols="2">0.5/0.4 0.9/0.2</cell></row><row><cell>QNLI</cell><cell>1e-5</cell><cell>16</cell><cell>128</cell><cell>0.8</cell><cell>0.8/0.4</cell></row><row><cell>QQP</cell><cell>1e-5</cell><cell>16</cell><cell>120</cell><cell cols="2">0.2/0.7 0.7/0.9</cell></row><row><cell>CoLA</cell><cell>1e-5/2e-5</cell><cell>16</cell><cell>47</cell><cell cols="2">0.3/0.8 0.4/0.3</cell></row><row><cell>STS-B</cell><cell>1e-5/2e-5</cell><cell>16</cell><cell>100</cell><cell cols="2">0.9/0.1 0.7/0.5</cell></row><row><cell>MRPC</cell><cell>1e-5/2e-5</cell><cell>16</cell><cell>100</cell><cell cols="2">0.5/0.8 0.8/0.3</cell></row><row><cell>RTE</cell><cell>1e-5</cell><cell>16</cell><cell>128</cell><cell cols="2">0.6/0.7 0.7/0.1</cell></row><row><cell cols="3">D Appendix: More Experimental Results</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We provide more results and discussions in Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The negative loss will be used for both regression and token-level tasks, as introduced in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Results on the other datasets are shown in Appendix D.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Results on QQP are shown in Appendix D.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We follow the official colab implementation (https://huggingface.co/transformers/v4.7.0/ notebooks.html) for the two tasks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We appreciate the anonymous reviewers for their valuable comments. We thank <rs type="person">Feifan Yang</rs>, <rs type="person">Zihong Liang</rs>, and <rs type="person">Hong Ding</rs> for their early discussions. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62176270</rs>), the <rs type="funder">Program for Guangdong Introducing Innovative and Entrepreneurial Teams</rs> (No. <rs type="grantNumber">2017ZT07X355</rs>), and the <rs type="funder">Foundation of Key Laboratory of Machine Intelligence and Advanced Computing of the Ministry of Education</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_y47snHf">
					<idno type="grant-number">62176270</idno>
				</org>
				<org type="funding" xml:id="_4CPUZpu">
					<idno type="grant-number">2017ZT07X355</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse is enough in scaling transformers</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jaszczur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonni</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="9895" to="9907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4365" to="4374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2793" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the stability of finetuning bert: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Raise a child in large language model: Towards effective and generalizable fine-tuning</title>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9514" to="9528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Good-enough compositional data augmentation</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7556" to="7566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hiddencut: Simple data augmentation for natural language understanding with better generalizability</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4380" to="4390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
		<title level="m">Adversarial training for large neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simple contrastive representation adversarial learning for nlp tasks</title>
		<author>
			<persName><forename type="first">Deshui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13301</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">R-drop: regularized dropout for neural networks</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3584" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A regularization method for convolutional networks</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dropblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autodropout: Learning dropout patterns to regularize deep networks</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9351" to="9359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attention attribution: Interpreting information interactions inside transformer</title>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12963" to="12971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ale? Leonardis, and Ke Tang. A survey on neural network interpretability</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ti?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is not explanation</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Influence patterns for explaining information flow in bert</title>
		<author>
			<persName><forename type="first">Kaiji</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<title level="s">Shared Task Papers</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01007</idno>
		<title level="m">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Paws-x: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11828</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leqi</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/quora-question-pairs" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Cola: The corpus of linguistic acceptability</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>with added annotations</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of t4 phage lysozyme</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA)-Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pearson correlation coefficient</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israel</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noise reduction in speech processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Super tickets in pre-trained language models: From model compression to improving generalization</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6524" to="6538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">1058</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pathologies of neural models make interpretations difficult</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3719" to="3728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Restricting the flow: Information bottlenecks for attribution</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Sixt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Landgraf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1xWh1rYwB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On identifiability in transformers</title>
		<author>
			<persName><forename type="first">Gino</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Revisiting over-smoothing in bert from the perspective of graph</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen Ms</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
