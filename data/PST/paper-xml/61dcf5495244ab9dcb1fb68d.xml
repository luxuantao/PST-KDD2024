<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUPERVISED CONTRASTIVE LEARNING FOR RECOMMENDATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-10">10 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chun</forename><surname>Yang</surname></persName>
							<email>beiluo@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUPERVISED CONTRASTIVE LEARNING FOR RECOMMENDATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-10">10 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.03144v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Supervised contrastive Learning</term>
					<term>Graph convolution network</term>
					<term>Recommended system</term>
					<term>Representational learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared with the traditional collaborative filtering methods, the graph convolution network can explicitly model the interaction between the nodes of the user-item bipartite graph and effectively use higher-order neighbors, which enables the graph neural network to obtain more effective embeddings for recommendation, such as NGCF And LightGCN. However, its representations is very susceptible to the noise of interaction. In response to this problem, SGL explored the self-supervised learning on the user-item graph to improve the robustness of GCN. Although effective, we found that SGL directly applies SimCLR's comparative learning framework. This framework may not be directly applicable to the scenario of the recommendation system, and does not fully consider the uncertainty of user-item interaction.</p><p>In this work, we aim to consider the application of contrastive learning in the scenario of the recommendation system adequately, making it more suitable for recommendation task. We propose a supervised contrastive learning framework to pre-train the user-item bipartite graph, and then fine-tune the graph convolutional neural network. Specifically, we will compare the similarity between users and items during data preprocessing, and then when applying contrastive learning, not only will the augmented views be regarded as the positive samples, but also a certain number of similar samples will be regarded as the positive samples, which is different from SimCLR who treats other samples in a batch as negative samples. We term this learning method as Supervised Contrastive Learning(SCL) and apply it on the most advanced LightGCN. In addition, in order to consider the uncertainty of node interaction, we also propose a new data augment method called node replication. Empirical research and ablation study on a data set prove the effectiveness of SCL and node replication, which improve the accuracy of recommendations and robustness to interactive noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In order to meet the diverse and personalized information needs of users, the personalized recommendation system has emerged <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. A recommendation system can help users find items of interest in a large amount of candidate items. It recommends similar items to users with similar behaviors based on interactive content such as purchases, clicks, and ratings. This method is called collaborative filtering. Collaborative filtering generally uses embedding to represent users and items, and predicts scores based on embedding. In recent years, a large number of collaborative filtering methods have been successfully implemented, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Although these collaborative filtering methods are effective, they only consider the explicit interaction between a single user and a single item, and do not encode the collaborative signal explicitly. When the interaction data is very sparse, it is difficult to learn embedded representations with sufficient knowledge. Can not effectively characterize the similarities between users or items <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Figure <ref type="figure">1</ref>: The limitation of contrastive learning framework used by SGL. SGL treats other samples in a batch as negative samples, which makes the representation of similar users further in the representation space, and it should be closer to be more effective for subsequent recommendation task. Our proposed SCL introduces supervised information and labels similar users as positive samples, as shown by the red word in the figure. After contrastive learning, similar users are closer in the representation space, which can obtain representations that are more conducive to recommendation.</p><p>In order to make use of the user-item bipartite graph high-hop neighbor information fully, more effectively repersent the embedding of users and items, Wang et al. recently proposed graph convolution collaborative filtering, and the best experimental results were obtained on three data sets <ref type="bibr" target="#b11">[12]</ref>. They transferred the concept of graph convolution network to the field of recommendation systems, and regarded the user-item bipartite graph as a isomorphic graph, and carried out feature embedding, information dissemination and feature aggregation on the graph to obtain the final representation of users and items. Their method encoded the high-hop neighbors between the user and the item explicitly, so they get more reliable and more informative embedding representations and achieve better results.</p><p>Based on NGCF, He et al. proposed lightGCN <ref type="bibr" target="#b12">[13]</ref>. They believe that many operations of NGCF are directly inherited from GCN. These operations are not necessarily suitable for collaborative filtering tasks in the recommendation field. Therefore, they simplify many operations of NGCF, reduce the difficulty of model training, and achieve the current state-of-the-art on multiple data sets.</p><p>GCN provides a most advanced and efficient solution for solving user-item bipartite graph sparsity problems and interactive modeling problems. However, Wu et al. believe that the representations learned by GCN are easily biased towards high-frequency items or users and are also easily affected by noise of interactions. In order to address the above limitations, they proposed Self-supervised Graph Learning(SGL) for recommendation, which uses unlabeled data space by augmenting the input data, thereby achieving a significant improvement in downstream tasks <ref type="bibr" target="#b13">[14]</ref>. SGL refer to the contrastive learning of SimCLR and use the framework of simCLR as a paradigm of self-supervised learning <ref type="bibr" target="#b14">[15]</ref>.</p><p>Although SGL has achieved a certain degree of effect improvement, we believe that the contrastive learning framework of SimCLR may not be very suitable for recommendation tasks. Specifically, the application background of SimCLR is a computer vision(CV) task. It treats two augmenting samples of a input data as the positive samples and treats other samples in the batch as the negative samples. Due to the diversity of samples in CV tasks, this design is reasonable and helps to mine hard negative samples which improve the quality of the learned representations. However, in the recommendation system, the core of the comparison is the users or items node and the ultimate goal is to compare the cosine similarity of the data. This means that there is a high probability that there are similar users or items in a batch. Regarding these samples as the negative samples, similar users or items will become farther away in the representation space, which violates the optimization purpose of the recommendation system, affects the final representation learned by GCN, and reduces the performance of the recommendation system.</p><p>In order to solve the above limitations, we believe that it is necessary to improve the application of contrastive learning. We propose a Supervised Contrastive Learning(SCL) for recommendation. We hope to design a framework of contrastive learning with the recommendation task as the goal, so as to obtain the representations that is more in line with the requirements of the recommendation task and improve the performance of downstream tasks. Specifically, it consists of two steps: (1) data augmentation, which generates multiple views for each node; (2) contrastive learning, which makes similar samples are closer and different samples are further in the representation space. Since the Bayesian Personalized Ranking(BPR) loss of the recommendation task takes node interaction as input, a batch may contain a large number of similar users or nodes <ref type="bibr" target="#b9">[10]</ref>. Directly treating other samples as the negative samples may not be conducive to the learning of representation, which is different from SGL directly taking contrast learning as In the way of auxiliary tasks, SCL use comparative learning as a pre-training task to obtain a preliminary representation, and then use BPR loss for fine-tuning. In addition, in order to be more adaptable to the recommendation task, we will not simply treat other samples in the same batch as negative samples when we perform comparative learning, but treat all similar samples as positive samples, and dissimilar samples as negative samples. This comparative learning method introduces supervised information, which makes similar users or items more inclined to learn similar representations, which is beneficial to downstream recommendation tasks. This supervised and contrasted representation learning method starts from the goal of the recommendation system, and its design is more suitable for the recommendation system and can further improve the performance of the recommendation task.</p><p>In addition, we found that the BPR loss is simply optimized by constructing interactive and non-interactive triples. However, the user-item bipartite graph may have noisy interaction or similar user item pairs without interaction. From the purpose of the recommendation system, in order to further improve the recommendation performance, we believe that the edge drop and node drop on the user-item bipartite graph does not bring enough diversity information. In order to improve the model's adaptability to noise interaction and the diversity of the representations, we propose a new data augmentation method called node replication. Specifically, we replace part of the interaction of the current node with the corresponding interaction of similar nodes according to a certain probability. This data augmentation method can effectively improve recommendation performance and help increase the diversity of recommendations.</p><p>It is worth noting that our SCL can be applied to any collaborative filtering network based on the user-item bipartite graph. In this work, we choose to implement it on the most advanced LightGCN, and prove the effectiveness of SCL on a benchmark data set, which can significantly improve the recommendation performance.</p><p>To summarize, The main contributions of this paper are summarized as follows:</p><p>• We propose a new supervised comparative learning paradigm, which considers the purpose of the recommendation system and provides supervised information for representation learning, so that similar nodes are closer in the representation space, which helps to improve the recommendation performance.</p><p>• In order to improve the representation ability of nodes and obtain more diverse information, we propose a new data augmentation method called node replication, which improves the robustness and diversity of the representations, and makes the recommendation results more diverse and has better performance.</p><p>• We verified the effect of SCL on a benchmark dataset and proved the effectiveness of SCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 PROBLEM STATEMENT</head><p>The set of user-item interactions can be easily modeled as a bipartite graph G = (U, V, E) , where U represents the user set, V represents the item set, and E represents the edge set. If there is an interaction between a user and a item, then there is an edge between them <ref type="bibr" target="#b15">[16]</ref>. The collaborative filtering based on GCN needs to embed each user i ∈ U and item j ∈ V into a unified d-dimensional representation space, and the corresponding representations are expressed by u ∈ d and v ∈ d . By calculating the inner product between the representation s of user and the item we can know whether the item should be recommended to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH CONVOLUTIONAL NETWORK</head><p>The core of GCN is to aggregate the domain representations of each node on the bipartite graph G, and update the representation of current node by considering the information of high-order neighbors, and finally obtain an effective representation. For user i, its representation is calculated as follows:</p><formula xml:id="formula_0">u i (l) = δ(W (l) [u (l−1) i |A(v j ∈ N (u i )])<label>(1)</label></formula><p>Where v j ∈ N (u i ) represents the domain node of user i, A denotes the node aggregation function, which can be average, weighted sum, linear mapping, etc <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. [• | •] is a concatenation operation and W (l) is the weight matrix of the l-th layer, δ is the activation function of the l-th layer, which can be sigmoid, Relu, or None, etc <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. u i (l) denotes the representation of user i at layer l, u (l−1) i is that of previous layer, and u (0) i is the ID embeddings which are trainble parameters. Specifically, GCN first aggregates the representations of all domain nodes of user i at layer l − 1, then combines it with its own representation, and then obtains the representation of layer l through linear linear mapping. For a certain item j, the calculation method of its representation is the same as that on the user side. The obtained representation of the l layer corresponds to the information aggregation of the node's l hop neighbor. For different layers of information, there are also different ways to generate the final node representation, such as using the last layer only, splicing, averaging, weighted sum, attention and so on <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>After obtaining the final representation of each node, the prediction layer can be used to calculate a certain user i's preference for item j. In order to ensure the calculation efficiency of the GCN, the vector inner product is widely used as the prediction layer at present <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_1">ŷij = u i T v j<label>(2)</label></formula><p>When optimizing model parameters, GCN generally use BPR loss, and BPR loss chooses a user, an interactive item and a non-interactive item to form a triple, and the predicted value of interaction is expected to be greater than the predicted value of no interaction:</p><formula xml:id="formula_2">L BP R = − M u=1 i∈Nu j / ∈Nu lnσ (ŷ ui − ŷuj ) + λL 2<label>(3)</label></formula><p>Where λ is the parameter that controls the intensity of L2 regularization, and M is the total number of interactions. BPR loss is the key to achieving the final recommendation result, and we apply it in the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BACKGROUND CONTRASTIVE LEARNING</head><p>The framework of contrastive learning is shown in the figure <ref type="figure" target="#fig_1">3</ref>. It maximizes the similarity between augmented samples generated by the same data and minimizes the similarity with other samples in the same batch to learn representations. The framework includes four components: data augmentation, encoder, projection head, and contrast loss function <ref type="bibr" target="#b14">[15]</ref>.</p><p>Specifically, the data augmentation module generates related views from the original data X 1 through some random data augmentation methods, denoted as X 1 1 and X 2 1 . In the bipartite graph, the data augmentation methods we used include node drop, edge drop, and node replication. Different data augmentation methods have a great impact on the performance of comparative learning.</p><p>The encoder generates a representation vector H 1  1 and H 2 1 from the augmented view. In the user-item bipartite graph problem, the encoder is the GCN network. The purpose of the projection head is to project the representation into the contrast loss space. The projection head is generally an MLP with a hidden layer. Many experiments on contrastive learning have proved that the projection head is beneficial to improve the performance of contrastive learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The contrast loss function which called InfoNCE aims to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>:</p><formula xml:id="formula_3">i,j = − log exp (sim (z i , z j ) /τ ) 2N k=1 1 [k =i] exp (sim (z i , z k ) /τ ) (4)</formula><p>Where sim(u, v) = u v/ u v denote the dot product between 2 normalized u and v. 1 [k =i] is an indicator function, when k is not equal to i, the value is 1, otherwise it is 0. τ denotes a temperature parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We present the proposed paradigm of Supervised Constrastive Learning(SCL) for recommendation system. The learning process is divided into two stages, as shown in the figure <ref type="figure">4</ref>. First, we use supervised constrastive learning as the pre-training part to learn preliminary representations, and then apply the BPR loss to fine-tune the model.</p><p>In this section, we first introduce how to perform data augmentation to generate multiple views, and then introduce the design details of SCL. The two-stage training method is adopted because BPR loss uses interactions as input, while contrastive learning uses nodes as input.</p><p>SCL focuses on recommended task, hoping to make similar nodes similar in the representation space, and address the limitations of the comparative learning framework used in SGL, which makes SCL more suitable for recommendation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Augmentation</head><p>In SimCLR, the authors discusses various augmentation methods for images <ref type="bibr" target="#b14">[15]</ref>, but for recommendation task, the data structure is different so that the data augmentation methods cannot be directly migrated. In the user-item bipartite graph recommendation task, the input data is a sparse matrix composed of interactions. We regard this data as the graph structure data as shown in the figure <ref type="figure">4</ref>. In order to learn the representation of each node fully, we need to design new data augmentation schemes for graph structure data.</p><p>In SGL, the authors has designed various augmentation methods including node drop and edge drop for graph structure data <ref type="bibr" target="#b13">[14]</ref>. In order to further improve the diversity of the recommendation results and ensure that the representation of Figure <ref type="figure">4</ref>: The framework of contrastive learning, it amit to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space. each node contains the user's possible interests as much as possible, we proposed a new data augmentation method called node replication. We detail the augmentation methods as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Node Drop(ND)</head><p>Node drop discards a certain node in the graph and any interactions(i.e. edge in graph) related to it according to probability ρ 1 :</p><formula xml:id="formula_4">V (G) = (M 1 N , E)<label>(5)</label></formula><p>Where V (G) is the view generated by G as input. M 1 is the masking vectors which apply on the node set, where 0 indicates that the corresponding node is discarded. N represents the set of nodes, including user nodes set U and item nodes set V.</p><p>When performing data augmentation, we can generate multiple views by applying formula 8 multiple times on the original graph data. This augmentation can reduce the impact of high-frequency nodes on the representation by randomly drop some nodes, so that the representations can learn more from the long tail node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Edge Drop(ED)</head><p>Edge drop discards an certain interaction in the graph (i.e. edge in graph) according to the probability ρ 2 :</p><formula xml:id="formula_5">V (G) = (N , M 2 E)<label>(6)</label></formula><p>Where M 2 is the masking vectors which apply on the edge set, where 0 indicates that the corresponding edge is discarded. E represents the set of edges.</p><p>This augmentation randomly discards some edges, and it is expected that the representation learned by contrastive learning will not be affected by specific interactions, and the robustness of the representation will be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Node Replication(NR)</head><p>Node replication will replace part of the interactions of the current node with the corresponding interactions of similar nodes according to probability ρ 3 . The similarity between the nodes is represented by the cosine, that is, the more similar is the interaction history, the more similar are the nodes. The matrix form of similarity calculation is as follows: Where G is the sparse matrix represents the interaction history of each node. Then for each node we will sort S and select the first N nodes that can be used for replacement. It is worth noting that when calculating similarity, user-side nodes and item-side nodes are calculated separately.</p><formula xml:id="formula_6">S = GG T<label>(7)</label></formula><p>We divide the interaction history of each node into k segments, and randomly replace one of them when performing node replication. Node replication is as follows:</p><formula xml:id="formula_7">V (G) = (M 3 N , E)<label>(8)</label></formula><p>Where M 3 is the masking vectors which apply on the node set, where 0 indicates that the corresponding node is applied node replication.</p><p>This augmentation randomly replaces part of the interactions, and it is expected that the representation can contain more information, increase the diversity of recommendation results, and improve the performance of the recommendation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised Comparative Learning</head><p>After establishing multiple views of the node, we input them into the GCN respectively, and then perform contrastive learning loss on the generated representations. Different from InfoNCE, which treats the views of the same node as a positive sample pair, and treats the views of any other different nodes as the negative samples, we propose a supervised contrastive learning loss, which treat similar nodes as the same type and all are regarded as positive sample. We call this loss supervised InfoNCE(S-InfoNCE):</p><formula xml:id="formula_8">i,j = − log 2N k=1 1 [j∈i] exp (sim (z i , z j ) /τ ) 2N k=1 1 [k / ∈i] exp (sim (z i , z k ) /τ )<label>(9)</label></formula><p>Where 1 [k / ∈i] denotes an indicator function, when k is not similar to i, the value is 1, otherwise it is 0. While 1 [j∈i] denotes that function, when j is similar to i. sim (i, j) and τ is same as infoNCE.</p><p>Our S-InfoNCE encourages the representations of similar nodes to be close to each other in the representation space to ensure their consistency. In addition, dissimilar samples are taken as negative samples to ensure that the representations of these nodes are significantly different. We take the recommendation task as the goal, and believe that similar users should have similar representations to facilitate collaborative filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity Analyses of SCL</head><p>It is worth noting that our SCL learning paradigm will only perform pre-training with contrastive learning on the basis of GCN without introducing any additional parameters. In other words, the learning paradigm we proposed will not bring any parameter burden to the original model, and it will improve the recommendation performance while ensuring its high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>To verify the effectiveness of the SCL for recommendation paradigm proposed in this paper, a dataset named MovieLens-100k(ML-100K) is adopted for top-K recommendation and the results are compared with other state-of-the-art methods. The specific statistics of ML-100K are shown in the table ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental Metrics</head><p>In top-k recommendation, we follow the strategy described in <ref type="bibr" target="#b11">[12]</ref> and the ranking protocol described in <ref type="bibr" target="#b11">[12]</ref>, and choose MAP (Mean Average Precision), MRR (Mean Reciprocal Rank) and MNDCG (Mean Normalized Discounted Cumulative Gain) to evaluate the recommendation performance. And consider the case where K is equal to 3, 5, 10 respectively.</p><p>The proposed method is compared with the pure MLP, LSTM, BiLSTM and IDCNN to verify validity and efficiency. Since the core innovation of this paper is the proposed time relationship unit and decoupling position embedding unit that can be regarded as a new feature extractor, for fairness, the same classification layer is applied when comparing the performance among the those methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Compared Baselines</head><p>We compare the proposed SCL with the following strong baselined CF models: DeepWalk <ref type="bibr" target="#b30">[31]</ref>, Node2vec <ref type="bibr" target="#b31">[32]</ref>, LINE <ref type="bibr" target="#b32">[33]</ref>, PinSage <ref type="bibr" target="#b33">[34]</ref>, GC-MC <ref type="bibr" target="#b34">[35]</ref>, IGMC <ref type="bibr" target="#b35">[36]</ref>, NeuMF <ref type="bibr" target="#b5">[6]</ref>, NGCF <ref type="bibr" target="#b11">[12]</ref>, LightGCN <ref type="bibr" target="#b14">[15]</ref> and SGL <ref type="bibr" target="#b13">[14]</ref>. On the basis of LightGCN, we implement three variants of SCL, namely SCL-ND, SCL-ED, and SCL-NR, which respectively applied the data augmentation method of Node Drop, Edge drop, Node Replication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>The above methods are all using the official implementation. For fairness, all methods use the same training and testing data. The embedding size is uniformly fixed as 128 and the learning rate is 0.001. IGMC uses 1-hop subgraphs with an encoder depth of 2. The adam is used as optimizer and the batch size is set to 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>Table <ref type="table">2</ref>: Performance comparison of top-K recommendation on ML-100K Method MAP@3 MAP@5 MAP@10 MRR@3 MRR@5 MRR@10 NDCG@3 NDCG @5 NDCG @10 As can be seen from the table, our proposed SCL learning paradigm is more effective on ML-100K data, and has achieved significant performance improvements in all three indicators. In addition, comparing different data augmentation method, it can be seen that NR is significantly effective. This data augmentation method can effectively improve the diversity of recommendation results, thereby improving recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have only conducted experiments on small datasets at present, and we will conduct more adequate experiments on large datasets such as gowalla in the future. In addition, we will further conduct ablation experiments on SCL to illustrate its performance. We also hope to further analyze the principle of SCL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Motivation for node replication. By replacing part of the data of user 1 with the corresponding data of similar user 2, the augmented sample are more diverse, and the obtained representation can better express the user's preferences, thereby making the recommendation results more diverse.</figDesc><graphic url="image-2.png" coords="3,81.67,72.00,446.17,168.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework of contrastive learning, it amit to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space.</figDesc><graphic url="image-3.png" coords="5,181.47,72.00,246.57,187.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,73.07,72.00,465.87,168.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="6,73.85,72.01,461.79,212.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets</figDesc><table><row><cell>Dataset</cell><cell cols="4">Users# Items# Interactions# Density</cell></row><row><cell>ML-100K</cell><cell>943</cell><cell>1682</cell><cell>100000</cell><cell>6.30%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
				<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5941" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practice on long sequential user behavior modeling for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2671" to="2679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time personalization using embeddings for search ranking at airbnb</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
				<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative memory network for recommendation systems</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st international ACM SIGIR conference on research &amp; development in information retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
				<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A non negative matrix factorization for collaborative filtering recommender systems based on a bayesian probabilistic model. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="188" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><surname>Bpr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent relational metric learning via memory-based attention for collaborative ranking</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neighbor interaction aware graph convolution networks for recommendation</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1289" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03575</idno>
		<title level="m">Bilinear graph neural network with neighbor interactions</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervising learning, transfer learning, and knowledge distillation with simclr</title>
		<author>
			<persName><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00587</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards domain-agnostic contrastive learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10530" to="10541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Inductive matrix completion based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12058</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
