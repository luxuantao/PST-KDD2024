<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Systematic Study of Joint Representation Learning on Protein Sequences and Structures</title>
				<funder>
					<orgName type="full">Canada CIFAR AI Chair Program</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</orgName>
				</funder>
				<funder ref="#_ApBjCFK">
					<orgName type="full">NRC Collaborative R&amp;D Project</orgName>
				</funder>
				<funder ref="#_bvvy2xh">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Research and Mila</orgName>
				</funder>
				<funder ref="#_EXsvfhM">
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-18">18 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuanrui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
							<email>minghao.xu@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijil</forename><surname>Chenthamarakshan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aur?lie</forename><surname>Lozano</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">HEC Montr?al</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Systematic Study of Joint Representation Learning on Protein Sequences and Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-18">18 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.06275v2[q-bio.QM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein functions. Recent sequence representation learning methods based on Protein Language Models (PLMs) excel in sequence-based tasks, but their direct adaptation to tasks involving protein structures remains a challenge. In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks, but still suffers from the limited number of available structures. To bridge this gap, our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv). We introduce three representation fusion strategies and explore different pre-training techniques. Our method achieves significant improvements over existing sequence-and structure-based methods, setting new state-of-the-art for function annotation. This study underscores several important design choices for fusing protein sequence and structure information. Our implementation is available at https://github.com/ DeepGraphLearning/ESM-GearNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Proteins, as fundamental building blocks of life, play a pivotal role in numerous biological processes, ranging from enzymatic reactions to cellular signaling. Their intricate three-dimensional structures and dynamic behaviors underscore their functional diversity. Effective understanding of proteins is crucial for unraveling mechanisms underlying diseases, drug discovery, and synthetic biology. Herein, protein representation learning has emerged as a highly promising avenue, showcasing its efficacy across diverse protein comprehension tasks, such as protein structure prediction <ref type="bibr" target="#b24">(Jumper et al. 2021;</ref><ref type="bibr" target="#b1">Baek et al. 2021)</ref>, protein function annotation <ref type="bibr" target="#b15">(Gligorijevi? et al. 2021;</ref><ref type="bibr" target="#b29">Meier et al. 2021;</ref><ref type="bibr">Zhang et al. 2022b</ref>), protein-protein docking <ref type="bibr" target="#b6">(Corso et al. 2023;</ref><ref type="bibr">Zhang et al. 2023a</ref>) and protein design <ref type="bibr" target="#b21">(Hsu et al. 2022;</ref><ref type="bibr" target="#b7">Dauparas et al. 2022)</ref>.</p><p>Given the recent strides in the advancement of large pretrained language models for natural languages <ref type="bibr" target="#b40">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b8">Devlin et al. 2019;</ref><ref type="bibr" target="#b2">Brown et al. 2020)</ref>, various categories of language models have been repurposed for protein representation learning. These protein language models (PLMs) consider protein sequences as the essence of life's language, treating individual amino acids as tokens. Self-supervised learning methods are applied to acquire informative protein representations from billions of natural protein sequences. Notable instances include long shortterm memory (LSTM)-based PLMs like UniRep <ref type="bibr" target="#b0">(Alley et al. 2019)</ref>, as well as transformer-based PLMs like ProtTrans <ref type="bibr">(Elnaggar et al. 2021b)</ref>, Ankh <ref type="bibr">(Elnaggar et al. 2023a</ref>) and ESM <ref type="bibr" target="#b34">(Rives et al. 2021;</ref><ref type="bibr" target="#b26">Lin et al. 2023)</ref>. While these methods exhibit substantial potential in protein function prediction tasks <ref type="bibr" target="#b33">(Rao et al. 2019;</ref><ref type="bibr" target="#b48">Xu et al. 2022)</ref>, their direct application to tasks involving structural inputs, such as protein structure assessment and protein-protein interaction prediction, presents challenges.</p><p>Inspired by advancements in protein structure prediction tools <ref type="bibr" target="#b24">(Jumper et al. 2021;</ref><ref type="bibr" target="#b26">Lin et al. 2023</ref>) and the critical role of protein structures in determining functionality, another strand of methods focuses on acquiring protein representations based on 3D structures. These approaches model proteins as graphs, with atoms or amino acids serving as nodes and edges indicating spatial adjacency. Subsequently, 3D graph neural networks (GNNs) facilitate message propagation to capture interactions between residues, enabling the extraction of representations invariant to structural translation and rotation. Typical examples include GearNet <ref type="bibr">(Zhang et al. 2022b)</ref>, GVP <ref type="bibr" target="#b23">(Jing et al. 2021)</ref>, CDConv <ref type="bibr" target="#b13">(Fan et al. 2023)</ref>. Additionally, efforts have been made to design pre-training strategies that leverage unlabeled protein structures from PDB <ref type="bibr" target="#b2">(Berman et al. 2000)</ref> and the AlphaFold Database <ref type="bibr" target="#b39">(Varadi et al. 2021</ref>). These methods rely on self-supervised learning techniques such as contrastive learning <ref type="bibr">(Zhang et al. 2022b;</ref><ref type="bibr">Chen et al. 2023b</ref>), self-prediction <ref type="bibr">(Zhang et al. 2022b)</ref>, and denoising <ref type="bibr" target="#b16">(Guo et al. 2022;</ref><ref type="bibr">Zhang et al. 2023b)</ref>, enabling structure encoders to achieve top-tier performance on tasks related to protein structure, even pre-trained on a relatively small set of unlabeled proteins. Nonetheless, these structurebased approaches still suffer from the limited number of available structures compared with PLMs, raising questions about their ability to surpass sequence-based methods.</p><p>In order to understand how to combine the advantages of both worlds, we conduct a comprehensive investigation into joint protein representation learning. Our study combines a state-of-the-art PLM (ESM-2) with three distinct structure encoders <ref type="bibr">(GVP, GearNet, and CDConv)</ref>. We introduce three fusion strategies-serial, parallel, and cross fusion-to combine sequence and structure representations. We further explore six diverse pre-training techniques, employing the optimal model from the aforementioned choices and leveraging pre-training on the AlphaFold Database. Our findings indicate that:</p><p>1. Serial fusion, a straightforward approach, proves remarkably effective, outperforming the other two fusion strategies across most tasks. 2. Adapting a reduced learning rate for PLMs is crucial to safeguard their representations from disruption. 3. Despite GearNet's relative performance lag behind the other encoders, it demonstrates superior results after integration with PLMs. 4. The two pre-training methods leveraging both sequence and structure information can yield superior performance compared to other methods relying solely on either sequence or structure information.</p><p>Drawing from these insights, our method achieves significant improvements over existing sequence-and structure-based methods, establishing a new state-of-theart on Enzyme Commission and Gene Ontology annotation tasks. We believe that this work holds practical significance in the adaptation of PLMs with structure-based encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Sequence-based representation learning. Regarding protein sequences as the language of life, models from the rapidly developing field of NLP are widely used in modeling protein sequence data. Examples include the CNN-based models <ref type="bibr" target="#b35">(Shanehsazzadeh, Belanger, and Dohan 2020)</ref>, LSTM-based models <ref type="bibr" target="#b33">(Rao et al. 2019)</ref>, ResNet <ref type="bibr" target="#b33">(Rao et al. 2019</ref>) and transformer-based models <ref type="bibr">(Elnaggar et al. 2021b;</ref><ref type="bibr" target="#b34">Rives et al. 2021;</ref><ref type="bibr" target="#b26">Lin et al. 2023;</ref><ref type="bibr">Zhang et al. 2022a;</ref><ref type="bibr">Xu et al. 2023b;</ref><ref type="bibr" target="#b30">Notin et al. 2022;</ref><ref type="bibr" target="#b28">Madani et al. 2023;</ref><ref type="bibr">Chen et al. 2023a)</ref>. Given the rising number of protein sequences and the substantial cost of labeling their functions, representation learning is typically conducted in a self-supervised manner to leverage the extensive protein sequence datasets, via autoregressive modeling <ref type="bibr" target="#b30">(Notin et al. 2022;</ref><ref type="bibr" target="#b28">Madani et al. 2023;</ref><ref type="bibr" target="#b20">Hesslow et al. 2022;</ref><ref type="bibr">Elnaggar et al. 2021a</ref><ref type="bibr">Elnaggar et al. , 2023a))</ref>, masked language modeling (MLM) <ref type="bibr">(Elnaggar et al. 2021b;</ref><ref type="bibr" target="#b34">Rives et al. 2021;</ref><ref type="bibr" target="#b26">Lin et al. 2023)</ref>, pairwise MLM <ref type="bibr" target="#b17">(He et al. 2021)</ref>, contrastive learning <ref type="bibr" target="#b27">(Lu et al. 2020)</ref>, etc. PLMs have shown impressive performance on capturing underlying patterns of sequences, thus predicting protein structures <ref type="bibr" target="#b26">(Lin et al. 2023</ref>) and functionality <ref type="bibr" target="#b33">(Rao et al. 2019;</ref><ref type="bibr" target="#b48">Xu et al. 2022;</ref><ref type="bibr">Chen et al. 2023a</ref>). However, these existing PLMs cannot explicitly encode protein structures, which are actually determinants of diverse protein functions. In this work, we seek to overcome this limitation by enhancing a PLM with a protein structure encoder so as to capture detailed protein structural characteristics.</p><p>Structure-based representation learning. Diverse types of protein structure encoders have been devised to capture different granularities of protein structures, including residue-level structures <ref type="bibr" target="#b15">(Gligorijevi? et al. 2021;</ref><ref type="bibr">Zhang et al. 2022b;</ref><ref type="bibr">Xu et al. 2023a;</ref><ref type="bibr" target="#b23">Jing et al. 2021;</ref><ref type="bibr" target="#b21">Hsu et al. 2022;</ref><ref type="bibr" target="#b7">Dauparas et al. 2022</ref>), atom-level structures <ref type="bibr" target="#b23">(Jing et al. 2021;</ref><ref type="bibr" target="#b20">Hermosilla et al. 2021</ref>) and protein surfaces <ref type="bibr" target="#b14">(Gainza et al. 2020;</ref><ref type="bibr" target="#b36">Sverrisson et al. 2021)</ref>. These structure encoders have boosted protein function understanding <ref type="bibr" target="#b15">(Gligorijevi? et al. 2021;</ref><ref type="bibr">Zhang et al. 2022b</ref>), protein design <ref type="bibr" target="#b23">(Jing et al. 2021;</ref><ref type="bibr" target="#b21">Hsu et al. 2022;</ref><ref type="bibr" target="#b7">Dauparas et al. 2022;</ref><ref type="bibr" target="#b14">Gao, Tan, and Li 2023)</ref> and protein structure generation <ref type="bibr">(Wu et al. 2022b;</ref><ref type="bibr" target="#b38">Trippe et al. 2023)</ref>. Various self-supervised learning algorithms are designed to learn informative protein structure representations, including contrastive learning <ref type="bibr">(Zhang et al. 2022b;</ref><ref type="bibr">Hermosilla and Ropinski 2022)</ref>, self-prediction <ref type="bibr">(Zhang et al. 2022b;</ref><ref type="bibr">Chen et al. 2023b)</ref>, denoising score matching <ref type="bibr" target="#b16">(Guo et al. 2022;</ref><ref type="bibr">Wu et al. 2022a</ref>) and structure-sequence multimodal diffusion <ref type="bibr">(Zhang et al. 2023b)</ref>. Structurally pre-trained models outperform PLMs on function prediction tasks <ref type="bibr">(Zhang et al. 2022a;</ref><ref type="bibr">Hermosilla and Ropinski 2022)</ref>, given the principle that protein structures are the determinants of their functions.</p><p>Joint representation learning. The integration of protein sequence-based models with protein structure models remains unexplored. Early attempts like LM-GVP sought to combine PLMs with structure encoders <ref type="bibr">(Wang et al. 2022a)</ref>. While recent methods have been introduced, their outcomes have not consistently surpassed those of singlemodality models <ref type="bibr" target="#b22">(Huang et al. 2023;</ref><ref type="bibr" target="#b18">Heinzinger et al. 2023)</ref>. In this study, we introduce three novel fusion methods aimed at harnessing the bimodal information of both sequence and structure. Different from earlier approaches, we emphasize the potential benefits of leveraging bimodal data. This is achieved by incorporating sequential information into distinct residue-level encoders-GearNet, GVP, and CDConv <ref type="bibr">(Zhang et al. 2022b;</ref><ref type="bibr" target="#b23">Jing et al. 2021;</ref><ref type="bibr" target="#b13">Fan et al. 2023)</ref>. Furthermore, we enhance the effectiveness of the proposed sequence-structure hybrid encoder, ESM-GearNet, through structure-based pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we describe basic concepts of proteins and sequence-and structure-based protein representation learning methods. Next, we propose three different strategies for combining sequence and structure representations. Finally, we present how different pre-training algorithms can be applied on the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proteins</head><p>Proteins are large molecules composed of residues, a.k.a. amino acids, linked together in chains. Despite there being only 20 standard residue types, their numerous combinations contribute to the immense diversity of proteins found in nature. The specific arrangement of these residues determines the 3D positions of all the atoms within the protein, forming what we call the protein's structure. A residue includes elements like an amino group, a carboxylic acid group, and a side chain group that defines its type. These components connect to a central carbon atom known as the alpha carbon. For simplicity in our work, we use only the alpha carbon atoms to represent the main backbone structure of each protein. Each protein can be represented as a sequence-structure pair P = (R, X ), where R = [r 1 , r 2 , ? ? ? , r n ] denotes the sequence of the protein with r i ? {1, ..., 20} indicating the type of the i-th residue, and X = [x 1 , x 2 ..., x n ] ? R n?3 denotes its structure with x i representing the Cartesian coordinates of the i-th alpha carbon atom, and n denotes the number of residues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-Based Protein Representation Learning</head><p>Treating protein sequences as the language of life, recent works draw inspirations from large pre-trained language models to learn the evolutionary information from billions of protein sequences via self-supervised learning. In this work, we illustrate our method using the transformerbased Protein Language Model (PLM) ESM <ref type="bibr" target="#b34">(Rives et al. 2021;</ref><ref type="bibr" target="#b26">Lin et al. 2023</ref>). These models process residue type sequences through multiple self-attention layers and feedforward networks to capture inter-residue dependencies. Specifically, we represent the hidden state of the ith residue as h (l) i , initialized with the residue's type embedding h</p><formula xml:id="formula_0">(0) i = Embedding(r i ) ? R d ,</formula><p>where d denotes the hidden representation dimension. Self-attention layers compute attention coefficients ? ij , measuring residue contact strength between i and j. The output representations are further fed into forward networks.</p><formula xml:id="formula_1">? (l) ij = Softmax j ( 1 ? d Linear q (h (l) i ) ? Linear k (h (l) j )) h (l+0.5) i = h (l) i + j ? (l) ij ? Linear v (h (l) j ) h (l+1) i = h (l+0.5) i + FeedForward(h (l+0.5) i )</formula><p>In practice, positional embeddings, multi-head attention and layer norm layers are incorporated, enhancing the modeling process (details omitted here).</p><p>These models are pre-trained with masked language modeling (MLM) loss by predicting the type of a masked residue given the surrounding context. An additional linear head employs the final-layer representations h (L) for the prediction. The loss function for each sequence is</p><formula xml:id="formula_2">L M LM = E M [ i?M -log p(r i |r /M )],</formula><p>where a random set of indices M is chosen for masking, replacing the true token at each index i with a mask token. For each masked token, the loss aims to minimize the negative log likelihood of the true residue r i given the masked sequence r /M as context. By fully utilizing massive unlabeled data, these models have achieved state-of-the-art performance on various protein understanding tasks <ref type="bibr" target="#b26">(Lin et al. 2023;</ref><ref type="bibr">Elnaggar et al. 2023b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure-Based Protein Representation Learning</head><p>The achievements of AlphaFold2 <ref type="bibr" target="#b24">(Jumper et al. 2021</ref>) have revolutionized precise protein structure prediction, triggering a wave of research on structure-driven pretraining <ref type="bibr">(Zhang et al. 2022b;</ref><ref type="bibr">Chen et al. 2023b;</ref><ref type="bibr">Zhang et al. 2023b</ref>) due to the direct influence of structures on protein functionalities. Given a protein, structure-based techniques often establish a graph incorporating both sequential and spatial details, leveraging graph neural networks to learn representations. In this work, we focus on three commonly used protein structure encoder: GearNet, GVP and CDConv.</p><p>GearNet <ref type="bibr">(Zhang et al. 2022b</ref>) GearNet represents proteins using a multi-relational residue graph G = (V, E, R), where V and E denote the sets of residues and edges, respectively, and R represents the edge types. Three directed edge types are incorporated into the graph: sequential edges, radius edges, and K-nn edges. Specifically,</p><formula xml:id="formula_3">E (seq) = {(i, j)|i, j ? V, |j -i| &lt; d seq }, E (radius) = {(i, j)|i, j ? V, |x j -x i | &lt; d radius }, E (knn) = {(i, j)|i, j ? V, j ? knn(i)}, E = E (seq) ? E (radius) ? E (knn) ,</formula><p>where d seq = 3 defines the sequential distance threshold, d radius = 10 ? defines the spatial distance threshold, and knn(i) indicates the K-nearest neighbors of node i with k = 10. For sequential edges, edges with different sequential distances are treated as different types. These edge types collectively reflect distinct geometric attributes, contributing to a holistic featurization of proteins. Upon constructing the graph, a relational message passing procedure is conducted. We denote u (l) as the representations at the l-th layer, initialized with u (0) i = Embedding(r i ). The message passing process can be written as:</p><formula xml:id="formula_4">u (l) i = u (l-1) i + ? r?R W r j?Nr(i) u (l-1) j ,</formula><p>where N r (i) is the set of neighbors of i with edge type r, and ?(?) is the ReLU function.</p><p>GVP <ref type="bibr" target="#b23">(Jing et al. 2021)</ref> The GVP module replaces standard MLPs in GNN aggregation and feed-forward layers, operating on scalar and geometric features-features that transform as vectors with spatial coordinate rotations A radius graph is constructed with E = E (radius) where d radius = 10 ?. A radius graph, E = E (radius) with d radius = 10 ?, is constructed. Node features begin as u (0) i = (Embedding(r i ), 0), while edge features are e(j, i) = (rbf(x j -x i ), x j -x i ), using rbf(?) for pairwise distance features. For message functions, the GVP network concatenates node and edge features, applying the GVP module for message passing on the (scalar, vector) representations. A feed-forward network follows each message passing layer. Formally,</p><formula xml:id="formula_5">u (l+0.5) i = u (l) i + 1 |N (i)| j?N (i) GVP([u (l) j , e (j,i) ]), u (l+1) i = u (l+0.5) i + GVP(u (l+0.5) i ) , where u (l) i ? R d ? R d ? ?3</formula><p>denotes hidden representation tuples at the l-th layer, and N (i) is neighbors of i. GVP(?) is the proposed module maintaining SE(3)-invariance of scalar features and SE(3)-equivariance of vector features. The scalar features at the last layer of each node are utilized for property prediction to keep SE(3)-invariance. CDConv <ref type="bibr" target="#b13">(Fan et al. 2023</ref>) CDConv adopts GearNet's concept of multi-type message passing to capture sequential and spatial interactions among residues. Instead of using distinct kernel matrices for varied edge types, CDConv employs an MLP to parameterize kernel matrices, relying on relative spatial and sequential information between two residues. With the same initialization as GearNet, the message passing procedure is written as</p><formula xml:id="formula_6">u (l) i = u (l-1) i + ? j?N (i) W (x j -x i , j -i) u (l-1) j</formula><p>, where W (?, ?) represents an MLP that takes relative positions in Euclidean space and sequences as input, producing the kernel matrix as output. The edge set is the intersection of sequential and spatial edges E = E (seq) ? E (radius) with d seq = 11. To reduce node counts and expand reception field, a half pooling approach is employed every two CDConv layers, merging adjacent nodes. For the i-th CDConv layer, the radius is set to ?i/2 + 1?d radius , and the output dimension is set to ?i/2 + 1?d, where d radius = 4 ? and d denote the initial radius and initial hidden dimensions, respectively. Due to the pooling scheme, CDConv cannot yield residue-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing Sequence and Structure Representations</head><p>While protein language models implicitly capture structural contact information, explicitly incorporating detailed structures can effectively model spatial interactions among residues. Huge-scale pre-training of PLMs also significantly bolsters relatively small protein structure encoders. In this subsection, we propose fusing representations from protein language models and protein structure encoders, presenting three fusion strategies illustrated in Figure <ref type="figure" target="#fig_0">1:</ref> 1. Serial fusion. Rather than initializing structure encoder input node features with residue type embeddings, we initialize them with PLM outputs, denoted as u (0) = h (L) , and utilize the structure encoder's output as the final protein representations, z = u L . This approach provides more powerful residue type representations incorporating sequential context. 2. Parallel fusion. We concatenate outputs of sequence encoders and structure encoders for final representations, yielding z = [h (L) , u (L) ]. This fusion method combines both representations while keeping the structure encoder from affecting pre-trained sequence representations. 3. Cross fusion. To enhance interaction, we introduce a cross-attention layer over sequence and structure representations as</p><formula xml:id="formula_7">z i = SelfAttn([h (L) i , u<label>(L)</label></formula><p>i ]). The attention layer's output is averaged over the protein to produce final representations z.</p><p>Ultimately, the resulting representation is employed for residue-level or protein-level predictions.</p><p>Reduced learning rate of PLMs Given that structure encoders start from random initialization while PLMs are pre-trained, we've observed practical benefits in utilizing a lower learning rate for PLMs to prevent catastrophic forgetting. In our experiments, we maintain a learning rate ratio of 0.1, a strategy we find crucial for the robust generalization of our proposed fusion approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Pre-Training on Unlabeled Proteins</head><p>The current joint encoder effectively utilizes knowledge acquired from extensive unlabeled protein sequences. Recent strides in accurate protein structure prediction, have provided access to a substantial collection of precise protein structures, such as AlphaFold Database <ref type="bibr" target="#b39">(Varadi et al. 2021)</ref>. Several structure-based pre-training techniques have emerged, including self-prediction <ref type="bibr">(Zhang et al. 2022b</ref>), multiview contrast <ref type="bibr">(Chen et al. 2023b)</ref>, and denoising objectives <ref type="bibr">(Zhang et al. 2023b)</ref>. Taking a step further, we next discuss how to apply these pre-training algorithms upon the joint encoder, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. During pre-training, ESM remains fixed while only the structure encoder is tuned, preserving sequence representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Loss function</head><p>Residue Type Prediction</p><formula xml:id="formula_8">CE(f res (z i ), r i ) Distance Prediction (f dis (z i , z j ) -?x i -x j ? 2 ) 2 Angle Prediction CE(f ang (z i , z j , z k ), bin(?ijk)) Dihedral Prediction CE(f dih (z i , z j , z k , z t ), bin(?ijkt))</formula><p>Table <ref type="table">1</ref>: Self-prediction methods. We use i,j,k,t to denote sampled residue indices. Tasks are associated with respective MLP heads: f res , f dis , f ang , and f dih . CE(?) is the cross entropy loss, and angles are discretized with bin(?).</p><p>Self-prediction methods <ref type="bibr">(Zhang et al. 2022b)</ref>. Based on the recent progress of self-prediction methods in natural language processing <ref type="bibr" target="#b8">(Devlin et al. 2019;</ref><ref type="bibr" target="#b2">Brown et al. 2020)</ref>, these methods aim to predict one part of the protein given the remaining context. Four self-supervised tasks are introduced, guided by geometric attributes. These methods perform masked prediction on individual residues, residue pairs, triplets, and quadruples, subsequently predicting residue types, distances, angles, and dihedrals, respectively. The corresponding loss functions are summarized in Table <ref type="table">1</ref>.</p><p>Multiview contrastive learning <ref type="bibr">(Zhang et al. 2022b</ref>). The frameworks aim to maintain similarity between correlated protein subcomponents after mapping to a lower-dimensional latent space. For a protein graph G, we utilize subsequence cropping to randomly select consecutive subsequences. This scheme captures protein domains-recurring consecutive subsequences in different proteins that signify functions <ref type="bibr" target="#b31">(Ponting and Russell 2002)</ref>. After subsequence sampling, following common selfsupervised learning practice <ref type="bibr" target="#b5">(Chen et al. 2020)</ref>, we employ a noise function for diverse views, specifically random edge masking that hides 15% of edges in the protein graph. We align their representations in the latent space with an InfoNCE loss <ref type="bibr" target="#b5">(Chen et al. 2020)</ref>. Let x, y represent subcomponent graphs from the same protein, and k from other proteins within the same batch, with corresponding representations z x , z y , z k . The loss function is written as</p><formula xml:id="formula_9">L x,y = -log exp(sim(g(z x ), g(z y ))/? ) 2B k=1 1 [k? =x] exp(sim(g(z y ), g(z k ))/? )</formula><p>, where g(?) denotes an MLP applied to latent representations, B, ? denote batch size and temperature, and 1[k ? = x] ? {0, 1} acts as an indicator function that equals 1 iff k ? = x. Diffusion-based pre-training <ref type="bibr">(Zhang et al. 2023b</ref>). These methods are inspired by the success of diffusion models in capturing the joint distribution of sequences and structures. During pre-training, noise levels t ? {1, .., T } are sampled and applied to structures and sequences, where higher levels indicate larger noise. The encoder's representations are used for denoising with loss functions:</p><formula xml:id="formula_10">L struct = E t?{1,..,T } E ??N (0,I) ?? -f noise (z (t) , x (t) )? 2 2 , L seq = E t?{1,..,T } i CE r i , f res (z (t) i ) ,</formula><p>with f noise , f res as denoising networks. SiamDiff enhances this diffusion-based pre-training by generating correlated conformers via torsional perturbation and performing mutual denoising between two diffusion trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Setup</head><p>In this section, we evaluate the effectiveness of our proposed methods on function annotation and structure property Table <ref type="table">2</ref>: Evaluation results on EC, GO, PSR and MSP under various fusion schemes and structure encoders. "PLM" and "Struct. Info." indicate the usage of protein language models and structural information in the model, respectively. We employ underlining to highlight the best outcomes within each block and use bold symbols to highlight the best results for each task. prediction tasks in Atom3D <ref type="bibr" target="#b37">(Townshend et al. 2021)</ref>. Four key downstream tasks are considered:</p><p>1. Enzyme Commission (EC) Number Prediction: This task involves predicting EC numbers that describe a protein's catalytic behavior in biochemical reactions. It's formulated as 538 binary classification problems based on the third and fourth levels of the EC tree <ref type="bibr" target="#b43">(Webb et al. 1992</ref>). We use dataset splits from <ref type="bibr" target="#b15">Gligorijevi? et al. (2021)</ref> and test on sequences with up to 95% identity cutoff.</p><p>2. Gene Ontology (GO) Term Prediction: This benchmark includes three tasks: predicting a protein's biological process (BP), molecular function (MF), and cellular component (CC). Each task is framed as multiple binary classification problems based on GO term annotations. We employ dataset splits from <ref type="bibr" target="#b15">Gligorijevi? et al. (2021)</ref> with a 95% sequence identity cutoff.</p><p>3. Protein Structure Ranking (PSR): This task involves predicting global distance test scores for structure predictions submitted to the Critical Assessment of Structure Prediction (CASP) <ref type="bibr" target="#b25">(Kryshtafovych et al. 2019</ref>). The dataset is partitioned by competition year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mutation Stability Prediction (MSP):</head><p>The goal is to predict if a mutation enhances a protein complex's stability. The dataset is divided based on a 30% sequence identity.</p><p>We employ the AlphaFold protein structure database v1 <ref type="bibr" target="#b39">(Varadi et al. 2021)</ref> for pre-training, following <ref type="bibr">(Zhang et al. 2022b</ref>). This dataset encompasses 365K proteomewide predictions from AlphaFold2. For evaluation, we report the protein-centric maximum F-score (F max ) for EC and GO prediction-common metrics in CAFA challenges <ref type="bibr" target="#b32">(Radivojac et al. 2013)</ref>. Additionally, we present global Spearman correlation for PSR and AUROC for MSP.</p><p>Training. Considering the model capacity and computational budget, we selected ESM-2-650M as the base PLM. For structure encoders, we follow the original paper's default settings: 6 layers of GearNet with 512 hidden dimensions, 8 layers of CDConv with an initial hidden dimension of 256, and 5 layers of GVP with scalar features at 256 and vector features at 16 dimensions. We perform 50 epochs of pre-training on the AlphaFold Database, following the hyperparameters in <ref type="bibr">(Zhang et al. 2022b)</ref>. Pre-training employs a batch size of 256 and a learning rate of 2e-4. For downstream evaluation, we utilize Adam optimizer with a batch size of 2 and a learning rate of 1e-4. These models are implemented using the TorchDrug library <ref type="bibr" target="#b52">(Zhu et al. 2022</ref>) and trained across 4 A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Evaluation of different fusion methods We evaluate our methods across the four tasks, employing the fusion of three structure encoders with ESM-2-650M using three distinct fusion strategies. The results are presented in Table <ref type="table">2</ref>. To provide context, we also include the outcomes of the structure encoders, along with two protein language models: ESM-2-650M and ProtBERT-BFD <ref type="bibr">(Elnaggar et al. 2021b)</ref>.</p><p>First, upon intra-block result comparisons, it is evident that serial fusion, while simple in concept, is remarkably effective, surpassing the other two fusion strategies in the majority of tasks. The sole exceptions are ESM-CDConv on GO-MF and MSP, where the CDConv features yield marginal enhancements to PLMs for both fusion schemes.</p><p>Next, through inter-block result comparisons, we observe that while the raw performance of vanilla GearNet lags behind other encoders like CDConv, its integration with PLMs yields better outcomes, particularly for tasks like EC, GO-BP, and GO-MF. This underscores the efficacy of augmenting protein language representations onto structure encoders. Notably, ESM-GVP's enhanced performance in PSR highlights the importance of model capcity in capturing structural details for such tasks. Furthermore, upon comparing ESM-GearNet with ESM-2-650M, we observe substantial enhancements attributed to the incorporation of structural representations. This also enables it to effectively address structure-related tasks.</p><p>Effects of diminished learning rate To investigate the impact of reduced learning rates on representation fusion, we conducted experiments on EC and GO-MF using ESM-GearNet (serial fusion). We set different learning rate ratios for structure encoders relative to PLMs: 1, 0.1, and 0 (fixed). As illustrated in Figure <ref type="figure">3</ref>, the results consistently show that keeping PLMs fixed leads to inferior performance compared to fine-tuning them. When using equal learning rates for both PLMs and structure encoders, a notable performance drop occurs during GO-MF training, indicating significant deterioration of the PLM representations. This underscores the significance of employing reduced learning rates for PLMs to safeguard their representations from degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of different pre-training methods</head><p>We choose the best-performing model, ESM-GearNet (serial fusion), and pre-train it with the introduced six algorithms. The results are shown in Table <ref type="table" target="#tab_2">4</ref>. Notably, the top two pre-training methods are Multiview Contrast and SiamDiff. The former significantly enhances function annotation tasks such as EC, GO-BP, and GO-MF, while the latter Comparison with state-of-the-art To showcase the robust performance of our proposed approaches, we compare them with previously established state-of-the-art methods, namely PromptProtein <ref type="bibr">(Wang et al. 2022b</ref>) for EC and GO, and GVP <ref type="bibr" target="#b23">(Jing et al. 2021)</ref> for PSR and MSP tasks. Our method demonstrates superior performance across most function annotation tasks, with the exception of GO-CC. This outcome could be attributed to the fact that GO-CC pertains to predicting the cellular component of a protein's function, which may be less directly related to the protein's primary function. Additionally, our approach yields competitive results in the PSR and MSP tasks, aligning with the previous state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This study presents a comprehensive exploration of joint protein representation learning, effectively fusing protein language models (PLMs) and structure encoders to harness the strengths of both domains. The integration of ESM-2 with diverse structure encoders, alongside the introduction of innovative fusion strategies, has yielded valuable insights into effective joint representation learning. Our findings highlight the mutually beneficial relationship between sequence and structure information during pre-training, emphasizing the importance of a holistic approach. By achieving new state-of-the-art results in tasks such as Enzyme Commission number and Gene Ontology term annotation, our work not only advances the adaptation of PLMs and structure encoders but also holds broader implications for protein representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three different ways to fuse sequence and structure representations. (a) Serial fusion, where sequence representations are used as residue features in structure encoders. (b) Parallel fusion, involving the concatenation of sequence and structure representations. (c) Cross fusion, where sequence and structure representations are combined via multi-head self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pre-training ESM-GearNet on AlphaFold Database with six different methods: residue type prediction, angle prediction, distance prediction, dihedral prediction, multiview contrastive learning and SiamDiff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Protein language models do not take structures as input and thus cannot handle structure-related tasks like PSR and MSP. 2 Since CDConv does not yield residue-level representations, we cannot use cross fusion for ESM-CDConv.</figDesc><table><row><cell>Method</cell><cell>PLM</cell><cell>Struct. Info.</cell><cell>EC F max</cell><cell>GO-BP F max</cell><cell>GO-MF F max</cell><cell>GO-CC F max</cell><cell>PSR Global ?</cell><cell>MSP AUROC</cell></row><row><cell>ProtBERT-BFD 1</cell><cell></cell><cell></cell><cell>0.838</cell><cell>0.279</cell><cell>0.456</cell><cell>0.408</cell><cell>-</cell><cell>-</cell></row><row><cell>ESM-2-650M 1</cell><cell></cell><cell></cell><cell>0.880</cell><cell>0.460</cell><cell>0.661</cell><cell>0.445</cell><cell>-</cell><cell>-</cell></row><row><cell>GearNet</cell><cell></cell><cell></cell><cell>0.730</cell><cell>0.356</cell><cell>0.503</cell><cell>0.414</cell><cell>0.708</cell><cell>0.549</cell></row><row><cell>ESM-GearNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-w/ serial fusion</cell><cell></cell><cell></cell><cell>0.890</cell><cell>0.488</cell><cell>0.681</cell><cell>0.464</cell><cell>0.829</cell><cell>0.685</cell></row><row><cell>-w/ parallel fusion</cell><cell></cell><cell></cell><cell>0.792</cell><cell>0.384</cell><cell>0.573</cell><cell>0.407</cell><cell>0.760</cell><cell>0.644</cell></row><row><cell>-w/ cross fusion</cell><cell></cell><cell></cell><cell>0.884</cell><cell>0.470</cell><cell>0.660</cell><cell>0.462</cell><cell>0.747</cell><cell>0.408</cell></row><row><cell>GVP</cell><cell></cell><cell></cell><cell>0.489</cell><cell>0.326</cell><cell>0.426</cell><cell>0.420</cell><cell>0.726</cell><cell>0.664</cell></row><row><cell>ESM-GVP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-w/ serial fusion</cell><cell></cell><cell></cell><cell>0.881</cell><cell>0.473</cell><cell>0.668</cell><cell>0.485</cell><cell>0.866</cell><cell>0.617</cell></row><row><cell>-w/ parallel fusion</cell><cell></cell><cell></cell><cell>0.872</cell><cell>0.446</cell><cell>0.657</cell><cell>0.455</cell><cell>0.702</cell><cell>0.592</cell></row><row><cell>-w/ cross fusion</cell><cell></cell><cell></cell><cell>0.880</cell><cell>0.465</cell><cell>0.664</cell><cell>0.469</cell><cell>0.764</cell><cell>0.583</cell></row><row><cell>CDConv</cell><cell></cell><cell></cell><cell>0.820</cell><cell>0.453</cell><cell>0.654</cell><cell>0.479</cell><cell>0.786</cell><cell>0.529</cell></row><row><cell>ESM-CDConv 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-w/ serial fusion</cell><cell></cell><cell></cell><cell>0.880</cell><cell>0.465</cell><cell>0.658</cell><cell>0.475</cell><cell>0.851</cell><cell>0.566</cell></row><row><cell>-w/ parallel fusion</cell><cell></cell><cell></cell><cell>0.879</cell><cell>0.448</cell><cell>0.662</cell><cell>0.455</cell><cell>0.803</cell><cell>0.602</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results of ESM-GearNet (serial fusion) pre-trained with six algorithms. The second and third column mark whether the pre-training methods include sequence and structure objectives, respectively. The best results are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Sequence Objective</cell><cell>Structure Objective</cell><cell>EC F max</cell><cell>GO-BP F max</cell><cell>GO-MF F max</cell><cell>GO-CC F max</cell><cell>PSR Global ?</cell><cell>MSP AUROC</cell></row><row><cell></cell><cell></cell><cell cols="3">ESM-GearNet (serial fusion)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>0.890</cell><cell>0.488</cell><cell>0.681</cell><cell>0.464</cell><cell>0.829</cell><cell>0.685</cell></row><row><cell></cell><cell></cell><cell cols="3">-w/ Residue Type Prediction</cell><cell></cell><cell></cell><cell></cell><cell>0.892</cell><cell>0.507</cell><cell>0.680</cell><cell>0.484</cell><cell>0.832</cell><cell>0.680</cell></row><row><cell></cell><cell></cell><cell cols="2">-w/ Distance Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.891</cell><cell>0.498</cell><cell>0.680</cell><cell>0.485</cell><cell>0.856</cell><cell>0.615</cell></row><row><cell></cell><cell></cell><cell cols="2">-w/ Angle Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.887</cell><cell>0.504</cell><cell>0.679</cell><cell>0.481</cell><cell>0.851</cell><cell>0.702</cell></row><row><cell></cell><cell></cell><cell cols="2">-w/ Dihedral Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.891</cell><cell>0.499</cell><cell>0.680</cell><cell>0.502</cell><cell>0.845</cell><cell>0.515</cell></row><row><cell></cell><cell></cell><cell cols="2">-w/ Multiview Contrast</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.896</cell><cell>0.514</cell><cell>0.683</cell><cell>0.497</cell><cell>0.853</cell><cell>0.599</cell></row><row><cell></cell><cell></cell><cell cols="2">-w/ SiamDiff</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.897</cell><cell>0.500</cell><cell>0.682</cell><cell>0.505</cell><cell>0.863</cell><cell>0.692</cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell>Fmax on EC</cell><cell>0.60 0.65 0.70 0.75 0.80</cell><cell>0</cell><cell>20 Epoch lr_ratio=1 40 lr_ratio=0.1 lr_ratio=0</cell><cell>Fmax on GO-MF</cell><cell>0</cell><cell cols="2">20 Epoch lr_ratio=1 40 lr_ratio=0.1 lr_ratio=0</cell><cell>0.2 0.3 0.4 0.5</cell></row><row><cell cols="9">Figure 3: ESM-GearNet (serial fusion) results on EC and</cell></row><row><cell cols="7">GO-MF with different learning rate ratios.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Comparison between our methods with the state-ofthe-art (SOTA) methods on benchmark tasks. in capturing structural intricacies for the GO-CC and PSR tasks. SiamDiff's superiority can be attributed to its incorporation of both sequential and structural pre-training objectives. In contrast to methods that directly use either sequential or structural objectives, Multiview Contrast offers a more comprehensive consideration of sequence and structure dependencies. By aligning representations from subsequences derived from the same protein, Multiview Contrast captures co-occurring sequences and structural motif dependencies. This utilization of ESM-2 and GearNet representations proves advantageous for function prediction.</figDesc><table><row><cell>Method</cell><cell>EC</cell><cell>GO-BP</cell><cell>GO-MF</cell><cell>GO-CC</cell><cell>PSR</cell><cell>MSP</cell></row><row><cell></cell><cell>Fmax</cell><cell>Fmax</cell><cell>Fmax</cell><cell>Fmax</cell><cell>Global ?</cell><cell>AUROC</cell></row><row><cell>SOTA</cell><cell>0.888</cell><cell>0.495</cell><cell>0.677</cell><cell>0.551</cell><cell>0.862</cell><cell>0.709</cell></row><row><cell cols="2">ESM-GearNet 0.890</cell><cell>0.488</cell><cell>0.681</cell><cell>0.464</cell><cell>0.829</cell><cell>0.685</cell></row><row><cell>w/ pre-training</cell><cell>0.897</cell><cell>0.514</cell><cell>0.683</cell><cell>0.505</cell><cell>0.863</cell><cell>0.702</cell></row></table><note><p>excels</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This project is supported by <rs type="programName">AIHN IBM-MILA partnership program</rs>, the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> <rs type="grantName">Discovery Grant</rs>, the <rs type="funder">Canada CIFAR AI Chair Program</rs>, collaboration grants between <rs type="funder">Microsoft Research and Mila</rs>, <rs type="institution">Samsung Electronics Co., Ltd.</rs>, <rs type="funder">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</rs>, a <rs type="funder">NRC Collaborative R&amp;D Project</rs> (<rs type="grantNumber">AI4D-CORE-06</rs>) as well as the <rs type="programName">IVADO Fundamental Research Project</rs> grant <rs type="grantNumber">PRF-2019-3583139727</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EXsvfhM">
					<orgName type="grant-name">Discovery Grant</orgName>
					<orgName type="program" subtype="full">AIHN IBM-MILA partnership program</orgName>
				</org>
				<org type="funding" xml:id="_ApBjCFK">
					<idno type="grant-number">AI4D-CORE-06</idno>
					<orgName type="program" subtype="full">IVADO Fundamental Research Project</orgName>
				</org>
				<org type="funding" xml:id="_bvvy2xh">
					<idno type="grant-number">PRF-2019-3583139727</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kinch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">6557</biblScope>
			<biblScope unit="page" from="871" to="876" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000. 2020</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2023" to="2027" />
		</imprint>
	</monogr>
	<note>et al. 2023a. xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein. bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2023b. Structure-aware protein self-supervised learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">189</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<title level="m">DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust deep learning-based protein sequence design using ProteinMPNN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Wicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6615</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2023a. Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Essam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Salah-Eldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rochereau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Essam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Salah-Eldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F O</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rochereau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimized Protein Language Model Unlocks General-Purpose Modelling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2021a. Prottrans: Toward understanding the language of life through self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7112" to="7127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020. 2023</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
		</imprint>
	</monogr>
	<note>PiFold: Toward effective and efficient protein inverse folding</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-based protein function prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vlamakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15527</idno>
		<title level="m">Pre-training Coevolutionary Protein Representation via A Pairwise Masked Language Model</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weissenow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ProstT5: Bilingual Language Model for Protein Sequence and Structure</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15675</idno>
		<title level="m">Contrastive representation learning for 3d protein structures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fackelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kozl?kov?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zanichelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Notin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
	<note>2022 ICML Workshop on Computational Biology</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<title level="m">Learning inverse folding from millions of predicted structures. ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10888</idno>
		<title level="m">Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from Protein Structure with Geometric Vector Perceptrons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-Round XIII</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Topf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1011" to="1020" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evolutionary-scale prediction of atomic-level protein structure with a language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smetanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="issue">6637</biblScope>
			<biblScope unit="page" from="1123" to="1130" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-supervised contrastive learning of protein representations by mutual information maximization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Moses</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>BioRxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large language models generate functional protein sequences across diverse families</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Holton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Olmos</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models enable zero-shot prediction of the effects of mutations on protein function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tranception: protein fitness prediction with autoregressive transformers and inferencetime retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Notin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16990" to="17017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ponting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Russell</surname></persName>
		</author>
		<title level="m">The natural history of protein domains. Annual review of biophysics and biomolecular structure</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="45" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large-scale evaluation of computational protein function prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schnoes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wittkop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Graim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating Protein Transfer Learning with TAPE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Is transfer learning necessary for protein landscape prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shanehsazzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03443</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast end-to-end learning on protein surfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feydy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15272" to="15281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ATOM3D: Tasks on Molecules in Three Dimensions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J L</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>V?gele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Derry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Laloudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AlphaFold Protein Structure Database: massively expanding the structural coverage of proteinsequence space with high-accuracy models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anyango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Natassia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yordanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laydon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">2022a. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Combs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Golovach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Salawu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponnapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-level Protein Structure Pretraining via Prompt Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shuang-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Enzyme nomenclature</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Bienkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Digrazia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sayler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08663</idno>
		<title level="m">Pre-training of Deep Protein Models with Molecular Dynamics Simulations for Drug Binding</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Amini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15611</idno>
		<title level="m">Protein structure generation via folding diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">EurNet: Efficient Multi-Range Relational Modeling of Protein Structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2023 -Machine Learning for Drug Discovery workshop</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="38749" to="38767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">OntoProtein: Protein Pretraining With Gene Ontology Embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>An End-to-End Equivariant Network for Protein-Ligand Docking</orgName>
		</respStmt>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Protein Representation Learning by Geometric Structure Pretraining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>First Workshop on Pre-training</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Pitfalls</forename><surname>Perspectives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Paths ; Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>ICML 2022</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08320</idno>
		<title level="m">TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
