<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ByteGNN: Efficient Graph Neural Network Training at Large Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zheng</surname></persName>
							<email>cgzheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Cheng</surname></persName>
							<email>jcheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhezheng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Wu</surname></persName>
							<email>yifanwu@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changji</forename><surname>Li</surname></persName>
							<email>cjli@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDacne Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Bytegnn</surname></persName>
						</author>
						<title level="a" type="main">ByteGNN: Efficient Graph Neural Network Training at Large Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.14778/3514061.3514069</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have shown excellent performance in a wide range of applications such as recommendation, risk control, and drug discovery. With the increase in the volume of graph data, distributed GNN systems become essential to support efficient GNN training. However, existing distributed GNN training systems suffer from various performance issues including high network communication cost, low CPU utilization, and poor end-to-end performance. In this paper, we propose ByteGNN, which addresses the limitations in existing distributed GNN systems with three key designs: (1) an abstraction of mini-batch graph sampling to support high parallelism, (2) a two-level scheduling strategy to improve resource utilization and to reduce the end-to-end GNN training time, and (3) a graph partitioning algorithm tailored for GNN workloads. Our experiments show that ByteGNN outperforms the state-ofthe-art distributed GNN systems with up to 3.5-23.8 times faster end-to-end execution, 2-6 times higher CPU utilization, and around half of the network communication cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Existing systems for neural network training, such as TensorFlow <ref type="bibr" target="#b3">[5]</ref> and PyTorch <ref type="bibr" target="#b52">[54]</ref>, are designed for training euclidean data such as images, texts and audio data. In recent years, a new type of neural networks, called graph neural networks (GNNs), become popular because of the ubiquity of graph data today such as semantic web graphs, knowledge graphs, social networks, and e-commerce networks. GNNs combine the non-euclidean graph structures with traditional neural networks to extract rich information from graph data for machine learning. Recent research results <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b73">75]</ref> have shown that GNNs achieve significant performance improvements over traditional methods on many important tasks such as node classification, link predication, and graph clustering. GNNs have been applied in a broad range of applications including recommendation systems <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b73">75]</ref>, computer vision <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b56">58]</ref>, natural language processing <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b71">73]</ref>, drug discovery <ref type="bibr" target="#b22">[24]</ref>, and social networks <ref type="bibr" target="#b66">[68]</ref>.</p><p>Although many graph computing systems <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b77">79,</ref><ref type="bibr" target="#b79">81]</ref> have been proposed, they are designed for batch graph analytics workloads such as the computation of PageRank, shortest paths, label propagation and connected components, and thus they lack of operators for neural network training. Thus, dedicated GNN systems have been developed upon neural network training systems (e.g., TensorFlow, PyTorch) for GNN training.</p><p>Among existing GNN systems, most of them are still singlemachine systems, e.g., DGL <ref type="bibr" target="#b64">[66]</ref>, PyTorch Geometric (PyG) <ref type="bibr" target="#b19">[21]</ref>, NeuGraph <ref type="bibr" target="#b46">[48]</ref>, FeatGraph <ref type="bibr" target="#b31">[33]</ref> and Seastar <ref type="bibr" target="#b68">[70]</ref>, which are optimized for training GNN models on a relatively small graph but cannot scale to process large graphs generally available in industry today. Note that for GNNs, a graph not only contains the graph topology information (which is typically used for computations such as PageRank, shortest paths, etc.), but each vertex and edge in the graph also contain a feature vector. Thus, depending on the dimensions of the feature vectors (typically around 100 to hundreds), the size of a graph for GNNs can be easily many times larger than the graph topology processed by existing graph computing systems. For example, for the Ogbn-Papers <ref type="bibr" target="#b30">[32]</ref> graph used in our experiments, a feature vector has 128 dimensions and the size of the features is 4 times larger than the size of the graph topology.</p><p>For GNN training on large graphs, distributed systems such as Euler <ref type="bibr">[1]</ref>, GraphLearn (also called AliGraph) <ref type="bibr" target="#b78">[80]</ref>, AGL <ref type="bibr" target="#b75">[77]</ref> and DistDGL <ref type="bibr" target="#b76">[78]</ref> have been proposed. During the training, these systems collect and aggregate the feature vectors of the ?-hop neighbors in order to compute the feature vector of each vertex, where ? is the number of layers of the GNN model to be trained. However, the ?-hop neighbors of a vertex can be many, especially for a power-law graph, and a large portion of them can be located in remote machines. Thus, fetching all the ?-hop neighbors to a local machine for each vertex (referred to as full mini-batch training) incurs high network communication overheads and memory consumption.</p><p>To address the problem of full mini-batch training, mini-batch sampling training was proposed <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b32">34]</ref>, which works as follows. Distributed GNN training is conducted in iterations and for each iteration, a machine processes a mini-batch of vertices in (1) the sampling phase -for each vertex ? in the mini-batch, the sampler samples a limited number of neighbors of ? in each hop, fetches the sampled remote neighbors to the local machine, and constructs the neighborhood subgraph of ? from its sampled neighbors locally; <ref type="bibr" target="#b0">(2)</ref> the training phasethe trainer trains the model on the neighborhood subgraphs of the vertices in the mini-batch locally. While distributed mini-batch sampling has become the default method for GNN training on a large graph (for which full-batch training and full mini-batch training are not practical), existing distributed GNN training systems suffer from a number of performance problems. One main problem is that sampling can take significantly longer time to complete than training, due to large amounts of random data access and remote data fetching involved in the sampling phase. For example, Figure <ref type="figure" target="#fig_0">1</ref> reports the average sampling time and training time in an epoch of training a 2-layered and 3-layered GraphSAGE model (both supervised and unsupervised) on four machines by GraphLearn <ref type="bibr" target="#b78">[80]</ref> on the Reddit dataset <ref type="bibr" target="#b29">[31]</ref> and Ogbn-Product dataset <ref type="bibr" target="#b30">[32]</ref>, which shows that the sampling phase takes an order of magnitude longer time than the training phase. For example, in the 2-layer supervised GraphSAGE training on Ogbn-Product, GraphLearn's training time is only 2.66s while its sampling time is 24.17s. Under the same setting, the sampling time of DistDGL <ref type="bibr" target="#b76">[78]</ref> is also 4.22x of its training time.</p><p>The imbalance between the sampling and training phases also leads to the under-utilization of computing resources and the problem is worsen if GPUs are used for training (which further widens the gap between the sampling and training time) <ref type="bibr" target="#b57">[59]</ref>. To address this imbalanced computing pattern in mini-batch GNN training, existing systems have attempted to apply neighborhood caching <ref type="bibr" target="#b44">[46]</ref> and fixed size prefetching <ref type="bibr" target="#b76">[78]</ref> to shorten the sampling time. However, it is difficult to set the right hyper-parameters (i.e., cache ratio and prefetching number) for training different GNN models on different graphs. Nextdoor <ref type="bibr" target="#b34">[36]</ref> proposed to sample neighborhood using GPUs, but the GPU memory capacity limits the size of the graph it can handle. Graph partitioning has also been applied to reduce the cost of remote data fetching <ref type="bibr" target="#b78">[80]</ref>. However, existing graph partitioning algorithms are designed for traditional graph workloads (e.g., distributed PageRank) and they do not consider the data access pattern and load balancing in GNN training.</p><p>In this paper, we propose ByteGNN, a distributed GNN training framework to support fast end-to-end GNN training in large graphs. To improve the efficiency of sampling, we abstract the sampling phase of a mini-batch as a directed acyclic graph (DAG) of small tasks, so that we can run DAGs and tasks within each DAG in parallel. The fine-grained task abstraction in DAG modeling also leads to the design of a two-level scheduling. First, coarse-grained scheduling determines how much resources should be used for minibatch sampling, in order to dynamically adjust the computation loads between the sampling and training phases to avoid resource contention and maximize CPU utilization. Then, fine-grained scheduling decides the execution order of tasks in the DAGs in order to pipeline the sampling outputs to be consumed by the training phase at the right pace. The two scheduling strategies work together to minimize the end-to-end GNN training time. We also propose an effective graph partitioning algorithm tailored for mini-batch graph sampling, which maintains the data locality according to the data access pattern of mini-batch sampling and balances the computation loads in the training, validation and testing stages.</p><p>We implemented ByteGNN based on GraphLearn <ref type="bibr" target="#b2">[4]</ref>. Our performance evaluation shows that ByteGNN achieves significantly higher training throughput and is more scalable than the state-ofthe-art distributed GNN systems. Experimental results show that ByteGNN achieves up to 23.8x speedup over GraphLearn and 3.5x over DistDGL. The results verify that our system designs lead to efficient GNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>We first introduce the necessary background of GNN and briefly discuss sampling-based GNN training. Then, we motivate our work by presenting the limitations of existing systems for large-scale GNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>GNN models are designed to capture the information contained in both the relationship among vertices in a graph and the vertex/edge attributes. The core idea of GNNs is recursively aggregating the neighbor information, including the features of the neighbors and the features of the connecting edges, and then applying feature transformation functions.</p><p>Take the GraphSAGE model <ref type="bibr" target="#b29">[31]</ref> as an example. The training process for one layer of the model can be expressed as follows:</p><formula xml:id="formula_0">? ? N (?) ? ???????? ? ? ( {? ?-1 ? , ?? ? N (?) }),<label>(1)</label></formula><formula xml:id="formula_1">? ? ? ? ? (? ? ? ?????? (? ?-1 ? , ? ? N (?) )),<label>(2)</label></formula><p>where N (?) is the set of neighbors of vertex ?. In this ?-th convolution layer, each vertex ? first uses the AGGREGATE function to collect the feature vectors of ?'s neighbors from the (? -1)-th layer. The aggregation result is then concatenated with ?'s feature vector from the (? -1)-th layer, followed by a dot-product operation with a learnable weight matrix W. The dot-product result is further transformed by a nonlinearity activation function ? such as the sigmoid function, which gives the feature vector of ? for the ?-th layer.</p><p>As the number of layers increases, the vertices are required to gather and aggregate more and more information from neighbors that are farther away (i.e., expanding from the ?-hop neighbors to the ?-hop neighbors for ? &gt; ?). When the training for all the ? layers completes (for a user-specified ?), the final feature vector ? ? ? for each vertex ? is fed into a mapping function for a specific downstream task (e.g., node classification, link prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributed Mini-Batch Graph Sampling</head><p>Existing distributed GNN systems adopt data parallelism and sampling is commonly applied in order to train a GNN model on a large graph efficiently. However, the sampling process in distributed GNN training is quite different from that in training DNN models for computer vision and natural language processing, for which each sample is independent and small. For GNNs, the distributed training may access the entire neighborhood of a sampled vertex, including both vertices and edges along with their feature vectors. Due to the structural connection among vertices in different partitions, the data access pattern usually leads to high network communication cost.</p><p>In a ?-layered GNN model, for each sampled seed vertex ?, we need to obtain the ?-hop neighborhood of ? to construct a neighborhood subgraph to update ?'s feature vector. As most real-world graphs have a power-law degree distribution, the size of the ?-hop neighborhood subgraph of a vertex grows exponentially as the number of hops increases. To address this problem, mini-batch neighborhood sampling <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b78">80]</ref> has been used to sample a limited number of neighbors for each sampled seed vertex. Figure <ref type="figure">2</ref> illustrates how mini-batch graph sampling is applied in the training of a 2-layered GNN model. We show the sampled 2-hop neighborhood subgraphs of two seed vertices, ? 1 and ? 2 , where we set the sampling configuration ? 1 = 2 and ? 2 = 2, meaning that a vertex ? first samples at most ? 1 of its 1-hop neighbors, and then each ? of ?'s sampled 1-hop neighbors further samples ? 2 of ?'s neighbors. Remote sampling requests are sent to remote devices to access the ?-hop neighbors that are stored there. After the sampling finishes, the sampled neighbors, along with their attributes (which are used to construct the initial feature vectors), are fetched to the local device of ? 1 (and ? 2 ) to construct its sampled 2-hop neighborhood subgraph, which is then fed into the GNN model to calculate the gradients and update the model parameters.</p><p>Although the tradeoff is a potential loss in the model accuracy, mini-batch graph neighborhood sampling still converges to the required model accuracy. Take the mean aggregator in Graph-SAGE <ref type="bibr" target="#b29">[31]</ref> as an example, using the Monte Carlo estimation, for the layer ?, we obtain:</p><formula xml:id="formula_2">E[? ? N ? (?) ] = E[ 1 | N ? (?) | ?? ??N ? (?) ? ?-1 ? ] = 1 | N (?) | ?? ??N (?) ? ?-1 ? = ? ? N (?) ,</formula><p>where N ? (?) is the set of random sampled neighbors of vertex ? and |N ? (?)| = ? ? , which is the fanout of layer ?. Unfortunately, though ? ? N ? (?) is an unbiased estimator of ? ? N (?) , ? ? ? ? is not an unbiased estimator of ? ? ? due to the non-linearity of ? (?) in Equation (2) <ref type="bibr" target="#b12">[14]</ref>. Thus, the gradient is biased and the convergence of SGD is not guaranteed, unless the fanout ? ? goes to infinity. But in practice, GraphSAGE sets ? 1 = 25 and ? 2 = 10 to provide statistically significant gains over existing approaches <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b54">56]</ref>. In addition, AGL <ref type="bibr" target="#b75">[77]</ref> also reported that with a suitable sample size, the sample can well approximate the ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Limitations of Existing GNN Systems</head><p>Existing systems for distributed GNN training suffer from the following major limitations.</p><p>(1) The overhead of network communication is large. As the sampling procedure shows in Section 2.2, for every sampled seed vertex ? in each iteration, we need to construct ?'s ?-hop neighborhood subgraph together with the feature vectors of the vertices and edges in the subgraph. For example, in a 3-hop neighborhood subgraph where the sampling configuration is set at ? 1 = 15, ? 2 = 10 and ? 3 = 5 in the Reddit dataset <ref type="bibr" target="#b29">[31]</ref>, there are 915 vertices each with a 602-dimension feature vector, which are what we need to prepare for one sampled seed vertex. As many of the neighbors and their features may be stored in remote machines, the ?-hop neighborhood subgraph construction incurs a high network communication overhead. Figure <ref type="figure" target="#fig_14">13(a)</ref> shows that the number of remote vertices is about six times that of local vertices with the widely used Hash partitioning. In fact, existing graph partitioning algorithms only consider to reduce the inter-partition edges, but do not consider the data access pattern and load balancing of graph sampling in GNN training. This calls for a new design of a more effective graph partitioning strategy tailored for GNN training.</p><p>(2) CPU utilization is low. Our performance profiling shows that existing distributed GNN systems had poor CPU utilization as shown in Figure <ref type="figure" target="#fig_1">3</ref>. By analyzing their system designs, we list the main causes to their low CPU utilization below.</p><p>The sampling phase of GraphLearn <ref type="bibr" target="#b78">[80]</ref> is handled using the Gremlin semantics <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3]</ref> to express each sampling step. For each step, the Gremlin statement is translated by a parser and converted into several execution operations. An operation is a minimum execution unit in GraphLearn. GraphLearn has low CPU utilization since all the graph sampling operations within each device do not overlap with each other. DistDGL <ref type="bibr" target="#b76">[78]</ref> takes a similar approach but also has many optimizations such as replicating the neighbors of its local vertices.</p><p>Euler [1], on the other hand, wraps each graph operator into one TensorFlow dataflow operator. This design is convenient for users to build the whole computation graph in TensorFlow. However, as all the sampling operators and the training operators are contained in one big computation dataflow graph, existing deep learning systems (including TensorFlow) cannot process it efficiently. It is difficult to have the optimal execution order for the dataflow graph with the newly defined graph sampling operators, which is totally different from the normal tensor computation. Besides, due to the convergence requirement, TensorFlow only runs one dataflow graph at a time. Each iteration always starts graph sampling after the previous training process finishes. This design also eliminates the opportunities to apply the data prefetching mechanism to the independent sampling stages.</p><p>(3) GPU does not bring enough benefit for GNN training in large graphs. As mentioned in Section 1, distributed GNN training on large graphs consists of the sampling phase and training phase. Due to limited GPU memory capacity, graph data are stored in the host memory of the machines and thus the sampling phase is conducted by CPUs. When GPUs are used to conduct the training phase, the sampling results are loaded into GPU memory from CPU host memory via PCIe links. As shown in <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b57">59]</ref>, even in the case of single-machine GNN training using GPU (i.e., data are not fetched through network), the sampling and data loading time still take a significant portion of the end-to-end training time. The training phase can indeed be accelerated using GPUs (compared with using CPUs), but this only reduces the model updating time while the sampling phase still dominates the overall processing cost. This is because most GNN models are considerably small (unlike DNN models) and the training phase only needs to conduct model computation on densely packed vectors, while sampling a large graph involves large amounts of random data access and remote data fetching in order to construct the neighborhood subgraph for each sampled seed vertex.</p><p>We evaluated the performance of DistDGL <ref type="bibr" target="#b76">[78]</ref> on a GPU server (40 cores Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 256 GB Memory, and one Nvidia RTX 2080Ti graphics card). We tested DistDGL with different fanout and hidden sizes to show the influence from the workloads of sampling and training. As Figure <ref type="figure" target="#fig_2">4</ref> shows, the largest difference in epoch time is only 10% between using GPU and using CPU. The average GPU utilization for GNN training is only around 20%, which is consistent with the GPU utilization of DGL reported in <ref type="bibr" target="#b44">[46]</ref>. In fact, even if we purely use CPUs for the training phase, the sampling phase still dominates the overall cost as we have shown in Figure <ref type="figure" target="#fig_0">1</ref>. In addition, we also need to consider the operational costs. GPU servers are expensive and GPU quota is more restricted to training DNNs even in big companies like ByteDance. In the cloud environment, Dorylus <ref type="bibr" target="#b59">[61]</ref> also shows that   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Worker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Worker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphStore</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Worker S-Worker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-Worker T-Worker T-Worker T-Worker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PS PS ? ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Worker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Abstraction of Mini-Batch Graph Sampling</head><p>The sampling process in existing GNN systems <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b78">80]</ref> is not well-organized as the tasks in each sampling phase are executed without overlapping, which often leads to CPU under-utilization.</p><p>In addition, sampling is conducted for one iteration (i.e., one minibatch) after another, even though different mini-batches are independent of each other. To support parallel sampling within a mini-batch and among mini-batches, so as to maximize CPU utilization, we model the sampling process as a DAG of tasks. Then, we can execute the DAGs of sampling multiple mini-batches in parallel. We also introduce a scheduler in Section 3.2 to effectively utilize the computing resources for both intra-DAG and inter-DAG parallelization, while balancing the loads between sampling and training so that one is not waiting for the other to finish in order to continue.</p><p>To construct this DAG for a general GNN model, we analyzed the sampling phase of a broad range of existing GNN models, i.e., those that follow a similar neighborhood aggregation as described in Section 2.1, which cover most of the widely-adopted models such as GCN <ref type="bibr" target="#b41">[43]</ref>, GAT <ref type="bibr" target="#b62">[64]</ref>, GraphSAGE <ref type="bibr" target="#b29">[31]</ref>, PinSAGE <ref type="bibr" target="#b73">[75]</ref>, and Graph-SAINT <ref type="bibr" target="#b74">[76]</ref>. We provide a common abstraction for the sampling phase of these GNN models with a set of five operators: (1) Seed Sampler: sampling a set of vertices as seeds from the local graph store; (2) Positive Sampler: sampling vertices from the direct neighbors of each seed; (3) Negative Sampler: sampling vertices from those that are not the direct neighbors of each seed; (4) Neighborhood Subgraph Construction (NSC): sampling vertices from the multi-hop neighborhood of a given vertex and constructing the sampled neighborhood subgraph; (5) Feature Fetching: fetching the attributes of a given vertex/edge to construct its feature vector.</p><p>With the above five operators, we can present the workflow of the sampling phase as a DAG, as shown in Figure <ref type="figure">6</ref>. The DAG on the left of Figure <ref type="figure">6</ref> models supervised training, which consists of three tasks: Seed Sampling, NSC, and Feature Fetching. For unsupervised training, we also need to construct the neighborhood subgraphs of each positively and negatively sampled vertices of the seed vertices, as shown in the DAG on the right of Figure <ref type="figure">6</ref>. The three branches in the DAG for unsupervised training can be executed in parallel, and the results are then collected in the "End" node to be fed into a T-Worker for training.</p><p>To enable higher parallelism for both supervised and unsupervised training, we create an instance of the two dominating operations (i.e., NSC and Feature Fetching, as they access multi-hop neighbors and their attributes) for each sampled vertex and execute these instances in parallel. In addition, as NSC (along with Feature Fetching) is executed repeatedly for each hop of neighborhood expansion, we can break the multi-hop operations into many smaller tasks of one-hop operations. As shown in Figure <ref type="figure">6</ref>, each small task of Feature Fetching can start immediately when the corresponding small NSC task finishes. The more fine-grained task abstraction results in higher parallelism and better resource utilization (e.g., less head-of-line blocking and stragglers, less fragmentation in resource utilization).</p><p>To construct a DAG, users only need to specify the customized sampling functions in Seed Sampler, Positive Sampler, Negative Sampler, and also in NSC (e.g., how and how many neighbors in each hop should be sampled). This design also leaves space for researchers and engineers to explore new, high-quality sampling strategies using the framework. Note that the logical DAG is created only once and physical instances are generated and executed for each mini-batch by the S-Workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-Level Scheduling</head><p>ByteGNN adopts a two-level scheduling strategy to improve CPU utilization and reduce the end-to-end GNN training time. Although many scheduling strategies have been proposed, they are mostly for job scheduling at the cluster level <ref type="bibr">[17, 19, 20, 25, 28-30, 35, 40, 47, 57, 60, 63, 74]</ref> or heterogeneous jobs/tasks in dataflow systems <ref type="bibr" target="#b37">[39]</ref>, which are over-complicated and incur extra overheads for scheduling the simple tasks in our system (note that for the training of a GNN model, we only need to schedule instances of the same DAG instead of many different DAGs).</p><p>Coarse-grained scheduling. The S-Worker in each machine executes multiple DAGs in parallel to increase throughput and reduce the end-to-end GNN training time. The first question we need to answer is how many DAGs should be launched in a machine. If we launch too many DAGs, which means more resource is needed by sampling, then resource contention becomes a problem. Resource contention does not just occur among the DAGs, but also between sampling (i.e., DAG execution) and training (i.e., model computation). The training time increases significantly when too many DAGs are launched. On the other hand, if too few DAGs are running, the resource is under-utilized. The training phase finishes quickly and the next iteration's training waits for the neighborhood subgraphs to be produced by the DAGs.</p><p>To control the resource utilization, we need to decide when to launch a DAG. We can model this problem as a variation of the classical Job-Shop Scheduling Problem (JSP) <ref type="bibr" target="#b5">[7]</ref>. Each DAG can be regarded as a job, where a set of operations (tasks) in each job need to be processed in a specific order, and we have a set of jobs that are to be processed on a given set of workers. Knowing the best timing for DAG launching is equivalent to getting the earliest starting time of each job in the solution to this special Job-Shop Scheduling Problem. The Job-Shop Scheduling Problem has been well studied and to find a schedule that minimizes the makespan or minimizes the sum of the job completion time was proved to be strongly NP-hard <ref type="bibr" target="#b9">[11]</ref>. Some new research also shows that the currently best approximation algorithms have worse than logarithmic performance guarantee <ref type="bibr" target="#b24">[26]</ref>.</p><p>We propose a heuristic strategy to decide when to launch a DAG based on three runtime measures: ? util , ? size , and ? gap .</p><p>? util is the CPU utilization rate. If ? util is low, we may launch a new DAG; otherwise, we may wait until ? util drops to a suitable level. Note that high ? util does not necessarily result in better performance because there could be much contention and switching among DAGs and between sampling and training.</p><p>In addition to CPU utilization, We also need to consider the memory footprint. The neighborhood subgraph constructed from each DAG execution is kept in the DAG output queue in the S-Worker and ? size is the size of this queue. The neighborhood subgraphs are then consumed by the T-Worker for training. Thus, ? size is essentially an indicator of the speed of production (by the S-Worker) and the speed of consumption (by the T-Worker) of the neighborhood subgraphs. If ? size is small, we may launch new DAGs; otherwise, we pause the launching. If ? size is large, it implies an over-supply of neighborhood subgraphs and we may shift more computing resource from sampling to accelerate training. Thus, ? size not only controls the memory usage, but also balances the overall resource usage between sampling and training.</p><p>We also found that the real-time measure for ? util is not sensitive enough since newly launched DAGs may not change the CPU utilization in a short time period and many DAGs may be launched during the period. Later, when the tasks in these DAGs start to run in parallel and use up the computing resource, the system suffers from severe resource contention. To avoid such delayed performance punishments, we introduce ? gap , which is the time gap elapsed since the previous DAG launch. If ? gap is too small, we may want to wait for a bit longer before we launch a new DAG.</p><p>It would be undesirable if users need to set the thresholds for the three measures, as it is hard to determine what values of ? util , ? size , and ? gap are good and how to relate them to each other. To this end, we integrate them into one single score, launch-score, to decide whether we should launch a new DAG. The idea is to maintain the balance between the production speed and the consumption speed of neighborhood subgraphs, while keeping CPU utilization high. Ideally, we hope that the output of each DAG will be consumed immediately by the training phase, which means that ? size should be close to 0 all the time. However, in most of the cases a very low ? size happens with a very low ? util . Thus, we need to consider ? size together with ? util .</p><p>Algorithm 1 shows the algorithm for coarse-grained scheduling.</p><p>First, we want to maintain ??????? = ? avg_sample ? avg_train * ? size = 1, where ? avg_sample and ? avg_train are the average time for sampling and training a mini-batch. If balance &gt; 1, it means that it would take less time to consume the current ? size sampling results than to produce a new sampling result, which is an indicator that a new DAG should be launched. Next, we first attempt to use (100 -? util ) to give a higher weight to balance if ? util is low and penalize balance (i.e., delay new DAG launching) when ? util is high. However, simply using (100 -? util ) does not work well as it is a linear scale. Instead, we want to quickly increase CPU utilization when ? util is low and prevent contention promptly when ? util is already very high. Thus, we use an exponential function, ? (? util ) = 101 -? ? util /? , where ? = 100</p><p>??101 is a constant used to align the range of ? (? util ) with that of ? util , i.e., ? (0) = 100, ? (100) = 0, and 0 ? ? (? util ) ? 100. Finally, we also put ? gap as a weight to reflect the delay in the real-time measurement of ? util , which leads to the definition of launch-score in Algorithm 1.</p><p>We monitor launch-score in real time and launch a new DAG when launch-score ? ?, where ? is a threshold set as follows. As shown in Algorithm 1 and explained above, launch-score connects ???????, ? (? util ) and ? ??? together to determine whether a new DAG job should be launched. In practice, there exist reasonable values of ???????, ? (? util ) and ? ??? for which a new DAG should be launched; Note that there are always trade-offs between ??????? and ? (? util ), e.g., a higher ??????? and a lower ? (? util ), to achieve a high launch-score. Such tradeoffs in runtime allows the system to automatically adjust the resource allocation to balance the sampling and training progress.</p><p>Fine-grained scheduling. After new DAGs are launched, the S-Worker executes the tasks in the DAGs, in parallel with the tasks in other DAGs. These tasks are put in a queue when their dependency is cleared (i.e., their parent tasks in the DAG are completed) and are handled by a pool of processing threads. If we execute the tasks in an FIFO order, some tasks of newly launched DAGs could be in front of the tasks in those almost-finished DAGs. For example, when the ??? 1 pushes the "END" node in the task queue and there are already "NSC" tasks from ??? 2 and ??? 3 in the queue, the "NSC" tasks will be executed first and the "END" task will be processed later even although the "END" task is the last task in ??? 1 , completing which will immediately return the sampled data to the T-Worker for training. Meanwhile, one task may unlock a lot of downstream tasks in the same DAG, and heavy tasks may block many light tasks. Thus, the average completion time of the DAG jobs and hence the end-to-end GNN training time can be significantly increased.</p><p>We schedule tasks according to the following orders: (1) tasks in a DAG with a smaller ID will be executed first; (2) tasks in the same DAG will be executed in ascending order of their costs. We assign a smaller ID to a DAG launched earlier to prioritize earlier DAGs to be completed first. We calculate the cost of a task by the data it needs to handle. For example, for sampling tasks in each hop of NSC, the cost is equal to the total number of neighbors of the input vertices; for Feature Fetching, the cost is the number of vertices/edges to be fetched multiplied by the vertex/edge feature dimension. As tasks may require data from remote machines, the S-Worker sends data fetching requests to the local Graph Store, which communicates with remote Graph Stores to fetch the data. The remote requests are also scheduled in a similar way and the network operations are processed concurrently with the CPU operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNN-based Graph Partitioning</head><p>Existing graph partitioning algorithms <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b44">46]</ref> are mainly designed to reduce inter-partition edges and balance the workload. They have been widely adopted in distributed graph processing systems <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b79">81]</ref> to reduce inter-machine communication. However, sample-based GNN training focuses on the ?-hop neighborhood of only the vertices in the training, validation and test sets (instead of all vertices). For example, in Figure <ref type="figure" target="#fig_5">7</ref>, traditional partitioning strategies cut the graph into two parts by the left dotted line since it not only balances the vertices but also has the least cut edge. But for a 2-layer GNN training, since vertex ? and vertex ? are the labeled vertices, partitioning by the right dotted line is actually a better choice. Even if this results in two cut edges, it would not cause any data movement in the training process as only the 2-hop neighbors of the labeled vertices are required.</p><p>In addition, the ratio of the sizes of the training, validation, and test sets of different real-world graphs may differ significantly. For example, in the Ogbn-Product dataset, the test set size is 11 times the training set size and 56 times the validation set size; while in the Ogbn-Papers dataset, the test set size is only 0.18 times the training set size and 1.7 times the validation set size. Thus, the partitioning algorithm should consider both the special data access pattern of ?-layer GNN training and the balanced distribution of the training, validation, and test sets.</p><p>It is known that the traditional graph partitioning problem is proved to be APX-hard <ref type="bibr" target="#b4">[6]</ref>. Thus, our graph partitioning problem is also APX-hard as it can be reduced to the traditional graph partitioning problem. We propose a heuristic two-step graph partitioning strategy tailored for GNN sampling workloads. The main idea is to group vertices into multi-hop neighborhood-based blocks and Step (1) neighborhood block construction. To better preserve the locality of graph data for GNN sampling workloads, we construct a neighborhood block for each vertex in the training, validation and test sets. We start a ?-hop breadth-first search from each vertex ? (? is called the block center) and broadcast the unique block ID of ? to its ?-hop neighbors being visited. Every vertex only keeps the first block ID it receives, except for block centers which keep their own block ID. A block is then formed of all the vertices that keep the same block ID. Figure <ref type="figure" target="#fig_5">7</ref> demonstrates how to construct the blocks.</p><p>Step (2) block assignment. Just as existing graph partitioning algorithms aim to balance the number of vertices in the partitions, our objective is to also balance the number of training, validation and test vertices in the partitions so that the work of training, validation and test is also balanced among the machines. Algorithm 2 shows how to assign the blocks. For each block ? ? , it is assigned to the partition with the highest score. ? ? is the set of vertices that have already been assigned to partition ?. ?? [ ?] is the number of cross-edges between ? ? and ? ? , which will be eliminated if ? ? is assigned to ? ? . Thus, the larger ?? [ ?] is, the more likely ? ? is assigned to ? ? . As the size of different partitions may vary during the assignment, we normalize means that more training vertices can be assigned to partition ?.</p><p>The above applies to the validation and test vertices as well. In addition, we also use a weight to put more attention on a specific type of vertices according to the scale of that type in order to obtain a better overall performance. For example, if the number of training vertices is significantly more, we may set a larger ? to favor the training process, which can improve the end-to-end processing time.</p><p>Before the block assignment, we sort the blocks in descending order of max{|? (train)|, |? (val)|, |? (test)|}. Then, we start the block assignment according to this order. In this way, larger blocks are assigned to different partitions first, so that smaller blocks may be used later to fill the partitions more easily when the partitions begin to fill up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM IMPLEMENTATION</head><p>We implemented ByteGNN based on GraphLearn <ref type="bibr" target="#b2">[4]</ref>, using Tensor-Flow <ref type="bibr" target="#b3">[5]</ref> as the backend deep learning framework for the training phase. We used the data loader and distributed graph storage in GraphLearn, where the graph topology data is stored in adjacency list format and the features are stored separately and indexed by their vertex/edge ID. Our implementation focuses on efficient DAG construction and execution, graph partitioning, and gradient synchronization.</p><p>DAG construction and execution. We adopt the Gremlin syntax to help us construct the DAG. We redesigned the parsing method to encode necessary metadata from a Gremlin query for generating DAG nodes. Since one Gremlin statement may become several nodes in the final DAG, we implemented the parsing phase to carefully handle the complex dependency among the task nodes. We also changed all the communication methods from synchronous in GraphLearn to asynchronous in ByteGNN.</p><p>Graph partitioning. We implemented our graph partitioning strategy on the streaming graph partitioning framework in <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b44">46]</ref>. The random start seed vertices in <ref type="bibr" target="#b10">[12]</ref> were replaced with labeled vertices/edges. The framework first does the multi-source distributed BFS to build the ?-hop neighborhood blocks, and then applies our block assignment strategy in Section 3.3 to assign blocks to the partitions. The partitions are written into HDFS and then loaded by the system for sampling and training.</p><p>Gradient synchronization. To address the potential convergence issue, we implemented the bulk synchronous parallel (BSP) and stale synchronous parallel (SSP) models based on the Tensor-Flow API, so that users may also choose to use BSP or SSP to obtain faster model convergence and reduce the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM EVALUATION</head><p>We evaluate the performance of ByteGNN by comparing with Graph-Learn <ref type="bibr" target="#b2">[4]</ref>, Euler [1] and Distributed DGL (DistDGL) <ref type="bibr" target="#b76">[78]</ref>. We also examine the effects of our system designs on the performance.</p><p>Testbed. We ran our experiments on a cluster of machines where each machine is equipped with 1T DDR4 main memory and two 2.40GHz Intel(R) Xeon(R) Platinum 8260 CPU (each CPU has 24 cores or 48 virtual cores by hyper-threading). All the machines are connected by a 25Gbps network and the OS is the Debian 9.13 with Linux kernel 4.19.117.</p><p>Datasets. We used three datasets in the evaluation, as shown in Table <ref type="table" target="#tab_2">1</ref>. ????-??????? and ????-?????? are the largest two graphs in the Open Graph Benchmark (OGB). ????-??????? is an undirected and unweighted graph modeling an Amazon product copurchasing network <ref type="bibr" target="#b30">[32]</ref>. ????-?????? is a directed citation graph of 111 million papers indexed by MAG <ref type="bibr" target="#b63">[65]</ref>. The ?????? dataset is a directed graph in industry from the social network scenario.</p><p>Models. We used three representative GNN models, Graph Convolutional Network (GCN) <ref type="bibr" target="#b41">[43]</ref>, GraphSAGE <ref type="bibr" target="#b29">[31]</ref> and Graph Attention network(GAT) <ref type="bibr" target="#b62">[64]</ref>, in our evaluation. In order to demonstrate the expressiveness and efficiency of ByteGNN, we also tested the unsupervised variants of these three models. Although unsupervised learning shares most of the GNN architectures with supervised learning, it involves the negative sampling operator in the sampling phase and is also widely used in important tasks such as link prediction. Since many works are proposed to improve the sampling of GNN models, we used GraphSAINT <ref type="bibr" target="#b74">[76]</ref> as a typical example to show how our sampling abstraction can be applied. As shown by prior works <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b35">37]</ref>, deeper and larger GNN architectures can achieve better model accuracy. We used three network layers for the models and set the sampling configuration to ? 1 = 10, ? 2 = 5 and ? 3 = 3 for the neighborhood sampling models. The mini-batch size was set to 512 in all the experiments.</p><p>Systems. We compared with three distributed GNN training systems, GraphLearn, Euler (v1.0) and DistDGL (DGL v0.5.3). GraphLearn is a distributed framework designed for the development and application of GNNs on large scale graphs within Alibaba. Euler is also developed by Alibaba but it has been used in many companies for large scale GNN training. Both GraphLearn and Euler use Ten-sorFlow as the backend system. DistDGL is a popular GNN system and its latest version (v0.5.3) supports distributed GNN training. The computational patterns of DistDGL are highly optimized by dedicated sparse tensor operations, which are currently lacking in ByteGNN as this work focuses on improving the sampling performance. Unless otherwise stated, we used the default configuration of these systems in our experiments. ByteGNN used the BSP model to obtain better test accuracy. All the systems adopt the random neighborhood sampling method as the default sampling method and use the same hop number and fanout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>We first compared the overall performance of the systems. We report the throughput of each system, i.e., the number of samples being processed per second, which is a metric commonly used to measure the performance of model training of a system. The throughput is calculated as the total number of seed vertices processed divided by the end-to-end GNN training time. Thus, the larger the throughput of a system, the shorter is the end-to-end GNN training time of the system. The hidden size is set to 32 in GCN and GraphSAGE. For GAT, we used 4 attention heads with hidden size 16. Since Euler failed to run unsupervised GAT training, we ignore this result.  Compared with DistDGL, ByteGNN achieves 2.1 ~3.5 times speedup for training the dense graph ????-??????? in both supervised and unsupervised training. For the sparse graphs ????-?????? and ??????, ByteGNN still has better performance. In the supervised training, ByteGNN is 1.5x and 1.3x faster than Dist-DGL in GCN and GraphSAGE. But the speedup is less significant compared with that on the dense graph, especially for GAT. This is because sparse tensor operations in the training phase of Dist-DGL have been highly optimized, while currently there is no such optimization in ByteGNN. For unsupervised training that has heavier sampling workloads, Figures <ref type="figure">8(b</ref>)&amp;(d)&amp;(f) show that ByteGNN achieves considerably better performance as ByteGNN's design enables higher parallelism in sampling execution. (e.g., 2.4x for unsupervised GraphSAGE and 1.6x for unsupervised GAT).</p><p>We also report the average CPU utilization of ByteGNN for training all the models in Figure <ref type="figure" target="#fig_8">9</ref>. The result is reported for ????-??????, while ByteGNN's CPU utilization for the other two datasets is similar. Compared with the average CPU utilization of GraphLearn, Euler and DistDGL as shown in Figure <ref type="figure" target="#fig_1">3</ref>, ByteGNN achieves 3 -6 times higher CPU utilization. ByteGNN has lower CPU utilization for supervised GCN and GraphSAGE because the number of neighborhood subgraphs in the DAG output queue is sufficient, S-Worker dynamically frees up some resource to T-Worker and the training workload for GCN is not heavy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scalability</head><p>Figure <ref type="figure" target="#fig_10">10</ref> reports the throughput scalability of the systems for the ????-?????? dataset, where we increase the number of machines from 4 to 64. ByteGNN achieves better scalability than all the other three systems. We omit the results for the other two datasets due to the page limitation, but the patterns are similar and ByteGNN's performance on the dense graph ????-??????? is even better. The hidden size is set to 256 here to demonstrate the performance of our system in different configuration.</p><p>In general, the throughput performance in distributed GNN training has sub-linear scalability due to the synchronization overhead (when the BSP model is used to achieve high accuracy) and heavy network I/O among the machines. GraphLearn and Euler scale poorly and their throughputs are relatively low. Although GraphLearn and Euler are built on top of TensorFlow, the default asynchronous gradient update in distributed TensorFlow does not  cause much synchronization overhead (though with potential accuracy loss). However, without an effective graph partitioning algorithm to preserve the locality of neighborhood access, remote data fetching results in high network communication overhead. In contrast, DistDGL's main issue in scalability is due to the synchronization overhead for gradient update. If the sampling output of a mini-batch cannot return on time, the trainer will get the forward loss later and all the other machines will wait for this loss to begin the back propagation. Even with the fixed prefetching mechanism, the possibility of the back propagation waiting increases as the number of machines increases. In comparison, ByteGNN's scheduling allows the sampling outputs to be pipelined to the trainers while other sampling processes continue, which results in better resource utilization. ByteGNN's GNN-tailored graph partitioning algorithm also leads to lower network communication overhead as the number of machines increases. As a result, ByteGNN achieves better scalability than the other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Accuracy</head><p>We also report the correctness of ByteGNN by evaluating the test accuracy of the GraphSAGE model on the ????-??????? dataset, comparing with GraphLearn and DistDGL. Euler has similar accuracy as GraphLearn. In Figure <ref type="figure" target="#fig_11">11</ref>, we report the test accuracy of different systems at every epoch until the training converges.</p><p>The result shows that the systems achieve similar or the same accuracy eventually, but ByteGNN converges the fastest, in both the single-machine setting (1M) and distributed 4-machine setting (4M). We also note that as the mini-batch training can update the model many times in one epoch, the accuracy increases quickly in the first several epochs. The single-machine accuracy of GraphLearn can also be seen as the baseline to demonstrate that our code changes to GraphLearn do not affect the semantics of the GNN models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on System Designs</head><p>We further evaluate the effectiveness of each individual system design in ByteGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Sampling Abstraction.</head><p>We used the GraphSAINT model to demonstrate how to build a DAG using our sampling abstraction. Different from sampling neighbors across the layers, GraphSAINT constructs mini-batches by sampling the whole input graph once and then building a full GCN on the sampled subgraph. It provides three light-weight and efficient samplers, NodeSampler, EdgeSampler, and RandomWalkSampler. Due to space constraints, we only show the implementation of the Seed Sampler function in our sampling abstraction for GraphSAINT's NodeSampler and EdgeSampler. Note that the training part is the same for different samplers. . by ( " E d g e W e i g h t " ) . bothV ( ) For GraphSAINT's NodeSampler, we sample ? vertices from all the training vertices according to a vertex probability distribution ? (?) ? || ?:,? || 2 . We call this "InDegree" sampling as it is associated with the in-degree of each vertex. For EdgeSampler, the edge probability distribution follows ? ? (?,?) ? 1 ??? (?) + 1 ??? (?) . Normally, it can be pre-calculated dependent on the graph topology only and become the weight of edges. The code above shows the Seed Sampler function of these two samplers using our sampling abstraction. Using Gremlin, users can easily write the sampling logic. Then, the sampling stage can be completed by the NSC and Feature Fetching functions as discussed in Section 3.1.</p><p>We also implemented the GraphSAINT model in GraphLearn to compare the end-to-end training performance. Table <ref type="table" target="#tab_1">2</ref> reports the speedup ratio of ByteGNN over GraphLearn for training Graph-SAINT on different graphs using four machines, using the same sampling setting from <ref type="bibr" target="#b74">[76]</ref>. Even though GraphSAINT has a light sampling workload, ByteGNN can still achieve significant speedup compared with GraphLearn. Note that ByteGNN has better performance with EdgeSampler because EdgeSampler needs to obtain the two end-vertices of the sampled edge and has a higher workload than NodeSampler.  We first evaluate the performance of the coarse-grained scheduling strategy. We used three different batch sizes: 512, 1024 and 2048. We created a light sampling workload by setting the sampling configuration to ? 1 = 10, ? 2 = 5 and ? 3 = 3. We also created a heavy sampling workload by setting the sampling configuration to ? 1 = 15, ? 2 = 10 and ? 3 = 5.</p><p>We used two baselines. The first baseline is sequential DAG execution, which runs DAGs one after another. The second baseline is running a fixed number of DAGs at any time. The DAG size is set to 16 which is the same as the default in DistDGL prefetching.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows that coarse-grained scheduling achieves the best performance in all cases. For sequential DAG execution, the execution of a single DAG at a time results in resource under-utilization and thus has poor performance. For the light sampling workload, the fixed number of DAGs has performance close to that of coarsegrained scheduling. This is because the sampling workload is light and can be finished quickly so that DAGs can already produce the sampling results fast enough for the trainer to consume. However, the lower sampling rate leads to more biased results and the light workload also results in resource under-utilization. When the sampling workload is heavier, the higher random data access overhead and higher network I/O cost to retrieve remote neighbors become the performance bottleneck. In this case, our coarse-grained scheduling strategy becomes effective as it dynamically adjusts the resource allocation to sampling and training in order to maximize resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.3</head><p>Fine-Grained Scheduling. We further show the impact of the fine-grained scheduling strategy on the DAG completion time. We ran ByteGNN for 10 epochs and measured the completion time of each DAG of mini-batch sampling under two settings: using the priority-based scheduling in the fine-grained scheduling strategy Figure <ref type="figure" target="#fig_13">12</ref> shows that the priority-based scheduling can significantly reduce the completion time of the DAGs, and the DAG completion time is also more stable, which avoids the short time period of under-supply or over-supply of the sampling outputs. In supervised training, when the number of DAGs is in the suitable range, there are not too many small tasks in the DAGs so that the FIFO scheduling can handle it. However, unsupervised training launches more sampling tasks during the DAG execution. The median DAG completion time of the FIFO scheduling is almost two times greater than the median of the priority-based scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Graph Partitioning.</head><p>To validate the effectiveness of our GNNtailored graph partitioning (GNN-P) algorithm, we compared GNN-P with three well-known graph partitioning methods: hash partitioning, Fennel partitioning <ref type="bibr" target="#b60">[62]</ref> and METIS partitioning <ref type="bibr" target="#b39">[41]</ref>. Both hash and METIS partitioning have been widely adopted in distributed graph computing systems. Fennel is the representative of one-pass streaming partitioning algorithms. Figure <ref type="figure" target="#fig_14">13</ref> reports the distribution of the requests for remote and local neighborhood data in one training epoch by each machine (the distributions of the requests for validation and test show a similar pattern). First, Figure <ref type="figure" target="#fig_14">13</ref>(a) shows that hash partitioning achieves the best balanced distribution because hash partitioning assigns each type of vertices to different partitions with the same possibility. However, it does not consider the locality of neighborhood data access and thus incurs much higher remote data requests, which result in high network communication overhead. The number of remote requests is about 6.32 times the local data requests. Although METIS keeps the total number of vertices similar in each partition, the number of training vertices varies significantly among the partitions (also true for validation and test vertices). Half of the training vertices are assigned to one partition in Machine 1, which indeed reduces remote data requests; however, the imbalanced distribution results in Machine 1 being a severe straggler, which processes around 80% of the data requests in each training epoch. Fennel roughly balances the total load in each partition. Fennel considers data locality but it is only limited to direct neighbors, and thus remote data requests still take a major portion of the total number of data requests in each partition. In contrast, the multi-hop block construction of GNN-P significantly improves the data locality of each partition. The ratio of remote data requests and local data requests in the partitions of GNN-P is also considerably smaller than that of the hash and Fennel partitions. In addition, with the balance-aware assignment algorithm, GNN-P achieves a much more balanced distribution of the total workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Single-machine GNN systems. PyG <ref type="bibr" target="#b19">[21]</ref> integrates with Py-Torch <ref type="bibr" target="#b52">[54]</ref> to provide a message-passing API for GNN training. Incorporated with the Apache TVM compiler <ref type="bibr" target="#b13">[15]</ref>, FeatGraph <ref type="bibr" target="#b31">[33]</ref> generates optimized kernels for both CPU and GPU. But to implement new GNN operators, users need to have the background of TVM primitives. NeuGraph <ref type="bibr" target="#b46">[48]</ref> proposes Scatter-ApplyEdge-Gather-ApplyVertex programming model to express GNN models and supports full-batch training in a single machine with multiple GPUs. It divides a graph into 2-D chunks and introduces a streaming scheduler to handle the CPU-GPU data transfer when GNN computation for a graph cannot fit in the GPU memory. Seastar <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b68">70]</ref> proposes a vertex-centric programming model to express GNN models using native Python syntax and identifies a common seastar computation pattern in GNN training to generate high-performance fused kernels. There are also works <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b44">46]</ref> that focus on addressing the bottleneck of mini-batch sampling. PaGraph <ref type="bibr" target="#b44">[46]</ref> is a samplingbased training framework on multi-GPUs that addresses the expensive subgraph data loading issue by a GNN computation-aware cache policy. NextDoor <ref type="bibr" target="#b34">[36]</ref> enables users to express the sampling tasks in GPUs by a high-level API and also proposes a novel transit parallelism approach to parallelize graph sampling. However, these single-machine systems have the limitation of processing large industrial graphs due to limited GPU memory.</p><p>Distributed GNN systems. For training GNNs on large graphs in a distributed manner, AliGraph <ref type="bibr" target="#b78">[80]</ref> provides sampling-based distributed GNN training and reduces network communication by caching vertices on local machines. DistDGL <ref type="bibr" target="#b76">[78]</ref> uses a distributed in-memory key-value store to support efficient access to graph topology and feature data in distributed GNN training. DGCL <ref type="bibr" target="#b8">[10]</ref> proposes an efficient communication library for distributed fullbatch GNN training on multi-GPUs using NVLink. DGCL needs to load all the graph data into GPUs first and is not suitable for training large graphs. Based on FlexFlow <ref type="bibr" target="#b36">[38]</ref>, a distributed DNN training framework, Roc <ref type="bibr" target="#b35">[37]</ref> also adopts full-batch training in multi-GPUs using dynamic programming to minimize data swapping between host DRAM and GPU memory. P3 <ref type="bibr" target="#b21">[23]</ref> reduces communication by model parallelism for the first layer, while it uses data parallelism for the remaining layers. However, if the hidden size is larger than the input dimension, it still incurs a high cost for the synchronization of the output of the first layer. AGL <ref type="bibr" target="#b75">[77]</ref> uses MapReduce to preprocess a graph, which samples multiple neighborhood subgraphs for each vertex and stores them in a distributed file system. During training, AGL loads the required samples of neighborhood subgraphs of the vertices directly from the disk. However, the preprocessing cost is high and the storage overhead can also be large. Dorylus <ref type="bibr" target="#b59">[61]</ref> designs a computation separation mechanism and pipelines the different computation patterns in the Amazon EC2 machine and serverless Lambdas in the cloud environment.</p><p>Graph partitioning in GNN. METIS <ref type="bibr" target="#b39">[41]</ref> is commonly used for graph partitioning in GNN algorithms <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45]</ref> and systems <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b76">78]</ref>. Cluster-GCN <ref type="bibr" target="#b14">[16]</ref> adopts METIS to build small clusters and then uses the partitions to perform an SGD update. DistDGL <ref type="bibr" target="#b76">[78]</ref> adjusts the METIS algorithm to balance the training vertices in each partition. NeuGraph <ref type="bibr" target="#b46">[48]</ref> uses the Kernighan-Lin algorithm to make the chunks in the diagonal have as many edges as possible. Roc <ref type="bibr" target="#b35">[37]</ref> uses an linear-regression based algorithm to achieve balanced partitioning for both GNN training and inference; but it still treats all the vertices equally, which makes the computation load unbalanced. PaGraph <ref type="bibr" target="#b44">[46]</ref> partitions a graph based on the neighborhood of a training vertex. However, with the multi-hop feature cache to avoid feature communication between different trainers, the memory overhead is too high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We presented ByteGNN, a distributed GNN training system for GNN training in large graphs. ByteGNN abstracts the sampling phase of a mini-batch as a DAG of small tasks to support high parallelism. Leveraging the DAG abstraction, ByteGNN designs a two-level scheduling to improve resource utilization and reduce the end-toend GNN training time. ByteGNN also tailors graph partitioning for GNN workloads to reduce network I/O and balance the workload. Experimental results show that ByteGNN can significantly shorten the end-to-end training time compared with existing distributed GNN systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sampling and training time of GraphLearn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: 2-hop mini-batch graph sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The epoch time of DistDGL with different hidden sizes and fanout on the Ogbn-product dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: System architecture of ByteGNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 Figure 6 :</head><label>56</label><figDesc>Figure5shows the architecture of ByteGNN, which consists of four main components in each machine where ByteGNN is deployed. Graph Store stores a partition of the input graph data and the Graph Stores of all machines form a distributed Graph Store. PS is a parameter server that stores the model parameters. Sampling Worker (S-Worker), handles the sampling phase and constructs sampled neighborhood subgraphs for sampled seed vertices. Training Worker (T-Worker), handles the training phase, which computes model gradients on the sampled neighborhood subgraphs constructed by the S-Worker in the same machine and synchronizes the gradients with PS to update the model parameters.In Sections 3.1-3.3 we focus on three key designs in ByteGNN, which address the limitations of existing GNN systems discussed in Section 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 :</head><label>7</label><figDesc>Traditional partitioning vs. GNN partitioning then assign these blocks to partitions by balancing the numbers of training, validation and test vertices in the partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>?? [ ?] by |? ? |. ?? [ ?] is the balancing score that controls the number of training/validation/test vertices in partition ? to be close to the average value. For example, the expected number of training vertices in each partition is ? (train) = |? (train)|/? , where ? (train) is the set of all training vertices and ? is the total number of partitions. Let ? ? (train) be the set of training vertices currently in partition ?. Thus, a smaller |? ? (train) | ? (train)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 Figure 8 :</head><label>88</label><figDesc>Figure 8: The throughput of GraphLearn, Euler, DistDGL, and ByteGNN for training different models on 4 machines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average CPU utilization of ByteGNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Scalability comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Accuracy comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>s e e d _ s a m p l e r ( s e l f ) : 3 r e t u r n s e l f . g . V ( n o d e _ t y p e = " t r a i n " ) 4 . b a t c h ( b a t c h _ s i z e = n ) 5 . by ( " I n D e g r e e " ) 6 7 / / E d g e S a m p l e r 8 d e f s e e d _ s a m p l e r ( s e l f ) : 9 r e t u r n s e l f . g . E ( e d g e _ t y p e = " t r a i n " ) 10 . b a t c h ( b a t c h _ s i z e = m) 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Distribution of DAG completion time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The distribution of remote/local data requests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: The Coarse-Grained Scheduling Strategy Variable: ? util , ? size , ? ??? Given: ?=launch-score while more_dag do //more_dag=1 when more DAGs can be launched</figDesc><table><row><cell>??????? =</cell><cell>? avg_sample ? avg_train  * Q size ;</cell></row><row><cell cols="2">? (? util ) = (101 -? ? ???? /? ), where ? = 100 ln 101 launch-score = ? ??? * ? (? util ) * ???????;</cell><cell>;</cell></row><row><cell cols="2">if launch-score ? ? then</cell></row><row><cell cols="2">more_dag = Launch_DAG();</cell></row><row><cell cols="3">// launches a new DAG; returns 0 when no more DAG to</cell></row><row><cell>launch</cell><cell></cell></row><row><cell cols="3">? ???? _?????? = ? ??? () // used to calculate ? ???</cell></row><row><cell>else</cell><cell></cell></row><row><cell cols="2">sleep(5ms);</cell></row><row><cell>end</cell><cell></cell></row><row><cell>end</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 2 :</head><label>2</label><figDesc>Block Assignment Input: List of Blocks B = ? 1 , ? 2 , ....., ? ? Output: Graph partitions ? 1 , ? 2 , ? 3 , ......, ? ? for each block ? ? in B do for j ? 1 to ? do CE[j] = |Cross_Edge(? ? , ? ? )| / |? ? | ? ? = ? ? ? ? ? end return ? 1 , ? 2 , ? 3 , ......, ? ?</figDesc><table><row><cell cols="2">BS[j] = (1-?  *</cell><cell>|? ? (train) | ? (train) -?  *</cell><cell>|? ? (val) | ? (val) -?  *</cell><cell>|? ? (test) | ? (test) )</cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell></row><row><cell>? = argmax</cell><cell cols="2">{CE[t] * BS[t]}</cell><cell></cell></row><row><cell>1?? ??</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Graph datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">Ogbn-Product Ogbn-Papers</cell><cell>Social</cell></row><row><cell></cell><cell>(Product)</cell><cell>(Papers)</cell><cell></cell></row><row><cell>Vertices</cell><cell>2,449,029</cell><cell>111,059,956</cell><cell>66,351,656</cell></row><row><cell>Edges</cell><cell>123,718,280</cell><cell cols="2">1,615,685,872 1,751,915,191</cell></row><row><cell>Feature</cell><cell>100</cell><cell>128</cell><cell>150</cell></row><row><cell>Classes</cell><cell>47</cell><cell>172</cell><cell>2</cell></row><row><cell>Training set</cell><cell>196,615</cell><cell>1,207,179</cell><cell>6,631,989</cell></row><row><cell>Validation set</cell><cell>39,323</cell><cell>125,265</cell><cell>19,908,461</cell></row><row><cell>Test set</cell><cell>2,213,091</cell><cell>214,338</cell><cell>39,811,206</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>And as ByteGNN uses BSP to ensure model convergence in distributed training, it achieves approximately the same accuracy as DistDGL but uses less time.</figDesc><table><row><cell></cell><cell cols="5">GraphLearn 1M DistDGL 1M ByteGNN 1M</cell><cell></cell></row><row><cell></cell><cell cols="5">GraphLearn 4M DistDGL 4M ByteGNN 4M</cell><cell></cell></row><row><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy( %)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell><cell>2,500</cell><cell>3,000</cell></row><row><cell></cell><cell></cell><cell cols="3">Running Time(s)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Speedup ratio of ByteGNN over GraphLearn</figDesc><table><row><cell>Type</cell><cell>Ogbn-Product</cell><cell>Ogbn-Papers</cell><cell>Social</cell></row><row><cell>NodeSampler</cell><cell>3.40</cell><cell>2.80</cell><cell>4.05</cell></row><row><cell>EdgeSampler</cell><cell>4.72</cell><cell>3.25</cell><cell>5.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The execution time (sec) of one epoch for different sampling settings, running on ????-?????? using 8 machines</figDesc><table><row><cell cols="4">(a) The execution time of light sampling workload</cell></row><row><cell></cell><cell cols="3">Sequential Fixed DAGs Coarse-Grained</cell></row><row><cell>512</cell><cell>79.04</cell><cell>19.56</cell><cell>18.62</cell></row><row><cell>1024</cell><cell>75.29</cell><cell>19.21</cell><cell>17.52</cell></row><row><cell>2048</cell><cell>74.52</cell><cell>19.90</cell><cell>17.75</cell></row><row><cell cols="4">(b) The execution time of heavy sampling workload</cell></row><row><cell></cell><cell cols="3">Sequential Fixed DAGs Coarse-Grained</cell></row><row><cell>512</cell><cell>314.04</cell><cell>63.41</cell><cell>56.70</cell></row><row><cell>1024</cell><cell>312.14</cell><cell>68.72</cell><cell>57.20</cell></row><row><cell>2048</cell><cell>310.43</cell><cell>78.10</cell><cell>62.46</cell></row><row><cell cols="3">5.4.2 Coarse-Grained Scheduling.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>? This work was done when the authors were in ByteDance. The work of Chenguang Zheng, Changji Li and James Cheng was partially supported by a ByteDance Research Collaboration Project.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous VLDB reviewers, for their constructive comments and suggestions that have helped greatly improve the quality of the paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gremlin</title>
		<ptr target="http://tinkerpop.apache.org/gremlin.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://tinkerpop.apache.org/" />
		<title level="m">TinkerPop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GraphLearn</title>
		<ptr target="https://github.com/alibaba/graph-learn" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Gordon</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-11-02">2016. 2016. November 2-4, 2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Balanced graph partitioning</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Andreev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>R?cke</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007912.1007931</idno>
		<ptr target="https://doi.org/10.1145/1007912.1007931" />
	</analytic>
	<monogr>
		<title level="m">SPAA 2004: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and Architectures</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004-06-27">2004. June 27-30, 2004</date>
			<biblScope unit="page" from="120" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Computational Study of the Job-Shop Scheduling Problem</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1287/ijoc.3.2.149</idno>
		<ptr target="https://doi.org/10.1287/ijoc.3.2.149" />
	</analytic>
	<monogr>
		<title level="j">INFORMS J. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="149" to="156" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Giraph: Large-scale graph processing infrastructure on hadoop</title>
		<author>
			<persName><forename type="first">Ching</forename><surname>Avery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Hadoop Summit</title>
		<meeting>the Hadoop Summit<address><addrLine>Santa Clara</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A faster algorithm for betweenness centrality</title>
		<author>
			<persName><forename type="first">Ulrik</forename><surname>Brandes</surname></persName>
		</author>
		<idno type="DOI">10.1080/0022250X.2001.9990249</idno>
		<ptr target="https://doi.org/10.1080/0022250X.2001.9990249" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Mathematical Sociology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="163" to="177" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DGCL: an efficient communication library for distributed GNN training</title>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447786.3456233</idno>
		<ptr target="https://doi.org/10.1145/3447786.3456233" />
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;21: Sixteenth European Conference on Computer Systems, Online Event</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-04-26">2021. April 26-28, 2021</date>
			<biblScope unit="page" from="130" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Review of Machine Scheduling: Complexity, Algorithms and Approximability</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Woeginger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4613-0303-9_25</idno>
		<ptr target="https://doi.org/10.1007/978-1-4613-0303-9_25" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="21" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">G-Miner: an efficient task-oriented graph mining system</title>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3190508.3190545</idno>
		<ptr target="https://doi.org/10.1145/3190508.3190545" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference</title>
		<meeting>the Thirteenth EuroSys Conference<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23">2018. 2018. April 23-26, 2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rytstxWAW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic Training of Graph Convolutional Networks with Variance Reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/chen18p.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi18/presentation/chen" />
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10-08">2018. 2018. October 8-10, 2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330925</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330925" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. 2019. August 4-8, 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="153" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Work-Efficient Parallel GPU Methods for Single-Source Shortest Paths</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2014.45</idno>
		<ptr target="https://doi.org/10.1109/IPDPS.2014.45" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 28th International Parallel and Distributed Processing Symposium</title>
		<meeting><address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-05-19">2014. May 19-23, 2014</date>
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Paragon: QoS-aware scheduling for heterogeneous datacenters</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 18th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quasar: resource-efficient and QoS-aware cluster management</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<ptr target="http://arxiv.org/abs/1903.02428" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MapGraph: A High Level API for Fast Development of High Performance Graph Analytics on GPUs</title>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Personick</surname></persName>
		</author>
		<idno type="DOI">10.1145/2621934.2621936</idno>
		<ptr target="https://doi.org/10.1145/2621934.2621936" />
	</analytic>
	<monogr>
		<title level="m">Second International Workshop on Graph Data Management Experiences and Systems, GRADES 2014, co-loated with SIGMOD/PODS 2014</title>
		<meeting><address><addrLine>Snowbird, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-22">2014. June 22. 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">P3: Distributed Deep Graph Learning at Scale</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand Padmanabha</forename><surname>Iyer</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi21/presentation/gandhi" />
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2021-07-14">2021. July 14-16, 2021</date>
			<biblScope unit="page" from="551" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Utilising Graph Machine Learning within Drug Discovery and Development</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><forename type="middle">R</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyothish</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertrude</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">B R</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Roblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">L</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">P</forename><surname>Taylor-King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05716</idno>
		<ptr target="https://arxiv.org/abs/2012.05716" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Firmament: Fast, centralized cluster scheduling at scale</title>
		<author>
			<persName><forename type="first">Ionel</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Robert Nm Watson</surname></persName>
		</author>
		<author>
			<persName><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="99" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Better Approximation Guarantees for Job-shop Scheduling</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Sweedyk</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=314161.314395" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>New Orleans, Louisiana, USA. ACM/SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-01">1997. January 1997</date>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez" />
	</analytic>
	<monogr>
		<title level="m">10th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Hollywood, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012-10-08">2012. 2012. October 8-10, 2012</date>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-resource packing for cluster schedulers</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM Computer Communication Review</title>
		<meeting>the ACM SIGCOMM Computer Communication Review</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Altruistic scheduling in multi-resource clusters</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="65" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GRAPHENE: Packing and Dependency-Aware Scheduling for Data-Parallel Clusters</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janardhan</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="81" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/5" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/fb60" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
	<note>d411a5c5b72b2e7d3527cfc84fd0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11359</idno>
		<ptr target="https://arxiv.org/abs/2008.11359" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Wen-Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/01" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
	<note>eee509ee2f68dc6014898c309e86bf-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quincy: fair scheduling for distributed computing clusters</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udi</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Symposium on Operating Systems Principles</title>
		<meeting>the 22nd ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerating graph sampling for graph machine learning using GPUs</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jangda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Polisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Serafini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447786.3456244</idno>
		<ptr target="https://doi.org/10.1145/3447786.3456244" />
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;21: Sixteenth European Conference on Computer Systems, Online Event</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-04-26">2021. April 26-28, 2021</date>
			<biblScope unit="page" from="311" to="326" />
		</imprint>
	</monogr>
	<note>United Kingdom</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/book/300.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-03-02">2020. 2020. 2020. March 2-4, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/book/265.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems<address><addrLine>MLSys; Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03-31">2019. 2019. 2019. March 31 -April 2, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving resource utilization by timely fine-grained scheduling</title>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengguang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanxian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3342195.3387551</idno>
		<ptr target="https://doi.org/10.1145/3342195.3387551" />
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;20: Fifteenth EuroSys Conference 2020</title>
		<meeting><address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04-27">2020. April 27-30, 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MIFO: A Query-Semantic Aware Resource Allocation Policy</title>
		<author>
			<persName><forename type="first">Prajakta</forename><surname>Kalmegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivnath</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM International Conference on Management of Data</title>
		<meeting>the 2019 ACM International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1678" to="1695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1137/S1064827595287997</idno>
		<ptr target="https://doi.org/10.1137/S1064827595287997" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CuSha: vertex-centric graph processing on GPUs</title>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laxmi</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600212.2600227</idno>
		<ptr target="https://doi.org/10.1145/2600212.2600227" />
	</analytic>
	<monogr>
		<title level="m">The 23rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC&apos;14</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-06-23">2014. June 23 -27, 2014</date>
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast Haar Transforms for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosheng</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2020.04.028</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2020.04.028" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="188" to="198" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph Neural Network Based Coarse-Grained Mapping Prediction</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geemi</forename><forename type="middle">P</forename><surname>Wellawatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maghesree</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heta</forename><forename type="middle">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>White</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04921</idno>
		<ptr target="https://arxiv.org/abs/2007.04921" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PaGraph: Scaling GNN training on large graphs via computation-aware caching</title>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3419111.3421281</idno>
		<ptr target="https://doi.org/10.1145/3419111.3421281" />
	</analytic>
	<monogr>
		<title level="m">SoCC &apos;20: ACM Symposium on Cloud Computing, Virtual Event</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-21, 2020</date>
			<biblScope unit="page" from="401" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Elasecutor: Elastic Executor Scheduling in Data Analytics Systems</title>
		<author>
			<persName><forename type="first">Libin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="107" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">NeuGraph: Parallel Deep Neural Network Computation on Large Graphs</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc19/presentation/ma" />
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference, USENIX ATC 2019</title>
		<meeting><address><addrLine>Renton, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019-07-10">2019. July 10-12, 2019</date>
			<biblScope unit="page" from="443" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pregel: a system for largescale graph processing</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Aart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naty</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><surname>Czajkowski</surname></persName>
		</author>
		<idno type="DOI">10.1145/1807167.1807184</idno>
		<ptr target="https://doi.org/10.1145/1807167.1807184" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The More You Know: Using Knowledge Graphs for Image Classification</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.10</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.10" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21">2017. 2017. July 21-26, 2017</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable GPU graph traversal</title>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Grimshaw</surname></persName>
		</author>
		<idno type="DOI">10.1145/2145816.2145832</idno>
		<ptr target="https://doi.org/10.1145/2145816.2145832" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP 2012</title>
		<meeting>the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP 2012<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-02-25">2012. February 25-29, 2012</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6960-geometric-matrix-completion-with-recurrent-multi-graph-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017. 2017, 4-9 December 2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Do We Need Specialized Graph Databases? Benchmarking Real-Time Social Networking Applications</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Pacaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Tamer</forename><surname>?zsu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3078447.3078459</idno>
		<ptr target="https://doi.org/10.1145/3078447.3078459" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Workshop on Graph Datamanagement Experiences &amp; Systems, GRADES@SIGMOD/PODS 2017</title>
		<meeting>the Fifth International Workshop on Graph Datamanagement Experiences &amp; Systems, GRADES@SIGMOD/PODS 2017<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-05-14">2017. May 14 -19, 2017</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/bdbca" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. 288fee7f92f2bfa9f7012727740-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186005</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web, WWW 2018<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-23">2018. April 23-27, 2018</date>
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623732</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623732" />
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08-24">2014. August 24 -27, 2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Autopilot: workload autoscaling at Google</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Rzadca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Findeisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacek</forename><surname>Swiderski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Zych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Broniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarek</forename><surname>Kusmierek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beata</forename><surname>Strack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Witusowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3342195.3387524</idno>
		<ptr target="https://doi.org/10.1145/3342195.3387524" />
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;20: Fifteenth EuroSys Conference 2020</title>
		<meeting><address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-04-27">2020. April 27-30, 2020</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estrach</forename><surname>Bruna</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
		<ptr target="https://openreview.net/forum?id=BJj6qGbRW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable Graph Neural Network Training: The Case for Sampling</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Serafini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3469379.3469387</idno>
		<ptr target="https://doi.org/10.1145/3469379.3469387" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ROSE: Cluster Resource Scheduling via Speculative Over-Subscription</title>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Garraghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Wo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 38th International Conference on Distributed Computing Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="949" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads</title>
		<author>
			<persName><forename type="first">John</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eyolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><forename type="middle">Harry</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi21/presentation/thorpe" />
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2021-07-14">2021. July 14-16, 2021</date>
			<biblScope unit="page" from="495" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">FENNEL: streaming graph partitioning for massive scale graphs</title>
		<author>
			<persName><forename type="first">Charalampos</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Gkantsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozidar</forename><surname>Radunovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556195.2556213</idno>
		<ptr target="https://doi.org/10.1145/2556195.2556213" />
	</analytic>
	<monogr>
		<title level="m">Seventh ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-02-24">2014. 2014. February 24-28, 2014</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">TetriSched: global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems</title>
		<meeting>the 11th European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Microsoft Academic Graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
		<idno type="DOI">10.1162/qss_a_00021</idno>
		<ptr target="https://doi.org/10.1162/qss_a_00021" />
	</analytic>
	<monogr>
		<title level="j">Quant. Sci. Stud</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<ptr target="http://arxiv.org/abs/1909.01315" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gunrock: a high-performance graph processing library on the GPU</title>
		<author>
			<persName><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Riffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<idno type="DOI">10.1145/2851141.2851145</idno>
		<ptr target="https://doi.org/10.1145/2851141.2851145" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-03-12">2016. 2016. March 12-16, 2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep Reasoning with Knowledge Graph for Social Relationship Understanding</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/142</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. July 13-19, 2018</date>
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Vertex-Centric Visual Programming for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yidi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3452770</idno>
		<ptr target="https://doi.org/10.1145/3448016.3452770" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data, Virtual Event</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-20">2021. June 20-25, 2021</date>
			<biblScope unit="page" from="2803" to="2807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Seastar: vertex-centric programming for graph neural networks</title>
		<author>
			<persName><forename type="first">Yidi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengguang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447786.3456247</idno>
		<ptr target="https://doi.org/10.1145/3447786.3456247" />
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;21: Sixteenth European Conference on Computer Systems, Online Event</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-04-26">2021. April 26-28, 2021</date>
			<biblScope unit="page" from="359" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Blogel: A Block-Centric Framework for Distributed Computation on Real-World Graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.14778/2733085.2733103</idno>
		<ptr target="https://doi.org/10.14778/2733085.2733103" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1981" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The Thirty-First Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017370</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33017370" />
	</analytic>
	<monogr>
		<title level="m">The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. 2019. 2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
	<note>The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">OpERA: opportunistic and efficient resource allocation in Hadoop YARN by harnessing idle resources</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningfang</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computer Communication and Networks</title>
		<meeting>the 25th International Conference on Computer Communication and Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219890</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. 2018. August 19-23, 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">AGL: A Scalable System for Industrial-purpose Graph Machine Learning</title>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.14778/3415478.3415539</idno>
		<ptr target="https://doi.org/10.14778/3415478.3415539" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3125" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05337</idno>
		<ptr target="https://arxiv.org/abs/2010.05337" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Medusa: Simplified Graph Processing on GPUs</title>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2013.111</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2013.111" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distributed Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">AliGraph: A Comprehensive Graph Neural Network Platform</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352127</idno>
		<ptr target="https://doi.org/10.14778/3352063.3352127" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2094" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Gemini: A Computation-Centric Distributed Graph Processing System</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/zhu" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah</title>
		<meeting><address><addrLine>GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-11-02">2016. November 2-4, 2016</date>
			<biblScope unit="page" from="301" to="316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
