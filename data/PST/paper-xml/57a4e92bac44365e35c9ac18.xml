<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
							<email>philipo@uow.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Wu</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Action Recognition From Depth Maps Using Deep Convolutional Neural Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Research Laboratory</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<postCode>2522</postCode>
									<settlement>Wollongong</settlement>
									<region>N.S.W</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89722C607495758A300C51598FD05439</idno>
					<idno type="DOI">10.1109/THMS.2015.2504550</idno>
					<note type="submission">received May 30, 2015; revised September 15, 2015; accepted November 1, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action recognition</term>
					<term>deep learning</term>
					<term>depth maps</term>
					<term>pseudocolor coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new method, i.e., weighted hierarchical depth motion maps (WHDMM) + three-channel deep convolutional neural networks (3ConvNets), for human action recognition from depth maps on small training datasets. Three strategies are developed to leverage the capability of ConvNets in mining discriminative features for recognition. First, different viewpoints are mimicked by rotating the 3-D points of the captured depth maps. This not only synthesizes more data, but also makes the trained ConvNets view-tolerant. Second, WHDMMs at several temporal scales are constructed to encode the spatiotemporal motion patterns of actions into 2-D spatial structures. The 2-D spatial structures are further enhanced for recognition by converting the WHDMMs into pseudocolor images. Finally, the three ConvNets are initialized with the models obtained from ImageNet and finetuned independently on the color-coded WHDMMs constructed in three orthogonal planes.</p><p>The proposed algorithm was evaluated on the MSRAction3D, MSRAction3DExt, UTKinect-Action, and MSRDailyActivity3D datasets using cross-subject protocols. In addition, the method was evaluated on the large dataset constructed from the above datasets. The proposed method achieved 2-9% better results on most of the individual datasets. Furthermore, the proposed method maintained its performance on the large dataset, whereas the performance of existing methods decreased with the increased number of actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ESEARCH on action recognition has mainly focused on conventional red, green, and blue (RGB) video and handcrafted features. However, there is no universally best handengineered feature for all datasets <ref type="bibr" target="#b0">[1]</ref>. Microsoft Kinect Sensors provide an affordable technology to capture depth maps and RGB images in real time. Compared with traditional images, depth maps offer better geometric cues and less sensitivity to illumination changes for action recognition <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Current approaches to recognizing actions from RGB-D data are still based on hand-crafted features, which are often shallow. In addition, their high-dimensional description of local or global spatiotemporal information and performance vary across datasets.</p><p>Inspired by the multistage processing of the visual cortex <ref type="bibr" target="#b5">[6]</ref>, deep convolutional neural networks (ConvNets) can automatically learn discriminative features from data and have been used on tasks related to image classification, recognition, segmentation, detection, and retrieval <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>. ConvNets employ techniques for scaling up the networks to millions of parameters and learn the parameters from a massive labeled dataset.</p><p>This paper presents a novel method to apply ConvNets trained on ImageNet to depth map sequences for action recognition with a small number of training samples. Generally speaking, Con-vNets require a sufficiently large number of training samples, and how to apply the ConvNets to small datasets is still an unsolved problem. To address this issue, an architecture comprising weighted hierarchical depth motion maps (WHDMM) and three-channel convolutional neural network (3ConvNets) is proposed. WHDMM is a strategy for transforming the problem of action recognition to image classification and making effective use of the rich information offered by the depth maps. Specifically, 3-D pointclouds constructed from the original depth data are rotated to mimic the different camera viewpoints so that our algorithm becomes view-tolerant. Each rotated depth frame is first projected onto three orthogonal Cartesian planes, and then, for each projected view, the absolute differences (motion energy) between consecutive frames or subsampled frames are accumulated through an entire depth sequence. To encode the temporal order of body poses, weights are introduced such that recent frames contribute more to WHDMMs so that pair actions (e.g., "sit down" and "stand up," having similar but reverse temporal patterns) can be distinguished. To leverage the Con-vNets trained over ImageNets, the WHDMM are encoded into pseudocolor images. Such encoding converts the spatiotemporal motion patterns in videos into spatial structures (edges and textures), thus enabling the ConvNets to learn the filters <ref type="bibr" target="#b11">[12]</ref>. Three ConvNets are trained on the three WHDMMs constructed from the projected Cartesian planes independently, and the results are fused to produce the final classification score.</p><p>This paper is an extension of <ref type="bibr" target="#b12">[13]</ref>. It includes a novel WHDMM for exploitation of temporal information.</p><p>This paper is organized as follows. Section II reviews the related work on RGB video-based action recognition using deep learning and depth map-based action recognition. Section III describes the proposed method. The experimental results are presented in Section IV. Section V concludes the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There have been several attempts to apply deep architectures in RGB video-based action recognition tasks <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, several ConvNet architectures for action recognition are compared using the Sport-1M dataset. A network operating on individual video frames performs similarly to networks whose input is a stack of frames. This suggests that the learned spatiotemporal features do not capture the motion. In <ref type="bibr" target="#b16">[17]</ref>, spatial and temporal streams are proposed for action recognition by using precomputed dense optical flow fields, stacked over consecutive frames, in order to bypass the difficult problem of directly training a convolutional net to extract motion information.</p><p>With Microsoft Kinect Sensors, researchers have developed methods for depth map-based action recognition. Li et al. <ref type="bibr" target="#b1">[2]</ref> sampled points from a depth map to obtain a bag of 3-D points to encode spatial information and employ an expandable graphical model to encode temporal information <ref type="bibr" target="#b17">[18]</ref>. One limitation of this method is view dependence. Yang et al. <ref type="bibr" target="#b18">[19]</ref> stacked differences between projected depth maps as a depth motion map (DMM) and then used histogram of oriented gradient to extract relevant features from the DMM. This method transforms the problem of action recognition from spatiotemporal space to spatial space. However, this method is also view dependent. In <ref type="bibr" target="#b3">[4]</ref>, a feature called histogram of oriented 4D normals (HON4D) was proposed; surface normal is extended to 4-D space and quantized by regular polychorons. Following this method, Yang and Tian <ref type="bibr" target="#b4">[5]</ref> cluster hypersurface normals and form the polynormal that can be used to jointly capture the local motion and geometry information. Super normal vector (SNV) is generated by aggregating the low-level polynormals. In <ref type="bibr" target="#b19">[20]</ref>, a fast binary range sample feature was proposed based on a test statistic by carefully designing the sampling scheme to exclude most pixels that fall into the background and to incorporate spatiotemporal cues.</p><p>Depth maps have been augmented with skeleton data in order to improve recognition. Wang et al. <ref type="bibr">[3]</ref> designed a 3-D local occupancy pattern (LOP) feature to describe the local depth appearance at joint locations to capture the information related to subject-object interactions. The intuition is to count the number of points that fall into a spatiotemporal bin when the space around the joint is occupied by the object. Wang et al. <ref type="bibr" target="#b20">[21]</ref> adopted LOP feature calculated from the 3-D point cloud around a particular joint to discriminate different types of interactions and Fourier temporal pyramid to represent the temporal structure. Based on these two types of features, the Actionlet Ensemble Model was proposed, which combines the features of a subset of the joints. To fuse depth-based features with skeleton-based features, Althloothi et al. <ref type="bibr" target="#b21">[22]</ref> presented two sets of features: features for shape representation extracted from depth data by using a spherical harmonics representation and features for kinematic structure extracted from skeleton data. The shape features are used to describe the 3-D silhouette structure, while the kinematic features are used to describe the movement of the human body. Both sets of features are fused at the kernel level for action recognition by using multiple kernel learning technique. Chaaraoui et al. <ref type="bibr" target="#b22">[23]</ref> proposed a fusion method to combine skeleton and silhouette-based features.</p><p>The methods reviewed are based on carefully hand-designed features. The work presented in this paper is inspired by the authors of <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b18">[19]</ref>, where the problem of action recognition is transformed in a novel way to that of image classification in order to take advantage of the trained ImageNet models <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED WEIGHTED HIERARCHICAL DEPTH MOTION MAPS + THREE-CHANNEL DEEP CONVOLUTIONAL NEURAL NETWORKS</head><p>The proposed WHDMM + 3ConvNets method consists of two major components (see Fig. <ref type="figure" target="#fig_0">1</ref>): three ConvNets and the construction of WHDMMs from sequences of depth maps as the input to the ConvNets. Given a sequence of depth maps, 3-D points are created and three WHDMMs are constructed by projecting the 3-D points to the three orthogonal planes. Each WHDMM serves as an input to one ConvNet for classification. Final classification of the given depth sequence is obtained through a late fusion of the three ConvNets. A number of strategies have been developed to deal with the challenges posed by small datasets. First, more training data are synthesized by 1) rotating the input 3-D points to mimic different viewpoints and 2) constructing WHDMMs at different temporal scales. Second, the same ConvNet architecture as used for ImageNet is adopted so that the model trained over ImageNet <ref type="bibr" target="#b6">[7]</ref> can be adapted to our problem through transfer learning. Third, each WHDMM goes through a pseudocolor coding process to encode, with enhancement, different motion patterns into the pseudo-RGB channels before being input to the ConvNets. In the rest of this section, rotation of the 3-D points, construction and pseudocolor coding of WHDMMs, and training of the ConvNets are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rotation to Mimic Different Camera Viewpoints</head><p>Fig. <ref type="figure" target="#fig_1">2</ref>(a) illustrates how to covert a pixel in a depth map into a 3-D point by calculating its location (X, Y, Z) in the real-world coordinate system centered on the camera by using the pair of equations:</p><formula xml:id="formula_0">X = Z • (U -C x ) f x , Y = Z • (V -C y ) f y .<label>(1)</label></formula><p>In ( <ref type="formula" target="#formula_0">1</ref>), (U, V ) and Z denote screen coordinates and depth value, respectively, C x and C y denote the center of a depth map, and f x and f y are the focal lengths of the camera. For Kinect-V1 cameras, f x = f y = 580 <ref type="bibr" target="#b23">[24]</ref>. The rotation of the 3-D points can be performed equivalently by assuming that a virtual RGB-D camera moves around and points at the subject from different viewpoints (see Fig. <ref type="figure" target="#fig_1">2</ref>). Suppose the virtual camera moves from position P o to P d ; its motion can be decomposed into two steps: first move from P o to P t , with rotation angle denoted by θ and, then, moves from P t to P d , with rotation angle denoted by β. The coordinates after rotation can be computed through multiplication by the transformation matrices Tr y and Tr x , as</p><formula xml:id="formula_1">[X , Y , Z , 1] T = Tr y Tr x X, Y, Z, 1 T (2)</formula><p>where X , Y , and Z represent the 3-D coordinates after rotation, Tr y denotes the transform around Y axis (right-handed coordinate system), and Tr x denotes the transform around Xaxis. The transformation matrices can be expressed as</p><formula xml:id="formula_2">Tr y = R y (θ) T y (θ) 0 1 , Tr x = R x (β) T x (β) 0 1<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">R y (θ) = ⎡ ⎢ ⎣ 1 0 0 0 cos(θ) -sin(θ) 0 sin(θ) cos(θ) ⎤ ⎥ ⎦ T y (θ) = ⎡ ⎢ ⎣ 0 Z • sin(θ) Z • (1 -cos(θ)) ⎤ ⎥ ⎦; R x (β) = ⎡ ⎢ ⎣ cos(β) 0 sin(β) 0 1 0 -sin(β) 0 cos(β) ⎤ ⎥ ⎦ T x (β) = ⎡ ⎢ ⎣ -Z • sin(β) 0 Z • (1 -cos(β)) ⎤ ⎥ ⎦.</formula><p>After rotation, a depth map from a different viewpoint can be obtained from</p><formula xml:id="formula_4">U = X • f x Z + C x , V = Y • f y Z + C y (4)</formula><p>where U , V , and Z , respectively, denote the new screen coordinates and their corresponding depth value.</p><p>Since RGB-D camera only captures 2 1 2 D, and not the full 3-D information, the rotation has to be within a range such that the synthesized depth maps still capture sufficient spatiotemporal information of the actions. In other words, both θ and β have to be limited to a certain range. Fig. <ref type="figure" target="#fig_2">3</ref> shows some examples of the synthesized depth maps and the original one from which they were created. Even at relatively large angles (|θ| = 45 • , |β| = 45 • ), the synthesized depth maps still capture the shape of the body well. In some extreme cases where θ and β become very large (see Fig. <ref type="figure" target="#fig_3">4</ref>), the synthesized depth maps do not capture sufficient spatial information of the subject. Empirically, the useful range of the angles is between (-60 • , 60 • ) for both θ and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction of Weighted Hierarchical Depth Motion Maps</head><p>Each of the original and synthesized depth maps is projected to three orthogonal Cartesian planes, referred to as front, side, and top views and denoted by map p , where p ∈ {f, s, t}. Unlike the DMM <ref type="bibr" target="#b18">[19]</ref> where it is calculated by accumulating the thresholded difference between consecutive frames, three extensions are proposed to construct a WHDMM. First, in order to preserve subtle motion information, for example, turning a page when reading books, for each projected map, the motion energy is calculated as the absolute difference between consecutive or subsampled depth frames without thresholding. Second, to exploit speed invariance and suppress noise, several temporal scales, referred to as hierarchical temporal scales, are generated as illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>, where N is the number of frames, and a WHDMM is constructed for each of the temporal scales. Through this process, the number of training samples are further increased, and at the same time, the issue of speed variation is addressed. Finally, in order to differentiate motion direction, a temporal weight is introduced, giving a higher weight to recent depth frames than to past frames.</p><p>Let WHDMM t p n be the WHDMM being projected to view p and accumulated up to frame t at the nth temporal scale. It can be expressed as</p><formula xml:id="formula_5">WHDMM t p n = γ|map (t-1)n +1 p -map (t-2)n +1 p | + (1 -γ)WHDMM t-1 p n</formula><p>(5)   where map i p denotes the ith depth map in the original video sequence and being projected to view p; γ ∈ (0, 1) stands for the temporal weight and WHDMM 1</p><formula xml:id="formula_6">p n = |map n +1 p -map 1 p |.</formula><p>Using this simple temporal weighting scheme along with the pseudocolor coding of the WHDMMs (to be described in the next section), pair actions, such as "sit down" and "stand up," can be distinguished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudocolor Coding of Weighted Hierarchical Depth Motion Maps</head><p>Abidi et al. <ref type="bibr" target="#b24">[25]</ref> reported gaining an enhanced perceptual quality and more information from gray-scale texture through a human-perception-based color coding. Motivated by this result, we transform the WHDMM into a pseudocolor that enhances motion patterns of actions and improves signal-to-noise ratio (SNR). Methods of pseudocolor coding include spectrum-based maps, naturally ordered maps, uniformly varying maps, shapeand value-based maps, and function-based maps <ref type="bibr" target="#b24">[25]</ref>. Furthermore, nonlinear transformations can increase/decrease the contrast of certain gray levels without truncating low/high pixel intensities <ref type="bibr" target="#b25">[26]</ref>. In this paper, an improved rainbow transform (a special case of the sine transform), which is a variant of the spectrum-based mapping method, is developed. The improved rainbow transform is expressed as</p><formula xml:id="formula_7">C i=1,2,3 = {sin[2π • (-I + ϕ i ) • 1 2 + 1 2 ]} α • f (I) (6)</formula><p>where C i=1,2,3 represent the BGR channels, respectively; I is the normalized gray value; ϕ i denotes the phase of the three channels; α is the power; f (I) is an amplitude modulation function; and the added value, 1 2 , guarantees nonnegativity. The value of parameter α can be chosen to vary noise suppression. In our work, ϕ i=1,2,3 and f (I) are set to</p><formula xml:id="formula_8">1 5 -1 2 π, 1 5 -1 2 π -3 14 , 1 5 -1 2 π -6</formula><p>14 , 1 4 + 3 4 I, respectively. To encode a WHDMM, linear mapping is used to convert WHDMM values to I ∈ [0, 1]. Fig. <ref type="figure" target="#fig_6">7</ref> shows the transform with α = 1 and α = 10. As observed, higher level noise in WHDMM is found in the areas of background or at edges of the subjects where the WHDMM values are either very large or very small. Thus, in order to improve the SNR of WHDMM, the parameters of the transform in <ref type="bibr" target="#b5">(6)</ref> are chosen so as to suppress both the small and large values of WHDMM. In addition, Fig. <ref type="figure" target="#fig_6">7</ref> shows that the improved rainbow transform with a relatively large α encodes the gray intensities to RGB values in a drastic manner than small α (e.g., α = 1) and suppresses the noise in the color-encoded WHDMMs more effectively.</p><p>Fig. <ref type="figure" target="#fig_5">6</ref> shows sample pseudocolor coded WHDMMs of the actions from the MSRAction3D dataset. Although the WHDMMs for the actions "forward kick" see [Fig. <ref type="figure" target="#fig_5">6(n)</ref>] and "jogging" [see Fig. <ref type="figure" target="#fig_5">6(p)</ref>] appear similar, the pseudocoloring has highlighted the differences. Since the texture and edges in a WHDMM are accumulation of spatial and temporal information, the pseudocolor coding remaps the spatiotemporal information of actions.</p><p>The value of α controls how well the pseudocolor coding improves the SNR of a WHDMM. Fig. <ref type="figure" target="#fig_7">8</ref> shows the pseudocolor coded WHDMM of action "eat" at the fifth temporal scale. Notice that when α = 1, both the background (i.e., sofa and person in the background) and the foreground subject are clearly noticeable. With increased α value, say α = 10, the background is suppressed with little loss of the foreground information. However, if the value of α is very large (say α = 20), both the foreground and background are suppressed.</p><p>Fig. <ref type="figure" target="#fig_8">9</ref> illustrates how the recognition accuracy on the MSR-DailyActivity3D dataset varies with the value of α when only the WHDMMs of frontal projection are used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Training and Class Score Fusion</head><p>Three ConvNets are trained on the pseudocolor coded WHD-MMs in the three Cartesian planes. The layer configuration of the ConvNets follows those in <ref type="bibr" target="#b6">[7]</ref> and is schematically shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Each ConvNet contains eight layers with weights; the first five are convolutional layers and the remaining three are fully connected layers. The implementation is derived from the publicly available Caffe toolbox <ref type="bibr" target="#b26">[27]</ref> based on one NVIDIA Tesla K40 card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training:</head><p>The training procedure is similar to that in <ref type="bibr" target="#b6">[7]</ref>, wherein the network weights are learned using the mini-batch stochastic gradient descent with the momentum value set to 0. initially set to 10 -2 and used to directly train the networks from data without initializing the weights with pretrained models on ILSVRC-2012 (Large Scale Visual Recognition Challenge 2012, a version of ImageNet). The rate is set to 10 -3 for finetuning with pretrained models on ILSVRC-2012, and then, it is decreased according to a fixed schedule, which is kept the same for all training sets. For each ConvNet, the training undergoes 100 cycles and the learning rate decreases every 20 cycles. For all experiments, the dropout regularization ratio was set to 0.5 in order to reduce complex coadaptations of neurons in the nets.</p><p>2) Class Score Fusion: Given a test depth video sequence (sample), WHDMMs at different temporal scales are classified using the trained ConvNets. The average scores of n scales for each test sample are calculated for each of the three ConvNets. The final class score for a test sample is the average of the outputs from the three ConvNets. Thus</p><formula xml:id="formula_9">score test = 3 c=1 n i=1 score i c 3n<label>(7)</label></formula><p>where score test represents the final class score for a test sample, while score i c denotes the score of ith temporal scale for cth channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>The proposed method was evaluated on three public benchmark datasets: MSRAction3D <ref type="bibr" target="#b1">[2]</ref>, UTKinect-Action <ref type="bibr" target="#b27">[28]</ref>, and MSRDailyActivity3D <ref type="bibr">[3]</ref>. An extension of MSRAction3D, called MSRAction3DExt dataset, was used. It contains more than twice (i.e., 23) as many subjects as in the previous dataset performing the same set of actions. In order to test the stability of the proposed method with respect to the number of actions, a new dataset was created by combining MSRAction3DExt, UTKinect-Action, and MSRDailiyActivity3D datasets; the new dataset is referred to as Combined dataset. In all experiments, θ varied over the range (-30 • : 15 • : 30 • ) and β varied over the range (-5 • : 5 • : 5 • ). For WHDMM, γ was set to 0.495. Different temporal scales, as detailed below, were set according to the noise level, complexity, and average cycle in frames of actions a pretrained model. These six scenarios evaluate the proposed method from different perspectives and effectiveness of the proposed strategies. Scenario S6 provides an evaluation of the overall performance of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MSRAction3D Dataset</head><p>The MSRAction3D dataset <ref type="bibr" target="#b1">[2]</ref> is composed of depth sequences captured by an MS Kinect-V1 camera. It contains 20 actions performed by ten subjects facing the camera, with each subject performing each action two or three times. The 20 actions are: "high arm wave," "horizontal arm wave," "hammer," "hand catch," "forward punch," "high throw," "draw X," "draw tick," "draw circle," "hand clap," "two hand wave," "sideboxing," "bend," "forward kick," "side kick," "jogging," "tennis swing," "tennis serve," "golf swing," and "pick up &amp; throw."</p><p>In order to obtain a fair comparison, the same experimental setting as that in <ref type="bibr">[3]</ref> is followed, namely, the cross-subject settings: subjects 1, 3, 5, 7, and 9 for training and subjects 2, 4, 6, 8, and 10 for testing. For this dataset, temporal scale n = 1 and α = 2, and the proposed method achieved 100% accuracy. Results of scenarios S1-S4 are shown in Table <ref type="table">I</ref>. As seen, without using temporal scaling, i.e., n = 1, the recognition can reach 100%.</p><p>Pretraining on ILSVRC-2012 (i.e., S3 and S4) is very effective. Because the volume of training data is not enough to train millions of parameters of the deep networks, without good initialization, overfitting becomes inevitable. When the networks were directly trained from the original samples (i.e., S1), the performance is only slightly better than a random guess. 74.70% Actionlet Ensemble <ref type="bibr">[3]</ref> 82.22% Depth Motion Maps <ref type="bibr" target="#b18">[19]</ref> 88.73% HON4D <ref type="bibr" target="#b3">[4]</ref> 88.89% SNV <ref type="bibr" target="#b4">[5]</ref> 93.09% Range Sample <ref type="bibr" target="#b19">[20]</ref> 95.62% Proposed Method 100.00%  The proposed method outperforms all previous methods. This is probably because 1) the WHDMM can filter out the simple and static background in MSRAction3D; 2) the pretrained model can initialize the three ConvNets well so that they can learn the filters well even though action recognition and image classification belong to domains; and 3) the WHDMM and pseudocolor coding can encode the spatiotemporal information into a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MSRAction3DExt Dataset</head><p>The MSRAction3DExt dataset is an extension of MSRAc-tion3D dataset with an additional 13 subjects performing the same 20 actions two to four times in a similar environment as that of MSRAction3D. Thus, there are 20 actions, 23 subjects, and 1379 video clips. Similarly to MSRAction3D, the proposed method was evaluated under four settings, and the results are listed in Table <ref type="table" target="#tab_2">III</ref>. For this dataset, samples of oddnumbered subjects were used for training, and samples of the even-numbered subjects were used for testing.</p><p>Tables <ref type="table">I</ref> and<ref type="table" target="#tab_2">III</ref> show that as the volume of dataset increases with respect to the MSRAction3D dataset, the performance improved from 34.23% to 53.05% when the Nets are directly trained from the original and synthesized samples. However, the performance is not comparable with that obtained when the pretrained model on ImageNet was used for initialization. The proposed method achieved again 100% using the pretrained model followed by fine-tuning even though this dataset has more variations across subjects. Table <ref type="table">IV</ref> shows the results of SNV <ref type="bibr" target="#b4">[5]</ref> on the MSRAction3DExt dataset. As seen, the proposed method outperforms SNV <ref type="bibr" target="#b4">[5]</ref>.</p><p>Using the pretrained model followed by fine-tuning is effective on small datasets. In the following experiments, results obtained by training the networks using the pretrained model and fine-tuning, i.e., S3-6, are reported.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UTKinect-Action Dataset</head><p>The UTKinect-Action dataset <ref type="bibr" target="#b27">[28]</ref> was captured using a stationary Kinect-V1 sensor. It consists of ten actions: "walk," "sit down," "stand up," "pick up," "carry," "throw," "push," "pull," "wave," and "clap hands." There are ten different subjects, and each subject performs each action twice. Notice that one of the challenges of this dataset is viewpoint variation.</p><p>For this dataset, temporal scale n = 5 and α = 2 were set to exploit the temporal information because, unlike, the MSRAc-tion3D dataset, each samples in this dataset contains multiple cycles of the actions. The cross-subject evaluation scheme used in <ref type="bibr" target="#b28">[29]</ref> was adopted. This is different from the scheme used in <ref type="bibr" target="#b27">[28]</ref>, where more subjects were used for training in each round. Experiments on the four training scenarios S3, S4, S5, and S6 were conducted, and the results are shown in Table <ref type="table">V</ref>.</p><p>Table <ref type="table">V</ref> shows that inclusion of synthesized samples improved tolerance to viewpoint variation, and thus, the recognition accuracy was 6 percentage points higher. The confusion matrix for S6 is shown in Fig. <ref type="figure" target="#fig_9">10</ref>. The most confused actions are hand clap and wave, which share similar appearance of WHDMMs.</p><p>Table <ref type="table" target="#tab_6">VI</ref> shows the performance of the proposed method compared with the previous depth-based methods on the UTKinect-Action dataset. The improved performance will suggest that the proposed method has better viewpoint tolerance than other depth-based algorithms. 78.78% Random Forests <ref type="bibr" target="#b29">[30]</ref> 87.90% SNV <ref type="bibr" target="#b4">[5]</ref> 88.89% Proposed Method 90.91%   <ref type="bibr" target="#b18">[19]</ref> 43.13% Local HON4D <ref type="bibr" target="#b3">[4]</ref> 80.00% Actionlet Ensemble <ref type="bibr">[3]</ref> 85.75% SNV <ref type="bibr" target="#b4">[5]</ref> 86.25% Range Sample <ref type="bibr" target="#b19">[20]</ref> 95.63% Proposed Method 85.00%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MSRDailyActivity3D Dataset</head><p>The MSRDailyActivity3D dataset <ref type="bibr">[3]</ref> was also captured by an MS Kinect-V1 sensor. It has 16 activities: "drink," "eat," "read book," "call cellphone," "write on paper," "use laptop," "use vacuum cleaner," "cheer up," "sit still," "toss paper," "play game," "lay down on sofa," "walking," "play guitar," "stand up," and "sit down." There are ten subjects, and each subject performed each activity twice, one in standing position and the other in sitting position. Compared with MSRAction3D(Ext) and UTKinect-Action datasets, actors in this dataset present large spatial and temporal changes. Most activities in this dataset involve human-object interactions.</p><p>For this dataset, the temporal scale was set to n = 21 and α = 10, a larger number of scales and power than those used for MSRAction3D and UTKinect-Action datasets. This choice of values was made to exploit temporal information and suppress the high-level noise in this dataset. The same experimental setting as in <ref type="bibr">[3]</ref> was adopted, and the final recognition accuracy reached 85.00%. Results for the training settings, S3, S4, S5, and S6, are reported in Table <ref type="table" target="#tab_6">VII</ref>. The samples (T2) synthesized through rotation improved the recognition accuracy by 15 percentage points, and the samples (T3) synthesized through temporal scaling further improved the recognition accuracy by additional 15 percentage points Table VIII compared the performance of the proposed method and that of existing depth-based methods, and Fig. <ref type="figure" target="#fig_10">11</ref> depicts the confusion matrix of the proposed method.</p><p>Table <ref type="table" target="#tab_8">VIII</ref> shows that the proposed method outperforms the DMM method <ref type="bibr" target="#b18">[19]</ref> and has comparable performance with SNV <ref type="bibr" target="#b4">[5]</ref> and Actionlet Ensemble <ref type="bibr">[3]</ref>. Notice that local HON4D <ref type="bibr" target="#b3">[4]</ref> used skeleton data for localizing the subjects in depth maps, while Actionlet Ensemble <ref type="bibr">[3]</ref> and SNV <ref type="bibr" target="#b4">[5]</ref> both used depth and skeleton data to extract features. However, the proposed method performed worse than the Range Sample <ref type="bibr" target="#b19">[20]</ref>. Two reasons are adduced for this observation. First, the background of this dataset is complex and much more temporally dynamic compared with MSRAction3D(Ext), and this introduced noise in the WHDMMs. However, the Range Sample <ref type="bibr" target="#b19">[20]</ref> method has a mechanism to remove/reduce the interference from the background using skeleton data for preprocessing. Second, WHDMM is not sufficient to differentiate subtle differences in motion between some actions when interactions with objects become a key differentiating factor (e.g., call cellphone, drink, and eat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Combined Dataset</head><p>The Combined dataset is a combination of MSRAction3DExt, UTKinect-Action, and MSRDailyActivity3D datasets and was created to test the performance of the proposed method when the number of actions increased. The Combined dataset is challenging due to its large variations in background, subjects, viewpoints, and imbalanced number of samples for each actions. The same actions in different datasets are combined into one action, and there are in total 40 distinct actions in the Combined dataset, as described in Table <ref type="table" target="#tab_10">IX</ref>.</p><p>For this dataset, a cross-subject scheme was adopted with half of the subjects used for training and the rest for testing; the choice was made such that the training and testing subjects were the same as those when each of the original individual datasets was used for evaluation. This provides a fair basis for the evaluation of the performance of the proposed method on the individual datasets when trained using the Combined dataset.</p><p>The temporal scale was set to n = 5 and α = (2, 5) in the experiments. The proposed method was tested with four settings, and the results are shown in Table <ref type="table" target="#tab_10">X</ref>. As expected, the strategy   of using synthesized samples and multiple temporal scaling becomes less apparent in improving the overall performance. One probable reason is that the training of ConvNets has benefited from the large number of samples in the Combined dataset. The performance of the proposed method is compared with SNV <ref type="bibr" target="#b4">[5]</ref> on this dataset in the following manner. A model is first trained over the Combined dataset and then tested on the original individual datasets and Combined dataset. Note that this was done according to the cross-subject evaluation scheme as described, and the training and testing samples were kept the same as when the methods were applied to individual datasets separately. The results and corresponding confusion matrices are shown in Tables XI, XII and Fig. <ref type="figure" target="#fig_11">12</ref>, respectively. In order to compare the performance of the methods on individual datasets and the combined case, the rate change, i.e., η = |X c -X | X × 100%, was calculated, where X and X c denote, respectively, the accuracies when performing the training and recognition on individual datasets separately and on the combined dataset.</p><p>Tables XI and XII show that the proposed method can maintain the accuracy without a large drop, while the number of actions increased and the dataset becomes more complex. In addition, it outperformed the SNV method on the Combined dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis</head><p>In the proposed method, ConvNets serve the purpose of feature extraction and classification. Generally, ConvNets require a large amount of data to tune millions of parameters to avoid overfitting. Directly training the ConvNets with a small set of data would lead to poor performance due to overfitting, and this has been demonstrated in Tables I and III. However, the small amount (even for the Combined dataset) of available data can be compensated with data augmentation. In our method, two strategies are used for this purpose: synthesized viewpoints and temporal scaling with additional benefits of making the method viewpoint and speed tolerant, respectively. However, without initializing the ConvNets with a model pretrained over Ima-geNet, the artificially augmented data seem insufficient to train the nets. This is probably because the data synthesized from the original data do not contain the same amount of independent information as would have been captured by real cameras. Nonetheless, their contribution to the training is apparent as demonstrated in the experiments. In addition, the scheme of pretraining followed by fine-tuning provides a promising remedy for small datasets.</p><p>For each dataset, a different temporal scale was set to obtain the best results, and the reasons are as follows. For simple actions (or gestures), such as MSRAction3D(Ext), one scale is sufficient to distinguish the differences between actions (gestures), due to their low motion complexity and short duration of motion. For activities, such as those in the UTKinect-Action and MSRDailyActivity3D datasets, more scales (e.g., 5) are needed, because the duration of the actions are long, and each action usually contains several simple actions (gestures). Use of a large number (e.g., over 5) of scales can capture the motion information in different temporal scales. When we consider noisy samples, such as the MSRDailyActivity3D dataset, larger temporal scales (e.g., 21) should be set in order to suppress the effects of complex background in these datasets. However, the performance is not sensitive to the number of temporal scales and gain in performance by tuning the scale is rather marginal (around 2 percentage points).</p><p>For pseudocoloring, the power α is, in general, set between 2 and 10 according to the characteristic of noise. A large value of α can suppress the noise in areas having small or large WHDMM values. However, the performance gain over different α values is around 3 percentage points for the datasets used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Computational Cost</head><p>Table XIII compares the computational cost of the SNV and the proposed method on the MSRAction3D dataset. The dataset   The CPU platform used in the testing is a small HPC running CentOS6.5 with 2× Intel(R) Xeon(R) Processor E5-4620 at 2.20 GHz, and the GPU test platform is equipped with an NVIDIA Tesla K40 card. The SNV method was implemented in MATLAB and executed on the CPU platform. The proposed method was implemented in C/C++, and much of its computation is performed by the GPU. It should be pointed out that the computational cost of the SVN method increases exponentially with the number of frames, whereas the computation cost of the proposed method increases linearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a method for action recognition from depth map sequences using deep learning has been proposed. The method has been evaluated on the most widely used datasets and compared with state-of-the-art methods. The proposed method achieved state-of-the-art results on individual datasets and maintained the accuracy on the combined datasets. The experimental results showed that the strategies developed for applying Con-vNets to small datasets worked effectively. The experimental results have also shown that dynamic background could adversely impact the recognition. The proposed method can benefit from any coarse segmentation of the subjects who perform the action in front of a dynamic or cluttered background.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed WHDMM + 3ConvNets architecture for depth-based action recognition.</figDesc><graphic coords="2,50.63,68.81,412.94,192.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Process of rotating 3-D points to mimic different camera viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example depth maps synthesized by the virtual RGB-D camera. (a) original depth map, depth maps synthesized, respectively, with the parameters (b) (θ = 45 • , β = 45 • ), (c) (θ = 45 • , β = -45 • ), (d) (θ = -45 • , β = 45 • ), and (e) (θ = -45 • , β = -45 • ).</figDesc><graphic coords="4,50.16,67.94,498.24,88.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of synthesized depth maps for cases where θ and β are very large. (a) (θ = -75 • , β = -75 • ). (b) (θ = -85 • , β = -85 • ).</figDesc><graphic coords="4,44.65,201.61,246.24,102.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Hierarchical temporal scales: For the nth temporal scale, the subsampled sequence is constructed by taking one frame, starting from the first frame, from every n frames.</figDesc><graphic coords="4,44.65,346.64,246.24,116.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of pseudocolor coded WHDMMs of actions in the MSRAction3D dataset performed by randomly selected subjects: (a) high arm wave, (b) horizontal arm wave, (c) hammer, (d) hand catch, (e) forward punch, (f) high throw, (g) draw X, (h) draw tick, (i) draw circle, (j) hand clap, (k) two hand wave, (l) side-boxing, (m) bend, (n) forward kick, (o) side kick, (p) jogging, (q) tennis swing, (r) tennis serve, (s) golf swing, and (t) pick up &amp; throw.</figDesc><graphic coords="5,45.95,68.54,498.24,276.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual comparison of improved rainbow transform with α = 1 and α = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Sample color-coded WHDMM of action "eat" with different α values.</figDesc><graphic coords="5,40.44,400.29,246.24,245.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Variation of recognition accuracy with increasing value of α. MSR-DailyActivity3D dataset has been used with only the frontal channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Confusion matrix of the proposed method for UTKinect-Action dataset.</figDesc><graphic coords="7,338.03,273.05,211.10,115.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Confusion matrix of proposed method for MSRDailyActivity3D dataset.</figDesc><graphic coords="8,353.87,68.69,199.32,138.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Confusion matrix of the proposed method for Combined dataset.</figDesc><graphic coords="10,95.27,202.85,452.58,331.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RECOGNITION</head><label>III</label><figDesc>RESULTS ACHIEVED ON THE MSRACTION3DEXT DATASET</figDesc><table><row><cell>Training Setting</cell><cell>Accuracy</cell></row><row><cell>S1</cell><cell>10.00%</cell></row><row><cell>S2</cell><cell>53.05%</cell></row><row><cell>S3</cell><cell>100.00%</cell></row><row><cell>S4</cell><cell>100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc>II compares the performance of the proposed WHDMM + 3ConvNets with recently reported depth-based results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARATIVE</head><label>VI</label><figDesc>ACCURACY OF THE PROPOSED METHOD AND PREVIOUS DEPTH-BASED METHODS USING THE UTKINECT-ACTION DATASET</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>DSTIP+DCSF [29]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII COMPARATIVE</head><label>VIII</label><figDesc>ACCURACY OF THE PROPOSED METHOD AND PREVIOUS DEPTH-BASED METHODS USING THE MSRDAILYACTIVITY3D DATASET</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>LOP [3]</cell><cell>42.50%</cell></row><row><cell>Depth Motion Maps</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X COMPARATIVE</head><label>X</label><figDesc>PERFORMANCE OF THE PROPOSED METHOD BASED ON THE COMBINED DATASET AND WITH RESPECT TO THE FOUR TRAINING SETTINGS</figDesc><table><row><cell>Training Setting</cell><cell>Accuracy (α = 2)</cell><cell>Accuracy (α = 5)</cell></row><row><cell>S3</cell><cell>87.20%</cell><cell>87.59%</cell></row><row><cell>S4</cell><cell>-</cell><cell>90.63%</cell></row><row><cell>S5</cell><cell>-</cell><cell>89.94%</cell></row><row><cell>S6</cell><cell>90.92%</cell><cell>91.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI COMPARATIVE</head><label>XI</label><figDesc>RECOGNITION ACCURACIES OF THE SNV AND THE PROPOSED METHODS USING THE COMBINED DATASET AND ITS ORIGINAL DATASETS</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Method</cell><cell></cell></row><row><cell></cell><cell>SNV</cell><cell>Proposed</cell><cell>Proposed</cell></row><row><cell></cell><cell></cell><cell>(α = 2)</cell><cell>(α = 5)</cell></row><row><cell>MSRAction3D</cell><cell>89.83%</cell><cell>94.58%</cell><cell>94.92%</cell></row><row><cell>MSRAction3DExt</cell><cell>91.15%</cell><cell>94.05%</cell><cell>94.35%</cell></row><row><cell>UTKinect-Action</cell><cell>93.94%</cell><cell>91.92%</cell><cell>92.93%</cell></row><row><cell>MSRDailyActivity3D</cell><cell>60.63%</cell><cell>78.12%</cell><cell>80.63%</cell></row><row><cell>Combined</cell><cell>86.11%</cell><cell>90.92%</cell><cell>91.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XII COMPARATIVE</head><label>XII</label><figDesc>PERFORMANCE OF THE SNV AND THE PROPOSED METHODS USING INDIVIDUAL AND COMBINED DATASET</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SNV</cell><cell>SNV c</cell><cell>η</cell><cell>Proposed</cell><cell>Proposed c</cell><cell>η</cell></row><row><cell>MSRAction3D</cell><cell>93.09%</cell><cell>89.58%</cell><cell>3.77%</cell><cell>100.00%</cell><cell>94.92%</cell><cell>5.08%</cell></row><row><cell>MSRAction3DExt</cell><cell>90.54%</cell><cell>91.15%</cell><cell>0.67%</cell><cell>100.00%</cell><cell>94.35%</cell><cell>5.65%</cell></row><row><cell>UTKinect-Action</cell><cell>88.89%</cell><cell>93.94%</cell><cell>5.68%</cell><cell>90.91%</cell><cell>92.93%</cell><cell>2.22%</cell></row><row><cell>MSRDailyActivity3D</cell><cell>86.25%</cell><cell>60.63%</cell><cell>28.70%</cell><cell>85.00%</cell><cell>80.83%</cell><cell>4.91%</cell></row><row><cell cols="4">Recognition accuracy and change of accuracy are reported.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII COMPARATIVE</head><label>XIII</label><figDesc>COMPUTATIONAL COST OF THE SNV AND THE PROPOSED METHOD BASED ON MSRACTION3D DATASET</figDesc><table><row><cell>Cost</cell><cell cols="2">Method</cell></row><row><cell></cell><cell>SNV</cell><cell>Proposed</cell></row><row><cell>Training</cell><cell></cell><cell>667 (CPU time) +</cell></row><row><cell>(seconds)</cell><cell>22 913 (CPU time)</cell><cell>2246 (GPU time)</cell></row><row><cell>Testing</cell><cell></cell><cell>0.80 (CPU time) +</cell></row><row><cell>(seconds per sample)</cell><cell>76 (CPU time)</cell><cell>0.24 (GPU time)</cell></row><row><cell>Memory usage</cell><cell>16G RAM</cell><cell>4G video RAM +</cell></row><row><cell></cell><cell></cell><cell>4G RAM</cell></row></table><note><p>has 567 samples; 292 samples were used for training and the rest for testing. The average number of frames per sample is 42.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf<address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="124" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="804" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962-01">Jan. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf</title>
		<meeting>Adv. Neural Inf<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations<address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">Apr. 2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convnetsbased action recognition from depth maps through virtual ccamera and pseudocoloring</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Multimedia</title>
		<meeting>ACM Conf. Multimedia<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Expandable data-driven graphical modeling of human actions based on salient postures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Multimedia</title>
		<meeting>ACM Conf. Multimedia<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="1057" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Range-sample depth feature for action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="772" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3D human action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="927" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human activity recognition using multi-features and multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Althloothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1800" to="1812" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fusion of skeletal and silhouette-based features for human action recognition with rgb-d devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaaraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padilla-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Florez Revuelta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision Workshops</title>
		<meeting>IEEE Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
			<biblScope unit="page" from="91" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D with kinect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Smisek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jancosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Workshops<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="3" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving weapon detection in single energy X-ray images through pseudocoloring</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Gribok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="784" to="796" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Not seeing is not believing: Improving the visibility of your fluorescence images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Cell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="754" to="757" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="2834" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fusing spatiotemporal features and joints for 3D action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshops<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="486" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">His current research interests include computer vision and machine learning</title>
	</analytic>
	<monogr>
		<title level="m">He is currently working toward the Ph.D. degree with the School of Computing and Information Technology</title>
		<meeting><address><addrLine>China; Tianjin, China; Wollongong, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Nanchang University, Nanchang ; University of Wollongong</orgName>
		</respStmt>
	</monogr>
	<note>2010, and the M.S. degree in communication and information system from Tianjin University</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">He is currently an Associate Professor and Co-Director of Advanced Multimedia Research Lab</title>
		<author>
			<persName><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SM&apos;12) received the Ph.D. degree in electronic engineering from the University of Western Australia, Crawley, Australia</title>
		<meeting><address><addrLine>Wollongong, Australia</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University of Wollongong</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include 3-D computer vision</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">D multimedia signal processing, and medical image analysis. Zhimin Gao received the B.S. degree from North China Electric Power University</title>
	</analytic>
	<monogr>
		<title level="m">She is currently working toward the Ph.D. degree with the School of Computing and Information Technology</title>
		<meeting><address><addrLine>Beijing, China; Tianjin, China; Wollongong, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Wollongong</orgName>
		</respStmt>
	</monogr>
	<note>Her research interests include computer vision and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
