<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON-TRASTIVE LEARNING FOR DENSE TEXT RETRIEVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON-TRASTIVE LEARNING FOR DENSE TEXT RETRIEVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient. We also empirically validate our theory that negative sampling with ANCE better approximates the oracle gradient-norm based importance sampling, thus improves the convergence of stochastic training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many language systems rely on text retrieval as their first step to find relevant information. For example, search ranking <ref type="bibr">(Nogueira &amp; Cho, 2019)</ref>, open domain question answering (OpenQA) <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, and fact verification <ref type="bibr" target="#b45">(Thorne et al., 2018)</ref> all first retrieve relevant documents for their later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the advancements of deep learning techniques <ref type="bibr" target="#b43">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b47">Wang et al., 2018)</ref>, while, the first stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become the pain point of many systems <ref type="bibr">(Nogueira &amp; Cho, 2019;</ref><ref type="bibr" target="#b35">Luan et al., 2020;</ref><ref type="bibr" target="#b52">Zhao et al., 2020)</ref>.</p><p>Dense Retrieval (DR) aims to overcome the sparse retrieval bottleneck by matching texts in a continuous representation space learned via deep neural networks <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. It has many desired properties: fully learnable representation, easy integration with pretraining, and efficiency support from approximate nearest neighbor (ANN) search <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>. These grant dense retrieval an intriguing potential to fundamentally overcome some intrinsic limitations of sparse retrieval, for example, vocabulary mismatch <ref type="bibr" target="#b7">(Croft et al., 2010)</ref>.</p><p>A key challenge in dense retrieval is to construct proper negative instances when learning the representation space <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>. Unlike in reranking <ref type="bibr" target="#b33">(Liu, 2009)</ref> where the training negatives are naturally the irrelevant documents from previous retrieval stages, in first stage retrieval, DR models need to distinguish relevant documents from all irrelevant ones in a corpus with millions or billions of documents. As illustrated in Fig. <ref type="figure">1</ref>, these global negatives are quite different from negatives retrieved by sparse models.</p><p>Recent research explored various ways to construct negative training instances for dense retrieval <ref type="bibr" target="#b19">(Huang et al., 2020;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020)</ref>, e.g., using contrastive learning <ref type="bibr" target="#b11">(Faghri et al., 2017;</ref><ref type="bibr" target="#b42">Oord et al., 2018;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref> to select hard negatives in current or recent mini-batches. However, as observed in recent research <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>, the in-batch local negatives, though effective in learning word or visual representations, are not significantly better than spare-retrieved negatives in representation learning for dense retrieval. In addition, the accuracy of dense retrieval models often underperform BM25, especially on documents <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b13">Gao et al., 2020b;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Relevant DR Neg BM25 Neg Rand Neg</head><p>Figure <ref type="figure">1</ref>: T-SNE <ref type="bibr" target="#b36">(Maaten &amp; Hinton, 2008)</ref> representations of query, relevant documents, negative training instances from BM25 (BM25 Neg) or randomly sampled (Rand Neg), and testing negatives (DR Neg) in dense retrieval.</p><p>In this paper, we first theoretically analyze the convergence of dense retrieval training with negative sampling. Using the variance reduction framework <ref type="bibr" target="#b0">(Alain et al., 2015;</ref><ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret, 2018)</ref>, we show that, under conditions commonly met in dense retrieval, local in-batch negatives lead to diminishing gradient norms, resulted in high stochastic gradient variances and slow training convergence -the local negative sampling is the bottleneck of dense retrieval's effectiveness.</p><p>Based on our analysis, we propose Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a new contrastive representation learning mechanism for dense retrieval. Instead of random or in-batch local negatives, ANCE constructs global negatives using the beingoptimized DR model to retrieve from the entire corpus. This fundamentally aligns the distribution of negative samples in training and of irrelevant documents to separate in testing. From the variance reduction perspective, these ANCE negatives lift the upper bound of per instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster learning convergence.</p><p>We implement ANCE using an asynchronously updated ANN index of the corpus representation. Similar to <ref type="bibr" target="#b17">Guu et al. (2020)</ref>, we maintain an Inferencer that parallelly computes the document encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index used for negative sampling once it finishes, to keep up with the model training. Our experiments demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>, OpenQA <ref type="bibr" target="#b43">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b28">Kwiatkowski et al., 2019)</ref>, and in a commercial search engine's retrieval system. We also empirically validate our theory that the gradient norms on ANCE sampled negatives are much bigger than local negatives, thus improving the convergence of dense retrieval models.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we discuss the preliminaries of dense retrieval and its representation learning.</p><p>Task Definition: Given a query q and a corpus C, the first stage retrieval is to find a set of documents relevant to the query D + = {d 1 , ..., d i , ..., d n } from C (|D + | |C|), which then serve as input to later more complex models <ref type="bibr" target="#b7">(Croft et al., 2010)</ref>. Instead of using sparse term matches and inverted index, Dense Retrieval calculates the retrieval score f () using similarities in a learned embedding space <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b35">Luan et al., 2020;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020)</ref>:</p><formula xml:id="formula_0">f (q, d) = sim(g(q; θ), g(d; θ)),<label>(1)</label></formula><p>where g() is the representation model that encodes the query or document to dense embeddings. The encoder parameter θ provides the main capacity. The similarity function (sim()) is often simply cosine or dot product to leverage efficient ANN retrieval <ref type="bibr" target="#b21">(Johnson et al., 2019;</ref><ref type="bibr" target="#b15">Guo et al., 2020)</ref>.</p><p>BERT-Siamese Model: A standard instantiation of Eqn. 1 is to use the BERT-Siamese/twotower/dual-encoder model <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>:</p><formula xml:id="formula_1">f (q, d) = BERT(q) • BERT(d) = MLP( [CLS] q ) • MLP( [CLS] d ).</formula><p>(2)</p><p>It encodes the query and document separately with BERT as the encoder g(), using their last layer's [CLS] token representation, and applied dot product (•) on them. This enables offline precomputing of the document encodings and efficient first-stage retrieval. In comparison, the BERT reranker <ref type="bibr">(Nogueira et al., 2019)</ref> applies BERT on the concatenation of each to-rerank query-document pair: BERT(q • d), which has explicit access to term level interactions between query-document with transformer attentions, but is often infeasible in first stage retrieval as enumerating all documents in the corpus for each query is too costly.</p><p>Learning with Negative Sampling: The effectiveness of DR resides in learning a good representation space that maps query and relevant documents together, while separating irrelevant ones. The learning of this representation often follows standard learning to rank <ref type="bibr" target="#b33">(Liu, 2009)</ref>: Given a query q, a set of its relevant document D + q and irrelevant ones D − q , find the best θ * that:</p><formula xml:id="formula_2">θ * = argmin θ q d + ∈D + q d − ∈D − q l(f (q, d + ), f (q, d − )).<label>(3)</label></formula><p>The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).</p><p>A unique challenge in dense retrieval, targeting first stage retrieval, is that the irrelevant documents to separate are from the entire corpus (D − q = C \ D + q ). This often leads to millions of negative instances, which have to be sampled in training:</p><formula xml:id="formula_3">θ * = argmin θ q d + ∈D + d − ∈ D− l(f (q, d + ), f (q, d − )).</formula><p>(4)</p><p>Here we start to omit the subscript q in D q . All D + and D − are query dependent. A natural choice is to sample negatives D− from top documents retrieved by BM25. However, they may bias the DR model to merely mimic sparse retrieval <ref type="bibr" target="#b35">(Luan et al., 2020)</ref>. Another way is to sample negatives in local mini-batches, e.g., as in contrastive learning <ref type="bibr" target="#b42">(Oord et al., 2018)</ref>, however, these local negatives do not significantly outperform BM25 negatives <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING</head><p>In this section, we theoretically analyze the convergence of dense retrieval training. We first show the connections between learning convergence and gradient norms (Sec. 3.1), then how local negatives in dense retrieval yield less optimal convergence (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ORACLE NEGATIVE SAMPLING ACCORDING TO PER-INSTANCE GRADIENT-NORM</head><p>Let l(d + , d − ) = l(f (q, d + ), f (q, d − ) be the loss function on the training triple (q, d + , d − ), P D − the negative sampling distribution for the given (q, d + ), and p d − the sampling probability of negative instance d − , a stochastic gradient decent (SGD) step with importance sampling <ref type="bibr" target="#b0">(Alain et al., 2015)</ref> is:</p><formula xml:id="formula_4">θ t+1 = θ t − η 1 N p d − ∇ θt l(d + , d − ),<label>(5)</label></formula><p>with θ t the parameter at t-th step, θ t+1 the one after, and N the total number of negatives. The scaling factor 1 N p d − ensures Eqn. 5 is an unbiased estimator of the non-stochastic gradient on the full data. Then we can characterize the converge rate of this SGD step as the movement to optimal θ * . Following derivations in variance reduction <ref type="bibr" target="#b25">(Katharopoulos &amp; Fleuret, 2018;</ref><ref type="bibr" target="#b22">Johnson &amp; Guestrin, 2018)</ref>, let</p><formula xml:id="formula_5">g d − = 1 N p d − ∇ θt l(d + , d −</formula><p>) the weighted gradient, the convergence rate is:</p><formula xml:id="formula_6">E∆ t = ||θ t − θ * || 2 − E P D − (||θ t+1 − θ * || 2 ) (6) = ||θ t || 2 − 2θ T t θ * − E P D − (||θ t − ηg d − || 2 ) + 2θ * T E P D − (θ t − ηg d − ) (7) = −η 2 E P D − (||g d − || 2 ) + 2ηθ T t E P D − (g d − ) − 2ηθ * T E P D − (g d − ) (8) = 2ηE P D − (g d − ) T (θ t − θ * ) − η 2 E P D − (||g d − || 2 ) (9) = 2ηE P D − (g d − ) T (θ t − θ * ) − η 2 E P D − (g d − ) T E P D − (g d − ) − η 2 Tr(V P D − (g d − )).</formula><p>(10) This shows we can obtain better convergence rate by sampling from a distribution P D − that minimizes the variance of the stochastic gradient estimator</p><formula xml:id="formula_7">E P D − (||g d − || 2 ), or Tr(V P D − (g d − )</formula><p>) as the estimator is unbiased. The variance reflects how good the stochastic gradient from negative sampling represents the full gradient on all negatives-the latter is ideal but infeasible thus sampling is required. Intuitively, we prefer the stochastic estimator to be stable and have smaller variances.</p><p>A well known result in importance sampling <ref type="bibr" target="#b0">(Alain et al., 2015;</ref><ref type="bibr" target="#b22">Johnson &amp; Guestrin, 2018)</ref> is that there exists an optimal distribution that: To prove this, one can apply Jensen's inequality on the gradient variance and verify that Eqn. 11 achieves the minimum. The detailed derivations can be find in <ref type="bibr" target="#b22">Johnson &amp; Guestrin (2018)</ref>.</p><formula xml:id="formula_8">p * d − ∝ ||∇ θt l(d + , d − )|| 2 = argmin p d − Tr(V P D − (g d − )). (<label>11</label></formula><formula xml:id="formula_9">) q 𝑑 ! 𝐷 ! ! "# " Trainer Inferencer q 𝑑 ! 𝐷 ! ! "$ " Checkpoint k-1 … Checkpoint k q 𝑑 ! 𝐷 ! ! "$ " q 𝑑 ! 𝐷 ! ! " … Checkpoint k+1 q 𝑑 ! 𝐷 ! ! "# " … Inferencing Index &amp; Search Training Positives ANCE Negatives Index &amp; Search</formula><p>Eqn. 11 shows that the convergence rate can be improved by sampling negatives proportional to their per-instance gradient norms (though too expensive to calculate). Intuitively, an negative instance with larger gradient norm is more likely to reduce the non-stochastic training loss more, thus should be sampled more frequently than those with diminishing gradients. The correlation of larger gradient norm and better training convergence is also observed in BERT fine-tuning <ref type="bibr" target="#b39">(Mosbach et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UNINFORMATIVE IN-BATCH NEGATIVES AND THEIR DIMINISHING GRADIENTS</head><p>Diminishing Gradients of Uninformative Negatives: Though the close form of gradient norms often does not exist, <ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret (2018)</ref> derives the following upper bound:</p><formula xml:id="formula_10">||∇ θt l(d + , d − )|| 2 ≤ Lρ||∇ φ L l(d + , d − )|| 2 , (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>where L is the number of layers, ρ is composed by pre-activation weights and gradients in intermediate layers, and ||∇ φ L l(d + , d − )|| 2 is the gradient on the last layer. The derivation of this upper bound is on multi-layer perception with any depths and any activation function that is Lipschitz continuous <ref type="bibr" target="#b25">(Katharopoulos &amp; Fleuret, 2018)</ref>. On complicated neural networks, the intermediate layers are regulated by various normalization and this upper bound often holds empirically (Sec. 6.3).</p><p>In addition, for many loss functions, for example, BCE loss and pairwise hinge loss, we can verify that when the loss goes to zero the gradient norm of the last layer also goes to zero: <ref type="bibr" target="#b25">Katharopoulos &amp; Fleuret, 2018)</ref>.</p><formula xml:id="formula_12">l(d + , d − ) → 0 ⇒ ||∇ φ L l(d + , d − )|| 2 → 0 (</formula><p>Putting everything together, using uninformative negative samples with near zero loss results in the following chain of undesirable properties:</p><formula xml:id="formula_13">||∇ φ L l(d + , d − )|| 2 → 0 low upper bound ⇒ ||∇ θt l(d + , d − )|| 2 → 0 diminishing gradient norm ⇒ Tr(V P D − (g d − )) ↑ large scholastic variance ⇒ E∆ t ↓ . slow convergence (13)</formula><p>The uninformative negative samples yield diminishing gradient norms, larger variances of the scholastic gradient estimator, and less optimal learning convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inefficacy of Local</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION</head><p>Our analyses show the importance, if not necessity, to construct negatives globally from the corpus to avoid uninformative negatives for better learning convergence. In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation (ANCE), which selects negatives from the entire corpus using an asynchronously updated ANN index.</p><p>ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:</p><formula xml:id="formula_14">θ * = argmin θ q d + ∈D + d − ∈D − ANCE l(f (q, d + ), f (q, d − )),<label>(14)</label></formula><p>with ANCE can pair with many DR models. For simplicity, we use BERT-Siamese (Eqn. 2), with shared encoder weights between q and d and negative log likelihood (NLL) loss <ref type="bibr" target="#b35">(Luan et al., 2020)</ref>.</p><formula xml:id="formula_15">D − ANCE = ANN f (q,d) \ D + and ANN f (q,d</formula><p>Asynchronous Index Refresh: During stochastic training, the DR model f () is updated each minibatch. Maintaining an update-to-date ANN index to select fresh ANCE negatives is challenging, as the index update requires two operations: 1) Inference: refresh the representations of all documents in the corpus with an updated DR model; 2) Index: rebuild the ANN index using updated representations. Although Index is efficient <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>, Inference is too expensive to compute per batch as it requires a forward pass on a corpus much bigger than a training batch.</p><p>Thus we implement an asynchronous index refresh similar to <ref type="bibr" target="#b17">Guu et al. (2020)</ref>, and update the ANN index once every m batches, i.e., with checkpoint f k . As illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>, besides the Trainer, we run an Inferencer that takes the latest checkpoint (e.g., f k ) and recomputes the encodings of the entire corpus. In parallel, the Trainer continues its stochastic learning using</p><formula xml:id="formula_16">D − f k−1 from ANN f k−1 .</formula><p>Once the corpus is re-encoded, the Inferencer updates the index (ANN f k ) and feed it to the Trainer. In this process, the ANCE negatives (D − ANCE ) are asynchronously updated to "catch up" with the stochastic training, with an async-gap determined by the computing resources allocated to the Inferencer. Our experiment in Sec 6.4 studies the influence of this async-gap in learning convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL METHODOLOGIES</head><p>This section describes our experimental setups. More details can be found in Appendix A.1 and A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmarks:</head><p>The web search experiments use the TREC 2019 Deep Learning (DL) Track <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>. It is a standard ad hoc retrieval benchmark: given web queries from Bing, to retrieval passages or documents from the MS MARCO corpus <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>. We use the official setting and focus on the first stage retrieval, but also show results when reranking top 100 BM25 candidates.</p><p>The OpenQA experiments use the Natural Questions (NQ) <ref type="bibr" target="#b28">(Kwiatkowski et al., 2019)</ref> and TriviaQA (TQA) <ref type="bibr" target="#b23">(Joshi et al., 2017)</ref>, following the exact settings from <ref type="bibr" target="#b24">Karpukhin et al. (2020)</ref>. The metrics are Coverage@20/100, which evaluate whether the Top-20/100 retrieved passages include the answer. We also evaluate whether ANCE's better retrieval can propagate to better answer accuracy, by running the state-of-the-art systems' readers on top of ANCE retrieval. The readers are RAG-Token <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> on NQ and DPR Reader on TQA, using their suggested settings.</p><p>We also study the effectiveness of ANCE in the first stage retrieval of a commercial search engine's production system. We change the training of a production-quality DR model to ANCE, and evaluate the offline gains in various corpus sizes, encoding dimensions, and exact/approximate search.</p><p>Baselines: In TREC DL, we include best runs in relevant categories and refer to <ref type="bibr" target="#b6">Craswell et al. (2020)</ref> for more baseline scores. We implement various DR baselines using the same BERT-Siamese (Eqn. 2), but with different training negative construction: random sampling in batch (Rand Neg), random sampling from BM25 top 100 (BM25 Neg) <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b13">Gao et al., 2020b)</ref> and the 1:1 combination of BM25 and Random negatives (BM25 + Rand Neg) <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. We also compare with contrastive learning/Noise Contrastive Estimation, which uses hardest negatives in batch (NCE Neg) <ref type="bibr" target="#b16">(Gutmann &amp; Hyvärinen, 2010;</ref><ref type="bibr" target="#b42">Oord et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2020a)</ref>. In OpenQA, we compare with DPR, BM25, and their combinations <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>.</p><p>Implementation Details: In TREC DL, recent research found MARCO passage training labels cleaner <ref type="bibr" target="#b51">(Yan et al., 2019)</ref> and BM25 negatives can help train dense retrieval <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. Thus, we include a "BM25 Warm Up" setting (BM25 → * ), where the DR models are first trained using MARCO official BM25 Negatives. ANCE is also warmed up by BM25 negatives. All DR models in TREC DL are fine-tuned from RoBERTa base <ref type="bibr" target="#b34">(Liu et al., 2019)</ref>. In OpenQA, we warm up ANCE using the released DPR checkpoints <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref>.</p><p>To fit long documents in BERT-Siamese, ANCE uses the two settings from <ref type="bibr" target="#b9">Dai &amp; Callan (2019b)</ref>, FirstP which uses the first 512 tokens of the document, and MaxP, where the document is split to 512-token passages (maximum 4) and the passage level scores are max-pooled. The max-pooling operation is natively supported in ANN. The ANN search uses the Faiss IndexFlatIP Index <ref type="bibr" target="#b21">(Johnson et al., 2019)</ref>. We use batch size 8 and gradient accumulation step 2 on 4 GPUs. For each positive, we uniformly sample one negative from ANN top 200. We measured ANCE efficiency using one 32GB V100 GPU, Intel(R) Xeon(R) Platinum 8168 CPU and 650GB of RAM memory.</p><p>In asynchronous training, we allocate equal amounts of GPUs to the Trainer and the Inferencer, often four or eight each. The Trainer produces a model checkpoint every 5k or 10k training batches. The Inferencer loads the recent model checkpoint and calculates the embeddings of the corpus in parallel.</p><p>Once the embedding calculation finishes, a new ANN index is built and the Trainer switches to it for negative construction. All their communications are through a shared file system. On MS MARCO, the ANN negative index is refreshed about every 10K training steps. Model NQ TQA T5-11B (Closed) <ref type="bibr" target="#b44">(Roberts et al., 2020)</ref> 34.5 -T5-11B + SSM (Closed) <ref type="bibr" target="#b44">(Roberts et al., 2020)</ref> 36.6 -REALM <ref type="bibr" target="#b17">(Guu et al., 2020)</ref> 40.4 -DPR <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref> 41.5 56.8 DPR + BM25 <ref type="bibr" target="#b24">(Karpukhin et al., 2020)</ref> 39.0 57.0 RAG-Token <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> 44.1 55.2 RAG-Sequence <ref type="bibr" target="#b32">(Lewis et al., 2020b)</ref> 44.5 56.1 ANCE + Reader 46.0 57.5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION RESULTS</head><p>In this section, we first evaluate the effectiveness and efficiency of ANCE. Then we empirically study the convergence of ANCE training and the influence of the asynchronous gap. More analyses can be found in Appendix, including the overlap of dense and sparse retrieval (A.2), hyperparameter study (A.3), and case study (A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EFFECTIVENESS</head><p>In web search (Table <ref type="table" target="#tab_0">1</ref>), ANCE significantly outperforms all sparse retrieval, including the BERTbased DeepCT <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>. Among DR models with different training strategies, ANCE is the only one robustly exceeding sparse methods in document retrieval. In OpenQA, ANCE outperforms DPR and its fusion with BM25 (DPR+BM25) in retrieval accuracy (Table <ref type="table" target="#tab_1">2</ref>). It also improves end-to-end QA accuracy, using the same QA components with previous state-of-the-arts but ANCE's better retrieval (Table <ref type="table" target="#tab_2">4</ref>). ANCE's effectiveness is even more observed in real production (Table <ref type="table">3</ref>). Among all DR models, ANCE has the smallest gap between its retrieval and reranking accuracy, showing the importance of global negatives in training retrieval models. ANCE retrieval nearly matches the accuracy of the cascade IR with interaction-based BERT Reranker <ref type="bibr">(Nogueira &amp; Cho, 2019)</ref>, even though BERT-Siamese does not explicitly capture term-level interactions. With ANCE, we can learn a representation space that effectively captures the finesse of search relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EFFICIENCY</head><p>The efficiency of ANCE (FirstP) in TREC DL doc retrieval is shown in Table <ref type="table" target="#tab_3">5</ref>. In serving, we measure the online latency to retrieve/rank 100 documents per query, in the batch setting. DR provides a 100x speed up compared to BERT Rerank. This is a natural benefit of BERT-Siamese, where the document encodings are calculated offline and separately with the query. Only query encoding and the dot product are online. In comparison, the interaction-based BERT Reranker runs BERT 100 times, once per query and each of the 100 candidate documents. Its expensive online operation often  requires caching and distillation. In training, the bulk of computing is to recalculate the encoding of the corpus for ANCE negative construction. Assuming the same DR model is used to construct negatives from the same corpus, this is inevitable but can be mitigated by asynchronous index refresh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">EMPIRICAL ANALYSES ON TRAINING CONVERGENCE</head><p>We first show the long tail distribution of search relevance in dense retrieval. As plotted in Fig. <ref type="figure" target="#fig_3">3</ref>, there are a few instances per query with significant higher retrieval scores, while the majority form a long tail. In retrieval/ranking, the key challenge is to distinguish the relevant ones among those highest scored ones; the rest is trivially irrelevant. We also empirically measure the probability of local in-batch negatives including informative negatives (D − * ), by their overlap with top 100 highest scored negatives. This probability, either using NCE Neg or Rand Neg, is zero, the same as our theory shows. In comparison, the overlap between BM25 Neg with top dense retrieved negatives is 15%, while that of ANCE negatives starts at 63% and converges to 100% by design.</p><p>Then we empirically validate our theory that local negatives lead to lower loss, bounded gradient norm, non-ideal importance sampling, and thus slow convergence (Eqn. 13). The training loss and pre-clip gradient norms during DR training are plotted in Fig. <ref type="figure" target="#fig_4">4</ref>. As expected, the uninformative local negatives resulted in near-zero gradient norms, while ANCE global negatives maintain a higher gradient norm. The gradient norm of the last layer in the BERT-Siamese model during ANCE training (black dotted lines in Fig. <ref type="figure" target="#fig_4">4</ref>) is consistently bigger than the other layers, which empirically aligns with the upper bound in Eqn. 12. Also as our theory suggests, the gradient norms of local negatives are bounded close to zero, while those of ANCE global negatives are bigger by orders of magnitude. This confirms ANCE better approximates the oracle importance sampling distribution</p><formula xml:id="formula_17">(p * d − ∝ ||∇ θt l(d + , d − )|| 2</formula><p>) and improves learning convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">IMPACT OF ASYNCHRONOUS GAP</head><p>The need for global negatives enforces an asynchronous gap (async-gap) in ANCE training: The negatives are selected using the encodings from an earlier stage of the being optimized DR model. The async-gap is determined by the target index refreshing rate, which is determined by the allocation of computing resource on the Trainer versus the Inferencer, as well as the learning rate. This experiment studies the impact of this async-gap in ANCE training. The training curves and testing NDCG of different configurations are plotted in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>A too large async-gap, either from large learning rate (Fig. <ref type="figure" target="#fig_5">5</ref> In many scenarios, using a same amount of extra GPUs for ANCE as a one-time training cost is a good return of investment. The efficiency bottleneck in production is often in inference and serving, where ANCE thrives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>In early research on neural information retrieval (Neu-IR) <ref type="bibr" target="#b38">(Mitra et al., 2018)</ref>, a common belief was that the interaction models, those that explicitly handle term level matches, are more effective though more expensive <ref type="bibr" target="#b14">(Guo et al., 2016;</ref><ref type="bibr" target="#b49">Xiong et al., 2017;</ref><ref type="bibr">Nogueira &amp; Cho, 2019)</ref>. Many techniques are developed to reduce their cost, for example, distillation <ref type="bibr" target="#b12">(Gao et al., 2020a)</ref> and caching <ref type="bibr" target="#b20">(Humeau et al., 2020;</ref><ref type="bibr" target="#b27">Khattab &amp; Zaharia, 2020;</ref><ref type="bibr" target="#b37">MacAvaney et al., 2020)</ref>. ANCE shows that a properly trained representation-based BERT-Siamese is in fact as effective as the interaction-based BERT ranker. This finding will motivate many new research explorations in Neu-IR.</p><p>Deep learning has been used to improve various components of sparse retrieval, for example, term weighting <ref type="bibr" target="#b9">(Dai &amp; Callan, 2019b)</ref>, query expansion <ref type="bibr" target="#b53">(Zheng et al., 2020)</ref>, and document expansion <ref type="bibr">(Nogueira et al., 2019)</ref>. Dense Retrieval chooses a different path and conducts retrieval purely in the embedding space via ANN search <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b2">Chang et al., 2020;</ref><ref type="bibr" target="#b24">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b35">Luan et al., 2020)</ref>. This work demonstrates that a simple dense retrieval system can achieve SOTA accuracy, while also behaves dramatically different from classic retrieval. The recent advancement in dense retrieval may raise a new generation of search systems.</p><p>Recent research in contrastive representation learning also shows the benefits of sampling negatives from a larger candidate pool. In computer vision, <ref type="bibr" target="#b18">He et al. (2019)</ref> decouple the negative sampling pool size with training batch size, by maintaining a negative candidate pool of recent batches and updating their representation with momentum. This enlarged negative pool significantly improves unsupervised visual representation learning <ref type="bibr" target="#b5">(Chen et al., 2020b)</ref>. A parallel work <ref type="bibr" target="#b50">(Xiong et al., 2020)</ref> improves DPR by sampling negatives from a memory bank <ref type="bibr" target="#b48">(Wu et al., 2018)</ref> -in which the representations of negative candidates are frozen so more candidates can be stored. Instead of a bigger local pool, ANCE goes all the way along this trajectory and constructs negatives globally from the entire corpus, using an asynchronously updated ANN index.</p><p>Besides being a real world application itself, dense retrieval is also a core component in many other language systems, for example, to retrieval relevant information for grounded language models <ref type="bibr" target="#b26">(Khandelwal et al., 2019;</ref><ref type="bibr" target="#b17">Guu et al., 2020)</ref>, extractive/generative QA <ref type="bibr" target="#b24">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b32">Lewis et al., 2020b)</ref>, and fact verification <ref type="bibr" target="#b50">(Xiong et al., 2020)</ref>, or to find paraphrase pairs for pretraining <ref type="bibr" target="#b31">(Lewis et al., 2020a)</ref>. There dense retrieval models are either frozen or optimized indirectly by signals from their end tasks. ANCE is orthogonal with those lines of research and focuses on the representation learning for dense retrieval. Its better retrieval accuracy can benefit many language systems.   <ref type="table" target="#tab_4">8</ref>.</p><p>relevant to the query, for example, "yoga pose" for "bow in yoga". In other cases, ANCE retrieved wrong documents due to the lack of the domain knowledge: the pretrained language model may not know "active margin" is a geographical terminology, not a financial one (which we did not know ourselves and took some time to figure out when conducting this case study). There are also some cases where the dense retrieved documents make sense to us but were labeled irrelevant.</p><p>The t-SNE plots in Fig. <ref type="figure" target="#fig_7">6</ref> and Fig. <ref type="figure" target="#fig_8">7</ref> show many interesting patterns of the learned representation space. The ANCE winning cases often correspond to clear separations of different document groups. For losing cases the representation space is more mixed, or there is too few relevant documents which may cause the variances in   <ref type="table" target="#tab_5">9</ref>.</p><p>model performances. There are also many different interesting patterns in the ANCE-learned representation space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in the corpus and once finished, refreshes the ANN index with most up-to-date encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>In-Batch Negatives: We argue that, when training DR models, the in-batch local negatives are unlikely to provide informative samples due to two properties of text retrieval. Let D − * be the set of informative negatives that are hard to distinguish from D + , and b be the batch size, we have (1) b |C|, the batch size is far smaller than the corpus size; (2) |D − * | |C|, that only a few negatives are informative and the majority of corpus is trivially unrelated. Both conditions can be verified empirically in dense retrieval. The two together make the probability that a random mini-batch includes meaningful negatives (p = b|D − * | |C| 2 ) close to zero. Selecting negatives from local training batches unlikely provides optimal training signals for dense retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) the top retrieved documents by f () from the ANN index. By definition, D − ANCE are the hardest negatives for the current DR model: D − ANCE ≈ D − * . In theory, these more informative negatives have higher training loss, elevate the upper bound on the gradient norms (first component of Eqn 13), and prevent the slow convergence indicated in Eqn 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The top DR scores for 10 random TREC DL testing queries. The x-axes are their ranking order. The y-axes are their retrieval scores minus corpus average. All models are warmed up by BM25 Neg. The percentages are the overlaps between the testing and training negatives near convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The loss and gradient norms during DR training (after BM25 warm up). The gradient norms are the per-layer average of the bottom (1-4), middle (5-8), and top (9-12) transformer layers. Black dotted lines are the grad norm of the last layer in ANCE (FirstP). The x-axes are training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training loss and testing NDCG of ANCE (FirstP) on documents.The sub captions list the ANN index refreshing rate (e.g., per 10k Batch), Trainer:Inferencer GPU allocation (e.g., 4:4), and learning rate (e.g., 1e-5). The x-axes are the training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a)) or low refreshing rate (Fig.5(b)), makes the training unstable, perhaps because the refreshed index changes too dramatically, as indicated by the peaks in training loss and dips of testing NDCG. The async-gap is not significant when we allocate an equal amount of GPUs to the index refreshing and to the training (Fig.5(d)). Further reducing the gap (Fig.5(c)) does not improve learning convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: t-SNE Plots for Winning Cases in Table8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: t-SNE Plots for Losing Cases in Table9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results in TREC 2019 Deep Learning Track. Results not available are marked as "n.a.", not applicable are marked as "-". Best results in each category are marked bold. Dense Retrieval baselines use the same BERT-Siamese but different training strategies.</figDesc><table><row><cell></cell><cell cols="2">MARCO Dev</cell><cell cols="2">TREC DL Passage</cell><cell cols="2">TREC DL Document</cell></row><row><cell></cell><cell cols="2">Passage Retrieval</cell><cell cols="2">NDCG@10</cell><cell cols="2">NDCG@10</cell></row><row><cell></cell><cell cols="5">MRR@10 Recall@1k Rerank Retrieval Rerank</cell><cell>Retrieval</cell></row><row><cell>Sparse &amp; Cascade IR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>0.240</cell><cell>0.814</cell><cell>-</cell><cell>0.506</cell><cell>-</cell><cell>0.519</cell></row><row><cell>Best DeepCT</cell><cell>0.243</cell><cell>n.a.</cell><cell>-</cell><cell>n.a.</cell><cell>-</cell><cell>0.554</cell></row><row><cell>Best TREC Trad Retrieval</cell><cell>0.240</cell><cell>n.a.</cell><cell>-</cell><cell>0.554</cell><cell>-</cell><cell>0.549</cell></row><row><cell>BERT Reranker</cell><cell>-</cell><cell>-</cell><cell>0.742</cell><cell>-</cell><cell>0.646</cell><cell>-</cell></row><row><cell>Dense Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rand Neg</cell><cell>0.261</cell><cell>0.949</cell><cell>0.605</cell><cell>0.552</cell><cell>0.615</cell><cell>0.543</cell></row><row><cell>NCE Neg</cell><cell>0.256</cell><cell>0.943</cell><cell>0.602</cell><cell>0.539</cell><cell>0.618</cell><cell>0.542</cell></row><row><cell>BM25 Neg</cell><cell>0.299</cell><cell>0.928</cell><cell>0.664</cell><cell>0.591</cell><cell>0.626</cell><cell>0.529</cell></row><row><cell>DPR (BM25 + Rand Neg)</cell><cell>0.311</cell><cell>0.952</cell><cell>0.653</cell><cell>0.600</cell><cell>0.629</cell><cell>0.557</cell></row><row><cell>BM25 → Rand</cell><cell>0.280</cell><cell>0.948</cell><cell>0.609</cell><cell>0.576</cell><cell>0.637</cell><cell>0.566</cell></row><row><cell>BM25 → NCE Neg</cell><cell>0.279</cell><cell>0.942</cell><cell>0.608</cell><cell>0.571</cell><cell>0.638</cell><cell>0.564</cell></row><row><cell>BM25 → BM25 + Rand</cell><cell>0.306</cell><cell>0.939</cell><cell>0.648</cell><cell>0.591</cell><cell>0.626</cell><cell>0.540</cell></row><row><cell>ANCE (FirstP)</cell><cell>0.330</cell><cell>0.959</cell><cell>0.677</cell><cell>0.648</cell><cell>0.641</cell><cell>0.615</cell></row><row><cell>ANCE (MaxP)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.671</cell><cell>0.628</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Table 3: Relative gains in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">first stage retrieval of a commercial</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">search engine. The gains are from</cell></row><row><cell></cell><cell cols="2">Single Task</cell><cell cols="2">Multi Task</cell><cell cols="4">changing the training of a produc-</cell></row><row><cell></cell><cell>NQ</cell><cell>TQA</cell><cell>NQ</cell><cell>TQA</cell><cell cols="3">tion DR model to ANCE.</cell></row><row><cell>Retriever</cell><cell cols="4">Top-20/100 Top-20/100 Top-20/100 Top-20/100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>59.1/73.7</cell><cell>66.9/76.7</cell><cell>-/-</cell><cell>-/-</cell><cell cols="3">Corpus Size Dim Search</cell><cell>Gain</cell></row><row><cell>DPR</cell><cell>78.4/85.4</cell><cell>79.4/85.0</cell><cell>79.4/86.0</cell><cell>78.8/84.7</cell><cell>250 Million</cell><cell>768</cell><cell cols="2">KNN +18.4%</cell></row><row><cell>BM25+DPR</cell><cell>76.6/83.8</cell><cell>79.8/84.5</cell><cell>78.0/83.9</cell><cell>79.9/84.4</cell><cell>8 Billion</cell><cell>64</cell><cell cols="2">KNN +14.2%</cell></row><row><cell>ANCE</cell><cell>81.9/87.5</cell><cell>80.3/85.3</cell><cell>82.1/87.9</cell><cell>80.3/85.2</cell><cell>8 Billion</cell><cell>64</cell><cell cols="2">ANN +15.5%</cell></row></table><note>Retrieval results (Answer Coverage at Top-20/100) on Natural Questions (NQ) and Trivial QA (TQA) in the setting from<ref type="bibr" target="#b24">Karpukhin et al. (2020)</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>OpenQA Test Scores in Single Task Setting. ANCE+Reader switches the retrieve of the OpenQA systems from DPR to ANCE and keeps their QA components, which is RAG-Token on Natural Questions (NQ) and DPR Reader on Trivia QA (TQA). T5 results are "closed-book". The others are open-book.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Efficiency of ANCE Serving and Training.</figDesc><table><row><cell>Operation</cell><cell cols="2">Offline Online</cell></row><row><cell>BM25 Index Build</cell><cell>3h</cell><cell>-</cell></row><row><cell>BM25 Retrieval</cell><cell>-</cell><cell>37ms</cell></row><row><cell>BERT Rerank</cell><cell>-</cell><cell>1.15s</cell></row><row><cell>Sparse IR Total (BM25 + BERT)</cell><cell>-</cell><cell>1.42s</cell></row><row><cell>ANCE Inference</cell><cell></cell><cell></cell></row><row><cell>Encoding of Corpus/Per doc</cell><cell>10h/4.5ms</cell><cell>-</cell></row><row><cell>Query Encoding</cell><cell>-</cell><cell>2.6ms</cell></row><row><cell>ANN Retrieval (batched q)</cell><cell>-</cell><cell>9ms</cell></row><row><cell>Dense Retrieval Total</cell><cell cols="2">-11.6ms</cell></row><row><cell>ANCE Training</cell><cell></cell><cell></cell></row><row><cell>Encoding of Corpus/Per doc</cell><cell>10h/4.5ms</cell><cell>-</cell></row><row><cell>ANN Index Build</cell><cell>10s</cell><cell>-</cell></row><row><cell>Neg Construction Per Batch</cell><cell>72ms</cell><cell>-</cell></row><row><cell>Back Propagation Per Batch</cell><cell>19ms</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 :</head><label>8</label><figDesc>Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs better than BM25. Snippets are manually extracted. The documents in the first disagreed ranking position are shown, where on all examples ANCE won. The NDCG@10 of ANCE and BM25 in the corresponding query is listed.</figDesc><table><row><cell></cell><cell>ANCE</cell><cell></cell><cell>BM25</cell></row><row><cell>Query:</cell><cell cols="3">qid (104861): Cost of interior concrete flooring</cell></row><row><cell>Title:</cell><cell cols="3">Concrete network: Concrete Floor Cost Pinterest: Types of Flooring</cell></row><row><cell>DocNo:</cell><cell>D293855</cell><cell></cell><cell>D2692315</cell></row><row><cell>Snippet:</cell><cell cols="2">For a concrete floor with a basic finish,</cell><cell>Know About Hardwood Flooring And</cell></row><row><cell></cell><cell cols="2">you can expect to pay $2 to $12 per</cell><cell>Its Types White Oak Floors Oak Floor-</cell></row><row><cell></cell><cell>square foot. . .</cell><cell></cell><cell>ing Laminate Flooring In Bathroom . . .</cell></row><row><cell cols="2">Ranking Position: 1</cell><cell></cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell cols="2">3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.86</cell><cell></cell><cell>0.15</cell></row><row><cell>Query:</cell><cell cols="3">qid (833860): What is the most popular food in Switzerland</cell></row><row><cell>Title:</cell><cell cols="2">Wikipedia: Swiss cuisine</cell><cell>Answers.com: Most popular traditional</cell></row><row><cell></cell><cell></cell><cell></cell><cell>food dishes of Mexico</cell></row><row><cell>DocNo:</cell><cell>D1927155</cell><cell></cell><cell>D3192888</cell></row><row><cell>Snippet:</cell><cell cols="2">Swiss cuisine bears witness to many re-</cell><cell>One of the most popular traditional Mex-</cell></row><row><cell></cell><cell cols="2">gional influences, . . . Switzerland was</cell><cell>ican deserts is a spongy cake . . . (in</cell></row><row><cell></cell><cell cols="2">historically a country of farmers, so tra-</cell><cell>the related questions section) What is</cell></row><row><cell></cell><cell cols="2">ditional Swiss dishes tend not to be. . .</cell><cell>the most popular food dish in Switzer-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>land?. . .</cell></row><row><cell cols="2">Ranking Position: 1</cell><cell></cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell cols="2">3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.90</cell><cell></cell><cell>0.14</cell></row><row><cell>Query:</cell><cell cols="2">qid (1106007): Define visceral</cell></row><row><cell>Title:</cell><cell cols="2">Vocabulary.com: Visceral</cell><cell>Quizlet.com: A&amp;P EX3 autonomic 9-10</cell></row><row><cell>DocNo:</cell><cell>D542828</cell><cell></cell><cell>D830758</cell></row><row><cell>Snippet:</cell><cell cols="2">When something's visceral, you feel it</cell><cell>Acetylcholine A neurotransmitter liber-</cell></row><row><cell></cell><cell cols="2">in your guts. A visceral feeling is in-</cell><cell>ated by many peripheral nervous system</cell></row><row><cell></cell><cell cols="2">tuitive -there might not be a rational</cell><cell>neurons and some central nervous sys-</cell></row><row><cell></cell><cell cols="2">explanation, but you feel that you know</cell><cell>tem neurons. . .</cell></row><row><cell></cell><cell>what's best. . .</cell><cell></cell></row><row><cell cols="2">Ranking Position: 1</cell><cell></cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell cols="2">3 (Very Relevant)</cell><cell>0 (Irrelevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.80</cell><cell></cell><cell>0.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Query Relevant ANCE Neg BM25 Neg Rand Neg</cell></row><row><cell cols="2">(a) 104861: interior flooring cost.</cell><cell cols="2">(b) 833860: popular Swiss food</cell><cell>(c) 1106007: define visceral</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 :</head><label>9</label><figDesc>Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs worse than BM25. Snippets are manually extracted. The documents in the first position where BM25 wins are shown. The NDCG@10 of ANCE and BM25 in the corresponding query is listed. Typos in the query are from the real web search queries in TREC.</figDesc><table><row><cell></cell><cell>ANCE</cell><cell></cell><cell>BM25</cell></row><row><cell>Query:</cell><cell cols="3">qid (182539): Example of monotonic function</cell></row><row><cell>Title:</cell><cell cols="2">Wikipedia: Monotonic function</cell><cell>Explain Extended: Things SQL needs:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sargability of monotonic functions</cell></row><row><cell>DocNo:</cell><cell>D510209</cell><cell></cell><cell>D175960</cell></row><row><cell>Snippet:</cell><cell cols="2">In mathematics, a monotonic function</cell><cell>I'm going to write a series of articles</cell></row><row><cell></cell><cell cols="2">(or monotone function) is a function be-</cell><cell>about the things SQL needs to work</cell></row><row><cell></cell><cell cols="2">tween ordered sets that preserves or re-</cell><cell>faster and more efficienly. . .</cell></row><row><cell></cell><cell cols="2">verses the given order... For example, if</cell></row><row><cell></cell><cell cols="2">y=g(x) is strictly monotonic on the range</cell></row><row><cell></cell><cell>[a,b] . . .</cell><cell></cell></row><row><cell cols="2">Ranking Position: 1</cell><cell></cell><cell>1</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell></cell><cell>2 (Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.25</cell><cell></cell><cell>0.61</cell></row><row><cell>Query:</cell><cell cols="2">qid (1117099): What is a active margin</cell></row><row><cell>Title:</cell><cell cols="2">Wikipedia: Margin (finance)</cell><cell>Yahoo Answer: What is the difference</cell></row><row><cell></cell><cell></cell><cell></cell><cell>between passive and active continental</cell></row><row><cell></cell><cell></cell><cell></cell><cell>margins</cell></row><row><cell>DocNo:</cell><cell>D166625</cell><cell></cell><cell>D2907204</cell></row><row><cell>Snippet:</cell><cell cols="2">In finance, margin is collateral that the</cell><cell>An active continental margin is found on</cell></row><row><cell></cell><cell cols="2">holder of a financial instrument . . .</cell><cell>the leading edge of the continent where</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell></row><row><cell cols="2">Ranking Position: 2</cell><cell></cell><cell>2</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell></cell><cell>3 (Very Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.44</cell><cell></cell><cell>0.74</cell></row><row><cell>Query:</cell><cell cols="3">qid (1132213): How long to hold bow in yoga</cell></row><row><cell>Title:</cell><cell cols="2">Yahoo Answer: How long should you</cell><cell>yogaoutlet.com: How to do bow pose in</cell></row><row><cell></cell><cell cols="2">hold a yoga pose for</cell><cell>yoga</cell></row><row><cell>DocNo:</cell><cell>D3043610</cell><cell></cell><cell>D3378723</cell></row><row><cell>Snippet:</cell><cell cols="2">so i've been doing yoga for a few weeks</cell><cell>Bow Pose is an intermediate yoga back-</cell></row><row><cell></cell><cell cols="2">now and already notice that my flexi-</cell><cell>bend that deeply opens the chest and the</cell></row><row><cell></cell><cell cols="2">ablity has increased drastically. . . . That</cell><cell>front of the body. . . Hold for up to 30</cell></row><row><cell></cell><cell cols="2">depends on the posture itself . . .</cell><cell>seconds . . .</cell></row><row><cell cols="2">Ranking Position: 3</cell><cell></cell><cell>3</cell></row><row><cell>TREC Label:</cell><cell>0 (Irrelevant)</cell><cell></cell><cell>3 (Very Relevant)</cell></row><row><cell>NDCG@10:</cell><cell>0.66</cell><cell></cell><cell>0.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Query Relevant ANCE Neg BM25 Neg Rand Neg</cell></row><row><cell cols="2">(a) 182539: monotonic function</cell><cell cols="2">(b) 1117099: active margin</cell><cell>(c) 1132213: yoga bow</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code and trained models are in the supplementary material and will be open-sourced.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1">CONCLUSIONIn this paper, we first provide theoretical analyses on the convergence of representation learning in dense retrieval. We show that under common conditions in text retrieval, the local negatives used in DR training are uninformative, yield low gradient norms, and contribute little to the learning convergence. We then propose ANCE to eliminate this bottleneck by constructing training negatives globally from the entire corpus. Our experiments demonstrate the advantage of ANCE in web search, OpenQA, and the production system of a commercial search engine. Our studies empirically validate our theory that ANCE negatives have much bigger gradient norms, reduce the stochastic gradient variance, and improve training convergence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/facebookresearch/DPR.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://huggingface.co/transformers/master/model d oc/rag.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 MORE EXPERIMENTAL DETAILS</head><p>More Details on TREC DL Benchmarks: There are two tasks in the TREC DL 2019 Track: document retrieval and passage retrieval. The training and development sets are from MS MARCO, which includes passage level relevance labels for one million Bing queries <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>. The document corpus was post-constructed by back-filling the body texts of the passage's URLs and their labels were inherited from its passages <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>. The testing sets are labeled by NIST accessors on the top 10 ranked results from past Track participants <ref type="bibr" target="#b6">(Craswell et al., 2020)</ref>.</p><p>TREC DL official metrics include NDCG@10 on test and MRR@10 on MARCO Passage Dev. MARCO Document Dev is noisy and the recall on the DL Track testing is less meaningful due to low label coverage on DR results. There is a two-year gap between the construction of the passage training data and the back-filling of their full document content. Some original documents were no longer available. There is also a decent amount of content changes in those documents during the two-year gap, and many no longer contain the passages. This back-filling perhaps is the reason why many Track participants found the passage training data is more effective than the inherited document labels. Note that the TREC testing labels are not influenced as the annotators were provided the same document contents when judging.</p><p>All the TREC DL runs are trained using these training data. Their inference results on the testing queries of the document and the passage retrieval tasks were evaluated by NIST assessors in the standard TREC-style pooling technique <ref type="bibr" target="#b46">(Voorhees, 2000)</ref>. The pooling depth is set to 10, that is, the top 10 ranked results from all participated runs are evaluated, and these evaluated labels are released as the official TREC DL benchmarks for passage and document retrieval tasks.</p><p>More Details on OpenQA Experiments: All the DPR related experimental settings, baseline systems, and DPR Reader are based on their open source libarary 2 . The RAG-Token reader uses their open-source release in huggingface 3 . The RAG-Seq release in huggingface is not yet stable by the time we did our experiment, thus we choose the RAG-Token in our OpenQA experiment. RAG only releases the NQ models thus we use DPR reader on TriviaQA. We feed top 20 passages from ANCE to RAG-Token on NQ and top 100 passages to DPR's BERT Reader, following the guideline in their open-source codes.</p><p>More Details on Baselines: The most representative sparse retrieval baselines in TREC DL include the standard BM25 ("bm25base" or "bm25base_p"), Best TREC Sparse Retrieval ("bm25tuned_rm3" or "bm25tuned_prf_p") with tuned query expansion <ref type="bibr" target="#b29">(Lavrenko &amp; Croft, 2017)</ref>, and Best DeepCT ("dct_tp_bm25e2", doc only), which uses BERT to estimate the term importance for BM25 <ref type="bibr" target="#b8">(Dai &amp; Callan, 2019a)</ref>. These three runs represent the standard sparse retrieval, best classical sparse retrieval, and the recent progress of using BERT to improve sparse retrieval. We also include the standard cascade retrieval-and-reranking systems BERT Reranker ("bm25exp_marcomb" or "p_exp_rm3_bert"), which is the best run using standard BERT on top of query/doc expansion, from the groups with multiple top MARCO runs <ref type="bibr">(Nogueira &amp; Cho, 2019;</ref><ref type="bibr">Nogueira et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Siamese Configurations:</head><p>We follow the network configurations in <ref type="bibr" target="#b35">Luan et al. (2020)</ref> in all Dense Retrieval methods, which we found provides the most stable results. More specifically, we initialize the BERT-Siamese model with RoBERTa base <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> and add a 768 × 768 projection layer on top of the last layer's "[CLS]" token, followed by a layer norm.</p><p>We use BERT-Siamese, NLL loss, and dot product to be consistent with recent research. We have obtained better accuracy with more vectors per document, BCE loss, and cosine similarity, but that is not the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>The training often takes about 1-2 hours per ANCE epoch, which is whenever new ANCE negative is ready, it immediately replaces existing negatives in training, without waiting. It converges in about 10 epochs, similar to other DR baselines. The optimization uses LAMB optimizer, learning rate 5e-6 for document and 1e-6 for passage retrieval, and linear warm-up and decay after 5000 steps. More detailed hyperparameter settings can be found in our code release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 OVERLAP WITH SPARSE RETRIEVAL IN TREC 2019 DL TRACK</head><p>As a nature of TREC-style pooling evaluation, only those ranked in the top 10 by the 2019 TREC participating systems were labeled. As a result, documents not in the pool and thus not labeled are all considered irrelevant, even though there may be relevant ones among them. When reusing TREC style relevance labels, it is very important to keep track of the "hole rate" on the evaluated systems, i.e., the fraction of the top K ranked results without TREC labels (not in the pool). A larger hole rate shows that the evaluated methods are very different  from those systems that participated in the Track and contributed to the pool, thus the evaluation results are not perfect. Note that the hole rate does not necessarily reflect the accuracy of the system, only the difference of it.</p><p>In TREC 2019 Deep Learning Track, all the participating systems are based on sparse retrieval. Dense retrieval methods often differ considerably from sparse retrievals and in general will retrieve many new documents. This is confirmed in Table <ref type="table">6</ref>. All DR methods have very low overlap with the official BM25 in their top 100 retrieved documents. At most, only 25% of documents retrieved by DR are also retrieved by BM25. This makes the hole rate quite high and the recall metric not very informative. It also suggests that DR methods might benefit more in this year's TREC 2020 Deep Learning Track if participants are contributing DR based systems.</p><p>The MS MARCO ranking labels were not constructed based on pooling the sparse retrieval results. They were from Bing <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref>, which uses many signals beyond term overlap. This makes the recall metric in MS MARCO more robust as it reflects how a single model can recover a complex online system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 HYPERPARAMETER STUDIES</head><p>We show the results of some hyperparameter configurations in Table <ref type="table">7</ref>. The cost of training with BERT makes it difficult to conduct a lot hyperparameter exploration. Often a failed configuration leads to divergence early in training. We barely explore other configurations due to the time-consuming nature of working with pretrained language models. Our DR model architecture is kept consistent with recent parallel work and the learning configurations in Table <ref type="table">7</ref> are about all the explorations we did. Most of the hyperparameter choices are decided solely using the training loss curve and otherwise by the loss in the MARCO Dev set. We found the training loss, validation NDCG, and testing performance align well in our (limited) hyperparameter explorations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 CASE STUDIES</head><p>In this section, we show Win/Loss case studies between ANCE and BM25. Among the 43 TREC 2019 DL Track evaluation queries in the document task, ANCE outperforms BM25 on 29 queries, loses on 13 queries, and ties on the rest 1 query. The winning examples are shown in Table <ref type="table">8</ref> and the losing ones are in Table <ref type="table">9</ref>. Their corresponding ANCE-learned (FirstP) representations are illustrated by t-SNE in Fig. <ref type="figure">6</ref> and Fig. <ref type="figure">7</ref>.</p><p>In general, we found ANCE better captures the semantics in the documents and their relevance to the query. The winning cases show the intrinsic limitations of sparse retrieval. For example, BM25 exact matches the "most popular food" in the query "what is the most popular food in Switzerland" but using the document is about Mexico. The term "Switzerland" only appears in the related question section of the web page.</p><p>The losing cases in Table <ref type="table">9</ref> are also quite interesting. Many times we found that it is not that DR fails completely and retrieves documents not related to the query's information needs at all, which was a big concern when we started research in DR. The errors ANCE made include retrieving documents that are related just not exactly</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03932</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer open-oomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text REtrieval Conference (TREC)</title>
				<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Search engines: information retrieval in practice</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Addison-Wesley Reading</publisher>
			<biblScope unit="volume">520</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context-aware sentence/passage term importance estimation for first stage retrieval</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper text understanding for ir with contextual neural language modeling</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer-XL: attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding bert rankers under distillation</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
				<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Complementing lexical retrieval with semantic residual embedding</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13969</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10396</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: retrieval-augmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Embedding-based retrieval in facebook search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training deep models faster with robust, approximate importance sampling</title>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7265" to="7275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triviaqa: a large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Not all samples are created equal: Deep learning with importance sampling</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00942</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00172</idno>
		<title level="m">Generalization through memorization: Nearest neighbor language models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12832</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relevance-based language models</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM) Special Interest Group on Information Retrieval (SIGIR) Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15020</idno>
		<title level="m">Pre-training via paraphrasing</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends in information retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: a robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00181</idno>
		<title level="m">Sparse, dense, and attentional representations for text retrieval</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient document re-ranking for transformers by precomputing term representations</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName><surname>Frieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An introduction to neural information retrieval</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="126" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04884</idno>
		<title level="m">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The fact extraction and verification (FEVER) shared task</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Fact Extraction and VERification (FEVER)</title>
				<meeting>the 1st Workshop on Fact Extraction and VERification (FEVER)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Glue: a multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><surname>Oguz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12756</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Idst at trec 2019 deep learning track: Deep cascade ranking with generation-based document expansion and pre-trained language modeling</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text REtrieval Conference</title>
				<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformer-xh: multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Bert-qe: Contextualized query expansion for document re-ranking</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07258</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
