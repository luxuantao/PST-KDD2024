<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 IEPT: INSTANCE-LEVEL AND EPISODE-LEVEL PRE-TEXT TASKS FOR FEW-SHOT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 IEPT: INSTANCE-LEVEL AND EPISODE-LEVEL PRE-TEXT TASKS FOR FEW-SHOT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The need of collecting large quantities of labeled training data for each new task has limited the usefulness of deep neural networks. Given data from a set of source tasks, this limitation can be overcome using two transfer learning approaches: few-shot learning (FSL) and self-supervised learning (SSL). The former aims to learn 'how to learn' by designing learning episodes using source tasks to simulate the challenge of solving the target new task with few labeled samples. In contrast, the latter exploits an annotation-free pretext task across all source tasks in order to learn generalizable feature representations. In this work, we propose a novel Instance-level and Episode-level Pretext Task (IEPT) framework that seamlessly integrates SSL into FSL. Specifically, given an FSL episode, we first apply geometric transformations to each instance to generate extended episodes. At the instancelevel, transformation recognition is performed as per standard SSL. Importantly, at the episode-level, two SSL-FSL hybrid learning objectives are devised: (1) The consistency across the predictions of an FSL classifier from different extended episodes is maximized as an episode-level pretext task. (2) The features extracted from each instance across different episodes are integrated to construct a single FSL classifier for meta-learning. Extensive experiments show that our proposed model (i.e., FSL with IEPT) achieves the new state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep convolutional neural networks (CNNs) <ref type="bibr" target="#b20">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b14">He et al., 2016b;</ref><ref type="bibr" target="#b16">Huang et al., 2017)</ref> have seen tremendous successes in a wide range of application fields, especially in visual recognition. However, the powerful learning ability of CNNs depends on a large amount of manually labeled training data. In practice, for many visual recognition tasks, sufficient manual annotation is either too costly to collect or not feasible (e.g., for rare object classes). This has severely limited the usefulness of CNNs for real-world application scenarios. Attempts have been made recently to mitigate such a limitation from two distinct perspectives, resulting in two popular research lines, both of which aim to transfer knowledge learned from the data of a set of source tasks to a new target one: few-shot learning (FSL) and self-supervised learning (SSL).</p><p>FSL <ref type="bibr" target="#b5">(Fei-Fei et al., 2006;</ref><ref type="bibr" target="#b51">Vinyals et al., 2016;</ref><ref type="bibr" target="#b6">Finn et al., 2017;</ref><ref type="bibr" target="#b44">Snell et al., 2017;</ref><ref type="bibr" target="#b47">Sung et al., 2018)</ref> typically takes a 'learning to learn' or meta-learning paradigm. That is, it aims to learn an algorithm for learning from few labeled samples, which generalizes well across any tasks. To that end, it adopts an episodic training strategy -the source tasks are arranged into learning episodes, each of which contains n classes and k labeled samples per class to simulate the setting for the target task. Part of the CNN model (e.g., feature extraction subnet, classification layers, or parameter initialization) is then meta-learned for rapid adaptation to new tasks.</p><p>In contrast, SSL <ref type="bibr" target="#b4">(Doersch et al., 2015;</ref><ref type="bibr" target="#b30">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b17">Iizuka et al., 2016;</ref><ref type="bibr" target="#b3">Doersch &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b31">Noroozi et al., 2018)</ref> does not require the source data to be annotated. Instead, it exploits an annotation-free pretext task on the source task data in the hope that a task-generalizable feature representation can be learned from the source tasks for easy adoption or adaptation in a target task. Such a pretext task gets its self-supervised signal at the per-instance level. Examples include rotation and context prediction <ref type="bibr">(Gidaris et al., 2018;</ref><ref type="bibr" target="#b4">Doersch et al., 2015)</ref>, jigsaw solving <ref type="bibr" target="#b30">(Noroozi &amp; Favaro, 2016)</ref>, and colorization <ref type="bibr" target="#b17">(Iizuka et al., 2016;</ref><ref type="bibr" target="#b21">Larsson et al., 2016)</ref>. Since these pretext tasks are class-agnostic, solving them leads to the learning of transferable knowledge. (2) In the middle branch, an FSL classifier is exploited to predict the FSL classification probabilities for each episode. We maximize the classification consistency among the extended episodes by forcing the four probability distributions to be consistent using L epis . The average supervised FSL loss L aux is also computed.</p><p>(3) In the bottom branch, we utilize an integration transformer module to fuse the features extracted from each instance with different rotation transformations; they are then used to compute an integrated FSL loss L integ . Among the four losses, L inst and L epis are the self-supervised losses, and L aux and L integ are the supervised losses.</p><p>Since both FSL and SSL aim to reduce the need of collecting a large amount of labeled training data for a target task by transferring knowledge from a set of source tasks, it is natural to consider combining them in a single framework. Indeed, two recent works <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref> proposed to integrate SSL into FSL by adding an auxiliary SSL pretext task in an FSL model. It showed that the SSL learning objective is complementary to that of FSL and combining them leads to improved FSL performance. However, in <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref>, SSL is combined with FSL in a superficial way: it is only taken as a separate auxiliary task for each single training instance and has no effect on the episodic training pipeline of the FSL model. Importantly, by ignoring the class labels of samples, the instance-level SSL learning objective is weak on its own. Since meta-learning across episodes is the essence of most contemporary FSL models, we argue that adding instance-level SSL pretext tasks alone fails to exploit fully the complementarity of the aforementioned FSL and SSL, for which a closer and deeper integration is needed.</p><p>To that end, in this paper we propose a novel Instance-level and Episode-level Pretext Task (IEPT) framework for few-shot recognition. Apart from adding an instance-level pretext SSL task as in <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref>, we introduce two episode-level SSL-FSL hybrid learning objectives for seamless SSL-FSL integration. Concretely, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, our full model has three additional learning objectives (besides the standard FSL one): (1) Different rotation transformations are applied to each original few-shot episode to generate a set of extended episodes, where each image has a rotation label for the instance-level pretext task (i.e., to predict the rotation label).</p><p>(2) The consistency across the predictions of an FSL classifier from different extended episodes is maximized as an episode-level pretext task. For each training image, the rotation transformation does not change its semantic content and hence its class label; the FSL classifier predictions across different extended episodes thus should be consistent, hence the consistency regularization objective.</p><p>(3) The correlation of features across instances from these extended episodes is modeled by a transformer-based attention module, optimizing the fusion of the features of each instance/image and its various rotation-transformed versions mainly for task adaptation during meta-testing. Importantly, with these three new learning objectives introduced in IEPT, any meta-learning based FSL model can now benefit more from SSL by fully exploiting their complementarity.</p><p>Our main contributions are three-fold: (1) For the first time, we propose both instance-level and episode-level pretext tasks (IEPT) for integrating SSL into FSL. The episode-level pretext task enables episodic training of SSL and hence closer integration of SSL with FSL.</p><p>(2) In addition to these pretext tasks, FSL further benefits from SSL by integrating features extracted from various rotationtransformed versions of the original training instances. The optimal way of feature integration is learned by a transformer-based attention module, which is mainly designed for task adaptation during meta-testing. (3) Extensive experiments show that our model (i.e., FSL with IEPT) achieves the new state-of-the-art. The code will be released soon.</p><p>Few-Shot Learning. The recent FSL studies are dominated by meta-learning based methods. They can be divided into three groups: (1) Metric-based methods <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b44">Snell et al., 2017;</ref><ref type="bibr" target="#b47">Sung et al., 2018;</ref><ref type="bibr" target="#b1">Allen et al., 2019;</ref><ref type="bibr" target="#b55">Xing et al., 2019;</ref><ref type="bibr" target="#b23">Li et al., 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b54">Wu et al., 2019;</ref><ref type="bibr" target="#b56">Ye et al., 2020;</ref><ref type="bibr" target="#b0">Afrasiyabi et al., 2020;</ref><ref type="bibr" target="#b25">Liu et al., 2020;</ref><ref type="bibr" target="#b57">Zhang et al., 2020)</ref> aim to learn the distance metric between feature embeddings. The focus of these methods is often on meta-learning of a feature-extraction CNN, whilst the classifiers used are of simple form such as a nearest-neighbor classifier.</p><p>(2) Optimization-based methods <ref type="bibr" target="#b6">(Finn et al., 2017;</ref><ref type="bibr">Ravi &amp; Larochelle, 2017;</ref><ref type="bibr" target="#b40">Rusu et al., 2019;</ref><ref type="bibr" target="#b22">Lee et al., 2019)</ref> learn to optimize the model rapidly given a few labeled samples per class in the new task.</p><p>(3) Model-based methods <ref type="bibr" target="#b41">(Santoro et al., 2016;</ref><ref type="bibr" target="#b28">Munkhdalai &amp; Yu, 2017;</ref><ref type="bibr" target="#b27">Mishra et al., 2018)</ref> focus on designing either specific model structures or parameters capable of rapid updating.</p><p>Apart from these three groups of methods, other FSL methods have attempted feature hallucination <ref type="bibr" target="#b42">(Schwartz et al., 2018;</ref><ref type="bibr" target="#b12">Hariharan &amp; Girshick, 2017;</ref><ref type="bibr" target="#b7">Gao et al., 2018;</ref><ref type="bibr" target="#b53">Wang et al., 2018;</ref><ref type="bibr" target="#b58">Zhang et al., 2019;</ref><ref type="bibr" target="#b49">Tsutsui et al., 2019)</ref> which generates additional samples from the given few shots for network finetuning, and parameter predicting <ref type="bibr" target="#b36">(Qiao et al., 2018;</ref><ref type="bibr" target="#b35">Qi et al., 2018;</ref><ref type="bibr">Gidaris &amp; Komodakis, 2019;</ref><ref type="bibr">2018)</ref> which learns to predict part of the parameters of a network given few samples of new classes for quick adaptation. In this work, we adopt the metric-based Prototypical Network (ProtoNet) <ref type="bibr" target="#b44">(Snell et al., 2017)</ref> as the basic FSL classifier for the main instantiation of our IEPT framework due to its simplicity and popularity. However, we show that any meta-learning based FSL method can be combined with our IEPT (see results in Figure <ref type="figure" target="#fig_3">2</ref>(c)).</p><p>Self-Supervised Learning. In SSL, it is assumed that the source task data is label-free and a pretext task is designed to provide self-supervision signals at the instance-level. Existing SSL approaches differ mainly in the pretext task design. These include predicting the rotation angle <ref type="bibr">(Gidaris et al., 2018)</ref> and the context of image patch <ref type="bibr" target="#b4">(Doersch et al., 2015;</ref><ref type="bibr" target="#b29">Nathan Mundhenk et al., 2018)</ref>, jigsaw solving <ref type="bibr" target="#b30">(Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b31">Noroozi et al., 2018)</ref> (i.e. shuffling and then reordering image patch), and performing images reversion <ref type="bibr" target="#b17">(Iizuka et al., 2016;</ref><ref type="bibr" target="#b34">Pathak et al., 2016;</ref><ref type="bibr" target="#b21">Larsson et al., 2016)</ref>. SSL has been shown to be beneficial to various down-steam tasks such as semantic object matching <ref type="bibr" target="#b32">(Novotny et al., 2018)</ref>, object segmentation <ref type="bibr" target="#b18">(Ji et al., 2019)</ref> and object detection <ref type="bibr" target="#b3">(Doersch &amp; Zisserman, 2017</ref>) by learning transferable feature presentations for these tasks.</p><p>Integrating Self-Supervised Learning into Few-Shot Learning. To the best of our knowledge, only two recent works <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref> have attempted combining SSL with FSL. However, the integration of SSL into FSL is often shallow: the original FSL training pipeline is intact; in the meantime, an additional loss on each image w.r.t. a self-supervised signal like the rotation angle or relative patch location is introduced. With pretext tasks solely at the instance level, combining the two approaches (i.e., SSL and FSL) can only be superficial without fully exploiting the episodic training pipeline unique to FSL. Different from <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref>, we introduce an episode-level pretext task to integrate SSL into the episodic training in FSL fully. Specifically, the consistency across the predictions of an FSL classifier from different extended episodes is maximized to reflect the fact that various rotation transformations should not alter the class-label prediction. Moreover, features of each instance and its various rotation-transformed versions are now fused for FSL classification, to integrate SSL with FSL for the supervised classification task. Our experimental results show that thanks to the closer integration of SSL and FSL, our IEPT clearly outperforms <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref> (see Table <ref type="table">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><formula xml:id="formula_0">S e = {(x i , y i )|y i ∈ C e , i = 1, ..., n × k} and Q e = {(x i , y i )|y i ∈ C e , i = 1, ..., n × q}, where S e Q e = ∅.</formula><p>For simplicity, we denote l k = n × k and l q = n × q. In the meta-training stage,</p><p>the training process has an inner and an outer loop in each episode: in the inner loop, the model is updated using S e ; its performance is then evaluated on the query set Q e in the outer loop to update the model parameters or algorithm that one wants to meta-learn.</p><p>Basic FSL Classifier. We employ ProtoNet <ref type="bibr" target="#b44">(Snell et al., 2017)</ref> as the basic FSL model. This model has a feature-extraction CNN and a simple non-parametric classifier. The parameter of the feature extractor is to be meta-learned. Concretely, in the inner loop of an episode, ProtoNet fixes the feature extractor and computes the mean feature embedding for each class as follows:</p><formula xml:id="formula_1">h c = 1 k • (xi,yi)∈Se f φ (x i ) • I(y i = c),<label>(1)</label></formula><p>where class c ∈ C e , f φ is a feature extractor with learnable parameters φ, and I is the indicator function. By computing the distance between the feature embedding of each query sample and that of the corresponding class, the loss function used to meta-learn φ in the outer loop is defined as:</p><formula xml:id="formula_2">L f sl (S e , Q e ) = 1 |Q e | (xi,yi)∈Qe − log exp(−d(f φ (x i ), h yi )) c∈Ce exp(−d(f φ (x i ), h c )) ,<label>(2)</label></formula><p>where d(•, •) denotes a distance function (e.g., the l 2 distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PRETEXT TASKS IN IEPT</head><p>The schematic of our IEPT is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We first define a set of 2D-rotation operators G = {g r |r = 0, ..., R − 1}, where g r means the operator of rotating the image by r*90 degrees and R is the total number of rotations (R = 4 in our implementation). Given an original episode E e = {S e , Q e } as described in Sec. 3.1, we utilize the 2D-rotation operators from G in turn to transform each image in E e . This results in a set of R extended episodes (including the original one)</p><formula xml:id="formula_3">E = {{S r e , Q r e }|r = 0, ..., R − 1}, where S r e = {(x i , y i , r)|y i ∈ C e , i = 1, ..., l k } and Q r e = {(x i , y i , r)|y i ∈ C e , i = 1, ..., l q }. Now each episode is denoted as E r e = {(x i , y i , r)|y i ∈ C e , i = 1, ..., l k , l k + 1, ..., l k + l q },</formula><p>where the first l k samples are from S r e and the rest from Q r e . Note that {S 0 e , Q 0 e } is the original episode {S e , Q e }. With the rotation transformations, each sample (x i , y i , r i ) in E carries a class label y i for supervised learning (from the inherent class) and a label r i from the rotation operator for self-supervised learning. After generating the set of extended episodes E, the feature extractor f φ is applied to each image x i in E. On these episodes, we design two self-supervised pretext tasks, one at the instance-level and the other episode-level.</p><p>Instance-Level Pretext Task. The instance-level task is to recognize different rotation transformations. The idea is that if the model to be meta-learned here (i.e., f φ ) can be used to distinguish different transformations, it must understand the canonical poses of objects (e.g., animals have legs touching the ground and trees have leaves on top), a vital part of class-agnostic and thus transferable knowledge. With the self-supervised rotation label r i , we consider the mapping: f θrot : x i → r i for each instance (x i , y i , r i ) ∈ E, where f θrot is a rotation classifier with learnable parameters θ rot . Given the input pair (x i , r i ), the total instance-level rotation loss is a cross-entropy loss:</p><formula xml:id="formula_4">L inst = 1 R(l k + l q ) R−1 r=0 (xi,yi,ri)∈E r e − log exp([f θrot (f φ (x i ))] ri ) R−1 r =0 exp([f θrot (f φ (x i ))] r ) ,<label>(3)</label></formula><p>where </p><formula xml:id="formula_5">[f θrot (f φ (x i ))] ∈ R R is</formula><formula xml:id="formula_6">[p r i ] c = exp(−d(f φ (x i ), h r c )) c exp(−d(f φ (x i ), h r c )) . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>The above probability is computed as in Sec. 3.1 and the class embedding h r c is obtained from S r e . The mean probability distribution of the R extended episodes is thus given by:</p><formula xml:id="formula_8">pi = 1 R • R−1 r=0 p r i .<label>(5)</label></formula><p>The total episode-level consistency regularization loss is computed with the KL divergence loss:</p><formula xml:id="formula_9">L epis = 1 Rl q • R−1 r=0 lq i=1 mean(p r i (log p r i − log pi )).<label>(6)</label></formula><p>where mean(•) is an element-wise averaging function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INTEGRATED FSL TASK</head><p>The two tasks introduced so far are self-supervised tasks without using the class labels in the query set. Now we describe how in the supervised classification task, the extended episodes can be used.</p><p>Given the set of extended episodes E, we denote the feature set of E as E emb , where</p><formula xml:id="formula_10">E emb = {f φ (x i )|(x i , y i , r) ∈ E r e , r = 0, • • • , R − 1, i = 1, ..., l k + l q }.</formula><p>Note that each extended episode in E corresponds to one specific rotation transformation of the same set of images from the original episode E e . Therefore, in order to capture the correlation among instances with different transformations and learn how best combine them to form the class mean for meta-learning, an instance attention module is deployed w.r.t. each image in E e (i.e., all images are assumed to be independent). Specifically, based on E emb , we construct the feature tensor F ∈ R (l k +lq)×R×d , where d is the feature dimension. We then adopt a transformer to obtain the integrated representation for FSL classification. The transformer architecture is based on a self-attention mechanism, as in <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>. It receives the triplet input (F, F, F ) as (Q, K, V ) (Query, Key, and Value, respectively). With F (i) being the i-th row of F (w.r.t. the i-th image in E e ), the attentive module is defined as:</p><formula xml:id="formula_11">(F (i) Q , F (i) K , F (i) V ) = (F (i) W Q , F (i) W K , F (i) W V ),<label>(7)</label></formula><formula xml:id="formula_12">F (i) att = F (i) + softmax( F (i) Q (F (i) K ) T √ d K ) F (i) V ,<label>(8)</label></formula><p>where d K = d, and W Q , W K , W V represent the parameters of three fully-connected layers respectively (the parameters of the integration transformer are collected as θ int ). Note that the key and value are computed from each image and its augmented versions, i.e., they are computed independently without using inter-image correlation. With the attentive feature F att ∈ R (l k +lq)×R×d , the integrated representation</p><formula xml:id="formula_13">F integ = [F S ; F Q ] ∈ R (l k +lq</formula><p>)×Rd (F S and F Q are respectively for the support set and query set) is given by:</p><formula xml:id="formula_14">F integ = flatten(F att ),<label>(9)</label></formula><p>where flatten(•) denotes flattening F att along the last two dimensions, i.e., concatenating the attentive features from different extended episodes for the corresponding images. The integrated representation is then inputted to the FSL classifier to define the FSL classification loss:</p><formula xml:id="formula_15">L integ = 1 l q • lq i=1 − log exp(−d(F Q i , h f yi )) c∈Ce exp(−d(F Q i , h f c ))<label>(10)</label></formula><p>where the class embedding</p><formula xml:id="formula_16">h f c = 1 k • l k i=1 F S i • I(y i = c</formula><p>) is computed on the support set. Note that the integrated FSL task actually acts as an alternative to prediction averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TOTAL LOSS</head><p>The total training loss for our full model consists of the self-supervised losses from the pretext tasks and the supervised losses from the FSL tasks. In this work, in addition to L integ in Eq. ( <ref type="formula" target="#formula_15">10</ref>), another supervised FSL loss L aux is also used (see Figure <ref type="figure" target="#fig_0">1</ref>). L aux is the average FSL classification loss over the extended episodes. Formally, it can be written as:</p><formula xml:id="formula_17">L aux = 1 R • R−1 r=0 L f sl (S r e , Q r e )<label>(11)</label></formula><p>Therefore, the total loss L total for training our full model is given as follows:</p><formula xml:id="formula_18">L total = instance−level w1 * Linst + episode−level w2 * Lepis self-supervised loss + w3 * Laux + Linteg supervised loss ,<label>(12)</label></formula><p>where w 1 , w 2 , w 3 are the loss weight hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">INFERENCE</head><p>During the test stage, we only exploit the integrated representation F integ for the final FSL prediction.</p><p>The predicted class label for x i ∈ Q e can be computed with Eq. (10) as:</p><formula xml:id="formula_19">y pred i = argmax y∈Ce exp(−d(F Q i , h f y )) c∈Ce exp(−d(F Q i , h f c ))</formula><p>.</p><p>(13)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">FULL IEPT ALGORITHM</head><p>For easy reproduction, we present the full algorithm for FSL with IEPT in Algorithm 1. Once learned, with the learned ψ, we can perform the inference over the test episodes with Eq. ( <ref type="formula">13</ref>). The code and trained models will be released soon.</p><p>Algorithm 1 FSL with IEPT Generate the set of extended episodes E from Ee using G 5:</p><p>Compute the SSL loss Linst for the instance-level pretext task with Eq. (3) 6:</p><p>Compute the SSL loss Lepis for the episode-level pretext task with Eq. (6) 7:</p><p>Compute the supervised FSL loss Laux over the extended episodes with Eq. (11) 8:</p><p>Compute the supervised FSL loss Linteg for the integrated episode with Eq. (10) 9:</p><p>L total = w1 * Linst + w2 * Lepis + w3 * Laux + Linteg 10:</p><p>Update ψ based on ∇ ψ L total 11: end for 12: return ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Datasets. Two widely-used FSL datasets are selected: miniImageNet <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> and tieredImageNet <ref type="bibr" target="#b38">(Ren et al., 2018)</ref>. The first dataset consists of a total number of 100 classes (600 images per class) and the train/validation/test split is set to 64/16/20 classes as in <ref type="bibr">(Ravi &amp; Larochelle, 2017)</ref>. The second dataset is a larger dataset including 608 classes totally (nearly 1,200 images per class), which is split into 351/97/160 classes for train/validation/test. Both datasets are subsets sampled from ImageNet <ref type="bibr" target="#b39">(Russakovsky et al., 2015)</ref>.</p><p>Feature Extractors. For fair comparison with published results, our IEPT adopts three widely-used feature extractors: Conv4-64 <ref type="bibr">(Vinyals et al., 2016), Conv4-512, and</ref><ref type="bibr">ResNet-12 (He et al., 2016a)</ref>. Particularly, Conv4-512 is almost the same as Conv4-64 except having a different channel size of the last convolution layer. To speed up the training process, as in many previous works <ref type="bibr" target="#b56">(Ye et al., 2020;</ref><ref type="bibr" target="#b57">Zhang et al., 2020;</ref><ref type="bibr" target="#b43">Simon et al., 2020)</ref>, we pretrain all the feature extractors on the training split of each dataset for our IEPT. Following <ref type="bibr" target="#b13">(He et al., 2016a)</ref>, we use the temperature scaling skill during the training phase. On both datasets, the input image size is 84 × 84. The output feature dimensions of <ref type="bibr">512,</ref><ref type="bibr">and 640,</ref><ref type="bibr">respectively.</ref> Table <ref type="table">1</ref>: Comparative results for 5-way 1/5-shot FSL. The mean classification accuracies (top-1, %) with the 95% confidence intervals are reported. † indicates the result is reproduced by ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>miniImageNet tieredImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone 1-shot 5-shot 1-shot 5-shot MatchingNet <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> Conv4-64 43.56 ± 0.84 55.31 ± 0.73 --ProtoNet † <ref type="bibr" target="#b44">(Snell et al., 2017)</ref> Conv4-64 52.61 ± 0.52 71.33 ± 0.41 53.33 ± 0.50 72.10 ± 0.41 MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref> Conv4-64 48.70 ± 1.84 63.10 ± 0.92 51.67 ± 1.81 70.30 ± 0.08 Relation Net <ref type="bibr" target="#b47">(Sung et al., 2018)</ref> Conv4-64 50.40 ± 0.80 65.30 ± 0.70 54.48 ± 0.93 71.32 ± 0.78 IMP † <ref type="bibr" target="#b1">(Allen et al., 2019)</ref> Conv4-64 52.91 ± 0.49 71.57 ± 0.42 53.63 ± 0.51 71.89 ± 0.44 DN4 <ref type="bibr" target="#b24">(Li et al., 2019b)</ref> Conv4-64 51.24 ± 0.74 71.02 ± 0.64 --DN PARN <ref type="bibr" target="#b54">(Wu et al., 2019)</ref> Conv4-64 55.22 ± 0.84 71.55 ± 0.66 --PN+rot <ref type="bibr">(Gidaris et al., 2019)</ref> Conv4-64 53.63 ± 0.43 71.70 ± 0.36 --CC+rot <ref type="bibr">(Gidaris et al., 2019)</ref> Conv4-64 54.83 ± 0.43 71.86 ± 0.33 --DSN-MR <ref type="bibr" target="#b43">(Simon et al., 2020)</ref> Conv4-64 55.88 ± 0.90 70.50 ± 0.68 --Centroid <ref type="bibr" target="#b0">(Afrasiyabi et al., 2020)</ref> Conv4-64 53.  <ref type="bibr" target="#b25">(Liu et al., 2020)</ref> ResNet-12 63.85 ± 0.81 81.57 ± 0.56 --Distill <ref type="bibr" target="#b48">(Tian et al., 2020)</ref> ResNet-12 64.82 ± 0.60 82.14 ± 0.43 71.52 ± 0.69 86.03 ± 0.49 DSN-MR <ref type="bibr" target="#b43">(Simon et al., 2020)</ref> ResNet-12 64.60 ± 0.72 79.51 ± 0.50 67.39 ± 0.82 82.85 ± 0.56 DeepEMD <ref type="bibr" target="#b57">(Zhang et al., 2020)</ref> ResNet-12 65.91 ± 0.82 82.41 ± 0.56 71.16 ± 0.87 86.03 ± 0.58 FEAT <ref type="bibr" target="#b56">(Ye et al., 2020)</ref> ResNet-12 66.78 ± 0.20 82.05 ± 0.14 70.80 ± 0.23 84.79 ± 0.16 ProtoNet+Rotation <ref type="bibr" target="#b45">(Su et al., 2020)</ref>  Evaluation Metrics. We take the 5-way 5-shot (or 1-shot) FSL evaluation setting, as in previous works. We randomly sample 2,000 episodes from the test split and report the mean classification accuracy (top-1, %) as well as the 95% confidence interval. Since the integration transformer copes with each sample independently, we take a strict non-transductive setting during evaluation.</p><p>Implementation Details. PyTorch is used for our implementation. We utilize the Adam optimizer (Kingma &amp; Ba, 2015) for Conv4-64 &amp; Conv4-512 and the SGD optimizer for ResNet-12 to train our IEPT model. The hyperparameters of our IEPT model are selected according to the performance on the validation split.We will release the code soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>Comparison to State-of-the-Arts. We compare our IEPT with two groups of baselines: (1) Recent SSL-based FSL methods <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref>; (2) Representative/latest FSL methods (w/o SSL) <ref type="bibr" target="#b44">(Snell et al., 2017;</ref><ref type="bibr" target="#b6">Finn et al., 2017;</ref><ref type="bibr" target="#b22">Lee et al., 2019;</ref><ref type="bibr" target="#b37">Ravichandran et al., 2019;</ref><ref type="bibr" target="#b43">Simon et al., 2020;</ref><ref type="bibr" target="#b57">Zhang et al., 2020;</ref><ref type="bibr" target="#b56">Ye et al., 2020;</ref><ref type="bibr" target="#b25">Liu et al., 2020)</ref>. The comparative results for 5-way 1/5-shot FSL are shown in Table <ref type="table">1</ref>. We have the following observations: (1) When compared with the representative/latest FSL methods (w/o SSL), our IEPT achieves the best performance on all datasets and under all settings, validating the effectiveness of SSL with IEPT for FSL.</p><p>(2) Our IEPT also clearly outperforms the two SSL-based FSL methods <ref type="bibr">(Gidaris et al., 2019;</ref><ref type="bibr" target="#b45">Su et al., 2020)</ref> which only use instance-level pretext tasks, demonstrating the importance of closer/episode-level integration of SSL into FSL.</p><p>(3) The improvements achieved by our IEPT over ProtoNet range from 2% to 5%. Since our IEPT takes ProtoNet as the baseline, the obtained margins provide direct evidence that SSL brings significant benefits to FSL. Note that our IEPT is also shown to be effective under both the fine-grained FSL and cross-domain FSL settings in Sec. 4.3 (see Table <ref type="table" target="#tab_4">3</ref>).  Ablation Study. Our full IEPT model is trained with four losses (see Eq. ( <ref type="formula" target="#formula_18">12</ref>)), including two self-supervised losses and two supervised losses: the episode-level SSL loss L epis , the instance-level SSL loss L inst , the auxiliary FSL loss L aux and the integrated FSL loss L integ . To demonstrate the contribution of each loss, we present the ablation study results for our full IEPT model in Table <ref type="table" target="#tab_3">2</ref>, where Conv4-64 is used as the backbone. We start with L integ and then add the additional three losses successively. It can be observed that the performance of our model continuously increases when more losses are used, indicating that each loss contributes to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">FURTHER EVALUATIONS</head><p>Different Combination Methods over Episodes. We have introduced a transformer-based attention module to fuse the features of each instance from all extended episodes (and an integrated episode can be obtained) for the supervised classification task (see <ref type="bibr">Sec. 3.3)</ref>. In this experiment, we compare it with two alternative ways of across-episode integration: (1) Averaging extended episodes: the extended episodes are directly fused for FSL classification;</p><p>(2) Averaging all episodes: the extended episodes as well as the integrated episode are fused for FSL classification. We present the comparative results on miniImageNet in Figure <ref type="figure" target="#fig_3">2</ref>(a). For comprehensive comparison, the results of FSL with each single extended episode are also reported. We can observe that: (1) The performance of 'Episode 0 • ' is the highest among the four baselines (i.e., FSL with single extended episode), perhaps because the feature extractor is pretrained on the original images without rotation transformations. (2) FSL by averaging extended episodes (i.e., 'Averaging extended episodes') indeed improves each of the four baselines.</p><p>(3) FSL with integrated episode (i.e., 'Integrated episode') is superior to FSL by simply averaging extended episodes. (4) Comparing 'Integrated episode' with 'Averaging all episodes', the performance of FSL with integrated episode is more stable across different settings, furthering validating the usefulness of our across-episode integration. Overall, the episode-integration module is indeed effective in FSL with self-supervision. This is also supported by the visualization results in Figure <ref type="figure" target="#fig_4">3</ref> (see more visualization results in Appendices A.3 &amp; A.4).  It can be seen that the performance of our model consistently grows when R increases from 1 to 4. Additionally, the study on exploiting other pretext tasks for our IEPT is presented in Appendix A.1.</p><p>Different Basic FSL Classifiers. As mentioned in Sec. 3.1, we adopt ProtoNet as the basic FSL classifier due to its scalability and simplicity. To further show the effectiveness of our IEPT when other basic FSL classifiers are used, we provide the results obtained by our IEPT using ProtoNet, FEAT, and IMP for FSL in Figure <ref type="figure" target="#fig_3">2</ref>(c). It can be clearly observed that our IEPT leads to an improvement of about 1-4% over each basic FSL method (ProtoNet, FEAT, or IMP), indicating that our IEPT can be applied to improve a variety of popular FSL methods.</p><p>Comparative Results for Fine-Grained FSL and Cross-Domain FSL. To evaluate our IEPT algorithm under the fine-grained FSL and cross-domain FSL settings, we conduct experiments on CUB <ref type="bibr" target="#b52">(Wah et al., 2011)</ref> and miniImageNet → CUB, respectively. For fine-grained FSL on CUB, following <ref type="bibr" target="#b56">(Ye et al., 2020)</ref>, we randomly split the dataset into 100 training classes, 50 validation classes, and 50 test classes. For cross-domain FSL on miniImageNet → CUB, the 100 training classes are from miniImageNet; the 50 validation classes and 50 test classes (using the aforementioned split for fine-grained FSL) are from CUB. Under both settings, we use Conv4-64 as the feature extractor.</p><p>The 5-way 1/5-shot FSL results are shown in Table <ref type="table" target="#tab_4">3</ref>. Our IEPT clearly achieves the best results, yielding 1-3% improvements over the second-best FEAT. This shows the effectiveness of our IEPT under both fine-grained and cross-domain settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have proposed a novel Instance-level and Episode-level Pretext Task (IEPT) framework for integrating SSL into FSL. For the first time, we have introduced an episode-level pretext task for FSL with self-supervision, in addition to the conventional instance-level pretext task. Moreover, we have also developed an episode extension-integration framework by introducing an integration transformer module to fully exploit the extended episodes for FSL. Extensive experiments on two benchmarks demonstrate that the proposed model (i.e., FSL with IEPT) achieves the new state-of-theart. Our ongoing research directions include: exploring other episode-level pretext tasks for FSL with self-supervision, and applying FSL with self-supervision to other vision problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 FEATURE VISUALIZATIONS OF MORE TEST EPISODES</head><p>In addition to the feature visualizations of test episodes in Figure <ref type="figure" target="#fig_4">3</ref>, we provide more feature visualizations in Figure <ref type="figure" target="#fig_5">5</ref>. It can be seen that an integrated episode (the last one in each row) clearly has a better cluster data structure than the corresponding four extended episodes (the first four ones in each row). This indicates that our transformer-based across-episode integration is indeed effective for few-shot classification with self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ATTENTION VISUALIZATION FOR TEST EPISODES</head><p>We present attention map visualization of two test episodes (left and right) in Figure <ref type="figure">6</ref>. Each average attention map is computed by averaging the attention map of all instances of a certain class. We can observe that: (1) The average attention maps from different classes vary significantly, showing that the diverse semantics of different classes can be reflected by our attention-based integration transformer.</p><p>(2) When the classes of two episodes overlap (e.g., 'trifle' and 'dalmatian'), the average attention maps of an overlapped class from two episodes are similar, illustrating that our attention-based integration transformer can well capture the semantics of classes across episodes.  A.8 HYPER-PARAMETER SENSITIVITY TEST</p><p>We select the hyper-parameters w 1 , w 2 and w 3 from the candidate set {0.1, 0.5, 1.0, 5.0, 10.0} and show the hyper-parameter analysis results in Figure <ref type="figure" target="#fig_6">7</ref>. We find that the performance of our IEPT is relatively stable. Concretely, the performance of our IEPT is not sensitive to w 1 and w 2 with proper values, but too large w 1 (i.e. w 1 = 10.0) tends to cause obvious degradation, perhaps because the FSL task is biased by the rotation prediction loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic of our approach to FSL. Given a training episode, we apply 2D rotations by 0, 90, 180, and 270 degrees to each instance to generate four extended episodes. After going through a feature extraction CNN, four losses over three branches are designed: (1) In the top branch, we employ a self-supervised rotation classifier with the instance-level SSL loss L inst .(2) In the middle branch, an FSL classifier is exploited to predict the FSL classification probabilities for each episode. We maximize the classification consistency among the extended episodes by forcing the four probability distributions to be consistent using L epis . The average supervised FSL loss L aux is also computed. (3) In the bottom branch, we utilize an integration transformer module to fuse the features extracted from each instance with different rotation transformations; they are then used to compute an integrated FSL loss L integ . Among the four losses, L inst and L epis are the self-supervised losses, and L aux and L integ are the supervised losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the rotation scoring vector and [•] r means taking the r-th element.Episode-Level Pretext Task. We design the episode-level task based on a simple principle: although different extended episodes contain images with different rotation transformations, these transformations do not change their class labels. Consequently, the FSL classifier should produce consistent probability distributions for each instance across different extended episodes. Such consistency can be measured using the Kullback-Leibler (KL) divergence. Formally, for each extended episode {S r e , Q r e } in E, we first define the probability distribution of FSL classification over the query set Q r e as P r e = [p r 1 ; • • • ; p r lq ] ∈ R lq×n , where p r i ∈ R n is the probability distribution for x i in Q r e with its c-th element [p r i ] c (c = 1, ..., n) being:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Input:</head><label></label><figDesc>The training set Ds The rotation operator set G The loss weight hyperparameters w1, w2, w3 Output: The learned ψ 1: Randomly initialize all learnable parameters ψ = {φ, θrot, θint} 2: for iteration = 1, ..., MaxIteration do 3: Randomly sample episode Ee from Ds 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Comparison among different combination methods over episodes for FSL with selfsupervision. (b) Illustration of the effect of different choices of R on the performance of our model (R denotes the number of extended episodes used for SSL). (c) Comparative results obtained by our IEPT using different basic FSL classifiers (i.e. ProtoNet, FEAT, and IMP). It can be seen clearly that integrated episode-based fusion leads to more separation between classes. All figures present 5-way 1-shot/5-shot results on miniImageNet, using Conv4-64 as the feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Feature visualizations of a group of test extended episodes (the first four columns, rotation by 0 • , 90 • , 180 • , 270 • ) and their integrated episode (the last column), using the UMAP algorithm (McInnes et al., 2018). The 5-way 5-shot FSL (with Conv4-64) is adopted on miniImageNet.</figDesc><graphic url="image-25.png" coords="9,51.38,62.27,494.76,98.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Feature visualizations of more test episodes using the UMAP algorithm (McInnes et al., 2018). Each row indicates a group of test extended episodes (the first four columns, rotation by 0 • , 90 • , 180 • , 270 • ) and their integrated episode (the last column). The 5-way 5-shot FSL (with Conv4-64) is adopted on miniImageNet.</figDesc><graphic url="image-48.png" coords="14,54.88,396.84,479.73,95.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of our hyper-parameter analysis under 5-way 1-shot (left) and 5-shot (right) on miniImageNet. Conv4-64 is used as the feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Lee et al., 2019) ResNet-12 62.64 ± 0.61 78.63 ± 0.46 65.99 ± 0.72 81.56 ± 0.63 MTL (Sun et al., 2019) ResNet-12 61.20 ± 1.80 75.50 ± 0.80 65.62 ± 1.80 80.61 ± 0.90 CAN (Hou et al., 2019) ResNet-12 63.85 ± 0.48 79.44 ± 0.34 69.89 ± 0.51 84.23 ± 0.37 AM3 (Xing et al., 2019) ResNet-12 65.21 ± 0.49 75.20 ± 0.36 67.23 ± 0.34 78.95 ± 0.22 Shot-Free (Ravichandran et al., 2019) ResNet-12 59.04 ± 0.43 77.64 ± 0.39 66.87 ± 0.43 82.64 ± 0.43 Neg-Cosine</figDesc><table><row><cell></cell><cell>14 ± 1.06 71.45 ± 0.72</cell><cell>-</cell><cell>-</cell></row><row><cell>Neg-Cosine (Liu et al., 2020)</cell><cell>Conv4-64 52.84 ± 0.76 70.41 ± 0.66</cell><cell>-</cell><cell>-</cell></row><row><cell>IEPT (ours)</cell><cell cols="3">Conv4-64 56.26 ± 0.45 73.91 ± 0.34 58.25 ± 0.48 75.63 ± 0.46</cell></row><row><cell>ProtoNet  † (Snell et al., 2017)</cell><cell cols="3">Conv4-512 53.25 ± 0.44 73.15 ± 0.35 57.88 ± 0.50 76.82 ± 0.40</cell></row><row><cell>MAML (Finn et al., 2017)</cell><cell cols="3">Conv4-512 49.33 ± 0.60 65.17 ± 0.49 52.84 ± 0.56 70.91 ± 0.46</cell></row><row><cell>Relation Net (Sung et al., 2018)</cell><cell cols="3">Conv4-512 50.86 ± 0.57 67.32 ± 0.44 54.69 ± 0.59 72.71 ± 0.43</cell></row><row><cell>PN+rot (Gidaris et al., 2019)</cell><cell>Conv4-512 56.02 ± 0.46 74.00 ± 0.35</cell><cell>-</cell><cell>-</cell></row><row><cell>CC+rot (Gidaris et al., 2019)</cell><cell>Conv4-512 56.27 ± 0.43 74.30 ± 0.33</cell><cell>-</cell><cell>-</cell></row><row><cell>IEPT (ours)</cell><cell cols="3">Conv4-512 58.43 ± 0.46 75.07 ± 0.33 60.91 ± 0.59 79.61 ± 0.45</cell></row><row><cell>ProtoNet  † (Snell et al., 2017)</cell><cell cols="3">ResNet-12 62.39 ± 0.51 80.53 ± 0.42 68.23 ± 0.50 84.03 ± 0.41</cell></row><row><cell>TADAM (Oreshkin et al., 2018)</cell><cell>ResNet-12 58.50 ± 0.30 76.70 ± 0.38</cell><cell>-</cell><cell>-</cell></row><row><cell>MetaOptNet (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study results for our full IEPT model over miniImageNet and tieredImageNet. Our full model includes two self-supervised losses (i.e. L epis and L inst ) and two supervised losses (i.e. L aux and L integ ). Conv4-64 is used as the feature extractor.</figDesc><table><row><cell>miniImageNet</cell><cell>tieredImageNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparative results for the fine-grained FSL on CUB<ref type="bibr" target="#b52">(Wah et al., 2011)</ref> and the cross-domain FSL on miniImageNet → CUB. MatchingNet<ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> Conv4-64 61.16 ± 0.89 72.86 ± 0.70 42.62 ± 0.55 56.53 ± 0.44 ProtoNet (Snell et al., 2017) Conv4-64 63.72 ± 0.22 81.50 ± 0.15 50.51 ± 0.56 69.28 ± 0.40 MAML (Finn et al., 2017) Conv4-64 55.92 ± 0.95 72.09 ± 0.76 43.59 ± 0.54 54.18 ± 0.41 Relation Net (Sung et al., 2018) Conv4-64 62.45 ± 0.98 76.11 ± 0.69 49.84 ± 0.54 68.98 ± 0.42 FEAT (Ye et al., 2020) Conv4-64 68.87 ± 0.22 82.90 ± 0.15 51.52 ± 0.54 70.16 ± 0.40 IEPT (ours) Conv4-64 69.97 ± 0.49 84.33 ± 0.33 52.68 ± 0.56 72.98 ± 0.40 Different Number of Extended Episodes. In all the above experiments, the number of the extended episodes R is set to 4 (rotation by 0 • , 90 • , 180 • , 270</figDesc><table><row><cell>CUB</cell><cell>miniImageNet→CUB</cell></row></table><note>• ). Figure2(b) shows the impact of the value of R. Note that when R = 1, our IEPT model is equivalent to ProtoNet which is without self-supervision.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Comparative results by applying IEPT to the optimization-based method MAML.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">miniImageNet</cell><cell cols="2">tieredImageNet</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MAML</cell><cell>Conv4-64</cell><cell>48.70 ± 1.84</cell><cell>63.10 ± 0.92</cell><cell>51.67 ± 1.81</cell><cell>70.30 ± 0.80</cell></row><row><cell>MAML+IEPT</cell><cell>Conv4-64</cell><cell>49.68 ± 0.50</cell><cell>65.22 ± 0.48</cell><cell>52.85 ± 0.52</cell><cell>71.04 ± 0.49</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 COMPARISON AMONG DIFFERENT SSL STRATEGIES</head><p>To generate the extended episodes in IEPT, we apply four rotation transformations (i.e. rotation by 0 • , 90 • , 180 • , 270 • ) to each image. It makes sense to explore whether other self-supervised strategies are also effective for our IEPT. To this end, we exploit shuffling image patches (see Figure <ref type="figure">4</ref>) for self-supervised learning (SSL). Specifically, we divide each image into 2*2 patches and reorganize the patch orders to obtain a shuffling label. To compare with the rotation strategy fairly, we choose only four shuffling orders: <ref type="table">(1, 2, 3, 4), (2, 3, 4, 1), (3, 4, 1, 2) and (4, 1, 2, 3</ref>). Note that the (1, 2, 3, 4) shuffling order equals to the original image. Similar to the rotation strategy, a fully-connected layer is utilized to recognize the shuffling order. The comparative results are shown in Table <ref type="table">4</ref>. We can see that both IEPT with shuffling and IEPT with rotation achieve better performance than the original ProtoNet. Particularly, IEPT with shuffling yields 1-3% and 3-4% improvements under 5-shot and 1-shot, respectively. This clearly shows the effectiveness of our IEPT for FSL even when different SSL strategies are used to define the pretext tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 COMPARISON AMONG DIFFERENT INTEGRATION APPROACHES</head><p>We employ the integration transformer to find the intrinsic correlation of various rotation-transformed instances. The transformer architecture is based on a self-attention mechanism. Concretely, it receives the feature sets of extended episodes as input Q, K and V . Further, it matches each query in Q with a list of keys in K and returns the weighted sum of corresponding values. To show the importance of the transformer module, we compare it with two other integration approaches (i.e. concatenating and averaging) to integrate the features of extended episodes. The comparative results in Table <ref type="table">5</ref> demonstrate that the integration transformer consistently performs better than the simply concatenating/averaging approaches. This suggests that the attention-based integration transformer is a better choice for designing the integration module.   <ref type="table">6</ref>. It can be observed that the performance of our IEPT is much more effective than that of simple integration, due to the extra use of L integ + L epis for FSL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 DIFFERENT ALTERNATIVES OF SELF-SUPERVISED LOSSES</head><p>In Table <ref type="table">7</ref>, we provide further ablation study regarding different alternatives of L epis and L inst . For the episode-level self-supervised loss L epis , we compare our implementation (using the KL loss between each distribution and the mean distribution) with that using a pairwise KL loss (i.e., the KL loss between each pair of distributions). For the instance-level self-supervised loss L inst , we compare our implementation (using the rotation prediction loss) with the recent self-supervised learning technique <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>. We observe that our implementation achieves slight performance improvements over those using the pairwise KL loss or the contrastive learning loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 APPLICATION OF IEPT TO OPTIMIZATION-BASED METHOD MAML</head><p>In Table <ref type="table">8</ref>, we show the results obtained by applying our IEPT to the optimization-based model MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>. We use Conv4-64 as the feature extractor. We can see that our IEPT brings 0.7%-2.1% improvements to MAML. This further shows the flexibility (as well as effectiveness) of our IEPT for FSL.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Associative alignment for few-shot image classification</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Afrasiyabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Franc ¸ois</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Gagné</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName><forename type="first">Kelsey</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-shot learning via covariancepreserving adversarial augmentation networks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="975" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3037" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based imageto-class measure for few-shot learning</title>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9339" to="9348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9359" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Rodríguez López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Journal of Computer Vision (IJCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2850" to="2860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4136" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">When does self-supervision improve few-shot learning?</title>
		<author>
			<persName><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Meta-reinforced synthetic data for one-shot fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Tsutsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3057" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Parn: Position-aware relation networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6659" to="6667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro O O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4847" to="4857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-toset functions</title>
		<author>
			<persName><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12203" to="12213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
