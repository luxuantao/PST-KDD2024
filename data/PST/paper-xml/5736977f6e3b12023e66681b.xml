<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoRec: Autoencoders Meet Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
							<email>suvash.sedhain@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
							<email>scott.sanner@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
							<email>lexing.xie@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Nicta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoRec: Autoencoders Meet Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D</term>
					<term>2</term>
					<term>8 [Information Storage and Retrieval]: Information Filtering Collaborative Filtering, Autoencoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes AutoRec, a novel autoencoder framework for collaborative filtering (CF). Empirically, AutoRec's compact and efficiently trainable model outperforms stateof-the-art CF techniques (biased matrix factorization, RBM-CF and LLORMA) on the Movielens and Netflix datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Collaborative filtering (CF) models aim to exploit information about users' preferences for items (e.g. star ratings) to provide personalised recommendations. Owing to the Netflix challenge, a panoply of different CF models have been proposed, with popular choices being matrix factorisation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and neighbourhood models <ref type="bibr" target="#b4">[5]</ref>. This paper proposes AutoRec, a new CF model based on the autoencoder paradigm; our interest in this paradigm stems from the recent successes of (deep) neural network models for vision and speech tasks. We argue that AutoRec has representational and computational advantages over existing neural approaches to CF <ref type="bibr" target="#b3">[4]</ref>, and demonstrate empirically that it outperforms the current state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE AUTOREC MODEL</head><p>In rating-based collaborative filtering, we have m users, n items, and a partially observed user-item rating matrix R ∈ R m×n . Each user u ∈ U = {1 . . . m} can be represented by a partially observed vector r (u) = (Ru1, . . . , Run) ∈ R n . Similarly, each item i ∈ I = {1 . . . n} can be represented by a partially observed vector r (i) = (R1i, . . . Rmi) ∈ R m . Our aim in this work is to design an item-based (user-based) autoencoder which can take as input each partially observed r (i) (r (u) ), project it into a low-dimensional latent (hidden) space, and then reconstruct r (i) (r (u) ) in the output space to predict missing ratings for purposes of recommendation.</p><p>Formally, given a set S of vectors in R d , and some k ∈ N+, an autoencoder solves min where h(r; θ) is the</p><formula xml:id="formula_0">θ r∈S ||r − h(r; θ)|| 2 2 ,<label>(1)</label></formula><formula xml:id="formula_1">r (i) = ( ) R1i R3i R2i Rmi +1 +1 r (i) = ( ) R1i R3i R2i Rmi . . . . . . V W i = 1...n</formula><formula xml:id="formula_2">reconstruction of input r ∈ R d , h(r; θ) = f (W • g(Vr + µ) + b) for activation functions f (•), g(•). Here, θ = {W, V, µ, b} for transformations W ∈ R d×k , V ∈ R k×d , and biases µ ∈ R k , b ∈ R d .</formula><p>This objective corresponds to an auto-associative neural network with a single, k-dimensional hidden layer. The parameters θ are learned using backpropagation.</p><p>The item-based AutoRec model, shown in Figure <ref type="figure" target="#fig_0">1</ref>, applies an autoencoder as per Equation 1 to the set of vectors {r (i) } n i=1 , with two important changes. First, we account for the fact that each r (i) is partially observed by only updating during backpropagation those weights that are associated with observed inputs, as is common in matrix factorisation and RBM approaches. Second, we regularise the learned parameters so as to prevent overfitting on the observed ratings. Formally, the objective function for the Item-based AutoRec (I-AutoRec) model is, for regularisation strength λ &gt; 0,</p><formula xml:id="formula_3">min θ n i=1 ||r (i) − h(r (i) ; θ))|| 2 O + λ 2 • (||W|| 2 F + ||V|| 2 F ),<label>(2)</label></formula><p>where || • || 2 O means that we only consider the contribution of observed ratings. User-based AutoRec (U-AutoRec) is derived by working with {r (u) } m u=1 . In total, I-AutoRec requires the estimation of 2mk + m + k parameters. Given learned parameters θ, I-AutoRec's predicted rating for user u and item i is Rui = (h(r (i) ; θ))u.</p><p>(3)</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the model, with shaded nodes corresponding to observed ratings, and solid connections corresponding to weights that are updated for the input r (i) .</p><p>ML We remark that I-RBM did not converge after one week of training. LLORMA's performance is taken from <ref type="bibr" target="#b1">[2]</ref>.</p><p>AutoRec is distinct to existing CF approaches. Compared to matrix factorisation (MF) approaches, which embed both users and items into a shared latent space using Ω(mk + nk) parameters, the item-based AutoRec model only embeds users into latent space via W, V. Hence, I-AutoRec requires the estimation of 2mk + m + k parameters (which is much smaller when m n as typical in most CF applications). Further, while MF learns a linear latent representation, Au-toRec can learn a nonlinear latent representation through activation function g(•). Compared to the RBM-based CF model (RBM-CF) <ref type="bibr" target="#b3">[4]</ref>, there are several distinctions. First, RBM-CF proposes a generative, probabilistic model based on restricted Boltzmann machines, while AutoRec is a discriminative model based on autoencoders. Second, RBM-CF estimates parameters by maximising log likelihood, while AutoRec directly minimises RMSE, the canonical performance in rating prediction tasks. Third, training RBM-CF requires the use of contrastive divergence, whereas training AutoRec requires the comparatively faster gradient-based backpropagation. Finally, RBM-CF is only applicable for discrete ratings, and estimates a separate set of parameters for each rating value. For r possible ratings, this implies nkr or (mkr) parameters for user-(item-) based RBM. AutoRec is agnostic to r and hence requires fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate and compare AutoRec with RBM-CF <ref type="bibr" target="#b3">[4]</ref>, Biased Matrix Factorisation <ref type="bibr" target="#b0">[1]</ref> (BiasedMF), and Local Low-Rank Matrix Factorisation (LLORMA) <ref type="bibr" target="#b1">[2]</ref> on the Movielens 1M, 10M and Netflix datasets. Following <ref type="bibr" target="#b1">[2]</ref>, we use a default rating of 3 for test users or items without training observations. We split the data into random 90%-10% train-test sets, and hold out 10% of the training set for hyperparamater tuning. We repeat this splitting procedure 5 times and report average RMSE. 95% confidence intervals on RMSE were ±0.003 or less in each experiment. For all baselines, we tuned the regularisation strength λ ∈ {0.001, 0.01, 0.1, 1, 100, 1000} and the appropriate latent dimension k ∈ {10, 20, 40, 80, 100, 200, 300, 400, 500}.</p><p>A challenge training autoencoders is non-convexity of the objective. We found resilient propagation (RProp) <ref type="bibr" target="#b2">[3]</ref> to give comparable performance to L-BFGS, while being much faster. Thus, we use RProp for all subsequent experiments: Which is better, item-or user-based autoencoding with RBMs or AutoRec? Table <ref type="table" target="#tab_0">1a</ref> shows item-based (I-) methods for RBM and AutoRec generally perform better; this is likely since the average number of ratings per item is much more than those per user; high variance in the number of user ratings leads to less reliable prediction for user-based methods. I-AutoRec outperforms all RBM variants. How does AutoRec performance vary with linear and nonlinear activation functions f (•), g(•)? Table <ref type="table" target="#tab_0">1b</ref> indicates that nonlinearity in the hidden layer (via g(•)) is critical for good performance of I-AutoRec, indicating its potential advantage over MF methods. All other AutoRec experiments use identity f (•) and sigmoid g(•) functions. How does performance of AutoRec vary with the number of hidden units? In Figure <ref type="figure" target="#fig_1">2</ref>, we evaluate the performance of AutoRec model as the number of hidden units varies. We note that performance steadily increases with the number of hidden units, but with diminishing returns. All other AutoRec experiments use k = 500. How does AutoRec perform against all baselines? Table <ref type="table" target="#tab_0">1c</ref> shows that AutoRec consistently outperforms all baselines, except for comparable results with LLORMA on Movielens 10M. Competitive performance with LLORMA is of interest, as the latter involves weighting 50 different local matrix factorization models, whereas AutoRec only uses a single latent representation via a neural net autoencoder. Do deep extensions of AutoRec help? We developed a deep version of I-AutoRec with three hidden layers of (500, 250, 500) units, each with a sigmoid activation. We used greedy pretraining and then fine-tuned by gradient descent. On Movielens 1M, RMSE reduces from 0.831 to 0.827 indicating potential for further improvement via deep AutoRec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Item-based AutoRec model. We use plate notation to indicate that there are n copies of the neural network (one for each item), where W and V are tied across all copies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RMSE of I-AutoRec on Movielens 1M as the number of hidden units k varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>(a) Comparison of the RMSE of I/U-AutoRec and RBM models. (b) RMSE for I-AutoRec with choices of linear and nonlinear activation functions, Movielens 1M dataset. (c) Comparision of I-AutoRec with baselines on MovieLens and Netflix datasets.</figDesc><table><row><cell>U-RBM I-RBM U-AutoRec I-AutoRec</cell><cell>-1M ML-10M 0.881 0.823 0.854 0.825 0.874 0.867 0.831 0.782</cell><cell>f (•) Identity Identity 0.872 g(•) RMSE Sigmoid Identity 0.852 Identity Sigmoid 0.831 Sigmoid Sigmoid 0.836</cell><cell>BiasedMF I-RBM U-RBM LLORMA I-AutoRec</cell><cell>ML-1M ML-10M Netflix 0.845 0.803 0.844 0.854 0.825 -0.881 0.823 0.845 0.833 0.782 0.834 0.831 0.782 0.823</cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<title level="m">Local low-rank matrix approximation. ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: the RProp algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Restricted Boltzmann machines for collaborative filtering. ICML</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
