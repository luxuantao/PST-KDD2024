<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Learning of Sum-Product Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Gens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195-2350</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
							<email>pedrod@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195-2350</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Learning of Sum-Product Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16F52E341A78969BB32AF376D9CEED36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sum-product networks are a new deep architecture that can perform fast, exact inference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the first discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an efficient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably using "hard" gradient descent, where marginal inference is replaced by MPE inference (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classification tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architecture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic models play a crucial role in many scientific disciplines and real world applications. Graphical models compactly represent the joint distribution of a set of variables as a product of factors normalized by the partition function. Unfortunately, inference in graphical models is generally intractable. Low treewidth ensures tractability, but is a very restrictive condition, particularly since the highest practical treewidth is usually 2 or 3 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. Sum-product networks (SPNs) <ref type="bibr" target="#b22">[23]</ref> overcome this by exploiting context-specific independence <ref type="bibr" target="#b6">[7]</ref> and determinism <ref type="bibr" target="#b7">[8]</ref>. They can be viewed as a new type of deep architecture, where sum layers alternate with product layers. Deep networks have many layers of hidden variables, which greatly increases their representational power, but inference with even a single layer is generally intractable, and adding layers compounds the problem <ref type="bibr" target="#b2">[3]</ref>. SPNs are a deep architecture with full probabilistic semantics where inference is guaranteed to be tractable, under general conditions derived by Poon and Domingos <ref type="bibr" target="#b22">[23]</ref>. Despite their tractability, SPNs are quite expressive <ref type="bibr" target="#b15">[16]</ref>, and have been used to solve difficult problems in vision <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Poon and Domingos introduced an algorithm for generatively training SPNs, yet it is generally observed that discriminative training fares better. By optimizing P (Y|X) instead of P (X, Y) conditional random fields retain joint inference over dependent label variables Y while allowing for flexible features over given inputs X <ref type="bibr" target="#b21">[22]</ref>. Unfortunately, the conditional partition function Z(X) is just as prone to intractability as with generative training. For this reason, low treewidth models (e.g. chains and trees) of Y are commonly used. Research suggests that approximate inference can make it harder to learn rich structured models <ref type="bibr" target="#b20">[21]</ref>. In this paper, discriminatively training SPNs will allow us to combine flexible features with fast, exact inference over high treewidth models.</p><p>With inference and learning that easily scales to many layers, SPNs can be viewed as a type of deep network. Existing deep networks employ discriminative training with backpropagation through softmax layers or support vector machines over network variables. Most networks that are not purely feed-forward require approximate inference. Poon and Domingos showed that deep SPNs could be learned faster and more accurately than deep belief networks and deep Boltzmann machines on a generative image completion task <ref type="bibr" target="#b22">[23]</ref>. This paper contributes a discriminative training algorithm that could be used on its own or with generative pre-training.</p><p>For the first time we combine the advantages of SPNs with those of discriminative models. In this paper we will review SPNs and describe the conditions under which an SPN can represent the conditional partition function. We then provide a training algorithm, demonstrate how to compute the gradient of the conditional log-likelihood of an SPN using backpropagation, and explore variations of inference. Finally, we show state-of-the-art results where a discriminatively-trained SPN achieves higher accuracy than SVMs and deep models on image classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sum-Product Networks</head><p>SPNs were introduced with the aim of identifying the most expressive tractable representation possible. The foundation for their work lies in Darwiche's network polynomial <ref type="bibr" target="#b13">[14]</ref>. We define an unnormalized probability distribution Φ(x) ≥ 0 over a vector of Boolean variables X. The indicator function [.] is one when its argument is true and zero otherwise; we abbreviate [X i ] and [ Xi ] as x i and xi . To distinguish random variables from indicator variables, we use roman font for the former and italic for the latter. Vectors of variables are denoted by bold roman and bold italic font, respectively. The network polynomial of Φ(x) is defined as x Φ(x) (x), where (x) is the product of indicators that are one in state x. For example, the network polynomial of the Bayesian network X 1 → X 2 is P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1 x2 + P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1 x2 . To compute P (X 1 = true, X 2 = false), we access the corresponding term of the network polynomial by setting indicators x 1 and x2 to one and the rest to zero. To find P (X 2 = true), we fix evidence on X 2 by setting x 2 to one and x2 to zero and marginalize X 1 by setting both x 1 and x1 to one. Notice that there are two reasons we might set an indicator x i = 1: (1) evidence {X i = true}, in which case we set xi = 0 and (2) marginalization of X i , where xi = 1 as well. In general the role of an indicator x i is to determine whether terms compatible with variable state X i = true are included in the summation, and similarly for xi . With this notation, the partition function Z can be computed by setting all indicators of all variables to one.</p><p>The network polynomial has size exponential in the number of variables, but in many cases it can be represented more compactly using a sum-product network <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref>. Definition 1. (Poon &amp; Domingos, 2011) A sum-product network (SPN) over variables X 1 , . . . , X d is a rooted directed acyclic graph whose leaves are the indicators x 1 , . . . , x d and x1 , . . . , xd and whose internal nodes are sums and products. Each edge (i, j) emanating from a sum node i has a non-negative weight w ij . The value of a product node is the product of the values of its children. The value of a sum node is j∈Ch(i) w ij v j , where Ch(i) are the children of i and v j is the value of node j. If we could replace the exponential sum over variable states in the partition function with the linear evaluation of the network, inference would be tractable. For example, the SPN in Figure <ref type="figure" target="#fig_1">1</ref> represents the joint probability of three Boolean variables P</p><formula xml:id="formula_0">(X 1 , X 2 , X 3 ) in the Bayesian network X 2 ← X 1 → X 3 using six indicators S[x 1 , x1 , x 2 , x2 , x 3 , x3 ].</formula><p>To compute P (X 1 = true), we could sum over the joint states of X 2 and X 3 , evaluating the network a total of four times S[1, 0, 0, 1, 0, 1]+. . .+ S[1, 0, 1, 0, 1, 0]. Instead, we set the indicators so that the network sums out both X 2 and X 3 . An indicator setting of S[1,0,1,1,1,1] computes the sum over all states compatible with our evidence e = {X1 = true} and requires only one evaluation.</p><p>However, not every SPN will have this property. If a linear evaluation of an SPN with indicators set to represent evidence equals the exponential sum over all variable states consistent with that evidence, the SPN is valid. The scope of a node is defined as the set of variables that have indicators among the node's descendants. To "appear in a child" means to be among that child's descendants. If a sum node is incomplete, the SPN will undercount the true marginals. Since an incomplete sum node has scope larger than a child, that child will be non-zero for more than one state of the sum (e.g. if</p><formula xml:id="formula_1">S[x 1 , x1 , x 2 , x2 ] = (x 1 + x 2 ), S[1, 0, 1, 1] &lt; S[1, 0, 1, 0] + S[1, 0, 0, 1]).</formula><p>If a product node is inconsistent, the SPN will overcount the marginals as it will incorporate impossible states (e.g.</p><formula xml:id="formula_2">x 1 × x1 ) into its computation.</formula><p>Poon and Domingos show how to generatively train the parameters of an SPN. One method is to compute the likelihood gradient and optimize with gradient descent (GD). They also show how to use expectation maximization (EM) by considering each sum node as the marginalization of a hidden variable <ref type="bibr" target="#b16">[17]</ref>. They found that online EM using most probable explanation (MPE or "hard") inference worked the best for their image completion task.</p><p>Gradient diffusion is a key issue in training deep models. It is commonly observed in neural networks that when the gradient is propagated to lower layers it becomes less informative <ref type="bibr" target="#b2">[3]</ref>. When every node in the network takes fractional responsibility for the errors of a top level node, it becomes difficult to steer parameters out of local minima. Poon and Domingos also saw this effect when using gradient descent and EM to train SPNs. They found that online hard EM could provide a sparse but strong learning signal to synchronize the efforts of upper and lower nodes. Note that hard training is not exclusive to EM. In the next section we show how to discriminatively train SPNs with hard gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discriminative Learning of SPNs</head><p>We define an SPN S[y, h|x] that takes as input three disjoint sets of variables H, Y, and X (hidden, query, and given). We denote the setting of all h indicator functions to 1 as S[y, 1|x], where the bold 1 is a vector. We do not sum over states of given variables X when discriminatively training SPNs. Given an instance, we treat X as constants. This means that one ignores X variables in the scope of a node when considering completeness and consistency. Since adding a constant as a child to a product node cannot make that product inconsistent, a variable x can be the child of any product node in a valid SPN. To maintain completeness, x can only be the child of a sum node that has scope outside of Y or H. The parameters of an SPN can be learned using an online procedure as in Algorithm 1 as proposed by Poon and Domingos. The three dimensions of the algorithm are generative vs. discriminative, the inference procedure, and the weight update. Poon and Domingos discussed generative gradient descent with marginal inference as well as EM with marginal and MPE inference. In this section we will derive discriminative gradient descent with marginal and MPE inference, where hard gradient descent can also be used for generative training. EM is not typically used for discriminative training as it requires modification to lower bound the conditional likelihood <ref type="bibr" target="#b24">[25]</ref> and there may not be a closed form for the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discriminative Training with Marginal Inference</head><p>A component of the gradient of the conditional log likelihood takes the form</p><formula xml:id="formula_3">∂ ∂w log P (y|x) = ∂ ∂w log h Φ(Y = y, H = h|x) - ∂ ∂w log y ,h Φ(Y = y , H = h|x) = 1 S[y, 1|x] ∂S[y, 1|x] ∂w - 1 S[1, 1|x]</formula><p>∂S <ref type="bibr">[1, 1|x]</ref> ∂w</p><p>where the two summations are separate bottom-up evaluations of the SPN with indicators set as S[y, 1|x] and S[1, 1|x], respectively.</p><p>The partial derivatives of the SPN with respect to all weights can be computed with backpropagation, detailed in Algorithm 2. After performing a bottom-up evaluation of the SPN, partial derivatives are passed from parent to child as follows from the chain rule and described in <ref type="bibr" target="#b14">[15]</ref>. The form of backpropagation presented takes time linear in the number of nodes in the SPN if product nodes have a bounded number of children.</p><p>Our gradient descent update then follows the direction of the partial derivative of the conditional log likelihood with learning rate η: ∆w = η ∂ ∂w log P (y|x). After each gradient step we optionally renormalize the weights of a sum node so they sum to one. Empirically we have found this to produce the best results. The second SPN evaluation that marginalizes H and Y can reuse computation from the first, for example, when Y is modeled by a root sum node. In this case the values of all non-root nodes are equivalent between the two evaluations. For any architecture, one can memoize values of nodes that do not have a query variable indicator as a descendant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: BackpropSPN</head><p>Input: A valid SPN S, where Sn denotes the value of node n after bottom-up evaluation. Output: Partial derivatives of the SPN with respect to every node ∂S ∂Sn and weight ∂S ∂w i,j</p><p>Initialize all ∂S ∂Sn = 0 except ∂S ∂S = 1 forall the n ∈ S in top-down order do if n is a sum node then forall the j ∈ Ch(n) do</p><formula xml:id="formula_4">∂S ∂S j ← ∂S ∂S j + wn,j ∂S ∂Sn ∂S ∂w n,j ← Sj ∂S ∂Sn else forall the j ∈ Ch(n) do ∂S ∂S j ← ∂S ∂S j + ∂S ∂Sn k∈Ch(n)\{j} S k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminative Training with MPE Inference</head><p>There are several reasons why MPE inference is appealing for discriminatively training SPNs. As discussed above, hard inference was crucial for overcoming gradient diffusion when generatively training SPNs. For many applications the goal is to predict the most probable structure, and therefore it makes sense to use this also during training. Finally, it is common to approximate summations with maximizations for reasons of speed or tractability. Though summation in SPNs is fast and exact, MPE inference is still faster. We derive discriminative gradient descent using MPE inference. </p><formula xml:id="formula_5">+ + + + + + f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f Figure 2:</formula><p>Positive and negative terms in the hard gradient. The root node sums out the variable Y, the two sum nodes on the left sum out the hidden variable H 1 , the two sum nodes on the right sum out H 2 , and a circled 'f' denotes an input variable X i . Dashed lines indicate negative elements in the gradient.</p><p>We define a max-product network (MPN) M [y, h|x] based on the max-product semiring. This network compactly represents the maximizer polynomial max x Φ(x) (x), which computes the MPE <ref type="bibr" target="#b14">[15]</ref>. To convert an SPN to an MPN, we replace each sum node by a max node, where weights on children are retained. The gradient of the conditional log likelihood with MPE inference is then</p><formula xml:id="formula_6">∂ ∂w log P (y|x) = ∂ ∂w log max h Φ(Y = y, H = h|x) - ∂ ∂w log max y ,h Φ(Y = y , H = h|x)</formula><p>where the two maximizations are computed by M [y, 1|x] and M <ref type="bibr">[1, 1|x]</ref>. MPE inference also consists of a bottom-up evaluation followed by a top-down pass. Inference yields a branching path through the SPN called a complete subcircuit that includes an indicator (and therefore assignment) for every variable <ref type="bibr" target="#b14">[15]</ref>. Analogous to Viterbi decoding, the path starts at the root node and at each max (formerly sum) node it only travels to the max-valued child. At product nodes, the path branches to all children. We define W as the multiset of weights traversed by this path <ref type="foot" target="#foot_0">1</ref> . The value of the MPN takes the form of a product wi∈W w ci i , where c i is the number of times w i appears in W . The partial derivatives of the MPN with respect to all nodes and weights is computed by Algorithm 2 modified to accommodate MPNs: (1) S becomes M , (2) when n is a sum node, the body of the forall loop is run once for j as the max-valued child.</p><p>The partial derivative of the logarithm of an MPN with respect to a weight takes the form</p><formula xml:id="formula_7">∂ log M ∂wi = ∂ log M ∂M ∂M ∂wi = 1 M ∂M ∂wi = ci • w c i -1 i w j ∈W \{w i } w c j j w j ∈W w c j j = ci wi</formula><p>The gradient of the conditional log likelihood with MPE inference is therefore ∆c i /w i , where ∆c i = c i -c i is the difference between the number of times w i is traversed by the two MPE inference paths in M [y, 1|x] and M [1, 1|x], respectively. The hard gradient update is then ∆w i = η ∂ ∂wi log P (y|x) = η ∆ci wi . The hard gradient for a training instance (x d , y d ) is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. In the first two expressions, the complete subcircuit traveled by each MPE inference is shown in bold. Product nodes do not have weighted children, so they do not appear in the gradient, depicted in the last expression</p><p>We can also easily add regularization to SPN training. An L2 weight penalty takes the familiar form of -λ||w|| 2 and partial derivatives -2λw i can be added to the gradient. With an appropriate optimization method, an L1 penalty could also be used for learning with marginal inference on dense SPN architectures. However, sparsity is not as important for SPNs as it is for Markov random fields, where a non-zero weight can have outsize impact on inference time; with SPNs inference is always linear with respect to model size.</p><p>A summary of the variations of Algorithm 1 is provided in Tables <ref type="table" target="#tab_2">1</ref> and<ref type="table">2</ref>. The generative hard gradient can be used in place of online EM for datasets where it would be prohibitive to store inference results from past epoch. For architectures that have high fan-in sum nodes, soft inference may be able to separate groups of modes faster than hard inference, which can only alter one child of a sum node at a time.</p><p>We observe the similarity between the updates of hard EM and hard gradient descent. In particular, if we reparameterize the SPN so that each child of a sum node is weighted by w i = e w i , the form of </p><formula xml:id="formula_8">= k∈P a(n) ∂S ∂S k l∈Ch(k)\{n} S l ∂M ∂Mn = k∈P a(n) ∂M ∂M k l∈Ch(k)\{n} M l Product ∂S ∂Sn = k∈P a(n) w kn ∂S ∂S k ∂M ∂Mn = k∈P a(n) w kn ∂M ∂M k : w kn ∈ W 0 : otherwise Weight ∂S ∂w ki = ∂S ∂S k S i ∂M ∂w ki = ∂M ∂M k M i Table 2: Weight updates Update Soft Inference Hard Inference Gen. GD ∆w = η ∂S[x,y] ∂w ∆w i = η ci wi Gen. EM P (H k = i|x, y) ∝ w ki ∂S[x,y] ∂S k P (H k = i|x, y) = 1 : w ki ∈ W 0 : otherwise Disc. GD ∆w = η 1 S[y,1|x] ∂S[y,1|x] ∂w - ∆w i = η ∆ci wi 1 S[1,1|x]</formula><p>∂S <ref type="bibr">[1,1|x]</ref> ∂w the partial derivative of the log MPN becomes</p><formula xml:id="formula_9">∂ log M ∂w i = 1 M ∂M ∂w i = ci w j ∈W e c j •w j w j ∈W e c j •w j = ci</formula><p>This means that the hard gradient update for weights in logspace is ∆w i = ∆c i , which resembles structured perceptron <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have applied discriminative training of SPNs to image classification benchmarks. CIFAR-10 and STL-10 are standard datasets for deep networks and unsupervised feature learning. Both are 10-class small image datasets. We achieve the best results to date on both tasks.</p><p>We follow the feature extraction pipeline of Coates et al. <ref type="bibr" target="#b9">[10]</ref>, which was also used recently to learn pooling functions <ref type="bibr" target="#b19">[20]</ref>. The procedure consists of extracting 4 × 10 5 6x6 pixel patches from the training set images, ZCA whitening those patches <ref type="bibr" target="#b18">[19]</ref>, running k-means for 50 rounds, and then normalizing the dictionary to have zero mean and unit variance. We then use the dictionary to extract K features at every 6x6 pixel site in the image (unit stride) with the "triangle" encoding</p><formula xml:id="formula_10">f k (x) = max{0, z -z k }, where z k = ||x -c k || 2 ,</formula><p>c k is the k-th item in the dictionary, and z is the average z k . For each image of CIFAR-10, for example, this yields a 27 × 27 × K feature vector that is finally downsampled by max-pooling to a G × G × K feature vector.  We experiment with a simple architecture that allows for discriminative learning of local structure. This architecture cannot be generatively trained as it violates consistency over X. Inspired by the successful star models in Felzenszwalb et al. <ref type="bibr" target="#b17">[18]</ref>, we construct a network with C classes, P parts per class, and T mixture components per part. A part is a pattern of image patch features that can occur anywhere in the image (e.g. an arrangement of patches that defines a curve). Each part filter f cpt is of dimension W × W × K and is initialized to 0. The root of the SPN is a sum node with a child S c for each class c in the dataset multiplied by the indicator for that state of the label variable Y. S c is a product over P nodes S cp , where each S cp is a sum node over T nodes S cpt . The hidden variables H represent the choice of cluster in the mixture over a part and its position (S cp and S cpt , respectively). Finally, S cpt sums over positions i, j in the image of the logistic function e xij • fcpt where the given variable x ij is the same dimension as f and parts can overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GxGxK</head><p>Notice that the mixture S cp models an additional level of spatial structure on top of the image patch features learned by k-means. Coates and Ng <ref type="bibr" target="#b11">[12]</ref> also learn higher-order structure, but whereas our method learns structure discriminatively in the context of a parts-based model, their unsupervised algorithm greedily groups features based on correlation and is unable to learn mixtures. Compared with the pooling functions in Jia et al. <ref type="bibr" target="#b19">[20]</ref> that model independent translation of patch features, our architecture models how nearby features move together. Other deep probabilistic architectures should be able to model high-level structure, but considering the difficulty in training these models with approximate inference, it is hard to make full use of their representational power. Unlike the star model of Felzenswalb et al. <ref type="bibr" target="#b17">[18]</ref> that learns filters over predefined HOG image features, our SPN learns on top of learned image features that can model color and detailed patterns.</p><p>Generative SPN architectures on the same features produce unsatisfactory results as generative training is led astray by the large number of features, very few of which differentiate labels. In the generative SPN paper <ref type="bibr" target="#b22">[23]</ref>, continuous variables are modeled with univariate Gaussians at the leaves (viewed as a sum node with infinite children but finite weight sum). With discriminative training, X can be continuous because we always condition on it, which effectively folds it into the weights.</p><p>All networks are learned with stochastic gradient descent regularized by early stopping. We found that using marginal inference for the root node and MPE inference for the rest of the network worked best. This allows the SPN to continue learning the difference between classes even when it correctly classifies a training instance. The fraction of the training set reserved for validation with CIFAR-10 and STL-10 were 10% and 20%, respectively. Learning rates, P , and T were chosen based on validation set performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on CIFAR-10</head><p>CIFAR-10 consists of 32x32 pixel images: 5 × 10 4 for training and 10 4 for testing. We first compare discriminative SPNs with other methods as we vary the size of the dictionary K. The results are seen in Figure <ref type="figure" target="#fig_5">4</ref>. To fairly compare with recent work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> we also set G = 4. In general, we observe that SPNs can achieve higher performance using half as many features as the next best approach, the learned pooling function. We hypothesize that this is because the SPN architecture allows us to discriminatively train large moveable parts, image structure that cannot be captured by larger dictionaries. In Jia et al. <ref type="bibr" target="#b19">[20]</ref> the pooling functions blur individual features (i.e. a 6x6 pixel dictionary item), from which the classifier may have trouble inferring the coordination of image parts.</p><p>We then experimented with a finer grid and fewer dictionary items (G = 7, K = 400). Pooling functions destroy information, so it is better if less is done before learning. Finer grids are less feasible for the method in Jia et al. <ref type="bibr" target="#b19">[20]</ref> as the number of rectangular pooling functions grows O(G 4 ). Our best test accuracy of 83.96% was achieved with W = 3, P = 200, and T = 2, chosen   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dictionary Accuracy Logistic Regression <ref type="bibr" target="#b23">[24]</ref> 36.0% SVM <ref type="bibr" target="#b4">[5]</ref> 39.5% SIFT <ref type="bibr" target="#b4">[5]</ref> 65.6% mcRBM <ref type="bibr" target="#b23">[24]</ref> 68.3% mcRBM-DBN <ref type="bibr" target="#b23">[24]</ref> 71.0% Convolutional RBM <ref type="bibr" target="#b9">[10]</ref> 78.9% K-means (Triangle) <ref type="bibr" target="#b9">[10]</ref> 4000, 4x4 grid 79.6 % HKDES <ref type="bibr" target="#b3">[4]</ref> 80.0% 3-Layer Learned RF <ref type="bibr" target="#b11">[12]</ref> 1600, 9x9 grid 82.0% Learned Pooling <ref type="bibr" target="#b19">[20]</ref> 6000, 4x4 grid 83.11% Discriminative SPN 400, 7x7 grid 83.96%  <ref type="bibr" target="#b10">[11]</ref> 54.9% (± 0.4%) 1-layer Sparse Coding <ref type="bibr" target="#b10">[11]</ref> 59.0% (± 0.8%) 3-layer Learned Receptive Field <ref type="bibr" target="#b11">[12]</ref> 60.1% (± 1.0%) Discriminative SPN 62.3% (± 1.0%) by validation set performance. This architecture achieves the highest published test accuracy on the CIFAR-10 dataset, remarkably using one fifth the number of features of the next best approach. We compare top CIFAR-10 results in Table <ref type="table" target="#tab_3">3</ref>, highlighting the dictionary size of systems that use the feature extraction from Coates et al. <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on STL-10</head><p>STL-10 has larger 96x96 pixel images and less labeled data (5,000 training and 8,000 test) than CIFAR-10 <ref type="bibr" target="#b9">[10]</ref>. The training set is mapped to ten predefined folds of 1,000 images. We experimented on the STL-10 dataset in a manner similar to CIFAR-10, ignoring the 10 5 items of unlabeled data. Ten models were trained on the pre-specified folds, and test accuracy is reported as an average. With K=1600, G=8, W =4, P =10, and T =3 we achieved 62.3% (± 1.0% standard deviation among folds), the highest published test accuracy as of writing. Notably, this includes approaches that make use of the unlabeled training images. Like Coates and Ng <ref type="bibr" target="#b11">[12]</ref>, our architecture learns local relations among different feature maps. However, the SPN is able to discriminatively learn latent mixtures, which can encode a more nuanced decision boundary than the linear classifier used in their work. After we carried out our experiments, Bo et al. <ref type="bibr" target="#b5">[6]</ref> reported a higher accuracy with their unsupervised features and a linear SVM. Just as with the features of Coates et al. <ref type="bibr" target="#b9">[10]</ref>, we anticipate that using an SPN instead of the SVM would be beneficial by learning spatial structure that the SVM cannot model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Sum-product networks are a new class of probabilistic model where inference remains tractable despite high treewidth and many hidden layers. This paper introduced the first algorithms for learning SPNs discriminatively, using a form of backpropagation to compute gradients. Discriminative training allows for a wider variety of SPN architectures than generative training, because completeness and consistency do not have to be maintained over evidence variables. We proposed both "soft" and "hard" gradient algorithms, using marginal inference in the "soft" case and MPE inference in the "hard" case. The latter successfully combats the diffusion problem, allowing deep networks to be learned. Experiments on image classification benchmarks illustrate the power of discriminative SPNs.</p><p>Future research directions include applying other discriminative learning paradigms to SPNs (e.g. max-margin methods), automatically learning SPN structure, and applying discriminative SPNs to a variety of structured prediction problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b22">(Poon &amp; Domingos, 2011</ref>) A sum-product network S is valid iff S(e) = Φ S (e) for all evidence e.In their paper, Poon and Domingos prove that there are two conditions sufficient for validity: completeness and consistency. Definition 3. (Poon &amp; Domingos, 2011) A sum-product network is complete iff all children of the same sum node have the same scope. Definition 4. (Poon &amp; Domingos, 2011) A sum-product network is consistent iff no variable appears negated in one child of a product node and non-negated in another. Theorem 1. (Poon &amp; Domingos, 2011) A sum-product network is valid if it is complete and consistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>LearnSPN Input: Set D of instances over variables X and label variables Y, a valid SPN S with initialized parameters. Output: An SPN with learned weights repeat forall the d ∈ D do UpdateWeights(S, Inference(S,x d ,y d )) until convergence or early stopping condition;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SPN architecture for experiments. Hidden variable indicators omitted for legibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Pooling, Jia et al. K-means (tri.), white, Coates et al. Auto-encoder, raw, Coates et al. RBM, whitened, Coates et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Impact of dictionary size K with a 4x4 pooling grid (W =3) on CIFAR-10 test accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The value of an SPN S[x 1 , x1 , . . . , x d , xd ] is the value of its root.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x 1</cell><cell>+ 0.3</cell><cell>0.7</cell><cell>0.5</cell><cell>+ 0.5</cell><cell>+ 0.6</cell><cell>0.4</cell><cell>0.9</cell><cell>+ 0.1</cell><cell>x 1</cell></row><row><cell></cell><cell>x 2</cell><cell></cell><cell></cell><cell>x 2</cell><cell>x 3</cell><cell></cell><cell></cell><cell>x 3</cell><cell></cell></row><row><cell cols="10">Figure 1: SPN over Boolean variables X 1 , X 2 , X 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Inference procedures</figDesc><table><row><cell>Node</cell><cell>Soft Inference</cell><cell>Hard Inference</cell></row><row><cell>Sum</cell><cell>∂S ∂Sn</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracies on CIFAR-10.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of average test accuracies on all folds of STL-10.</figDesc><table><row><cell>Method</cell><cell>Accuracy (±σ)</cell></row><row><cell>1-layer Vector Quantization</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A consistent SPN allows for MPE inference to reach the same indicator more than once in the same branching path</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research was partly funded by ARO grant W911NF-08-1-0242, AFRL contract FA8750-09-C-0181, NSF grant IIS-0803481, and ONR grant N00014-12-1-0312. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, AFRL, NSF, ONR, or the United States Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sum-product networks for modeling activities with stochastic structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thin junction trees</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="569" to="576" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Learning deep architectures for AI. Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for RGB-D based object recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISER</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-specific independence in bayesian networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twelfth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On probabilistic inference by weighted model counting. Artificial Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chavira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="772" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient principled learning of thin junction trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chechetka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In aistats11. Society for Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A differential approach to inference in Bayesian networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="280" to="305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling and Reasoning with Bayesian Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shallow vs. deep sum-product networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Neural Information Processing Systems</title>
		<meeting>the 25th Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Independent component analysis: algorithms and applications. Neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="411" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured learning with approximate inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williamstown, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sum-product networks: A new deep architecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Conf. on Uncertainty in Artificial Intelligence</title>
		<meeting>12th Conf. on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling pixel means and covariances using factorized third-order Boltzmann machines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2551" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expectation maximization algorithms for conditional likelihoods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
