<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<email>weilunchao@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jian-Jiun</forename><surname>Ding</surname></persName>
							<email>jjding@ntu.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
								<address>
									<addrLine>No. 1, Sec. 4, Roosevelt Rd</addrLine>
									<postCode>10617</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan, ROC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun-Zuo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
								<address>
									<addrLine>No. 1, Sec. 4, Roosevelt Rd</addrLine>
									<postCode>10617</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan, ROC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Q3 University of Southern California</orgName>
								<address>
									<addrLine>3601 South Flower Street, Tyler 1</addrLine>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>org/10.1016/j.sigpro.2015.04.007i 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51</addrLine>
									<postCode>53 55 57 59</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFA89DBF0C3C424C0EC00E1A40894D46</idno>
					<idno type="DOI">10.1016/j.sigpro.2015.04.007</idno>
					<note type="submission">Received 31 October 2014 Received in revised form 9 April 2015 Accepted 10 April 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Facial expression Expression-specific local binary pattern Class-regularized locality preserving projection Dimensionality reduction Feature extraction þ886 2 33669652; fax: þ 886 2 33663662. Signal Processing ] (]]]]) ]]]-]]]</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides a novel method for facial expression recognition, which distinguishes itself with the following two main contributions. First, an improved facial feature, called the expression-specific local binary pattern (es-LBP), is presented by emphasizing the partial information of human faces on particular fiducial points. Second, to enhance the connection between facial features and expression classes, class-regularized locality preserving projection (cr-LPP) is proposed, which aims at maximizing the class independence and simultaneously preserving the local feature similarity via dimensionality reduction. Simulation results show that the proposed approach is very effective for facial expression recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Because of its important role in human-computer interfaces (HCI), surveillance systems, and human entertainment, face recognition has attracted significant attention in pattern recognition and computer vision. Many algorithms about face verification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, facial age estimation <ref type="bibr" target="#b2">[3]</ref>, gender identification <ref type="bibr" target="#b3">[4]</ref>, and facial expression recognition <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> have been developed in recent years. In this paper, we focus on the problem of image-based facial expression recognition.</p><p>In general, algorithms of facial expression recognition can be simply divided into two steps: feature extraction and expression classification. In the first step, features that are related to the facial appearance or geometry <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> are extracted from the input face image for compact representation. Then, in the second step, according to the extracted features, an expression classifier <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> is applied to assign the input face an expression label (e.g. seven expression labels-angry, disgust, fear, joy, sadness, surprise, and neutral, which are usually considered in the literature). Besides these two steps, some algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> further include dimensionality reduction <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> as an intermediate step for avoiding the overfitting problem and filtering out some irrelevant features on facial expression.</p><p>However, even with so much work as mentioned above, there is still a salient gap between human and machines' ability on facial expression recognition. More accurately, some technical challenges have not been well solved. For example, different people, or even the same person, can have different expression patterns at different time or on different conditions. Therefore, how to design a robust feature extraction method that can handle this variety is still a critical problem in facial expression recognition. In addition, there intrinsically exist correlations among the seven expression classes, making some pairs of classes easy to be recognized; some others, hard to be classified. Nevertheless, the classifiers applied to facial expression recognition usually assume the independency among these classes: The widely-used support vector machine (SVM) <ref type="bibr" target="#b11">[12]</ref> and K-nearest neighbor classifier (KNN) <ref type="bibr" target="#b18">[19]</ref> consider no correlation among classes. This mismatch unavoidably leads to the bottleneck of expression recognition Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/sigpro where some pairs of classes are always with low recognition rates.</p><p>Considering these two challenges, in this paper, we present an improved method for facial expression recognition, which is based on the conventional three-step framework and with the following two main contributions:</p><p>Inspired by the human ability on expression recognition- with only partial information on faces, humans can still recognize facial expression with high accuracy-we propose the expression-specific local binary pattern (es-LBP) by computing the local LBP histograms <ref type="bibr" target="#b9">[10]</ref> around some particular fiducial points of human faces. Besides, to further include the spatial information in each local LBP window, a symmetric extension for the es-LBP is also presented.</p><p>To alleviate the mismatch between the properties of expression classes and classifiers, we propose the classregularized locality preserving projection (cr-LPP), which aims to push the samples of each class towards some pre-defined locations during the process of LPP <ref type="bibr" target="#b15">[16]</ref>, a popular dimensionality reduction algorithm in recent pattern recognition researches. Through specifically setting the pre-defined locations, the independence among classes can be enhanced, therefore effectively reducing the degree of mismatch.</p><p>From the simulations on the widely-used Japanese Female Facial Expression (JAFFE) database <ref type="bibr" target="#b19">[20]</ref>, the proposed method does outperform other existing approaches on both the leave-one-person-out (LOPO) and the cross validation settings, demonstrating the effectiveness of our ideas.</p><p>This paper is organized as follows: In Section 2, an overview on the related work is presented. In Section 3, we introduce the framework of our method, and describe the proposed feature extraction approach in Section 4; the detail of our cr-LPP algorithm is then presented in Section 5. In Section 6, the experimental results are provided and discussed. In Section 7 we conclude our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous and related work</head><p>Based on the existing literature, there are generally two tasks of facial expression recognition: action unit (AU) recognition based on the facial action coding system (FACS) <ref type="bibr" target="#b5">[6]</ref>, and discrete emotion recognition <ref type="bibr" target="#b4">[5]</ref>. In this paper, we focus on the second task because of its higher semantic meanings, and decide to recognize seven basic expressionsangry, disgust, fear, joy, sadness, surprise, and neutral-which are widely considered in previous work.</p><p>As mentioned in Section 1, a facial expression recognition system can be simply divided into feature extraction and expression classification, and some work further includes an intermediate dimensionality reduction step. In the rest of this section, a broad overview of the previous and related work in each step is given, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature extraction</head><p>As summarized from the previous work, feature extraction algorithms for facial expression recognition can be categorized into three types: geometric features, appearance features, and hybrid features. The first type, geometric features, focuses on extracting the shapes and locations of facial components, and computes several measures (e.g., the corresponding locations between pairs of facial components) to represent the face. Representative algorithms for this type can be referred to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>; some work further used the optical flow analysis to model muscular activities or estimate the displacements of feature points <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. However, as mentioned in <ref type="bibr" target="#b7">[8]</ref>, the geometric features commonly require accurate and reliable facial component detection and tracking, which are difficult to accommodate in many situations.</p><p>The second type of features are appearance (texture) features, which include Gabor features <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, radon discrete cosine transform (DCT) features <ref type="bibr" target="#b31">[32]</ref>, curvelet transform features <ref type="bibr" target="#b32">[33]</ref>, contourlet transform features <ref type="bibr" target="#b33">[34]</ref>, and local binary pattern (LBP) <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> are representative features. Gabor features, which could extract the spatialfrequency information at specific locations, orientations, and scales, were used for texture representation in the early age <ref type="bibr" target="#b8">[9]</ref>, and later brought into expression recognition <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. Gabor features do result in plausible recognition performance, but are both time and memory intensive to compute.</p><p>The local binary pattern (LBP) is another popular feature for expression recognition. It is similar to Gabor wavelets. The LBP was first proposed for texture recognition by Ojala et al. <ref type="bibr" target="#b36">[37]</ref> and later became widely-used for extracting local information in face recognition and related topics. The LBP essentially converts the neighboring pixels into the binary form using some thresholds and then uses these binary values to construct a label. Later, through summarizing the labels of all the pixels by histograms, a LBP feature vector could be obtained. In regular settings <ref type="bibr" target="#b9">[10]</ref>, faces images are first separated into several local patches; the LBP features (histogram) are computed from each patch and then concatenated into a long vector. The most important property of LBP features are their tolerance against illuminant changes and their computational simplicity. To better selecting the locations of local patches and filtering out some irrelevant histogram bins, boosting <ref type="bibr" target="#b18">[19]</ref> has been considered in the LBP for feature selection, resulting in the so-called boosted-LBP algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>There are still other features that have been created or introduced into expression recognition. For instance, the local phase quantization (LPQ) <ref type="bibr" target="#b37">[38]</ref>, a powerful texture descriptor robust to image blurring, was recently used in expression recognition <ref type="bibr" target="#b38">[39]</ref> and resulted in higher accuracy than the LBP features in many experiments. Berretti et al. <ref type="bibr" target="#b39">[40]</ref> and Hu et al. <ref type="bibr" target="#b40">[41]</ref> exploited the scaled invariant feature transform (SIFT) descriptor, which is widely-used in object recognition, extracted on a set of fiducial points for face representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Expression classification</head><p>Facial expression recognition is a multi-class classification problem. Hence, any such classification algorithm, such as the K nearest neighbors (KNN) <ref type="bibr" target="#b13">[14]</ref>, support vector machine (SVM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>, neural networks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, Bayesian networks <ref type="bibr" target="#b44">[45]</ref>, and rule-based classifiers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref>, have all been applied to expression classification. In this paper, we also utilized SVM with the radial basis function (RBF) kernel for expression classification, but introduced a preprocessing step by our class-regularized locality preserving projection (cr-LPP) to enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dimensionality reduction</head><p>The extracted features from human faces are usually of high dimensionality and may contain irrelevant or noisy information about facial expression. In order to avoid overfitting and enhance the discriminative ability of features, dimensionality reduction was included as an intermediate step before training the expression classifiers. Algorithms like the principal component analysis (PCA) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, linear discriminant analysis (LDA) <ref type="bibr" target="#b18">[19]</ref>, locally Euclidean embedding (LLE) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>, and locality preserving projection (LPP) <ref type="bibr" target="#b15">[16]</ref> all have been used for this task. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>, different dimensionality reduction algorithms was compared with the use of LBP and Boosted-LBP features.</p><p>Dimensionality reduction algorithms could also been directly applied on facial images for feature extraction. For example, PCA and LDA were utilized in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b47">48]</ref>; the independent component analysis (ICA) was exploited in <ref type="bibr" target="#b48">[49]</ref>. In fact, in face recognition, dimensionality reduction is the major way to extract facial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the proposed facial expression approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The problem setting of expression recognition</head><p>In this section, we give an overview about the proposed expression recognition approach. The problem setting used in this paper is defined as follows: Given a training set fi ðnÞ g N n ¼ 1 with N face images, and its corresponding label set Y ¼ fy ðnÞ A Lg N n ¼ 1 with L ¼ fl 1 ; …; l C g, building a facial expression recognition system can be modeled as a supervised machine learning task where the symbol C indicates the total number of expression labels concerned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework overview</head><p>The flowchart of the proposed approach is depicted in Fig. <ref type="figure" target="#fig_1">1</ref>, which follows the three-step framework-facial feature extraction, dimensionality reduction, and expression classification-as mentioned in the previous sections. When a facial image i is input, we first performed face detection <ref type="bibr" target="#b49">[50]</ref> and eye detection <ref type="bibr" target="#b50">[51]</ref> to extract, align, and normalize the face into a fixed-size patch î where the important facial components, or say fiducial points, of every input faces are located at approximately the same location in the patch. Then, the proposed es-LBP is performed to extract a d-dimensional feature vector xAR d for representing î; the vector xAR d is later fed into our cr-LPP, which results in a p-dimensional feature vector zAR p with much lower dimensionality (p⪡d) and with higher discriminative ability. Finally, the RBF-SVM is utilized for classifying zAR p into a proper facial expression class ŷ. In our approach, both cr-LPP and RBF-SVM in dimensionality reduction and expression classification are machine learning algorithms, and each requires a training phase to generate the corresponding projection matrix W LPP AR d Â p or the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expression-specific local binary pattern (es-LBP)</head><p>In this section, we first give a more detailed description on LBP features. Then, the proposed es-LBP and its symmetric extension for spatial information extraction are described. They can improve the accuracy of facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Local binary pattern (LBP)</head><p>As mentioned in Section 2, by thresholding the neighbors of each pixel with the central pixel value, the LBP gives each pixel a sequence of binary numbers for labeling. In the 3 Â 3 neighborhood case (with eight neighboring pixels) proposed in the original paper <ref type="bibr" target="#b36">[37]</ref>, each pixel will be assigned an 8-bit LBP label (as shown in Fig. <ref type="figure" target="#fig_2">2</ref>), meaning that there are totally 256 distinct labels. Then, through performing the histogram operation on the whole image, a 256-bin histogram could be obtained for representing the image. Since the histogram operation does not consider the spatial relationship among pixels, in tasks that are sensitive to the spatial displacement (e.g., face related topics), an image is first divided into several local patches, maybe overlapped and with different scales; the 256-bin histograms extracted from all these patches are then concatenated into a long feature vector for representation. Hence, the LBP feature is usually with high dimensionality and may result in the over-fitting problem during the classifier training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The idea of es-LBP</head><p>The human visual system is probably the best system in the world for recognizing facial expression, which can produce   high accuracy even under the far distance, low resolution, high image noise, and partial occlusion situations. It would be better if an automatic facial expression recognition system can also be robust to scaling, occlusion, and noise. To achieve this, we think that the LBP patches for expression recognition should be of different scales and specifically placed at the fiducial points with salient appearance variations under different expressions. Based on this idea, we propose the expression-specific local binary pattern (es-LBP).</p><p>As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, different from the original LBPbased system, which performs the histogram operation on the whole image, the proposed es-LBP method determines the local histograms in different scales. The input face image is first normalized at two different scales. Then, local patches around eight selected fiducial points: one forehead, two eyebrows, two eyes, two cheeks, and one mouth, are placed, as depicted in Fig. <ref type="figure" target="#fig_7">4</ref>. Then, given a face image, a longer LBP feature vector with the face information from two scales and eight specific fiducial points could be reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Symmetric extension for es-LBP</head><p>The eight selected patches in Fig. <ref type="figure" target="#fig_7">4</ref> provide significant information for expression recognition, even if some patches are occluded. However, if one performs the histogram operation for each patch directly, an accurate identification result may not be obtained in some condition.</p><p>For example, the two eyebrow pairs in Fig. <ref type="figure">5</ref> obviously have quite different shapes (the upper one corresponds to a sad face and the lower one corresponds to a happy face). However, their 59-bin LBP histograms look pretty similar. This serious problem is mainly due to that the histogram operator does not take the spatial information into consideration. After switching the left-half part and the righthalf part of an eyebrow, although the expression is changed, the resulting LBP histogram remains unchanged.</p><p>To solve this problem, we proposed the symmetric extension method and applied it to the patches of symmetric shapes (i.e., the mouth, eyebrow, and eye regions) by separating them into two symmetric regions before computing LBP histograms. That is, the mouth region is separated into the left-half mouth region and the right-half mouth region. The two eyebrow patches are separated into four patches (i.e., the left-half of the left eyebrow, the righthalf of the left eyebrow, the left-half of the right eyebrow, and the right-half of the right eyebrow). The eye part is separated in the similar way. With the symmetric extension method, the spatial information could be better preserved for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Class-regularized locality preserving projection (cr-LPP)</head><p>In the previous section, we introduce the proposed es-LBP feature and another powerful facial feature LPQ. The training set after feature extraction becomes X ¼ fx ðnÞ A R d g N n ¼ 1 with N d-dimensional feature vectors. In this section, we will describe the proposed dimensionality reduction method, class-regularized locality preserving projection (cr-LPP), which aims to maximize the class independence and simultaneously preserve the local feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The concept of class regularization</head><p>As mentioned in Section 1, there intrinsically exist correlations among the seven expression classes, making some pairs of classes easy to be recognized; some others, hard to be classified. In other words, given a training set fi ðnÞ g N n ¼ 1 and its corresponding label set Y ¼ fy ðnÞ A Lg N n ¼ 1 , some classes may originally separate from each other, but some are overlapped. This correlation, however, is seldom considered in classifier design for expression recognition. For example, the popular support vector machine (SVM) <ref type="bibr" target="#b11">[12]</ref>, widely-used in expression recognition, does not take the class correlation into consideration in either the oneversus-one setting or the one-versus-all setting. Therefore, the classes that are strongly overlapped are still hard to recognize even by finely tuned the classifier. To solve this problem and jointly consider the possible over-fitting problem, we include the dimensionality reduction step in our work, and try to separate the overlapped classes, or say maximize the independence among classes, in this step before training the classifier.  Among many existing dimensionality reduction algorithms, we select the locality preserving projection (LPP) because of its effectiveness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> and the ease for extension. Given a training set X ¼ fx ðnÞ A R d g N n ¼ 1 and a desired output dimensionality p (i.e., p od), LPP aims to learn a matrix W LPP A R d Â p that minimizes the average neighbor distance among the projected output samples between any pair of neighboring samples for local geometry modeling; if x (i) and x (j) are not neighbors, b ij ¼b ji ¼0. Finally in the third step, the projection matrix W LPP is reached through minimizing the following objective function E LPP (W)</p><formula xml:id="formula_0">Z ¼ fz ðnÞ A R p g N n ¼ 1 ,</formula><formula xml:id="formula_1">E LPP ðWÞ ¼ X N i ¼ 1 X N j ¼ 1 b ij jjW T ðx ðiÞ À x ðjÞ Þjj 2 ¼ X N i ¼ 1 X N j ¼ 1 b ij jjz ðiÞ Àz ðjÞ jj 2<label>ð2Þ</label></formula><p>with the constraint W T XDX T W¼I (for preventing trivial solutions) where X is a d Â N matrix containing each training sample in the corresponding column, and D is a diagonal matrix with d ii ¼ P b ij . To sum up, LPP tries to bring the neighbors (searched in X) as closer as possible in the output pdimensional space, according to the neighbor similarity B. Furthermore, the learned matrix W LPP can be used directly for dimensionality reduction on the testing data.</p><p>When the class label y(n) of each training sample x(n) is provided in the training phase, the algorithm of LPP could be adjusted into the supervised version by searching neighbors with only the same class label in the first step <ref type="bibr" target="#b17">[18]</ref>; the other parts of the LPP algorithm are remained unchanged. Since now the neighbors are defined not only by feature similarity, but also by the same-label constraint, the connection between features and labels can be explicitly linked, hence leading to more representative features for classification after dimensionality reduction. The implementation details of LPP could be traced from the proposed method in Table <ref type="table" target="#tab_0">1</ref>.  As described in Eq. ( <ref type="formula" target="#formula_1">2</ref>), supervised LPP tries to minimize the distance of same-label neighbors after projection; however, it does not aim to separate samples from different classes. To include the concept of class regularization (maximizing the independence among classes) into LPP, we proposed to setup a target location for each class in the pdimensional space before training W LPP . That is, we want the training samples of each class to be located around the predefined target location after projection. Then through a proper definition of the target locations, the correlation among classes could be much reduced. Note that to maintain the local preserving property of LPP, only part of the training samples (i.e., seed samples) are selected to fit the new criterion. The flowchart of cr-LPP is shown in Fig. <ref type="figure" target="#fig_10">6</ref>.  The detailed process of the proposed cr-LPP algorithm.</p><formula xml:id="formula_2">X 2 ¼ fx ðmÞ 2 A R d g N À M m ¼ 1 where X 1 \ X 2 ¼ ϕ. Meanwhile, the</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Presetting:</head><p>Training set:</p><formula xml:id="formula_3">X ¼ fx ðnÞ A R d g N n ¼ 1 (X is represented as a d Â N matrix), Y ¼ fy ðnÞ A L ¼ fl 1 ; …; l C gg N n ¼ 1 .</formula><p>Set the C pre-defined locations T Pre ¼ ft Pre ðcÞ A R p g C c ¼ 1 for each class where t Pre ðcÞ means the target location for class l c .</p><p>Set up the seed training set X 1 ¼ fx ðmÞ</p><formula xml:id="formula_4">1 A R d g M m ¼ 1 by extracting M (Mo N) samples from X.</formula><p>The remaining (N À M) samples then form another set X 2 ¼ fx ðmÞ</p><formula xml:id="formula_5">2 A R d g N À M m ¼ 1 where X 1 \ X 2 ¼ ϕ. Meanwhile, the label set Y could also be separated into Y 1 ¼ fy ðmÞ 1 g M m ¼ 1 and Y 2 ¼ fy ðmÞ 2 g N À M m ¼ 1</formula><p>, and X and Y can be rewritten as</p><formula xml:id="formula_6">X ¼ X 1 X 2 Â Ã and Y ¼ Y 1 Y 2 Â Ã with permutation.</formula><p>Based on Y 1 and T Pre , the seed target setZ 1 ¼ fz ðmÞ</p><formula xml:id="formula_7">1 ¼ t ðy ðmÞ 1 Þ Pre g M m ¼ 1 is also defined. Create an N Â N sample similarity matrix B ¼ ½b ij ¼ 0 1 r i;j r N .</formula><p>The goal of cr-LPP is to achieve the projection matrix W crÀLPP A R dÂp ; then we have z ¼ W T crÀLPP x A R p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm:</head><p>Define the same-label set NeiðiÞ for each sample x ðiÞ : NeiðiÞ ¼ fx ðjÞ y ðjÞ ¼ y ðiÞ ; ja i g.</p><p>For each sample x, find the k 1 -nearest samples in NeiðiÞ, and denote these samples as KNN(i). The parameter k 1 defines the number of neighboring samples.</p><p>For each sample pair fx ðiÞ ; x ðjÞ g, if x ðjÞ A KNNðiÞ or x ðiÞ A KNNðjÞ, set:</p><formula xml:id="formula_8">b i j ¼ expðÀ‖x ðiÞ À x ðjÞ ‖ 2 =tÞ: Compute L ¼ D À B where D is a diagonal matrix with d ii ¼ P j b ij .</formula><p>We have (given 0r β r1 the balancing factor): </p><formula xml:id="formula_9">W crÀLPP ¼ X 2βL 11 þð1À βÞI 2βL 12 2βL 21 2βL 22 " # X ( ) À 1 ð1 À βÞX 1 Z T 1 where L ¼ L 11 A R MÂM L 12 A R MÂðN À MÞ L 21 A R ðN À MÞÂM L 22 A R ðN À MÞÂðN À MÞ " # A R NÂN :</formula><formula xml:id="formula_10">seed target set Z 1 ¼ fz ðmÞ 1 ¼ t ðy ðmÞ 1 Þ</formula><p>Pre g M m ¼ 1 is also defined. According to these new-defined sets and notations, we propose the new objective function E cr-LPP (W)</p><formula xml:id="formula_11">E crÀLPP W ð Þ¼β X N i ¼ 1 X N j ¼ 1 b ij :W T x ðiÞ Àx ðjÞ : 2 |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} ELPP<label>ðWÞ</label></formula><p>þð1À βÞ</p><formula xml:id="formula_12">X M i ¼ 1 ‖W T x ðiÞ À z ðiÞ 1 ‖ 2 |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} Ecr ðWÞ<label>ð3Þ</label></formula><p>where 0 rβr1 is a balancing factor between E LPP (W) and E cr (W). To achieve the projection matrix W cr-pp , we perform minimization on E cr-LPP (W)</p><formula xml:id="formula_13">W crÀLPP ¼ min W E cr À LPP ðWÞ ¼ min W β X N i ¼ 1 X N j ¼ 1 b ij :W T x ðiÞ À x ðjÞ : 2 þð1 À βÞ X M i ¼ 1 ‖W T x ðiÞ À z ðiÞ 1 ‖ 2 ¼ min W β X N i ¼ 1 X N j ¼ 1 2b ij x ðiÞT WW T x ðiÞ À 2b ij x ðiÞT WW T x ðiÞ 2 4 þð1 À βÞ X M i ¼ 1 x ðiÞT WW T x ðiÞ À 2z ðiÞT 1 W T x ðiÞ þ z ðiÞT 1 z ðiÞ 1 # ¼ min W 2β X N i ¼ 1 d ii x ðiÞT WW T x ðiÞ " À 2β X N i ¼ 1 X N j ¼ 1 b ij x ðiÞT WW T x ðiÞ þð1 À βÞ X M i ¼ 1</formula><p>x ðiÞT WW T x ðiÞ À 2ð1 À βÞ</p><formula xml:id="formula_14">X M i ¼ 1 z ðiÞT 1 W T x ðiÞ # ¼ min W 2β Â trðW T X 1 X 2 Â Ã L X 1 X 2 Â Ã T WÞ h þð1 À βÞ Â trðW T X 1 X T 1 WÞÀ2ð1 À βÞ Â trðW T X 1 Z T 1 Þ i<label>ð4Þ</label></formula><p>where D is a diagonal matrix with d ii ¼ P b ij and L ¼D À B is the Laplacian matrix of B. Then through taking the partial derivative of Eq. ( <ref type="formula" target="#formula_14">4</ref>) to be zero</p><formula xml:id="formula_15">2β½X 1 X 2 L X 1 X 2 T W þð1 ÀβÞX 1 X 1 T W Àð1 À βÞX 1 Z 1 T ¼ 0; h<label>ð5Þ</label></formula><formula xml:id="formula_16">we get W ¼ X 1 X 2 Â Ã 2βL 11 þð1 ÀβÞI 2βL 12 2βL 21 2βL 22 " # X 1 X 2 Â Ã T ( ) À 1 ð1 À βÞX 1 Z T 1<label>ð6Þ</label></formula><p>where</p><formula xml:id="formula_17">L ¼ L 11 A R MÂM L 12 A R MÂðN À MÞ L 21 A R ðN À MÞÂM L 22 A R ðN À MÞÂðN À MÞ " # A R NÂN :<label>ð7Þ</label></formula><p>The main difference between the algorithms of LPP and cr-LPP lies in the optimization process: No constraint like W T XDX T W¼I is required for cr-LPP. This is because of the E cr (W) term included in Eq. ( <ref type="formula" target="#formula_12">3</ref>). The detailed algorithm of the proposed cr-LPP is presented in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The connection with RBF-SVM classifier</head><p>With the learned W crÀLPP A R MÂM , the training set X ¼ fx ðnÞ A R d g N n ¼ 1 now can be projected into a lower dimensional space using the process in Section 5.3 and get Z ¼ fz ðnÞ A R p g N n ¼ 1 where p od to reduce the effect of noise and the computation time for classification. The learned W crÀLPP A R MÂM is also used to reduce the dimensionality of the extracted facial features on the testing phase.</p><p>After dimensionality reduction, the output p-dimensional feature vector is then fed into the trained RBF-SVM classifier for recognition. We adopt the SVM because it can generate the hyperplane that has the largest distance to the data points of the two classes. The RBF kernel is applied because it uses a Gaussian function K(x, y)¼exp(À ||xÀ y|| 2 / σ) to map the distance of two feature vectors x and y into a value between 0 and 1. We can set x as the feature vector of the landmark data and apply K(x, y) as a new feature of the input data. Although the SVM model with an RBF kernel is nonlinear, it is easy to implement since its optimization process is similar to that of the linear SVM model. We randomly choose a part of the input data as the training set and treat others as the test set and use the code provided in <ref type="bibr" target="#b51">[52]</ref> to find the optimal classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Simulation results</head><p>In this section, several simulations are performed and discussed. In Section 6.1, the experimental settings are introduced. Then, we compare the combinations of different feature extraction, dimensionality reduction, and classification algorithms in Section 6.2. In Section 6.3, the best combination in Section 6.2 is further compared with the existing algorithms to demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The experimental setting</head><p>As mentioned in Section 2, seven expressions-angry, disgust, fear, joy, sadness, surprise, and neutral-are considered in our experiments. We adopt the widely-used Japanese Female Facial Expression (JAFFE) database <ref type="bibr" target="#b19">[20]</ref>, which is composed of 213 images from 10 individuals: For each individual, there are 3-4 images corresponding to the same expression.</p><p>To evaluate the performance, two criteria are utilized: 10-fold cross validation and leave-one-person-out (LOPO). To balance the size of each fold in 10-fold cross validation, only 210 images of the JAFFE database are considered. All of the 213 images are considered when applying LOPO. Both these two criteria are widely-used in literature.  <ref type="figure" target="#fig_1">1</ref>). In the facial feature extraction step, the LBP features, the local phase quantization (LPQ) features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref>, the proposed es-LBP features, the proposed es-LBP-s features (es-LBP with symmetric extension), and their combinations (concatenations) are compared. In the dimensionality reduction step, we compare LPP, MFA <ref type="bibr" target="#b17">[18]</ref>, and the proposed cr-LPP method. Finally in the expression classification step, the K-nearest neighbor classifier (KNN) and the RBF-SVM <ref type="bibr" target="#b11">[12]</ref> are experimented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">The comparison of different algorithm combinations</head><p>Since in our simulations the LPQ feature is adopted, we provide a brief introduction about it. First, the short-time Fourier transform is performed in a block f(x) around a pixel y. Then, extract the Fourier coefficients F(u, y) at four specific u, u 1 ½α; 0 T ; u 2 ½0; α T ; u 3 ½α; α T ; u 4 ½α; Àα T where α is small enough to ensure ∠HðuÞ ¼ 0. The extracted four complex-valued coefficients F y ¼ ½Fðu 1 ; yÞ; Fðu 2 ; yÞ; Fðu 3 ; yÞ; Fðu 4 ; yÞ are then rewritten as an 8-dimensional vector q y ¼ ½RefF y g; ImfF y g T . Finally, by taking the sign of q y , an 8bit binary vector qy could be achieved for the block around each y. After performing the above operation to all of the pixels and computing the histogram in each local patch, a feature vector similar to the output of the LBP can be obtained.</p><p>The 10-fold cross validation results and the LOPO results are then presented in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table">3</ref>. As shown, the proposed es-LBP feature outperforms the conventional LBP feature, and the proposed es-LBP-s feature further improves the performance of the es-LBP feature. In summary, the combination of the es-LBP-s feature and cr-LPP outperforms the combination of the LBP feature and LPP by about 6-10% in both criteria, demonstrating the effectiveness of our methods.</p><p>Moreover, the LPQ feature, which was originally used for texture classification, is also included in the proposed system to improve the performance of feature extraction. When concatenating the LPQ features and the LBP features, we observe further improvement on the recognition results by 1-2%. However, even higher recognition rates can be achieved if one combines LPQ features with the proposed es-LBP (or es-LBP-s) features.</p><p>When the method of feature extraction is fixed, we find that the proposed dimensionality reduction method of cr-LPP always outperform LPP and MFA methods in almost all of the cases. Moreover, when the feature extraction and the dimensionality reduction methods are both fixed, the RBF-SVM system always outperforms the KNN.</p><p>The highest recognition rate can be acquired if one applies the features of LPQþes-LBP-s, the dimensionality reduction method of cr-LPP, and the RBF-SVM classification system. Using this combination, the recognition rates of 96.19% in the 10-fold cross validation case and 76.67% in the LOPO case can be achieved, which proves the effectiveness of the proposed techniques of es-LBP-s feature extraction and cr-LPP for facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison with existing algorithms</head><p>In this subsection, we compare the best combination of the proposed method in Section 6.2 with existing facial expression recognition algorithms and the results are presented in Table <ref type="table" target="#tab_2">4</ref> (the LOPO case) and Table <ref type="table" target="#tab_3">5</ref> (the 10-fold cross validation case). Note that, to avoid poor fold selection, we perform the 10-fold cross validation several times and shows the average recognition rate. As shown in Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table" target="#tab_3">5</ref>, the proposed method, which is a combination of the LPQþes-LBP-s feature extraction method, the cr-LPP dimensionality reduction method, and the RBF-SVM classifier, outperforms the existing algorithms in both criteria. Since the proposed es-LBP-s features take the spatial information into consideration and the proposed cr-LPP method can maximize the class independence, using these techniques can improve the performance of facial expression recognition.  In this paper, a very effective facial expression recognition method is proposed. To enhance the performance of the popular LBP feature, we applied the techniques of expression specific extension and symmetric extension and proposed the es-LBP-s feature, which could better capture the local information of faces on important fiducial points. Moreover, the LPQ feature, which was originally used for texture classification, is also included in the proposed facial expression recognition system. Furthermore, to better separate different expressions and match the usage of classifiers, we proposed the class-regularized locality preserving projection (cr-LPP) method for dimensionality reduction.</p><p>Simulations show that the proposed method achieves the highest recognition rate against existing algorithms in both the leave-one-person-out (LOPO) and 10-fold cross validation criteria. Especially, for the 10-fold cross validation case, the recognition rate of the proposed method is 96.19%. Since the es-LBP-s feature and the LPQ features can well extract the spatial information and the cr-LPP method can simultaneously maximizes the class independence and preserves the local feature similarity, the proposed system can achieve good performance for facial expression identification.</p><p>Moreover, some pre-processing techniques can be applied to further improve the performance of the proposed facial expression recognition system. For example, in the case where the input image is acquired from the unconstrained environment and seriously affected by noise, one can impose the denoising algorithm (such as PCA) in the pre-processing process to improve the performance. When the input image is seriously blurred, one can apply the enhancement technique (such as the inverse Gaussian filter) to clarify the edge part of the input image. Uncited Q2 references <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Gabor filter Adaboost SVM 71.9 <ref type="bibr" target="#b13">[14]</ref> LBP LPP KNN 72.4 <ref type="bibr" target="#b13">[14]</ref> BoostLBP LPP KNN 74.2 <ref type="bibr" target="#b4">[5]</ref> BoostLBP N/A SVM 81.0 <ref type="bibr" target="#b12">[13]</ref> LBP N/A FSVM 83.25 <ref type="bibr" target="#b14">[15]</ref> LBP LFDA SVM 85.71 <ref type="bibr" target="#b41">[42]</ref> ASM N/A SVM 89.5 <ref type="bibr" target="#b25">[26]</ref> Gabor filter Linear Programming Linear Programming 91.0 <ref type="bibr" target="#b25">[26]</ref> Gabor filter N/A SVM 91.9 <ref type="bibr" target="#b29">[30]</ref> DCT </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Please cite this article as: W.-L. Chao, et al., Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection, Signal Processing (2015), http://dx.doi.org/10.1016/j.sigpro.2015.04</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart of the proposed method.</figDesc><graphic coords="3,284.52,436.41,216.00,105.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The flowchart for extracting the LBP feature at a specific pixel: The operator assigns a label to every pixel of an image by thresholding the 3 Â 3 neighborhood of each pixel with the center pixel value, and considers the result as a binary number.</figDesc><graphic coords="3,302.52,572.10,180.00,80.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed es-LBP feature, which includes three parts: the large scaled face part, the small scaled face part, and fiducial points from left to right, respectively.</figDesc><graphic coords="4,111.96,535.66,324.00,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>Locality preserving projection (LPP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where z ðnÞ ¼ W T LPP x ðnÞ A R p . The algorithm of LPP comprises three steps. In the first step, each sample in X searches its k 1 nearest neighbors via the Euclidean distance. Then, in the second step, an N Â N matrix B¼[b ij ] 1ri,jrN is constructed to record the feature similarity b ij ¼ expðÀ‖x ðiÞ Àx ðjÞ ‖ 2 =tÞ ð 1Þ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The fiducial points selected in the proposed es-LPP. From left to right are cheeks, mouth, eyebrows with forehead, and eyes, respectively.</figDesc><graphic coords="5,89.97,55.40,360.00,120.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 . 3 .</head><label>53</label><figDesc>Fig. 5. Illustration of LBP histograms from the eyebrows of a happy face and a sad face. As shown, the two histograms look very similar.</figDesc><graphic coords="5,107.97,203.07,324.00,203.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Given a training set X ¼ fx ðnÞ A R d g N n ¼ 1 with Y¼fy ðnÞ A L ¼ fl 1 ; …; l C gg N n ¼ 1 and C pre-defined locations T Pre ¼ ft Pre ðcÞ A R p g C c ¼ 1 , where t Pre ðcÞ indicates the target location for class l c , we first set up a seed training set X 1 ¼ fx ðmÞ 1 A R d g M m ¼ 1 by extracting M (MoN) samples from X. The remaining (N À M) samples then form another set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Flowchart of the proposed cr-LPP: At first, some seed training samples are extracted from the training set and the target locations for these seed samples are pre-defined in the reduced (lower-dimensional) space after dimensionality reduction. Based on these seed samples and target locations, cr-LPP is performed to achieve better separation among classes in the reduced (lower-dimensional) space.</figDesc><graphic coords="6,288.50,412.23,216.00,203.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>To evaluate the effectiveness of the proposed algorithms, we compare different feature extraction, dimensionality reduction, and classification algorithms in our system flowchart Please cite this article as: W.-L. Chao, et al., Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection, Signal Processing (2015), http://dx.doi.org/10.1016/j.sigpro.2015.04in Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The 10-fold cross validation recognition rate of different combinations in our framework.</figDesc><table><row><cell></cell><cell>LPP</cell><cell></cell><cell>MFA</cell><cell></cell><cell>cr-LPP</cell><cell></cell></row><row><cell></cell><cell>KNN</cell><cell>SVM</cell><cell>KNN</cell><cell>SVM</cell><cell>KNN</cell><cell>SVM</cell></row><row><cell>LPQ</cell><cell>91.43</cell><cell>92.86</cell><cell>92.49</cell><cell>92.86</cell><cell>93.81</cell><cell>94.76</cell></row><row><cell>LBP</cell><cell>86.19</cell><cell>88.10</cell><cell>87.62</cell><cell>88.10</cell><cell>88.57</cell><cell>88.57</cell></row><row><cell>es-LBP</cell><cell>87.62</cell><cell>89.05</cell><cell>90.48</cell><cell>91.43</cell><cell>90.95</cell><cell>91.43</cell></row><row><cell>es-LBP-s</cell><cell>89.05</cell><cell>89.52</cell><cell>91.43</cell><cell>91.90</cell><cell>92.38</cell><cell>92.38</cell></row><row><cell>LPQ þLBP</cell><cell>92.38</cell><cell>93.33</cell><cell>93.33</cell><cell>93.33</cell><cell>94.76</cell><cell>94.76</cell></row><row><cell>LPQþ es-LBP</cell><cell>93.81</cell><cell>94.76</cell><cell>94.76</cell><cell>95.24</cell><cell>95.24</cell><cell>95.71</cell></row><row><cell>LPQþ es-LBP-s</cell><cell>94.29</cell><cell>94.76</cell><cell>95.24</cell><cell>95.71</cell><cell>95.71</cell><cell>96.19</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">The LOPO recognition rate of different combinations in our framework.</cell></row><row><cell></cell><cell>LPP</cell><cell></cell><cell>MFA</cell><cell></cell><cell>cr-LPP</cell><cell></cell></row><row><cell></cell><cell>KNN</cell><cell>SVM</cell><cell>KNN</cell><cell>SVM</cell><cell>KNN</cell><cell>SVM</cell></row><row><cell>LPQ</cell><cell>65.71</cell><cell>68.10</cell><cell>67.62</cell><cell>70.95</cell><cell>71.90</cell><cell>72.86</cell></row><row><cell>LBP</cell><cell>60.48</cell><cell>63.33</cell><cell>62.86</cell><cell>64.29</cell><cell>66.19</cell><cell>66.67</cell></row><row><cell>es-LBP</cell><cell>61.90</cell><cell>64.29</cell><cell>63.81</cell><cell>65.71</cell><cell>66.67</cell><cell>67.62</cell></row><row><cell>es-LBP-s</cell><cell>63.81</cell><cell>68.09</cell><cell>65.24</cell><cell>69.05</cell><cell>69.05</cell><cell>70.48</cell></row><row><cell>LPQ þLBP</cell><cell>66.19</cell><cell>69.05</cell><cell>69.52</cell><cell>71.43</cell><cell>71.90</cell><cell>73.33</cell></row><row><cell>LPQþ es-LBP</cell><cell>65.71</cell><cell>69.52</cell><cell>70.00</cell><cell>71.43</cell><cell>72.38</cell><cell>73.81</cell></row><row><cell>LPQþ es-LBP-s</cell><cell>67.62</cell><cell>70.48</cell><cell>70.48</cell><cell>71.43</cell><cell>72.86</cell><cell>76.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparison of the LOPO recognition rate on the JAFFE database.</figDesc><table><row><cell>Ref.</cell><cell>Feature</cell><cell>Dimensionality</cell><cell cols="2">Classifier Recognition</cell></row><row><cell></cell><cell>extraction</cell><cell>reduction</cell><cell></cell><cell>rate (%)</cell></row><row><cell>[36]</cell><cell>LBP</cell><cell>PCA</cell><cell>SVM</cell><cell>53.8</cell></row><row><cell>[27]</cell><cell>Gabor</cell><cell>MV-Boost</cell><cell>SVM</cell><cell>58.7</cell></row><row><cell></cell><cell>histogram</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[54]</cell><cell>LBP</cell><cell>SRC</cell><cell>SVM</cell><cell>62.9</cell></row><row><cell>[34]</cell><cell>CT</cell><cell>LLE</cell><cell>SVM</cell><cell>63.81</cell></row><row><cell>[36]</cell><cell>LBP</cell><cell>GDA</cell><cell>SVM</cell><cell>65.7</cell></row><row><cell>[36]</cell><cell>LBP</cell><cell>Adaboost</cell><cell>SVM</cell><cell>65.71</cell></row><row><cell>[34]</cell><cell>ASM</cell><cell>N/A</cell><cell>SVM</cell><cell>68.5</cell></row><row><cell>[31]</cell><cell>LBP þGabor</cell><cell>N/A</cell><cell>Ensemble</cell><cell>70.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SVMs</cell><cell></cell></row><row><cell>[29]</cell><cell>Gabor filter</cell><cell>N/A</cell><cell>Fusion</cell><cell>72.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SVM</cell><cell></cell></row><row><cell>[55]</cell><cell>MB-LGBP</cell><cell>N/A</cell><cell>Two level</cell><cell>74.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SVM</cell><cell></cell></row><row><cell cols="3">Proposed LPQ þ es-LBP-s cr-LPP</cell><cell>SVM</cell><cell>76.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Comparison of the 10-fold cross validation recognition rate on the JAFFE database.</figDesc><table><row><cell>Ref.</cell><cell>Feature extraction</cell><cell>Dimensionality reduction</cell><cell>Classifier</cell><cell>Recognition rate (%)</cell></row><row><cell>[14]</cell><cell>BoostLBP</cell><cell>SONPP</cell><cell>KNN</cell><cell>66.8</cell></row><row><cell>[26]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>W.-L. Chao et al. / Signal Processing ] (]]]]) ]]]-]]]</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face verification through tracking facial features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2969" to="2981" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminant analysis for recognition of human face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1724" to="1733" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial age estimation based on labelsensitive learning and age-oriented regression</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="628" to="641" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gender identification in face images using KPCA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jayanthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature &amp; Biologically Inspired Computing</title>
		<imprint>
			<biblScope unit="page" from="1414" to="1418" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: a comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fasela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active and dynamic information fusion for facial expression understanding from image sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="699" to="714" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facial expression recognition via fuzzy support vector machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Intell. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="495" to="500" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comprehensive empirical study on linear subspace methods for facial expression analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="153" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression recognition using local fisher discriminant analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="443" to="448" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph embedding and extension: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoke</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time inference of complex mental states from facial expressions and head gestures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">El</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-time Vision for Human-computer Interaction</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Kisacanin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science þ Business Media Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial action recognition for facial expression analysis from static face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1449" to="1461" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man. Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing human facial expression from long image sequences using optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Value directed learning of gestures and facial displays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1026" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous feature selection and classifier training via linear programming a case study for face expression recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="346" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on Gabor histogram feature and MVBoost</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1089" to="1096" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gabor feature-based face recognition using supervised locality preserving projection, Signal Process</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facial expression analysis across databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Multimedia Technology</title>
		<meeting>IEEE Conference on Multimedia Technology</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hybrid method of feature extraction for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Signal-Image Technology and Internet-Based System</title>
		<meeting>International Conference on Signal-Image Technology and Internet-Based System</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="422" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facial expression recognition using ensemble of classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zavaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1489" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Radon and discrete cosine transforms based feature extraction and dimensionality reduction approach for face recognition, Signal Process</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Raghunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="2604" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Curvelet based face recognition via dimension reduction, Signal Process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new approach of facial expression recognition based on contourlet transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Wavelet Analysis and Pattern Recognition</title>
		<meeting>International Conference on Wavelet Analysis and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="275" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial expression recognition with local binary patterns and linear programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="546" to="548" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining LBP and Adaboost for facial expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zilu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xieyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Signal Processing</title>
		<meeting>IEEE Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1461" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blur insensitive texture classification using local phase quantization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ojansivu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Signal Process</title>
		<imprint>
			<biblScope unit="volume">5099</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotion recognition using PHOG and LPQ features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition Workshop</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="878" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A set of selected SIFT features for 3D facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4125" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Geometric feature based facial expression recognition using multiclass support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="318" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comparison between geometry-based and Gabor-wavelets-based facial expression recognition using multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluation of face resolution for expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="82" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences: temporal and static modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Expert system for automatic analysis of facial expression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="881" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition by independent component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Projection functions for eye detection</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1049" to="1056" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">LIBSVM -A library for SVM</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="〈http://www.csie.ntu.edu.tw/$cjlin/libsvm/〉" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recognition of blurred faces using local phase quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ojansivu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A new method for facial expression recognition based on sparse representation plus LBP based on sparse representation plus LBP</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Congress on Image and Signal Processing</title>
		<meeting>International Congress on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on MB-LGBP feature and multi-level classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Intell. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
