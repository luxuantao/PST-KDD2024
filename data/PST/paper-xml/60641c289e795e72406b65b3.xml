<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Graph Neural Networks with Structural Adaptive Receptive Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Junshan Wang *</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Hanyue Chen</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Yuanpei College</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Guojie Song †</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Graph Neural Networks with Structural Adaptive Receptive Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449896</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>receptive fields</term>
					<term>mutual information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The abundant information in graphs helps us to learn more expressive node representations. Different nodes in the neighborhood have different importance to the central node. Thus, average weight aggregation in most Graph Neural Networks would fail to model such difference. GAT-based models introduce the attention mechanism to solve this problem, but they ignore the rich structural information and may suffer from the problem of over-smoothing. In this paper, we propose Graph Neural Networks with STructural Adaptive Receptive fields (STAR-GNN), which adaptively construct a receptive field for each node with structural information and further achieve better aggregation of information. Firstly, we model local structural distribution based on anonymous random walks, followed by using the structural information to construct receptive fields guided with mutual information. Then, as the generated receptive fields are irregular, we design a sub-graph aggregator to boost node representations and theoretically prove that it has the ability to capture the complex structures in receptive fields. Experimental results demonstrate the power of STAR-GNN in learning structural receptive fields adaptively and encoding more informative structural characteristics in real-world networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Composing entities with links between them, graph-structured data can represent various data in different domains. For example, social networks specify the social connections between people; citation networks show the citation relations between papers. With the proliferation of graph-structured data, the need for models that can capture rich information is becoming increasingly urgent.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref> have gained tremendous attention in recent years. They can mine rich information on graphs and be applied to different downstream tasks, including node classification, link prediction, etc, where they achieve great improvements compared with traditional methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. GNNs define the convolution operations on the neighbourhood of nodes. They collect the neighbourhood information of the central node through different aggregation functions, such as mean, sum or max-pooling aggregation, to generate representations of nodes.</p><p>For a central node, the importance of different neighbours varies. However, traditional GNNs do not distinguish the importance of nodes. Most of them treat all nodes in the neighbourhood equally or define the importance based on edge weights. Thus, they fail to capture other information that implicitly determines importance in the neighbourhood. Graph Attention Network (GAT) <ref type="bibr" target="#b30">[31]</ref> adopts the attention mechanism to solve this problem to some extent. GAT defines the importance of neighbours to the central node according to the feature similarity between them, so that the model can pay more attention to neighbours that bear similar features to the central node. However, GAT still has many shortcomings. On one hand, it only considers the similarity between node features, ignoring the similarity of topological structures of the neighbourhood. On the other hand, as mentioned in the paper <ref type="bibr" target="#b2">[3]</ref>, the soft attention method has the problem of over-smoothing. When the size of the neighbourhood becomes larger, the importance of all neighbours approaches 0 and hence all distinctions between them are lost.</p><p>Recently, some GAT based methods discuss how to better aggregate neighbourhood information to solve the problems of GAT. ADSF <ref type="bibr" target="#b34">[35]</ref> contextualizes each node with a weighted, learnable receptive field based on Gaussian functions or random walks. It can only capture some specific structural patterns based on artificial design but fails to preserve more complex local structures. Besides, GeniePath <ref type="bibr" target="#b18">[19]</ref> learns the importance of different neighbours and builds adaptive receptive fields from two complementary perspectives of breadth and depth. CS-GNN <ref type="bibr" target="#b12">[13]</ref> proposes two smoothness metrics to measure the quantity and quality of information on graphs to improve the use of neighbourhood information. These proposed methods improve attention-based GNN from different aspects, yet their structural learning techniques are basically heuristics and ad-hoc, which may not generalize well. Besides, although these models generate adaptive receptive fields, the aggregation schemes used are still tree-like aggregators (e.g. sum or mean aggregators), which will lose sight of topological structures of high-order neighbours and will also suffer from over-smoothing.</p><p>Therefore, our goal is to propose an adaptive receptive field constructing approach that integrates structural information. On one hand, the importance of neighbours should be measured according to not only node features but also structural information. On the other hand, suitable receptive fields should be constructed discretely to avoid the over-smoothing problem. Furthermore, more powerful aggregators should be designed to aggregate information on irregular neighbourhoods. However, the following challenges remain:</p><p>• Difficulty in modeling complex structural information. Unlike node features, the structural characteristics cannot be directly represented by vectors and need to be learned. However, the structural characteristics of different graphs vary. Consequently, methods based on ad-hoc designs can only be applied to specific graphs, and more general and versatile structural information should be explored. • High complexity of constructing receptive fields. Constructing the adaptive receptive field of a node is a non-differentiable process, hence being hard to directly optimize the solution. Reinforcement learning and combinatorial optimization methods will lead to high complexity, which makes them difficult to be applied. • Irregularities of adaptive receptive fields. Traditional aggregation approaches can handle k-order neighbourhood information. However, the receptive fields constructed by the adaptive methods would be irregularly shaped sub-graphs of the original neighbourhood, which may contain both low-order and high-order neighbours. Therefore, we need a better aggregation approach to capture complex sub-graph structure information.</p><p>To address these problems, in this paper, we propose a novel GNN model with STructural Adaptive Receptive fields (STAR-GNN), which integrates structural information into the construction process of receptive fields and achieves a better aggregation of information in the receptive fields. First, we propose a self-supervised structural modelling method. Based on the equivalence of anonymous random walks (ARWs) and sub-graph structures, we express the structural distribution of the neighbourhood of each node, thereby obtaining the importance of neighbours to the central node. Then, using structural representations, we construct receptive fields based on mutual information (MI) using an efficient greedy algorithm and theoretically prove that the algorithm is capable of constructing optimal receptive fields. Furthermore, we propose a novel aggregator for irregular receptive fields. We regard a receptive field as a sub-graph and learn the structural representation of the sub-graph so as to improve the representation of the central node.</p><p>Finally, we evaluate our model on multiple data sets comparing with several strong baselines. The results of node classification prove the effectiveness of our model. Meanwhile, through the ablation studies, we further prove the superiority of our receptive field construction approach and the effectiveness of our aggregator on irregular receptive fields.</p><p>Overall, our contributions are as follows:</p><p>• We propose a new GNN model based on structured adaptive receptive fields, which can more effectively aggregate important information in the neighbourhood of nodes based on structural information. • We design an approach to learn structural information based on ARWs and adopt mutual information to construct the receptive field of each node adaptively. In addition, we introduce a highly expressive aggregator to capture the sub-graph information of receptive fields. • We conduct experiments on multiple data sets, and our model performs better than other strong baselines, proving the effectiveness of the receptive field construction and aggregation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we introduce some basic notations, followed by the introduction of ARWs which will be used to model local structures in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>Definition 2.1 (graphs). We consider an undirected attributed graph denoted by G = (A, X) = (V , E, X), where V is the set of n = |V | vertices, E is the set of edges, A n×n ∈ {0, 1} is the adjacency matrix and X n×d ∈ [0, 1] is the node features. Definition 2.2 (Graph Neural Networks). Graph neural networks are a general neural network framework on graphs and generate representations by aggregating neighborhood information of nodes. To make better use of the neighborhood information, graph attention networks introduce the attention mechanism a to calculate the importance of node v j on node v i as</p><formula xml:id="formula_0">α i j = so f tmax j (e i j ) = exp(a(h i , h j )) k ∈N i exp(a(h i , h k )) ,<label>(1)</label></formula><p>where N i is the neighbors of node v i . Then the improved convolutional layer can be defined as</p><formula xml:id="formula_1">h i = σ ( j ∈N i α i j Wh j ),<label>(2)</label></formula><p>where σ is a non-linear activation function.</p><p>We then provide some definitions following <ref type="bibr" target="#b25">[26]</ref>, which will be used in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.3 (Orbits</head><p>). An orbit is the result of a group action n acting on elements of a group correspond to bijective transformations of the space that preserve some structure of the space. The orbit of an element ω ∈ Ω is the set of equivalent elements under action n , i.e., n (x) = {π (x)|π ∈ n }.  For each node, we learn a local structural embedding and construct its optimal receptive field under the guidance of the structural embeddings based on mutual information. Next, the elaborate convolutional layer encodes the induced subgraph of the receptive field to boost the final representations. The model is jointly optimized of both mutual information loss and semi-supervised node classification loss. Definition 2.4 (G-equivariant). Let Σ be the set of all possible attributed graphs G. More formally, Σ is the set of all tuples (A, X) with all-size adjacency tensors A and corresponding node attributes X. A function д : Σ → R n×• is G-equivariant w.r.t. valid permutations of the nodes V , whenever any permutation action π ∈ n in the Σ symmetric space is associated with the same permutation action of the nodes in the R n×• symmetric space. Definition 2.5 (Node Embeddings). The node embeddings of a graph G = (A, X) are defined as joint samples of random variables</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARW Reconstruct Graph</head><formula xml:id="formula_2">(Z i ) i ∈V |A, X ∼ p(•|A, X), Z i ∈ R d , d ≥ 1, where p(•|A, X) is a G-equivariant probability distribution on A and X, that is, π (p(•|A, X)) = p(•|π (A), π (X)) for any permutation π ∈ n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anonymous Random Walks</head><p>Denoting a random walk with length l as w = (w 1 , w 2 , . . . , w l ) where ⟨w i , w i+1 ⟩ ∈ E, the ARWs <ref type="bibr" target="#b19">[20]</ref> for w is defined as</p><formula xml:id="formula_3">aw(w) = (DIS(w, w1), DIS(w, w2), . . . DIS(w, w l )),</formula><p>where DIS(w, w i ) denotes the number of distinct nodes in w when w i first appears in w, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIS(w, w</head><formula xml:id="formula_4">i ) = |{w 1 , w 2 , . . . , w p }|, p = min j {w j = w i }.</formula><p>We denote anonymous random walk of length l as ω l 1 , ω l 2 . . . according to their lexicographical order. For example,</p><formula xml:id="formula_5">ω 4 1 = (1, 2, 1, 2), ω 4 2 = (1, 2, 1, 3), ω 4 3 = (1, 2, 3, 1)</formula><p>, etc. The difference between ARWs and random walks is that ARWs depict the underlying "patterns" of random walks, regardless of the exact nodes visited. For example, both  </p><formula xml:id="formula_6">w 1 = (v 1 , v 2 , v 3 , v 4 , v 2 ) and w 2 = (v 2 , v 1 , v 3 , v 4 , v 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL: STAR-GNN</head><p>In this section, we propose a new method of receptive field construction and aggregation. First, we model the local structural information of nodes and incorporate it into the importance of the node neighbourhood. Then an efficient approach to construct receptive fields is proposed based on mutual information. Furthermore, for the irregular receptive field, we propose a sub-graph aggregator to improve the expression ability of representation aggregated from the complex receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neighbourhood Contributions with Local Structural Distribution</head><p>In current GNNs with adaptive fields, most attention scores are calculated by the similarity of node features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref>. Some existing models introduce structural information through artificially designed patterns <ref type="bibr" target="#b34">[35]</ref>, so that they can only capture limited structural information, and lack generalization ability. Therefore, we combine anonymous random walks and mutual information, and propose a method to construct structural receptive fields, which can consider richer structural information in the neighbourhood of nodes.</p><p>We first characterize structural information of the neighbourhood with the help of ARWs. For each node v i , a set of ARWs W (v i ) starting from v i with length l are sampled, which is regarded as the distribution of anonymous random walks of this node. <ref type="bibr" target="#b19">[20]</ref> has proved a useful property of ARWs, that is, we reconstruct a local sub-graph with ARWs. The theorem is shown below. Theorem 3.1 (ARWs Reconstruction <ref type="bibr" target="#b19">[20]</ref>). Let B(v i , k) be the sub-graph induced by all nodes v j such that dist(v i , v j ) ≤ r and D l be the distribution of anonymous walks of length l starting from v i , one can reconstruct B(v i , k) using (D 1 , . . . , D l ) where l = 2(m + 1) and m is the number of edges in B(v i , k).</p><p>This theorem underscores the ability of anonymous random walks to capture structures in a highly general manner, in that they capture the complete k−hop neighbourhood.</p><p>In order to make the structure learning process fully automated and adaptive, we adopt to learn the representation of the structural distribution for each node. We design structural mutual information to learn node embeddings U = {u i } inspired by <ref type="bibr" target="#b21">[22]</ref> to better integrate both node features and structural information. It aims to maximize the mutual information of both features and edges between inputs and outputs. Given a center node v i and its local structure S(v i ), the MI loss is</p><formula xml:id="formula_7">L mi = I (u i ; S(v i )) = | N i | j w i j I (u i , x j ) + I (w i j , p s (v i , v j )),<label>(3)</label></formula><p>where N i denotes the set of nodes that anonymous random walks have reached starting from node v i and w i j = σ (u T i u j ) denotes the weight. The first term I (u i , x j ) is the mutual information of node features and the second term I (w i j , p s (v i , v j )) further incorporate edge weights and features to conform topological relations. p s (v i , v j ) is the structural proximity between structures S(v i ) and S(v j ), which can be efficiently calculated from their distributions of anonymous random walks as</p><formula xml:id="formula_8">p s (v i , v j ) = 2 × |{w ∈ W (v i )} {w ∈ W (v j )}| |{w ∈ W (v i )}| + |{w ∈ W (v j )}| . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>To be specific, if the neighbourhood structures of node v i and node v j are similar, their anonymous walk distributionsW (v i ) andW (v j ) will be close, and they will have a large proximity p s (v i , v j ).</p><p>We resort to Jensen-Shannon MI estimator (JSD) <ref type="bibr" target="#b20">[21]</ref> to maximize Equation 3 following <ref type="bibr" target="#b21">[22]</ref> and obtain the representations combining both node features and local structures. Then node v j 's contribution to the center node v i can be calculated as</p><formula xml:id="formula_10">α i j = exp(u T i u j ) exp k ∈N i exp(u T i u k ) . (<label>5</label></formula><formula xml:id="formula_11">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construction of Optimal Receptive Fields</head><p>Existing soft attention methods select the important neighbours according to their attention scores, but when the size of the neighbourhood increases, the phenomenon of over-smoothing will occur, as analyzed in <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b2">[3]</ref> utilizes reinforcement learning to construct discrete adaptive receptive fields to avoid over-smoothing, but the complexity of the model is very high. Therefore, we need to design an efficient discrete receptive field construction method to select the optimal receptive fields in the neighbourhood of nodes.</p><p>Considering that the optimal receptive field should provide the most information for the central node, and the MI value expresses the information that the central node gains from the neighbourhood. Specifically, when the information gain of a neighbour to the central node is greater, it means that it is more important to the central node, and it should be emphasized. Otherwise, it is more likely to be noise and should be ignored. So with the help of the mutual information, we define the information gain ratio as the ratio of information gain provided by a receptive field S to the gain by the whole neighbourhood N i .</p><p>However, since the mutual information values keep positive, the total mutual information increases monotonically as the receptive field becomes larger. As too large receptive fields will cause oversmoothing problems, we need to constrain the size of receptive fields. Thus, we define the goal of optimal receptive fields, which is to minimize the size of a receptive field under the premise that the information gain ratio reaches the threshold arg min</p><formula xml:id="formula_12">S ⊆N i |S|, s.t . I (u i ; S) I (u i ; N i ) ≥ δ,<label>(6)</label></formula><p>where u i is the node embeddings learned in Section 3.1, combining node features and structural information. δ is a threshold value to ensure the receptive field must contain enough information. Equation 6 is a combinatorial optimization problem. Finding the global optimal solution requires traversing every sub-graph in the neighbourhood of a node, which is impractical. Fortunately, we find that it can be solved with a simple and efficient greedy algorithm. Before that, we first introduce a theorem from <ref type="bibr" target="#b21">[22]</ref>. Theorem 3.2 (Mutual Information Decomposition <ref type="bibr" target="#b21">[22]</ref>). If the conditional probability p(h i |X i ) is multiplicative (see the definition of multiplicative in <ref type="bibr" target="#b24">[25]</ref>), the global mutual information I (h i ; X i ) can be decomposed as a weighted sum of local MIs, namely,</p><formula xml:id="formula_13">I (h; X i ) = j ∈N i w i j I (h i ; x j ),<label>(7)</label></formula><p>where X i consists of all neighbours of v i , x j is the j-th neighbour of v i and the weight w i j satisfies 1 ≤ w i j ≤ 1 for each j.</p><p>The theorem illustrates that the calculation of the mutual information of different neighbours is independent of each other, providing us with an idea based on a greedy algorithm. Specifically, we can calculate the mutual information value of each neighbour separately according to Equation 3 as</p><formula xml:id="formula_14">α i j = w i j I (u i , x j ) + I (w i j , p s (v i , v j )).<label>(8)</label></formula><p>Then the values are sorted in descending order, and the most important neighbours are added to the receptive field step by step until the average information gain ratio reaches the threshold. It is evident that the optimal solution obtained by the greedy algorithm is equivalent to the original global optimal solution in Equation <ref type="formula" target="#formula_12">6</ref>. The calculation of the values and the selection of neighbours is very fast, thereby ensuring the efficiency of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNN with Sub-Graph Structures</head><p>Existing aggregators, such as Mean, Max, and LSTM aggregators, aggregate neighbourhood information in k-hop. However, the adaptive receptive field is an irregular induced sub-graph of the original neighbourhood, which contains both low-order and high-order neighbours. These aggregators cannot distinguish different levels of neighbourhood information, which limits the expression of information in receptive fields. Therefore, we propose a stronger GNN, which regards the irregular receptive field as a sub-graph, and improves the expressive ability of the node representation by using a sub-graph aggregator to capture the complex structural information.</p><p>We want to learn a joint structural representation Γ ⋆ (S, A, X) of the adaptive receptive fields S, which is hard to capture directly. However, we can easily obtain node embeddings using methods like singular value decomposition (SVD) and DeepWalk <ref type="bibr" target="#b22">[23]</ref>, which preserve relative positions of nodes in a graph. Luckily, according to <ref type="bibr" target="#b25">[26]</ref>, there are some correlations between them, providing us with a way to learn an expressive structural representation with node embeddings in a sub-graph. Firstly, we introduce the theorem that expresses the relationship between node embeddings and structural representations as follows.</p><p>Theorem 3.3 (The statistical eqivalence between node embeddings and structural representations <ref type="bibr" target="#b25">[26]</ref>). Let Y(S, A, X) = (Y ( ì S, A, X)) S ∈S be a sequence of random variables defined over the sets S ∈ S of a graph G = (A, X), that are invariant to the ordering of ì S, S ∈ S, such that Y ( ì S 1 , A, X)</p><formula xml:id="formula_15">d = Y ( ì S 2</formula><p>, A, X) for any two jointly isomorphic subsets S 1 , S 2 ∈ S, where d = means equality in their marginal distributions. Consider a graph G = (A, X) ∈ Σ. Let Γ ⋆ ( ì S, A, X) be a most-expressive structural representation of nodes S ∈ P ⋆ (V ) in G, where P ⋆ (V ) is the power set of V excluding the empty set. Then,</p><formula xml:id="formula_16">Y ( ì S, A, X ) ⊥ ⊥ Γ ⋆ ( ì S,A,X ) Z |A, X , ∀S ∈ S,</formula><p>for any node embedding matrix Z that satisfies Definition 2.5, where</p><formula xml:id="formula_17">A ⊥ ⊥ B C means A is independent of C given B. Finally, ∀(A, X) ∈ Σ,</formula><p>there exists a most-expressive node embedding Z ⋆ |A, X such that,</p><formula xml:id="formula_18">Γ ⋆ ( ì S, A, X ) = E Z ⋆ f ( |S |) ((Z ⋆ v ) v∈S |A, X , ∀S ∈ S,<label>(9)</label></formula><p>for some appropriate collection of functions</p><formula xml:id="formula_19">{ f (k) (•)} k =1, ...,n .</formula><p>This theorem allows us to generate a joint structural representation of a sub-graph from simpler node embedding sampling methods. That is to say, if the irregular receptive field of each node is regarded as a sub-graph, we can aggregate node embeddings in the receptive field through a learnable function f , thereby boosting the representations of the central node. The key is how to define node embeddings Z|A, X, which are required to be G-equivariant as introduced in Definition 2.4. Alternatives of Z include node attributes, embeddings from SVD and so on. Here, we use U in Section 3.1, the node embeddings preserving local structural information, which has the strong expressive ability and satisfies the G-equivariant condition. Next, we show that the node embeddings U in Section 3.1 satisfies G-equivariant.</p><p>Theorem 3.4. The node embedding U of a graph G = (A, X) learnt in Section 3.1 satisfies Definition 2.5, that is,</p><formula xml:id="formula_20">(U i ) i ∈V |A, X ∼ p(•|A, X), U i ∈ R d , d ≥ 1, where p(•|A, X) is a G-equivariant proba- bility distribution on A and X.</formula><p>Proof. By definition, ARW ignores the impact of nodes and focus only on the structural patterns. For every anonymous random walk w starting from node v i , given any permutation π ∈ n (set of all permutations), and</p><formula xml:id="formula_21">∀v i ∈ V , W (v i ) is G-equivariant. Then p s (•, •</formula><p>) is also G-equivariant by definition. In Section 3.1, the latent embeddings U are learned via a mean field approximation. We note that in the case of mean field approximation, the probability distribution is a Dirac Delta, which means U is G-equivariant with respect to any permutation action of the nodes in the graph.</p><p>□ Therefore, we propose our GNN model for adaptive receptive fields. Node embeddings in the receptive field of node v i can be used to boost the joint structural representation with a sub-graph aggregator</p><formula xml:id="formula_22">hi = E f (k ) ((u j ) v j ∈N i ) ,<label>(10)</label></formula><p>where f (k ) (u j ) = σ (W k u j ) and the expectation is approximated by the mean value of samples. Then the final node representation h i can be generated as</p><formula xml:id="formula_23">h i = MLP[ hi ⊙ AGG(N i )],<label>(11)</label></formula><p>where AGG is a traditional feature aggregator for node neighbourhood. To learn node representations, we adopt the semi-supervised loss function as</p><formula xml:id="formula_24">L semi = − 1 |V l abel | v i ∈V l abel y i log [so f tmax(h i )] .<label>(12)</label></formula><p>Finally, the overall loss is</p><formula xml:id="formula_25">L = L semi + λL mi ,<label>(13)</label></formula><p>where λ is the weight for MI loss. We summarize our algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the effectiveness of our model from multiple perspectives. We perform node classification on several datasets to demonstrate that our model has a stronger ability to collect and express neighbourhood information. We also conduct an ablation study and parameter analysis to further illustrate the ability and stability of each component of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>4.1.1 Datasets. Our model is evaluated on six real-world datasets, including three citation networks, two social networks, and a proteinprotein interaction network. Some statistics including the data partitioning are shown in Table <ref type="table" target="#tab_0">1</ref>. Model structural characterization with loss 3 and get structural embeddings {u i } 5:</p><formula xml:id="formula_26">for i = 0 to |V | − 1 do 6: S i = ∅ 7:</formula><p>while</p><formula xml:id="formula_27">I (u i ;S i ) I (u i ;N i ) &lt; δ do 8: S i = S i ∪ v j where α i j ≥ α ik ∀v j , v k ∈ N i \S i , 9:</formula><p>end while • RAW FEATURE trains a per-node multi-layer perception (MLP) classifier with node attributes as inputs, which does not incorporate graph structures at all. • GCN <ref type="bibr" target="#b14">[15]</ref> and GraphSAGE <ref type="bibr" target="#b10">[11]</ref> aggregate the neighbourhood information to obtain node representations. We try all of their variants and report the best results on each dataset. • GAT <ref type="bibr" target="#b30">[31]</ref> distinguishes the difference of neighbourhood and collect information by the attention mechanism.</p><p>• GIN <ref type="bibr" target="#b31">[32]</ref> is a powerful GNN framework to capture different graph structures with the same ability as Weisfeiler-Lehman graph isomorphism test. • JK-Net <ref type="bibr" target="#b32">[33]</ref> leverages different neighbourhood ranges to enable structure-aware representations. We try JK-Nets with different final aggregation layers and report the best results using JK-Concat(2). • CS-GNN <ref type="bibr" target="#b12">[13]</ref> is a context-surrounding GNN framework to improve the use of graph information based on the smoothness values of different networks. • GeniePath <ref type="bibr" target="#b18">[19]</ref> is also a GNN model with adaptive receptive fields by the breadth and depth exploration and we report the best performance of two versions, GeniePath and GeniePathlazy-residual on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Settings.</head><p>In the transductive setting, we randomly sample 20 nodes per label to be used for training but attributes of all nodes are available. Then the trained models are evaluated on 1000 test nodes and 500 additional nodes are used for validation purposes, which is the same setting as in <ref type="bibr" target="#b30">[31]</ref>. In the inductive setting, 20 sub-graphs are used as the training set, 2 as the validation set, and 2 as the testing set. Critically, nodes in validation and testing sets are completely unobserved during training. The number of hidden layers is set to 2 and the dimensions of hidden representations are set to 128 for fairness. For GAT-based models, the number of heads is set to 8. Adam optimizer is adopted and hyperparameters such as learning rates, weight decay, and dropout in all baselines are tuned to achieve the best performances on validation sets. All baselines are trained with an early-stop strategy on validation sets with patience of 50 epochs. For our model, we set δ = 0.85, ARW length l = 6, number of ARWs for a node γ = 100, λ = 1, and the feature aggregator in Equation 11 is a multi-layer perception aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node classification</head><p>Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_4">3</ref> show the empirical results of node classification in transductive and inductive settings respectively, including means and standard deviations of Micro F1 across 10 replicas.</p><p>The results show that our model, STAR-GNN, performs consistently better than all other comparisons on all datasets. Among them, our model is significantly better than other models on Citeseer, Facebook, and PPI, achieving 3.6%, 2.9%, and 7.2% improvements respectively. On other datasets, our model also achieves at least 1% improvement. Therefore, it indicates that our model can make better use of information in the neighbourhood. GAT, JK-Net, and GeniePath distinguish the importance of different neighbours of nodes, so the results are better than traditional methods such as GraphSAGE and GCN that treat each neighbour equally. However, these methods, as introduced in Section 1, still have many shortcomings in assigning importance to neighbours. Our model constructs adaptive receptive fields according to structural information and has a stronger aggregator to utilize neighbourhood information, so that achieves better performance.</p><p>In addition, we notice that CS-GNN performs worse than GAT on Pubmed, Github and Facebook. This is because the network structures during the training phase in the transductive setting are limited. As a result, CS-GNN fails to calculate reasonable global smoothness metrics, which weakens the ability of the model. And the performance of GIN is also poor, as it is more suitable for the graph classification task than node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we discuss the effectiveness of each component in our model. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. STAR-GNN-NoStructure refers to not incorporating the structural information into the construction of receptive fields, but only using node features. STAR-GNN-RandomSample refers to randomly sampling 10 nodes in the neighbourhood as in GraphSAGE but not selecting important neighbours according to mutual information. STAR-GNN-NoBoosting means that we do not use our sub-graph-based boosting method, but only use a mean aggregator. And STAR-GNN refers to our proposed model.</p><p>It can be seen that when constructing receptive fields, incorporating the structural information can bring about a 2% improvement, proving that structural information helps to select more useful neighbours. Also, the selection of receptive fields based on mutual information can also improve 2% compared with the random sampling strategy. It demonstrates the limitation of random sampling for information selection, and with the help of mutual information, we can construct better receptive fields. Given the adaptive receptive fields, the aggregator based on the joint structural representations of sub-graphs has better performance than the traditional mean aggregator, which shows that the mean aggregator cannot capture irregular sub-graph structural features, while our boosting method is able to aggregate more complex information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Analysis</head><p>We discuss the sensitivity of some parameters in our model.</p><p>In the sub-graph based aggregator, we try to use different node embeddings to boost the final representations in Equation <ref type="formula" target="#formula_22">10</ref>. STAR-GNN-RawFeatures and STAR-GNN-SVDEmbeddings refer to using node attributes and node embeddings obtained from SVD on the adjacency matrix of graphs respectively. On Github, the adjacency matrix is too large to perform SVD. We find that the expressive power of these naive node embeddings is insufficient. Even if the boosting method is used, the improvement is still limited. But we use the structural embeddings of nodes to enhance the joint structural representations of sub-graphs, achieving better results. We also compare the performance of different aggregators for node features in Equation <ref type="formula" target="#formula_23">11</ref>. STAR-GNN-MeanAggregator and STAR-GNN-AttenAggregator represent mean aggregator and attention aggregator respectively. The choice of node feature aggregators has little effect on the final results. This is because we provide a good sub-graph aggregator, so the result is relatively stable for different choices of node feature aggregators.</p><p>Besides, we compare the performance of our model, RAW FEA-TURE, and GAT with the number of layers from 1 to 3. We conduct experiments with 10 replicas per model and report the averaged performances and 95% confidence intervals. The Micro F1 of our model is generally higher than that of GAT, and the results also remain stable, proving that the construction of receptive fields and collection of information in our model is superior to GAT. On Cora and Citeseer, as the number of layers increases, the results of GAT drop significantly and become more unstable. When the number of layers changes from 2 to 3, Micro F1 of GAT drops by about 5% and 10% respectively, which indicates that GAT causes the over-smoothing problem. Our model can maintain good performance, demonstrating that it can avoid over-smoothing when the neighbourhood size becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Graph Representation Learning</head><p>Having strong expression ability, graph representation learning (GRL) approaches have been widely used in various tasks in graph domain, such as node classification, link prediction, clustering, and graph classification. Current GRL models can be categorized into two types: models that learn node embeddings and that learn structural representations. For the former kind of methods, most researchers accept this statement: these methods learn node embedding with measures of relative node similarity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>. Methods based on Skip-Gram like <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref> and variational auto-encoder methods like <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> are also included in this category. Other methods focus on the classification task of nodes and the whole graphs. With solid graph-based theoretic work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, these methods exploit classification task to model structural representations. Most GNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> fall into the latter category of GRL, while some others learn node embeddings, which are used for the link prediction task.  <ref type="bibr" target="#b14">[15]</ref> 76.82 ± 0.44% 66.41 ± 1.71% 74.67 ± 0.37% 79.70 ± 0.90% 30.83 ± 3.17% GraphSAGE <ref type="bibr" target="#b10">[11]</ref> 73.00 ± 1.17% 65.47 ± 2.13% 71.28 ± 0.84% 80.52 ± 0.65% 28.66 ± 1.18% GAT <ref type="bibr" target="#b30">[31]</ref> 76.45 ± 1.38% 66.07 ± 0.97% 75.05 ± 0.73% 83.63 ± 1.74% 35.09 ± 4.06% GIN <ref type="bibr" target="#b31">[32]</ref> 72.51 ± 0.99% 64.50 ± 0.95% 73.38 ± 1.96% 76.29 ± 2.63% 30.35 ± 2.09% JK-Net <ref type="bibr" target="#b32">[33]</ref> 78.38 ± 0.38% 66.94 ± 0.75% 75.07 ± 0.29% 78.46 ± 1.15% 37.22 ± 0.31% CS-GNN <ref type="bibr" target="#b12">[13]</ref> 76.13 ± 1.38% 67.51 ± 1.07% 72.38 ± 0.37% 80.03 ± 1.23% 34.12 ± 3.26% GeniePath <ref type="bibr" target="#b18">[19]</ref> 76.59 ± 1.92% 66.95 ± 1.33% 73.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method PPI</head><p>RAW FEATURE 51.34% GCN <ref type="bibr" target="#b14">[15]</ref> 59.69 ± 0.14% GraphSAGE <ref type="bibr" target="#b10">[11]</ref> 60.31 ± 0.22% GAT <ref type="bibr" target="#b30">[31]</ref> 60.36 ± 0.21% GIN <ref type="bibr" target="#b31">[32]</ref> 62.00 ± 1.10% JK-Net <ref type="bibr" target="#b32">[33]</ref> 59.34 ± 1.10% CS-GNN <ref type="bibr" target="#b12">[13]</ref> 59.91 ± 0.22% GeniePath <ref type="bibr" target="#b18">[19]</ref> 62.31 ± 0.34% STAR-GNN (ours) 69.29 ± 1.43% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mutual Information in Graph</head><p>With strong ability to be combined with unsupervised methods, many methods like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> adopted the idea of <ref type="bibr" target="#b4">[5]</ref> to maximize the mutual information between inputs and outputs, which brought huge success to unsupervised learning. However, it is hard to introduce mutual information into neural networks due to its impractical complexity. Unsupervised learning with mutual information in neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> was not possible until the advent of <ref type="bibr" target="#b3">[4]</ref>, which provided the estimation of mutual information between high dimensional continuous random variables over neural networks. Recently, mutual information is further introduced into graph-structure data <ref type="bibr" target="#b26">[27]</ref> With the power of mutual information, these methods reached the results as state-of-art supervised GNNs and semi-supervised GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adaptive Receptive Fields</head><p>GNNs belong to a family of message-passing architectures that use different message-passing techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. These approaches show power in different tasks and datasets. Further, to automatically adapt to different tasks and datasets, constructing receptive fields adaptively in GNNs becomes an important problem. ADSF <ref type="bibr" target="#b34">[35]</ref> contextualizes each node with a weighted, learnable receptive field based on Gaussian function or random walks to model diverse local graph structures. But it can only capture artificially designed structural information. PGNN <ref type="bibr" target="#b33">[34]</ref> takes advantage of anchor sets to incorporate position information in GNNs. Ge-niePath <ref type="bibr" target="#b18">[19]</ref> proposes a breadth function to learn the importance of different sized neighbourhoods and a depth function to filter signals aggregated from neighbours respectively. JK-Net <ref type="bibr" target="#b32">[33]</ref> designs an architecture to leverages different neighbourhood ranges and learn structure-aware representations. CS-GNN <ref type="bibr" target="#b12">[13]</ref> proposes two smoothness metrics, namely feature smoothness and label smoothness, to measure the quantity and quality of information on graphs so as to improve the use of neighbourhood information. But all of them are still based on soft-attention and prone to the oversmoothness, leading to poor results. Besides, GRARF <ref type="bibr" target="#b2">[3]</ref> proposes to construct adaptive receptive fields with the help of reinforcement learning to get rid of the over-smoothness but suffers from high computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we tackle a key issue in the GNN field: sampling and aggregation of neighbourhoods of nodes. We propose STAR-GNN that collects and aggregates useful neighbourhood information (i.e. receptive fields), and generate node representations with stronger expressive capabilities.</p><p>Considering that the importance of neighbours in GAT is calculated only on node features while ignoring the structural information, we first propose a method based on anonymous random walks to model the local structural embeddings of nodes. The embeddings will be used to measure the importance of different neighbours to the central node more accurately. But existing receptive field selection methods are mainly based on soft-attention, which will cause the over-smoothing problem when the size of neighbourhood increases. So we then propose to construct receptive fields discretely and adaptively based on the mutual information of the neighbourhood to the central node, which also refers to information gain or contribution. The construction approach can be efficiently implemented through a greedy strategy. Furthermore, the receptive fields obtained by adaptive construction are irregular, and traditional aggregators like mean or max aggregators cannot capture their complex structural information. We propose a subgraph aggregator, which can learn joint structural representations in irregular sub-graphs, so as to better gather information in the receptive fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of STAR-GNN. For each node, we learn a local structural embedding and construct its optimal receptive field under the guidance of the structural embeddings based on mutual information. Next, the elaborate convolutional layer encodes the induced subgraph of the receptive field to boost the final representations. The model is jointly optimized of both mutual information loss and semi-supervised node classification loss.</figDesc><graphic url="image-1.png" coords="3,317.96,421.38,240.25,55.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) correspond to the same anonymous random walk aw(w1) = aw(w2) = (1, 2, 3, 4, 2), even though w 1 and w 2 visited different nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of anonymous random walks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of number of layers on Cora, Citeseer and Github.</figDesc><graphic url="image-2.png" coords="8,75.65,247.68,151.33,115.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. An undirected attributed graph G = (V , E, X), the threshold of information gain ratio δ , anonymous random walk number s per node and walk length l. Output: Node representations {h i } for node classification task.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Github</cell><cell>Facebook</cell><cell>PPI</cell></row><row><cell>Task</cell><cell cols="6">Transductive Transductive Transductive Transductive Transductive Inductive</cell></row><row><cell># Nodes</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>37,700</cell><cell>22,470</cell><cell>56,944</cell></row><row><cell># Edges</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell><cell>289,003</cell><cell>171,002</cell><cell>818,716</cell></row><row><cell># Features</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell><cell>600</cell><cell>200</cell><cell>50</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>121</cell></row><row><cell>Avg. Degree</cell><cell>2.00</cell><cell>1.42</cell><cell>2.25</cell><cell>7.76</cell><cell>7.61</cell><cell>28.8</cell></row><row><cell># Training Nodes</cell><cell>140</cell><cell>120</cell><cell>60</cell><cell>40</cell><cell>80</cell><cell>44,906</cell></row><row><cell># Validation Nodes</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>500</cell><cell>6,514</cell></row><row><cell># Testing Nodes</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>1,000</cell><cell>5,524</cell></row><row><cell>Algorithm 1 STAR-GNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1: Generate ARW set W (v i ) for each node v i 2: Build new graph based on the similarity based on equation 4 3: for e = 1 to num_epochs do 4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Update the final node representation h i with h i = MLP[ hi ⊙ AGG(N i )] Citation networks: In Cora, Citeseer and Pubmed 1 , nodes correspond to papers, edges to undirected paper citations and node labels to research fields. • Social networks: In Github and Facebook 2 , nodes correspond to users and edges to their friendship relationships. Node labels on Github represent whether the user is a web or a machine learning developer. Node labels on Facebook have four categories: politicians, organizations, television shows, and companies. We reduce the number of node attributes of social networks to the figures in Table1by selecting the most frequent ones as all attributes are binary and sparse. • Protein-protein interaction network: PPI 3 consists of 24 subgraphs and each of them represents human tissue. Nodes correspond to proteins and edges to interactions between them. Each node has multiple labels, resulting in a multi-label classification task.</figDesc><table><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell>for layer = 1 to num_layers do</cell></row><row><cell>12:</cell><cell>Aggregate features in the generated ARF for each node</cell></row><row><cell>13:</cell><cell>Sample structural embedding from N i and calculate hi</cell></row><row><cell cols="2">with equation 10</cell></row><row><cell>14:</cell><cell></cell></row><row><cell>15:</cell><cell>end for</cell></row><row><cell cols="2">16: end for</cell></row><row><cell>•</cell><cell></cell></row></table><note>4.1.2 Baselines. We compare our model with eight baselines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Micro F1 of Node classification in transductive settings.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Github</cell><cell>Facebook</cell></row><row><cell>RAW FEATURE</cell><cell>53.86%</cell><cell>53.22%</cell><cell>67.70%</cell><cell>61.47%</cell><cell>26.98%</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>15 ± 1.04% 83.48 ± 0.80% 35.32 ± 2.34% STAR-GNN (ours) 79.86 ± 0.59% 71.14 ± 0.47% 76.16 ± 0.41% 85.26 ± 0.73% 40.18 ± 0.98%</figDesc><table /><note>(a) Cora (b) Citeseer (c) Github</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Micro F1 of Node classification in inductive settings.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on Cora.</figDesc><table><row><cell>Method</cell><cell>Cora</cell></row><row><cell>GraphSAGE [11]</cell><cell>73.00 ± 1.17%</cell></row><row><cell>GAT [31]</cell><cell>76.45 ± 1.38%</cell></row><row><cell>STAR-GNN-NoStructure</cell><cell>77.71 ± 0.82%</cell></row><row><cell cols="2">STAR-GNN-RandomSample 77.81 ± 0.86%</cell></row><row><cell>STAR-GNN-NoBoosting</cell><cell>77.87 ± 1.21%</cell></row><row><cell>STAR-GNN (ours)</cell><cell>79.86 ± 0.59%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Parameter analysis on Cora, Citeseer and Github. RawFeatures 77.20 ± 0.79% 71.40 ± 0.77% 82.41 ± 1.20% STAR-GNN-SVDEmbeddings 77.74 ± 0.68% 70.80 ± 1.18% _ STAR-GNN-MeanAggregator 79.68 ± 1.69% 69.54 ± 0.66% 84.50 ± 0.93% STAR-GNN-AttenAggregator 79.59 ± 1.86% 68.56 ± 1.77% 84.10 ± 0.70% STAR-GNN (ours) 79.86 ± 0.59% 71.14 ± 0.47% 85.26 ± 0.73%</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Github</cell></row><row><cell>STAR-GNN-</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Available at https://linqs.soe.ucsc.edu/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Available at http://snap.stanford.edu/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Available at http://snap.stanford.edu/graphsage/ppi.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (Grant No.61876006).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advances in metric embedding theory</title>
		<author>
			<persName><forename type="first">Ittai</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Bartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neimany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-eighth annual ACM symposium on Theory of computing</title>
				<meeting>the thirty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="271" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MISEP-Linear and nonlinear ICA based on mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1297" to="1318" />
			<date type="published" when="2003-12">2003. Dec (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Discrete Adaptive Receptive Fields for Graph Convolutional Networks</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=pHkBwAaZ3UKunderreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06082</idno>
		<title level="m">Probabilistic symmetry and invariant neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15894" to="15902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphite: Iterative Generative Modeling of Graphs</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2434" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-Implicit Graph Variational Auto-Encoders</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">R</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10711" to="10722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Measuring and Improving the Use of Graph Information in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2752" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Mutual Information Maximization Perspective of Language Representation Learning</title>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08350</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
				<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GeniePath: Graph Neural Networks with Adaptive Receptive Paths</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reconstructing markov processes from independent and anonymous experiments</title>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="108" to="122" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">About the mutual (conditional) information</title>
		<author>
			<persName><forename type="first">Renato</forename><surname>Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ueli</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISIT</title>
				<meeting>IEEE ISIT</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">364</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the Equivalence between Positional Node Embeddings and Structural Graph Representations</title>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlated Variational Auto-Encoders</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Ruozzi</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tang19b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019</meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6135" to="6144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international on world wide web</title>
				<meeting>the 24th international on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10">2018. July 10-15. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Position-aware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adaptive Structural Fingerprints for Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaokang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
