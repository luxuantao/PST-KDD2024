<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Driver stress detection via multimodal fusion using attention-based CNN-LSTM</title>
				<funder ref="#_TT3dgut">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-11">11 February 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Luntian</forename><surname>Mou</surname></persName>
							<email>ltmou@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Multimedia and Intelligent Software Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Multimedia and Intelligent Software Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Multimedia and Intelligent Software Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bahareh</forename><surname>Nakisa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Science Engineering and Built Environment</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>VIC</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Naim</forename><surname>Rastgoo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institute for Future Health</orgName>
								<orgName type="department" key="dep2">School of Information and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Bren, Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Driver stress detection via multimodal fusion using attention-based CNN-LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-11">11 February 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.eswa.2021.114693</idno>
					<note type="submission">Received 15 July 2020; Received in revised form 18 November 2020; Accepted 5 February 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Driver stress detection Convolutional neural network Long short-term memory Eye data Vehicle data Attention mechanism</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stress has been identified as one of major contributing factors in car crashes due to its negative impact on driving performance. It is in urgent need that the stress levels of drivers can be detected in real time with high accuracy so that intervening or navigating measures can be taken in time to mitigate the situation. Existing driver stress detection models mainly rely on traditional machine learning techniques to fuse multimodal data. However, due to the non-linear correlations among modalities, it is still challenging for traditional multimodal fusion methods to handle the real-time influx of complex multimodal and high dimensional data, and report drivers' stress levels accurately. To solve this issue, a framework of driver stress detection through multimodal fusion using attention based deep learning techniques is proposed in this paper. Specifically, an attention based convolutional neural networks (CNN) and long short-term memory (LSTM) model is proposed to fuse non-invasive data, including eye data, vehicle data, and environmental data. Then, the proposed model can automatically extract features separately from each modality and give different levels of attention to features from different modalities through self-attention mechanism. To verify the validity of the proposed method, extensive experiments have been carried out on our dataset collected using an advanced driving simulator. Experimental results demonstrate that the performance of the proposed method on driver stress detection outperforms the state-of-the-art models with an average accuracy of 95.5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the contributing factors to road traffic crashes which lead to a large number of injuries and fatalities, is being stressed while driving. Stress can be defined as a nonspecific bodily response to a combination of external demands and internal concerns. Stress can be distinguished in the concept of eustress (positive stress) and distress (negative stress) <ref type="bibr" target="#b53">(Selye, 1974)</ref>. Generally, there are two types of stress: eustress and distress, by which eustress refers to positive correlation with life satisfaction while distress is the opposite mental state. In daily life, we often use the term "stress" to describe negative stress rather than positive stress. In this study, the term "stress" also refers only to negative stress.</p><p>According to the World Health Organization (WHO)'s report on road safety, the total number of deaths caused by various traffic accidents has reached 1.3 million each year <ref type="bibr" target="#b51">(Sauerzapf, 2012)</ref>. The European Commission estimated that the cost of car accidents in Europe reached 160 billion euros, of which 60%-80% came from the drivers' psychophysical condition <ref type="bibr" target="#b58">(Vivoli, Bergomi, Rovesti, Bussetti, &amp; Guaitoli, 2006)</ref>. Stress often leads to poor psychophysical condition that can increase the risk of crash almost tenfold, according to Virginia Tech Transportation Institute <ref type="bibr" target="#b8">(Brown et al., 2016)</ref>. National crash reports in Australia also show that feeling stressed is a critical factor in fatal crashes <ref type="bibr" target="#b3">(Beanland, Fitzharris, Young, &amp; Lenn?, 2013)</ref>. Stress increases the risk of crash by weakening the cognitive ability of drivers, which would result in undermined driving performance <ref type="bibr" target="#b56">(Useche, Ortiz, &amp; Cendales, 2017)</ref>. Therefore, in order to reduce the risk of crashes and improve driving safety, it is essential to build a system which can detect drivers' stress levels accurately.</p><p>It has been shown that the use of multimodal data can substantially improve driver stress classification performance <ref type="bibr" target="#b19">(Healey &amp; Picard, 2005;</ref><ref type="bibr" target="#b25">Katsis, Katertsidis, Ganiatsas, &amp; Fotiadis, 2008;</ref><ref type="bibr" target="#b48">Rigas, Goletsis, &amp; Fotiadis, 2012;</ref><ref type="bibr" target="#b27">Lanata et al., 2015)</ref>. Different types of data such as eye data, vehicle data and environmental parameters are used to monitor driver stress <ref type="bibr" target="#b46">(Rastgoo, Nakisa, Rakotonirainy, Chandran, &amp; Tjondronegoro, 2018)</ref>. Several studies have shown that there is strong correlation between eye data and driver's behaviour <ref type="bibr" target="#b14">(Haak, Bos, Panic, &amp; Rothkrantz, 2009;</ref><ref type="bibr" target="#b40">Palinko, Kun, Shyrokov, &amp; Heeman, 2010;</ref><ref type="bibr" target="#b62">Wu, Zhao, Rong, &amp; Ma, 2013;</ref><ref type="bibr" target="#b66">Zhang, Liu, &amp; Tang, 2015)</ref>. Pupillary response from eye data has been shown to be a potential physiological data for detecting driver stress <ref type="bibr" target="#b42">(Pedrotti et al., 2014)</ref>. The autonomic nervous system (ANS) can continuously regulate pupil size. When a driver is under stress, the pupils are dilated due to sympathetic nervous system (SNS) stimulation. Therefore, pupil diameter (PD) can be used in driver stress detection. <ref type="bibr" target="#b2">Baltaci and Gokcay (2016)</ref> combined the features of PD and face temperature to detect driver stress. In addition to PD, blink and gaze of eye are also concerned by researchers <ref type="bibr" target="#b16">(Hansen &amp; Ji, 2010;</ref><ref type="bibr" target="#b26">K?bler et al., 2014)</ref>. Some researchers even combined PD, blink and gaze data of eye and electroencephalogram (EEG) to perform emotion recognition on drivers <ref type="bibr" target="#b54">(Soleymani, Pantic, &amp; Pun, 2012;</ref><ref type="bibr" target="#b32">Liu, Zheng, &amp; Lu, 2016)</ref>.</p><p>In stressful situations, drivers reacts physically to control the vehicle to avoid collisions. Depending on the types of reaction, the reaction time can range from milliseconds to seconds <ref type="bibr" target="#b13">(Green, 2000)</ref>. The drivers' physical reactions can be monitored with vehicle data such as steering wheel, acceleration and deceleration data <ref type="bibr" target="#b6">(Bo?il, Sadjadi, Kleinschmidt, &amp; Hansen, 2010;</ref><ref type="bibr" target="#b48">Rigas et al., 2012;</ref><ref type="bibr" target="#b27">Lanata et al., 2015;</ref><ref type="bibr" target="#b28">Lee, Chong, &amp; Lee, 2017)</ref>. In addition, environmental data have also been shown to be helpful in detecting driver stress levels <ref type="bibr" target="#b20">(Hill &amp; Boyle, 2007)</ref>. The environmental data include different information affecting drivers, for instance, weather conditions, visibility, time of day, road situations, and other driver behaviors.</p><p>Building a multimodal fusion model based on eye data, vehicle data and environmental data can improve the performance of driver stress detection. The strategies for fusion mainly include sensor-level, featurelevel and decision-level fusion. At present, feature-level strategies are the main fusion approach, which uses handcrafted features or deep learning features to build multimodal models <ref type="bibr" target="#b22">(Hu &amp; Li, 2016;</ref><ref type="bibr" target="#b44">Pourbabaee, Roshtkhari, &amp; Khorasani, 2017;</ref><ref type="bibr" target="#b34">Nakisa, Rastgoo, Rakotonirainy, Maire, &amp; Chandran, 2018</ref><ref type="bibr" target="#b52">, 2020)</ref>. Compared to handcrafted features methods, deep learning methods can automatically extract features without expertise. Although existing deep learning models perform relatively well in driver stress detection <ref type="bibr" target="#b38">(Ngiam, Khosla, Kim, Nam, Lee, &amp; Ng, 2011;</ref><ref type="bibr" target="#b24">Kanjo, Younis, &amp; Ang, 2019;</ref><ref type="bibr" target="#b45">Rastgoo, Nakisa, Maire, Rakotonirainy, &amp; Chandran, 2019)</ref>, there is still space for performance improvement. This is due to the fact that these methods just learn the relationships of features within a single modality and lack proper mechanisms to handle the non-linearity across modalities.</p><p>To effectively fuse features from different modalities, attention mechanism is introduced in this paper to integrate eye data, vehicle data and environmental data. Originally, the attention mechanism was used in machine translation, which can quickly extract sparse features for natural language processing tasks <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>. Self-attention is an improvement of the attention mechanism, which can reduce the dependence on external data and capture the internal relationship of longer data or features <ref type="bibr" target="#b30">(Lin et al., 2017)</ref>. The self-attention mechanism can be used to process the hidden states of an LSTM or Gated Recurrent Unit (GRU) for classification tasks. For example, an attention-based LSTM network was proposed to classify aspect-level sentiment <ref type="bibr" target="#b59">(Wang, Huang, Zhao, &amp; Zhu, 2016)</ref>. To classify documents, a hierarchical attentional network was constructed using GRU and attention mechanisms <ref type="bibr" target="#b64">(Yang et al., 2016)</ref>. In recent years, researchers have begun to focus on combining CNN-LSTM network <ref type="bibr" target="#b50">(Sainath, Vinyals, Senior, &amp; Sak, 2015)</ref> with attention mechanism in a variety of areas. In the script recognition problem of scene text images and video scripts, an attentionbased CNN-LSTM framework was proposed to extract local and global features and dynamically weight them <ref type="bibr" target="#b5">(Bhunia et al., 2019)</ref>. On the issue of electrocardiogram (ECG) based arrhythmia classification, <ref type="bibr" target="#b33">Liu et al. (2019)</ref> proposed an attention-based hybrid LSTM-CNN model to extract overall variation trends and local features of ECG.</p><p>In this study, we propose a multimodal fusion model based on an attentional CNN-LSTM network to fuse eye data, vehicle data, and environmental data. The proposed model first uses convolutional neural networks (CNN) and long short-term memory networks (LSTM) to extract features, and then allocates different levels of attention to features with different modalities using a self-attention mechanism. Thus, it can capture the relationship between multimodal data and driver stress levels. Here, three driver stress levels are currently considered, namely low, medium and high. We have collected a dataset for this study from an advanced driving simulator. This dataset contains eye data, vehicle dynamics data and environmental data. And the data is sampled from 22 participants from multiple driving situations designed to induce different levels of stress.</p><p>The main contributions of this study are as follows:</p><p>? A framework based on attentional CNN-LSTM network is proposed to build an accurate driver stress detection system. This attention-based multimodal fusion model can not only automatically extract features but also weigh the features from different modalities to improve performance on driver stress level classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Since stress detection tends to utilize multimodal data, a stable and reliable stress detection model can be established by analysing and fusing multimodal data. There are different modalities that can be used to measure driver stress, including stressors that stimulate drivers, the driving environment, personal parameters, and the physiological, psychological, and physical responses of drivers under stressors. Since eye data and vehicle dynamics data are easy to obtain, researchers have studied the relationship between these data and stress <ref type="bibr" target="#b6">(Bo?il et al., 2010;</ref><ref type="bibr" target="#b42">Pedrotti et al., 2014;</ref><ref type="bibr" target="#b27">Lanata et al., 2015)</ref>. The studies in the literature have mainly used traditional machine learning methods to extract features manually from data, and then combine the features to create stress detection models. Although the handcrafted features approach has yielded positive results, it is always a challenge to accurately extract important and representative features <ref type="bibr" target="#b36">(Nakisa, Rastgoo, Tjondronegoro, &amp; Chandran, 2017)</ref>. In addition, the handcrafted features approach requires specialized knowledge and is less robust to noise and data variations.</p><p>There are several researchers who have built stress level detection models using multimodal data. <ref type="bibr" target="#b4">Benoit et al. (2009)</ref> proposed a driver simulator that uses multimodal data to monitor stress states, including video data (facial activity) and physiological data. The model used facial activities such as blinking, yawning and head turning, as well as ECG and electrical skin responses, to assess the driver's stress levels. <ref type="bibr" target="#b47">Rigas, Goletsis, Bougia, and Fotiadis (2011)</ref> proposed a model to detect driver stress levels and predict driving performance. The multimodal data include physiological signals, video recordings (eye data, head movement), and environmental data. The model used a support vector machine (SVM) classifier to distinguish between two stress levels (no stress, stress) with an accuracy of 86%.</p><p>Most of the above studies fuse multimodal data at feature-level. The traditional feature-level fusion method combines feature data from each L. <ref type="bibr">Mou et al.</ref> modality into a feature vector, which is fed into a classifier. However, this approach lacks the ability to handle complex multimodal and high dimensional data <ref type="bibr" target="#b38">(Ngiam et al., 2011)</ref>. To improve the model's robustness and data processing ability, deep learning techniques are used to process multimodal data. Deep learning techniques can directly process the original data and are widely used in high dimensional signals processing, such as speech recognition <ref type="bibr" target="#b21">(Hinton et al., 2012)</ref> and time series data analysis <ref type="bibr" target="#b31">(Liu, Chen, Peng, &amp; Wang, 2017)</ref>.</p><p>In deep learning methods, CNN has shown strong performance in the field of image recognition <ref type="bibr" target="#b12">(George &amp; Routray, 2016)</ref>. One-dimensional convolutional neural network (1D-CNN) is a type of CNN that is primarily used in sequence modeling and natural language processing <ref type="bibr" target="#b9">(Burkert, Trier, Afzal, Dengel, &amp; Liwicki, 2015;</ref><ref type="bibr" target="#b39">Ord??ez &amp; Roggen, 2016)</ref>. CNN can extract locally dependent and invariant features of the data. Moreover, the deep CNN can extract local features from the original data, and then extract the global high dimensional feature representation in deeper layers. Some studies proved that CNN surpasses the traditional handcrafted features approach in feature extraction <ref type="bibr" target="#b15">(Haidar, Koprinska, &amp; Jeffries, 2017;</ref><ref type="bibr" target="#b55">Urtnasan, Park, Joo, &amp; Lee, 2018)</ref>. <ref type="bibr" target="#b18">He, Li, Liao, Zhang, and Jiang (2019)</ref> proposed to use CNN to process the heart rate variability (HRV) of ECG signals in order to achieve rapid detection of acute cognitive stress.</p><p>As an improvement to recurrent neural network (RNN), LSTM network can learn long-term dependence information and avoid gradient explosion. LSTM has excellent performance in dealing with time series problems such as machine translation, emotion recognition and speech recognition <ref type="bibr" target="#b21">(Hinton et al., 2012;</ref><ref type="bibr" target="#b63">Yan &amp; Mikolajczyk, 2015;</ref><ref type="bibr" target="#b37">Neverova et al., 2016)</ref>. In addition, some researchers have proposed the CNN-LSTM model, which uses CNN for feature extraction on input data and LSTM to perform sequence prediction on the feature vectors <ref type="bibr" target="#b65">(Zhang, Chan, &amp; Jaitly, 2017;</ref><ref type="bibr" target="#b57">Valiente, Zaman, Ozer, &amp; Fallah, 2019)</ref>. <ref type="bibr" target="#b10">Donahue et al. (2015)</ref> combined LSTM and CNN to solve visual recognition problems. <ref type="bibr" target="#b45">Rastgoo et al. (2019)</ref> proposed a multimodal fusion model based on CNN-LSTM network to detect the driver's stress level. Arefnezhad et al. ( <ref type="formula">2020</ref>) proposed a CNN-LSTM deep network structure using vehicle data on driver drowsiness detection that significantly outperforms traditional machine learning methods.</p><p>Although LSTM can handle long time series data, it is still an RNN structure, which focuses on time step relationships and lacks extraction of global information. Thus, some researchers have begun to focus on attention mechanism <ref type="bibr" target="#b59">(Wang et al., 2016;</ref><ref type="bibr" target="#b61">Winata, Kampman, &amp; Fung, 2018)</ref>. Attention mechanism was first proposed in machine translation and then widely used in text classification and representation learning <ref type="bibr" target="#b1">(Bahdanau, Cho, &amp; Bengio, 2015;</ref><ref type="bibr" target="#b64">Yang et al., 2016)</ref>. Self-attention is an improvement on attention mechanism with better performance in capturing the internal correlation of data and features. <ref type="bibr" target="#b61">Winata et al. (2018)</ref> proposed a bidirectional LSTM model based on self-attention mechanism using recorded texts to classify psychological stress.</p><p>Recently, the combination of CNN-LSTM network and attention mechanism has been concerned by researchers. <ref type="bibr" target="#b43">Peng, Tian, Yu, Lv, and Wang (2019)</ref> proposed a CNN-LSTM network based on attention, which has a good effect on identifying and detecting malicious uniform resource locator (URL). <ref type="bibr" target="#b29">Li et al. (2020)</ref> proposed an attention-based CNN-LSTM model to predict urban PM2.5 concentration. The model used the CNN-LSTM network to learn the correlation of multivariate time series data related to air quality, and used the attention mechanism to weigh the past features to improve the prediction accuracy. Therefore, for the first time, this paper applies the deep learning model of CNN-LSTM combined with the self-attention mechanism in the field of driver stress classification. Specifically, the CNN-LSTM model is used to extract features from non-invasive multimodal time series data, and the self-attention mechanism is used to fuse features of eye, vehicle and environment, and weigh features from different modalities to effectively detect drivers' stress levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, a multimodal fusion model based on attentional CNN-LSTM network is proposed for driver stress detection. The dataset was measured in a simulated driving environment. In the following subsections, the dataset acquisition, the experimental design, and the multimodal fusion architecture are described one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset acquisition</head><p>The dataset was measured by simulating different stressful environments using an advanced driving simulator (see Fig. <ref type="figure" target="#fig_0">1</ref>). The driving simulator consists of SCANeR? studio software, computers, projectors, a real cabin and a six degree of freedom (6DOF) motion platform. The driving simulator can move and twist in three-dimensional space to approximate the actual driving environment. The simulated environment includes 180 degree front view, rear view images, engine and various environment sounds and vehicle movements. Therefore, the driving simulator can simulate real visual scenes, surrounding environmental sounds, and vehicle motion feedback, making the driver immersed in a virtual environment that is close to real vehicle. More information on the advanced driving simulator is available at https://re search.qut.edu.au/carrsq/wp-content/uploads/sites/45/2019/02/ Simulator.pdf.</p><p>The experiment used SCANeR? studio software <ref type="bibr" target="#b52">(Scanerstudio, 2020)</ref> and FaceLAB? <ref type="bibr" target="#b11">(Funke et al., 2016)</ref> remote video eye tracker to acquire data. FaceLAB? was used to obtain eye data such as pupil diameter, gaze dispersion in X and Y axes and blink frequency, sampled at 60 Hz. Vehicle dynamics and environmental data were collected by SCANeR? studio, also sampled at 60 Hz. Vehicle data include steering wheel angle, brake pedal and gas pedal. Environmental data include the distance to the preceding vehicle, lane width, number of lanes, time of day, weather conditions (sunny, rain density) and visibility (fog). In particular, all data from FaceLAB? and SCANeR? studio were collected synchronously. In this study, there were 22 participants involved in data collection, aged 21-40 years (55% male). All participants were required to have two years of driving experience and qualified driving license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental design</head><p>Before the start of the experiment, participants were asked to get familiar with the details and precautions of the experiment, such as avoiding alcohol and caffeinated beverages for a week before the experiment.</p><p>In this experiment, six driving scenarios were built to collect participants' eye data, vehicle dynamics data and environmental data. Firstly, participants were asked to drive on a simple road to become familiar with the driving environment and operating methods. After that, each participant's driving data was collected in five driving scenarios (Urban1, Urban2, Highway, CBD1 and CBD2) with a random order. These operations and the advanced driving simulator can help avoid simulator sickness while ensuring high realism rating of the volunteers. And each driving scenario contains several different stressors to induce different levels of stress in participants. The data labels were obtained through a verbal question and answer. During each scenario, the participants were asked every two minutes to provide their responses (verbally) to a short questionnaire about their average stress levels. They were asked to express their stress levels between 0 and 3 (0-No stress to 3-High stress) during each scenario. These numbers are then mapped into three different stress levels (0.1-1 = Low, 1.1-2 = Medium, 2.1-3 = High). These mappings allow us to compare our work with other existing works <ref type="bibr" target="#b42">(Pedrotti et al., 2014;</ref><ref type="bibr" target="#b60">Wang, Lin, &amp; Yang, 2013;</ref><ref type="bibr" target="#b2">Baltaci &amp; Gokcay, 2016)</ref>.</p><p>In this study, the stressors were designed with reference to different studies <ref type="bibr" target="#b20">(Hill &amp; Boyle, 2007;</ref><ref type="bibr" target="#b49">Rodrigues, Kaiseler, Aguiar, Cunha, &amp; Barros, 2015;</ref><ref type="bibr" target="#b28">Lee et al., 2017;</ref><ref type="bibr" target="#b46">Rastgoo et al., 2018)</ref>. There are five main stressors: traffic congestion, driving road situations, the behavior of other drivers, weather, and time of day. Table <ref type="table" target="#tab_1">1</ref> shows the different stressors in different driving scenarios. Since traffic congestion creates stress <ref type="bibr" target="#b46">(Rastgoo et al., 2018)</ref>, this study designed different vehicle densities per kilometer under different driving scenarios. Then, narrow roads, curvy roads, and sharp bends were used to induce different stress levels in drivers. The behaviors of other drivers can also cause varying degrees of stress, such as overtaking, lane changing, speeding and tailgating. In the Highway, CBD1 and CBD2 scenarios, we set several parameters (stay on lane, sign observing, priority observing, safety time, speed limit risk, and overtake risk) in the simulator to simulate the above behaviors. This study designed rain density (0-1) and foggy under different driving scenarios, and simulated the driving environment during the day and night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multimodal fusion architecture</head><p>This subsection presents a framework for a multimodal fusion model based on attentional CNN-LSTM network. The proposed model extracts stress related information by fusing eye data, vehicle dynamics data and environmental data to classify drivers' stress levels. The proposed framework consists of four steps: preprocessing, feature extraction, feature fusion and classification (see Fig. <ref type="figure">2</ref>).</p><p>In the first step, to synchronize eye data, vehicle dynamics data and environmental data, missing eye data and existing anomalies in the data are removed. To reduce the differences in data among participants, all data are normalized to zero mean and unit variance. Finally, we use the sliding window method to divide each feature of each modality into time window with fixed window size and degree of overlap. A new training dataset is produced by consisting of generated time windows, of which each label is the same as the original dataset.</p><p>Next, the new training dataset for each modality is fed into the 1D-CNN and LSTM framework to extract features. Specifically, the segmented time window (e.g., window t) from training dataset is first fed into the 1D-CNN to automatically learn features. Since the time window is time series, one-dimensional convolutional layer is used. This feature extraction framework consists of three 1D convolutional layers, three max pooling layers, and two-layer LSTMs. The detailed parameter settings are shown in Table <ref type="table">2</ref>. The parameter combination and model framework with the best detection accuracy are selected through trial and error. The convolutional layer uses sliding filters to extract effective features. The activation function of the convolution layer is exponential linear units (ELU), which can accelerate the convergence speed and improve the robustness of the model. Each layer of convolution is followed by a max pooling layer. In order to reduce data complexity, the max pooling layer reduces the amount of data to half of the original. The dropout layer is adopted after the pooling layer to avoid overfitting. In each training epoch, a portion of the neurons of dropout layer are randomly selected and are not allowed to participate in weight optimization. After three layers of convolution and pooling, the input data are transformed into high dimensional feature maps. Since the feature maps are extracted from the time window and the operations of convolution and pooling do not change their temporal order, the feature maps are fed directly into the two-layer LSTMs. The LSTM network processes time series by a gating mechanism, which includes forget gate, input gate and output gate. They can control the discarding or adding of information to enable forgetting and remembering. The feature maps are converted to the corresponding hidden states through the LSTM network.</p><p>In the fusion step, the generated hidden states from the eye data, vehicle dynamics data and environmental data are integrated to create a new feature map. There are n hidden states h t in this feature map. The feature map is denoted as H:</p><formula xml:id="formula_0">H = (h 1 , h 2 , ..., h n ).</formula><p>(1)</p><p>Since different hidden states have different degrees of impact on stress detection, we introduce a self-attention mechanism to weigh all hidden states. These hidden states are aggregated into a vector representation s by the attention layer, of which the formulas are as follows:</p><formula xml:id="formula_1">u t = tanh(Wh t + b),</formula><p>(2)</p><formula xml:id="formula_2">? t = exp(u T t u) ? n t=1 exp(u T t u) , (3) s = ? n t=1 ? t h t .</formula><p>(4)</p><p>The hidden state h t is first fed into a fully connected layer with an  -Daytime activation function of tanh to get u t as a hidden representation of h t . The transpose of the output u t is multiplied by the trainable parameter vector u to get the alignment coefficient of attention. Then, the softmax function is used to normalize the alignment coefficient to obtain the summation weight ? t . After that, we compute the vector representation s as a weighted sum of hidden states. In the last step of classification, the vector representation s can be used as a feature vector to be fed into the Softmax layer for stress level classification. The W in the formula ( <ref type="formula">2</ref>) is a weight matrix. The b is a bias vector of the fully connected layer in the attention layer, its dimensions is d a . And the u is a trainable parameter vector used to represent context information and its dimensions is also d a . Here, d a is an important hyper-parameter. After extensive experimental verification, it has been shown that the larger the dimension of d a the better the performance (accuracy and false positive) of the model is in the 5 s window (Fig. <ref type="figure" target="#fig_1">3</ref>). Finally, in order to make the model reach a balance between model performance and computational complexity, the best dimension of d a is set to 64. During the training process, the weight matrix W, bias vector b, and the parameter vector u are initialized randomly. This subsection a new framework to multimodal data fusion. Specifically, a self-attention mechanism is used to fuse the hidden states of the LSTM output and give different attention to each hidden state. It is worth noting that the proposed framework can not only fuse eye data, vehicle dynamics data and environment data, but also be suitable for processing other multimodal time series data. The proposed framework uses an end-to-end approach to training without the need for Fig. <ref type="figure">2</ref>. The multimodal fusion model based on attentional CNN-LSTM network is used to detect driver stress levels. The inputs of the model are eye data, vehicle data and environmental data. The outputs of the model are three levels of driver stress: low, medium and high. The preprocessed data for each are fed into the corresponding 1D-CNN and LSTM to extract features (Feature extraction step). The output of hidden states from the three modalities are concatenated to obtain a feature map (h 1 , h 2 , ..., h n ). The vector representation s is calculated as multiple weighted sums of hidden states from the feature map, where the summation weights (? 1 , ? 2 , ..., ? n ) are calculated in formulas (2) (3) (Fusion step). Finally, the s is fed into the Softmax layer for stress level classification (Classification step).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The 1D-CNN and LSTM model parameters. handcrafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>To validate the proposed multimodal fusion method based on attentional CNN-LSTM network, we have conducted comparative experiments in this section. This section also evaluates the performance of eye data on stress detection (refer to Section 4.1 for details of eye feature analysis).</p><p>In this study, eye data, vehicle dynamics data, and environmental data were segmented and synchronized using sliding windows. We used different window sizes to verify the performance of the proposed method. And, the overlapping degree of sliding windows in this study is 90%. Since the research goal is to establish a real-time driver stress detection system, the 5 s window size is currently selected (Section 4.2).</p><p>For the division of the dataset, we used a 10-fold cross validation method to verify the performance of our model. Specifically, the dataset is randomly shuffled and divided into 10 parts, and then one part is used as the test set and the others as the training set in turn. Finally, the experimental results are the average of ten test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Eye data-based stress detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">The framework of handcrafted eye features</head><p>The feature extraction framework of eye data mainly includes three steps: preprocessing, feature extraction, and classification.</p><p>First of all, maximum-minimum normalization was adopted to normalize all data. Since blinking can result in the loss of pupil diameter and gaze dispersion data, linear interpolation was used to fill in the missing sample data <ref type="bibr" target="#b54">(Soleymani et al., 2012)</ref>. Then, the average pupil diameter of left and right eyes was used as the time series of pupil diameter.</p><p>After preprocessing, the power spectrum and time domain features of the pupil diameter were analyzed. The mean, standard deviation and power spectral density (PSD) were extracted from the pupil diameter. The Hippus effect refers to a small amplitude oscillation in the frequency domain of the pupil diameter between 0.05 and 0.3 Hz with an amplitude of 1 mm <ref type="bibr" target="#b41">(Pamplona, Oliveira, &amp; Baranoski, 2009;</ref><ref type="bibr" target="#b7">Bouma &amp; Baghuis, 1971)</ref>. It has been shown that the Hippus effect occurs when a person is in a relaxed or passive state. As long as there is mental activity and psychological stress, this effect will disappear and the pupil diameter will expand. Moreover, the PSD features of pupil diameter were computed using the Welch's method in four frequency bands (0-0.2 Hz, 0.2-0.4 Hz, 0.4-0.6 Hz, and 0.6-1 Hz) <ref type="bibr" target="#b54">(Soleymani et al., 2012)</ref>. In addition to pupil diameter, the dispersion (mean and standard deviation) of eye gaze in X and Y axes were extracted to detect drivers' stress levels <ref type="bibr" target="#b67">(Zheng, Dong, &amp; Lu, 2014)</ref>. This eye gaze mainly refers to fixation which is a slight deviation of the fixation point. It usually occurs within 2-5 degrees of central vision, and lasts about 80-100 ms <ref type="bibr" target="#b16">(Hansen &amp; Ji, 2010)</ref>. Blink frequency has also been shown to be correlated to anxiety and stress <ref type="bibr" target="#b23">(Kanfer, 1960)</ref>. Ultimately, 11 kinds of eye features were extracted from the eye data. A summary of the extracted eye features is shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Last, all eye features were formed into a feature vector, which was fed into a classifier. The LSTM network was adopted as a classifier for handcrafted eye features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Comparing the performance of handcrafted features and automatically extracted features</head><p>In this subsection, we analyze eye features associated with stress levels and compare the performance of handcrafted features approach with the proposed automatic feature extraction model. The accuracy (ACC) and false positive (FP) of pupil diameter, gaze dispersion (X and Y), and blink frequency in the 5 s window are shown in Table <ref type="table">4</ref>. The experimental results are averaged values of ten times. It can be seen that the accuracies of all feature groups are higher than the 33% random level. Therefore, all feature groups are related to stress. The classification accuracies of pupil diameter and gaze dispersion are higher than that of the blink frequency. The average accuracy of 11-dimensional features reaches 74.3% (FP = 13.3%). This result shows that these eye features can effectively distinguish stress levels.</p><p>Based on Table <ref type="table">4</ref>, the proposed automatic feature extraction model has better performance than handcrafted features approach. Compared with the handcrafted features approach, the accuracy of pupil diameter, gaze dispersion (X and Y), and blink frequency under the proposed model are improved by 24.6%, 34.4%, and 3.5%, respectively. And their false positives have dropped significantly. Since the blink frequency is calculated at a certain time and is not a continuous time series, its improvement is smaller than the other two features. Finally, all eye features form a modality that is fed into the proposed model. The average accuracy of the proposed model based on all eye features reaches 92.9%, which is nearly twenty percent higher than that of the handcrafted features approach. The results show that the proposed model is better in identifying driver stress levels in a short window than the handcrafted features approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of multimodal fusion model</head><p>To verify the performance of the proposed multimodal fusion model, we collected vehicle dynamics data and environmental data besides eye data. For vehicle dynamics data, steering wheel angle, brake pedal, and gas pedal indirectly reflect the driver's instantaneous reaction to stress. For environmental data, the distance to the preceding vehicle, lane width, number of lanes, time of day, weather conditions (sunny, rain density), and visibility (fog) were selected from the advanced driving simulator (see Table <ref type="table" target="#tab_2">3</ref>). These features have been verified to be related to driver stress <ref type="bibr" target="#b28">(Lee et al., 2017;</ref><ref type="bibr" target="#b27">Lanata et al., 2015;</ref><ref type="bibr" target="#b48">Rigas et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Attention based fusion model and comparison with other multimodal fusion models</head><p>In this subsection, we evaluate the performance of the proposed multimodal fusion model of attentional CNN-LSTM in detecting driver stress levels against other multimodal fusion models. Meanwhile, the performance of the proposed model is validated under different window sizes: 5 s, 10 s, and 15 s.</p><p>Similar to handcrafted features approach in subsection 4.1, the handcrafted features approach in this subsection combines the features from eye data, vehicle dynamics data and environmental data into a feature vector, which is fed into the LSTM network. Meanwhile, we use the LSTM model with direct input of raw data as a comparison model. The multimodal fusion model (CNN-LSTM) adopted by <ref type="bibr" target="#b45">Rastgoo et al. (2019)</ref> is also compared with the proposed model.</p><p>The comparison results of the proposed model with other fusion models are shown in Fig. <ref type="figure" target="#fig_2">4</ref>, which indicates that the proposed model has the highest accuracy rate under different window sizes. The accuracies of the multimodal fusion models were obtained through averaging on 10 times. As the window size increases, the accuracy of the proposed model is improved by 0.8% and 1.8% under the 10 s and 15 s windows, respectively. The accuracy of LSTM model is not improved with increased window size. Although handcrafted features approach and the CNN-LSTM model get good results in the 10 s window, their accuracies are still lower than the proposed model. Particularly, the average accuracy of the proposed model in the 15 s window reaches 97.3%, which is improved by 8%, 29.1%, and 9.4% respectively compared to handcrafted features approach, LSTM model, and CNN-LSTM model.</p><p>Although the long window sizes can improve the accuracy of the model, we finally chose the 5 s window size data in order to build a real-time driver stress detection system. Table <ref type="table" target="#tab_4">5</ref> shows the performance of different fusion models under single-modal data and multimodal data in the 5 s window. Obviously, the accuracies of the multimodal fusion models are superior to those of the single-modal models, which means that the fusion models can complement the information of each modality. Since the data of different modalities are very different, eye data and vehicle dynamics data have a greater influence on the detection results of the stress level, while the influence of environmental data is smaller. The self-attention mechanism was introduced to deal with features with different degrees of influence. Thus, the average accuracy of the proposed model reaches 95.5%, which is 4.7 percentage points higher than the traditional CNN-LSTM model. In multimodal fusion, the average accuracy and false positives of multimodal fusion model based on attentional CNN-LSTM network have verified its superior performance in driver stress detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Comparison of confusion matrices for multimodal fusion models</head><p>In this subsection, the confusion matrix of each multimodal fusion model in the 5 s window is shown in Fig. <ref type="figure" target="#fig_3">5</ref>, which details the advantages and disadvantages of each model at three stress levels. Obviously, the accuracy of each model for detecting high stress levels is lower than that of low and medium stress levels. This is because the number of high stress samples is less than that of low or medium stress, which leads the model to misclassify high stress as low or medium stress.</p><p>Among the confusion matrices of the four multimodal fusion models, the LSTM model has the worst performance (Fig. <ref type="figure" target="#fig_3">5b</ref>). The accuracy of the handcrafted features approach is significantly improved, but it is only 76% accurate for high stress level (Fig. <ref type="figure" target="#fig_3">5a</ref>). The handcrafted features approach is less effective than the CNN-LSTM model in detecting high stress level (Fig. <ref type="figure" target="#fig_3">5c</ref>). Finally, the accuracies of the proposed model have been significantly improved at low, medium, and high stress levels (Fig. <ref type="figure" target="#fig_3">5d</ref>). It can be seen that the introduction of the self-attention mechanism strengthens the ability of model to detect three stress levels, and significantly improves the detection accuracy of high stress</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The average performance (accuracy and false positive) of handcrafted features and automatically extracted features.   level. As a result, the attention-based CNN-LSTM model has the best capability for driver stress levels among all multimodal fusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Comparison of attention based fusion model with other works</head><p>In this subsection, the experimental results of the multimodal model based on attentional CNN-LSTM network are compared with other recent works. Table <ref type="table" target="#tab_5">6</ref> shows some works using eye data and multimodal fusion methods.</p><p>As can be seen, <ref type="bibr" target="#b42">Pedrotti et al. (2014)</ref>, <ref type="bibr" target="#b60">Wang et al. (2013)</ref>, and Baltaci and Gokcay (2016) mainly used physiological modalities to detect the driver's stress level. Among them, pupil diameter (PD) obtained without contact was concerned by researchers. <ref type="bibr" target="#b47">Rigas et al. (2011)</ref>   two stress levels (no stress, stress). Compared with the work using only one modality, the performance of stress detection by multimodalities has been improved. The first three works used handcrafted features approach, which required a large window size of data, making it difficult to monitor driver stress status in real time. <ref type="bibr" target="#b45">Rastgoo et al. (2019)</ref> proposed a multimodal deep learning model to fuse physiological signals (ECG), vehicle dynamics data and environmental data and achieved good stress detection performance, but the applied physiological signals were invasive. Our proposed model is non-invasive, and achieves better accuracy, and the sensitivity, specificity, and precision of the proposed model are improved as well.</p><p>In summary, the comprehensive performance of our proposed model outperforms other recent works on driver stress level detection. Specifically, there are some advantages of the proposed model. Firstly, the proposed model uses eye data, vehicle dynamics data and environmental data, which is non-invasive compared with other works using traditional physiological data (ECG, EDA, and EEG). In addition, the proposed model using a small window size (5 s) of data can detect the driver stress in real time. Finally, the self-attention mechanism can effectively fuse data from different modalities by assigning different weights to different features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a multimodal fusion model based on attentional CNN-LSTM network for driver stress detection. It is characterized by being non-invasive, accurate and real-time. On the dataset collected by the advanced driving simulator, extensive experiments were carried out to verify the performance of the proposed model. Experimental results demonstrate that eye data is highly correlated to stress levels and very effective in stress detection. Furthermore, the performance of the proposed model is verified as superior to other stateof-the-art models under different window sizes. Additionally, the proposed model can effectively complement and weigh the information from eye data, vehicle dynamics data and environmental data. Therefore, the attention-based CNN-LSTM network is a promising method for driver stress detection.</p><p>Though the non-invasive multimodal data combination and featurelevel multimodal fusion model have good application prospects, concerns of its application in real vehicles could be raised. First, limitations could be induced by the number and the selection method of participants. Specifically, the sample size of 22 participants is relatively small and the experiment did not consider people over 40 years old. Second, the ground truth of driver stress is determined by the participant's subjective assessment. This is completely subjective, even if the participants are already familiar with the evaluation method of the stress level before the experiment and the subjective assessment is the most reliable technique to annotate data. Yet, it may fall short of objectiveness.</p><p>For future work, we will adhere to multimodal fusion method and introduce non-invasive physiological data. Our research has shown that different modalities can compensate for each other, thus the choice of appropriate fusion levels (sensor, feature, score, or decision) could be the key to building an optimal model. Next, we will increase the sample size and age range of participants to improve the generalization ability of the model. Due to the labeling of data relies entirely on subjective assessment, we need to improve the ground truth of driver stress through combining subjective assessment with expert assessment by video or physiological data. Recently, unsupervised learning has made great strides in other areas, and has even surpassed supervised learning in some areas <ref type="bibr" target="#b17">(He, Fan, Wu, Xie, &amp; Girshick, 2020)</ref>. Unsupervised learning can learn features related to driver stress on unlabeled data, which can solve the problem of laborious, tedious, and inaccurate labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CARRS-Q's driving simulator was used to collect data. The simulator consists of a front view (180 degrees), rear view mirror image, real cabin, audio system to simulate the driving environment and a six degree of freedom motion platform, as well as the SCANeR? studio and FaceLAB? remote video eye tracker.</figDesc><graphic url="image-4.png" coords="4,127.62,55.43,340.08,142.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The average performance (accuracy and false positive) against hyper-parameter d a in different dimensions.</figDesc><graphic url="image-6.png" coords="5,127.56,524.76,340.16,201.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The average accuracy of different fusion models with different window sizes.</figDesc><graphic url="image-7.png" coords="7,127.56,146.09,340.16,202.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The confusion matrix of multimodal fusion models: (a) handcrafted features approach, (b) LSTM model, (c) CNN-LSTM model, (d) attention-based CNN-LSTM model.</figDesc><graphic url="image-8.png" coords="8,70.87,55.40,453.60,345.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Different stressors in different driving scenarios.</figDesc><table><row><cell>Scenarios</cell><cell>Number</cell><cell>Road</cell><cell>Simulator</cell><cell>Weather</cell><cell>Time</cell></row><row><cell></cell><cell>of</cell><cell>situations</cell><cell>parameters</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Vehicles</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Urban 1</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>Daytime</cell></row><row><cell>Urban 2</cell><cell>30</cell><cell>Narrow,</cell><cell>-</cell><cell>-</cell><cell>Night</cell></row><row><cell></cell><cell></cell><cell>Curve</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Highway</cell><cell>50</cell><cell>Curve</cell><cell>Stay on</cell><cell>Rain density</cell><cell>Night</cell></row><row><cell></cell><cell></cell><cell></cell><cell>lane, etc.</cell><cell>(0.2-1),</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Foggy</cell><cell></cell></row><row><cell>CBD 1</cell><cell>50</cell><cell>Narrow,</cell><cell>Stay on</cell><cell>Rain density</cell><cell>Daytime</cell></row><row><cell></cell><cell></cell><cell>Curve,</cell><cell>lane, etc.</cell><cell>(0.3-0.6),</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tight</cell><cell></cell><cell>Foggy</cell><cell></cell></row><row><cell></cell><cell></cell><cell>corner</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBD 2</cell><cell>60</cell><cell>Curve,</cell><cell>Stay on</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tight</cell><cell>lane, etc.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>corner</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Handcrafted features from eye data, vehicle dynamics data and environmental data.</figDesc><table><row><cell></cell><cell>Handcrafted Features</cell></row><row><cell>Eye data</cell><cell>Pupil diameter (Mean, standard deviation)</cell></row><row><cell></cell><cell>Pupil diameter (PSD in four bands:0-0.2 Hz, 0.2-0.4 Hz,</cell></row><row><cell></cell><cell>0.4-0.6 Hz, 0.6-1 Hz)</cell></row><row><cell></cell><cell>Dispersion (X and Y) (Mean, standard deviation)</cell></row><row><cell></cell><cell>Blink frequency</cell></row><row><cell>Vehicle dynamics</cell><cell>Steering wheel angle (Mean, standard deviation)</cell></row><row><cell>data</cell><cell>Brake pedal data (Mean, standard deviation)</cell></row><row><cell></cell><cell>Gas pedal data (Mean, standard deviation)</cell></row><row><cell>Environmental data</cell><cell>Distance to preceding vehicle</cell></row><row><cell></cell><cell>Time of day</cell></row><row><cell></cell><cell>Road situations (Lane width, Number of lanes)</cell></row><row><cell></cell><cell>Visibility (fog)</cell></row><row><cell></cell><cell>Weather conditions (sun, low rain, medium rain, high rain)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>The average performance (accuracy and false positive) of handcrafted features approach, LSTM model, CNN-LSTM model, and attention-based CNN-LSTM model in different modalities.</figDesc><table><row><cell>L. Mou et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>Environmental data</cell><cell></cell><cell>Vehicle data</cell><cell></cell><cell>Eye data</cell><cell></cell><cell>Fusion</cell><cell></cell></row><row><cell></cell><cell>ACC (%)</cell><cell>FP (%)</cell><cell>ACC (%)</cell><cell>FP (%)</cell><cell>ACC (%)</cell><cell>FP (%)</cell><cell>ACC (%)</cell><cell>FP (%)</cell></row><row><cell>Handcrafted</cell><cell>49.9</cell><cell>27.0</cell><cell>49.2</cell><cell>27.7</cell><cell>74.3</cell><cell>13.3</cell><cell>88.8</cell><cell>5.9</cell></row><row><cell>LSTM</cell><cell>50.6</cell><cell>26.6</cell><cell>44.9</cell><cell>30.1</cell><cell>58.1</cell><cell>22.1</cell><cell>71.5</cell><cell>14.7</cell></row><row><cell>CNN-LSTM</cell><cell>51.0</cell><cell>26.4</cell><cell>73.6</cell><cell>13.6</cell><cell>85.8</cell><cell>7.4</cell><cell>90.8</cell><cell>4.8</cell></row><row><cell>Attention</cell><cell>52.6</cell><cell>25.1</cell><cell>85.1</cell><cell>7.8</cell><cell>92.9</cell><cell>3.6</cell><cell>95.5</cell><cell>2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>The comparison of the best performance of the multimodal model based on attentional CNN-LSTM network with other recent works.</figDesc><table><row><cell>Reference</cell><cell>No.</cell><cell>of Method</cell><cell>Physiological</cell><cell>Used modalities</cell><cell></cell><cell>Window</cell><cell>Performance</cell><cell>No. Classes</cell></row><row><cell></cell><cell>Subjects</cell><cell></cell><cell></cell><cell>Physical</cell><cell>Context</cell><cell>size</cell><cell></cell><cell></cell></row><row><cell>(Pedrotti</cell><cell>33</cell><cell>Handcrafted</cell><cell>PD, EDA</cell><cell>-</cell><cell>-</cell><cell>80 s</cell><cell>Accuracy: 79.2%</cell><cell>4 stress class (low,</cell></row><row><cell>et al.,</cell><cell></cell><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>medium, high, very</cell></row><row><cell>2014)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>high)</cell></row><row><cell>(Wang et al.,</cell><cell>17</cell><cell>Handcrafted</cell><cell>ECG</cell><cell>-</cell><cell>-</cell><cell>300 s</cell><cell>Accuracy: 80%</cell><cell>3 stress class (low,</cell></row><row><cell>2013)</cell><cell></cell><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>medium, high)</cell></row><row><cell>(Baltaci et</cell><cell>11</cell><cell>Handcrafted</cell><cell>PD, Face</cell><cell>-</cell><cell>-</cell><cell>18 s</cell><cell>Accuracy: 83.8%</cell><cell>2 stress class</cell></row><row><cell>al., 2016)</cell><cell></cell><cell>features</cell><cell>temperature</cell><cell></cell><cell></cell><cell></cell><cell>Sensitivity: 83.9%</cell><cell>(no stress, stress)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Specificity: 83.8%</cell><cell></cell></row><row><cell>(Rigas et al.,</cell><cell>1</cell><cell>Handcrafted</cell><cell>ECG, EDA,</cell><cell>Eye, Head</cell><cell>Environmental</cell><cell>10 s</cell><cell>Accuracy: 86%</cell><cell>2 stress class (no</cell></row><row><cell>2011)</cell><cell></cell><cell>features</cell><cell>RSP</cell><cell>movement</cell><cell>data</cell><cell></cell><cell></cell><cell>stress, stress)</cell></row><row><cell>(Rastgoo et</cell><cell>27</cell><cell>Deep learning</cell><cell>ECG</cell><cell>Vehicle</cell><cell>Environmental</cell><cell>5 s</cell><cell>Accuracy: 92.8%</cell><cell>3 stress class</cell></row><row><cell>al., 2019)</cell><cell></cell><cell>(CNN-LSTM)</cell><cell></cell><cell>dynamic data</cell><cell>data</cell><cell></cell><cell>Sensitivity: 94.13%</cell><cell>(low, medium, high)</cell></row><row><cell></cell><cell></cell><cell>network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Specificity: 97.37%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision: 95.00%</cell><cell></cell></row><row><cell>Our work</cell><cell>22</cell><cell>Attention based</cell><cell>PD</cell><cell>Eye</cell><cell>Environmental</cell><cell>5 s</cell><cell>Accuracy: 95.5%</cell><cell>3 stress class</cell></row><row><cell></cell><cell></cell><cell>CNN-LSTM</cell><cell></cell><cell>movement,</cell><cell>data</cell><cell></cell><cell>Sensitivity: 95.31%</cell><cell>(low, medium, high)</cell></row><row><cell></cell><cell></cell><cell>network</cell><cell></cell><cell>Vehicle dynamic</cell><cell></cell><cell></cell><cell>Specificity: 97.67%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>data</cell><cell></cell><cell></cell><cell>Precision: 95.34%</cell><cell></cell></row></table><note><p><p>L.</p>Mou et al.   </p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under grant <rs type="grantNumber">61672068</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TT3dgut">
					<idno type="grant-number">61672068</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying deep neural networks for multi-level classification of driver drowsiness using vehicle-based measures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arefnezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eichberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fr?hwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klotz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.113778</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2020.113778" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">113778</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, SD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">2015. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stress detection in human-computer interaction: Fusion of pupil dilation and facial temperature features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baltaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gokcay</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2016.1220069</idno>
		<ptr target="https://doi.org/10.1080/10447318.2016.1220069" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="956" to="966" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Driver inattention and driver distraction in serious casualty crashes: Data from the Australian National Crash In-depth Study</title>
		<author>
			<persName><forename type="first">V</forename><surname>Beanland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fitzharris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lenn?</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aap.2012.12.043</idno>
		<ptr target="https://doi.org/10.1016/j.aap.2012.12.043" />
	</analytic>
	<monogr>
		<title level="j">Accident Analysis &amp; Prevention</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="99" to="107" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal focus attention and stress detection and feedback in an augmented driver simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bonnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Trevisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-007-0173-0</idno>
		<ptr target="https://doi.org/10.1007/s00779-007-0173-0" />
	</analytic>
	<monogr>
		<title level="j">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Script identification in natural scene image and video frames using an attention based Convolutional-LSTM network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.07.034</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2018.07.034" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="172" to="184" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis and detection of cognitive load and frustration in drivers&apos; speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bo?il</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kleinschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="502" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hippus of the pupil: Periods of slow oscillations of unknown origin</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C J</forename><surname>Baghuis</surname></persName>
		</author>
		<idno type="DOI">10.1016/0042-6989(71)90016-2</idno>
		<ptr target="https://doi.org/10.1016/0042-6989" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="90016" to="90018" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personality, executive control, and neurobiological characteristics associated with different forms of risky driving</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Ouimet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eldeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vingilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yechiam</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.015022710.1371/journal.pone.0150227.t00110.1371/journal.pone.0150227.t00210.1371/journal.pone.0150227.t003</idno>
		<idno>pone.0150227.t00110.1371/journal.pone.0150227.t00210.1371/journal. pone.0150227.t003</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.015022710.1371/journal" />
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">150227</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dexpression: Deep convolutional neural network for expression recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Trier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno>ArXiv:1509. 05371</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Which eye tracker is right for your research? Performance evaluation of several cost variant eye trackers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greenlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dukes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Menke</surname></persName>
		</author>
		<idno type="DOI">10.1177/1541931213601289</idno>
		<ptr target="https://doi.org/10.1177/1541931213601289" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1240" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time eye gaze direction classification using convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
		<idno type="DOI">10.1109/spcom.2016.7746701</idno>
		<ptr target="https://doi.org/10.1109/spcom.2016.7746701" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing and Communications (SPCOM)</title>
		<meeting><address><addrLine>Bangalore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">2016. June. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Methodological analysis of driver perception-brake times</title>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Human Factors</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="216" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>How long does it take to stop?</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Detecting stress using eye blinks and brain activity from EEG signals. Proceeding of the 1st Driver Car Interaction and Interface</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2009. 2008</date>
			<biblScope unit="page" from="35" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sleep Apnea event detection from nasal airflow using convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jeffries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="819" to="827" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">In the eye of the beholder: A survey of models for eyes and gaze</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2009.30</idno>
		<ptr target="https://doi.org/10.1109/tpami.2009.30" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="500" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00975</idno>
		<idno>42600.2020.00975</idno>
		<ptr target="https://doi.org/10.1109/cvpr" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time detection of acute cognitive stress using a convolutional neural network from electrocardiographic signal</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/Access.628763910.1109/ACCESS.2019.2907076</idno>
		<idno>628763910.1109/ACCESS.2019.2907076</idno>
		<ptr target="https://doi.org/10.1109/Access" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="42710" to="42717" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting stress during real-world driving tasks using physiological sensors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2005.848368</idno>
		<ptr target="https://doi.org/10.1109/TITS.2005.848368" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="166" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Driver stress as influenced by driving maneuvers and roadway conditions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part F: Traffic Psychology and Behaviour</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2012.2205597</idno>
		<ptr target="https://doi.org/10.1109/MSP.2012.2205597" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal multimodal learning in audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3574" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Verbal rate, eyeblink, and content in structured psychiatric interviews</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Kanfer</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0038933</idno>
		<ptr target="https://doi.org/10.1037/h0038933" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="347" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M G</forename><surname>Younis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="46" to="56" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward emotion recognition in car-racing drivers: A biosignal processing approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katertsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ganiatsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmca.2008.918624</idno>
		<ptr target="https://doi.org/10.1109/tsmca.2008.918624" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="512" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stress-indicators and exploratory gaze for the analysis of hazard perception in patients with visual field loss</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.trf.2014.04.016</idno>
		<ptr target="https://doi.org/10.1016/j.trf.2014.04.016" />
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part F: Traffic Psychology and Behaviour</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How the autonomic nervous system and driving style change with incremental stressing conditions during simulated driving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Scilingo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2014.2365681</idno>
		<ptr target="https://doi.org/10.1109/TITS.2014.2365681" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1505" to="1517" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stress events detection of driver by wearable glove system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSEN.2016.2625323</idno>
		<ptr target="https://doi.org/10.1109/JSEN.2016.2625323" />
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="204" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Urban PM2.5 concentration prediction via attention-based CNN-LSTM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3390/app10061953</idno>
		<ptr target="https://doi.org/10.3390/app10061953" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1953">2020. 1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017-04">2017. April. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">U</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">U</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="191" to="207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion recognition using multimodal deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="521" to="529" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Attention-based Hybrid LSTM-CNN Model for Arrhythmias Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2019.8852037</idno>
		<ptr target="https://doi.org/10.1109/ijcnn.2019.8852037" />
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019-07">2019. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short term memory hyperparameter optimization for a neural network based emotion recognition framework</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotonirainy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2868361</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2868361" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49325" to="49338" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic emotion recognition using temporal multimodal deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotonirainy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3027026</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3027026" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evolutionary computation algorithms for feature selection of EEG-based emotion recognition using mobile sensors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2017.09.062</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2017.09.062" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning human identity from motion patterns</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barbello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1810" to="1820" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ord??ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<idno type="DOI">10.3390/s16010115</idno>
		<ptr target="https://doi.org/10.3390/s16010115" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Estimating cognitive load using remote eye tracking in a driving simulator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Palinko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Kun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shyrokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications</title>
		<meeting>the 2010 Symposium on Eye-Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="141" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Photorealistic models for pupil light reflex and iridal pattern deformation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Pamplona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V G</forename><surname>Baranoski</surname></persName>
		</author>
		<idno type="DOI">10.1145/1559755.1559763</idno>
		<ptr target="https://doi.org/10.1145/1559755.1559763" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic stress classification with pupil diameter analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedrotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tedesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Chardonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>M?rienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2013.848320</idno>
		<ptr target="https://doi.org/10.1080/10447318.2013.848320" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="236" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Malicious uniform resource locator attention-based CNN-LSTM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3837/tiis.2019.11.017</idno>
		<ptr target="https://doi.org/10.3837/tiis.2019.11.017" />
	</analytic>
	<monogr>
		<title level="j">KSII Transactions on Internet and Information Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5580" to="5593" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and learning ecg features for screening paroxysmal atrial fibrillation patients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pourbabaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khorasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic driver stress level classification using multimodal deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotonirainy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.07.010</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.07.010" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page">112793</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A critical review of proactive detection of driver stress levels based on multimodal measurements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotonirainy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<idno type="DOI">10.1145/3186585</idno>
		<ptr target="https://doi.org/10.1145/3186585" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards Driver&apos;s State Recognition on Real Driving Conditions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goletsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bougia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
		<ptr target="http://www.hindawi.com/journals/ijvt/2011/617210/abs/" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Vehicular Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-Time Driver&apos;s Stress Event Detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goletsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
		<idno type="DOI">10.1109/tits.2011.2168215</idno>
		<ptr target="https://doi.org/10.1109/tits.2011.2168215" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A mobile sensing approach to stress detection and memory activation for public bus drivers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaiseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P S</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barros</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2015.2445314</idno>
		<ptr target="https://doi.org/10.1109/TITS.2015.2445314" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3294" to="3303" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2015.7178838</idno>
		<ptr target="https://doi.org/10.1109/icassp.2015.7178838" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Road Traffic Crash Fatalities: An Examination of National Fatality Rates and Factors Associated with the Variation in Fatality Rates between Nations with Reference to the World Health Organisation Decade of Action for Road Safety 2011-2020</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Sauerzapf</surname></persName>
		</author>
		<ptr target="https://ueaeprints.uea.ac.uk/id/eprint/46589" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of East Anglia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><surname>Scanerstudio</surname></persName>
		</author>
		<ptr target="https://www.avsimulation.com/scanerstudio/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Selye</surname></persName>
		</author>
		<title level="m">Stress without distress</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="26" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in response to videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<idno type="DOI">10.1109/t-affc.2011.37</idno>
		<ptr target="https://doi.org/10.1109/t-affc.2011.37" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automated detection of obstructive sleep apnea events from a single-lead electrocardiogram using a convolutional neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Urtnasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10916-018-0963-0</idno>
		<idno>10916-018-0963-0</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stress-related psychosocial factors at work, fatigue, and risky driving behavior in bus rapid transport (BRT) drivers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Cendales</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aap.2017.04.023</idno>
		<ptr target="https://doi.org/10.1016/j.aap.2017.04.023" />
	</analytic>
	<monogr>
		<title level="j">Accident Analysis &amp; Prevention</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="106" to="114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Controlling Steering Angle for Cooperative Self-driving Vehicles utilizing CNN and LSTM-based Deep Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Valiente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Fallah</surname></persName>
		</author>
		<idno type="DOI">10.1109/ivs.2019.8814260</idno>
		<ptr target="https://doi.org/10.1109/ivs.2019.8814260" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2423" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Biological and Behavioral Factors Affecting Driving Safety</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vivoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bergomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rovesti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bussetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Guaitoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Preventive Medicine and Hygiene</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="69" to="73" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A k-nearest-neighbor classifier with heart rate variability feature-based transformation algorithm for driving stress recognition</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T.-C</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2011.10.047</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2011.10.047" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="136" to="143" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Kampman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2018.8461990</idno>
		<ptr target="https://doi.org/10.1109/icassp.2018.8461990" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6204" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Effects of chevron alignment signs on driver eye movements, driving performance, and stress</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.3141/2365-02</idno>
		<ptr target="https://doi.org/10.3141/2365-02" />
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">2365</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="16" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298966</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298966" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Conference of the North American Chapter</title>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>NAACL HLT</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">In Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Real-time system for driver fatigue detection by RGB-D Camera</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using EEG and eye tracking data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2014.6944757</idno>
		<ptr target="https://doi.org/10.1109/embc.2014.6944757" />
	</analytic>
	<monogr>
		<title level="m">2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5040" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
