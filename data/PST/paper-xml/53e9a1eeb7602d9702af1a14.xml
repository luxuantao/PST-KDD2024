<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Properties and Complexity of Some Formal Inter-agent Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Parsons</surname></persName>
							<email>parsons@sci.brooklyn.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">Brooklyn College</orgName>
								<orgName type="institution" key="instit2">City University of New York</orgName>
								<address>
									<addrLine>2900 Bedford Avenue</addrLine>
									<postCode>11210</postCode>
									<settlement>Brooklyn</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Wooldridge</surname></persName>
							<email>m.j.wooldridge@csc.liv.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<postCode>L69 7ZF</postCode>
									<settlement>Liverpool</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leila</forename><surname>Amgoud</surname></persName>
							<email>leila.amgoud@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Properties and Complexity of Some Formal Inter-agent Dialogues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E573DCDCCD536FF809C9CFC70DDA519</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argumentation</term>
					<term>dialogue</term>
					<term>complexity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies argumentation-based dialogues between agents. It defines a set of locutions by which agents can trade arguments, a set of agent attitudes which relate what arguments an agent can build and what locutions it can make, and a set of protocols by which dialogues can be carried out. The paper then considers some properties of dialogues under the protocols, in particular termination, dialogue outcomes, and complexity, and shows how these relate to the agent attitudes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When building multi-agent systems, we take for granted the fact that the agents that make up the system will need to communicate. They need to communicate in order to resolve differences of opinion and conflicts of interest, work together to resolve dilemmas or find proofs, or simply to inform each other of pertinent facts. Many of these communication requirements cannot be fulfilled by the exchange of single messages. Instead, the agents concerned need to be able to exchange a sequence of messages which all bear upon the same subject. In other words they need the ability to engage in dialogues. As a result of this requirement, there has been much work on providing agents with the ability to hold such dialogues -for example by Dignum and colleagues <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, Grosz and Kraus <ref type="bibr" target="#b14">[15]</ref>, Parsons and Jennings <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, Reed <ref type="bibr" target="#b36">[37]</ref>, Schroeder et al. <ref type="bibr" target="#b39">[40]</ref>, and Sycara <ref type="bibr" target="#b41">[42]</ref>.</p><p>Reed's work built on an influential model of human dialogues due to argumentation theorists Doug Walton and Erik Krabbe <ref type="bibr" target="#b44">[45]</ref>, and we also take their dialogue typology as our starting point. Walton and Krabbe set out to analyse the concept of commitment in dialogue, so as to 'provide conceptual tools for the theory of argumentation' <ref type="bibr">[45, p. ix]</ref>. This led to a focus on persuasion dialogues, and their work presents formal models for such dialogues. In attempting this task, they recognized the need for a characterization of dialogues, and so they present a broad typology for interpersonal dialogue. They make no claims for its comprehensiveness.</p><p>Their categorization identifies six primary types of dialogues and three mixed types. The categorization is based upon: first, what information the participants each have at the commencement of the dialogue (with regard to the topic of discussion); second, what goals the individual participants have; and, third, what goals are shared by the participants, goals we may view as those of the dialogue itself. As defined by Walton and Krabbe, the six primary dialogue types are (re-ordered from <ref type="bibr" target="#b44">[45]</ref>):</p><p>Information-seeking Dialogues: One participant seeks the answer to some question(s) from another participant, who is believed by the first to know the answer(s). Inquiry Dialogues: The participants collaborate to answer some question or questions whose answers are not known to any one participant. Persuasion Dialogues: One party seeks to persuade another party to adopt a belief or pointof-view he or she does not currently hold. These dialogues begin with one party supporting a particular statement which the other party to the dialogue does not, and the first seeks to convince the second to adopt the proposition. The second party may not share this objective. Negotiation Dialogues: The participants bargain over the division of some scarce resource in a way acceptable to all, with each individual party aiming to maximize his or her share. The goal of the dialogue may be in conflict with the individual goals of each of the participants.<ref type="foot" target="#foot_1">1</ref> Deliberation Dialogues: Participants collaborate to decide what course of action to take in some situation. Participants share a responsibility to decide the course of action, and either share a common set of intentions or a willingness to discuss rationally whether they have shared intentions. Eristic Dialogues: Participants quarrel verbally as a substitute for physical fighting, with each aiming to win the exchange. We include Eristic dialogues here for completeness, but we do not discuss them further.</p><p>In previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, we began to investigate how these different types of dialogue can be captured using a formal model of argumentation. Here we extend this work, examining some of the possible forms of information seeking, inquiry and persuasion dialogues that are possible, and identifying how the properties of these dialogues depend upon the properties of the agents engaging in them. Note that, despite the fact that the types of dialogue we are considering are drawn from the analysis of human dialogues, we are only concerned here with dialogues between artificial agents. Unlike <ref type="bibr" target="#b15">[16]</ref> for example, we choose to focus in this way in order to simplify our task-doing this allows us to deal with artificial languages and avoid much of the complexity inherent in natural language dialogues. This issue is discussed in more depth in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section we briefly introduce the formal system of argumentation which forms the backbone of our approach. This is inspired by the work of Dung <ref type="bibr" target="#b10">[11]</ref> but goes further in Properties and Complexity of Some Formal Inter-agent Dialogues 349 dealing with preferences between arguments. Further details are available in <ref type="bibr" target="#b0">[1]</ref>. We start with a possibly inconsistent knowledge base ¦ with no deductive closure. We assume ¦ contains formulas of a propositional language Ä. stands for classical inference and for logical equivalence. An argument is a proposition and the set of formulae from which it can be inferred. <ref type="bibr">DEFINITION</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>An argument is a pair = ´À µ where is a formula of Ä and À a subset of ¦ such that:</p><p>1. À is consistent; 2. À ; and 3. À is minimal, so no subset of À satisfying both 1. and 2. exists.</p><p>À is called the support of A, written À = Support( ) and is the conclusion of written = Conclusion( ).</p><p>We talk of being supported by the argument ´À µ.</p><p>In general, since ¦ is inconsistent, arguments in ´¦µ, the set of all arguments which can be made from ¦, will conflict, and we make this idea precise with the notion of undercutting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 2.2</head><p>Let ½ and ¾ be two arguments of ´¦µ. ½ undercuts ¾ iff ¾ ËÙÔÔÓÖØ´ ¾ µ such that ÓÒ ÐÙ× ÓÒ´ ½ µ.</p><p>In other words, an argument is undercut if and only if there is another argument which has as its conclusion the negation of an element of the support for the first argument.</p><p>To capture the fact that some facts are more strongly believed <ref type="foot" target="#foot_3">2</ref> we assume that any set of facts has a preference order over it (other approaches to quantifying belief, such as probability, could also be used in conjunction with our approach). We suppose that this ordering derives from the fact that the knowledge base ¦ is stratified into non-overlapping sets ¦ ½ ¦ Ò such that facts in ¦ are all equally preferred and are more preferred than those in ¦ where . The preference level of a nonempty subset À of ¦, Ð Ú Ð´À µ, is the number of the highest numbered layer which has a member in À .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 2.3</head><p>Let ½ and ¾ be two arguments in ´¦µ. ½ is preferred to ¾ according to ÈÖ iff Ð Ú Ð´ËÙÔÔÓÖØ´ ½ µµ Ð Ú Ð´ËÙÔÔÓÖØ´ ¾ µµ.</p><p>By ÈÖ we denote the strict pre-order associated with ÈÖ . If ½ is preferred to ¾ , we say that ½ is stronger than ¾ . This is clearly a very restricted notion of how to handle preferences. Other approaches to handling preferences, which could also be used along with our approach, are surveyed in <ref type="bibr" target="#b9">[10]</ref>, and we discuss how this argumentation system can be extended with a more flexible notion of preferences in <ref type="bibr" target="#b4">[5]</ref> (which also allows arguments for and against preferences themselves). We stick with the simple model here for ease of exposition, noting that nothing in the rest of the paper hinges upon it-what is required for the argumentation system is a notion of preference, the use made of this notion does not depend upon the way that preferences are represented.</p><p>We can now define the argumentation system we will use. <ref type="bibr">DEFINITION 2.4</ref> An argumentation system (AS) is a triple ´¦µ ÍÒ Ö ÙØ ÈÖ such that:</p><p>¯ ´¦µis a set of the arguments built from ¦, ¯ÍÒ Ö ÙØ is a binary relation representing the defeat relationship between arguments, ÍÒ Ö ÙØ ´¦µ ¢ ´¦µ, and ¯ÈÖ is a (partial or complete) preordering on ´¦µ ¢ ´¦µ.</p><p>The preference order makes it possible to distinguish different types of relation between arguments:</p><formula xml:id="formula_0">DEFINITION 2.5 Let ½ , ¾ be two arguments of ´¦µ. ¯If ¾ undercuts ½ then ½ defends itself against ¾ iff ½ ÈÖ ¾ . Otherwise, ½ does not defend itself.</formula><p>¯A set of arguments Ë defends iff: undercuts and does not defend itself against then ¾ Ë such that undercuts and does not defend itself against .</p><p>Henceforth, ÍÒ Ö ÙØ ÈÖ will gather all non-undercut arguments and arguments defending themselves against all their undercutting arguments. In <ref type="bibr" target="#b1">[2]</ref>, it was shown that the set Ë ¦ of acceptable arguments of the argumentation system ´¦µ ÍÒ Ö ÙØ ÈÖ is the least fixpoint of a function :</p><formula xml:id="formula_1">Ë ´¦µ ´Ëµ ´À µ ¾ ´¦µ ´À µ × Ò Ý Ë DEFINITION 2.6</formula><p>The set of acceptable arguments for an argumentation system ´¦µ ÍÒ Ö ÙØ ÈÖ is:</p><formula xml:id="formula_2">Ë ¦ ¼ ´ µ</formula><p>Note that since:</p><formula xml:id="formula_3">¼ ´ µ ÍÒ Ö ÙØ ÈÖ it follows that: Ë ¦ ÍÒ Ö ÙØ ÈÖ ½ ´ ÍÒ Ö ÙØ ÈÖ µ</formula><p>An argument is acceptable if it is a member of the acceptable set. If the argument ´À µ is acceptable, we talk of there being an acceptable argument for , and we say that the proposition is acceptable to an agent that has an acceptable argument for it. An acceptable argument is one which is, in some sense, proven since all the arguments which might undermine it are themselves undermined.</p><p>It should be stressed that our choice of this particular argumentation system as a basis for the models of dialogue discussed below was somewhat arbitrary (it was the system that the third author studied in her PhD thesis, and so is one that we are familiar with). What is important is that it has a notion of what an argument is, a notion of the strength of an argument, and a notion of an argument being acceptable-these are features that control the exchange of locutions in a dialogue. Any other argumentation system that has such features could be used as the basis of the dialogue systems discussed here without the need to change any of the technical details, albeit with the side effect of possibly changing some of the properties of the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Locutions</head><p>As in our previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, agents use the argumentation mechanism described above as a basis for their reasoning and their dialogues. Agents decide what they themselves know by determining which propositions they have acceptable arguments for. They trade propositions for which they have acceptable arguments, and accept propositions put forward by other agents if they find that the arguments are acceptable. As discussed in <ref type="bibr" target="#b3">[4]</ref>, this gives argumentation-based dialogues a social semantics in the sense of Singh <ref type="bibr" target="#b40">[41]</ref>-when agents assert something, they are committed to back up that something by giving the argument for it. The exact locutions and the way that they are exchanged define a formal dialogue game which agents engage in.</p><p>Dialogues are assumed to take place between two agents, which we will call È and . <ref type="foot" target="#foot_6">3</ref>Each agent has a knowledge base, ¦ È and ¦ respectively, containing their beliefs. In addition, following Hamblin <ref type="bibr" target="#b16">[17]</ref> each agent has a further knowledge base, accessible to both agents, containing commitments made in the dialogue. These commitment stores are denoted Ë´Èµ and Ë´ µ respectively, and in this dialogue system (unlike that of <ref type="bibr" target="#b5">[6]</ref> for example) an agent's commitment store is just a subset of its knowledge base. Note that the union of the commitment stores can be viewed as the state of the dialogue at a given time. Each agent has access to their own private knowledge base and both commitment stores. Thus È can make use of ´¦È Ë´ µµ ÍÒ Ö ÙØ ÈÖ <ref type="foot" target="#foot_7">4</ref>and can make use of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>´¦</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ë´Èµ ÍÒ Ö ÙØ ÈÖ</head><p>We denote the set of all arguments ´¦È Ë´ µµ by ´È ).</p><p>All the knowledge bases contain propositional formulas and are not closed under deduction, and all are stratified by degrees of belief as discussed above. Here we assume that these degrees of belief are static and that both the players agree on them. As with the model of preferences itself, this is a very restrictive assumption, but once again we will stick with it for ease of explication. Elsewhere <ref type="bibr" target="#b4">[5]</ref> we have discussed how to combine different sets of preferences, and it is also possible to have agents modify their beliefs on the basis of the reliability of their acquaintances <ref type="bibr" target="#b26">[27]</ref>.</p><p>With this background, we can present the set of dialogue moves that we will use. For each move, we give what we call rationality rules, dialogue rules, and update rules. These are based on the rules suggested by <ref type="bibr" target="#b22">[23]</ref> which, in turn, were based on those in the dialogue game DC introduced by MacKenzie <ref type="bibr" target="#b21">[22]</ref>. The rationality rules specify the preconditions for making the move. Unlike those in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> these are not absolute, but are defined in terms of the agent attitudes discussed in Section 4. The update rules specify how commitment stores are modified by the move.</p><p>In the following, player P addresses move of the dialogue to player C. The first move of the dialogue is move 1, Ë ¼ ´È µ , Ë ¼ ´ µ , and È and strictly alternate. (A more formal description of the dialogue may be found in <ref type="bibr" target="#b4">[5]</ref>.) We start with the assertion of facts:</p><formula xml:id="formula_4">assert(p)</formula><p>where Ô is a propositional formula.</p><p>rationality the usual assertion condition for the agent.</p><p>update Ë ´È µ Ë ½ ´È µ Ô and Ë ´ µ Ë ½ ´ µ. Here Ô can be any propositional formula, as well as the special characters Í and È , discussed below. assert(S) where Ë is a set of formulas representing the support of an argument. rationality the usual assertion condition for the agent.</p><formula xml:id="formula_5">update Ë ´È µ Ë ½ ´È µ Ë and Ë ´ µ Ë ½ ´ µ.</formula><p>The counterpart of these moves are the acceptance moves:</p><formula xml:id="formula_6">accept(p) Ô is a propositional formula.</formula><p>rationality the usual acceptance condition for the agent.</p><formula xml:id="formula_7">update Ë ´È µ Ë ½ ´È µ Ô and Ë ´ µ Ë ½ ´ µ.</formula><p>accept(S) S is a set of propositional formulas. rationality the usual acceptance condition for every × ¾ Ë. update Ë ´È µ Ë ½ ´È µ Ë and Ë ´ µ Ë ½ ´ µ.</p><p>There are also moves which allow questions to be posed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>challenge(p)</head><p>where Ô is a propositional formula.</p><formula xml:id="formula_8">rationality . update Ë ´È µ Ë ½ ´È µ and Ë ´ µ Ë ½ ´ µ.</formula><p>A challenge is a means of making the other player explicitly state the argument supporting a proposition. (This locution could easily be named 'explain', but we inherit 'challenge' from DC, and keep the name to make the heritage clear.)</p><p>A question can be used to query the other player about any proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>question(p)</head><p>where p is a propositional formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rationality .</head><p>update Ë ´È µ Ë ½ ´È µ and Ë ´ µ Ë ½ ´ µ. We refer to this set of moves as the set Å ¼ since they are a variation on the set Å from <ref type="bibr" target="#b2">[3]</ref>-the main difference from the latter is that there are no 'dialogue conditions' to specify the protocol. Instead we explicitly define the protocol for each type of dialogue in Section 5.</p><p>The locutions in Å ¼ are similar to those discussed in legal reasoning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> and, unlike in some dialogue systems, there is no Ö ØÖ Ø locution. Note that these locutions are ones used within dialogues-further locutions such as those discussed in <ref type="bibr" target="#b24">[25]</ref> would be required for agents to agree to engage in dialogues, and to agree to switch between different kinds of dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Agent attitudes</head><p>One of the main aims of this paper is to explore how the kinds of dialogue in which agents engage depend upon features of the agents themselves (as opposed, for instance, to the kind of dialogue in which the agents are engaged or the information in the knowledge bases of the agents). In particular, we are interested in the effect of these features on the way in which agents determine what locutions can be made within the confines of a given dialogue protocol through the application of differing rationality conditions.</p><p>As is clear from the definition of the locutions, there are two different kinds of rationality conditions-one which determines if something may be asserted, and another which determines whether something can be accepted. The former we call assertion conditions, the latter we call acceptance conditions and talk of agents having different attitudes which relate to particular conditions. We deal first with assertion conditions. Note that we now name our agents and À , to make it clear that either or À can be the È or of the previous section. ¯If is confident, then it can assert any proposition Ô for which there is an argument ´Ë Ôµ ¾ ´ À ). ¯If is careful then it can assert any proposition Ô for which there is an argument ´Ë Ôµ if no stronger argument ´Ë ¼ Ôµ exists in ´ À ). ¯If is thoughtful then it can assert any proposition Ô for which there is an acceptable argument ´Ë Ôµ ¾ ´ À ).</p><p>Thus a thoughtful agent will only put forward propositions which, so far as it knows, are correct. A careful agent will only put forward propositions which aren't directly rebutted. A confident agent won't stop to make either of these checks. Thus thoughtful and careful agents might be considered more discriminating informants than their confident counterparts, but neither can be considered more discriminating than the other. PROPOSITION 4.2 Consider an agent . If is thoughtful or careful, then the assertions it can make are a subset of those that it could make were it confident. If is thoughtful, then the set of assertions it can make overlap with those it could make were it careful.</p><p>PROOF. If is confident, it can assert any Ô for which it has an argument ´Ë Ôµ that is in</p><p>´ À ). If is careful, it can only assert those Ôs for which it has an argument ´Ë Ôµ and no stronger argument for Ô. These are clearly a subset of those for which it has an argument.</p><p>If is thoughtful, it can only assert Ô if ´Ë Ôµ is in the set Ë ¦ ¦À . By Definition 2.6, Ë ¦ ¦À can include arguments for propositions Ô for which there are stronger arguments for Ô (just so long as there are also even stronger arguments which undercut the arguments for Ô), and so the first part of the result holds. If there is an argument for Ô which is both acceptable and stronger than any argument for Ô, then both a thoughtful and a careful could assert Ô. However, it is also possible that the argument for Ô could be stronger than any argument for Ô and also be undercut by a stronger argument for some element of its support which itself is not undercut. In this case a careful could assert Ô but a thoughtful could not. Finally, it is possible that there is an argument for Ô which is stronger than that for Ô and is undercut by a yet stronger argument which is not undercut. Then a thoughtful could assert Ô and a careful could not.</p><p>Given the fact that the set of possible assertions increases from thoughtful and careful to confident, and that, as we shall see, the ease with which these assertions can be computed increases also, it might seem worthwhile also defining what we might call a thoughtless agent, which can assert any proposition which is either in, or may be inferred from, its knowledge base. However, it is easy to show that: PROPOSITION 4.3 The set of nontrivial propositions which can be asserted by a thoughtless agent using an argumentation system ´¦µ ÍÒ Ö ÙØ ÈÖ is exactly the set which can be asserted by a confident agent using the same argumentation system. PROOF. Consider a confident agent and a thoughtless agent À with the same argumentation system.</p><p>´¦µ ÍÒ Ö ÙØ ÈÖ can assert exactly those propositions that it has an argument for. So by Definition 2.1 it can assert any Ô which it can infer from a minimal consistent subset of ¦, including all the propositions Õ in ¦ (these are the conclusions of the arguments ´ Õ Õµ). À can assert any proposition which is either in ¦ (which will be exactly the same as those can assert) or can be inferred from it. Those propositions that are nontrivial will be those that can be inferred from a consistent subset of ¦. These latter will clearly be ones for which an argument can be built, and so exactly those that can be asserted by .</p><p>Thus the idea of a thoughtless agent adds nothing to our classification.</p><p>At the risk of further overloading some well-used terms we can define acceptance conditions. <ref type="bibr">DEFINITION 4.4</ref> An agent may have one of three acceptance attitudes. If agent is engaged in a dialogue with agent À , then:</p><p>¯If is credulous then it can accept any proposition Ô previously asserted by À if ´Ë Ôµ ¾</p><p>´ À ). ¯If is cautious then it can accept any proposition Ô previously asserted by À for which there is an argument ´Ë Ôµ ¾ ´ À ) if no stronger argument ´Ë ¼ Ôµ exists in</p><p>´ À ). ¯If is skeptical then it can accept any proposition Ô previously asserted by À for which there is an acceptable argument ´Ë Ôµ ¾ ´ À ).</p><p>Again we can identify the relationship between the sets of propositions acceptable to different types of agent. <ref type="bibr">PROPOSITION</ref>  If it is presented with an argument for Ô which is both acceptable given what it knows and is stronger than any argument it has for Ô, then both a skeptical and a cautious could accept Ô. However, it is also possible that the argument for Ô could be stronger than any argument has for Ô and also be undercut by a stronger argument which has for some element of its support which itself is not undercut. In this case a cautious could accept</p><p>Ô but a skeptical could not. Finally, it is possible that has an argument for Ô which is stronger than that for Ô and is undercut by a yet stronger argument which is not undercut. Then a skeptical could accept Ô and a cautious could not.</p><p>Clearly skeptical agents are more demanding than credulous ones in terms of the conditions they put on accepting information. Typically, a skeptical agent when presented with an assertion of Ô will challenge Ô to obtain the argument for it, and then validate that this argument is acceptable given what it knows. We can consider even more demanding agents. For example, we can imagine a querulous agent that will only accept a proposition if it can not only validate the acceptability of the argument for that proposition, but also the acceptability of arguments for all the propositions in that argument, and all the propositions in those arguments, and so on. However, it turns out that: PROPOSITION 4.6</p><p>The set of propositions acceptable to a skeptical agent using an argumentation system ´¦µ ÍÒ Ö ÙØ ÈÖ is exactly the same as the set of propositions acceptable to a querulous agent using the same argumentation system. PROOF. Consider a skeptical agent and a querulous agent À with the same argumentation system. By definition, can accept any proposition Ô whose support Ë is either not attacked by any argument that is built from ¦, or else is defended by an argument that is part of the acceptable set of ´¦µ. In other words, will only accept Ô if all the × ¾ Ë are themselves supported by acceptable arguments (which might just be ´ × ×µ if there is no argument for ×). This is exactly the set of conditions under which À will accept Ô.</p><p>In other words, once we require an argument to be acceptable, we also require that any proposition that is part of the support for that argument is also acceptable. Thus the notion of a querulous agent adds nothing to our classification. Since agents will typically both assert and accept propositions during a dialogue, both their assertion attitudes and their acceptance attitudes need to be specified. We write ÔØ ×× ÖØ to denote an agent with acceptance attitude ÔØ and assertion attitude ×× ÖØ .</p><p>With a pair of agents that are skeptical/thoughtful, we recover the rationality conditions of the dialogue system in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dialogue types</head><p>With the agent attitudes specified, we can begin to look at different types of dialogue in detail, giving protocols for each. Note that these are very simple protocols, intentionally so. Indeed they are the simplest protocols we can think of that meet the criteria for the different types of dialogue laid down by Walton and Krabbe. As a result these protocols are very rigid, and more flexible protocols will very likely be required. However, what we aim to do here is to establish a baseline by looking at the properties of these simple protocols before going on to examine the properties of more complex protocols (such as those we have defined in <ref type="bibr" target="#b23">[24]</ref>). An important feature common to all these protocols is that if an agent repeats a locution, then the dialogue terminates. We call protocols with this feature noncircular protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Information seeking</head><p>In an information seeking dialogue, one participant seeks the answer to some question from another participant. If the information seeker is agent and the other agent is (again the name change distinguishes these agents, which play particular roles in a dialogue, from and À , which can take any role), then we can define the protocol Á Ëfor an information seeking dialogue about a proposition Ô as follows:</p><p>1. asks ÕÙ ×Ø ÓÒ´Ôµ. response is given will depend upon the contents of its knowledge base and its assertion attitude. 5 Í indicates that, for whatever reason, cannot give an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">either</head><p>ÔØs 's response, if its acceptance attitude allows, or ÐÐ Ò s. Í cannot be ÐÐ Ò d and as soon as it is asserted, the dialogue terminates without the question being resolved. 4. replies to a ÐÐ Ò with an ×× ÖØ´Ëµ, where Ë is the support of an argument for the last proposition challenged by . 5. Go to 3 for each proposition in Ë in turn. <ref type="foot" target="#foot_9">6</ref>. accepts Ô if its acceptance attitude allows.</p><p>Note that accepts whenever possible, and is only able to challenge when unable to accept-'only' in the sense of only being able to challenge then and ÐÐ Ò being the only locution other than ÔØ that it is allowed to make. then has to give a response if it has one. All of these seem to us to be reasonable conditions. More flexible dialogue protocols are allowed, as in <ref type="bibr" target="#b2">[3]</ref>, but at the cost of possibly running forever. 6  There are a number of interesting properties that we can prove about this protocol, some of which hold whatever acceptance and assertion attitudes the agents have, and some of which are more specific. We have: PROPOSITION 5.1 When subject to ÐÐ Ò ´Ôµ for any Ô it has asserted, a confident, careful, or thoughtful agent can always respond. PROOF. In order to respond to a ÐÐ Ò ´Ôµ, the agent has to be able to produce an argument ´Ë Ôµ. Since, by definition, confident, careful and thoughtful agents only assert propositions for which they have arguments, these arguments can clearly be produced if required. This holds even for the propositions in Ë. For a proposition to be in Ë by Definition 2.1 it must be part of a consistent, minimal subset of ¦ Ë´À µ (where À is the other agent in the dialogue) which entails Ô. Any such proposition Õ is the conclusion of an argument ´ Õ Õµ and this argument is easily generated. This first result ensures that step 4 can always follow from step 3, and the dialogue will not get stuck at that point. It also leads to another result-since with this protocol our agents only put forward propositions that are backed by arguments, a credulous agent would have to accept any proposition asserted by an agent: PROPOSITION 5.2 A credulous agent operating under protocol Á Ëwill always accept a proposition asserted by a confident, careful, or thoughtful agent À . PROOF. When À asserts Ô, will initially challenge it (for Ô to be acceptable it must be backed by an argument, but no argument has been presented by À and if had an argument for Ô it would not have engaged in the information seeking dialogue). By Proposition 5.1, À will always be able to generate such an argument, and by the definition of its acceptance condition and the protocol Á Ë , will then accept it. This result is crucial in showing that if is a credulous agent, then the dialogue will always terminate immediately after 's first assertion, but what if it is more demanding? Well, it turns out that:</p><formula xml:id="formula_9">PROPOSITION 5.3</formula><p>All dialogues carried out using noncircular protocols and the set of moves Å ¼ will terminate.</p><p>PROOF. We have agents with finite knowledge bases, a set of locutions that are instantiated with some subset of the knowledge bases, and protocols that terminate the dialogue if an agent repeats itself. If the dialogue does not end before every possible locution is made, then it will end once the (finite) set of possible locutions have all been made once. This, of course, does not bound the length of the dialogue very tightly. Since agents are allowed to assert sets of propositions, it is conceivable that an agent can make Ç´ ¾ ¦ µ moves before it repeats itself, so a dialogue between and another agent À might take as many as Ç´ ¾ ¦ ¦À µ steps (since as soon as one agent asserts something the other can use it in an argument). We can get much tighter bounds on the length of the dialogue by considering the protocol in detail. <ref type="bibr">PROPOSITION 5.4</ref> An information seeking dialogue under protocol Á Ëbetween a credulous, cautious or skeptical agent and a confident, careful or thoughtful agent À , where À moves first, will always terminate in Ç´ ¦ À µ moves. PROOF. At step 2 of the protocol À either replies with Ô, Ô or Í. If it is Í, the dialogue terminates. then considers Ô. If is credulous, then by Proposition 5.2, will accept the proposition and the dialogue will terminate.</p><p>If is cautious, then at step 3, it will either accept Ô, or have a stronger argument for Ô. In the former case the dialogue terminates immediately. In the latter case will challenge Ô and by Proposition 5.1 receive the support Ë. If doesn't have an argument against any of the × ¾ Ë, then they will be accepted, and this may be enough to make accept Ô in which case the dialogue will terminate. If not, the only locution that could utter is ÐÐ Ò ´Ôµ, and the dialogue terminates. If does have an argument for the negation of any of the × ¾ Ë, then it will challenge them. As in the proof of Proposition 5.1 this will produce an argument</p><p>´ × ×µ from À , and will not be able to accept this any more than it could accept the × initially. will therefore challenge ×, which repeats its previous locution and so the dialogue will terminate.</p><p>If is skeptical, then the process will be very similar. At step 3, will not be able to accept Ô (for the same kind of reason as in the proof of Proposition 5.2), so will challenge it and receive the support Ë. This support may mean that has an acceptable argument for Ô in which case the dialogue terminates. If this argument is not acceptable, then will challenge the × ¾ Ë for which it has an undercutting argument. Again, this will produce an argument</p><p>´ × ×µ from À , which won't make the argument for Ô acceptable. will therefore repeat its last challenge, and the dialogue will terminate. Since the behaviour of À only depends on it having an argument for Ô or Ô, the result holds whether À is confident, careful or thoughtful.</p><p>In the worst possible case, the dialogue will run on until has examined all of the × in turn (and either accepted all of them or accepted all but one, challenged this last, found it unacceptable and then terminated the dialogue unsuccessfully with another challenge), and in the very worst case the Ë in question will be the whole of ¦ À .</p><p>This result gives us much tighter bounds on the number of steps than Proposition 5.3, and, of course, on average the number of steps will be even less since the set of propositions in an argument will usually be much less than the whole knowledge base of an agent. While this result is a good one, because of the guarantee of quick termination, the proof illustrates a limitation of the dialogue protocol and the way that the agents handle utterances in the dialogue. If is skeptical or cautious, it may never come to accept Ô whatever À says. That is, À may not persuade to change its mind even though it has information which undermines 's argument for Ô. The reason for this is that the dialogue protocol neither makes assert into Ë´ µ the grounds for not accepting Ô (thus giving À the opportunity to attack the relevant argument), nor gives À the chance to do anything other than assert arguments which support Ô.</p><p>This position can be justified since Á Ëis intended only to capture information seeking. If we want À to be able to persuade , then the agents should engage in a persuasion dialogue, albeit one that is embedded in an information seeking dialogue as in <ref type="bibr" target="#b24">[25]</ref>. However, since persuasion dialogues suffer from similar problems, we return to this limitation in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inquiry</head><p>In an inquiry dialogue, the participants collaborate to answer some question whose answer is not known to either. There are a number of ways in which one might construct an inquiry dialogue (for example see <ref type="bibr" target="#b23">[24]</ref>). Here we present one simple possibility. We assume that two agents and have already agreed to engage in an inquiry about some proposition Ô by some control dialogue as suggested in <ref type="bibr" target="#b24">[25]</ref>, and from this point can adopt the following protocol Á:</p><p>1. asserts Õ Ô for some Õ or Í. ×× ÖØs Õ, or Ö Õ for some Ö, or Í.</p><p>7. accepts the previous assertion if its acceptance attitude allows, or challenges it. 8. replies to a ÐÐ Ò with an ×× ÖØ´Ëµ, where Ë is the support of an argument for the last proposition challenged by . 9. Goto 7 for each proposition × ¾ Ë in turn.</p><p>10. accepts 's assertion in 6 if its acceptance attitude allows, or the dialogue terminates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">If</head><p>´ Ë´ µ Ë´ µµ includes an argument for Ô that both agents' attitude allows them to accept, then the dialogue terminates.</p><p>12. Go to 1, substituting Ö for Ô and some Ø for Õ.</p><p>Note also that in step 2, when agent makes an assertion Õ Ô then it is bound to not know Õ (so its assertion is almost counterfactual)-if it did know Õ, then the initial conditions for the inquiry would not be met. However, for all subsequent steps in the dialogue, the agent making the assertion Ö Ø might also know Ö. Note also that the protocol could equally well have been written with agent making step 1-the decision to make the first agent to move was arbitrary, unlike the case for the information seeking dialogue.</p><p>This protocol is basically a series of implied Á Ëdialogues. First asks 'do you know of anything which would imply Ô were it known?'. replies with one, or the dialogue terminates with Í. If does not accept the implication as for an information seeking dialogue, the dialogue terminates unsuccessfully. If does accept the implication, asks 'now, do you know Õ, or any Ö which would imply Õ were it known?', and the process repeats until either the process bottoms out in a proposition which both agents agree on and which completes the chain of implications, or there is no new implication to add to the chain. Because of this structure, it is easy to show that: <ref type="bibr">PROPOSITION 5.5</ref> An inquiry dialogue Á between two agents and À with any acceptance and assertion attitudes will always terminate in Ç´ ¦ ¦ À µ steps. PROOF. The dialogue starts with what is effectively an implied Á Ëdialogue and runs exactly as in Proposition 5.4. If it terminates successfully (that is, with a result other than Í or not accepting 's assertion), then it is followed with a second Á Ëdialogue in which the roles of the agents are reversed. Again, this dialogue will run exactly as in Proposition 5.4, possibly ending with a proof that is acceptable to both agents. If this second dialogue does not end with a proof or a Í, then it is followed with another Á Ëdialogue in which the roles of the agents are again reversed. This third dialogue runs just like the first. The iteration will continue until either one of the agents responds with a Í, or the chain of implications is ended. One or other will happen since the agents can only build a finite number of arguments (since arguments have supports which are minimal consistent sets of the finite knowledge base), and agents are not allowed to repeat themselves. When the iteration terminates, so does the dialogue. Now, from Proposition 5.4, we know what the worst case length of each of these iterated dialogues is. Since each can in theory involve Ç´ ¦ µ or Ç´ ¦ À µ steps, it might appear that this dialogue can run for much longer than one under Á Ë , running Ç´ ¦ µ or Ç´ ¦ À µ for each implication in the proof. However each of the propositions in ¦ and ¦ À can only be asserted once at most so, no matter how many sub-dialogues there are, there can be at most</p><formula xml:id="formula_10">Ç´ ¦ ¦ À µ steps.</formula><p>This simple protocol, like that for information seeking, is flawed, and this time we will consider ways to fix the flaws. One problem is that Á may not permit a proof to be found even though one is available to the agents if they were to make a different set of assertions. More precisely, we have: PROPOSITION 5.6 Two agents and À that engage in a inquiry dialogue for Ô, using protocol Á, may find the dialogue terminates unsuccessfully even when ´¦ ¦ À µ provides an argument Ô that both agents would be able to accept.</p><p>PROOF. Assume has ¦ Õ Ô Ö Ô and À has ¦ À Ö . Clearly together both agents can produce ´ Ö Ö Ô Ôµ, and this will be acceptable to both agents no matter their acceptance attitude, but if starts by asserting Õ Ô the agents will never find this proof.</p><p>There is another flaw in the structure of the dialogue. As it stands it assumes that agents can take strict turns in constructing the proof. If an agent cannot fill in a new step in its turn, its only alternative is to to utter Í and bring the dialogue to an end. More formally, this means that there is another kind of case that could prove Proposition 5.6, namely that in which has ¦ Õ Ô and À has ¦ À Ö Õ Ö . In our experience of inquiry dialogues, albeit ones between human agents, it is not unusual for one agent to fill in several steps-indeed it is much more common than for agents to strictly alternate.</p><p>Of course, it is possible to design protocols which don't suffer from these problems, for example by allowing an agent to assert all the Ö Õ relevant at any point in the dialogue (turning the dialogue into a breadth-first search for a proof rather than a depth first one) and by allowing agents to explicitly 'pass' if they cannot add to the proof. For example we have the protocol Á ¼ :</p><p>1. asserts either Ë Õ Ô for all Õ Ô, which its assertion attitude allows it to assert or È . 2. If asserts È , then goto 1 switching agent roles. 3. For each in turn, accepts Õ Ô if its acceptance attitude allows, or challenges it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">replies to a</head><p>ÐÐ Ò with an ×× ÖØ´Ëµ, where Ë is the support of an argument for the last proposition challenged by . 5. Goto 3 for each proposition × ¾ Ë in turn, replacing Õ Ô by ×. 6. accepts any of the challenged Õ Ô if its acceptance attitude allows. 7. If has accepted none of the Õ Ô then the dialogue terminates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>×× ÖØs either some Õ , or Ë Ö Õ for all Ö Õ that its assertion attitude allows it to assert or È . 9. If asserts È , then A asserts È , Õ or Ö Õ for Õ Ô. Goto 3 replacing Õ Ô with whatever A asserted. 10. For each in turn, accepts Ö Õ if its acceptance attitude allows, or challenges it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">replies to a</head><p>ÐÐ Ò with an ×× ÖØ´Ëµ, where Ë is the support of an argument for the last proposition challenged by .</p><p>12. Goto 10 for each proposition × ¾ Ë in turn, replacing Ö Õ by ×.</p><p>13. accepts any of 's assertions in 10 that it challenged if its acceptance attitude allows. 14. If has accepted none of 's assertions in 10, then the dialogue terminates. If ´ Ë´ µ Ë´ µµ includes an argument for Ô that both agents can accept, then the dialogue terminates successfully. <ref type="bibr" target="#b15">16</ref>. Go to 1, substituting Ö for Ô and Ø Ð for Õ . The locution È indicates that the agent 'passes', and if two are uttered in sequence then the dialogue terminates. We can easily show that: PROPOSITION 5.7</p><p>An inquiry dialogue Á ¼ between two agents and À with any acceptance and assertion attitudes will always terminate in Ç´ ¦ ¦ À µ steps.</p><p>PROOF. An inquiry dialogue under Á starts like an information seeking dialogue in which the question has already been asked. An inquiry dialogue under Á ¼ starts like a set of information seeking dialogues all with the same initial question since can reply with a set of answers, and these will run as in Proposition 5.4. If all terminate without accepting any of 's assertions, then the dialogue ends unsuccessfully.</p><p>If there is no È , and one of the sub-dialogues terminates successfully, then it is followed with a second Á Ëdialogue in which the roles of the agents are reversed and is expected to start a new set of sub-dialogues for every implication that it accepted. Again these will run as in Proposition 5.4, and the sub-dialogue will terminate, possibly with a proof that is acceptable to both agents. If this second dialogue does not end with a proof or a È , then it is followed with another set of Á Ëdialogues in which the roles of the agents are again reversed. This third dialogue set runs just like the first. The iteration will continue until one of the agents utters a È (indicating it has nothing else it can legally say), one repeats itself, or the chain of implications is ended. One of these things will happen since the agents can only build a finite number of arguments (since arguments have supports which are minimal consistent sets of the finite knowledge base).</p><p>Any time that a È is uttered, the agent that did not utter it becomes the agent that must assert something. That will either be a È , or a new step in the proof (starting a new cycle of sub-dialogues, all of which will eventually end (as argued above) with either the completion of the proof, a repetition or a È . Thus we will either get two È s in a row, ending the dialogue unsuccessfully, the proof will be completed and we will have a successful termination, or the dialogue will end unsuccessfully with a repetition.</p><p>Thus it is clear that there is nothing in the protocol Á ¼ that significantly increases the number of steps in the worst case with respect to Á. There will be more steps typically, because more assertions of implications will be made, and so there will be more challenges and assertions of grounds. However, the agents still terminate the dialogue if they assert the same proposition twice, and so cannot increase the number of steps above Ç´ ¦ ¦ À µ. Although this protocol will solve both the problems with Á outlined above, it won't ensure that a proof is found if one exists because only the agent which currently 'holds the floor' is allowed to make assertions; these are restricted to those assertions that connect to things that have just been uttered, and an agent will only È if it has nothing to say. Thus a critical step in the proof might be passed over if the agent that knows it is not able to utter it at the right place because the other agent is saying something which, although it connects to the proof tree the agents are jointly constructing, is not on a path that ultimately leads to a proof. One might, of course, further improve the protocol by allowing agents to assert anything that extends the proof tree at any point, but doing this would lead us a bit too far from our aim, which is to look at some simple protocols that allow agents to carry out various kinds of dialogue-we now have two inquiry dialogue protocols and any further dialogue would be rather more complex than we have set out to define here. It is time to move on.</p><p>It is worth noting that, as hinted at above, in contrast to the information seeking dialogue, in any inquiry dialogue the relationship between the agents is symmetrical in the sense that both are asserting and accepting arguments. Thus both an agent's assertion attitude and acceptance attitude come into play. As a result, in the case of a confident but skeptical agent, for instance, it is possible for an agent to assert an argument that it would not find acceptable itself. This might seem odd at first, but on reflection seems more reasonable (consider the kind of inquiry dialogue one might have with a child), not least when one considers that a confident assertion attitude can be seen as one that responds to resource limitations-assert something that seems reasonable and only look to back it up if there is a reason (its unacceptability to another agent) that suggests that it is problematic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Persuasion</head><p>In a persuasion dialogue, one party seeks to persuade another party to adopt a belief or pointof-view he or she does not currently hold. The dialogue game DC, on which the moves in <ref type="bibr" target="#b2">[3]</ref> are based, is fundamentally a persuasion game, so the protocol below results in games that are very like those described in <ref type="bibr" target="#b2">[3]</ref>. This protocol, È, is as follows, where agent is trying to persuade agent to accept Ô.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>×× ÖØs Ô.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>ÔØs Ô if its acceptance attitude allows, if not ×× ÖØs Ô if it is allowed to, or otherwise ÐÐ Ò s Ô.</p><p>3. If asserts Ô, then goto 2 with the roles of the agents reversed and Ô in place of Ô.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">If has</head><p>ÐÐ Ò d, then: (a) asserts Ë, the support for Ô; (b) Goto 2 for each × ¾ Ë in turn. 5. accepts Ô if its acceptance attitude allows, or the dialogue terminates.</p><p>If at any point an agent cannot make the indicated move, it has to concede the dialogue game. An agent also concedes the game if at any point there are no propositions made by the other agent that it hasn't accepted. If concedes, it fails to persuade that Ô is true (and may have been persuaded that Ô is true). If concedes, then has succeeded in persuading it.</p><p>Once again the form of this dialogue has much in common with information seeking dialogues. The dialogue starts as if has asked if Ô is true, and 's response is handled in the same way as in an Á Ëunless has a counter-argument, in which case it can assert it. This assertion is like spinning off a separate Á Ëdialogue in which asks if Ô is true. Since we already have a termination result for Á Ëdialogues, it is simple to show that: PROPOSITION 5.8</p><p>A persuasion dialogue under protocol È between two agents and À will always terminate in Ñ Ü´Ç ´ ¦ µ Ç´ ¦ À µµ steps. PROOF. A dialogue under È is just like an information seeking dialogue under Á Ëin which agents are allowed to reply to the assertion of a proposition Ô with the assertion of Ô as well as the usual responses. Since we know how a dialogue under Á Ëproceeds, it suffices to consider how the assertion of Ô affects things. Since the only difference between the subdialogue spawned by the assertion of Ô and an Á Ëdialogue is the possibility of the agent to which Ô is asserted asserting Ô in response, then this is the only way in which the dialogue can proceed differently. However, this assertion of Ô repeats the assertion that provoked the Ô and so the dialogue terminates immediately. Thus a È dialogue in which moves first will either have À challenging and then end after the examination of the grounds for Ô by À in a maximum of Ç´ ¦ µ steps, or will have À assert Ô and then terminate after has examined the grounds for Ô in a maximum of Ç´ ¦ À µ steps. Thus the dialogue will terminate in at most Ñ Ü´Ç ´ ¦ µ Ç´ ¦ À µµ steps.</p><p>Again there is some symmetry between the agents, but there is also a considerable asymmetry which stems from the fact that is effectively under a burden of proof so it has to win the argument in order to convince , while just has to fail to lose to not be convinced. Thus if and are both confident/cautious and one has an argument for Ô and the other has one for Ô, and neither argument is stronger than the other, despite the fact that the arguments 'draw', will lose the exchange and will not be convinced. This is exactly the same kind of behaviour that is exhibited by many persuasion dialogues in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Limitations of the protocols</head><p>As mentioned above, the protocols we have discussed here are intentionally simple. As a result, while the protocols capture the essential features of the types of dialogue as defined by Walton and Krabbe <ref type="bibr" target="#b44">[45]</ref>, they have a number of limitations.</p><p>The main limitation is the behaviour of the set of steps common to all the dialogues. One agent asserts a proposition Ô, the other either immediately accepts it, or challenges it and is then faced with a new set of propositions Ë. The × ¾ Ë are then individually accepted or challenged, and the dialogue terminates at the end of this exchange with either Ô and Ë accepted or without Ô and at least one × having been accepted (as soon as one × is rejected the dialogue must end unsuccessfully). While, as argued above, this is perfectly reasonable for information seeking dialogues, it means that only a very limited form of persuasion is possible. For to persuade to accept Ô when initially accepts Ô, the support for Ô that asserts must all be facts that are higher in the preference order than any arguments to the contrary that may possess (since the argument inherits its strength from its weakest link). If has any stronger counter-argument, the dialogue ends without being able to engage in a persuasion about the grounds for 's counter-argument. This forced termination seems rather unnatural. Much more natural would be to allow persuasion to continue in the vein sketched above, with agents permitted to engage in this kind of counter-argumentation and even to backtrack to examine propositions that were asserted several moves earlier in the dialogue. This kind of backtracking would extend the flexibility of inquiry dialogues as well, and is one of the things we hope to look at in the future. This is also the place to note that, of course, the kind of persuasion we are dealing with here is one that relies on the supply of new information. It allows us to capture the following kind of dialogue (though to capture exactly this dialogue would require a slightly different protocol):</p><p>A: I believe that Henry Kissinger is a bad man. B: I believe that Henry Kissinger is not a bad man. A: There is evidence that he helped to prolong the Vietnam war and so caused unnecessary deaths among American soldiers and the Vietnamese population in general (to say nothing of the bombing of Cambodia).</p><p>B: What is your evidence for this assertion? A: It is contained in Christopher Hitchens' book 'The Trial of Henry Kissinger' <ref type="bibr" target="#b17">[18]</ref>. B: (after examining the evidence in <ref type="bibr" target="#b17">[18]</ref>). I did not know those things. They outweigh my arguments for Kissinger not being a bad man. I now believe that Henry Kissinger is a bad man.</p><p>However, we cannot handle a similar kind of dialogue in which initially did know these facts, thought that an argument based on Kissinger's role as a statesman was stronger than the argument concerning prolonging the war, but was persuaded to reverse this preference (maybe by an appeal to what would think if one of the aforementioned deaths was their child). Persuasion dialogues of this latter kind, however, could be handled by using the dialogue system defined in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Properties of agent attitudes</head><p>In this section, we consider the result of the dialogues, in particular with respect to the arguments the agents end up accepting and how these relate to the agents' attitudes. The following results hold for all dialogue types, but, as we will see, some are more applicable to different types of dialogue, so it is helpful to have a formal means of distinguishing the different types.</p><p>To do this we need some additional definitions. DEFINITION 6.1</p><p>An agent is said to entertain a proposition Ô if it has an argument ½ for Ô.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 6.2</head><p>An agent is said to believe a proposition Ô if it has an argument ½ for Ô and this is stronger than any argument ¾ for Ô.</p><p>Thus we use the term 'believe' in the sense of 'believed more strongly than the contrary by direct proof'. We also refer to Ô being more strongly believed than Õ if argument ½ for Ô is stronger than any argument for Õ.</p><p>Note that both this idea of belief and the notion of acceptability are not subject to the law of the excluded middle, in the sense that it is quite possible for an agent to have neither an acceptable argument for Ô nor for Ô and to believe neither Ô nor Ô. As we will see below, it is also possible for an agent to believe Ô but not have an acceptable argument for it, and for Ô to be acceptable to an agent which does not believe it. In part we make these distinctions because it is computationally simpler (exactly how much simpler will be discussed in Section 7) for agents to identify entertained propositions than believed propositions, and to identify believed propositions than acceptable ones. Thus there may be computational advantages to building confident agents over thoughtful ones and credulous agents over skeptical ones.</p><p>We also distinguish:</p><formula xml:id="formula_11">DEFINITION 6.3</formula><p>An agent is said to be sure of a proposition Ô if it has an argument ½ for Ô and this is stronger than any undercutting argument ¾ . Properties and Complexity of Some Formal Inter-agent Dialogues 365</p><p>These terms allow us to distinguish the different kinds of dialogue. An information seeking dialogue about Ô opens with entertaining neither Ô nor Ô (and thus not believing or having an acceptable argument for either), while may entertain Ô, Ô, both, or neither (though believes that at least entertains Ô or Ô). An inquiry opens with neither nor entertaining either Ô or Ô. In contrast, a persuasion dialogue opens with one agent having an argument for Ô that accords to its own acceptance criterion, and the other either not having an argument for Ô that accords to its own acceptance criterion, or having an argument for Ô that accords to its own acceptance criterion.</p><p>Clearly these notions are related to each other and also to the notion of acceptability. PROPOSITION 6.4</p><p>Consider an agent with an argumentation system Ë .</p><p>1. Any proposition that is believed by is entertained by . 2. A proposition that is entertained by is not necessarily believed by .</p><p>3. Any proposition that is sure of is also believed by . 4. A proposition that is believed by is not necessarily one that is sure of. 5. A proposition that is acceptable to is not necessarily believed by . 6. A proposition that is believed by is not necessarily acceptable to . 7. Any proposition that is sure of is also acceptable to . 8. A proposition that is acceptable to is not necessarily one that is sure of.</p><p>PROOF. These properties follow almost immediately from the definitions and the fact that an argument can be both believed and undercut:</p><p>1. For an agent to believe a proposition Ô, it has to have a stronger argument for it than Ô.</p><p>The agent therefore has an argument for Ô, and so entertains it. 2. An agent that has an argument for a proposition Ô and a stronger argument for Ô will entertain Ô without believing it. 3. For an agent to be sure of a proposition Ô it has to have a stronger argument for Ô than any undercutting argument. An argument for Ô is an undercutter, so must be weaker than the argument for Ô. Thus Ô is believed. 4. For an agent to believe a proposition Ô, it has to have a stronger argument for it than its negation. However, it can have an undercutter which is stronger than the argument for Ô, and thus the agent is not sure of Ô. 5. For a proposition Ô to be acceptable to an agent, the agent has to have a stronger undercutter for any argument that undercuts the argument for Ô. Thus Ô will be acceptable if there is an argument for Ô, a stronger argument for Ô, and an even stronger argument that undercuts the argument for Ô. In this latter case the agent will not believe Ô. 6. For an agent to believe a proposition Ô, it has to have a stronger argument for Ô than Ô.</p><p>However, the agent can have an undercutter that is stronger than the argument for Ô, and, if this undercutter is itself not undercut by a stronger argument, this will prevent Ô from being acceptable.</p><p>7. For an agent to be sure of a proposition Ô it has to have a stronger argument for Ô than any undercutting argument. This means that the argument defends itself against any undercutter and so is acceptable, making Ô acceptable to the agent. 8. For a proposition Ô to be acceptable to an agent, the agent has to have a stronger undercutter for any argument that undercuts the argument for Ô. Thus Ô will be acceptable if there is an argument for Ô, a stronger argument that undercuts the first, and an even stronger argument that undercuts the second. In this latter case the agent will not be sure of Ô.</p><p>Thus Proposition 6.4 follows.</p><p>Propositions that an agent is sure of are a special case of acceptable propositions-they are ones that only have to be checked for undercutters. There is no need to look for undercutters of the undercutters (and so on) since the original arguments defend themselves. Propositions that an agent is sure of are also a special case of propositions that are believed-in essence they are propositions for which every element of their support is believed. We could therefore define a new 'super-thoughtful/super-skeptical' class of agent that only asserts and accepts propositions it is sure of, and we would find that it asserts and accepts propositions that are in the intersection between those that can be asserted and accepted by, respectively, thoughtful and careful agents and skeptical and cautious agents.</p><p>The classification of propositions can also be related to what agents can assert, a result that ties in with Proposition 4.2: PROPOSITION 6.5 A confident agent can assert any proposition it entertains, believes, is sure of, or that is acceptable to it. A careful agent can assert any proposition that it believes, or that it is sure of. A thoughtful agent can assert any proposition that it is sure of or that is acceptable to it. PROOF. Immediate from the definitions of the types of proposition, the definition of agent attitudes, and Proposition 6.4.</p><p>Underlying information seeking dialogues is the idea that agents have a reasonably benevolent attitude to one another. Partly this is implicit in the fact that is requesting information from -if is unhelpful it need never answer. More importantly, perhaps, is the fact that might be able to mislead . Of course, if grounds its reply in facts that knows to be untrue then may not accept the reply, but obviously this depends upon its acceptance attitude. As it turns out, there are a number of situations in which and can engage in an information seeking dialogue that results in accepting an argument while one or other of them has a stronger argument for the opposite. These dialogues can be considered pathological for this reason and so warrant further study. To investigate such situations we need the following: DEFINITION 6.6</p><p>An agent is said to convince an agent À about the truth of a proposition Ô if asserts Ô and À accepts it.</p><p>We say that one agent misleads another if it manages to convince the second of something the first would not accept itself, and we now look to see under what circumstances one agent can mislead another, easily obtaining some results which clarify the situation. Note that here, as throughout the paper, we assume that an assertion Ô is always immediately followed by an assertion of its support. <ref type="bibr">PROPOSITION 6.7</ref> An agent can convince an agent À of Ô even if À does not believe Ô.</p><p>PROOF. Whatever the assertion type of , it must have an argument ´Ë Ôµ in order to be able to assert Ô. If À is credulous, then by definition it will accept Ô and thus be convinced. If À is cautious, then it will be convinced if it has no argument ´Ë ¼ Ôµ such that ´Ë ¼ Ôµ ÈÖ ´Ë Ôµ. Similarly, if À is skeptical, it will still be convinced of Ô unless it has some argument ´Ë ¼¼ Õµ such that ´Ë ¼¼ Õµ undercuts ´Ë Ôµ. This result, which is very weak in the sense that it shows only the possibility of À being convinced, rather than that À will be convinced, holds no matter what acceptance attitude of À . If À is credulous, then we can get a much stronger result: PROPOSITION 6.8 An agent will always convince a credulous agent À of Ô even if À believes Ô. PROOF. Whatever the assertion type of it must have an argument ´Ë Ôµ in order to be able to assert Ô. If À is credulous, then by definition it will accept Ô and thus be convinced, even if it itself has ´Ë ¼ Ôµ such that ´Ë ¼ Ôµ ÈÖ ´Ë Ôµ.</p><p>Thus, as one might expect, allowing an agent to be credulous means that it is open to exploitation no matter how well informed it is. However, if À is cautious or skeptical, then, as the proof of Proposition 6.7 makes clear, it will only be misled in this way if it has no stronger information to the contrary. In other words: PROPOSITION 6.9 An agent can only convince a cautious agent À of Ô if À does not believe Ô more strongly than , and can only convince a skeptical agent À of Ô if ´¦ µ ´¦À µ contains an acceptable argument for Ô. PROOF. Suppose asserts ´Ë Ôµ. By definition, a cautious À can only accept Ô if it has no argument ´Ë ¼ Ôµ such that ´Ë ¼ Ôµ ÈÖ ´Ë Ôµ. Thus a cautious À can only accept Ô if it does not believe Ô. A skeptical À will only accept Ô if it has an acceptable argument for it, or one can be constructed once has asserted sufficient information to undercut any undercutters À might have for ´Ë Ôµ, which will only happen if ´Ë Ôµ is in the set of acceptable arguments of ´¦ µ ´¦À µ.</p><p>Note that there is a certain asymmetry in this result, which follows from the different kinds of check that the different attitudes require and the fact that a proposition can be believed but not acceptable to a given agent, and can be acceptable but not believed. In fact we can easily see that: PROPOSITION 6.10</p><p>An agent can convince a skeptical agent À of Ô even if À believes Ô, and can convince a cautious agent À of Ô even if ´¦ µ ´¦À µ does not contain an acceptable argument for Ô.</p><p>PROOF. Both of these possibilities follow directly from the acceptance attitudes of the types of agent and the results in Proposition 6.4. Thus, taking the obvious corollary, even agents that try to ensure the quality of information they are given can be misled when they don't have the right information with which to check what they are told, or don't apply this information in the right way (because of their acceptance attitude). This is particularly important in information seeking and inquiry dialogues since, by definition, in such dialogues the agents that initiate the dialogue do not have this information. Indeed, in information seeking dialogues under Á or Á ¼ , Propositions 6.7-6.10</p><p>show that the dialogue may terminate with the agent that initiated the dialogue being convinced of something that it would not assert itself. This, of course, does not amount to misleading as we have defined it, since the definition includes the proviso that the agent that is asserting the proposition in question would not accept it itself, so we have to look a bit further. Consider a dialogue between agents and .</p><p>Without loss of generality, we can consider that is trying to persuade that Ô is true, and so the critical attitudes here are the assertion and acceptance attitudes of and the acceptance attitude of . The three acceptance attitudes and three assertion attitudes would seem to give us a space of 27 combinations of agent attitude to consider. However, not all combinations of acceptance and assertion attitude of are sensible, since some will allow agents to assert things that they would not accept themselves. We define: DEFINITION 6.11 An agent is reliable if it is only able to assert propositions that it would always accept itself.</p><p>Then for an agent to be misleading, it has to not be reliable and be able to convince another agent to accept a proposition that it wouldn't accept itself. We have: PROPOSITION 6.12 An agent that is cautious/confident, cautious/thoughtful, skeptical/careful or skeptical/ confident is not reliable. All other agents are reliable.</p><p>PROOF. A confident agent can assert any proposition Ô for which it has an argument ´Ë Ôµ.</p><p>(It may well have a stronger argument ´Ë ¼ Ôµ as well.) If it is cautious or skeptical it would therefore not accept Ô and so is not reliable. If it is credulous it will accept any argument and thus be reliable.</p><p>If the agent is careful, it will not only have an argument for Ô (which will be acceptable to its credulous self), but it will also believe Ô so the argument will be acceptable to its cautious self. However, there might be a stronger undercutter for Ô (if Ô is not a proposition it is sure of), which might make the agent unable to get its skeptical self to accept Ô.</p><p>If the agent is thoughtful it can only assert Ô if it has an an acceptable argument. This would be accepted by a skeptical agent (by definition) and by a credulous agent since it is backed by an argument. A cautious agent, however, might not accept it since the argument ´Ë Ôµ might be undercut by a stronger argument for Ô (making Ô not believable) but then rehabilitated by a stronger undercutter of this second argument. This completes the proof.</p><p>Obviously a reliable agent cannot be misleading. Thus any agent that is credulous cannot be misleading because it will itself believe whatever it says, and a cautious/careful agent and a skeptical/thoughtful agent will not be misleading because it will perform the right kind of check on what it says (according to its acceptance attitude) before making an assertion. Ignoring such agents for the time being, we are ready to identify the conditions under which misleading will occur. <ref type="bibr">PROPOSITION 6.13</ref> In a dialogue between agents and À , can only mislead À about the proposition Ô if: ¯ is cautious/confident, cautious/thoughtful, skeptical/careful or skeptical/confident and À is credulous or cautious; or ¯ is cautious/confident or cautious/thoughtful and À is skeptical and ´¦ µ ´¦À µ contains an acceptable argument for Ô.</p><p>Properties and Complexity of Some Formal Inter-agent Dialogues 369 PROOF. If À is credulous, it will accept anything it is told, so any set of assertion and acceptance attitudes that render not reliable will allow to state something it would not accept itself and get À to accept it. If À is cautious then by Proposition 6.10 it will accept Ô provided it doesn't have a stronger argument against it. So, it is possible that any which can make assertions it won't accept can mislead À . Finally, if À is skeptical then by Proposition 6.10 it will only accept Ô if ´¦ µ ´¦À µ contains an acceptable argument for Ô. This means that can't be skeptical otherwise it would be able to accept Ô as well (meaning that no misleading was going on). Thus misleading will only happen for skeptical À if is cautious/confident or cautious/thoughtful.</p><p>Note that this result only tells us when misleading might occur, not when it will happen since to predict the latter we would need to look at exactly what was in the knowledge bases of the agents.</p><p>Having identified the types of agent that can mislead, and having thus provided a means of ruling out some kinds of dialogues that give results we might consider unreasonable, we turn to trying to obtain some guarantees about how dialogues might give reasonable results. To do this we will disregard agent types that are not reliable. In addition, we will discard credulous agents since they will always accept any argument that is put forward and are thus easy to mislead. As a result, the only combinations we will consider are agents which are cautious/careful and skeptical/thoughtful, and we will only consider dialogues between agents that are of the same type.</p><p>We can show that these pairs of agents end dialogues under circumstances that seem reasonable, given their attitudes. We have: <ref type="bibr">PROPOSITION 6.14</ref> In a dialogue between two cautious/careful agents and À , will only convince À of Ô if believes Ô more strongly than either agent believes Ô. PROOF. For to convince À of Ô, it has to assert Ô and have it accepted. To assert Ô, since is careful, then from Proposition 6.4 it must believe Ô (being sure of a proposition implies it is believed) and so by Definition 6.2 believe it more strongly than Ô. For À to accept Ô it must not have a stronger argument for Ô than puts forward, and so must not believe Ô more strongly than believes Ô. <ref type="bibr">PROPOSITION 6.15</ref> In a dialogue between two skeptical/thoughtful agents and À , will only convince À of Ô if, after the dialogue, Ô is acceptable to both agents.</p><p>PROOF. The proof is trivial but worth stating because it sheds some light on the way that the dialogues proceed. For to convince À of Ô, it has to assert Ô and have it accepted. To assert Ô, must have an acceptable argument for Ô. For À to accept Ô, either it finds Ô acceptable immediately, or is able to assert arguments that outweigh whatever arguments initially make Ô not acceptable, thus making it acceptable.</p><p>The key difference between these two results is that with cautious/careful agents it is computationally much simpler to determine if one agent will convince another-one just examines the arguments that directly relate to the proposition in question. With skeptical/thoughtful agents the determination is more complex, and to some extent hangs on exactly what the agents say at certain points in the dialogue (a subject investigated in <ref type="bibr" target="#b31">[32]</ref>), which is the reason for the condition 'after the dialogue' in Proposition 6.15.</p><p>These results shows that pairs of cautious/careful agents and skeptical/thoughtful agents have reasonable behaviour. In a dialogue between cautious/careful agents, no agent will be convinced of something unless both it and its opponent believe it. If we consider the relation of undercutting to indicate one argument throwing doubt on another, then by the obvious extension of this result in a dialogue between two skeptical/thoughtful agents either will only be convinced of Ô when neither agent has any reason to throw doubt on Ô.</p><p>Note that this does not ensure that the agents cannot be mistaken (they may just lack the information) and it does not prevent one agent lying to another, provided it has a suitably strong reason to want to tell the lie. The result can also be taken as validating the choice in <ref type="bibr" target="#b2">[3]</ref> to make all agents skeptical/thoughtful, but also suggests that cautious/careful agents warrant further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Complexity results</head><p>Having examined some of the properties of the dialogues, we consider their computational complexity. Since the protocols are based on reasoning in logic we know that the complexity will be high. Our aim in this analysis is to establish exactly where the complexity arises in order that we can reduce it by, for example as we did in <ref type="bibr" target="#b45">[46]</ref>, suitable choice of language.</p><p>We begin with a brief survey of the relevant key concepts from complexity theory (see <ref type="bibr" target="#b25">[26]</ref> for detailed definitions). We start with the complexity classes P (of languages/problems that may be recognized/solved in deterministic polynomial time), and NP (of languages/problems that may be recognized/solved in nondeterministic polynomial time). If and ¼ are complexity classes, then we denote by ¼ the class of languages/problems that are in assuming the availability of an oracle for languages/problems in ¼ [26, pp. 415-417]. Thus, for example, NP NP denotes the class of languages/problems that may be recognized/solved in nondeterministic polynomial time, assuming the presence of an oracle for languages/problems in NP. A language that is complete for NP NP would thus be NP-complete even if we had 'free' answers to NP-complete problems (such as propositional logic satisfiability). We define the polynomial hierarchy with reference to these concepts <ref type="bibr">[26, pp. 423-429]</ref>. First, define</p><formula xml:id="formula_12">¦ Ô ¼ ¥ Ô ¼ P Thus both ¦ Ô</formula><p>¼ and ¥ Ô ¼ denote the classes of languages/problems that may be recognized/solved in deterministic polynomial time. We then inductively define the remaining tiers of the hierarchy, as follows:</p><formula xml:id="formula_13">¦ Ô Ù•½ NP ¦ Ô Ù ¥ Ô Ù•½ co-¦ Ô Ù•½ Thus ¦ Ô</formula><p>½ is simply the class NP, and ¥ Ô ½ is the class co-NP, while ¦ Ô ¾ NP NP and ¥ Ô ¾ co-NP NP .</p><p>To study this issue, we return to Definition 2.1. Given a knowledge base ¦, we will say there is a prima facie argument for a particular conclusion if ¦ , i.e. if it is possible to prove the conclusion from the knowledge base. The existence of a prima facie argument does not imply the existence of a 'usable' argument, however, as ¦ may be inconsistent. Since establishing proof in propositional logic is co-NP-complete, we can immediately conclude: PROPOSITION 7.1 Given a knowledge base ¦ and a conclusion , determining whether there is a prima facie argument for from ¦ is co-NP-complete.</p><p>The next obvious question is as follows: given ´À µ, where À , is it minimal? COROLLARY 7.4 Given a knowledge base ¦ and prima facie argument ´À µ over ¦, the problem of determining whether ´À µ is minimal is ¥ Ô ¾ -complete. PROOF. For membership of ¥ Ô ¾ , consider the following ¦ Ô ¾ algorithm, which decides the complement of the problem:</p><p>1. Existentially select a subset À ¼ of À and a valuation Ú for À ¼ .</p><p>2. Verify that Ú À ¼ .</p><p>3. Universally select each valuation Ú ¼ for À ¼ . 4. Verify that Ú ¼ À ¼ .</p><p>The algorithm contains two alternations, an existential followed by an universal, and so is indeed a ¦ Ô ¾ algorithm. The algorithm works by guessing a subset À ¼ of À , showing that this subset is consistent, and then showing that À ¼ is a tautology, so À ¼ . Since the complement of the problem under consideration is in ¦ Ô ¾ , and co-¦ Ô ¾ ¥ Ô ¾ , it follows that the problem is in ¥ Ô ¾ . To show completeness, we reduce the QBF ¾ to the complement of the problem, i.e. to showing that an argument is not minimal. If an argument ´À µ is not minimal, then there will exist some consistent subset À ¼ of À such that À ¼ . The reduction is identical to that above: we set À Ü ½ ° Ü ½ ° Ü ° Ü ° and set . We then ask whether there is a consistent subset À ¼ of À such that À ¼ . Since we have reduced a ¦ Ô ¾ -complete problem to the complement of the problem under consideration, it follows that the problem is ¥ Ô ¾ -hard.</p><p>These results allow us to handle the complexity of dialogues involving confident, credulous and cautious agents, which are only interested in whether propositions are entertained or believed (which amounts to being interested in whether arguments can be built for given propositions). For thoughtful and skeptical agents we need to consider whether an argument is undercut so that we can determine acceptability. PROOF. The following ¦ Ô ¾ algorithm decides this problem:</p><p>1. Existentially guess (i) a subset À ¼ of ¦; (ii) a support formula ¼ ¾ À to undercut; and (iii) a valuation Ú.</p><formula xml:id="formula_14">2. Verify that Ú À ¼ . 3. Universally select each valuation Ú ¼ of À ¼ . 4. Verify that (i) Ú ¼ À ¼ ¼ and (ii) Ú ¼ ° ¼ .</formula><p>For hardness, there is a straightforward reduction from the QBF ¾ problem, essentially iden- tical to the reductions given in proofs above -we therefore omit it.</p><p>As a corollary, the problem of showing that ´À µ has no undercutter is ¥ Ô ¾ -complete and determining acceptability (which will include showing that at least one argument has no undercutter) is computationally harder than establishing whether propositions are entertained or believed.</p><p>From these results we can see that, as it stands, a direct implementation of argumentationbased dialogues will be computationally very expensive. Indeed it will be intractable. Of course, as mentioned above, this is not surprising. Of more interest is the fact that we can home in on three separate areas which give rise to this intractability. First there is the construction of arguments; then there is the problem of determining if an argument, once constructed, is minimal; finally there is the problem of determining if there are undercutters for a given argument. Considering the proofs of the relevant results, it is clear that the key element as far as generating complexity is concerned is the dependence on establishing proof-it is the co-NP-completeness of establishing proof in propositional logic that raises the complexity so high in the polynomial hierarchy. What we need to do next is to look at using languages that have more efficient mechanisms for establishing proof with our approach (it is possible to establish proof in propositional Horn clauses, for example, in polynomial time) and see how that affects the complexity of argumentation-based dialogue. Of course, more efficient languages are typically less expressive, and future work will concentrate on establishing the tradeoffs in a similar manner to that in <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related work</head><p>In the last few years, the formal study of argumentation became a hot topic in Artificial Intelligence, in particular in the area of multi-agent systems and in nonmonotonic and uncertain reasoning. In nonmonotonic and uncertain reasoning, argumentation systems have been used to define inference systems for existing nonmonotonic logics, as in the work of Geffner <ref type="bibr" target="#b11">[12]</ref>, or to define nonstandard (most often nonmonotonic) consequence relations for a particular logic based on some notion of argument. In this latter line of work, that of Dung <ref type="bibr" target="#b10">[11]</ref> has been particularly influential (not least upon the development of the approach we base our work on <ref type="bibr" target="#b0">[1]</ref>), and has echoes in the work of Prakken and Sartor <ref type="bibr" target="#b34">[35]</ref> and Vreeswijk <ref type="bibr" target="#b43">[44]</ref>. Many more approaches are surveyed in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b6">[7]</ref>.</p><p>On the multi-agent systems side, there has also been a good deal of work on argumentation and we will only discuss the most relevant examples. While the first work that we are aware of in the mainstream agent literature is that of Sycara <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, many of the concepts and problems were studied simultaneously in the field of Artificial Intelligence and Law. Particularly important in showing the scope of systems of argumentation was Gordon's work on The Pleadings Game <ref type="bibr" target="#b12">[13]</ref> and Zeno <ref type="bibr" target="#b13">[14]</ref> and subsequent refinements of these ideas proposed by Loui <ref type="bibr" target="#b20">[21]</ref> and Prakken <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. There has also been much relevant work in the area of natural language processing, though, as argued before, this deals with a much more complex task than one in which languages and agents can be engineered to carry out simple dialogues, and therefore a good deal of the work in natural language dialogues is not directly relevant to our work. However, the work of Grosz <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> deserves a mention to indicate the ancestry of work in natural language on argumentation, and the fact that it has covered topics such as shared plans which our work has not. Reed's work <ref type="bibr" target="#b36">[37]</ref> is also important for having brought the work of Walton and Krabbe to the attention of the agents community, and also for having provided a framework for combining different types of dialogue (later extended by <ref type="bibr" target="#b24">[25]</ref>).</p><p>We distinguish the work in this paper from that of others in the literature first of all by its scope. Much of the existing work has dealt with a particular kind of dialogue, a form of deliberation in the case of Dignum and colleagues <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> (though deliberation with strong overtones of persuasion), persuasion in the work of Prakken <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, Gordon <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and Loui <ref type="bibr" target="#b20">[21]</ref>, and negotiation (though again with strong overtones of persuasion and also delib-eration) in the work of Schroeder <ref type="bibr" target="#b39">[40]</ref> and some of our previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. In spirit our work is close to that of Sadri et al. <ref type="bibr" target="#b38">[39]</ref> who have looked at the termination of negotiation dialogues (again having overtones of deliberation in the terminology of Walton and Krabbe) and the effect of agent types <ref type="bibr" target="#b37">[38]</ref> and Kakas and Toni <ref type="bibr" target="#b19">[20]</ref> who have looked at the complexity of some types of argumentation. We also see it as a continuation of our previous work on the complexity of negotiation dialogues <ref type="bibr" target="#b45">[46]</ref>.</p><p>However, the main way in which our work differs from that which has come previously is that, so far as we are aware, we are the first to have tried to identify a common framework in which a number of different kinds of dialogue can be expressed, the first to specify protocols for a range of dialogue types, and the first to study the properties of these protocols in such detail. The protocols we have studied here are very simple, but this was a conscious choice which was made in order to simplify the tasks at hand (not least as a result of trying to obtain similar results for the marginally more complex dialogues of <ref type="bibr" target="#b4">[5]</ref>), and one that we believe to be justified by the fact that we have, for instance, exposed some interesting behaviour in these dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>This paper has examined three types of argumentation-based dialogue between agentsinformation seeking, inquiry and persuasion, from the typology of <ref type="bibr" target="#b44">[45]</ref>-defining a precise protocol for each and examining some important properties of that protocol. In particular we have shown that each protocol leads to dialogues that are guaranteed to terminate in a reasonable number of steps, and we have considered some aspects of the complexity of these dialogues. The exact form of the dialogues depends on what messages agents send and how they respond to messages they receive. This aspect of the dialogue is not specified by the protocol, but by some decision-making apparatus in the agent. Here we have considered this decision to be determined by the agents' attitude, and we have shown how this attitude affects their behaviour in the dialogues they engage in.</p><p>Both of these aspects extend previous work in this field. In particular, they extend the work of <ref type="bibr" target="#b2">[3]</ref> by precisely defining a set of protocols (albeit quite rigid ones) and a range of agent attitudes (in <ref type="bibr" target="#b2">[3]</ref> only one protocol, for persuasion, and only one attitude, broadly thoughtful/skeptical, were considered).</p><p>More work, of course, remains to be done in this area. Particularly important is determining the relationship between the locutions we use in these dialogues and those of agent communication languages such as the FIPA ACL-some initial results on this are presented in <ref type="bibr" target="#b3">[4]</ref>-examining the effect of adding new locutions (such as Ö ØÖ Ø) to the language, and identifying additional properties of the dialogues (such as the the extent to which the protocols defined here place restrictions on the outcomes of dialogues given what agents have in their knowledge bases). We are currently investigating these matters along with further dialogue types-negotiation and deliberation in the typology of <ref type="bibr" target="#b44">[45]</ref> and planning dialogues <ref type="bibr" target="#b14">[15]</ref>-as well as more complex kinds of the dialogue types studied here, and additional complexity issues (including the use of languages other than propositional logic). Preliminary thoughts on the outcome-related properties may be found in <ref type="bibr" target="#b31">[32]</ref>, and an analysis of a simple form of deliberation dialogue is given in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Another point that we are beginning to work on is the question of how the knowledge base of an agent evolves through both an individual dialogue (are there ways that an agent should increase its knowledge in addition to accepting propositions asserted by another agent?) and across a number of dialogues (what kind of belief revision is appropriate at the end of a dialogue?).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>DEFINITION 4. 1</head><label>1</label><figDesc>An agent may have one of three assertion attitudes. If agent is engaged in a dialogue with agent À , then:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>at</head><label></label><figDesc>McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from Properties and Complexity of Some Formal Inter-agent Dialogues 361 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>at</head><label></label><figDesc>McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>PROPOSITION 7. 5</head><label>5</label><figDesc>Given a knowledge base ¦ and an argument ´À µ over ¦, the problem of showing that ´À µ has an undercutter is ¦ Ô ¾ -complete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Consider an agent such that ´Ë Ôµ ¾ ´ À ). By definition, if is credulous, it can accept any Õ for which it is presented with an argument ´Ë ¼ Õµ. It can therefore accept Ô provided it is given ´Ë ¼ Ôµ even if ´Ë Ôµ ÈÖ ´Ë ¼ Ôµ. If were cautious, then it would not be able to accept Ô unless ´Ë ¼ Ôµ ÈÖ ´Ë Ôµ. Thus a cautious agent can accept only a subset of the arguments that a credulous agent can accept. If is skeptical, it might not accept Ô even if ´Ë ¼ Ôµ ÈÖ ´Ë Ôµ, because another argument ´Ë ¼¼ Öµ might exist which undercuts ´Ë ¼ Ôµ and makes ´Ë ¼ Ôµ unacceptable. Thus a skeptical agent can only accept a subset of the arguments that a credulous agent can accept, and the first part of the result is proved.</figDesc><table><row><cell>4.5</cell><cell></cell></row><row><cell>Consider an agent . If</cell><cell>is skeptical or cautious, then the assertions it can accept are a</cell></row><row><cell cols="2">subset of those it could accept were it credulous. If is skeptical, then the set of assertions</cell></row><row><cell cols="2">it can accept overlaps with the set of assertions it could accept were it cautious.</cell></row><row><cell>PROOF.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2. accepts Õ Ô if its acceptance attitude allows, or challenges it.Properties and Complexity of Some Formal Inter-agent Dialogues 359 4. Goto 2 for each proposition × ¾ Ë in turn, replacing Õ Ô by ×.5. accepts Õ Ô if its acceptance attitude allows, or the dialogue terminates.</figDesc><table><row><cell>6.</cell><cell></cell></row><row><cell></cell><cell>Downloaded from</cell></row><row><cell></cell><cell>http://logcom.oxfordjournals.org/</cell></row><row><cell></cell><cell>at McGill University Libraries on March 11, 2015</cell></row><row><cell>3. replies to a</cell><cell>ÐÐ Ò with an ×× ÖØ´Ëµ, where Ë is the support of an argument for</cell></row><row><cell cols="2">the last proposition challenged by .</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Logic Computat., Vol. 13 No. 3, c Oxford University Press 2003; all rights reserved at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note that this definition of negotiation is that of Walton and Krabbe. Arguably negotiation dialogues may involve other issues besides the division of scarce resources.at McGill University Libraries onMarch 11,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2015" xml:id="foot_2"><p>http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Here we deal only with beliefs, though the approach can also handle desires and intentions[6,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>30] and could be extended to cope with other mental attitudes. at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_6"><p>The names are from the study of persuasion dialogues-È argues 'pro' some proposition, and argues 'con'.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_7"><p>Which, of course, is the same as ´¦È Ë´Èµ Ë´ µµ ÍÒ Ö ÙØ ÈÖ . at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_8"><p>It might even be able to assert both Ô and Ô, in which case it chooses one and asserts that.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_9"><p>The protocol in<ref type="bibr" target="#b2">[3]</ref> allows an agent to interject with ÕÙ ×Ø ÓÒ´Ôµ for any Ô at virtually any point, allowing two agents to prolong a dialogue indefinitely by issuing endless ÕÙ ×Ø ÓÒs about arbitrary formulae. at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>Downloaded from</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p>Received 15 June 2002 at McGill University Libraries on March 11, 2015 http://logcom.oxfordjournals.org/ Downloaded from</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledegments</head><p>This work was partly funded by the EU funded Project IST-1999-10948, the EPSRC-funded project GR/R27518, and the NSF funded project REC-02-19347. We are extremely grateful for the detailed comments provided by the anonymous reviewers. These helped us to greatly improve the paper. This is a revised and expanded version of <ref type="bibr" target="#b30">[31]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties and Complexity of Some Formal Inter-agent Dialogues 371</head><p>We will say a pair ´À µ is a consistent prima facie argument over ¦ if À is a consistent subset of ¦ and À . Determining whether or not there is a consistent prima facie argument for some conclusion is immediately seen to be harder. PROPOSITION 7.2 Given a knowledge base ¦ and conclusion , determining whether there is a consistent prima facie argument for over ¦ is ¦ Ô ¾ -complete.</p><p>PROOF. The following ¦ Ô ¾ algorithm decides the problem:</p><p>1. Existentially guess a subset À of ¦ together with a valuation Ú for À . 2. Verify that Ú À . 3. Universally select each valuation Ú ¼ of À , and verify that Ú ¼ À . The algorithm has two alternations, the first being an existential, the second a universal, and so it is indeed a ¦ Ô ¾ algorithm. The existential alternation involves guessing a support for together with a witness to the consistency of this support. The universal alternation verifies that À is valid, and so À . Thus the problem is in ¦ Ô ¾ .</p><p>To show the problem is ¦ Ô ¾ -hard, we do a reduction from the QBF ¾ problem [19, p. 96]. An instance of QBF ¾ is given by a quantified boolean formula with the following structure:</p><p>where is a propositional logic formula over Boolean variables Ü ½ Ü Ý ½ Ý Ð . Such a formula is true if there are values we can give to Ü ½ Ü , such that for all values we can give to Ý ½ Ý Ð , the formula is true. Here is an example of such a formula:</p><p>Formula (7.2) in fact evaluates to true. (If Ü ½ is true, then for all values of Ü ¾ , the overall formula is true.)</p><p>Given an instance (7.1) of QBF ¾ , we define the conclusion to be , and then define the knowledge base ¦ as</p><p>where and are logical constants for truth and falsehood respectively. Any consistent subset of ¦ defines a consistent partial valuation for the body of (7.1); variables not given a valuation by a subset are assumed to be 'don't care'. We claim that input formula (7.1) is true iff there exists a consistent prima facie argument for given knowledge base ¦. Intuitively, in considering subsets of ¦, we are actually examining all values that may be assigned to the existentially quantified variables Ü ½ Ü . Since the reduction is clearly polynomial time, we are done. Now, knowing that there exists a consistent prima facie argument for conclusion over ¦ implies the existence of a minimal argument for over ¦ (although it does not tell us what this minimal argument is). We can thus conclude: COROLLARY 7.3 Given a knowledge base ¦ and conclusion , determining whether there is an argument for (i.e. a minimal consistent prima facie argument for -Definition 2.1) over ¦ is ¦ Ô ¾ - complete.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contribution a l&apos;integration des préferences dans le raisonnement argumentatif</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<pubPlace>Toulouse</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université Paul Sabatier</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the acceptability of arguments in preference-based argumentation framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cayrol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modelling dialogues using argumentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Multi-Agent Systems, E. Durfee</title>
		<meeting>the Fourth International Conference on Multi-Agent Systems, E. Durfee<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An argumentation-based semantics for agent communication languages</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Artificial Intelligence</title>
		<meeting>the Fifteenth European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Agent dialogues with conflicting preferences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Agent Theories, Architectures and Languages</title>
		<editor>
			<persName><forename type="first">J.-J</forename><surname>Meyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Tambe</surname></persName>
		</editor>
		<meeting>the Eighth International Workshop on Agent Theories, Architectures and Languages</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arguments, dialogue, and negotiation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth European Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Horn</surname></persName>
		</editor>
		<meeting>the Fourteenth European Conference on Artificial Intelligence<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argument-based applications to knowledge engineering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Carbogim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="119" to="149" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Agent theory for team formation by dialogue</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dignum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dunin-Ke ¸plicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Workshop on Agent Theories, Architectures, and Languages</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Castelfranchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lespérance</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="141" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating collective intention through dialogue</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dignum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dunin-Ke ¸plicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Formal and Applied Practical Reasoning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cunningham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Gabbay</surname></persName>
		</editor>
		<meeting>the International Conference on Formal and Applied Practical Reasoning<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
		<respStmt>
			<orgName>Department of Computing, Imperial College, University of London</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Background to qualitative decision theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Thomason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and Ò-person games</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Dung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional entailment: Bridging two approaches to default reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="209" to="244" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Pleadings Game: An exercise in computational dialectics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="239" to="292" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Zeno argumentation framework</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karacapilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on AI and Law</title>
		<meeting>the Sixth International Conference on AI and Law</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The evolution of SharedPlans</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Rational Agency</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wooldridge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</editor>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention, intentions, and the structure of discourse</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Hamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fallacies. Methuen and Co Ltd</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Trial of Henry Kissinger</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hitchens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Verso</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A catalog of complexity classes. In Handbook of Theoretical Computer Science Volume A: Algorithms and Complexity</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. van Leeuwen</title>
		<imprint>
			<biblScope unit="page" from="67" to="161" />
			<date type="published" when="1990">1990</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computing argumentation in logic programming</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="113" to="131" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Process and policy: Resource-bounded non-demonstrative reasoning</title>
		<meeting>ess and policy: Resource-bounded non-demonstrative reasoning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Question-begging in non-cumulative systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Philosophical Logic</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="117" to="133" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A generic framework for dialogue game implementation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Maudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Evrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Formal Semantics and Pragmatics of Dialogue</title>
		<meeting>the Second Workshop on Formal Semantics and Pragmatics of Dialogue<address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Risk agoras: Dialectical argumentation for scientific reasoning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcburney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</editor>
		<meeting>the Sixteenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Stanford, CA, UAI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Games that agents play: A formal framework for dialogues between autonomous agents</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcburney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic, Language, and Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="315" to="334" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An approach to using degrees of belief in BDI agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giorgini</surname></persName>
		</author>
		<editor>Information, Uncertainty, Fusion. B. Bouchon-Meunier, R. R. Yager, and L. A. Zadeh</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Negotiation through argumentation -a preliminary report</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second International Conference on Multi-Agent Systems</title>
		<meeting>Second International Conference on Multi-Agent Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Argumentation-based dialogues for agent co-ordination. Group Decision and Negotiation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcburney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Agents that reason and negotiate by arguing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An analysis of formal interagent dialogues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Castelfranchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</editor>
		<meeting>the First International Joint Conference on Autonomous Agents and Multi-Agent Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="394" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the outcomes of formal inter-agent dialogues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Joint Conference on Autonomous Agents and Multi-Agent Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Yakoo</surname></persName>
		</editor>
		<meeting>the Second International Joint Conference on Autonomous Agents and Multi-Agent Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On dialogue systems with speech acts, arguments, and counterarguments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh European Workshop on Logic in Artificial Intelligence</title>
		<meeting>the Seventh European Workshop on Logic in Artificial Intelligence<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relating protocols for dynamic dispute with logics for defeasible argumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="187" to="219" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modelling reasoning with precedents in a formal dialogue game. Artificial Intelligence and Law</title>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sartor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="231" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Logics for defeasible argumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vreeswijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Philosophical Logic</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gabbay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Guenthner</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dialogue frames in agent communications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Multi-Agent Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Demazeau</surname></persName>
		</editor>
		<meeting>the Third International Conference on Multi-Agent Systems</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dialogues for negotiation: Agent varieties and dialogue sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Agent Theories, Architectures and Languages, ATAL</title>
		<meeting>the International Workshop on Agent Theories, Architectures and Languages, ATAL</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Logic agents, dialogues and negotiation: an abductive approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Information Agents for E-Commerce</title>
		<editor>
			<persName><forename type="first">York</forename><surname>Aisb</surname></persName>
		</editor>
		<meeting>the Symposium on Information Agents for E-Commerce</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ultima ratio: should Hamlet kill Claudius</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Plewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Autonomous Agents</title>
		<meeting>the Second International Conference on Autonomous Agents</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="467" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A social semantics for agent communication languages</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI&apos;99 Workshop on Agent communication languages</title>
		<meeting>the IJCAI&apos;99 Workshop on Agent communication languages</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="75" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Argumentation: Planning other agents&apos; plans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Joint Conference on Artificial Intelligence</title>
		<meeting>the Eleventh Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="517" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Persuasive argumentation in negotiation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Decision</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="242" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Abstract argumentation systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vreeswijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="225" to="279" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C W</forename><surname>Krabbe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>State University of New York Press</publisher>
			<pubPlace>Albany, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Languages for negotiation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth European Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Horn</surname></persName>
		</editor>
		<meeting>the Fourteenth European Conference on Artificial Intelligence<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="393" to="397" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
