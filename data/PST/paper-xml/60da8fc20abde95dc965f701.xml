<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
							<email>chengxu@pku.edu.cnm</email>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Improving the Government&apos;s Governance Capability Big Data Application Technology 4 Alibaba Group 5 Tencent</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
							<email>yichi.zyc@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous research has demonstrated the power of leveraging prior knowledge to improve the performance of deep models in natural language processing. However, traditional methods neglect the fact that redundant and irrelevant knowledge exists in external knowledge bases. In this study, we launched an in-depth empirical investigation into downstream tasks and found that knowledgeenhanced approaches do not always exhibit satisfactory improvements. To this end, we investigate the fundamental reasons for ineffective knowledge infusion and present selective injection for language pretraining, which constitutes a modelagnostic method and is readily pluggable into previous approaches. Experimental results on benchmark datasets demonstrate that our approach can enhance state-of-the-art knowledge injection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised pre-trained language models (LMs) such as BERT, which can learn powerful contextualized representations, have achieved state-of-the-art results in natural language processing (NLP) tasks. However, open issues remain as these approaches lack domain-specific knowledge. Recent methods <ref type="bibr" target="#b6">[Peters et al., 2019]</ref> have revealed that the performance of the knowledge-driven downstream task (for example, question answering or relation extraction) is dependent on structured relational knowledge; thus, the direct finetuning of pre-trained LMs yields suboptimal results.</p><p>To address this issue, several works have attempted to integrate knowledge graphs (KGs) into pre-trained LMs <ref type="bibr" target="#b12">[Zhang et al., 2019;</ref><ref type="bibr" target="#b3">Levine et al., 2020;</ref><ref type="bibr" target="#b6">Peters et al., 2019;</ref><ref type="bibr" target="#b10">Xiong et al., 2020;</ref><ref type="bibr" target="#b13">Zhang et al., 2021a]</ref>, which has shed light on promising directions for knowledge-driven tasks. Such methods generally retrieve pre-trained graph embeddings <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref> or a KG subgraph via entity linking during pretraining and finetuning. Representations learned from knowledge-enhanced approaches have demonstrated expressive power and contributed to the performance improvement of downstream tasks. Thus, knowledge infusion has been widely adopted, as is a simple yet effective method that exploits external knowledge. Moreover, when sufficient training data are not available, the infusion of external knowledge into a pre-trained LM followed by finetuning to target tasks is more efficient <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref>.</p><p>To a certain extent, knowledge infusion integrates the knowledge which is insufficient into pre-trained representations and alleviates data requirements of the tasks. However, the adequate amount of external knowledge for effective infusion remains to be well understood. In recent years, <ref type="bibr" target="#b6">[Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Broscheit, 2020]</ref> found that pre-trained LMs were partially equipped with a specific type of relational knowledge. Furthermore, <ref type="bibr" target="#b5">[Liu et al., 2020]</ref> observed that the incorporation of excessive knowledge might divert the context representation from its correct meaning. These observations motivated us to study the effective infusion of knowledge into pre-trained LMs. We note that previous approaches treated all external knowledge equally, thereby inevitably leading to redundant or irrelevant knowledge infusion. We argue that knowledge is NOT always beneficial for downstream tasks, and an indiscriminate injection of knowledge may lead to negative knowledge infusion, which is detrimental to the performance of downstream tasks.</p><p>In this paper, we take the first step towards studying this phenomenon fundamentally and propose general approaches to restraining detrimental knowledge during knowledge infusion.</p><p>Firstly, we investigate the efficacy of infused knowledge and observe that external knowledge (for example, entities) with high frequencies in the pre-trained corpus are more likely to trigger negative knowledge infusion. We argue that pre-trained LMs have already captured such external knowledge, and the redundant knowledge retrieved from KGs could possibly amplify the negative effects of the external noise, which subsequently deteriorates the performance. Inspired by this observation, we propose a selective injection mechanism that infuses informative knowledge by considering both the knowledge frequency and mutual reachability detected in the text for effective knowledge injection.</p><p>Secondly, we investigate those irrelevant parts of knowledge, which lead to the negative knowledge infusion regarding small spectral components. In particular, we conduct spectral analysis from the perspective of parameters, and feature representations based on singular value decomposition (SVD) <ref type="bibr">[Golub and Reinsch, 2007]</ref> and make two observations: (1) small spectral components of weight parameters in high layers are not beneficial, and (2) when finetuning with sufficient training data, the small spectral singulars of the feature representations tend to decay autonomously. Inspired by these empirical observations, we leverage spectral regularization to suppress those small spectral components corresponding to irrelevant knowledge deliberately for effective knowledge exploitation. It should be noted that our approach is model-agnostic, and therefore orthogonal to existing approaches. We conduct numerous experiments on NLP benchmarks, which demonstrate the effectiveness in mitigating negative knowledge infusion. The contributions of this study can be summarized as follows:</p><p>• We investigate the problem of knowledge infusion into pre-trained LMs and observe that redundant and irrelevant knowledge exist for downstream tasks, which may lead to negative knowledge infusion. • We then propose selective injection as well as spectral regularization respectively for effective knowledge infusion and our method is orthogonal to existing knowledge-driven tasks. • Extensive experimental results on NLP benchmarks demonstrate the effectiveness of our method in alleviating negative knowledge infusion and our approach can enhance state-of-the-art knowledge injection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Background knowledge has been considered as an indispensable part of language understanding <ref type="bibr" target="#b13">[Zhang et al., 2021b]</ref>, which has inspired knowledge-enhanced models including ERNIE (Tsinghua)<ref type="foot" target="#foot_1">1</ref>  <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref>, ERNIE (Baidu) <ref type="bibr">[Sun et al., 2019]</ref>, KnowBERT <ref type="bibr" target="#b6">[Peters et al., 2019]</ref>, WKLM <ref type="bibr" target="#b10">[Xiong et al., 2020]</ref>, <ref type="bibr">LUKE [Yamada et al., 2020]</ref>, <ref type="bibr">KEPLER [Wang et al., 2019b]</ref>, GLM <ref type="bibr" target="#b7">[Shen et al., 2020]</ref>, K-Adaptor <ref type="bibr" target="#b9">[Wang et al., 2020], and</ref><ref type="bibr">CoLAKE [Sun et al., 2020]</ref>. ERNIE <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref> injects relational knowledge into the pretrained model BERT, which aligns entities from Wikipedia to facts in WikiData. KnowBERT <ref type="bibr" target="#b6">[Peters et al., 2019]</ref> incorporates external KGs into BERT with a novel attention and recontextualization approach. More recent methods, such as the GLM <ref type="bibr" target="#b7">[Shen et al., 2020]</ref>, and K-Adapter <ref type="bibr" target="#b9">[Wang et al., 2020]</ref>,</p><p>introduce promising techniques to exploit informative knowledge and mitigate catastrophic forgetting during knowledge infusion. However, the dilemma of negative knowledge infusion is still not well understood. Our work is motivated by approaches <ref type="bibr" target="#b5">[Liu et al., 2020;</ref><ref type="bibr" target="#b6">Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Broscheit, 2020]</ref> that have indicated the existence of redundant and irrelevant knowledge. <ref type="bibr">Liu et al. [2020]</ref> observes that excessive knowledge incorporation could divert the context representation and <ref type="bibr" target="#b0">Bian et al. [2021]</ref> finds that context-sensitive knowledge selection is critical, whereas <ref type="bibr" target="#b6">[Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Broscheit, 2020]</ref> demonstrates that pre-trained LMs had been partially equipped with relational knowledge. Negative knowledge infusion, which is a largely ignored issue in recent knowledge-driven tasks, has rarely been considered. Moreover, our work is inspired by negative transfer in transfer learning <ref type="bibr" target="#b2">[Chen et al., 2019a]</ref> as they both follow a pretrain-finetune paradigm. However, as opposed to these approaches, we focus on knowledge infusion, including injecting favorable knowledge and exploiting beneficial representations. In contrast, transfer learning uses the knowledge acquired for one task to solve related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge-Enhanced Models</head><p>Regarding a knowledge-enhanced language model, when finetuning, it generally consists of two parts: a feature extractor F and a task-specific architecture C. We denote F 0 and C 0 as the pre-trained weights. We study the negative knowledge infusion, which is a phenomenon whereby the model infuses knowledge but does not achieve satisfactory improvement or even suffers from performance decay. It is natural to pose the following questions: 1) Does negative knowledge infusion really exist in downstream tasks? 2) If it does, how does it affect the model performance? In this section, we investigate whether negative knowledge infusion exists and whether it has a negative impact on task performance. We design two experiments based on ERNIE <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref> and KnowBERT <ref type="bibr" target="#b6">[Peters et al., 2019]</ref> for evaluation 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Negative Knowledge Infusion</head><p>In the first experiment, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(a), we sampled test samples and grouped them into buckets with different-frequency entities where the frequency refers to the occurrence number of entities in Wikipedia. Contrary to the common assumption, knowledge-enhanced approaches such as ERNIE and KnowBERT does not always exhibit satisfactory improvement to vanilla BERT and may even achieve slightly inferior performance with high-frequency entities for some instances, which indicates that not all knowledge is beneficial and demonstrates the existence of negative knowledge infusion. The pretrained language model may have already learned factual knowledge for high-frequent entities, which constitute redundant knowledge.</p><p>In the second experiment, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(b), we experiment to identify the influence of irrelevant knowledge by replacing the entity with other entities of different types.</p><p>From Figure <ref type="figure" target="#fig_0">1</ref>(b), we observe that irrelevant knowledge hurts the performance more severely as the noise rates increase. Note that there exist incorrect facts or wrong linked entities 3 which constitute irrelevant knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Negative Knowledge Infusion?</head><p>As negative knowledge infusion exists, we can ask another two questions: 1) Which part of knowledge infusion causes negative knowledge infusion? 2) How can this problem be mitigated?</p><p>From the perspective of knowledge, we begin to explore which part of the external knowledge may contribute to this problem. It can be observed from Figure <ref type="figure" target="#fig_0">1</ref>(a) that there is no guarantee that the performance will always exhibit an improvement for samples with high-frequency entities. We argue that redundant information may not contribute to the performance and irrelevant knowledge may hinder the performance. Firstly, it should be noted that recent approaches <ref type="bibr" target="#b6">[Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Broscheit, 2020]</ref> have demonstrated that pretraining can obtain relational knowledge. Since the pretrained LM has already captured such knowledge and several noisy facts exist in the external knowledge base, it is unreasonable to infuse this redundant external knowledge, resulting in noise and reducing the semantics in the text. Secondly, excessive knowledge may also lead to catastrophic forgetting, as observed by <ref type="bibr" target="#b9">[Wang et al., 2020]</ref>.</p><p>From the perspective of features and parameters, we explore which part of the weight W and features f = F (x) may not be beneficial. Figure <ref type="figure" target="#fig_0">1</ref>(b) already illustrates that noises introduced either by incorrect facts or from erroneous entity linking may cause negative knowledge infusion. To further investigate the negative impact of irrelevant knowledge for downstream tasks, we analyze both the weights and features with principal angles <ref type="bibr" target="#b6">[Rebuffi et al., 2017]</ref>, which have been introduced to measure the similarity of subspaces. Specifically, we use the corresponding angles [Chen et al., 2019b],</p><p>2 Negative knowledge infusion can also be found in  which are defined as follows:</p><formula xml:id="formula_0">cos (θ i ) = u 1,i , u 2,i u 1,i u 2,i<label>(1)</label></formula><p>where u 1,i refers to the i-th eigenvector with the i-th largest singular value and u 2,i denotes the opposite case. We apply θ to measure the availability of the eigenvectors in the weight matrices. Naturally, if the eigenvectors of the corresponding angle are small, the prior knowledge is more beneficial. Specifically, we denote W 0 and W as the pre-trained weights of knowledge-enhanced models such as ERNIE and the finetuned weights on downstream tasks, respectively. We reshape the tensor into a matrix and subsequently perform SVD to obtain the eigenvectors U and singular values Σ, denoted as follows: We calculate the relative angles θ in the 4-th layer (solid lines) and 12-th layer (dotted lines) between W 0 and W, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. We observe that the lower layers (4th layer) have small relative angles, which is consistent with the finding in <ref type="bibr" target="#b6">[Rogers et al., 2020]</ref>. It is natural that lower layers are more beneficial for different tasks. Nevertheless, we note that relatively large singular values have rather small corresponding angles. Thus, it is intuitive to align weights indiscriminately with the initial pre-trained values to remedy the negative knowledge infusion. Moreover, we analyze the feature representations with different training set sizes. Similarly, we use SVD to calculate all singular eigenvectors U and values Σ of the feature matrices, denoted as follows:</p><formula xml:id="formula_1">W = UΣV<label>(</label></formula><formula xml:id="formula_2">F = UΣV<label>(3)</label></formula><p>As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, we draw the diagonal elements of the singular value matrix Σ in descending order to measure the importance of the eigenvectors. As demonstrated in [Chen et al., 2019a], finetuning and training from scratch can achieve comparable results with sufficient labeled data. It is natural to assume that finetuning with large datasets should provide greater generalization. Motivated by the observation of significantly suppressed, relatively small singular values of the features, we argue that promoting the similarity between these parts will give rise to negative knowledge infusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In this section, we preliminarily study how to alleviate negative knowledge infusion, as depicted in Figure <ref type="figure" target="#fig_3">4</ref>. As the above analysis demonstrates that redundant knowledge is not necessary for infusion, it is intuitive to assign different sampling weights to different entities, thereby injecting different ratios of external knowledge. Moreover, as both weights and features with large singular values are valuable for downstream tasks, it is logical to shrink the importance of the lower spectral components with smaller scores, particularly with limited supervision. Note that the computation of SVD in high-dimensional weight spaces is costly; hence we mainly apply our approach on the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dropping Redundant Knowledge with Selective Injection</head><p>The above redundant knowledge analysis of feature matrices results in the key inspiration. We propose a selective injection approach. Specifically, we randomly sample 85% of the injected entities as candidate knowledge and then introduce selective injection to infuse the necessary portions. Note that, as most frequently appearing entities are trivial and redundant, it is natural to assign lower sample probabilities to them. However, although several entities have a relatively high frequency, they cannot be neglected owing to their semantic importance. For example, one sampled entity should be assigned with a high probability if it can be inferred by numerous other entities in the same text (within K hop -hops).</p><p>To this end, we propose the selective injection approach regarding the following sampling equation<ref type="foot" target="#foot_2">4</ref> :</p><formula xml:id="formula_3">P (E ej ) ∝ I {DF(ej )&lt;Kthren } + λ [|S (e j )|] Kmax Kmin ,<label>(4)</label></formula><p>where DF(•) refers to the document frequency, E is a set of linked entities from text, S(e) {e|∀e s.t. distance (e , e) &lt; K hop ∧ e ∈ E}, | • | refers to the set size which denotes the number of neighbouring entities with distance shorter than K hop , [x] b a max(a, min(x, b)), and distance (e, e ) is the shortest undirected length between the two entities. Note that the neglected knowledge in the selective injection still has 15% possibility to be infused into the LMs. Our approach can be used as a knowledge-sampling function for different knowledge-enhanced approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shrinking Irrelevant Knowledge with Spectral Regularization</head><p>Motivated by the above spectral analysis of the features, we propose a spectral regularization approach to remedy the irrelevant knowledge obtained during finetuning. In particular, we conduct SVD on the feature matrix F following Equation and penalize the smallest k singular values, as indicated below:</p><formula xml:id="formula_4">L sr (F ) = η k i=1 σ 2 i , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where k is the number of singular values to be penalized, η is a hyperparameter, and σ i refers to the i-th smallest singular value.</p><p>Computational Complexity. The computational complexity of the selective injection can be ignored because it can be pre-computed prior to training. For a a × b matrix, the time complexity of the SVD is O min a 2 b, ab 2 , which is unacceptable. We calculate the spectral regularization with O b 2 d , where b is the batch size and d is the feature dimension (for example, 768). This is negligible in recent knowledge-enhanced approaches. Our approach can be embedded into existing fine-tuning scenarios, which can be formulated as:</p><formula xml:id="formula_6">min W n i=1 L (C (F (x i )) , y i ) + γΩ(W) + ηL sr (F ). (6)</formula><p>where L is the task loss, Ω is the L2 regularization, C is the task-specific function, L sr is our spectral regularization, γ and η are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Setup</head><p>TACRED <ref type="bibr" target="#b11">[Zhang et al., 2017]</ref>  In this case, η was set to 0.001, k was set to 1, λ was set to 0.5, γ was set to 0.0001, K hop/thresh/min/max was set to {6,100,5,20}, and the batch size was set to 32. Note that our approach can also leverage other kind of external knowledge such as ConceptNet which is different from the world knowledge database Wikidata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We leverage Wikidata as an external knowledge base for both ERNIE and KnowBERT. We pretrain our own ERNIE and KnowBERT initialized with RoBERTa <ref type="bibr" target="#b4">[Liu et al., 2019]</ref>. We compare our approach with baselines as shown below: <ref type="bibr">BERT [Devlin et al., 2018]</ref>. We utilize the BERT-base as the pre-trained language model baseline. RoBERTa <ref type="bibr" target="#b4">[Liu et al., 2019]</ref>. We utilize the RoBERTa-base as baseline. <ref type="bibr">KEPLER [Wang et al., 2019b]</ref>. It is a unified model for knowledge embedding and pre-trained language representation.</p><p>WKLM <ref type="bibr" target="#b10">[Xiong et al., 2020]</ref>. It is a weakly supervised pretraining approach that explicitly forces the model to incorporate knowledge about real-world entities. <ref type="bibr" target="#b9">[Wang et al., 2020]</ref>. It is an adaptor-based approach that fixed the pre-trained language model's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-Adaptor</head><p>ERNIE* <ref type="bibr" target="#b12">[Zhang et al., 2019]</ref>. Here the model ERNIE* refers to the results obtained from the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ERNIE.</head><p>Here, the model ERNIE refers to our implementation results, which has the same amount of tuning with ERNIE+SI+SR.</p><p>KnowBERT* <ref type="bibr" target="#b6">[Peters et al., 2019]</ref>. Here the model Know-BERT* refers to the results obtained from the paper.</p><p>KnowBERT. Here, the model KnowBERT refers to the results of our implementation, which has the same amount of tuning with KnowBERT+SI+SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Analysis</head><p>Main Results. From Table <ref type="table" target="#tab_1">1</ref>, we can observe the following: 1) ERNIE and KnowBERT embedded with our approach achieved improvement in all experiments and even performed better than RoBERTa in FIGER and TACRED, indicating the advantages of infusing informative knowledge and shrinking irrelevant features; 2) In SearchQA and Qusar-T, the improvement of our approach are relatively small, which could be owing to an insufficient quantity of available external knowledge, and thus fewer performance gains; 3) Both selective injection and spectral regularization contribute to the model performance, and selective injection obtains improvements in OpenEntity and TACRED, indicating the benefits of dropping redundant and irrelevant knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLUE Results.</head><p>From Table <ref type="table" target="#tab_0">2</ref>, we can observe the following: 1) ERNIE embedded with our approach achieved improvement in all experiments and obtained comparable results with RoBERTa-base on GLUE, further indicating the efficacy of our approach; 2) Our approach does not obtain much performance gains compared with RoBERTa-base. Note that those tasks are not knowledge-driven <ref type="bibr" target="#b2">[Devlin et al., 2018]</ref> which requires linguistic representations rather than structure facts; thus, knowledge-enhanced models such as ERNIE hurt the performance as it introduces noises, whereas our approach does not detour performances as it performs selective knowledge injection. It is advantageous for those indistinguishable situations whether knowledge is necessary or not (alleviating negative knowledge infusion).</p><p>Selective Injection. To evaluate the effectiveness of the selective injection, we conducted ablation studies. It can be observed from Figure <ref type="figure" target="#fig_5">5</ref>(a) that 1) the samples with highfrequency entities (redundant knowledge) exhibited a severe performance decay, which further demonstrates the negative impact of redundant knowledge; and 2) our approach with selective injection achieved more stable performance, suggesting that our mechanism of de-emphasizing the redundant knowledge was beneficial. Train sample ratio Spectral Regularization. The singular values of the features are drawn with (dotted) and without (solid) spectral regularization in Figure <ref type="figure" target="#fig_5">5</ref>(b). We observed that 1) the singular values shrank, demonstrating the effectiveness of our approach; and 2) although k = 1 (k is the number of singular values to be penalized), more than one singular value was surprisingly suppressed, which shows the capability of the automatic distribution adjustment. We also conducted a sensitivity analysis of different k values using Equation <ref type="formula" target="#formula_4">5</ref>. It can be observed from Figure <ref type="figure" target="#fig_5">5</ref>(c) that 1) the performance of the limited training data with a larger k value was slightly su-perior; and 2) the performance of the sufficient training data decayed with a relatively large k, indicating the necessity of a trade-off between penalization and knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Noise Tolerance. To further evaluate our approach's noise tolerance, we deliberately replaced entities with other entities of different types to simulate noisy facts in the knowledge base. We experimented with different ratios of noise in knowledge. According to 5(d), 1) all approaches exhibited a performance decay, indicating the negative effect of the irrelevant knowledge; and 2) our method significantly outperformed all of the baselines, suggesting that our approach was more robust and could remedy the noisy effect resulting in negative knowledge infusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have studied the knowledge infusion of knowledge-driven tasks and took the first step towards delving into knowledge infusion scenarios from a new perspective: negative knowledge infusion. Whereas recent approaches have generally focused on designing sophisticated architectures to infuse knowledge, the essential mechanism of knowledge infusion remains less understood. We empirically observed two main findings, namely that redundant and irrelevant knowledge will lead to negative infusion, which may shed light on future works on knowledge-enhanced approaches. We proposed selective injection and spectral regularization to inhibit negative components, which can be embedded into existing methods demonstrated performance gains. We anticipate further research on promising directions, including 1) exploiting more efficient approaches to identify the useful knowledge; 2) investigating the essence of knowledge-driven tasks and proposing more effective infusion across domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Analysis of negative knowledge infusion. (a) F1 of subtest set with different-frequency entities; (b) influence of irrelevant knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cosine values of corresponding angles between W and W 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Singular values of feature matrices with different ratio of finetuning instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>BobFigure 4 :</head><label>4</label><figDesc>Figure 4: Knowledge infusion with Selective Injection (SI) and Spectral Regularization (SR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is a large-scale relation extraction dataset that covers 42 relation types and contains 106,264 sentences. OpenEntity [Choi et al., 2018] is a completely manually annotated entity typing dataset. SearchQA [Dunn et al., 2017] is a large-scale question answering dataset that is constructed to reflect a full pipeline of general question answering. Quasa-T [Dhingra et al., 2017] is a large-scale questionanswering dataset consisting of 43,000 open-domain trivia questions and their answers that are obtained from various internet sources. GLUE [Wang et al., 2019a] is a benchmark with nine diverse NLP tasks. As WNLI mainly focuses on reasoning, we do not perform experiments on WNLI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Analysis of selective infusion, spectral regularization, hyper-parameter sensitivity, and noise tolerance: (a) analysis of selective injection; (b) all singular values of feature matrices; (c) sensitivity analysis of different k; (d) analysis with different ratios of incorrect knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>TagMe's performance on various benchmark datasets ranges from 0.37 to 0.72 F1 score[Kolitsas et al., 2019]    </figDesc><table><row><cell></cell><cell></cell><cell cols="4">Cosine values of corresponding angles</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>layer4.BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>layer4.ERNIE</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell cols="2">layer4.KnowBERT</cell></row><row><cell>Corresponding angles</cell><cell>0.4 0.5 0.6 0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell cols="2">layer12.BERT layer12.ERNIE layer12.KnowBERT</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Index</cell><cell></cell></row></table><note>3 </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on OpenEntity, TACRED, SearchQA, and Quasar-T datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">OpenEntity</cell><cell></cell><cell></cell><cell cols="2">TACRED</cell><cell>SearchQA</cell><cell>Quasar-T</cell></row><row><cell></cell><cell></cell><cell>P</cell><cell cols="4">Ma-F1 Mi-F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell cols="2">BERT-base [Devlin et al., 2018]</cell><cell>76.37</cell><cell cols="2">70.96</cell><cell cols="5">73.56 67.23 64.81 66.00 57.10 61.90 40.40 46.10</cell></row><row><cell>ERNIE* [Zhang et al., 2019]</cell><cell></cell><cell>78.42</cell><cell cols="2">72.90</cell><cell cols="5">75.56 69.97 66.08 67.97</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">KnowBERT* [Peters et al., 2019] 78.60</cell><cell cols="2">73.70</cell><cell cols="5">76.10 71.60 71.40 71.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">KEPLER [Wang et al., 2019b]</cell><cell>77.20</cell><cell cols="2">74.20</cell><cell cols="5">75.70 70.43 73.02 71.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WKLM [Xiong et al., 2020]</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.70 66.70 45.80 52.20</cell></row><row><cell>RoBERTa [Liu et al., 2019]</cell><cell></cell><cell>77.55</cell><cell cols="2">74.95</cell><cell cols="5">76.23 70.17 72.36 71.25 59.01 65.62 40.83 48.84</cell></row><row><cell cols="2">K-Adapter [Wang et al., 2020]</cell><cell>79.25</cell><cell cols="2">75.00</cell><cell cols="5">77.06 70.05 73.92 71.93 61.96 67.31 45.69 52.84</cell></row><row><cell>ERNIE</cell><cell></cell><cell>78.52</cell><cell cols="2">72.92</cell><cell cols="5">75.62 70.92 69.28 70.09 59.53 65.92 44.35 51.15</cell></row><row><cell>ERNIE+SI</cell><cell></cell><cell>78.81</cell><cell cols="2">74.70</cell><cell cols="5">76.70 71.25 74.03 72.61 61.56 67.01 45.59 52.58</cell></row><row><cell>ERNIE+SI+SR</cell><cell></cell><cell>78.91</cell><cell cols="2">74.80</cell><cell cols="5">76.80 71.05 74.33 72.65 61.64 67.31 45.79 52.98</cell></row><row><cell>KnowBERT</cell><cell></cell><cell>78.63</cell><cell cols="2">73.80</cell><cell cols="5">76.14 71.50 71.50 71.50 60.93 65.92 44.45 50.95</cell></row><row><cell>KnowBERT+SI</cell><cell></cell><cell>78.61</cell><cell cols="2">74.73</cell><cell cols="5">76.62 71.15 73.73 72.42 62.66 67.32 45.70 52.88</cell></row><row><cell>KnowBERT+SI+SR</cell><cell></cell><cell>78.93</cell><cell cols="2">75.56</cell><cell cols="5">77.21 71.35 74.49 72.89 62.86 67.52 45.73 53.10</cell></row><row><cell>Model</cell><cell cols="2">MNLI</cell><cell cols="7">QQP QNLI SST-2 CoLA STS-B MRPC RTE AVG.</cell></row><row><cell>RoBERTa</cell><cell cols="3">87.5/87.3 91.9</cell><cell cols="2">92.8</cell><cell>94.8</cell><cell>63.6</cell><cell cols="2">91.2</cell><cell>90.2</cell><cell>78.7 86.4</cell></row><row><cell>ERNIE</cell><cell cols="3">87.0/86.3 91.3</cell><cell cols="2">92.2</cell><cell>94.4</cell><cell>62.1</cell><cell cols="2">89.1</cell><cell>89.1</cell><cell>69.5 84.5</cell></row><row><cell>ERNIE+SI</cell><cell cols="3">87.1/87.0 91.5</cell><cell cols="2">92.0</cell><cell>94.4</cell><cell>62.2</cell><cell cols="2">90.0</cell><cell>89.3</cell><cell>75.6 85.4</cell></row><row><cell cols="4">ERNIE+SI+SR 87.4/87.1 92.0</cell><cell cols="2">92.3</cell><cell>94.6</cell><cell>63.3</cell><cell cols="2">90.6</cell><cell>90.5</cell><cell>76.5 86.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on different tasks of GLUE dev set.</figDesc><table><row><cell>F1</cell><cell>70 72 74 76 78</cell><cell>Knowledge frequency analysis BERT ERNIE ERNIE+SI KnowBERT KnowBERT+SI</cell><cell>Singular values</cell><cell>8.0 7.5 6.0 6.5 7.0</cell><cell></cell><cell>Singular values</cell><cell cols="2">w/o SR: 15% w/o SR: 30% w/o SR: 50% w/o SR: 100% SR: 15% SR: 30% SR: 50% SR: 100%</cell><cell></cell></row><row><cell></cell><cell>68</cell><cell></cell><cell></cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>66</cell><cell></cell><cell></cell><cell>5.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">high Test bucket with different frequent entities middle low</cell><cell></cell><cell>10</cell><cell>12</cell><cell>14 Index 16</cell><cell>18</cell><cell>20</cell><cell>15%</cell><cell>30%</cell><cell>50%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell>(a) redundancy</cell><cell></cell><cell></cell><cell></cell><cell>(b) σ</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">In this paper, ERNIE refers to the ERNIE (Tsinghua).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">λ and K hop/thresh/min/max are hyperparameters.Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to express gratitude to the anonymous reviewers for their hard work and kind comments. This work is funded by NSFCU19B2027/91846204, National Key R&amp;D Program of China (Funding No.SQ2018YFC000004).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Benchmarking knowledge-enhanced commonsense question answering via knowledge-to-text transformation</title>
		<author>
			<persName><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Investigating entity knowledge in bert with simple neural end-to-end entity linking</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><surname>Broscheit</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<idno>arXiv:1704.05179</idno>
	</analytic>
	<monogr>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
				<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1906">2019a. 1906-1916, 2019. 2019b. 2019. 2018. 2018. 2018. 2018. 2017. 2017. 2017. 2017. 2007. 2007. 2019</date>
			<biblScope unit="page" from="160" to="180" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Milestones in Matrix Computation. Kolitsas et al., 2019] Nikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas Hofmann. End-to-end neural entity linking. In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sensebert: Driving some sense into bert</title>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<meeting><address><addrLine>Anna Rogers, Olga Kovaleva</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2019. 2019. 2017. 2020. 2020</date>
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
	<note>EMNLP. and Anna Rumshisky. A primer in bertology: What we know about how bert works. In TACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting structured knowledge in text via graph-guided representation learning</title>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding</title>
				<imprint>
			<date type="published" when="2019">2020. 2020. 2019. 2019. 2020</date>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019a. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
				<imprint>
			<date type="published" when="2019">2019b. 2019. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>K-adapter: Infusing knowledge into pre-trained models with adapters</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Luke: deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021a. 2021. 2021b. 2021</date>
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
