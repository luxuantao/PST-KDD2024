<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Read Top News First: A Document Reordering Approach for Multi-Document News Summarization</title>
				<funder ref="#_gUAj34V">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-19">19 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhao</surname></persName>
							<email>zhaochao@cs.unc</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
							<email>tenghao@cs.unc</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Somnath</forename><surname>Basu</surname></persName>
							<email>somnath@cs.unc</email>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muthu</forename><forename type="middle">Kumar</forename><surname>Chandrasekaran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
							<email>snigdha@cs.unc</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Read Top News First: A Document Reordering Approach for Multi-Document News Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-19">19 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.10254v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common method for extractive multidocument news summarization is to reformulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-document news extractive summarization (MDS) aims to extract the salient information from multiple related news documents into a concise summary. Some approaches use task-specific architectures for this problem. For example, <ref type="bibr" target="#b18">Wang et al. (2020)</ref> organize multiple documents as a heterogeneous graph before summarizing them. <ref type="bibr" target="#b21">Zhong et al. (2020)</ref> formulate the extractive summarization task as a semantic matching problem. Recent works also explored reformulating this problem as a single-document summarization (SDS) problem by concatenating all documents into a single meta-document and then using an SDS model to summarize it <ref type="bibr" target="#b1">(Cao et al., 2017;</ref><ref type="bibr">Liu et al., 2018;</ref><ref type="bibr" target="#b13">Lebanoff et al., 2018;</ref><ref type="bibr" target="#b4">Fabbri et al., 2019)</ref>.</p><p>Due to the conventions of news writing <ref type="bibr" target="#b7">(Hong and Nenkova, 2014;</ref><ref type="bibr" target="#b6">Hicks et al., 2016)</ref>, salient information often appears at the beginning of a news article. As a result, many summarization systems, including recent neural models <ref type="bibr" target="#b10">(Kedzie et al., 2018;</ref><ref type="bibr" target="#b22">Zhong et al., 2019)</ref>, pay more attention to the beginning of the document. Therefore, in MDS, it is important to consider the order in which the docu- * Equal Contribution ments are concatenated to form the meta-document before applying the summarization model.</p><p>Specifically, we argue that the various documents in the input are not equally important. Some documents contain more salient or detailed information and are more important. Therefore, compared with concatenating documents in an arbitrary order, it would be beneficial to reorder the documents such that the important ones are in the front of the meta-document and it becomes easier for the summarization model to learn the salient content.</p><p>Motivated by these factors, we propose a simple yet effective approach to reorder the input documents according to their relative importance before applying a summarization model. We evaluate the effectiveness of our approach on Multi-News <ref type="bibr" target="#b4">(Fabbri et al., 2019)</ref> and DUC-2004. 1 Results show that our simple reordering approach significantly outperforms the state-of-the-art methods with more complex model architectures. We also observe that this approach brings more performance gain with the increase in the number of input documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We refer to D as a meta-document of m documents {d 1 , . . . , d m } with n sentences {s 1 , ..., s n } in total. The goal in extractive summarization is to extract a subset of sentences in D to summarize the input documents. It is usually formulated as a binary sentence classification problem, where each sentence is assigned a {0, 1} label to determine if it is to be included in the summary.</p><p>Below, we introduce our document reordering approach, and then the base summarization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Reordering</head><p>Document reordering aims to rearrange documents of the meta-document in order of their salience. It can be formulated as determining the relative im-portance score of each document and then reordering the documents according to their importance scores. Here we propose a supervised approach and an unsupervised approach for this task. Supervised Approach. In this approach, we use a BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> based model to learn document importance scores. For this, we first concatenate the documents together while inserting a [CLS] and a [SEP] token at the start and the end of each document. We then encode the concatenated documents using BERT to get the document representation t i ? R K , which is the representation of the [CLS] token preceding it. To enhance the model's ability to capture the inter-document relationships, we use a 2-layer Transformer to encode t i and finally obtain a document's contextualized</p><formula xml:id="formula_0">representation h i ? R K . t 1 , . . . , t m = BERT(d 1 , . . . , d m ) h 1 , . . . , h m = Transformer(t 1 , . . . , t m ) (1)</formula><p>Thereafter, in order to predict the importance score for the i-th document, ?i , we apply a linear transformation with a Softmax function.</p><formula xml:id="formula_1">?i = softmax (W h i + b) ,<label>(2)</label></formula><p>where W ? R K?K and b ? R K are parameters.</p><p>During training, we determine the oracle importance score of each document d i as the normalized ROUGE-1 F score<ref type="foot" target="#foot_1">2</ref> between d i and the gold abstractive summary S:</p><formula xml:id="formula_2">y i = ROUGE(d i , S) i ROUGE(d i , S)</formula><p>.</p><p>(3)</p><p>Our learning objective is to minimize the Kullback-Leibler divergence between the predicted distribution ? = {? 1 , . . . , ?m } and the oracle distribution y = {y 1 , . . . , y m } of importance scores.</p><formula xml:id="formula_3">L = KL(?, y)<label>(4)</label></formula><p>We train the document reordering model on the training set based on this learning objective.</p><p>During inference, we obtain the importance score of documents in the validation set and test set based on Eq. 2, and then reorder documents in descending order of their importance scores to create the meta-document.</p><p>Unsupervised Approach. We hypothesize that the importance of a document is related to its centrality. To test this hypothesis, we propose an unsupervised centrality-based document reordering approach. To compute the centrality of a document d i , we first represent the topic of the input cluster, T i , by concatenating the top-3 sentences of each document except d i , and then calculate the centrality as ROUGE(d i , T i ). We choose top-3 sentences to represent the topic as it is a strong unsupervised summarization baseline. We avoid sentences of d i to be included in T i to prevent the centrality of d i being dominated by its own sentences, leading to similar centrality scores for all documents.</p><p>Finally, we reorder the documents in descending order of their centrality scores and then concatenate them into a meta-document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Base Extractive Summarization Model</head><p>Once the documents have been reordered and concatenated to form a meta-document, they are fed to a base summarization model. For the supervised reordering approach, we use PreSumm <ref type="bibr" target="#b15">(Liu and Lapata, 2019)</ref>, a state-of-the-art SDS method. For training, the extractive oracle labels are obtained by incrementally adding sentences to the extracted summary until the ROUGE score between the extracted summary and the gold abstractive summary does not increase. Using an SDS-based model architecture also facilitates transferring knowledge from SDS datasets. For this, we first finetune the model on SDS datasets and then finetune it on our MDS dataset. For the unsupervised reordering approach, we use PacSum <ref type="bibr" target="#b20">(Zheng and Lapata, 2019)</ref>, a BERT-based model to measure the centrality of each sentence in the meta-document and then select sentences accordingly. We refer to Appendix A for details of both approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate our document reordering based summarization approach.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We conduct experiments on two MDS datasets: Multi-News and DUC-2004. For evaluation, we compare the extracted summary to the gold abstractive summary. Due to the small size of DUC-2004, we use it only for out-of-domain evaluation.</p><p>We also use CNN DailyMail (CNNDM) <ref type="bibr" target="#b17">(Nallapati et al., 2016)</ref>, a single-document news summarization dataset, to pretrain the base summarization model. More details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setup</head><p>We use BERT BASE as the encoder of both the document reordering model and base summarization model. We experiment with training the summarization model from scratch and also initializing it with parameters learned by training on CNNDM. The details can be found in Appendix B. During inference, we choose the top-K sentences with the highest score to compose the final summary, where K is selected based on the average length of summaries in the training set. We set K = 9 and 7 for Multi-News and DUC-2004, respectively.</p><p>We compare our approach with the following baselines: Lead-N , TextRank <ref type="bibr" target="#b16">(Mihalcea and Tarau, 2004)</ref>, LexRank <ref type="bibr" target="#b3">(Erkan and Radev, 2004)</ref>, HiBERT <ref type="bibr" target="#b19">(Zhang et al., 2019</ref><ref type="bibr">), MGSum-ext (Jin et al., 2020)</ref>, HDSG <ref type="bibr" target="#b18">(Wang et al., 2020)</ref>, and MatchSum <ref type="bibr" target="#b21">(Zhong et al., 2020)</ref>. Lead-N concatenates the top-N sentence of each document. We try N = {1, 2, 3} and report the best performance. Following these approaches, we evaluate the extractive summaries using ROUGE F 1 score. <ref type="foot" target="#foot_3">4</ref>We evaluate the document reordering model by comparing the predicted document order with the oracle order via Kendall's Tau (? ) and Perfect Match Ratio (PMR), two common metrics for ranking tasks <ref type="bibr" target="#b0">(Basu Roy Chowdhury et al., 2021)</ref>. We compare our approach with a random baseline and a length-based baseline that rearranges documents in decreasing order of their lengths.<ref type="foot" target="#foot_4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Automatic Evaluation Table <ref type="table" target="#tab_0">1</ref> shows results on Multi-News using either supervised or unsupervised document reordering approach.</p><p>We first investigate the utility of transferring knowledge from SDS. For this, we compare the PreSumm models trained from scratch (PreSumm w/o CNNDM) or initialized using CNNDM (Pre-Summ w/ CNNDM). Results show that PreSumm w/ CNNDM performs better than PreSumm w/o CNNDM (46.25 vs. 46.05 on ROUGE-1), indicating that the knowledge from SDS can be transferred MODEL R1 R2 RL Lead <ref type="bibr" target="#b4">(Fabbri et al., 2019)</ref> 43.08 14.27 38.97 LexRank <ref type="bibr" target="#b3">(Erkan and Radev, 2004)</ref> 41.77 13.81 37.87 TextRank <ref type="bibr">(Mihalcea and Tarau, 2004) 41.95 13.86 38.07</ref> HiBERT <ref type="bibr" target="#b19">(Zhang et al., 2019)</ref> 43.86 14.62 -MGSum-ext <ref type="bibr" target="#b9">(Jin et al., 2020)</ref> 44.75 15.75 -HDSG <ref type="bibr" target="#b18">(Wang et al., 2020)</ref> 46.05 16.35 42.08 MatchSum <ref type="bibr" target="#b21">(Zhong et al., 2020)</ref> 46 to MDS by continual training. We then test the performance of our supervised document reordering (DR sup ) approach. Using document reordering, our approach, PreSumm + DR sup , significantly outperforms the vanilla PreSumm on all ROUGE scores with or without <ref type="bibr">CNNDM (46.57 vs. 46.25,</ref><ref type="bibr">46.34 vs. 46</ref>.05 on ROUGE-1). Our unsupervised document reordering approach (DR unsup ) significantly outperforms PacSum on all ROUGE scores. These improvements demonstrate that document reordering is an effective way to leverage existing strong models for summarization. Our best approach, PreSumm + DR sup , also significantly outperforms all of the baselines on all ROUGE scores. The performance gain is not entirely from the CNNDM, since our approach without CNNDM also achieves substantial improvements compared with all baselines. Similarly, the unsupervised approach, PacSum + DR unsup , also outperforms the unsupervised baselines. These improvements demonstrate that document reordering helps in multi-document summarization. Human Evaluation We also conduct a human evaluation to better assess the performance of each system. We randomly select 100 test instances and evaluate the quality of a summary according to Informativeness, Conciseness, and Usefulness as in <ref type="bibr" target="#b8">Iskender et al. (2021)</ref>. We conduct a pairwise comparison of PreSumm+DR sup (the best model)  with PreSumm and MatchSum, two strongest neural baselines, as well as LEAD, the best unsupervised baseline. For each test instance, we obtain the output summary from our model and one of the baselines, and then ask three workers on Amazon Mechanical Turk to compare the two summaries according to the three measures listed above. More details can be found in Appendix C.</p><p>The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Negative scores indicate worse performance compared with PreSumm+DR sup . The results show that our approach can generate more informative, concise, and useful summaries compared to baselines, which is consistent with the automatic results. Out-of-domain Evaluation We further evaluate the performance of our approach in an out-ofdomain setting. We compare our best approach with Lead-1, TextRank, MatchSum, and PreSumm. All models except Lead-1 and TextRank were trained on Multi-news and evaluated on the DUC 2004 dataset via Rouge F 1 scores. As shown in Table 3, our approach (last row of the table) achieves consistently better performance than the baselines, indicating that our approach can effectively transfer to new unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document-wise Analysis</head><p>In this section, we first compare our two document reordering approaches using ranking measures (? and PMR) and ROUGE scores of the extracted summaries. Table <ref type="table" target="#tab_3">4</ref> shows the results. Our supervised ranking method (DR sup ) outperforms the unsupervised method (DR unsup ), demonstrating that the number of input documents. We don't include instances with 6 or more documents since the number of such instances is small. Our approach results in more performance gain for longer inputs. the oracle importance score of the document is an effective supervision signal for document reordering. DR unsup achieves higher scores than baselines. It supports our hypothesis that the importance of documents is related to their centrality to the topic.</p><p>We further analyze the impact of instance length (number of documents in the instance) on the model performance. In Figure <ref type="figure" target="#fig_0">1</ref>, we group the test instances of Multi-News based on their lengths, and show the gain in summarization performance obtained from supervised reordering (measured using the ROUGE-1 difference ?R between the models with and without document reordering). The figure shows that in general, ?R increases as the instance length increases, indicating that instances with more documents benefit more from our reordering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary-wise Analysis</head><p>The underlying assumption behind our document reordering approach is that extractive summarization models tend to select sentences from the beginning of the document. By reordering the important documents to the front of the meta-document, our approach makes the salient content easier to learn. In this section, we investigate if this is indeed what is happening by analyzing the distribution of the oracle and the generated summary sentences in the meta-document. We conduct three experiments.</p><p>Experiment 1: We first investigate how re- ordering is changing the placement of important sentences. We represent important sentences as those in the oracle summaries, which is obtained by following the procedure described in Section 2.2. Figure <ref type="figure" target="#fig_1">2(a)</ref> shows the distribution of oracle summary-sentences at various positions of the input meta-document when it is reordered (purple shaded bars) and when it is not reordered (blue solid bars). The x-axis shows the sentence positions in the input meta-document and the y-axis shows the fraction of sentences from the oracle summary that were at that position in the metadocument. Comparing the purple and blue bars in the left area, more oracle summary's sentences were located at the beginning of the reordered input meta-document compared with the unordered input meta-document. This indicates that reordering helps in placing the important sentences in the beginning of the input meta-document.</p><p>Experiment 2: We next investigate if the summarization model favors certain sentence positions. Figure <ref type="figure" target="#fig_1">2(b)</ref> shows the distribution of (generated) summary-sentences with respect to various positions of the input meta-document for Pre-Summ+DR (w/ reordering, purple shaded bars) and PreSumm (w/o reordering, blue solid bars). Like Figure <ref type="figure" target="#fig_1">2</ref>(a), the x-axis shows the sentence positions in the input meta-document, but the y-axis shows the fraction of sentences from the generated summary that are at that position in the meta-document. The bars on the left are, in general, higher than the bars on the right. This indicates that PreSumm tends to pick sentences appearing at the beginning of the input meta-document to create summaries.</p><p>Experiment 3: Finally, we want to investigate if the reordering can help the model select salient content that was originally scattered across the input. Figure <ref type="figure" target="#fig_1">2(c)</ref> shows the distribution of (generated) summary-sentences with respect to various positions of the original unordered meta-document for PreSumm+DR (w/ reordering, purple shaded bars) and PreSumm (w/o reordering, blue solid bars). The x-axis shows the sentence positions in the original meta-document and the y-axis shows the fraction of sentences from the generated summary that were at that position in the metadocument. We see that compared with the blue bars, the purple bars have a more uniform distribution. This indicates that the reordering based model has a greater tendency to pick sentences that were located at unfavorable positions (towards the end) in the original meta-document. The reordering helps in moving these sentences to the front, and then the summarization models pick them for generating the summary.</p><p>Overall, from these experiments, we can conclude that since the base summarization model pays more attention to the beginning of the input (Experiment 2), by moving important content towards the beginning of the input (Experiment 1), the reordering method helps the summarization model also focus on information that was scattered across the original unordered input (Experiment 3). We also provide a qualitative analysis in Appendix D to show how the document reordering helps the model generate better summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose a document reordering based approach for multi-document news summarization. We rearrange the documents according to their relative importance while concatenating them into a meta-document and then apply a summarization model. Our simple yet effective approach outperforms the baselines on two multi-document summarization datasets, demonstrating that document reordering is a promising direction for multidocument news summarization. A next step, which we leave for future work, is to explore the scalability of such approaches on large document clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Base Summarization Models</head><p>PreSumm <ref type="bibr" target="#b15">(Liu and Lapata, 2019)</ref> is the supervised base summarization model. It uses BERT as the encoder to get the sentence representations, and a linear transformation with a Sigmoid as the decoder to get the probability of selecting a sentence. The loss function is the averaged cross-entropy between the predicted probability and the oracle {0, 1} label of each sentence. When applying this model to MDS, we insert a null sentence ("[CLS] [SEP]") between consecutive (reordered) documents in the meta-document as the document delimiter. It helps the model to identify document boundaries and build inter-document relationships.</p><p>PacSum <ref type="bibr" target="#b20">(Zheng and Lapata, 2019)</ref> is the unsupervised base summarization model. It uses sentence centrality to identify salient sentences. Different from other centrality-based methods, PacSum builds directed graph to explicitly model the order of sentences. Therefore PacSum can benefit from a meta-document where the salient documents are rearranged to the front. When applying it to MDS, we build the graph for the meta-document and calculate the centrality of each sentence accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>We conduct experiments on Multi-News and DUC-2004. Multi-News is the largest multi-document summarization dataset in the news domain. It contains 44,972/5,622/5,622 instances for training/validation/test. Each instance contains a set of news articles and an abstractive summary. The number of articles varies between 2 and 10. For evaluation, we compare the extracted summary to the gold abstractive summary. DUC-2004 contains 50 instances. Each instance has 10 documents and their abstractive summaries. Due to its small size, we use this dataset for out-of-domain evaluation only. We also use CNN DailyMail (CNNDM) to pretrain the base summarization model. It contains around 300K news articles and corresponding summaries from CNN and the Daily Mail.</p><p>We list the training details as follows. The training loss is optimized using Adam <ref type="bibr" target="#b11">(Kingma and Ba, 2015)</ref> with a learning rate of 2 ? 10 -3 and 10,000 training steps. We apply the warmup <ref type="bibr" target="#b5">(Goyal et al., 2017)</ref> on the first 2,000 steps and the early stopping based on the ROUGE-1 score on the development set. The batch size is set as 6,000 tokens. Our model was trained on a single Quadro RTX 5000 GPU in 2 hours.</p><p>Source 1: these items are among those purchased by gary simpson , prior to taking 9-year-old carlie trent from her school in rogersville , tn on may 4th ... share this : twitter facebook linkedin google email like this : like loading ... Source 2: by hayes hickman of the knoxville news sentinel two knoxville banking executives are offering a $ 10,000 reward for information leading to the return of missing 9-year-old carlie marie trent, who was abducted a week ago by her uncle in hawkins county . matt daniels , president and chief executive officer of apex bank , said he and his business partner , 21st mortgage president tim williams , felt compelled to get involved as the search continues ... "this is agonizing , " daniels said wednesday ." "it ' s not a 24-hour amber alert. " daniels said carlie reminds him of his two daughters ... Source 3: it ' s been a week since carlie trent was in school . on wednesday , tennessee bureau of investigation asked the public to trust them that carlie trent really is in danger . josh devine , a spokesperson for tbi , said he has heard some disturbing rumors on social media that if simpson was trying to protect carlie trent . he told wate he has seen tweets that asked " if simpson didn ' t harm carlie when he had custody , why would he do it now ." " i don ' t think he ' s trying to protect her ." this was not an innocent camping trip , this was a crime .</p><p>Source 4: the mother of missing tennessee 9-year old carlie trent says she ?lways had a bad feeling ?bout gary simpson , the 57-year-old man who allegedly kidnapped her daughter last week on may 4 . simpson , carlie ' s uncle by marriage who , along with his wife , had once had custody of carlie , picked her up from her rogersville elementary school . simpson and carlie did not return home later that afternoon and the following day an amber alert was issued in tennessee ... MatchSum: by hayes hickman of the knoxville news sentinel two knoxville banking executives are offering a $ 10,000 reward for information leading to the return of missing 9-year-old carlie marie trent , who was abducted a week ago by her uncle in hawkins county . matt daniels , president and chief executive officer of apex bank , said he and his business partner , 21st mortgage president tim williams , felt compelled to get involved as the search continues ... " this is agonizing , " daniels said wednesday . " it ' s not a 24-hour amber alert . " daniels said carlie reminds him of his two daughters ... PreSumm: by hayes hickman of the knoxville news sentinel two knoxville banking executives are offering a $ 10,000 reward for information leading to the return of missing 9-year-old carlie marie trent , who was abducted a week ago by her uncle in hawkins county . matt daniels , president and chief executive officer of apex bank , said he and his business partner , 21st mortgage president tim williams , felt compelled to get involved as the search continues ... mother of allegedly abducted 9-year-old carlie trent ' always had a bad feeling ' about suspect . these items are among those purchased by gary simpson , prior to taking 9-year-old carlie trent from her school in rogersville , tn on may 4th ... Ours: the mother of missing tennessee 9-year old carlie trent says she " always had a bad feeling " about gary simpson , the 57-year-old man who allegedly kidnapped her daughter last week on may 4 , simpson, carlie ' s uncle ... picked her up from her rogersville elementary school . he told wate he has seen tweets that asked " if simpson didn ' t harm carlie when he had custody , why would he do it now . "it ' s not a 24-hour amber alert. this was not an innocent camping trip , this was a crime . " i don ' t think he ' s trying to protect her ." simpson and carlie did not return home later that afternoon and the following day an amber alert was issued in tennessee ... Reference: -authorities are combing through more than 1,200 leads in a desperate search for a 9-year-old girl they say was abducted by her uncle may 4 , wate reports . according to the knoxville news sentinel , 57-year-old gary simpson picked carlie trent up from her tennessee school ... the tbi says there have been rumors online that simpson is trying to protect carlie , but it says that couldn ' t be further from the truth . this was not an innocent camping trip , this was a crime ... shannon trent , who hasn t had custody of carlie in two years , says she " always had a bad feeling " about simpson ... Table <ref type="table">5</ref>: Sample summaries generated by our method and the baselines. MatchSum and PreSumm receives the documents as the original order, making them focus more on the top two documents. Our method first rearrange the documents as the order of {3, 4, 2, 1} and then create the summary. We highlight the contents of the generated summaries which are relevant to the referenced summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Human Evaluation Details</head><p>We randomly select 100 test instances to evaluate the performance of each system. The three measures we used are 1) Informativeness: whether or not the summary reflects the salient information of the reference summary; 2) Conciseness: whether or not the summary contains no redundant words or repeated information; and 3) Usefulness: whether or not the summary helps the reader catch the main idea of the news. Human judges were paid at a wage rate of $8 per hour, which is higher than the local minimum wage rate.</p><p>The pairwise scores of those measures are calculated as follows. When comparing a certain base-line approach to our model, we report the percentage of summaries created by the baseline that were judged to be better/worse/same than those of our model, yielding a score ranging from -1 (unanimously worse) to 1 (unanimously better). For example, when evaluating the informativeness scores, Lead performs better/worse/same than our model for 36%/56%/8% of the instances, yielding a pairwise score as 0.36-0.56=-0.20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Analysis</head><p>Table <ref type="table">5</ref> shows an example with 4 source documents listed in the original order. The main event of this example is about a child abduction case, where source 3 and 4 provide more direct and detailed information compared with source 1 and 2.</p><p>We show the summaries generated by Match-Sum, PreSumm, and our system, as well as the reference summary. MatchSum and PreSumm receive the documents in the original order, making them focus more on the top two documents. Our method first rearranges the documents as the order of {3, 4, 2, 1} and then creates the summary based on the new re-ordered documents. With the help of the document reordering, our summary better captures the main event from the latter source documents (source 3 and source 4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance gain of summarization w.r.t.the number of input documents. We don't include instances with 6 or more documents since the number of such instances is small. Our approach results in more performance gain for longer inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The distribution of oracle extractive summaries according to their sentence positions in the metadocument with and without document reordering. (b) The distribution of generated extractive summaries according to their sentence positions in the meta-document with and without document reordering. (c) The distribution of generated extractive summaries according to their sentence positions in the original, unordered meta-document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>.20 16.51 41.89    </figDesc><table><row><cell>Unsupervised</cell><cell></cell></row><row><cell>PACSUM</cell><cell>43.02 14.03 39.02</cell></row><row><cell>PACSUM + DRunsup (Ours)</cell><cell>43.57 14.41 39.52</cell></row><row><cell>Supervised, w/o finetune on CNNDM</cell><cell></cell></row><row><cell>PRESUMM</cell><cell>46.05 16.56 41.91</cell></row><row><cell>PRESUMM + DRsup (Ours)</cell><cell>46.34 16.88 42.20</cell></row><row><cell>Supervised, w/ finetune on CNNDM</cell><cell></cell></row><row><cell>PRESUMM</cell><cell>46.25 16.75 42.11</cell></row><row><cell>PRESUMM + DRsup (Ours)</cell><cell>46.57 17.10 42.44</cell></row><row><cell>Oracle</cell><cell>49.06 21.54 44.27</cell></row></table><note><p><p><p>Summarization results evaluated on Multi-News by ROUGE 1 (R1), ROUGE 2 (R2), and ROUGE L (RL). Our best results (in bold) show statistically significant difference with baselines (using paired bootstrap resampling, p &lt; 0.05</p><ref type="bibr" target="#b12">(Koehn and Monz, 2006)</ref></p>).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of human evaluation by comparing three baselines with PreSumm+DR sup . A positive score means the baseline is better than ours and vise versa.</figDesc><table><row><cell>Model</cell><cell cols="2">Informative</cell><cell>Concise</cell><cell>Useful</cell></row><row><cell>LEAD</cell><cell>-0.20</cell><cell></cell><cell>-0.14</cell><cell>-0.17</cell></row><row><cell>MatchSum</cell><cell>-0.12</cell><cell></cell><cell>-0.05</cell><cell>-0.08</cell></row><row><cell>PreSumm</cell><cell>-0.06</cell><cell></cell><cell>0.03</cell><cell>-0.07</cell></row><row><cell>MODEL</cell><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>Lead-1</cell><cell></cell><cell>33.86</cell><cell>7.51</cell><cell>29.64</cell></row><row><cell>TextRank</cell><cell></cell><cell>33.09</cell><cell>7.49</cell><cell>29.25</cell></row><row><cell>MatchSum</cell><cell></cell><cell>33.84</cell><cell>7.44</cell><cell>30.07</cell></row><row><cell>PreSumm</cell><cell></cell><cell>34.42</cell><cell>7.95</cell><cell>30.34</cell></row><row><cell cols="2">PreSumm + DRsup</cell><cell>34.62</cell><cell>8.22</cell><cell>30.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Out-of-domain summarization results evaluated on DUC 2004 using the model trained on Multi-News. Our approach (last row) outperforms baselines.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Reordering methods evaluated on Multi-News. Our approaches, PreSumm + DR sup and PreSumm + DR unsup outperform the baselines.</figDesc><table><row><cell>MODEL</cell><cell cols="2">Reordering ? PMR</cell><cell>Summarization R1 R2 RL</cell></row><row><cell cols="2">Random -0.005</cell><cell>31.8</cell><cell>46.25 16.75 42.11</cell></row><row><cell>Length</cell><cell>0.189</cell><cell>43.2</cell><cell>46.30 16.73 42.15</cell></row><row><cell>DRunsup</cell><cell>0.236</cell><cell>46.4</cell><cell>46.41 16.94 42.26</cell></row><row><cell>DRsup</cell><cell>0.325</cell><cell>51.7</cell><cell>46.57 17.10 42.44</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://duc.nist.gov/data.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We also tried ROUGE-2 F or ROUGE-1 R but didn't observe a significant difference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Code is available at https://github.com/ zhaochaocs/MDS-DR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We use pyrouge (https://github.com/ bheinzerling/pyrouge)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We do not include advanced baselines as performance of the document reordering is not the main focus of our work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS2112635</rs>. We thank anonymous reviewers for their thoughtful and constructive reviews.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gUAj34V">
					<idno type="grant-number">IIS2112635</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>We do not foresee any ethical concerns from the technology presented in this work. We used publicly available datasets designed for summarization, and do not annotate any data manually. The datasets used is in English language.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is everything in order? a simple way to order sentences</title>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10769" to="10779" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving multi-document summarization via text classification</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, February<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-04">2017. 4-9, 2017</date>
			<biblScope unit="page" from="3053" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">G?nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Writing for journalists</title>
		<author>
			<persName><forename type="first">Wynford</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Sally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harriett</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bentley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/E14-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="712" to="721" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reliability of human evaluation for text summarization: Lessons learned and challenges ahead</title>
		<author>
			<persName><forename type="first">Neslihan</forename><surname>Iskender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Polzehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</title>
		<meeting>the Workshop on Human Evaluation of NLP Systems (HumEval)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-granularity interaction network for extractive and abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hanqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6244" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Content selection in deep learning models of summarization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1818" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Manual and automatic evaluation of machine translation between European languages</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the Workshop on Statistical Machine Translation</title>
		<meeting>on the Workshop on Statistical Machine Translation<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adapting the neural encoder-decoder framework from single to multi-document summarization</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4131" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for extractive document summarization</title>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.553</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6209" to="6219" />
		</imprint>
	</monogr>
	<note>Xipeng Qiu, and Xuanjing Huang</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HI-BERT: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1499</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1628</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
	<note>Xipeng Qiu, and Xuanjing Huang</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Searching for effective neural extractive summarization: What works and what&apos;s next</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
