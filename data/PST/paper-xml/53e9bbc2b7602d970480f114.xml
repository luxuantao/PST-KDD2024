<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ABB Corporate Research</orgName>
								<address>
									<addrLine>Segelhofstrasse 1K</addrLine>
									<settlement>Baden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Daettwil</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB0E981BB58FD0E7021D7FE3E61B3496</idno>
					<idno type="DOI">10.1109/TII.2010.2085006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Development and Implementation of Parameterized FPGA-Based General Purpose Neural Networks for Online Applications</head><p>Alexander Gomperts, Abhisek Ukil, Senior Member, IEEE, and Franz Zurfluh</p><p>Abstract-This paper presents the development and implementation of a generalized backpropagation multilayer perceptron (MLP) architecture described in VLSI hardware description language (VHDL). The development of hardware platforms has been complicated by the high hardware cost and quantity of the arithmetic operations required in online artificial neural networks (ANNs), i.e., general purpose ANNs with learning capability. Besides, there remains a dearth of hardware platforms for design space exploration, fast prototyping, and testing of these networks. Our general purpose architecture seeks to fill that gap and at the same time serve as a tool to gain a better understanding of issues unique to ANNs implemented in hardware, particularly using field programmable gate array (FPGA). The challenge is thus to find an architecture that minimizes hardware costs, while maximizing performance, accuracy, and parameterization. This work describes a platform that offers a high degree of parameterization, while maintaining generalized network design with performance comparable to other hardware-based MLP implementations. Application of the hardware implementation of ANN with backpropagation learning algorithm for a realistic application is also presented.</p><p>Index Terms-Backpropagation, field programmable gate array (FPGA), hardware implementation, multilayer perceptron, neural network, NIR spectra calibration, spectroscopy, VHDL, Xilinx FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A RTIFICIAL NEURAL NETWORKs (ANNs) present an unconventional computational model characterized by densely interconnected simple adaptive nodes. From this model stem, several desirable traits uncommon in traditional computational models; most notably, an ANN's ability to learn and generalize upon being provided examples. Given these traits, an ANN is well suited for a range of problems that are challenging for other computational models like pattern recognition, prediction, or optimization <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>.</p><p>An ANN's ability to learn and solve problems relies in part on the structural characteristics of that network. Those characteris-tics include the number of layers in a network, the number of neurons per layer, and the activation functions of those neurons, etc. There remains a lack of a reliable means for determining the optimal set of network characteristics for a given application.</p><p>Numerous implementations of ANNs already exist <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, but most of them being in software on sequential processors <ref type="bibr" target="#b1">[2]</ref>. Software implementations can be quickly constructed, adapted, and tested for a wide range of applications. However, in some cases, the use of hardware architectures matching the parallel structure of ANNs is desirable to optimize performance or reduce the cost of the implementation, particularly for applications demanding high performance <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Unfortunately, hardware platforms suffer from several unique disadvantages such as difficulties in achieving high data precision with relation to hardware cost, the high hardware cost of the necessary calculations, and the inflexibility of the platform as compared to software.</p><p>In our work, we aimed to address some of these disadvantages by developing and implementing a field programmable gate array (FPGA)-based architecture of a parameterized neural network with learning capability. Exploiting the reconfigurability of FPGAs, we are able to perform fast prototyping of hardware-based ANNs to find optimal application specific configurations. In particular, the ability to quickly generate a range of hardware configurations gives us the ability to perform a rapid design space exploration navigating the cost/speed/accuracy tradeoffs affecting hardware-based ANNs.</p><p>The remainder of this paper will begin by more precisely describing the motivation of our work and the current state-of-theart in the field in Section II. Section III will provide the basics of ANNs and the backpropagation learning algorithm. Section IV will cover the system's hardware design and implementation details of interest. In Section V, we will report the results of our experimentation using the selected sample application. Following this, we will discuss the results as they relate to the system implementation, and consider areas for further improvement in Section VI, followed by conclusions in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION</head><p>In the past, the size constraints and the high cost of FPGAs when confronted with the high computational and interconnect complexity inherent in ANNs have prevented the practical use of the FPGA as a platform for ANNs <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Instead, the focus has been on development of microprocessor-based software implementations for real world applications, while FPGA platforms largely remained as a topic for further research. Despite the prevalence of software-based ANN implemen-tations, FPGAs and similarly, application specific integrated circuits (ASICs) have attracted much interest as platforms for ANNs because of the perception that their natural potential for parallelism and entirely hardware-based computation implementation provide better performance than their predominantly sequential software-based counterparts. As a consequence, hardware-based implementations came to be preferred for high performance ANN applications <ref type="bibr" target="#b8">[9]</ref>. While it is broadly assumed, it should be noted that an empirical study has yet to confirm that hardware-based platforms for ANNs provide higher levels of performance than software in all the cases <ref type="bibr" target="#b9">[10]</ref>.</p><p>Currently, no well defined methodology exists to determine the optimal architectural properties (i.e., number of neurons, number of layers, type of squashing function, etc.) of a neural network for a given application. The only method currently available to us is a systematic approach of educated trial and error. Software tools like MATLAB Neural Network Toolbox <ref type="bibr" target="#b12">[13]</ref> make it relatively easy for us to quickly simulate and evaluate various ANN configurations to find an optimal architecture for software implementations. In hardware, there are more network characteristics to consider, many dealing with precision related issues like data and computational precision. Similar simulation or fast prototyping tools for hardware are not well developed.</p><p>Consequently, our primary interest in FPGAs lies in their reconfigurability. By exploiting the reconfigurability of FPGAs, we aim to transfer the flexibility of parameterized softwarebased ANNs and ANN simulators to hardware platforms. Doing this, we will give the user the same ability to efficiently explore the design space and prototype in hardware as is now possible in software. Additionally, with such a tool we will be able to gain some insight into hardware specific issues such as the effect of hardware implementation and design decisions on performance, accuracy, and design size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Previous Works</head><p>Many ANNs have already been implemented on FPGAs. The vast majority are static implementations for specific offline applications without learning capability <ref type="bibr" target="#b13">[14]</ref>. In these cases, the purpose of using an FPGA is generally to gain performance advantages through dedicated hardware and parallelism. Far fewer are examples of FPGA-based ANNs that make use of the reconfigurability of FPGAs.</p><p>Flexible Adaptable Size Topology (FAST) <ref type="bibr" target="#b14">[15]</ref> is an FPGAbased ANN that utilizes runtime reconfiguration to dynamically change its size. In this way, FAST is able to skirt the problem of determining a valid network topology for the given application a priori. Runtime reconfiguration is achieved by initially mapping all possible connections and components on the FPGA, then only activating the necessary connections and components once they are needed. FAST is an adaptation of a Kohonen type neural network and has a significantly different architecture than our multilayer perceptron (MLP) network.</p><p>Interesting FPGA implementation schemes, specially using Xilinx FPGAs, are described in the book edited by Ormondi and Rajapakse <ref type="bibr" target="#b15">[16]</ref>. Izeboudjen et al. presented an implementation of an FPGA-based MLP with backpropagation in <ref type="bibr" target="#b16">[17]</ref>. Gadea et al. reported comparative implementation of pipelined online backpropagation in <ref type="bibr" target="#b17">[18]</ref> and using Xilinx Virtex XCV400 for implementation of pipelined backpropagation ANN in <ref type="bibr" target="#b18">[19]</ref>. Ferreira et al. discussed about optimized algorithm for activation functions for ANN implementations in FPGA <ref type="bibr" target="#b19">[20]</ref>. Girau described FPGA implementation of 2-D multilayer NN <ref type="bibr" target="#b20">[21]</ref>. Stochastic network implementation was reported by Bade and Hutchings <ref type="bibr" target="#b21">[22]</ref>. Hardware implementation of backpropagation algorithm was described by Elredge and Hutchings <ref type="bibr" target="#b22">[23]</ref>. Later on, we will compare our implementation with some of these.</p><p>From the application side, Alizadeh et al. used ANN in FPGA to predict cetane number in diesel fuel <ref type="bibr" target="#b23">[24]</ref>. Tatikonda and Agarwal used FPGA-based ANN for motion control and fault diagnosis of induction motor drive <ref type="bibr" target="#b24">[25]</ref>. Mellit et al. used ANN in Xilinx Virtex-II XC2V1000 FPGA for modelling and simulation of standalone photovoltaic systems <ref type="bibr" target="#b25">[26]</ref>. Rahnamaei et al. reported FPGA implementation of ANN to detect anthelmintics resistant nematodes in sheep flocks <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Platform</head><p>Our development platform is the Xilinx Virtex-5 SX50T FPGA <ref type="bibr" target="#b27">[28]</ref>. While our design is not directed exclusively at this platform and is designed to be portable across multiple FPGA platforms, we will mention some of the characteristics of the Virtex-5 important to the design and performance of our system.</p><p>This model of the Virtex-5 contains 4080 configurable logic blocks (CLBs), the basic logical units in Xilinx FPGAs. Each CLB holds eight logic function generators (in lookup tables), eight storage elements, a number of multiplexers, and carry logic. Relative to the time in which this paper is written, this is considered a large FPGA; large enough to test a range of online neural networks of varying size, and likely too large and costly to be considered for most commercial applications.</p><p>Arithmetic is handled using CLBs containing DSP48E slices. Of particular note is that a single DSP48E slice can be used to implement one of two of the most common and costly operations in ANNs: either two's complement multiplication or a single multiply accumulate (MACC) stage. Our model of the Virtex-5 holds 288 DSP48E slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ARTIFICIAL NEURAL NETWORKS (ANNS)</head><p>Artificial neural networks (ANN's, or simply NN's) are inspired by biological nervous systems and consist of simple processing elements (PE, artificial neurons) that are interconnected by weighted connections. The predominantly used structure is a multilayered feed-forward network (multilayer perceptron), i.e., the nodes (neurons) are arranged in several layers (input layer, hidden layers, output layer), and the information flow is only between adjacent layers <ref type="bibr" target="#b3">[4]</ref>. An artificial neuron is a very simple processing unit. It calculates the weighted sum of its inputs and passes it through a nonlinear transfer function to produce its output signal. The predominantly used transfer functions are so-called "sigmoid" or "squashing" functions that compress an infinite input range to a finite output range, e.g.,</p><p>, see <ref type="bibr" target="#b3">[4]</ref>. Neural networks can be "trained" to solve problems that are difficult to solve by conventional computer algorithms. Training refers to an adjustment of the connection weights, based on a  Training is an incremental process where after each presentation of a training example, the weights are adjusted to reduce the discrepancy between the network and the target output. Popular learning algorithms are variants of gradient descent (e.g., error-backpropagation) <ref type="bibr" target="#b28">[29]</ref>, radial basis function adjustments <ref type="bibr" target="#b3">[4]</ref>, etc. Neural networks are well suited to a variety of nonlinear problem solving tasks. For example, tasks related to the organization, classification, and recognition of large sets of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multilayer Perceptrons (MLPs)</head><p>MLPs (Fig. <ref type="figure" target="#fig_0">1</ref>) are layered fully connected feed-forward networks. That is, all PEs (Fig. <ref type="figure" target="#fig_1">2</ref>) in two consecutive layers are connected to one another in the forward direction.</p><p>During the network's forward pass each PE computes its output from the input it receives from each PE in the preceding layer as shown here <ref type="bibr" target="#b0">(1)</ref> where is the squashing function of PE whose role is to constrain the value of the local field <ref type="bibr" target="#b1">(2)</ref> is the weight of the synapse connecting neuron to neuron in the previous layer, and is the bias of neuron . Equation ( <ref type="formula">1</ref>) is computed sequentially by layer from the first hidden layer which receives its input from the input layer to the output layer, producing one output vector corresponding to one input vector.</p><p>The network's behavior is defined by the values of its weights and bias. It follows that in network training the weights and biases are the subjects of that training. Training is performed using the backpropagation algorithm after every forward pass of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Backpropagation Algorithm</head><p>The backpropagation learning algorithm <ref type="bibr" target="#b28">[29]</ref> allows us to compute the error of a network at the output, then propagate that error backwards to the hidden layers of the network adjusting the weights of the neurons responsible for the error. The network uses the error to adjust the weights in an effort to let the output approach the desired output .</p><p>Backpropagation minimizes the overall network error by calculating an error gradient for each neuron from which a weight change is computed for each synapse of the neuron. The error gradient is then recalculated and propagated backwards to the previous layer until weight changes have been calculated for all layers from the output to the first hidden layer.</p><p>The weight correction for a synaptic weight connecting neuron to neuron mandated by backpropagation is defined by the delta rule <ref type="bibr" target="#b2">(3)</ref> where is the learning rate parameter, is the local gradient of neuron , and is the output of neuron in the previous layer.</p><p>Calculation of the error gradient can be divided into two cases: for neurons in the output layer and for neurons in the hidden layers. This is an important distinction because we must be careful to account for the effect that changing the output of one neuron will have on subsequent neurons. For output neurons, the standard definition of the local gradient applies <ref type="bibr" target="#b3">(4)</ref> For neurons in a hidden layer, we must account for the local gradients already computed for neurons in the following layers up to the output layer. The new term will replace the calculated error since, because hidden neurons are not visible from outside of the network, it is impossible to calculate an error for them. So, we add a term that accounts for the previously calculated local gradients <ref type="bibr" target="#b4">(5)</ref> where is the hidden neuron whose new weight we are calculating, and is an index for each neuron in the next layer connected to .</p><p>As we can see from ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>), we are required to differentiate the activation function with respect to its own argument, the induced local field . In order for this to be possible, the activation function must of course be differentiable. This means that we cannot use noncontinuous activation functions in a backpropagation-based network. Two continuous, nonlinear activation functions commonly used in backpropagation networks are the sigmoid function <ref type="bibr" target="#b5">(6)</ref> and the hyperbolic tangent function <ref type="bibr" target="#b6">(7)</ref> Training is performed multiple times over all input vectors in the training set. Weights may be updated incrementally after each input vector is presented or cumulatively after the training set in its entirety has been presented (one training epoch). This second approach, called batch learning, is an optimization of the backpropagation algorithm designed to improve convergence by preventing individual input vectors from causing the computed error gradient to proceed in the incorrect direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HARDWARE IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Design Architecture</head><p>Our design approach is characterized by the separation of simple modular functional components and more complex intelligent control oriented components. The functional units consist of signal processing operations (e.g., multipliers, adders, squashing function realizations, etc.) and storage components (e.g., RAM containing weights values, input buffers, etc.). Control components consist of state machines <ref type="bibr" target="#b15">[16]</ref> generated to match the needs of the network as configured. During design elaboration, functional components matching the provided parameters are automatically generated and connected, and the state machines of control components are tuned to match the given architecture.</p><p>Network components are generated in a top-down hierarchical fashion, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Each parent is responsible for generating its children to match the parameters entered by the user prior to elaboration and synthesis. Using VHDL generated statements and a set of constants whose values are given by the user configuration, the top level ANN block generates a number of layer blocks as required by the configuration. Each layer subsequently generates a teacher if learning is enabled along with a number of PEs as configured for that layer. Each PE generates a number of MACC blocks equal to the width of the previous layer as well as a squashing function block. Fig. <ref type="figure" target="#fig_2">3</ref> shows that there is a one-to-one relationship between the logical blocks generated (e.g., Layers, PEs, MACCs) and the functions and architecture of the conceptual ANN model. This approach has been chosen over one in which logical blocks are time-multiplexed to allow for a high-performance fully pipelined implementation at the cost of greater resource demands.</p><p>In this context, it is interesting to note how it is done at PC-based system. Fig. <ref type="figure" target="#fig_3">4</ref> shows the graphical user interface to generate different networks using the MATLAB Neural Network toolbox <ref type="bibr" target="#b12">[13]</ref>. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, one can choose network type (feed-forward backpropagation, MLP, radial basis, etc.), number of layers, number of neurons per layer, the transfer functions, etc. With these inputs, the network structure is generated. Of course, one gets higher flexibility in terms of data precision, network size, etc., in PC-based ANN designs. Nevertheless, we wanted to have the same flexible design philosophy for the FPGA-based design. Please note, this study is limited to implementation of feed-forward MLP with backpropagation learning algorithm. Also, design parameters like number of layers, number of neurons, input-output sizes, types of interconnection, etc., are limited by the hardware resources available, depending on the type of FPGA used. There are no general rules. One has to derive the limits by experiments. Later on in this paper, we would present some statistics on the variations of these design parameters over hardware resource utilizations. Similar statistics for different platforms are also reported in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Representation</head><p>Network data is represented using a signed fixed point notation. This is implemented in VLSI hardware description language (VHDL), with the IEEE proposed fixed point package <ref type="bibr" target="#b29">[30]</ref>. Fixed point notation serves as a compromise between traditional integer math and floating point arithmetic, the latter being prohibitively costly in terms of hardware <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our network has a selectable data width which is subdivided into integer and fractional portions. For example, our implementation incorporates one sign bit , one integer place , and three fractional places</p><p>The network data precision thus becomes (0.125 in our example), where is the number of fraction bits and gives the data set a maximum range of (8)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Processing Element</head><p>An effort was made to keep the realization of a single PE as simple as possible. The motivation for this was that in our parallel hardware design many copies of this component would be generated for every network configuration. So, keeping this component small helps to minimize the size of the overall design. The control scheme was centralized external to the PE component to prevent the unnecessary duplication of functionality and complexity.</p><p>In paring the design of the PE component down to its essential features, we are left with a multiply accumulate function with a width equal to the number of neurons in the previous layer, the selected squashing function implementation, and memory elements such as registers contained the synaptic weights and input buffers.</p><p>The PE is described by [( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>)]. The full calculation of the PE is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. In Fig. <ref type="figure" target="#fig_4">5</ref>, the MACC units multiply and sum the inputs with the weights and the bias , to implement as in [( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>)]. After that, the final quantity from the MACC is passed through the squashing function (see top right corner) to produce the network output . As indicated in Fig. <ref type="figure" target="#fig_4">5</ref>, the squashing function utilizes lookup table (LUT) (via "lut_en" instruction), described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Squashing Function</head><p>The direct implementation of the preferred nonlinear squashing, e.g., the sigmoid function [see ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>)], presents a problem in hardware since both the division and exponentiation operations require an inordinate amount of time and hardware resources to compute. The only practical approach in hardware is to approximate the function <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, in order for training to converge or for us to obtain accurate offline results, a minimum level of accuracy must be reached <ref type="bibr" target="#b32">[33]</ref>. More accurate approximations will result in faster, better convergence, hence more accurate results. There has been a significant amount of research into how a sigmoid function can be efficiently approximated in hardware, while maintaining an acceptable degree of accuracy and the continuity required for reliable convergence in backpropagation learning <ref type="bibr" target="#b31">[32]</ref>. To create a generalized design we must add one additional requirement for sigmoid function approximation, that the method of approximation must be valid for a variety of sigmoid functions.</p><p>Based on size and accuracy requirements to be met by the network we are generating, we may select one of two implementation styles for a sigmoid function that we have implemented in our generalized design: a uniform Lookup Table (LUT) or a LUT with linear interpolation, described below. Comparative analysis of the hyperbolic tangent squashing function implementation, using LUT and piecewise linear approximation techniques are discussed in <ref type="bibr" target="#b19">[20]</ref>. Gadea et al. compared performances of different types of implementation of squashing function in <ref type="bibr" target="#b17">[18]</ref>.</p><p>LUT-based approach works much faster than piece-wise linear approximation, though LUT consumes memory. So, if there is not much concern about memory, LUT is preferred, as in our implementations, and real-time applications, e.g., motion control and fault diagnosis of induction motor drive <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Uniform Lookup Table (LUT) Implementation:</head><p>A uniform LUT implemented in block RAM may be used to approximate a function of any shape.</p><p>The LUT is addressed using the local field. The address is formed by taking the inverse of the sign bit of the local field and concatenating the most significant bits required to represent the highest input value of the function mapped onto the LUT down to the number of address bits of the LUT. Altogether, the computation requires one cycle and minimal hardware to hold the table itself.</p><p>The uniform LUT implementation, despite being popular in FPGA-based ANNs and while efficient in terms of speed and size, presents a problem in terms of its size versus accuracy tradeoff when it comes to modeling functions with steep slopes like a sigmoid function. As the slope increases so does the quantization error between LUT entries (Fig. <ref type="figure" target="#fig_6">6</ref>). A common solution for this problem is the use of a LUT with variable resolution [Fig. <ref type="figure" target="#fig_6">6(c)]</ref>. That is, a LUT with higher resolution for portions of the function with steeper slopes. However, this is a solution that must be custom crafted for every given sigmoid and thus is not easily generalized as we would like.</p><p>2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Lookup Table With Linear Interpolation Implementation:</head><p>To address the diminished accuracy of a uniform LUT while maintaining generalizations over a variety of functions, we incorporate linear interpolation alongside the LUT. This solution effectively draws a line between each point of a uniform LUT as in Fig. <ref type="figure" target="#fig_7">7</ref>. The resulting activation function is <ref type="bibr" target="#b8">(9)</ref> where is the value returned by a uniform LUT representing the target function for index (done as described in Section IV-D1), is the bit widths of the local field, is the bit  width of the LUT address bus, and is the quantization error described by <ref type="bibr" target="#b9">(10)</ref> The algorithm flow is controlled by a state machine inside the squashing function component enabled by the network controller. After receiving input, the result is registered at the output after five clock cycles. An analysis of the tradeoff between the hardware cost, performance, and accuracy related to this method is given in Section V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Backpropagation Computation</head><p>The backpropagation algorithm (see Section III-B) relies on the calculation of the network error at the output layer to estimate the error of neurons in the previous layers. The error is estimated and propagated backwards layer by layer until the first hidden layer is reached. It follows that the error estimation of any given layer except the output layer is dependent on the error calculation of its successor. Because of this, the training algorithm must be computed sequentially, layer by layer limiting parallelism to the neuron level.</p><p>The hardware implementation once again seeks to separate control and functional-based components. Each layer contains its own backpropagation teacher component which is responsible for the control flow of the algorithm for its layer. Since the backpropagation algorithm is only being executed for one layer at a time we need only one set of the necessary arithmetic components.</p><p>Since the number of multipliers and adders, and the size of the MACC are dependent on the size of a given layer and its predecessor, we must compute and generate the worst case number of arithmetic components that are needed during elaboration of the given network design. The set of arithmetic components used in the backpropagation calculation are then packaged into the BP ALU component.</p><p>The BP ALU is accessed by each backpropagation teacher via an automatically generated multiplexer (Fig. <ref type="figure" target="#fig_8">8</ref>) which is controlled by the network controller.</p><p>To optimize the performance of the training algorithm, we begin execution during the forward pass of the network. In the forward pass, we are able to compute two elements of the algorithm: once a given layer receives its input, and once the layer's output has been computed. In the hidden layers, the results of these preprocessing steps are then saved until the error gradient reaches them in the backward pass. The output layer teacher continues immediately by calculating the output error and the local error gradient (4) for every neuron in the output. Once the error gradient has been calculated at the output layer, the final hidden layer may calculate its own error gradient (5) and pass that back. Synthesis example of BP algorithm for a realistic application case would be presented in Section VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Network Controller</head><p>The design of the network controller was strongly guided by the highly generalized design of the network and the initial decision to separate functional and control units. The decision to centralize control of the network was based on the goal of minimizing the size and complexity of components that must be generated multiple times. This is contrary to a distributed control mechanism made up of independent components capable of determining their own state and communicating that state to one another. This would be the more modular solution, but would also inflict a significant time and hardware penalty caused by the control overhead in the network, since control mechanisms would be repeated many times through the network. The centralized control scheme on the other hand, relies on the predictability of the timing and behavior of any generated network configuration.</p><p>Depending on the network to be generated, the network controller is created as either an online or offline network controller. Here, offline indicates that a network is pretrained, and then the network object is used on test data, without any training capability. This type of network is often used in hardware implementations due to less implementation complexity. On the other hand, online indicates that the network has dynamic training capability. For different applications, the network with different architecture, would train itself following particular training methods, e.g., backpropagation, before acting on test data. This provides a generalized flexibility, however, it is complex to implement.</p><p>Different implementations are necessary since in offline mode a pipelined network is generated and the online controller must include control for the computation of the backpropagation algorithm. Despite this, both controllers are implemented in the same manner.</p><p>The network controller is a Mealy state machine (see Fig. <ref type="figure" target="#fig_9">9</ref>) based on a counter indicating the number of clock cycles that have passed in the current iteration (in the case of an online network) or the total number of clock cycles passed (in the case of an offline network). As mentioned above, for online application, we would need network with learning capability. It is to be noted that, for backpropagation learning algorithm, the error needs to be fed back. Therefore, Mealy state machine is suitable, as it is a finite-state transducer that generates an output based on its current state and input. For the value of the counter to have any meaning we must be able to precalculate the latency to reach milestones in the forward and back passes of the network. These milestones are calculated during elaboration of the design. Based on these milestones, the state machine outputs a set of enable signals to control the flow of the network. In comparison, for offline applications, one would not require training, hence the back passes in the network controller state machine (see Fig. <ref type="figure" target="#fig_9">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I WORST CASE ERROR OF APPROXIMATED HYPERBOLIC TANGENT FUNCTION USING A LUT WITH LINEAR INTERPOLATION AND A UNIFORM LUT V. EXPERIMENTAL RESULTS</head><p>To evaluate our system, we implement a basic sample application in a varied set of configurations. To speed the process, we carried out testing using Mentor Graphics' Mod-elSim simulator. A similar network implementation built using the MATLAB Neural Network Toolbox <ref type="bibr" target="#b12">[13]</ref> is used as a basis for comparison.</p><p>For comparison we take as metrics the root mean square error (RMSE) of the ANNs when applied to a test data set after 50, 100, 200, 400, and 800 epochs. We define the RMSE as <ref type="bibr" target="#b10">(11)</ref> where is the calculated output of the network, is the desired output, and is the number of input sets presented.</p><p>A speed comparison is not meaningful due to the dubious nature of any resulting measure considering the external factors affecting execution time on a PC.</p><p>We will also remark here on the results of our squashing function approximation method using a LUT combined with linear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LUT With Linear Interpolation</head><p>To judge the accuracy of our approximation technique for sigmoid function using a LUT with linear interpolation we calculated the average error and worst case error of the technique for a range of network data precisions and LUT sizes. Using a uniform LUT as a control, we approximated the hyperbolic tangent with input ranging <ref type="bibr">[ 4:4]</ref>. Table <ref type="table">I</ref> shows the worst case error using the two techniques and Table II the average error. From the results we see that the LUT with linear interpolation provides a significant improvement in accuracy. For example, in a case of a network that uses 15-bit fractional precision, an 8192 element uniform LUT can be replaced by a 128 element LUT with linear interpolation and achieve a slightly better quality approximation on average.</p><p>Using the LUT with linear interpolation it becomes possible to reduce the error such that the magnitude of the error falls under the resolution of the network data precision. At this point, we have reached a maximally precise approximation for the given network. Table <ref type="table" target="#tab_0">III</ref> shows the resolution necessary to register the approximation error in the worst case for each set up in our tests. The boxed entries show the minimum LUT size for a given network data resolution to reach a maximally precise approximation.</p><p>The logarithmic scale on the axis of Table <ref type="table" target="#tab_0">III</ref> is with base 2. This is done because number of elements in optimal LUTs in hardware always have a base 2 logarithm and because only these values are used, the step size is defined this way. This means that 10 corresponds to elements (the reason this is optimal is that to address 1024 elements, exactly 10 bits are used). Likewise, a bit resolution of 10 is equivalent to a resolution of . 10 represents the binary place, i.e., 10 places after the decimal point.</p><p>The purpose of Table <ref type="table" target="#tab_0">III</ref> is to show for a given data precision in the network, what the optimal size for a LUT implementing a maximally accurate standard sigmoid function is. Maximally accurate is defined by a worst case error that is less than the precision of data propagating through the network. The contents of Table III are based on Table <ref type="table">I</ref>, which describes the minimum bit precision needed to resolve the worst case error. The boxed elements in Table <ref type="table" target="#tab_0">III</ref> give the minimum size LUT necessary to achieve a worst case error less than the resolution of the network. For example, if we consider the top left boxed entries in Table <ref type="table">I</ref>, which have a worst case error of , the bit resolution as per Table <ref type="table" target="#tab_0">III</ref>, must be at least 12 . Based on this, we can say that a lin-LUT of size is the smallest size LUT that gives a maximally accurate result for a network fractional precision of 10 or 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spectrometry Application</head><p>Our test application attempts to draw out conclusions from the frequency spectrum returned by a spectrometer. In this case, we are analyzing several cuts of meat to determine the fat concentration of each. The network is provided with the magnitudes of ten principle components of detected frequencies and is trained at the output using the measured fat concentration of the cut <ref type="bibr" target="#b33">[34]</ref>. NNs are increasingly used as a nonlinear modeling tool for spectra calibration <ref type="bibr" target="#b34">[35]</ref>, at PC-level.</p><p>Here, we compare the accuracy of the FPGA-based implementation with that of a similar MATLAB simulation. Both use a 10-3-1 layer configuration trained using backpropagation. That is, ten noncomputing neurons in the input layer corresponding to the ten principle components, three neurons in the hidden layer, and one output neuron corresponding to the fat concentration. The MATLAB simulation employs batch learning with a learning rate of 0.05, while our system uses incremental learning with a learning rate of 0.1. The networks are trained over 200 input/target pairings, the final 40 pairings are used for testing to evaluate the network's ability to generalize. Input values were normalized between 2 and 2 and target values between 1 and 1. Both networks use the same initial set of weights randomized between 2 and 2.</p><p>Table <ref type="table" target="#tab_1">IV</ref> compares the RMSE of the test cases (not included in training) for the MATLAB and FPGA implementations over different number of training epochs. Comparison of performances for unseen test cases demonstrate the generalization capability of the networks. FPGA implementations use data widths with 1 sign bit, 2 integer bits, and fractional data precisions of varying size (the total data width of the network data is given in the left column). The FPGA-based implementations utilize LUTs with linear interpolation of the size required for maximally precise approximations as indicated in Table <ref type="table" target="#tab_0">III</ref>.</p><p>From Table <ref type="table" target="#tab_1">IV</ref>, we can notice, the best RMSE using the MATLAB implementation for the testset is 2.63 (obtained using 800 training epochs). In comparison, the FPGA implementation achieves the best testset RMSE of 3.05 (for 18-bit data width, using 400 training epochs). Expectedly, FPGA implementation results in less accuracy compared to PC-based implementation using MATLAB, possibly due to factors like limitation in data width, fixed point data precision, incremental learning instead of batch learning, etc. However, the best performance of the FPGA implementation is reasonably acceptable compared to that of MATLAB. Also, in the FPGA implementation of the online network, we could flexibly parameterize data widths, number of training epochs, etc., a central aim of this study.</p><p>A maximally precise approximation, however, is not always necessary for a functional network. In <ref type="bibr" target="#b31">[32]</ref>, Tommiska regards a maximum error of 0.0039 to be the functional limit on the accuracy of the squashing function for network training, and a maximum error of 0.0078 as the limit for the forward pass.</p><p>Table <ref type="table" target="#tab_2">V</ref> compares the RMSE over the test set of two FPGA implementations of networks with 12-bit data precision, one implemented with a 64-element LUT with linear interpolation and one with a uniform 64-element LUT. We see that indeed the network is still functional as Tommiska predicts. However, training becomes noticeably erratic and the accuracy of the network is significantly diminished as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>Ideally, we would like to make accuracy and performance comparisons only against other hardware-based systems. This is for two reasons. First, hardware-based ANNs allow for more meaningful measures of computation time. In software-particularly software in a PC environment-computation time is affected by a series of external factors such as the characteristics of the processor and the current load on the system. Second, software-based ANNs are able to employ a number of features that may not be practical or feasible in hardware such us floating point notation or advanced training techniques. Software and hardware-based ANNs are two very different creatures. Hardware in general-but not in all cases-tends to lend itself to faster performance. However, it is less capable of utilizing more complex techniques to achieve a higher degree of accuracy as software is able to. This disparity reduces the value of such comparisons since in reality we are comparing two significantly different networks. However, considering the scarcity of backpropagation ANN networks in hardware, and without access to training and test data for those few ANNs, the MATLAB Neural Network Toolbox <ref type="bibr" target="#b12">[13]</ref> remains a viable alternative benchmark for accuracy when care is taken to note the inequities inherent in the comparison. On the other hand, we are able to make performance comparisons against other hardware platforms using a set of standard performance metrics. This comparison will be presented in Section VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Accuracy</head><p>We found that in our sample application, the FPGA-based network converges over fewer iterations than the MATLAB simulation. This comes as a result of incremental learning and a higher learning rate. The quality of the FPGA-based network's convergence however, is not as good. The FPGA-based network must utilize a higher learning rate to reduce the increased chance of the training algorithm becoming trapped in a local minimum as a result of using incremental learning. This in turn causes the overall network error gradient to become unstable earlier in training; a symptom of overshooting the target value in training. The MATLAB simulation on the other hand, is less likely to overshoot a target in training thanks to its low learning rate and is at the same time less likely to become trapped in local minima using batch learning.</p><p>The extremes of the target output are other contributors to the error in the FPGA implementation. The range of the calculated output does not reach the extreme ends of the target range. This is attributed to the eventual averaging of the weight changes caused by changes to the error gradient for each iteration in training.</p><p>This sample application proved to be a somewhat special case. In testing, we found that we could train the network for an unusually large number of epochs without experiencing any overfitting in the test set. We found that the error in the test set bore a strong relation to the training set error in both implementations. In this sample application, we never saw the expected divergence caused by over training the network. This occurs when data sets contain few if any outliers, the training set is very representative of the relation, and the test set conforms well to that relation. Given a larger, more diverse test set or a smaller training set, we expect to observe a divergence between error of the training set and that of the test set.</p><p>From Table <ref type="table" target="#tab_1">IV</ref>, it is clear that the data precision of the ANN plays an important role in the network's ability to converge in training and its resulting accuracy. Our own testing seems to show that Holt and Hwang's assertion <ref type="bibr" target="#b32">[33]</ref>, that 13 bits is the minimum required precision for network weights and biases to reliably obtain convergence using backpropagation learning, is not a firm rule. We found that our 12-bit implementation can still yield a successful convergence, while indeed, lower bit widths like that of our 9-bit implementation results in rather poor convergence. The unexpected success of the 12-bit implementation  is likely due in part to the expansion of the data path for intermediate results in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hardware Implementation</head><p>To measure the performance of ANNs across the wide range of platforms that they have been implemented on, there are two common criteria: connections per second (CPS) and connection updates per second (CUPS). CPS is defined as the number of multiply-add operations that occur in the forward pass of a network per second. CUPS is the number of weight updates per second in the network. Two additional derivations of these metrics exist. First, to account for the number of connections in the network that must be computed or updated we may calculate connections per second per weight (CPSPW) or connection updates per second per weight (CUPSPW). Second, we may take into account the precision of the network by calculating the connection primitives per second (CPPS) or the connection primitives per second per weight (CPPSPW) to account for both the number of connections in the network and the precision of the network data <ref type="bibr" target="#b37">[38]</ref>.</p><p>Table <ref type="table" target="#tab_3">VI</ref> presents a performance comparison between our network and other implementations using these metrics. The performance data of our network reflects the 12-bit implementation of the spectrometry application presented earlier in Section V-B.</p><p>Due to the parallel implementation of FPGA-based neural networks like ours and the implementation presented in <ref type="bibr" target="#b16">[17]</ref>, the CPS metric increases as a function of the number of synaptic connections. This is because as more neurons are implemented in parallel, more MACC operations can occur simultaneously, thereby increasing the number of operations per second. The CPSPW metric is a means for normalizing this behavior by dividing by the number of weights. However, in parallel implementations, this metric punishes networks with wide input vectors by including weights in the normalization that are not connected to multiply-add operations on the input side.</p><p>The implementation of our system maximizes flexibility and parallelism over hardware cost. This is seen, for example, in that when a squashing function implementation for a layer of PEs requires a LUT, each PE is given a dedicated LUT as opposed to sharing it with other PEs. We do this to allow each PE the potential to have its own squashing function, implementation type, and level of accuracy. As a result of choices to maximize flexibility, network designs generated on our platform are likely to be somewhat larger than any application specific designs.</p><p>Parallelism in the network is implemented at the synaptic level, this represents the maximum level of parallelism for neural networks. Using synaptic level parallelism, all neurons and synaptic connections in a network layer operate in parallel with the goal of maximizing performance. Performance in the offline network is further improved with the use of a fully pipelined design.</p><p>As an example of the hardware resource demands of a network generated using our system, we take the online implementations of the 12-bit spectrometry application network. Table <ref type="table" target="#tab_4">VII</ref> shows the quantities of selected components generated during synthesis to implement the networks of different sizes as well as the total percentage of area consumed against the available resources on the Virtex-5 xc5vsx50t-2ff1136 platform <ref type="bibr" target="#b27">[28]</ref>.</p><p>The amount of hardware resources consumed by a network implemented using our platform is strongly influenced by many factors. The two most influential of these factors are the dimensions (governed by network structure) and the data width of the network. Naturally, as shown in Table VII, networks of greater size result in greater hardware requirements. The structure 10-3-1 has been used for the results shown in Tables <ref type="table" target="#tab_2">IV</ref> and<ref type="table" target="#tab_2">V</ref>. For brevity purpose, networks of lower sizes are not shown.</p><p>Besides, parallel calculation of the backpropagation makes consecutive wide layers especially costly, requiring a number of MACCs equal to the product of the width of the two widest consecutive layers. When choosing the data width of the network, careful consideration should be made to match the data width to the width of hardware arithmetic units on the target FPGA. Wider data widths than the width of hardware arithmetic units necessitate additional sets of those units to implement the design, thereby significantly increasing the final hardware requirements of the implementation.</p><p>Examples of hardware resource consumptions in other neural network implementation and applications, reported by the synthesis results of various works, are briefly shown in Table <ref type="table" target="#tab_5">VIII</ref>.</p><p>It is to be noted that Table VIII employs many different network structures for various applications. Hence, it should not be used as a direct comparison to Table <ref type="table" target="#tab_4">VII</ref>.</p><p>Nevertheless, there are probably no parametric rules to estimate how network implementation in FPGA scales up. Therefore, the statistics on hardware resource utilizations from the synthesis reports, as shown in Tables VII and VIII, should be used to estimate hardware demands (for specific platforms) with growing network structure. Of course, for a particular platform, there would be resource limitation, putting constraints on the upper limit of the possible network size. However, a rough overview on the possible hardware requirements, e.g., in Tables VII and VIII, could be a rough guide in choosing the appropriate FPGA platform for any particular application and network structure requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Future Directions</head><p>Some interesting additions could bring the accuracy of this hardware-based network implementation closer to that of software-based ones. The first step in this direction would include the implementation of a momentum factor in the backpropagation training algorithm. The momentum factor speeds up learning and brings us to a better convergence by rewarding training that consistently reduces error and punishes any increases in error. This could not be covered in this work mainly because of hard time-limits for the thesis.</p><p>A second more complex step is the implementation of batch learning. Batch learning is a common approach in softwarebased implementations, and as we saw in Section V-B, effective in finding good convergence. When combined with a momentum factor, batch learning is particularly adept at fast and accurate convergence. The implementation of batch learning on an FPGA platform faces challenges in either calculating intermediate weights quickly enough in order not to cause buffering of the weight array and network output, or, implementing external memory to buffer the necessary data without slowing the system to the point that a software-based solution becomes a better choice.</p><p>Because of our primary interest to implement and test ANN with learning on FPGA, we skipped real-time input data communication to the FPGA, which would have required considerable effort to design such communication module. Such realtime online input data feeding is important to check how fast the FPGA can process new data. However, that is somewhat dependent on the clock frequency of the FPGA platform, not the direct focus of this work. This would be part of the future work, where we plan to test the ANN implemented in FPGA with real-time input data communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have presented the development and implementation of a parameterized FPGA-based architecture for feed-forward MLPs with backpropagation learning algorithm. Our architecture makes native prototyping and design space exploration in hardware possible. Testing of the system using the spectrometry sample application showed that the system can reach 530 million connections per second offline and 140 mil-lion online. These speeds represent comparable performance to other hardware-based MLP implementations. We have also confirmed that our system is capable of producing accurate convergence in training on a par close to MATLAB simulations. Statistics on hardware resource utilizations from the synthesis reports of different network sizes are presented. These could be used to estimate hardware demands (for specific platforms) with growing network structure.</p><p>Also presented was a new method for approximation of a sigmoid function in hardware. We showed that by applying a linear interpolation technique to a uniform LUT, we could significantly reduce the size of the necessary LUT, while maintaining the same degree of accuracy at the cost of implementing one adder and one multiplier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. MLP model.</figDesc><graphic coords="3,82.68,66.30,164.00,57.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Processing element.</figDesc><graphic coords="3,85.32,156.52,159.00,53.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Block view of the hardware architecture. Solid arrows show which components are always generated. Dashed arrows show components that may or may not be generated depending on the given parameters.</figDesc><graphic coords="4,79.26,66.98,168.00,154.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Graphical user interface to generate networks in MATLAB Neural Network toolbox [13].</figDesc><graphic coords="4,307.86,66.42,237.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Functional blocks of the PE component.</figDesc><graphic coords="5,56.88,66.42,216.00,204.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(in our example) where is the total data width, , for our implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Sigmoid function implemented on a uniform LUT. (b) Uniform LUT error distribution for a sigmoid function. (c) Example of partitioning for a variable resolution LUT.</figDesc><graphic coords="6,97.98,66.50,131.00,160.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The hyperbolic tangent function is approximated by finding the nearest indices of the LUT corresponding to the local field, then drawing a line between the values referenced by those indices. The quantization error is then used to find the corresponding point on the line segment LUT(d)LUT(d + 1).</figDesc><graphic coords="6,40.62,279.28,246.00,119.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Backpropagation implementation.</figDesc><graphic coords="6,307.26,66.42,238.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. State machines for network controller.</figDesc><graphic coords="7,92.64,86.32,144.00,167.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II AVERAGE</head><label>II</label><figDesc>ERROR OF APPROXIMATED HYPERBOLIC TANGENT FUNCTION USING A LUT WITH LINEAR INTERPOLATION AND A UNIFORM LUTTABLE III BIT RESOLUTION REQUIRED TO RESOLVE THE WORST CASE APPROXIMATION ERROR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV RMSE</head><label>IV</label><figDesc>OVER THE TEST SET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V RMSE</head><label>V</label><figDesc>OF AN FPGA-BASED NETWORK WITH SQUASHING FUNCTIONS IN THE HIDDEN LAYER IMPLEMENTED USING A LUT WITH LINEAR INTERPOLATION AND THE SAME NETWORK IMPLEMENTED USING UNIFORM LUTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI A</head><label>VI</label><figDesc>COMPARISON OF ANN PLATFORMS AND IMPLEMENTATIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII HARDWARE</head><label>VII</label><figDesc>RESOURCE UTILIZATION IN 12-BIT SPECTROMETRY APPLICATION USING XILINX XC5VSX50T-2FF1136</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VIII RESOURCES</head><label>VIII</label><figDesc>USED IN DIFFERENT NN IMPLEMENTATION AND APPLICATIONS</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for the constructive review, helping to upgrade the paper. This paper is based on work carried out by Alexander Gomperts at ABB Corporate Research, Switzerland from September 2008 to February 2009 under the supervision of A. Ukil and F. Zurfluh in partial fulfillment of the requirements of a Master's of Electrical Engineering degree from the Technische Universiteit Eindhoven, The Netherlands. Supervision on the side of the University was provided by L. Jz wiak. An internship preceded this thesis work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by ABB Corporate Research, Switzerland. Paper no. TII-10-05-0116. A. Gomperts is with Satellite Services B.V., 2201 DK, Noordwijk, The Netherlands (e-mail:</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Artificial neural networks: Fundamentals, computing, design, and application</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Basheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hajmeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Microbio. Methods</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3" to="31" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and statistical techniques: A review of applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems With Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural networks: Applications in industry, business and science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ukil</surname></persName>
		</author>
		<title level="m">Intelligent Systems and Signal Processing in Power Engineering</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compact hardware liquid state machines on FPGA for real-time speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>D'haene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="511" to="523" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A silicon model of early visual processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The LNeuro chip: A digital VLSI with on-chip learning mechanism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Theeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mauduit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sirat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Networks</title>
		<meeting>Int. Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="593" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of FPGA-based hardware implementation of ANNs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Networks Brain</title>
		<meeting>Int. Conf. Neural Networks Brain</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="915" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Special-purpose digital hardware for neural networks: An architectural survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cornu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Signal Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural networks in FPGAs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ormondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rajapakse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inform. Process</title>
		<meeting>Int. Conf. Neural Inform. ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J A</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<title level="m">An Introduction to Neural Networks, 4</title>
		<imprint>
			<date type="published" when="1991-09">Sep. 1991</date>
		</imprint>
	</monogr>
	<note>th ed. Amsterdam, the Netherlands: The University of Amsterdam</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FPGA implementations of neural networks-A survey of a decade of progress</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2778</biblScope>
			<biblScope unit="page" from="1062" to="1066" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MATLAB Neural Network Toolbox User Guide</title>
	</analytic>
	<monogr>
		<title level="j">The Math-Works Inc</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
			<pubPlace>Natick, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An IP core and GUI for implementing multilayer perceptron with a fuzzy activation function on configurable logic devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosado-Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Frances</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Universal Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1678" to="1694" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FPGA implementation of an adaptable-size neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. ANN, 1996</title>
		<meeting>Int. Conf. ANN, 1996</meeting>
		<imprint>
			<biblScope unit="volume">1112</biblScope>
			<biblScope unit="page" from="383" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ormondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rajapakse</surname></persName>
		</author>
		<title level="m">FPGA Implementations of Neural Networks</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards a platform for FPGA implementation of the MLP based back propagation algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Izeboudjen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bessalah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouridene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chikhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4507</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FPGA implementation of a pipelined on-line backpropagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gadea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Palero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Boluda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Signal Process</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="189" to="213" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Artificial neural network implementation on a single FPGA of a pipeline on-line backpropagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gadea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mocholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Syst. Synthesis</title>
		<meeting>Int. Symp. Syst. Synthesis</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A high bit resolution FPGA implementation of a FNN with a new algorithm for the activation function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ferreiraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ribeiroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="71" to="77" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a 2D-compatible multilayer neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FPGA-based stochastic neural network implementation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hutchings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop FPGAs for Custom Comput. Mach</title>
		<meeting>IEEE Workshop FPGAs for Custom Comput. Mach</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RRANN A hardware tation of the backpropagation algorithm using reconfigurable FPGAs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elredge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hutchings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE World Conf. Comput. Intell</title>
		<meeting>IEEE World Conf. Comput. Intell</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An FPGA implementation of an artificial neural network for prediction of cetane number</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frounchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Nia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Zariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asgarifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Comm. Eng</title>
		<meeting>Int. Conf. Comp. Comm. Eng</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="605" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Field programmable gate array (FPGA) based neural network implementation of motion control and fault diagnosis of induction motor drive</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Ind. Tech</title>
		<meeting>IEEE Conf. Ind. Tech</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FPGA-based implementation of an intelligent simulator for stand-alone photovoltaic system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mellit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mekki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Messai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Expert Systems With Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="6036" to="6051" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FPGA implementation of an ANN for detection of anthelmintics resistant nematodes in sheep flocks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahnamaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarimajd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Ind. Electronics Applications</title>
		<meeting>IEEE Conf. Ind. Electronics Applications</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1899" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Virtex-5 FPGA User Guide</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fixed point package</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bishop</surname></persName>
		</author>
		<ptr target="http://www.eda.org/vhdl-200x/vhdl-200x-ft/packages/fixed_pkg.vhd" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feasibility of floating-point arithmetic in FPGA based artificial neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Areibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Applications in Industry and Engineering</title>
		<meeting>Int. Conf. Computer Applications in Industry and Engineering</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient digital implementation of the sigmoid function for reprogrammable logic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tommiska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE Comput</title>
		<meeting>IEE Comput</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="403" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Finite precision error analysis of neural network hardware implementations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tecator data set</title>
		<ptr target="http://lib.stat.cmu.edu/datasets/tecator" />
		<imprint/>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved calibration of near-infrared spectra by using ensembles of neural network models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ukil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bonenfant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="578" to="584" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An Electrically Trainable Artificial Neural Network (ETANN) With 10240 &quot;Floating Gate&quot; Synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>IEEE Press</publisher>
			<biblScope unit="page" from="50" to="55" />
			<pubPlace>Piscataway, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A VLSI architecture for high-performance, lowcost, on-chip learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hammerstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. NN, 1990</title>
		<meeting>Int. Joint Conf. NN, 1990</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural network hardware performance criteria</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Keulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Withagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hegt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE World Congr</title>
		<meeting>IEEE World Congr</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1955" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
