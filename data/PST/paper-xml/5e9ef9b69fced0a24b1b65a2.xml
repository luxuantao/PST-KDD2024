<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3366423.3380259</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automated construction of topic taxonomies can benefit numerous applications, including web search, recommendation, and knowledge discovery. One of the major advantages of automatic taxonomy construction is the ability to capture corpus-specific information and adapt to different scenarios. To better reflect the characteristics of a corpus, we take the meta-data of documents into consideration and view the corpus as a text-rich network. In this paper, we propose NetTaxo, a novel automatic topic taxonomy construction framework, which goes beyond the existing paradigm and allows text data to collaborate with network structure. Specifically, we learn term embeddings from both text and network as contexts. Network motifs are adopted to capture appropriate network contexts. We conduct an instance-level selection for motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node. Extensive experiments on two real-world datasets demonstrate the superiority of our method over the state-of-the-art, and further verify the effectiveness and importance of instance-level motif selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Constructing high-quality topic taxonomies for document collections is an important task. A topic taxonomy is a tree-structured hierarchy, where each taxonomy node contains a set of semantically similar terms. A high-quality topic taxonomy benefits various downstream applications, such as search and indexing <ref type="bibr" target="#b44">[43]</ref>, personalized content recommendation <ref type="bibr" target="#b47">[46]</ref>, and question answering <ref type="bibr" target="#b43">[42]</ref>. For example, organizing copious scientific papers into a well-structured taxonomy gives researchers a bird's-eye view of the field, and then they can quickly identify their interests, and easily acquire desired information <ref type="bibr" target="#b36">[35]</ref>. A high-quality taxonomy for business reviews on Yelp<ref type="foot" target="#foot_0">1</ref> can facilitate more accurate recommendations and improve user's browsing experience.</p><p>Different applications usually require different taxonomies, therefore, automatic taxonomy construction capturing corpus-specific information becomes beneficial. The last decade has witnessed an explosive growth of digital document collections. By linking documents with their meta-data, we can view any document collection as a text-rich network. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, a collection of scientific papers can be viewed as a text-rich network with interconnected venue, author, term and paper nodes, and raw texts are associated with the paper nodes. Similarly, reviews from online platforms like Yelp and TripAdvisor<ref type="foot" target="#foot_1">2</ref> can be seen as a part of a text-rich network with nodes of businesses, users, and reviews.</p><p>While most existing methods solely rely on text data <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b45">44]</ref>, incorporating network structures can bring additional, valuable information to text. Let's use the computer science paper collection to convey our intuition. The term "frequent pattern" appears along with "transaction database" frequently. Judging only from text data, one may put this term into the database community. However, information embedded in the network structure, such as its associated venues (e.g., "SIGKDD") and authors (e.g., "Charu C. Aggarwal"), indicates the strong relatedness between the term "frequent pattern" and the data mining community, enabling us to assign it to the right taxonomy node. Acknowledging that network provides useful information for taxonomy construction, how to effectively integrate network and text remains a major challenge. We leverage motif patterns in our framework to extract useful features from the heterogeneous textrich network. Meta-paths <ref type="bibr" target="#b32">[31]</ref> and motif patterns <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b19">18]</ref> have been widely adopted to extract useful structural information from networks. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, motifs are subgraph patterns that capture higher-order connectivity and the semantics represented by these connections. We observe two issues of applying motif patterns in our problem. First, motif patterns are not created equal. Some motif patterns are more useful in identifying top-level concepts, while other motif patterns are better at differentiating finer concepts. Second, even only looking at one motif pattern, its motif instances are by no means equally informative. Some of them could even interfere the taxonomy construction, leading to a worse result. For example, using the motif pattern in Figure <ref type="figure" target="#fig_1">2</ref>(a) which captures co-authorship, some of its instances may be occasional and coincidental collaborations, thus will not help much when constructing the scientific taxonomy. To address these two issues, we propose a novel instance-level motif selection mechanism, which is specifically tailored to current node's granularity and semantics. We show in our experiments that such selection mechanism is crucial especially when the network is relatively noisy.</p><p>We propose NetTaxo, a hierarchical embedding and clustering framework for automatic topic taxonomy construction. The general workflow is sketched in Figure <ref type="figure" target="#fig_2">3</ref>. To begin with, we ask the user to provide a set of motif patterns as guidance. This set is never assumed to be clean and equally effective. At each taxonomy node, we propose to learn term embedding from both text and network data, and then apply a soft clustering method to obtain term clusters. We first obtain initial term clusters based on term embedding learned on text data. An inter-cluster comparative analysis is then conducted to select the most representative terms as anchor terms from each cluster. We make an assumption that a helpful motif instance should have the ability to separate one cluster's anchor terms from others. Building upon this assumption, we further distill the motif instances to include those that are relevant to the clustering, thus avoiding to introduce noise from network data. After that, we combine textual context and selected motif instances to learn term embedding jointly. Final clusters are then decided based on such joint embedding.</p><p>Experimental results demonstrate the success of our instancelevel motif selection. For example, we show that, for a collection of computer science papers, at the top level of the taxonomy construction, our method locates the venue of publication (e.g., "SIGKDD") as a strong indicator of research fields (e.g., "data mining"). Drilling down to lower levels of the taxonomy, our objective becomes to distinguish research sub-areas. Our proposed method identified specific author groups as more useful signals, such as "Cheng-Wei Wu" and "Philip S. Yu" -All their collaborations focus on the topic of high-utility itemset discovery.</p><p>To our best knowledge, this is the first work that bridges text and network data for automatic construction of topic taxonomy. Our contributions can be summarized as follows.</p><p>• We propose a novel topic taxonomy construction framework, NetTaxo, which integrates text data and network structures effectively and systematically. • We design an instance-level motif selection method to choose the appropriate information from network data. Moreover, it's adaptive to the granularity and semantics of each taxonomy node. • We conduct extensive experiments on real-world datasets to demonstrate the superiority of NetTaxo over many baselines and verify the importance and effectiveness of the instance-level motif selection. Reproducibility: Data and code packages can be found on the GitHub: https://github.com/xinyangz/NetTaxo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Hyponymy-based Methods. Taxonomies have been designed to group entities into hierarchies where each node is a concept term and each parent-child pair expresses a hyponymy (a.k.a. "is-a") relation (e.g., panda "is-a" mammal). In order to construct such taxonomies automatically, researchers have developed a number of pattern-based methods. Typically, these methods first acquire hyponymy relations from text data using lexical patterns (e.g., "A such as B"), and then organize the extracted pairs into a taxonomy by applying algorithms like maximum spanning tree. The lexical patterns are either manually designed <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b26">25]</ref> or derived from the corpus using some supervision or seeds <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b48">47]</ref>. Such patterns have demonstrated their effectiveness at finding hyponymy relations, however, they are not suitable for constructing a topic taxonomy as (1) each node in a topic taxonomy is a cluster of terms instead of a single concept term, and (2) pattern-based methods often suffer from low recall due to the large variation of expressions in natural language on hyponymy relations.</p><p>Recently, term embedding has been widely adopted in automatic topic taxonomy construction. A common practice is to first learn term embedding from text data and then organize them into a structure based on their representation similarity <ref type="bibr" target="#b5">[4]</ref> and cluster separation measures <ref type="bibr" target="#b8">[7]</ref>. Utilizing pairwise hyponymy relation labels, taxonomic relations between terms and clusters can be identified through supervised models, for example, semantic projection in the embedding space <ref type="bibr" target="#b12">[11]</ref> and neural network classifier <ref type="bibr" target="#b3">[2]</ref>. In our setting, there are no hyponymy labels.</p><p>Term Clustering-based Methods. A number of clustering methods have been proposed towards automatic topic taxonomy construction from text corpora. In pioneer studies, hierarchical topic modeling <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref> and bottom-up agglomerative clusteringbased <ref type="bibr" target="#b9">[8]</ref> methods are arguably the most popular and effective frameworks, before word embedding techniques become mature.</p><p>Among unsupervised frameworks using term embedding, topdown hierarchical clustering methods <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b45">44]</ref> achieve the state-ofthe-art. For example, TaxoGen <ref type="bibr" target="#b45">[44]</ref> learns local term embedding from the documents associated with a taxonomy node, and then clusters terms at a deeper level. Most of these methods, including TaxoGen, only utilize the information embedded in text data, but ignore the underlying network structures in the digital document collections. In our NetTaxo framework, we follow the top-down, local embedding approach but go beyond and leverage network structures to significantly improve the quality of clustering.</p><p>Network Clustering-based Methods. CATHYHIN <ref type="bibr" target="#b39">[38]</ref> is arguably the state-of-the-art method solely based on network structures for automatic topic taxonomy construction. Specifically, with unigram words as a part of its node set, it attempts to mine terms (i.e., phrases) and clusters simultaneously. It ignores the context of the words, thus sacrificing the abundant information embedded in the text data, yielding unsatisfactory results in our experiments.</p><p>Another related thread is the clustering algorithms on heterogeneous information networks (i.e., networks of typed nodes and edges) <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. For example, NetClus <ref type="bibr" target="#b34">[33]</ref> starts with user-provided seed nodes and applies authority ranking together with node clustering to cluster nodes. We adopt a similar authority ranking process as a part of our instance-level motif seleciton.</p><p>Network Motifs. Network motifs are higher-order subgraph structures that are critical in complex networks across various domains, such as neuroscience <ref type="bibr" target="#b31">[30]</ref>, bioinformatics <ref type="bibr" target="#b19">[18]</ref>, and information networks <ref type="bibr" target="#b6">[5]</ref>. In the context of heterogeneous information networks, network motifs, sometimes also referred to as meta-graphs, can offer more flexibility and capture richer network semantics than the widely used meta-path <ref type="bibr" target="#b32">[31]</ref> patterns. Recent studies have shown that incorporating motifs for node embedding leads to superior performance <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b46">45]</ref> compared to conventional path-based methods <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b28">27]</ref>. In this work, the quality of term embedding is the key to the overall quality of the constructed taxonomy. While taking advantage of network motifs in our embedding learning, we further select a subset of motif instances according to the current taxonomy node. This novel approach enables us to refine the rich semantics captured by network motifs, generating embedding better suited for taxonomy construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we first introduce the preliminary concepts and then formulate the problem by specifying the input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Definitions</head><p>A topic taxonomy is a tree-structured hierarchy H , where each node c ∈ H contains a small set of terms T c ⊂ T , which are semantically coherent and represent a conceptual topic. Moreover, the parent-child nodes in H should follow the topic-subtopic relation. That is, suppose a node c has a set of children S c = c 1 , c 2 , . . . , c n , then each c i (1 ≤ i ≤ n) should be a sub-topic of c and of the same granularity as its siblings in S c .</p><p>Note that, one term may belong to multiple conceptual topics and thus appear in multiple nodes. For example, "deep learning" could be a part of both "deep learning theory in machine learning" and "deep learning models in computer vision"; "data stream" could belong to "stream data indexing in database" and "stream data classification in data mining".</p><p>As mentioned before, a document collection with meta-data can be naturally viewed as a text-rich network, consisting of text data and network structure: • Text Data: A corpus D and a set of terms T . T includes terms in D, which can be either specified by users or extracted from the corpus. In our experiments, we form the term set T by extracting high-quality phrases from the corpus D using AutoPhrase <ref type="bibr" target="#b27">[26]</ref>.</p><p>• Network Structure: A heterogeneous information network G = (V , E, ϕ,ψ ), where V is the node set and E is the edge set. Type mapping ϕ and ψ map each node v to its type ϕ(v) and each edge e to a relation ψ (e).</p><p>A motif pattern Ω refers to a subgraph pattern at the meta level (i.e., every node is abstracted by its type). In this paper, we study only the motif patterns having at least one node of term type. A motif instance m is an instantiation of a motif pattern by replacing the node types with concrete values. Figure <ref type="figure" target="#fig_1">2</ref> presents some examples. We define "open" nodes as those single-degree nodes except for the term node, playing a role of connecting two terms. We say that two terms are connected following a motif pattern, if and only if both terms appear in motif instances sharing the same values at those "open" nodes. Therefore, we represent motif instances only by the values of "open" nodes. As an example, in Figure <ref type="figure" target="#fig_1">2</ref>(b), the motif instances linking to the terms "social network" and "information cascade" are the same. Both motif instances can be represented by the combination of two authors (i.e., "Jure Leskovec" and "Jon Kleinberg").</p><p>It is worth noting that meta-path <ref type="bibr" target="#b32">[31]</ref> can be viewed as a special case of motif patterns when they degenerate to lines. For example, the meta-path describing the shared venue relation between two terms is equivalent to the 2nd motif pattern in Figure <ref type="figure" target="#fig_1">2(c</ref>). The only "open" node in this motif pattern is the venue node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Formulation</head><p>In this paper, we aim to construct a topic taxonomy with a text-rich network as input. In addition, we ask user to provide a set of motif patterns as the guidance to incorporate information from network. However, the user-provided set can be noisy and we will conduct a motif instance-level selection later. Our goal is to construct a tree-structured taxonomy hierarchy H , i.e., a topic taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR FRAMEWORK</head><p>In this section, we describe our proposed NetTaxo framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>NetTaxo is a top-down, recursive framework. Our main goal is to allocate terms into sub-topics at each taxonomy node. The allocation module relies on term embedding that is jointly learned from textual and motif contexts. We use local embedding and motif instance selection to refine the textual and motif contexts respectively.</p><p>To support our local embedding and motif instance selection module, we associate every taxonomy node with a set of weighted documents. Specifically, we maintain a weight w c,d ∈ [0, 1] for each document d at the taxonomy node c. The weights are initialized to 1 for all documents in the root node. Alongside with term allocation, we also allocate documents from a taxonomy node to its children nodes. During the allocation process, we update w c,d for documents in the children nodes c 1 , c 2 , . . . , c n .</p><p>Figure <ref type="figure" target="#fig_2">3</ref> gives an overview of NetTaxo. At each taxonomy node, the system needs to determine the sub-topics, and then distribute terms and documents into its children accordingly. The key contribution of NetTaxo is our designed effective way of leveraging both text data and network structures.</p><p>Based on our observations and previous work <ref type="bibr" target="#b45">[44]</ref>, using term embedding learned from textual contexts alone can cluster subtopics roughly, although not necessarily perfect. Therefore, we decide to leverage such clustering results as the initialization to our subsequent motif instance selection step. Specifically, we first follow previous work <ref type="bibr" target="#b45">[44]</ref> to learn local term embedding and obtain initial term clusters. To be more accurate, we conduct a comparative analysis between clusters to select the most representative terms from each cluster to serve as anchor terms. Such anchor terms can be viewed as consolidated clustering information. Based on the anchor terms, we choose the appropriate motif instances. After that, we learn the term embedding jointly from the text data and the selected motif instances, which will in turn yield better clustering results. Before recursing to the next level, anchor terms chosen from the new clustering results are set as the final term set for this taxonomy node.</p><p>The details are presented in the remaining of this section. Sections 4.2 and 4.3 present how to learn term embedding from textual and motif contexts, respectively. Section 4.4 introduces anchor term selection method which is used multiple times across our framework. Section 4.6 discusses the joint term embedding after we introduce our motif selection technique in Section 4.5. Finally, Section 4.7 shows how to allocate terms and documents into child taxonomy nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local Embedding from Text Data</head><p>In NetTaxo framework, term embedding is the key to discover subtopic clusters at every taxonomy node.</p><p>Term embedding learning is typically conducted on the entire document collection <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b23">22]</ref>. However, such learning paradigm faces a major drawback in topic taxonomy construction: the discriminative power of learned term embedding becomes limited at deep levels. For example, term embedding learned from all computer science papers shall be able to distinguish "machine learning"related terms from terms in other research field. However, it may have difficulties in further discovering sub-topics under "machine learning", as those "machine learning"-related terms are already quite close to each other. This problem will only get worse as we drill down further. Therefore, it is a necessity to condition the term embeddings to the current taxonomy node.</p><p>To this end, we follow previous work <ref type="bibr" target="#b45">[44]</ref> and adopt the idea of local embedding <ref type="bibr" target="#b14">[13]</ref> to learn term embedding from text data. The basic idea of local embedding is to fine-tune term embedding at each node according to its own associated (weighted) documents. Its effectiveness has been verified in <ref type="bibr" target="#b45">[44]</ref> through ablation tests.</p><p>We use skip-gram with negative sampling (SGNS) <ref type="bibr" target="#b18">[17]</ref> as our base embedding model. At each taxonomy node, we use local documents D c instead of D for training. Similar to the original SGNS model, the objective is to maximize the probability of the local context given a term in a document. The loss function to minimize is given by:</p><formula xml:id="formula_0">L text = E d ∼P D (D c )       t ∈d t ′ ∈C(t ) −P(t ′ | t)       (1)</formula><p>C(t) stands for the set of terms within a context window of term t. We sample documents according to the multinomial distribution P D (D c ) parameterized by the document weights {w c,d } under the current taxonomy node. Therefore, our loss function slightly differs from the ones in the previous work <ref type="bibr" target="#b45">[44]</ref> as well as the original local embedding work <ref type="bibr" target="#b14">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Motif Instances as Term Contexts</head><p>We generalize the distributional hypothesis, which is fundamental in word embedding, to network by using motif instances. In text data, every word within sliding windows of a term is regarded as a part of its contexts. Similarly, a term's motif context is characterized by the set of motif instances, which can be matched based on the network structures around the term and the provided motif patterns. The network version of distributional hypothesis therefore becomes: terms with similar motif contexts are similar. Now we can generalize the SGNS embedding model to incorporate motif context. Specifically, we use each term to predict its motif context, generating the following loss term.</p><formula xml:id="formula_1">L motif = E d ∼P D (D c )       t ∈d E m∼ Mc (t ) − log P(m | t)       (2)</formula><p>where Mc (t) is the associated motif instances of term t. We will describe how to select Mc in section 4.5.</p><p>The probabilities are approximated with negative sampling <ref type="bibr" target="#b18">[17]</ref>.</p><formula xml:id="formula_2">log P(m | t) = log σ (r T m u t ) − E m∼P neg (m) log σ (−r T j u t )<label>(3)</label></formula><p>where r and u are embedding vectors of motif instances and terms and P neg (m) is the negative sampling distribution. In this way, term embedding can be also derived from network structures given the user-provided motif patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Anchor Term Selection</head><p>In order to provide more accurate initialization for the latter instancelevel motif selection module, we first introduce our anchor term selection method.</p><p>The goal of the anchor term selection is to find a concise, discriminative subset of terms from each cluster. It is a critical step for us to obtain clean semantics of a cluster, given that our vocabulary is large and noisy. For this very reason, we use anchor terms (1) as the initialization for our instance-level motif selection module, in which they provide more accurate initial clustering information;</p><p>(2) as input to the clustering algorithm, in order to find sub-topics under the current taxonomy node; and (3) as the final list of terms presented at each taxonomy node.</p><p>We formulate the anchor term selection as an unsupervised term ranking problem.</p><p>Ranking Principles. Given a specific taxonomy node, we define the anchor terms from the following criteria.</p><p>• Popularity: An anchor term should be popular enough at the given node. Very low frequency terms within a node do not contribute substantially to its semantics, thus are not considered representative. • Discriminativeness: An anchor term should be able to distinguish a node from its parent node and its sibling nodes. Discriminativeness is particularly critical in taxonomy scenarios, so analysts won't be confused by two similar taxonomy nodes during the navigation to find subsets of interest. Non-discriminative terms will appear in documents associated with many nodes and offer redundant and confusing information. For example, "extensive experiments" might be popular at both nodes about "data mining" and "database", thus being non-discriminative. • Informativeness: An anchor term should not be a stopwordlike term. As the taxonomy construction goes deeper and deeper, some terms become less and less informative. For example, "data mining" is an informative term at the node representing the "computer science" field, but has much less information at the node focusing on "frequent pattern mining".</p><p>Bearing these principles in minds, we design the following scoring functions accordingly.</p><p>Popularity Score. We denote the number of occurrences of the term t in the document d as tf(t, d). As the documents are weighted, term frequency is weighted by the importance of the document.</p><p>Given the document weights w c,d , we define the popularity of the term t at the node c as</p><formula xml:id="formula_3">pop(c, t) = d ∈D w c,d • tf(t, d) d ∈D w c,d • |d |<label>(4)</label></formula><p>where |d | represents the total number of terms in the document d. This formula captures the relative weighted term frequency of the term t at the node c.</p><p>Discriminativeness Score. A discriminative term t at the taxonomy node c should have a significantly larger relative weighted term frequency at the node c than that at its parent node p c or other sibling nodes c ′ 1 , c ′ 2 , . . . , c ′ m . Therefore, we define the following ratio to capture this intuition.</p><formula xml:id="formula_4">discriminative(c, t) = pop c,t max{pop p c ,t , max m i=1 pop c ′ i ,t }<label>(5)</label></formula><p>The larger discriminative(c, t) should imply a better anchor term candidate. When discriminative(c, t) is smaller than 1, it is unlikely that t is a good choice of an anchor term at taxonomy node c.</p><p>Informativeness Score. Inverse document frequency (IDF) has been widely adopted in information retrieval to measure the informativeness of a term within a given corpus <ref type="bibr" target="#b30">[29]</ref>. At each taxonomy node c, we calculate the weighted inverse document frequency as follows.</p><p>idf</p><formula xml:id="formula_5">(c, t) = log d ∈D w c,d d ∈D I(t ∈ d) • w c,d<label>(6)</label></formula><p>where I(t ∈ d) is a boolean indicator function about whether the term t appears in the document d.</p><p>Combined Anchor Score. As an unsupervised ranking problem, we follow the previous comparative analysis work <ref type="bibr" target="#b37">[36]</ref> and use a geometric mean to combine these three signals.</p><formula xml:id="formula_6">anchor_score(c, t) = pop(c, t) • discriminative(c, t) • idf(c, t)</formula><p>1/3 <ref type="bibr" target="#b8">(7)</ref> In summary, at each taxonomy node c, we will rank the terms based on the anchor scores and pick the top K t terms as anchor terms. We expect these anchor terms can express clear semantics of the topic at each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Instance-Level Motif Selection</head><p>So far we have already shown how to learn term embedding separately from text and motif using local corpus. Trivially putting them together, however, gives sub-optimal performance based on our observation. As discussed before, motif instances should be weighed accordingly at each taxonomy node during the construction process. Specifically, based on anchor terms selected from initial clusters, we further narrow down a set of useful motif instances. This instancelevel motif selection step is designed to make the collaboration between text and network more effective.</p><p>We identify two principles for instance-level motif selection: • Importance: The motif instance should be associated with a set of important terms, providing useful information for term embedding learning. • Concentration: The motif instance should be concentrated on one or a small number of sub-topics under the current taxonomy node, thereby including it will help us better separate sub-topics.</p><p>We realize these two principles by applying authority ranking <ref type="bibr" target="#b33">[32]</ref> upon the motif context graph.</p><p>The motif context graph at a taxonomy node c is a bipartite graph G M c = (T c , M c ,W ), where T c is the terms under the current taxonomy node and M c is the set of motif instances. We use the notation G M c to avoid the ambiguity of mixing this graph with the network structure G. Note that we exclude motif instances which do not include any term or document under the current taxonomy node. The bipartite graph connects each term to the motif instances it occurs in. The weight matrix W ∈ R | T c |× | M c | describes the number of occurrences of term t in each motif instance m (i.e., W t,m ).</p><p>We apply authority ranking to obtain importance scores between each motif instance and each cluster. In the ranking process, we maintain two matrices I T ∈ R | T c |×n and I M ∈ R | M c |×n to store the importance scores of terms and motif instances. Each row of the matrix denotes the importance scores of a specific term (or a motif instance) under all n clusters. As initialization, we set I (0) T (t, k) = 1/K t for all anchor terms in all clusters and zero for all other terms. This is based on the assumption that all anchor terms are important in the first place. The authority ranking is an iterative importance propagation process. Specifically, in each iteration,</p><formula xml:id="formula_7">I (t ) M ← W T I (t −1) T , I<label>(t )</label></formula><formula xml:id="formula_8">T ← W I (t ) M W = D 1/2 r W D 1/2</formula><p>c is the normalized weight matrix with row degree D r and column degree D c matrices. The iterative process can be repeated to a max iteration number or until convergence. In practice, we found that 5 iterations are enough to achieve good results.</p><p>For each motif instance m, we take the mean of its importance score across different clusters as the overall importance. importance(m) = mean I M (m, •) Moreover, with the importance scores on different clusters, we can measure the concentration of a motif instance m based on entropy.</p><formula xml:id="formula_9">concentration(m) = 1 − 1 log n n i=1 Ĩ M (m, i) log Ĩ M (m, i)</formula><p>We use normalized entropy here to keep its range in 0 to 1. Ĩ M denotes I M after row normalization. We rank all motif instances based on their final scores, and select a subset Mc of the instances ranked in the top K m percent. Note that, the motif instance ranking is across all motif patterns. Therefore, we are implicitly selecting motif patterns by pruning most of instances from uninformative motif patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Joint Embedding from Textual and Motif Contexts</head><p>At each taxonomy node c, given the local corpus D c and locally selected motif instances Mc , we refine term embedding by joint embedding training of text and motif instances. Specifically, putting text and motif together, we minimize the joint loss function:</p><formula xml:id="formula_10">L = λL text + (1 − λ)L motif<label>(8)</label></formula><p>We use λ to balance text and motif losses. In our implementation, we optimize the loss function with stochastic gradient descent, and approximate the expectations in previous equations using sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Term and Document Allocation</head><p>With the joint embedding trained on text and motif instances, we are ready to allocate terms and documents into children nodes. In principle, our method is flexible in the choice of clustering method. Consider that cosine similarity between term embedding has demonstrated its effectiveness in term similarity search <ref type="bibr" target="#b18">[17]</ref>, we apply vMF mixture clustering <ref type="bibr" target="#b4">[3]</ref> in NetTaxo. It is a classical, effective soft clustering method on the unit hyper-sphere. Since the constructed topic taxonomy rarely changes, we leave the choice of k, the number of topics, to human experts.</p><p>It is worth noting that we fit the vMF distributions only on anchor terms of the current taxonomy node. The rationale is that the automatically extracted term vocabulary is often noisy, while anchor terms selected from comparative analysis are much cleaner, which makes the clustering more accurate. After fitting the vMF mixture model, each cluster is represented by a vMF distribution in the embedding space. We then use these distributions to estimate the clustering probability of each term in T c . Finally, we allocate terms to children clusters.</p><p>For documents in D c , we estimate their clustering probability by aggregating clustering probability from their connected terms. This process is the same as that in <ref type="bibr" target="#b45">[44]</ref>. The aggregated probabilities of a document, multiplied by its current weight, will be the weights of the document on the next level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first introduce the experimental settings, including datasets, compared methods, and evaluation metrics. We then present quantitative evaluation results. In the end, we showcase parts of the constructed topic taxonomies as well as several interesting findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conduct our experiments on two real-world document collections: computer science papers in DBLP and business reviews in  Yelp. The statistics about the two datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. Details about the two datasets are as below.</p><p>• DBLP-5. The first document collection is from the AMiner dataset about computer science papers <ref type="foot" target="#foot_2">3</ref> . We select five closely-related research areas: (1) data mining, (2) database, (3) machine learning, (4) computer vision, and ( <ref type="formula" target="#formula_4">5</ref>) natural language processing. From these five areas, 79,896 papers are selected, containing 26,684 distinct terms. The network contains node types of author, venue, year, paper, and term (as available in this DBLP dataset). We augment the network by adding "year range" nodes, each representing a five consecutive years (e.g., 2010-2014). The text data, i.e., title and abstract, is associated with each paper node. The edges describe author-paper, venue-paper, year-paper, year range-paper, and term-paper relations. Note that, previous methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> choose five areas from this dataset too, for example in <ref type="bibr" target="#b45">[44]</ref>, information retrieval, computer vision, robotics, security &amp; network, and machine learning. In contrast, our chosen five areas are more closely related to each other, thus being more challenging. • Yelp-5. The second document collection is from the Yelp Dataset Challenge <ref type="foot" target="#foot_3">4</ref> . Since some baselines are too slow if we use the full dataset, we have to choose a subset of these reviews. Particularly, we choose the most popular state (i.e., Arizona) and the top-5 popular business categories (i.e., (1) automotive, (2) beauty &amp; spas, (3) hotels &amp; travel, (4) restaurants, and (5) shopping). We also remove rare businesses with less than 50 reviews. As a result, we obtain 1,308,371 reviews in total and extract 74,951 terms from them. We build the network using nodes of business, user, review, and term and edges of business-review, user-review, and term-review, as they are available in the meta-data. The text data, i.e., review comments, is associated with each review node.</p><p>We present all motif patterns used in DBLP-5 and Yelp-5 datasets in Figures <ref type="figure" target="#fig_4">2 and 4</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Methods</head><p>We compare our proposed methods with different types of topic taxonomy construction methods: (1) using text data, (2) using network data, and (3) using both text and network data. The details are listed below.</p><p>• HPAM++ is a method enhanced by us from the original Hierarchical Pachinko Allocation Model (HPAM) <ref type="bibr" target="#b20">[19]</ref>. HPAM is a state-of-the-art hierarchical topic model built upon the Pachinko Allocation Model using text data. Although it was designed to work on all unigrams, to make the comparison more fair, we improve the HPAM by focusing on only very high-quality phrases. Also, we set the topic numbers at different levels as the same as the numbers of clusters in NetTaxo. We have tested our enhanced Hierarchical Latent Dirichlet Allocation (HLDA) model <ref type="bibr" target="#b13">[12]</ref> and its performance is quite similar to HPAM++. Therefore, we only present the results of HPAM++ here. • TaxoGen <ref type="bibr" target="#b45">[44]</ref> is the state-of-the-art topic taxonomy construction method using text data. As demonstrated in its paper, it beats many strong baselines, such as hierarchical topic models <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref>. It utilizes the same local embedding idea as our model, but ignores network structures. • CATHYHIN++ is a method enhanced by us from the original CATHYHIN <ref type="bibr" target="#b39">[38]</ref> method. CATHYHIN <ref type="bibr" target="#b39">[38]</ref> is a topic taxonomy construction method using network data. It treats unigrams as nodes and attempted to mine terms (i.e., phrases) and clusters simultaneously. Its performance is limited due to (1) the poor phrase quality compared to the state-of-the-art method <ref type="bibr" target="#b27">[26]</ref> and</p><p>(2) the poor term clustering results compared to methods that use the term embedding technique. To make the comparison more fair, we improve the CATHYHIN by adding only very highquality phrases. • HClusEmbed is a baseline method that we propose using both text and network data. It is a straightforward solution to combine the term embedding technique with the network structure. Specifically, we first learn term embedding vectors from text using word2vec <ref type="bibr" target="#b18">[17]</ref> and network using LINE <ref type="bibr" target="#b35">[34]</ref> separately, where every embedding vector has a dimension of 300. And then, we concatenate the two vectors for each term, and then apply hierarchical spherical k-Means algorithm. We name this method as hierarchical topic clustering based on term and node embedding, and therefore denote it as HClusEmbed.</p><p>We denote our proposed method as NetTaxo. To demonstrate the necessity and effectiveness of our proposed motif instance selection, we introduce an ablated version of NetTaxo without this step, denoted as NetTaxo w/o Selection. Note that, in order to conduct a fair comparison, the same set of terms are used across different methods. They are the extracted from raw texts by the state-of-the-art distantly supervised phrase mining method <ref type="bibr" target="#b27">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameter Setting</head><p>The number of mixtures k for vMF mixture clustering is manually selected by incrementally increasing k by 1 in the range of <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b7">6]</ref> until coherent clusters are observed. We set k = 5 for the top level and k = 4 for the second level of the taxonomy in both the DBLP and Yelp dataset. In TaxoGen <ref type="bibr" target="#b45">[44]</ref>, this number is set to 5 for all levels, which is not far from our observation. Note that this parameter will only need to set once for a given dataset, so this process will not put a large burden on humans. For anchor term selection, we use K t = 50 for each cluster. For motif selection, we keep top K m = 10% of motif instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Tasks &amp; Metrics</head><p>Systematic evaluation of the constructed topic taxonomy has long been a very challenging task. Inspired by the state-of-the-art work on topic taxonomy construction <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref> and recent work on topic modeling <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref>, we design a set of tasks for human evaluation.</p><p>For each dataset, we recruited 10 in-domain human experts. In their annotation process, they were encouraged to use search engines (e.g., Google) to better understand unfamiliar terms. We identify the following aspects for judging the taxonomy quality, and then design three evaluation tasks accordingly.</p><p>• Coherence. Within each node in the taxonomy, the terms should be able to form a semantically coherent topic. Similar to previous topic model evaluations <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref>, we present the top-5 terms to human annotators from the same taxonomy node. Annotators are asked to first judge whether these terms form an interpretable topic. If not, all five terms at this node are automatically labeled as irrelevant. Otherwise, annotators are then asked to identify specific terms that are relevant to this topic. We define the coherence measure as the ratio of the number of relevant terms over the total number of presented terms. • Exclusive Siblings. Besides the coherence, each taxonomy node should be distinguishable from its sibling nodes. Following previous taxonomy construction methods <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b45">44]</ref>, we perform the term intrusion test. Specifically, for each node, we collect its top-5 terms, and then randomly mix in an intruder term from the top-5 terms of its sibling nodes. We present the 6 terms in a random order and ask human annotators to identify the only intruder term. The more coherent and distinctive the topics are, the easier it is for human to spot intruder terms. We define the sibling exclusiveness as the successful identification ratio in this test. • Quality Parent-Child Relations. Each taxonomy node should be an appropriate sub-topic of its parent node. Considering the huge vocabulary size, it is difficult to enumerate all children terms of a given topic, and further evaluate the relation quality. We instead use a sampling-based method for evaluation. Specifically, between two adjacent levels in a taxonomy, we first sample a child term t from lower-level nodes, and present t together with all upper-level (i.e., parent-level) nodes. Each upper-level node is visualized using its top-10 terms. We ask human annotators to mark all reasonable parent nodes of the child term t, which is denoted as P(t). We merge the parent nodes of term t that identified by the model into a set P * (t). Precision, recall, and F1 are employed to evaluate P(t) against P * (t) by treating all sampled together. Formally, we have</p><formula xml:id="formula_11">Precision = t | P(t) ∩ P * (t)| t |P * (t)| Recall = t | P(t) ∩ P * (t)| t | P(t)|</formula><p>and F 1 is defined as their harmonic mean. A quality topic taxonomy should have high scores in all the three evaluation tasks.</p><p>Annotation Details. First of all, it is worth mentioning that we mix the results from different methods together and shuffle them randomly before sending them to annotators. The annotators will not be aware of the method from which the results are produced.</p><p>Second, in order to avoid bias during the annotation, we first ask the annotators to do the Exclusive Siblings task, then the Parent-Child Relations task, and finally theCoherence task. So the annotator will not have any prior knowledge about which terms are in the same taxonomy node in the first two tasks.</p><p>In all tasks, we observe that annotators have inter-annotator agreements of more than 90%. The scores presented in the experiments are therefore all averaged across different annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Quantitative Evaluation</head><p>In this section, we discuss the quantitative evaluation results of different methods on the two datasets. The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. Overall, the topic taxonomy constructed by NetTaxo has demonstrated its significant advantage over taxonomies constructed by other methods, in all three evaluated aspects.</p><p>The two datasets have slightly different properties as text in the DBLP dataset is written in a more formal style and thus consists of cleaner terms while Yelp reviews often contain colloquial language. In terms of the underlying taxonomy, the Yelp taxonomy spans from very high-level distinctions (e.g., auto repairs vs. restaurants) to subtle distinctions (e.g., fusion dishes should belong to multiple nodes while common products do not fit into any node). The DBLP taxonomy has much fewer cases of ambiguity. This leads to closer but generally lower scores on the Yelp dataset for most metrics.</p><p>Within two methods using text data only, TaxoGen outperforms HPAM++ in all evaluated aspects. Compared with TaxoGen, NetTaxo improves most on the identification of parent-child relations. The network information provides a better overview of the hierarchical topic structure, whereas parent term and child terms often share the  Each node is visualized as a rectangle block and its top-10 anchor terms. Arrows go from parent nodes to child nodes. In addition, at the first level, for each motif pattern, we show the percentage of selected instances over all instances of the motif.</p><p>same context in documents. This shows that the network structure truly provides complementary information to text. Compared with CATHYHIN++, NetTaxo shows significant improvements in sibling exclusiveness and coherence. By initializing clusters using term embedding, we are able to better capture semantically similar terms for creating coherence clusters. The increase in sibling exclusiveness can be credited to the comparative analysis component, which puts sibling nodes under contrast to discover anchor nodes. Both of these components are only available with text data, which CATHYHIN++ does not leverage.</p><p>In short, NetTaxo outperforms TaxoGen and CATHYHIN++ in all metrics, demonstrating that text and network information are able to enhance each other.</p><p>HClusEmbed takes the same input as NetTaxo, but performs very poor among the baselines. This shows that applying pre-existing embedding models on text and network separately and then putting them together trivially is not enough to generate a high quality taxonomy. In HClusEmbed, term embedding aim to preserve semantic similarity while network embedding aim to preserve node proximity, both may not directly contribute to a better taxonomy. Moreover, checking the results of NetTaxo w/o Selection, one can observe that only a careful selection of the information from network structures can lead to performance gains. This is more significant on the Yelp-5 dataset, as the network information is much more noisy in this dataset. NetTaxo is carefully designed to select the most relevant motif contexts from network structures, and then incorporate them into a joint term embedding learning to further improve the quality of constructed taxonomy.</p><p>These comparisons further confirm the importance and effectiveness of our proposed motif selection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Constructed Topic Taxonomies</head><p>After the quantitative comparison, we present some case studies on both datasets for a closer look at the topic taxonomy constructed by our proposed NetTaxo.  DBLP Taxonomy. We plot the final topic taxonomy in Figure <ref type="figure" target="#fig_5">5</ref>. Due to the space limit, we only present the five nodes at the first level and expand two of them into the second level.</p><p>Looking at the top level topics, one can easily recognize the topics of the five nodes from the left to right as: (1) natural language processing, (2) machine learning, (3) database, (4) computer vision, and (5) data mining. These are exactly the five areas used in preparing the DBLP dataset.</p><p>In addition, we present the selected motif instance percentage for each motif pattern at this top level in Figure <ref type="figure" target="#fig_5">5</ref>. The results are intuitive by putting more emphasis in venue-related motif patterns as well as the author-pair motif pattern. While we are conducting an instance-level motif selection, it actually implicitly selects motifs in pattern-level as well.</p><p>We then inspect the second-level results. Under the node about natural language processing, we can see four clear sub-topics: (1) parsing, (2) information extraction, (3) language &amp; grammar, and (4) machine translation. Under the node about data mining, we can find (1) social network analysis, (2) web mining and search, (3) frequent pattern/association rule mining, and (4) clustering.</p><p>Yelp Taxonomy. In Figure <ref type="figure" target="#fig_6">6</ref>, we present all four taxonomy nodes under the taxonomy node of "Asian Food" topic in our constructed Level 1: (*) -&gt; 5 Areas <ref type="bibr">ACL 1985</ref><ref type="bibr">-1989</ref><ref type="bibr">COLING 1985</ref><ref type="bibr">-1989</ref><ref type="bibr">EACL 1985</ref><ref type="bibr">-1989</ref><ref type="bibr">COLING 1980</ref><ref type="bibr">-1984</ref><ref type="bibr">ACL 1980</ref><ref type="bibr">-1984</ref><ref type="bibr">CIKM 2010</ref><ref type="bibr">-2014</ref><ref type="bibr">COLING 1990</ref><ref type="bibr">-1994 …</ref> Level 2: NLP -&gt; Sub-Areas Figure <ref type="figure">7</ref>: Top motif instances selected by NetTaxo at different taxonomy nodes. For venue + year range motif pattern, we merge consecutive instances for readability purpose if they are from the same venue and cover contiguous years. Three highlighted motif instances will be further elaborated with their most frequent terms in Figure <ref type="figure" target="#fig_7">8</ref>. topic taxonomy on the Yelp dataset. Top-10 terms are presented at each node. While "Asian Food" is already a relatively fine-grained topic, NetTaxo successfully recovers its sub-topics: Thai cuisine, Japanese cuisine, Chinese cuisine, and Other Asian (e.g., Indian, Mexican-Chinese Fusion, . . .) cuisines. The first three sub-topics are quite clear, while the fourth one is a little vague. Remember that we set k = 4 here. So it makes sense to have an "other" sub-topic. At the first glance, "Jade Red Chicken" and "Emrald Chicken" look like Chinese dishes, and "Jerk Fried Rice" sounds like something from the Caribbean area. However, if one searches "Jade Red Chicken" in Google, a popular restaurant in Arizona named "Chino Bandido" pops up at the first place. It offers Mexican-Chinese Fusion dishes and these three dishes are strongly recommended by Yelp reviewers 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Effects of Instance-Level Motif Selection</head><p>Besides the final taxonomy quality, we're also interested in how the instance-level motif selection mechanism works at different taxonomy nodes. We visualize top motif instances selected by our method on the DBLP dataset. Figure <ref type="figure">7</ref> shows two motif patterns and their top instances at two taxonomy nodes, one from the first level and the other from the second level. Taking a closer look at three specific motif instances, we show most frequent terms linked to these motif instances in Figure <ref type="figure" target="#fig_7">8</ref>. On the first level, our goal is to identify major research fields, i.e., separating the 5 research areas in this dataset. From co-authorship motif pattern, we observe pairs of database researchers who share lots of research papers. The top-2 instances are all professors working in the same research group at the same university. From venueand-year-range motif pattern, one can find many computer vision and database conferences. The reason for these motif instances to rank high is because database and computer vision are two relatively concentrated research areas, compared to machine learning, data mining, and natural language processing (NLP) which have more 5 https://www.yelp.com/menu/chino-bandido-chandler/item/jade-red-chicken interconnections. Besides, professors and venues involved in the top-ranked instances are all highly reputed.</p><p>On the second level, the goal becomes more challenging -distinguishing research sub-areas. We use the NLP taxonomy node as an example. The co-authorship motif instances give us some less-known researchers. Therefore, we picked two of co-author pairs, sampled and visualized their associated terms from the motif context graph, shown in Figure <ref type="figure" target="#fig_7">8</ref>. One can easily observe that these author groups work on relatively concentrated sub-topics under NLP, i.e., sentiment analysis and machine translation, respectively. From venue-and-year-range motif instances, we can see major NLP conferences in their early years, and data mining conferences in recent years. This is also quite interesting but explainable, as NLP conferences have a narrower scope in their early years, while the data mining community, as it evolves, has more overlaps with the NLP community recently. Specifically, we show most frequent terms linked to the motif instance "CIKM 2010-2014" in Figure <ref type="figure" target="#fig_7">8</ref>, where we observe many NLP sub-topics such as "question answering" and "information extraction". These topics are also studied by information retrieval and data mining researchers recently.</p><p>Overall, from the empirical observations, we can verify that our instance-level motif selection is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS &amp; FUTURE WORK</head><p>In this paper, towards automatic topic taxonomy construction, we propose a novel hierarchical term embedding and clustering framework NetTaxo, which consumes a text-rich network as the input. Through a careful selection of motif contexts, NetTaxo learns term embedding jointly from the text data together with the most helpful network structures. To consolidate the foundation of such selection, we further design a method to choose anchor terms from the initial clusters based on text data only. Extensive experiments on two datasets demonstrate the superiority of our framework compared with baselines. Ablation experiments confirm the necessity and effectiveness of our proposed instance-level motif selection. Case studies illustrate the quality of our constructed taxonomy.</p><p>In future work, we would like to further improve NetTaxo in the following aspects. First, we would like to develop a more principled solution to determine the number of sub-topics at each taxonomy node. Second, incorporating user-provided seed examples of the desired taxonomy in construction process could be a promising and practically useful direction to pursue. Last but not least, we are interested in integrating our constructed taxonomy into downstream applications, such as recommender systems and question answering tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Document collections with meta-data can be viewed as text-rich networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example Motif Patterns and Motif Instances. (a) This motif pattern suggests that two terms are similar when they are from the papers published by the same author pairs. (b) The two terms are connected by a motif instance of the motif patterns in (a), which has two authors instantiated. The shades indicate two full instantiations of the motif pattern. (c) The other motif patterns that we used in the DBLP-5 dataset, including meta-path shaped patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An overview of NetTaxo, which learns term embedding jointly from textual and motif contexts. We conduct a motif instance-level selection to pick the most informative network structures for better topic taxonomy construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Finally, we define</head><label></label><figDesc>the final score of a motif instance m as motif_score(m) = importance(m) • concentration(m) 1/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: All motif patterns used in Yelp-5 dataset. The most complex pattern indicates a term mentioned by two users under the same business.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The Topic Taxonomy Constructed by NetTaxo on the DBLP Dataset. Due to the space limit, it is partially expanded.Each node is visualized as a rectangle block and its top-10 anchor terms. Arrows go from parent nodes to child nodes. In addition, at the first level, for each motif pattern, we show the percentage of selected instances over all instances of the motif.</figDesc><graphic url="image-61.png" coords="9,54.20,178.84,76.61,70.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The Topic Taxonomy Constructed by NetTaxo on the Yelp Dataset. All sub-topics under the taxonomy node about "Asian food" are visualized.</figDesc><graphic url="image-87.png" coords="9,371.80,330.23,59.37,81.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Most frequent terms linked to the three highlighted motif instances in Figure 7. Note that the frequency is calculated based on the weighted documents associated with the taxonomy node about NLP. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics. Motifs patterns in DBLP-5 and Yelp-5 datasets are visualized in Figures2 and 4, respectively.</figDesc><table><row><cell></cell><cell>#doc</cell><cell>#term</cell><cell>#node</cell><cell>#edge</cell><cell>#motif</cell></row><row><cell>DBLP-5</cell><cell>79,896</cell><cell>26,684</cell><cell>182,290</cell><cell>1,897,226</cell><cell>5</cell></row><row><cell>Yelp-5</cell><cell cols="4">1,308,371 74,951 1,760,025 6,809,152</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>business</cell><cell></cell></row><row><cell></cell><cell>business</cell><cell>user</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>user 1</cell><cell></cell><cell>user 2</cell></row><row><cell>review</cell><cell>review</cell><cell>review</cell><cell>review 1</cell><cell></cell><cell>review 2</cell></row><row><cell>term</cell><cell>term</cell><cell>term</cell><cell></cell><cell>term</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Evaluations. Scores are averaged over 10 annotators.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DBLP-5</cell><cell></cell><cell></cell><cell></cell><cell>Yelp-5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Coherence</cell><cell>Sibling</cell><cell cols="3">Parent-Child Relations</cell><cell>Coherence</cell><cell>Sibling</cell><cell cols="3">Parent-Child Relations</cell></row><row><cell></cell><cell>Measure</cell><cell cols="3">Exclusiveness Precision Recall</cell><cell>F 1</cell><cell>Measure</cell><cell cols="3">Exclusiveness Precision Recall</cell><cell>F 1</cell></row><row><cell>HPAM++</cell><cell>0.796</cell><cell>0.680</cell><cell>0.348</cell><cell>0.451</cell><cell>0.393</cell><cell>0.832</cell><cell>0.740</cell><cell>0.171</cell><cell>0.247</cell><cell>0.202</cell></row><row><cell>TaxoGen</cell><cell>0.840</cell><cell>0.740</cell><cell>0.780</cell><cell>0.713</cell><cell>0.745</cell><cell>0.920</cell><cell>0.800</cell><cell>0.650</cell><cell>0.618</cell><cell>0.633</cell></row><row><cell>CATHYHIN++</cell><cell>0.880</cell><cell>0.533</cell><cell>0.850</cell><cell>0.744</cell><cell>0.793</cell><cell>0.742</cell><cell>0.420</cell><cell>0.705</cell><cell>0.638</cell><cell>0.670</cell></row><row><cell>HClusEmbed</cell><cell>0.624</cell><cell>0.420</cell><cell>0.525</cell><cell>0.409</cell><cell>0.460</cell><cell>0.744</cell><cell>0.560</cell><cell>0.655</cell><cell>0.610</cell><cell>0.632</cell></row><row><cell>NetTaxo w/o Selection</cell><cell>0.908</cell><cell>0.680</cell><cell>0.895</cell><cell>0.808</cell><cell>0.849</cell><cell>0.816</cell><cell>0.540</cell><cell>0.668</cell><cell>0.681</cell><cell>0.674</cell></row><row><cell>NetTaxo</cell><cell>0.912</cell><cell>0.880</cell><cell>0.898</cell><cell cols="2">0.810 0.852</cell><cell>0.928</cell><cell>0.854</cell><cell>0.790</cell><cell cols="2">0.825 0.807</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.yelp.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.tripadvisor.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://aminer.org/citation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://www.yelp.com/dataset/challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was sponsored in part by DARPA under Agreements No. W911NF-17-C-0099 and FA8750-19-2-1004, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, and DTRA HDTRA11810026. Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes not withstanding any copyright notation hereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">… Level 1: (*) -&gt; 5 Areas Level 2: NLP -&gt; Sub-Areas Hua Wu -Zhanyi Liu</title>
				<editor>
			<persName><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wenjie</forename><surname>Zhang -Xuemin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lin</forename><forename type="middle">Divesh</forename><surname>Srivastava -Nick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Koudas</forename><surname>Divyakant</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Agrawal -Amr</forename><surname>El</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abbadi</forename><surname>Lu Qin -Lijun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chang</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Zhang -Xuemin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lin</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qin -Jeffrey</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yu</forename></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callison-Burch</forename><surname>Zaidan -Chris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen -Roland</forename><surname>Boxing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Wu -Haifeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roland Kuhn -George</surname></persName>
		</author>
		<title level="m">Foster Yoan Gutiérrez -Andrés Montoyo John Makhoul -Richard M. Schwartz … Y1-Y2 term venue CVPR 2000-Present SIGMOD 2010</title>
				<imprint>
			<date type="published" when="1990">2014 SIGMOD 1990-2004 ICCV 2004-2014 VLDB 2005-2009 ICDE 2010-2014 EDBT 2005-2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM conference on Digital libraries</title>
				<meeting>the fifth ACM conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning term embeddings for taxonomic relation identification using dynamic weighting neural network</title>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See</forename><forename type="middle">Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von Mises-Fisher distributions</title>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1345" to="1382" />
			<date type="published" when="2005-09">2005. Sep (2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A cluster separation measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="1979">1979. 1979</date>
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparing conceptual, divisive and agglomerative clustering for learning taxonomies from text</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lopez De Mantaras</surname></persName>
		</author>
		<author>
			<persName><surname>Saitia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Artificial Intelligence Conference Proceedings</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient methods for inferring large sparse topic hierarchies</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="774" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Expert Finding in Heterogeneous Bibliographic Networks with Locally-trained Embeddings</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03370</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational</title>
				<meeting>the 14th conference on Computational</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metapad: Meta pattern discovery from massive text corpora</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lance M Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="877" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representing documents via latent keyphrase inference</title>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on World wide web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 25th international conference on World wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1057" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mixtures of hierarchical topics with pachinko allocation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PATTY: a taxonomy of relational patterns with semantic types</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TAXI at SemEval-2016 Task 13: a taxonomy induction method based on lexico-syntactic patterns, substrings and focused crawling</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Naets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cédrick</forename><surname>Fairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Paolo Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1320" to="1327" />
		</imprint>
	</monogr>
	<note>SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deriving a large scale taxonomy from Wikipedia</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1440" to="1445" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-GNN: Metagraph Neural Network for Semi-supervised learning in Attributed Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Large DataBase of Hypernymy Relations Extracted from the Web</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Seitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Meusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponzetto</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Meta-path guided embedding for similarity search in large-scale heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09769</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HiExpan: Task-Guided Taxonomy Construction by Hierarchical Tree Expansion</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Kötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motifs in brain networks</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">e369</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rankclus: integrating clustering with ranking for heterogeneous information network analysis</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology</title>
				<meeting>the 12th International Conference on Extending Database Technology: Advances in Database Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="565" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ranking-based clustering of heterogeneous information networks with star network schema</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yintao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Doc2Cube: Allocating Documents to Text Cube without Labeled Data</title>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1260" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-Dimensional, Phrase-Based Summarization in Text Cubes</title>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">Wang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Lance R Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A phrase mining framework for recursive construction of a topical hierarchy</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihit</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thrivikrama</forename><surname>Taula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="437" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Constructing Topical Hierarchies in Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihit</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2013 IEEE 13th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6874</idno>
		<title level="m">Integrating document clustering and topic modeling</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incorporating word correlation knowledge into topic modeling</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference of the north American chapter of the association for computational linguistics: human language technologies</title>
				<meeting>the 2015 conference of the north American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-graph based hin spectral embedding: Methods, analyses, and insights</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficiently Answering Technical Questions-A Knowledge Graph Approach</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3111" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Building taxonomy of web search intents for name entity queries</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2701" to="2709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Meta-Graph2Vec: complex semantic path augmented heterogeneous network embedding</title>
		<author>
			<persName><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="196" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Taxonomy discovery for personalized recommendation</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
				<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stat-Snowball: a statistical approach to extracting entity relationships</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on World wide web</title>
				<meeting>the 18th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
