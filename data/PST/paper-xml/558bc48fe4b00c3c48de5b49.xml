<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection via Boundary Structure Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
							<email>toshev@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">GRASP Laboratory</orgName>
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
							<email>taskar@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">GRASP Laboratory</orgName>
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<email>kostas@cis.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">GRASP Laboratory</orgName>
								<orgName type="institution">University of Pennsylvania Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection via Boundary Structure Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25F6FAD52E5A7639A739CB23AF20BA2A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of object detection and segmentation using holistic properties of object shape. Global shape representations are highly susceptible to clutter inevitably present in realistic images, and can be robustly recognized only using a precise segmentation of the object. To this end, we propose a figure/ground segmentation method for extraction of image regions that resemble the global properties of a model boundary structure and are perceptually salient. Our shape representation, called the chordiogram, is based on geometric relationships of object boundary edges, while the perceptual saliency cues we use favor coherent regions distinct from the background. We formulate the segmentation problem as an integer quadratic program and use a semidefinite programming relaxation to solve it. Obtained solutions provide the segmentation of an object as well as a detection score used for object recognition. Our single-step approach improves over state of the art methods on several object detection and segmentation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decade a multitude of different object representations have been explored, ranging from texture and local features to region descriptors and object shape. Although local features based on image gradients and texture perform relatively well for some object classes, many classes are not modeled sufficiently by local descriptors. For objects with distinctive shape local texture features provide weak description. In this paper we focus on the problem of exploiting global shape properties for object detection and relating those properties to object segmentation.</p><p>Shape is commonly defined in terms of the set of contours that describe the boundary of an object. Complementary to gradient-and texture-based representations, shape is more descriptive at a larger scale, ideally capturing the object of interest as a whole. Hence, a large number of global representations, such as curvature scale space, Fourier contour descriptors, Zernicke moments, etc., have been studied <ref type="bibr" target="#b25">[25]</ref>. Unfortunately, such descriptions are very susceptible to clutter, due to spurious internal or background contours, and cannot be easily applied to real scenes. For this reason, a variety of local or semi-local descriptors have been studied, such as Shape Context <ref type="bibr" target="#b0">[1]</ref> or PAS <ref type="bibr" target="#b5">[5]</ref>, which capture the shape of only a part of an object outline and are often integrated in an additional global description.</p><p>In this work we adhere to the Gestalt school's view that shape is perceived not simply as a collection of parts and propose a recognition method based on a holistic shapeboundary based representation. To apply a boundary-based representation in cluttered images, precise figure/ground segmentation is necessary to select the object boundaries for the computation of the shape descriptor. However, accurate automatic segmentation of the object from realistic clutter is often extremely difficult without familiarity of the target shape <ref type="bibr" target="#b18">[18]</ref>. Evidence from human perception <ref type="bibr" target="#b19">[19]</ref> suggests that familiarity plays a large role in figure/ground assignment. We propose the Boundary Structure Segmentation (BoSS) model, which addresses the problem of recognition and segmentation simultaneously in a unified framework. While matching the image with an object model, our method selects a set of foreground regions such that (i) their global shape as a whole, defined in terms of their boundary structure, resembles the shape of the object model, and (ii) the foreground represents a coherent region distinct from the background (see Fig. <ref type="figure" target="#fig_0">1</ref>). The main contributions of our approach are threefold: Shape description. We introduce a global boundarybased shape representation, called chordiogram, which is defined as the distribution of all geometric relationships (relative location and normals) between pairs of boundary edges -called chords -whose normals relate to the segmentation interior. This representation captures the boundary structure of a segmentation as well as the position of the interior relative to the boundary. Moreover, the chordiogram is translation invariant and robust to shape deformations.</p><p>Figure/ground Segmentation. We match the above boundary structure while simultaneously extracting figure/ground segmentation. This is possible due to the definition of the chordiogram, which relates the object boundaries to its interior. The perceptual grouping component of the segmentation model, which is defined in terms of configural cues of salient contours, color and texture coherence, and small perimeter prior, ensures that the detections constitute salient regions. More importantly, the joint matching and segmentation removes the irrelevant image contours during matching and allows us to obtain correct object detections and segmentation in highly cluttered images.</p><p>Inference. We pose BoSS in terms of selection of superpixels obtained via an initial over-segmentation, which is a hard combinatorial problem. We propose a concise formulation as an integer quadratic program, consisting of two terms -a boundary structure matching term defined over superpixel boundaries, and a perceptual grouping term defined over superpixels. The terms are coupled via linear constraints relating the superpixels with their boundary. The resulting optimization problem is solved using a Semidefinte Programming relaxation and yields shape similarity and figure/ground segmentation in a single step.</p><p>We achieve state-of-the-art results on two challenging object detection tasks -94.3% detection rate at 0.3 fppi on ETHZ Shape Dataset <ref type="bibr" target="#b8">[8]</ref> and 92.4% detection rate at 1.0 fppi on INRIA horses <ref type="bibr" target="#b6">[6]</ref> as well as accurate object boundaries, evaluated on the former dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the large volume of literature on recognition and segmentation, we review approaches closest to our work. Global shape descriptors, such as Fourier contour descriptors, Zernicke moments, Curvature Scale Space, etc. <ref type="bibr" target="#b25">[25]</ref> have a long tradition in shape retrieval. However, they are applicable only for already segmented objects and cannot deal robustly with clutter. Semi-local shape descriptors have been proposed to address this limitation. Belongie et al. <ref type="bibr" target="#b0">[1]</ref> introduce shape context as a histogram of contour edges, capturing parts of an object. To perform recognition with shape context one needs to integrate it in a global matching framework such as thin plate spline or voting, for example. To alleviate further the issues arising from clutter, Zhu et al. <ref type="bibr" target="#b26">[26]</ref> select relevant object contours while matching shape contexts. Boundary fragments combined with a classifier and subsequent voting for object centers have been explored as well <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b21">[21]</ref>. These approaches are partbased and do not use global descriptors. Moreover, all of the above methods recover a set of object contours, but not the figure/ground organization of the image.</p><p>A different approach to shape-based recognition is to search for a set of image contours which best matches to a model. Ferrari et al. <ref type="bibr" target="#b8">[8]</ref> search in a contour network for contour chains which resemble the model. In a subsequent work Ferrari et al. <ref type="bibr" target="#b5">[5]</ref> define a descriptor for groups of adjacent contour segments and use it in conjuction with an SVM classifier. Lu et al. <ref type="bibr" target="#b14">[14]</ref> explore particle filtering to search for a set of object contours. Felzenswalb and Schwarz <ref type="bibr">[4]</ref> propose a hierarchical representation by decomposing a contour into a tree of subcontours and using dynamic programming to perform matching. Dynamic programming has been applied also by Ravishankar et al. <ref type="bibr" target="#b20">[20]</ref> in a mutli-stage framework to search for a chain of object contours. All of the above approaches have to deal with a combinatorial search among image contours and have to decompose their inference into tractable subproblems, thus losing some of the global relationships between contours. On the contrary, we retain in our descriptor all relations between object boundaries to achieve a holistic representation. Although the above approaches recover some object contours, none of them recover full figure/ground organization.</p><p>Close interplay between segmentation and recognition has been studied by <ref type="bibr" target="#b23">[23]</ref> who guide segmentation using part detections, but do not use global shape descriptors. Segment shape descriptors have been used by <ref type="bibr" target="#b10">[10]</ref> for detection and segmentation. Leibe et al. <ref type="bibr" target="#b13">[13]</ref> combine recognition and segmentation in a probabilistic framework. Recently, Gu et al. <ref type="bibr" target="#b11">[11]</ref> use global shape features on image segments. However, segmentation is a preprocessing step, decoupled from the subsequent matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Boundary Structure Segmentation Model</head><p>For a given target object mask and image, the BoSS model extracts a region in the image such that: (i) it constitutes a perceptually salient figure/ground organization of the image and (ii) resembles the model in shape. In addition, BoSS provides a detection score for the particular object model. To define the BoSSmodel, we denote by s ∈ R N a figure/ground segment indicator vector for an image partitioned into N superpixels: s i = 1 if superpixel i belongs to the figure; -1 otherwise. We define our model over superpixels since this provides computational advantages, however it can be defined in the same way over pixels. We decompose the model into matching and perceptual grouping terms:</p><formula xml:id="formula_0">E BoSS (s) = match(s, m) + group(s)<label>(1)</label></formula><p>In the following, we describe our shape representation and the terms of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Chordiograms as Shape Representation</head><p>To evaluate the similarity between a figure/ground segmentation and the model mask we use a global boundarybased shape descriptor, called the chordiogram. It is inspired by the Gestalt principle postulating that shape is perceived as whole <ref type="bibr" target="#b18">[18]</ref>, as well as by the success of contourbased shape descriptors <ref type="bibr" target="#b0">[1]</ref>.</p><p>To define a chordiogram, consider all possible pairs of boundary edges of a segmented object, called chords. Each chord captures the geometric configuration of two boundary edges, and their distribution can be used to describe the global shape. More precisely, for each chord (p, q), its configuration is described as: the length l pq and the orientation ψ pq of the vector connecting p and q as well as the orientations θ p and θ q of the normals to the segmentation boundary at p and q (see Fig. <ref type="figure" target="#fig_1">2,</ref><ref type="figure">left</ref>). The latter orientations are defined such that they point towards the object interior. Note that in this way we capture not only the boundary but also the object interior. Thus, the configuration features of a chord (p, q) can be written as: f pq = (θ p -ψ pq , θ q -ψ pq , l pq , ψ pq ) T , where the normal orientations are w. r. t. ψ pq . We describe the set of all configurations, by defining the chordiogram d as a K-dimensional histogram of the above features for all chords:</p><formula xml:id="formula_1">d k = #{(p, q)|f p,q ∈ bin(k)} k = 1 . . . K<label>(2)</label></formula><p>The lengths l pq are binned together in a log space, which allows for larger shape deformation between points lying further apart, while all the angles are binned uniformly.</p><p>In terms of the definition of the pair configurations, the above descriptor is similar to Shape Context <ref type="bibr" target="#b0">[1]</ref>, which captures the relation of contour edges only to a fixed offset and is not global. The lack of an offset makes our descriptor translation invariant; however, it is not scale or rotation invariant. The descriptor is also inspired by Carlsson <ref type="bibr" target="#b2">[2]</ref>, which captures topological properties of set of points.</p><p>Another important difference is that we capture the contour orientation relative to object interior. Orienting the boundary normals with respect to the interior contributes to better discrimination, for example, between concave and convex structures (configurations f pq and f p q respectively in Fig. <ref type="figure" target="#fig_1">2</ref>), which otherwise would be indistinguishable. The discriminative power can be seen on the right side of Fig. <ref type="figure" target="#fig_2">3</ref>, where objects of four different types are well separated using chordiograms, provided we compute it on segmented objects. If, however, we use all image contours inside the object bounding box, we obtain cluttered descriptors (Fig. <ref type="figure" target="#fig_2">3</ref>, left), which are much harder to separate. This motivates the coupling of the chordiogram with figure segmentation, as explained next. This coupling allows us to use descriptor support which covers the whole object, thus the descriptor is used globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Boundary Structure Matching</head><p>The matching term in Eq. ( <ref type="formula" target="#formula_0">1</ref>) compares the chordiograms of the model and an image segmentation. To formalize the matching model, we need to express the descriptor as a function of the object segmentation s. It will prove useful to provide an equivalent definition to Eq. ( <ref type="formula" target="#formula_1">2</ref>). Suppose the contribution of a chord (p, q) to the descriptor is denoted by a chord descriptors d pq ∈ {0, 1} K : (d pq ) k = 1 iff f pq ∈ bin(k). Then Eq. ( <ref type="formula" target="#formula_1">2</ref>) can be expressed as a linear function of the chord contributions: d = p,q d pq (see Fig. <ref type="figure" target="#fig_1">2</ref>, right). Hence, if we can express the selection of chord descriptors as a function of s, then we can express the chordiogram in terms of s. The main difficulty in the selection of chords lies, as we can see in Fig. <ref type="figure" target="#fig_3">4</ref>, in the fact that each chord can result in four different configuration features depending on the position of the object interior with respect to the chord edges: each edge has two possible normal orientations depending on the object interior.</p><p>To relate this phenomenon to the figure/ground segmentation, we express the descriptor in terms of a selection of segment boundaries, which are related to the figure in the image by assigning the boundaries to the segments comprising the figure. This is motivated by the idea of figure/ground organization of the image, where the figure is defined as re-  The first case for both variables corresponds to b + being part of the object and b -part of the background; the second is the opposite case. Then we can differentiate the aforementioned four cases for a configuration of a chord (p, q). Suppose, p and q lie on boundary segments b and c respectively. Then the chord descriptor of (p, q) resulting by selecting b k and c l as foreground is denoted by d kl pq , k, l ∈ {+, -}. This allows us to express the global chordiogram in terms of the chordiograms of the descriptors of individual chords based on selected boundaries t:</p><formula xml:id="formula_2">d(t) = b,c∈B p∈c,q∈b k,l∈{+,-} d kl pq t k b t l c (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>After we have parameterized the chordiogram, we chose to compare it with the model using L 1 distance:</p><formula xml:id="formula_4">match(t, m) = ||d m -d(t)|| 1<label>(5)</label></formula><p>subject to the constraints (3) and t = t + t - ∈ {0, 1} 2N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Perceptual Grouping</head><p>Optimizing the matching term in Eq. ( <ref type="formula" target="#formula_4">5</ref>) will result in a figure, defined by s and boundaries t, of maximal shape similarity to the model. However, we need to assure that s represents a perceptually salient segmentation, i. e. the resulting figure should be a coherent region distinct from the background. If we denote by w e,g the similarity between the appearance of superpixels e and g, then we can express the above condition by the standard graph-cut score:</p><formula xml:id="formula_5">-s T W s = -1 T W 1 + 2 e∈figure g∈ground w e,g<label>(6)</label></formula><p>where the first term is constant. We also expect that the most selected superpixel boundaries are supported by edge response in the image, i. e. they are are not hallucinated. For a boundary segment b, we denote by c b the percent of the pixels of b not covered by image edges extracted using thresholded Pb <ref type="bibr" target="#b16">[16]</ref>. Then the boundary cost is defined as</p><formula xml:id="formula_6">c T (t + + t -) = b∈B c b t + b + b∈B c b t - b<label>(7)</label></formula><p>Finally, we combine both costs:</p><formula xml:id="formula_7">group(s, t) = -βs T W s + γc T (t + + t -)<label>(8)</label></formula><p>for s ∈ {-1, 1} N and t + , t -∈ {0, 1} N . The total cost minimized by the BoSS model combines costs from Eq. ( <ref type="formula" target="#formula_4">5</ref>) and Eq. ( <ref type="formula" target="#formula_7">8</ref>)</p><formula xml:id="formula_8">min s,t ||d m -d(t)|| 1 -βs T W s + γc T (t + + t -) (9) s. t. t + b -t - b = 1/2(s b + -s b -) ∀b ∈ B (10) t + b t - b = 0 ∀b ∈ B (11) s ∈ {-1, 1} N , t + b , t - b ∈ {0, 1}<label>(12)</label></formula><p>Constraints ( <ref type="formula">10</ref>) and ( <ref type="formula">11</ref>) are equivalent to constraints (3), which can be easily verified for all four possible integer values of the variables t + b , t - b and variables s b + , s b -. In summary, the matching cost operates on the boundary indicators t, while the grouping cost is expressed in terms of superpixel indicators s. Both costs are made consistent via coupling constraints, which ensure that the resulting figure segmentation resembles in shape the given model and represents meaningful grouping in the image. Example We examine the contribution of each term of the model on one concrete example presented in Fig. <ref type="figure" target="#fig_5">5</ref>. By using only the matching term we are able to localize the object and obtain a rough mask, which however extends the back of the horse and ignores its legs (first column). The inclusion of the superpixel grouping bias helps to remove some of the erroneous superpixels above the object which have a different color than the horse (second column). Finally, if we add the boundary term, it serves as a sparsity regularization on t and results in a tighter segmentation (third column). Thus, the incorrect superpixels above the horse get removed, since they contain hallucinated boundaries not supported by edge response. Additionally, it recovers some of the legs, since they exibit strong edge response along their boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization via Semidefinite Program</head><p>The optimization of the integer quadratic program in Eq. ( <ref type="formula">9</ref>) is NP-hard. We chose Semidefinite Programing (SDP) to obtain relaxed solutions. For this purpose, we introduce two variables, which bring the quadratic terms of Eq. ( <ref type="formula">9</ref>) into linear form: T = tt T , S = ss T . This allows us to state the SDP relaxation as follows:</p><formula xml:id="formula_9">min S,T,s,t ||d m -d(T )|| 1 -βtr(W T S) + γc T (t + + t -) s. t. t b -t m+b = 1/2(s b + -s b -) ∀b ∈ B (13) T b,m+b = 0 ∀b ∈ B (14) diag(S) = 1 N (15) t b = T b,b , t m+b = T m+b,m+b ∀b ∈ B (16) T t t T 1 0, S s s T 1 0 (<label>17</label></formula><formula xml:id="formula_10">)</formula><p>where N is the number of superpixels and m = |B| the number of boundaries.</p><p>The above problem was obtained from problem (9) in two steps. First, we relax the constraints T = tt T and S = ss T to T tt T and S ss T respectively, which by Schur complement are equivalent to <ref type="bibr" target="#b17">(17)</ref>. Second, we weakly enforce the domain of the variables from the constraint <ref type="bibr" target="#b12">(12)</ref>. The -1/1-integer constraint on s is expressed as diagonal equality constraint on the relaxed S (see Eq. 15), which can be interpreted as bounding the squared value of the elements of s by 1. The 0/1-integer constraint (see Eq. ( <ref type="formula">16</ref>)) is enforced by requiring that the diagonal and the first row of T have the same value. Since T = tt T , this has the meaning that the elements of t are equal to their squared values, which is true only if they are 0 or 1. Finally, the coupling constraints ( <ref type="formula">10</ref>) and <ref type="bibr" target="#b11">(11)</ref>, one of which is quadratic, naturally translate to linear constraints ( <ref type="formula">13</ref>) and <ref type="bibr" target="#b14">(14)</ref>.</p><p>Discretization Discrete solutions are obtained by thresholding s. Since s has N elements, there are at most N different discretizations, all of which are ranked using their distance to the model. If a threshold results in a set of several disconnected regions, we consider all possible subsets of this set. The algorithm outputs the top 5 ranked nonoverlapping masks. Note that we are capable of detecting several instances of an object class since they result in several disconnected regions which are evaluated independently. Implementation Details We use chordiograms with 4 log bins for the distance feature with the largest bin equal to the diameter of the model. For all angles we use 8 equally spaced bins, resulting in 2048-dimensional descriptor.</p><p>To obtain superpixels we oversegment the image using NCuts <ref type="bibr" target="#b3">[3]</ref> with N = 45 segments. The grouping cues used to define the affinity matrix W pixels are color and intervening contours <ref type="bibr" target="#b24">[24]</ref> based on Pb <ref type="bibr" target="#b16">[16]</ref>. To define the segmentation term <ref type="bibr" target="#b8">(8)</ref> in our model we can use any affinity matrix. We choose to use the same grouping cues as for segmentation above. For each pair of superpixels k and l we average the pixel affinities to obtain an affinity matrix over the superpixels:</p><formula xml:id="formula_11">W superpixels kl = 1 a k a l p∈k,q∈l W pixels pq ,</formula><p>a k and a l being the number of pixels contained in k and l respectively. Above, W pixels is obtained from the top N eigenvectors E of W pixels : W pixels = EΛE T ≈ W pixels , where Λ are the corresponding eigenvalues. This low-rank approximation represents a smoothed version of the original matrix and reduces the noise in the original affinities. Finally, the weights of the term in Eq. ( <ref type="formula">9</ref>) were chosen to be β = 0.01 and γ = 0.6 on five images from ETHZ dataset and held constant for all experiments.</p><p>For the optimization we use SeDuMi <ref type="bibr" target="#b22">[22]</ref>. To compute the number of variables in the SDP, one can assume that each superpixel has at most C neighboring superpixels. Hence we obtain m = CN boundary variables. The total variable number in the relaxed problem is bounded by</p><formula xml:id="formula_12">N 2 + C 2 N 2 ∈ O(N 2 ).</formula><p>In our experiments, we have N = 45 and the value of C is less than 5 which results in less than 200 boundary segment variables. The segmentation of an image takes 5 -15 secs on a 3.50 GHz processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Detection In this section we present object detection results on two datasets. The ETHZ Shape Dataset <ref type="bibr" target="#b7">[7]</ref> consists of 255 images of 5 different object classes. The images are highly cluttered -in the background as well as internal spurious contours -and the objects vary in scale. The second dataset, INRIA horses, has 340 images, half of which contain horses. This dataset presents challenges not only in terms of clutter and scale variation, but also in articulation, since the horses are in different poses.</p><p>We apply BoSS on both datasets with same parameters (see sec. 4). We use hand-drawn object outlines as shape models. In particular, we use one model per class for the ETHZ Shape Dataset and 6 horse models representing different poses for the INRIA horse dataset (see Fig. <ref type="figure" target="#fig_7">7</ref> and<ref type="figure" target="#fig_8">9</ref>). For each image and model we run BoSS over several scales<ref type="foot" target="#foot_1">1</ref> to produce detection and segmentation hypotheses and score them based on the output of the matching from eq. ( <ref type="formula" target="#formula_4">5</ref>). We use non-maximum suppression -for every two hypotheses, whose bounding boxes overlap by more than 50%, we retain the one with the higher score and discard the other one.</p><p>On the ETHZ Shape Dataset we achieve 89.2%/90.5% detection rate at 0.3/0.4 fppi using Pascal criterion<ref type="foot" target="#foot_2">2</ref> and 93.4%/94.2% under 20% overlap criterion 2 , as reported in Table <ref type="table">1</ref> and Fig. <ref type="figure">6</ref>. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, our method is capable of detecting objects of various scales in highly cluttered images, even under occlusion (image 1, 6), as well as multiple instances (images 8,12). The major sources for incorrect detections are accidental alignments with background contours (image 16) and partially incorrect boundaries (in image 15 the mug is correctly detected, but glued to a background segment).</p><p>On INRIA Horses dataset, we achieve state of the art detection rate of 92.4% at 1.0 fppi (see Fig. <ref type="figure" target="#fig_6">8</ref>). Examples of detections of horses in different poses, scales and in cluttered images are shown in Fig. <ref type="figure" target="#fig_8">9</ref>.</p><p>Reranking In order to compare with approaches on ETHZ Shape dataset which use supervision, we use weakly labeled data to rerank the detections obtained from BoSS. We use only the labels of the training images to train a classifier but not the bounding boxes. This classifier can be used to rerank new hypotheses obtained from BoSS.</p><p>More precisely, we use half of the dataset as training and the other half as test (we use 5 random splits). We use BoSS to mine for positive and negative examples. The top detection in a training image using a model which represents the label of that image is considered a positive example; all other detections are negative examples. The chordiograms of these examples are used as features to train one-vs-all SVM <ref type="bibr" target="#b12">[12]</ref> for each class. During test time, each detection is scored using the output of the SVM corresponding to the model used to obtain this detection. Note that this is a different setup of supervision which requires less labelingwhile we need one hand-drawn model per class to obtain detections via BoSS, we do not use the bounding boxes but only the labels of the training images to score them. We argue that the effort to obtain a model is constant while segmenting images by hand is much more time consuming.</p><p>The results are shown in Table <ref type="table">1</ref>. The weak supervision leads to 94.3%/96.0% detection rate under Pascal criterion, which is an improvement of approx. 5% over BoSS. It is attributed to the discriminatively learned weights of the chordiogram's bins. This corresponds to discriminatively learning object shape variations and builds on the power of BoSS to deal with clutter. Segmentation In addition to the detection results, we evaluate the quality of the detected object boundaries and object masks. For evaluation of the former we follow the test settings of <ref type="bibr" target="#b7">[7]</ref> <ref type="foot" target="#foot_3">3</ref> . We report recall and precision of the detected boundaries in correctly detected images in Table <ref type="table">2</ref>. We achieve higher recall at higher precision compared to <ref type="bibr" target="#b7">[7]</ref>. This is mainly result of the fact that BoSS attempts to recover a closed contour and in this way the complete object boundary. These statistics show that the combination of shape matching and figure/ground organization results in precise boundaries (&gt; 87% for all classes except Giraffes). The slightly lower results for Giraffes is due to the legs which are not fully captured in the provided class models. We also provide object mask evaluation as percentage of the image pixels classified incorrectly by the detected mask (see Table <ref type="table">2</ref>). For all classes we achieve less than 6% error, and especially classes with small shape variation such as Bottles and Applelogos we have precise masks (&lt; 3% error).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce a model for joint object segmentation and detection. It is based on a global boundary-based shape descriptor, the chordiogram, which captures the boundary structure and relates it to the interior of an object. This allows us to combine the shape matching with figure segmentation and thus to deal with highly cluttered images. The model, solved using single-step optimization, achieves state of the art results on two detection benchmarks.  <ref type="table">1</ref>. Detection rates at 0.3/0.4 false positives per image, using the 20% overlap and Pascal criteria. We achieve state of the art results on all categories under the first detection criterion. Under the Pascal criterion, we achieve state of the art rates on the dataset as well. For Applelogos, Swans and Bottles, the results are equal to the ones using the weaker criterion. This is due to the exact localization, which can be achieved when segmenting the object. For Giraffes and Mugs results are slightly lower due to imperfect segmentation (some segments leak into the background or miss parts) -the detections which are correct under the weaker 20% overlap criterion, are not counted as correct which Pascal criterion. However, there are correctly segmented objects under the Pascal criterion which are ranked lower. The employed reranking helps to recover some of them. ( † use only hand labeled models. * use strongly labeled training data with bounding boxes, while we use weakly labeled data in the reranking, i. e. no bounding boxes. considers in the experiments only at most one object per image and does not detect multiple objects per image. • uses a slightly weaker detection criterion than Pascal.) Figure <ref type="figure">6</ref>. Results on ETHZ Shape dataset. Top: detection rate vs false positives per image; bottom: precision recall curves. Results using BoSS are shown using 20% overlap as well as after reranking using the stricter Pascal criterion. Both consistently outperform other approaches, evaluated using the weaker 20% overlap criterion. Method Det. rate BoSS 92.4% <ref type="bibr" target="#b15">[15]</ref> 85.3% <ref type="bibr" target="#b5">[5]</ref> 80.8% <ref type="bibr" target="#b7">[7]</ref> 73.8%   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Boundary Structure Segmentation: Holistic shape matching in highly cluttered images with simultaneous object segmentation.</figDesc><graphic coords="1,310.66,200.54,232.65,114.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Left: Example of a configuration feature fpq (see Sec. 3.1); Right: A chordiogram d of the figure segmentation (we plot only the length l and orientation ψ dimensions of the descriptor). d can be decomposed as the sum of the descriptors of individual chords (bottom right).</figDesc><graphic coords="3,51.91,72.00,232.65,73.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The top 2 principal components of chordiograms computed using PCA for objects in the ETHZ Shape dataset (see Sec. 5). (We omit the class 'Applelogos' for the sake of cleaner illustration ).</figDesc><graphic coords="3,310.66,72.00,232.65,63.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Suppose, b is the common boundary between superpixels s b + and s b -; c is the boundary between s c + and s c -. If b and c are selected as object boundaries, there are four possible selections of the neighboring superpixels and thus four possible configurations of the chord (p, q). The selection s can be equivalently represented in terms of the indicator variables t of the boundary segments b and c, as shown under the diagrams for each case.</figDesc><graphic coords="4,51.91,72.00,232.65,158.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) For an input image and model, as shown in the first row, our algorithm computes an object segmentation displayed in (a) row. We present three solutions by using only the matching term from Eq. (5) in first column; the matching term together with the superpixel segmentation prior (first cost in Eq. 8) in second column; and the whole cost function consisting of the matching, segmentation and the boundary term in third column. (b) We also show for the three cost combinations the relaxed values of the segmentation variable s.</figDesc><graphic coords="5,315.61,72.00,222.73,171.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Detection rate vs false positives per image (fppi) for our and other approaches on INRIA Horse dataset.</figDesc><graphic coords="7,50.11,529.32,141.08,111.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Example detection on ETZ Shape dataset. For each example, we show on the left side the selected superpixel boundaries, and on the right the selected object mask.</figDesc><graphic coords="8,62.49,72.00,470.24,210.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Examples of detections for INRIA horses dataset. For each image we show the selected superpixel boundaries on the left and the detected object segmentation on the right. Bottom right: 6 models used in the experiments.</figDesc><graphic coords="8,51.91,321.75,232.65,147.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Ravishankar et. al [20] †• 95.5%/97.7% 90.9%/92.7% 91.2%/93.4% 93.7%/95.3% 93.9%/96.9% 93.0%/95.2%</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>Apple logos</cell><cell>Bottles</cell><cell>Giraffes</cell><cell>Mugs</cell><cell>Swans</cell><cell>Average</cell></row><row><cell>overlap 20%</cell><cell>BoSS  † Lu et. al [14]  † Fritz et. al [9]  *  Ferrari et. al [6], [7]  †</cell><cell cols="6">95.5%/95.5% 96.4%/96.4% 93.4%/95.6% 84.8%/86.4% 97.0%/97.0% 93.4%/94.2% 92.5%/92.5% 95.8%/95.8% 86.2%/92.0% 83.3%/92.0% 93.8%/93.8% 90.3%/93.2% -/89.9% -/76.8% -/90.5% -/82.7% -/84.0% -/84.8% 84.1%/86.4% 90.9%/92.7% 65.6%/70.3% 80.3%/83.4% 90.9%/93.9% 82.4%/85.3%</cell></row><row><cell>Pascal criterion</cell><cell>BoSS  † BoSS + reranking  *  Maji et. al [15]  *  Gu et. al [11]  *</cell><cell cols="6">95.5%/95.5% 96.4%/96.4% 81.3%/84.6% 75.8%/78.8% 97.0%/97.0% 89.2%/90.5% 100%/100% 96.3%/97.1% 86.1%/91.7% 90.1%/91.5% 98.8%/100% 94.3%/96.0% 95.0%/95.0% 92.9%/96.4% 89.6%/89.6% 93.6%/96.7% 88.2%/88.2% 91.9%/93.2% 90.6%/-94.8%/-79.8%/-83.2%/-86.8%/-87.1%/-</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-4244-6985-7/10/$26.00 ©2010 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>For ETHZ Shape dataset we use 7 different scales, such that the scale of the model, defined as the diameter of its bounding box, range from 100 to 300 pixels. Similarly, for INRIA Horse dataset we used 10 scales ranging from 55 to 450 pixels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Pascal criterion: the intersection of the hypothesis and ground truth bounding boxes overlap more than 50% with the union of both; 20% overlap detection criterion: the intersection of the hypothesis and ground truth bounding boxes overlap more than 20% with the each of them.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>A detected boundary point is considered a true positive if it lies within t pixels of a ground truth boundary point, where t is set to</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>4% of the diagonal of the ground truth mask. Based on this definition, one computes recall and precision.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>boundary precision/recall pixel error BoSS Ferrari et. al [7] BoSS Applelogos 91.8%/97.5% 91.6%/93.9%</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Precision/recall of the detected object boundaries and pixel classification error of the detected object masks for ETHZ Shape dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Order structure, correspondence and shape based categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Shape, Contour and Grouping</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral segmentation with multiscale graph decomposition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Benezit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical matching of deformable shapes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate object detection with deformable shape models learnt from images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From images to shape models for object detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection by contour segment networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decomposition, discovery and detection of visual categories using topic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape based detection and topdown delineation using image segments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making large-scale svm learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape guided contour grouping with particle filters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection using a max-margin hough transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A boundary-fragmentmodel for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vision science: Photons to phenomenology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Must Figured-Ground Organization Precede Object Recognition? An Assumption in Peril</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-stage contour based detection of deformable objects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contour-based learning for object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Methods and Software</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object-specic figure-ground segregation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiclass spectral clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of mpeg-7 shape descriptors against other shape descriptors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contour context selection for object detection: A set-to-set contour matching approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
