<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Basing One-Way Functions on NP-Hardness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adi</forename><surname>Akavia</surname></persName>
							<email>akavia@mit.edu</email>
						</author>
						<author>
							<persName><roleName>MA</roleName><forename type="first">Mit</forename><surname>Cambridge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oded</forename><surname>Goldreich</surname></persName>
							<email>oded.goldreich@weizmann.ac.il</email>
						</author>
						<author>
							<persName><forename type="first">Shafi</forename><surname>Goldwasser</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dana</forename><surname>Moshkovitz</surname></persName>
							<email>dana.moshkovitz@weizmann.ac.il</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Basing One-Way Functions on NP-Hardness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3490157123989CB658F215DBCCAD2E86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>F.1.3 [Complexity Measures and Classes]: Reducibility and completeness</term>
					<term>Complexity hierarchies One-Way Functions</term>
					<term>Average-Case complexity</term>
					<term>Reductions</term>
					<term>Adaptive versus Non-adaptive machines</term>
					<term>Interactive Proof Systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the possibility of basing one-way functions on NP-Hardness; that is, we study possible reductions from a worst-case decision problem to the task of average-case inverting a polynomial-time computable function f . Our main findings are the following two negative results:</p><p>1. If given y one can efficiently compute |f -1 (y)| then the existence of a (randomized) reduction of N P to the task of inverting f implies that coN P ⊆ AM. Thus, it follows that such reductions cannot exist unless coN P ⊆ AM.</p><p>2. For any function f , the existence of a (randomized) non-adaptive reduction of N P to the task of averagecase inverting f implies that coN P ⊆ AM.</p><p>Our work builds upon and improves on the previous works of Feigenbaum and Fortnow (SIAM Journal on Computing,  1993)  and Bogdanov and Trevisan (44th FOCS, 2003), while capitalizing on the additional "computational structure" of the search problem associated with the task of inverting polynomial-time computable functions. We believe that our results illustrate the gain of directly studying the context of one-way functions rather than inferring results for it from a the general study of worst-case to average-case reductions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>One-way functions are functions that are easy to compute but hard to invert, where the hardness condition refers to the average-case complexity of the inverting task. The existence of one-way functions is the cornerstone of modern cryptography: almost all cryptographic primitives imply the existence of one-way functions, and most of them can be constructed based either on the existence of one-way functions or on related (but seemingly stronger) versions of this assumption.</p><p>As noted above, the hardness condition of one-way functions is an average-case complexity condition. Clearly, this average-case hardness condition implies a worst-case hardness condition; that is, the existence of one-way functions implies that N P is not contained in BPP. A puzzling question of fundamental nature is whether or not the necessary worst-case condition is a sufficient one; that is, can one base the existence of one-way functions on the assumption that N P is not contained in BPP.</p><p>More than two decades ago, Brassard <ref type="bibr" target="#b12">[12]</ref> observed that the inverting task associated with a one-way permutation cannot be N P-hard, unless N P = coN P. The question was further addressed in the works of Feigenbaum and Fortnow <ref type="bibr" target="#b15">[15]</ref> and Bogdanov and Trevisan <ref type="bibr" target="#b11">[11]</ref>, which focused on the study of worst-case to average-case reductions among decision problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Main Results</head><p>In this paper we re-visit the aforementioned question, but do so explicitly. We study possible reductions from a worstcase decision problem to the task of average-case inverting a polynomial-time computable function (i.e., reductions that are supposed to establish that the latter function is oneway based on a worst-case assumption regarding the decision problem). Specifically, we consider (randomized) reductions of N P to the task of average-case inverting a polynomialtime computable function f , and capitalize on the additional "computational structure" of the search problem associated with inverting f . This allows us to strengthen previously known negative results, and obtain the following two main results:</p><p>1. If given y one can efficiently compute |f -<ref type="foot" target="#foot_0">1</ref> (y)| then the existence of a (randomized) reduction of N P to the task of inverting f implies that coN P ⊆ AM.</p><p>The result extends to functions for which the preimage size is efficiently verifiable via an AM protocol. For example, this includes regular functions (cf., e.g., <ref type="bibr" target="#b20">[20]</ref>) with efficiently recognizable range.</p><p>Recall that AM is the class of sets having two-round interactive proof systems, and that it is widely believed that coN P is not contained in AM. Thus, it follows that such reductions cannot exist (unless coN P ⊆ AM).</p><p>We stress that this result holds for any reduction, including adaptive ones. We note that the previously known negative results regarding worst-case to averagecase reductions were essentially confined to non-adaptive reductions (cf. <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">11]</ref>, where <ref type="bibr" target="#b15">[15]</ref> also handles restricted levels of adaptivity). Furthermore, the result holds also for reductions to worst-case inverting f , thus establishing a separation between this restricted type of one-way functions and the general ones (see <ref type="bibr">Remark 7)</ref>.</p><p>2. For any (polynomial-time computable) function f , the existence of a (randomized) non-adaptive reduction of N P to the task of average-case inverting f implies that coN P ⊆ AM.</p><p>This result improves over the previous negative results of <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">11]</ref> that placed coN P in non-uniform AM (instead of in uniform AM).</p><p>These negative results can be interpreted in several ways: see discussion in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relation to Feigenbaum-Fortnow and Bogdanov-Trevisan</head><p>Our work is inspired by two previous works. The first work, by Feigenbaum and Fortnow <ref type="bibr" target="#b15">[15]</ref>, posed the question of whether or not N P-complete problems can be random self-reducible. That is, can (worst case) instances of N Pcomplete problems be reduced to one or more random instances, where the latter instances are drawn according to a predetermined distribution. The main result of <ref type="bibr" target="#b15">[15]</ref> is that if such (non-adaptive) reductions exist, then coN P is in a nonuniform version of AM, denoted AM poly . Non-uniformity was used in their work to encode statistics about the target distribution of the reduction.</p><p>Bogdanov and Trevisan <ref type="bibr" target="#b11">[11]</ref> start by viewing the result of <ref type="bibr" target="#b15">[15]</ref> as a result about the impossibility of worst-case to average-case reductions for N P-complete problems. They note that even if one cares about the average-case complexity of a problem with respect to a specific distribution (e.g., the uniform one) then it needs not be the case that a worstcase to average-case reduction must make queries according to this distribution. Furthermore, the distribution of queries may depend on the input to the reduction, and so statistics regarding it cannot be given as advice. Nevertheless, combining the ideas of <ref type="bibr" target="#b15">[15]</ref> with additional ideas (some borrowed from the study of locally-decodable codes <ref type="bibr" target="#b27">[27]</ref>), <ref type="bibr">Bogdanov and</ref> Trevisan showed that any non-adaptive reduction of (worst-case) N P to the average-case complexity of N P (with respect to any sampleable distribution) implies that coN P ⊆ AM poly .</p><p>Although a main motivation of <ref type="bibr" target="#b11">[11]</ref> is the question of basing one-way functions on worst-case N P-hardness, its focus (like that of <ref type="bibr" target="#b15">[15]</ref>) is on the more general study of decision problems. Using known reductions between search and decision problems in the context of distributional problems <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b26">26]</ref>, <ref type="bibr">Bogdanov and Trevisan [11]</ref> also derive implications on the (im)possibility of basing one-way functions on N P-hardness. In particular, they conclude that if there exists an N P-complete set for which deciding any instance is non-adaptively reducible to inverting a one-way function (or, more generally, to a search problem with respect to a sampleable distribution), then coN P ⊆ AM poly .</p><p>We emphasize that the techniques of <ref type="bibr" target="#b11">[11]</ref> refer explicitly only to decision problems, and do not relate to the underlying search problems (e.g., inverting a supposedly one-way function). In doing so, they potentially lose twice: they lose the extra structure of search problems and they lose the additional structure of the task of inverting polynomial-time computable functions. To illustrate the latter aspect, we reformulate the problem of inverting a polynomial-time computable function as follows (or rather spell out what it means in terms of search problems). The problem of (average-case) inverting f on the distribution f (Un), where Un denotes the uniform distribution over {0, 1} n , has the following features:</p><p>1. We care about the average-case complexity of the problem; that is, the probability that an efficient algorithm given a random (efficiently sampled) instance y (i.e., y ← f (Un)) finds x ∈ f -1 (y).</p><p>2. The problem is in NP; that is, the solution is relatively short and given an instance of the problem (i.e., y) and a (candidate) solution (i.e., x), it is easy to verify that the solution is correct (i.e., y = f (x)).</p><p>3. There exists an efficient algorithm that generates random instance-solution pairs (i.e., pairs (y, x) such that y = f (x), for uniformly distributed x ∈ {0, 1} n ).</p><p>Indeed, the first two items are common to all average-case NP-search problems (with respect to sampleable distributions), but the third item is specific to the context of oneway functions (cf. [18, Sec. 2.1]). In contrast, a generic sampleable distribution of instances is not necessarily coupled with a corresponding sampleable distribution of random instance-solution pairs. Indeed, capitalizing on the third item is the source of our success to obtain stronger (negative) results regarding the possibility of basing oneway functions on N P-hardness.</p><p>The results of <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b15">15]</ref> are limited in two ways. First, they only consider non-adaptive reductions, whereas the celebrated worst-case to average-case reductions of lattice problems (cf. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b29">29]</ref>) are adaptive. Furthermore, these positive results seem to illustrate the power of adaptive versus non-adaptive reductions. 1 Second, <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b15">15]</ref> reach conclusions involving a non-uniform complexity class (i.e., AM poly ).</p><p>Non-uniformity seems an artifact of their techniques, and one may hope to conclude that coN P ⊆ AM rather than coN P ⊆ AM poly . (One consequence of the uniform conclusion is that it implies that the polynomial-time hierarchy collapses to the second level, whereas the non-uniform conclusion only implies a collapse to the third level.) As stated before, working directly with one-way functions allows us to remove the first shortcoming in some cases and remove the second shortcoming in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The Benefits of Direct Study of One-Way Functions</head><p>The results presented in this paper indicate the gains of studying the question of basing one-way functions on N Phardness directly, rather than as a special case of a more general study. The gains being, getting rid of the nonuniformity altogether, and obtaining a meaningful negative result for the case of general (adaptive) reductions. Specifically, working directly with one-way functions allows us to consider natural special cases of potential one-way functions and to establish stronger negative results for them (i.e., results regarding general rather than non-adaptive reductions).</p><p>In particular, we consider polynomial-time computable functions f for which, given an image y, one can verify the number of preimages of y under f via a constant-round protocol. We call such functions size-verifiable, and show that the complexity of inverting them resembles the complexity of inverting polynomial-time computable permutations (and is separated from the complexity of inverting general polynomial-time computable functions, see <ref type="bibr">Remark 7)</ref>.</p><p>Indeed, the simplest case of size-verifiable functions is that of a permutation (i.e., a length-preserving 1-1 function). Another interesting special case is that of regular functions that have an efficiently recognizable range, where f is regular if each image of f has a number of preimages that is determined by the length of the image. We prove that any reduction (which may be fully adaptive) of N P to inverting such a function f implies coN P ⊆ AM. Indeed, this is a special case of our result that holds for any size-verifiable function f . We remark that, in the context of cryptographic constructions, it has been long known that dealing with regular oneway functions is easier than dealing with general one-way functions (see, e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b23">23]</ref>). For example, constructions of pseudorandom generators were first shown based on one-way permutation <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b30">30]</ref>, followed by a construction that used regular one-way functions <ref type="bibr" target="#b20">[20]</ref>, and culminated in the complex construction of <ref type="bibr" target="#b25">[25]</ref> that uses any one-way function. Our work shows that regularity of a function (or, more generally, size-verifiability) is important also for classifying the complexity of inverting f , and not only the ease of using it within cryptographic constructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary.</head><p>We believe that the study of the possibility of basing one-way functions on worst-case N P-hardness is the most important motivation for the study of worst-case to average-case reductions for N P. In such a case, one should consider the possible gain from studying the former question directly, rather than as a special case of a more general study. We believe that the results presented in this paper indicate such gains. We hope that this framework may lead to resolving the general question of the possibility of basing the existence of general one-way functions on worst-case N P-hardness via general reductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Techniques</head><p>Our results are proved by using the hypothetical existence of corresponding reductions in order to construct constantround interactive proof systems for coN P (and using <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b22">22]</ref> to conclude that coN P ⊆ AM). Towards this end we develop constant-round protocols for verifying the size of various "NP-sets" (or rather to sets of NP-witnesses for given instances in some predetermined N P-sets). <ref type="foot" target="#foot_2">2</ref> Recall that lower-bound protocols for this setting are well-known (cf., e.g., Goldwasser and Sipser <ref type="bibr" target="#b22">[22]</ref> and <ref type="bibr" target="#b21">[21]</ref>), but the known upper-bound protocol of Fortnow <ref type="bibr" target="#b17">[17]</ref> (see also <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b11">11]</ref>) works only when the verifier knows a "secret" element in the set. The latter condition severely limits the applicability of this upper-bound protocol, and this is the source of all technical difficulties encountered in this work.</p><p>To overcome the aforementioned difficulties, we develop two new constant-round protocols for upper bounding the sizes of N P sets. These protocols suffice for our applications, and may be useful also elsewhere. The two protocols are inspired by the works of Feigenbaum and Fortnow <ref type="bibr" target="#b15">[15]</ref> and Bogdanov and Trevisan <ref type="bibr" target="#b11">[11]</ref>, respectively, and extend the scope of the original ideas.</p><p>The first protocol, called confidence by comparison, significantly extends the main idea of Feigenbaum and Fortnow <ref type="bibr" target="#b15">[15]</ref>. The common setting consists of a verifier that queries a prover such that the following two conditions hold:</p><p>1. The prover may cheat (without being detected) only in "one direction": For example, in the decision problem setting of <ref type="bibr" target="#b15">[15]</ref>, the prover may claim that some yesinstances (of an NP-set) are no-instances (but not the other way around since it must support such claims by NP-witnesses). In our setting (of verifying set sizes) the prover may claim that sets are smaller than their actual size, but cannot significantly overestimate the size of sets (due to the use of a lower-bound protocol).</p><p>2. The verifier can obtain (reliable) statistics regarding the distribution of answers to random instances. In <ref type="bibr" target="#b15">[15]</ref> the relevant statistics is the frequency of yes-instances in the distribution of instances of a certain size, which in turn is provided as non-uniform advice. In our setting the statistics is the expected logarithm of the size of a random set (drawn from some distribution), and this statistics can be generated by randomly selecting sets such that the upper-bound protocol of <ref type="bibr" target="#b17">[17]</ref> (and not merely the lower-bound protocols of <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b21">21]</ref>) can be applied.</p><p>Combining the limited ("one directional") cheating of Type 1 with the statistics of Type 2 yields approximately correct answers for the questions that the verifier cares about. In <ref type="bibr" target="#b15">[15]</ref> this means that almost all queried instances are characterized correctly, while in our setting it means that for almost all sets sizes we obtain good approximations.</p><p>The second protocol abstracts a central idea of Bogdanov and Trevisan <ref type="bibr" target="#b11">[11]</ref>, and is based on "hiding" (from the prover) queries of interest among queries drawn from a related distribution. This protocol can be used whenever an "NP-set" is drawn from a distribution D and the verifier can also sample sets from another distribution D that has the following two properties: (a) There exists an constant-round protocol for proving upper bounds on sets drawn from D, and (b) the distribution D dominates D in the sense that PrS∼D</p><formula xml:id="formula_0">[S] ≤ λ • Pr S∼ D [S]</formula><p>, where λ is polynomial in the relevant efficiency parameter. We stress that the protocol postulated in (a) need not be the upper-bound protocol of <ref type="bibr" target="#b17">[17]</ref>; it may also be a confidence by comparison protocol as outlined in the previous paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Organization</head><p>In Section 2, we provide an overview of our proofs as well as a formal statement of our main results. Detailed proofs can be found in our technical report <ref type="bibr">[4]</ref>. In Section 3 we discuss possible interpretations of our negative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW OF RESULTS AND PROOFS</head><p>Having observed the potential benefit of working explicitly with the problem of inverting a polynomial-time computable function f , materializing this benefit represents the bulk of the technical challenge and the technical novelty of the current work.</p><p>Let us first clarify what we mean by saying that a decision problem L is (efficiently and randomly) reducible to the problem of inverting a (polynomial-time computable) function f . We take the straightforward interpretation (while using several arbitrary choices, like in setting the threshold determining the definition of an inverting oracle):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (inverting oracles and reductions).</head><formula xml:id="formula_1">A function O : {0, 1} * → {0, 1} * is called an (average-case) f -inverting oracle if, for every n, it holds that Pr[O(f (x)) ∈ f -1 (f (x))] ≥ 1/2</formula><p>, where the probability is taken uniformly over x ∈ {0, 1} n .</p><p>For a probabilistic oracle machine R, we denote by R O (w) a random variable representing the output of R on input w and access to oracle O, where the probability space is taken uniformly over the probabilistic choices of machine R (i.e., its randomness).</p><p>A probabilistic polynomial-time oracle machine R is called a reduction of L to (average-case) inverting f if, for every w ∈ {0, 1} * and any f -inverting oracle O, it holds that</p><formula xml:id="formula_2">Pr[R O (w) = χL(w)] ≥ 2/3, where χL(w) = 1 if w ∈ L and χL(w) = 0 otherwise.</formula><p>A reduction as in Definition 1 may only establish that f is a i.o. and weak<ref type="foot" target="#foot_3">3</ref> one-way function (i.e., that f cannot be inverted with probability exceeding 1/2 on every input length), which makes our impossibility results even stronger. Throughout this work, the function f will always be polynomialtime computable, and for simplicity we will also assume that it is length preserving (i.e., |f (x)| = |x| for all x).</p><p>Let us take a closer look at the reduction R. On input w, it may ask polynomially many queries to the inverting oracle. In adaptive reductions, later queries may depend on the oracle answers to earlier queries. In non-adaptive reductions all queries are computed in advance (based solely on the input w and the randomness of the reduction, denoted r). For simplicity, we will assume throughout this section that all queries are of length |w|.</p><p>High-level structure of our proofs and their challenges. Suppose, that there exists a reduction R from deciding membership in L to inverting the function f . We aim to use this reduction to give an constant-round protocol for L, and conclude that if L is NP-complete (or just NP-hard) then coN P ⊆ AM. (We mention that a similar constantround protocol can be given for L itself, but we have no need for the latter protocol.)</p><p>As in <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">11]</ref>, the main backbone of our constant-round protocol for L is an emulation of the reduction R on input w (i.e., the common input of the protocol), which in turn yields an output indicating whether or not w ∈ L. Of course, the verifier cannot emulate the reduction on its own, because the reduction requires access to an f -inverting oracle. Instead, the prover will play the role of the inverting oracle, thus enabling the emulation of the reduction. Needless to say, the verifier will check that all answers are actually f -preimages of the corresponding queries (and for starters we will assume that all queries are in the image of f ). Since we aim at a constant-round protocol, we send all queries to the prover in one round, which in the case of an adaptive reduction requires sending the randomness r of the reduction to the prover. Note that also in the non-adaptive case, we may as well just send r to the prover, because the prover may anyhow be able to determine r from the queries.</p><p>The fact that r is given (explicitly or implicitly) to the prover is the source of all difficulties that follow. It means that the prover need not answer the queries obliviously of other queries (or of r), but may answer the queries depending on r. In such a case, the prover's answers (when considering all possible r) are not consistent with any single oracle. Indeed, all these difficulties arise only in case f is not 1-1 (and indeed in case f is 1-1 the correct answer is fully determined by the query). We stress that the entire point of this study is the case in which f is not 1-1. In the special case that f is 1-1 (and length preserving), inverting f cannot be N P-hard for rather obvious reasons (as has been well-known for a couple of decades; cf. <ref type="bibr" target="#b12">[12]</ref>). 4  To illustrate what may happen in the general case, consider a 2-to-1 function f . Note that an arbitrary reduction of L to inverting f may fail in the rare case that the choice of the f -preimages returned by the oracle (i.e., whether the query y is answered by the first or second element in f -1 (y)) 4 Intuitively, inverting such an f (which is a search problem in which each instance has a unique solution) corresponds to a decision problem in N P ∩coN P (i.e., given (y, i) determine the i-th bit of f -1 (y)). Thus, the fact that inverting f cannot be N P-hard (unless N P = coN P) is analogous to the fact that sets in N P ∩ coN P cannot be N P-hard (again, unless N P = coN P). In contrast, in case f is not 1-1, the corresponding decision problems are either not known to be in N P ∩coN P or are promise problems (cf. <ref type="bibr" target="#b14">[14]</ref>) in the "promise problem class" analogue of N P ∩ coN P. Recall that promise problems in the latter class may be N P-hard even if N P = coN P (see <ref type="bibr" target="#b14">[14]</ref>). matches the reduction's internal coin tosses. <ref type="foot" target="#foot_4">5</ref> This event may occur rarely in the actual reduction (no matter which f -inverting oracle it uses), but a cheating prover may always answer in a way that matches the reduction's coins (hence violating the soundness requirement of the protocol).</p><p>A different way of looking at things is that the reduction guarantees that, for any adequate (f -inverting) oracle O, with probability 2/3 over the choices of r, machine R decides correctly when given oracle access to O. However, it is possible that for every r there exists an oracle Or such that R, when using coins r, decides incorrectly when given oracle access to Or. If this is the case (which we cannot rule out) then the prover may cheat by answering like the bad oracle Or.</p><p>In the rest of this section, we provide an outline of how we deal with this difficulty in each of the two cases (i.e., size-verifiable functions and non-adaptive reductions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Size-Verifiable f (Adaptive Reductions)</head><p>Recall that our aim is to present an constant-round protocol for L, when we are given a general (adaptive) reduction R of the (worst-case) decision problem of L to inverting f . We denote by q the number of queries made by R, by R(w, r, a1, ..., ai-1) the i-th query made by R on input w and randomness r after receiving the oracle answers a1, ..., ai-1, and by R(w, r, a1, ..., aq) the corresponding final decision. Recall that for simplicity, we assume that all queries are of length n def = |w|. In the bulk of this subsection we assume that, given y, one can efficiently determine |f -1 (y)|.</p><p>A very simple case. As a warm-up we first assume that |f -1 (y)| ≤ poly(|y|), for every y. In this case, on common input w, the parties proceed as follows.</p><p>1. The verifier selects uniformly coins r for the reduction, and sends r to the prover.</p><p>2. Using r, the prover emulates the reduction as follows.</p><p>When encountering a query y, the prover uses the lexicographically first element of f -1 (y) as the oracle answer (and uses ⊥ if f -1 (y) = φ). Thus, it obtains the corresponding list of queries y1, ..., yq, which it sends to the verifier along with the corresponding sets f -1 (y1), ..., f -1 (yq).</p><p>3. Upon receiving y1, ..., yq and A1, ..., Aq, the verifier checks, for every i, that |Ai| = |f -1 (yi)| and that f (x) = yi for every x ∈ Ai. Letting ai denote the lexicographically first element of Ai, the verifier checks that R(w, r, a1, ..., ai-1) = yi for every i. The verifier accepts w (as a member of L) if and only if all checks are satisfied and R(w, r, a1, ..., aq) = 0.</p><p>Note that the checks performed by the verifier "force" the prover to emulate a uniquely determined (perfect) inverting oracle (i.e., one that answers each query y with the lexicographically first element of f -1 (y)). Thus, the correctness of the reduction implies the completeness and soundness of the above constant-round protocol.</p><p>In general, however, the size of f -1 (y), for y in the range of f may not be bounded by a polynomial in n (where n = |y| = |w|). In this case, we cannot afford to have f -1 (y) as part of a message in the protocol (because it is too long). The natural solution is to have the verifier send a random hash function h : {0, 1} n → {0, 1} , where = (log 2 |f -1 (y)|/poly(n)) , and let the prover answer with h -1 (0 ) ∩ f -1 (y) (rather than with f -1 (y)). The problem is that in this case the verifier cannot check the "completeness" of the list of preimages (because it cannot compute |h -1 (0 ) ∩ f -1 (y)|), which allows the prover to omit a few members of h -1 (0 ) ∩ f -1 (y) at its choice. Recall that this freedom of choice (of the prover) may obliterate the soundness of the protocol.</p><p>The solution is that, although we have no way of determining the size of h -1 (0 ) ∩ f -1 (y), we do know that its expected size is exactly |f -1 (y)|/2 , where the expectation is taken over the choice of h (assuming that a random h maps each point in {0, 1} n uniformly on {0, 1} ). Furthermore, the prover cannot add elements to h -1 (0 ) ∩ f -1 (y) (because the verifier can verify membership in this set), it can only omit elements. But if the prover omits even a single element, it ends-up sending a set that is expected to be noticeably smaller than |f -1 (y)|/2 (because the expected size of h -1 (0 ) ∩ f -1 (y) is a polynomial in n). Thus, if we repeat the process many times, the prover cannot afford to cheat in most of these repetitions, because in that case the statistics will deviate from the expectation by too much.</p><p>Before turning to the specific implementation of this idea, we mention that the above reasoning corresponds to the confidence by comparison paradigm outlined in Section 1.4. Specifically, the prover may cheat (without being detected) only in one direction; that is, the prover may send a proper subset of a set of preimages under f and h (rather than the set itself), but it cannot send elements not in this set because membership in the set is efficiently verifiable by the verifier. Protocol for the general case. In the following protocol we use families of hash functions of very high quality (e.g., poly(n)-wise independent ones). Specifically, in addition to requiring that a random h : {0, 1} n → {0, 1} maps each point uniformly, we require that, for a suitable polynomial p and for any S ⊆ {0, 1} n of size at least p(n) • 2 , with overwhelmingly high probability over the choice of h it is the case that |h -1 (0 ) ∩ S| &lt; 2|S|/2 . In particular, the probability that this event does not occur is so small that, when conditioning on this event, the expected size of |h -1 (0 ) ∩ S| is (1±2 -n )•|S|/2 . (Thus, under this conditioning and for S as above, the variance of 2 |h -1 (0 ) ∩ S|/|S| is smaller than 2.) 1. The verifier selects uniformly m = n•q 2 p(n) 2 = poly(n) sequences of coins, r (1) , ..., r (m) for the reduction, and sends them to the prover. In addition, for each k = 1, ..., m, i = 1, ..., q and = 1, ..., n, it selects and sends a random hash function h k,i, : {0, 1} n → {0, 1} .</p><p>To streamline the following description, for j ≤ 0, we artificially define h k,i,j such that h -1 k,i,j (0 j ) def = {0, 1} n . In such a case, S ∩ h -1 k,i,j (0 j ) = S, and so an instruction to do something with the former set merely means using the latter set.</p><p>2. For every k = 1, ..., m, the prover uses r (k) to emulate the reduction as follows. When encountering the i-th query, y</p><formula xml:id="formula_3">(k) i , it determines (k) i = (log 2 |f -1 (y (k) i )|/p(n))</formula><p>, and uses the lexicographically first element of f -1 (y</p><formula xml:id="formula_4">(k) i )∩ h -1 k,i, (k) i (0 (k) i</formula><p>) as the oracle answer (and uses ⊥ if the latter set is empty). 6 Thus, it obtains the corresponding list of queries y (k) 1 , ..., y (k) q , which it sends to the verifier along with the corresponding sets f -1 (y</p><formula xml:id="formula_5">(k) 1 ) ∩ h -1 k,1, (k) 1 (0 (k) 1 ), ..., f -1 (y (k) q ) ∩ h -1 k,q, (k) q (0 (k) q ).</formula><p>We assume that none of the latter sets has size greater than 4p(n). Note that the bad event occurs with negligible probability, and in such a case the prover halts and the verifier rejects. (Otherwise, all mq sets are sent in one message.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Upon receiving y</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, ..., A (m) q</head><p>, the verifier conducts the following checks: (a) For every k = 1, ..., m and i = 1, ..., q, the verifier checks that for every</p><formula xml:id="formula_6">x ∈ A (k) i it holds that f (x) = y (k) i and h k,i, (k) i (x) = 0 (k) i</formula><p>, where</p><formula xml:id="formula_7">(k) i = (log 2 |f -1 (y (k) i )|/p(n)</formula><p>) is efficiently computable due to the "size-computation" hypothesis. Letting a</p><formula xml:id="formula_8">(k) i be the lexicographically first element of A (k) i , it checks that R(w, r (k) , a (k) 1 , ..., a (k) i-1 ) = y (k) i . (b) For every i = 1, ..., q, it checks that 1 m • m k=1 2 (k) i • |A (k) i | |f -1 (y (k) i )| &gt; 1 - 1 100q • p(n) (1)</formula><p>where 0/0 is defined as 1.</p><p>The verifier accepts w if and only if all the foregoing checks are satisfied and it holds that</p><formula xml:id="formula_9">R(w, r (k) , a<label>(k)</label></formula><p>1 , ..., a (k) q ) = 0 for a uniformly selected k ∈ {1, ..., m}.</p><p>Analysis of the Protocol. We first note that the additional checks added to this protocol have a negligible effect on the completeness condition: the probability that either |f -1 (y</p><formula xml:id="formula_10">(k) i ) ∩ h -1 k,i, (k) i (0 (k) i</formula><p>)| &gt; 4p(n) for some i, k or that Eq. ( <ref type="formula">1</ref>) is violated for some i is exponentially vanishing. 7 Turning to the soundness condition, we note that the checks performed by the verifier force the prover to use</p><formula xml:id="formula_11">6 Note that if |f -1 (y (k) i )| = 0</formula><p>then the oracle answer is defined as ⊥. The formally inclined reader may assume that in this case log 2 0 is defined arbitrarily. 7 Recall that here we refer to the case that A</p><formula xml:id="formula_12">(k) i = f -1 (y (k) i )∩ h -1 k,i, (k) i (0 (k) i</formula><p>). Thus, regarding Eq. ( <ref type="formula">1</ref>), we note that the l.h.s is the average of m independent random variables, each having constant variance. Applying Chernoff bound, the probability that Eq. ( <ref type="formula">1</ref>) is violated is upper-bounded by exp(-Ω(m/(100q</p><formula xml:id="formula_13">• p(n)) 2 )) = exp(-Ω(n)). A (k) i ⊆ T (k) i def = f -1 (y (k) i ) ∩ h -1 k,i, (k) i (0 (k) i</formula><p>). Also, with overwhelmingly high probability, for every i = 1, ..., q, it holds that</p><formula xml:id="formula_14">1 m • m k=1 2 (k) i • |T (k) i | |f -1 (y (k) i )| &lt; 1 + 1 100q • p(n) (2)</formula><p>Combining Eq. ( <ref type="formula">1</ref>) and Eq. ( <ref type="formula">2</ref>), and recalling that A</p><formula xml:id="formula_15">(k) i ⊆ T (k) i (and |f -1 (y (k) i )| &lt; 2p(n) • 2 (k) i ), it follows that (1/m) • m k=1 (|T (k) i \A (k) i |/2p(n)) &lt; 2/(100q•p(n))</formula><p>for every i. Thus, for each i, the probability over a random k that A</p><formula xml:id="formula_16">(k) i = T (k) i</formula><p>is at most 1/25q. It follows that for a random k, the probability that</p><formula xml:id="formula_17">A (k) i = T (k) i for all i's is at least 1 -(1/25).</formula><p>In this case, the correctness of the reduction implies the soundness of the foregoing constant-round protocol.</p><p>Extension. The foregoing description presumes that the verifier can determine the size of the set of f -preimages of any string. The analysis can be easily extended to the case that the verifier can only check the correctness of the size claimed and proved by the prover. That is, we refer to the following definition.</p><p>Definition 2 (Size Verifiable). We say that a function f : {0, 1} * → {0, 1} * is size verifiable if there is a constantround proof system for the set {(y, |f -1 (y)|) : y ∈ {0, 1} * }.</p><p>A natural example of a function that is size verifiable (for which the relevant set is not known to be in BPP) is the integer multiplication function. That is, we consider the function that maps pairs of integers (which are not necessarily prime or of the same length) to their product. In this case the set {(y, |f -1 (y)|) : y ∈ {0, 1} * } is in N P (i.e., the NP-witness is the prime factorization) but is widely believed not to be in BPP (e.g., it is believed to be infeasible to distinguish product of two (n/2)-bit random primes from the product of three (n/3)-bit long random primes).</p><p>Theorem 3 (Adaptive Reductions). Unless coN P ⊆ AM, there exists no reduction (even not an adaptive one) from deciding an NP-hard language to inverting a size-verifiable polynomial-time computable function.</p><p>In other words, it is unlikely that the existence of sizeverifiable one-way functions can be based on NP-hardness. We note that the result can be extended to functions that are "approximately size-verifiable" (covering the "approximable preimage-size" function of <ref type="bibr" target="#b23">[23]</ref> as a special case).</p><p>Remark 4. The proof of Theorem 3 does not utilize the fact that the oracle accessed by the reduction is allowed to err on some of the queries. Thus, the proof holds also for the case of reductions to the task of inverting f in the worstcase (i.e., inverting f on every image). It follows that, unless coN P ⊆ AM, there exist no reductions from N P to inverting in the worst-case a size-verifiable polynomial-time computable function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Adaptive Reductions (General f )</head><p>We now turn to outline the proof of our second main result. Here we place no restrictions on the function f , but do restrict the reductions.</p><p>Theorem 5 (General Functions). Unless coN P ⊆ AM, there exists no non-adaptive reduction from deciding an NP-complete language to inverting a polynomial-time computable function.</p><p>Considering the constant-round protocol used in the adaptive case, we note that in the current case the verifier cannot necessarily compute (or even directly verify claims about) the size of sets of f -preimages of the reduction's queries. Indeed, known lower-bound protocols (cf. <ref type="bibr" target="#b22">[22]</ref>) could be applied to these sets, but known upper-bound protocols (cf. <ref type="bibr" target="#b17">[17]</ref>) cannot be applied because they require that the verifier has a random secret member of these sets. Fortunately, using the techniques described in Section 1.4 allows to overcome this difficulty (for the case of non-adaptive reductions), and to obtain constant-round protocols (rather than merely non-uniform constant-round protocols) for coN P (thus, implying coN P ⊆ AM).</p><p>Here R is a non-adaptive reduction of some set L ∈ N P to the average-case inverting of an arbitrary (polynomialtime computable) function f , and our goal again is to show that L ∈ AM. We may assume, without loss of generality, that the queries of R(w, •) are identically distributed (but typically not independently distributed), and represent this distribution by the random variable Rw; that is, Pr[Rw = y] = |{r ∈ {0, 1} n : R(w, r) = y}|/2 n , where n denotes the number of coins used by R(w, •).</p><p>Actually, our constructions do not rely on the non-adaptivity of the reduction R, but rather on the fact that its queries are distributed according to a single distribution (i.e., Rw) that depends on w. We note that the treatment can be extended to the case that, for every i, the i-th query of R is distributed in a manner that depends only on w and i (but not on the answers to prior queries). A simple case (queries distributed as f (Un)). We first consider the (natural) case that R's queries are distributed identically to Fn def = f (Un), where Un denotes the uniform distribution over {0, 1} n . Augmenting the protocol (for the general case) presented in Section 2.1, we require the prover to provide |f -1 (y</p><formula xml:id="formula_18">(k) i )| along with each query y (k) i</formula><p>made in the emulation of R(w, r (k) ). 8 In order to verify that the claimed set sizes are approximately correct, we require the prover to provide lower-bound proofs (cf., <ref type="bibr" target="#b22">[22]</ref>) and employ the confidence by comparison paradigm. Specifically, to prevent the prover from understating these set sizes, we com-8 Actually, a small modification is required for handling the following subtle problem that refers to the possible control of the prover on the hashing function being used in its answer. Recall that the hashing function in use (for query y) is determined by = (log 2 |f -1 (y)|/p(n)) , but in the setting of Section 2.1 the verifier knows |f -1 (y)| and thus the prover has no control on the value of . In the current context, the prover may be able to cheat a little about the value of |f -1 (y)|, without being caught, and this may (sometimes) cause a change of one unit in the value of (and thus allow for a choice among two hash functions). We resolve this problem by having the verifier "randomize" the value of |f -1 (y)| such that, with high probability, cheating a little about this value is unlikely to affect the value of . Specifically, as a very first step, the verifier selects uniformly ρ ∈ [0, 1] (and sends it to the prover), and the prover is asked to set = (ρ + log 2 sy/p(n)) (rather than = (log 2 sy/p(n)) ), where sy is prover's claim regarding the size of |f -1 (y)|. pare the value of (1/qm)</p><formula xml:id="formula_19">• q i=1 m k=1 log 2 |f -1 (y (k) i )| to the expected value of log 2 |f -1 (f (Un))|</formula><p>, where here and below we define log 2 0 as -1 (in order to account for the case of queries that have no preimages). Analogously to <ref type="bibr" target="#b15">[15]</ref>, one may suggest that the latter value (i.e., E[log 2 |f -1 (Fn)|]) be given as a non-uniform advice, but we can do better: We require the prover to supply E[log 2 |f -1 (f (Un))|] and prove its approximate correctness using the following protocol.</p><p>The verifier uniformly selects x1, ..., xm ∈ {0, 1} n , computes yi = f (xi) for every i, sends y1, ..., ym to the prover and asks for |f -1 (y1)|, ..., |f -1 (ym)| along with lower and upper bound constant-round interactive proofs. (As usual, the lower-bound AM-protocol of <ref type="bibr" target="#b22">[22]</ref> (or <ref type="bibr" target="#b21">[21]</ref>) can be applied because membership in the corresponding sets can be easily verified.) The point is that the upperbound protocol of <ref type="bibr" target="#b17">[17]</ref> can be applied here, because the verifier has secret random elements of the corresponding sets.</p><p>Recall that the lower-bound protocol (of <ref type="bibr" target="#b22">[22]</ref> or <ref type="bibr" target="#b21">[21]</ref>) guarantees that the prover cannot overstate any set size by more than an ε = 1/poly(n) factor (without risking detection with overwhelmingly high probability). Thus, we will assume throughout the rest of this section that the prover never overstates set sizes (by more than such a factor). The analysis of understated set sizes is somewhat more delicate, firstly because (as noted) the execution of upper-bound protocols requires the verifier to have a secret random element in the set, and secondly because an understatement by a factor of ε is only detected with probability ε (or so). Still this means that the prover cannot significantly understate many sets sizes and go undetected. Specifically, if the prover understates the size of f -1 (yi) by more than an ε factor for at least n/ε of the yi's then it gets detected with overwhelmingly high probability. Using a suitable setting of parameters, this establishes the value of E[log 2 |f -1 (f (Un))|] up to a sufficiently small additive term, which suffices for our purposes. Specifically, as in Section 2.1, such a good approximation of E[log 2 |f -1 (f (Un))|] forces the prover not to understate the value of |f -1 (y (k) i )| by more than (say) a 1/10p(n) factor for more than (say) m/10 of the possible pairs (i, k). (Note that, unlike in Section 2.1, here we preferred to consider the sum over all (i, k)'s rather than q sums, each corresponding to a different i.)<ref type="foot" target="#foot_5">9</ref> A special case (no "heavy" queries). We now allow Rw to depend on w, but restrict our attention to the natural case in which the reduction does not ask a query y with probability that exceeds Pr[Fn = y] by too much. Specifically, suppose that Pr[Rw = y] ≤ poly(|y|) • Pr[Fn = y], for every y. In this case, we modify the foregoing protocol as follows.</p><p>Here it makes no sense to compare the claimed value of (1/qm)• q i=1 m k=1 log 2 |f -1 (y</p><formula xml:id="formula_20">(k) i )| against E[log 2 |f -1 (Fn)|]. Instead we should compare the former (claimed) average to E[log 2 |f -1 (Rw)|].</formula><p>Thus, the verifier needs to obtain a good approximation to the latter value. This is done by gen-erating many yi's as before (i.e., yi = f (xi) for uniformly selected xi ∈ {0, 1} n ) along with fewer but still many yi's sampled from Rw, and sending all these yi's (in random order) to the prover. Specifically, for t ≥ max y∈{0,1} * {Pr[Rw = y]/Pr[Fn = y]}, we generate t times more yi's from Fn, and so each yi received by the prover is at least as likely to come from Fn than from Rw.</p><p>The prover will be asked to provide all |f -1 (yi)|'s along with lower-bound proofs, and afterwards (i.e., only after committing to these |f -1 (yi)|'s) the verifier will ask for upper-bound proofs for those yi's generated via Fn (for which the verifier knows a secret and uniformly distributed xi ∈ f -1 (yi)).</p><p>Recall that the prover cannot significantly overstate the size of any |f -1 (yi)| (i.e., overstate it by more than an ε = 1/poly(n) factor). If the prover significantly understates the sizes of too many of the |f -1 (yi)|'s, then it is likely to similarly overstate also the sizes of many |f -1 (yi)|'s that correspond to yi's that were generated by sampling Fn. But in this case, with overwhelmingly high probability, the prover will fail in at least one of the corresponding upper-bound proofs.</p><p>The general case (dealing with "heavy" queries). We now allow Rw to depend arbitrarily on w, without any restrictions whatsoever. For a threshold parameter t to be determined later, we say that a query y is t-heavy if Pr[Rw = y] &gt; t • Pr[Fn = y]. (In the special case, we assumed that there are no poly(n)-heavy queries.) Observe that the probability that an element sampled according to Fn is t-heavy is at most 1/t, and thus modifying an inverting oracle such that it answers t-heavy queries by ⊥ effects the inverting probability of the oracle by at most 1/t. Thus, for t ≥ 2, if we answer t-heavy queries by ⊥ (and answer other f -images with a preimage), then we emulate a legitimate inverting oracle (which inverts f with probability at least 1/2) and the reduction R is still supposed to work well. <ref type="foot" target="#foot_6">10</ref> Referring to y as t-light if it is not t-heavy, we note that t-light queries can be handled as in the foregoing special case (provided t ≤ poly(n)), whereas t-heavy queries are accounted for by the previous discussion. The problem is to determine whether a query is t-heavy or tlight, and certainly we have no chance of doing so if many (reduction) queries are very close to the threshold (e.g., if Pr[Rw = y] = (t ± n -ω (1) ) • Pr[Fn = y] for all y's). Thus, as in <ref type="bibr" target="#b11">[11]</ref>, we select the threshold at random (say, uniformly in the interval <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>). Next, we augment the foregoing protocol as follows.</p><p>• We ask the prover to provide for each query y i }. In addition, we ask for lowerbound proofs of these set sizes.</p><p>• Using lower and upper bound protocols (analogously to the simple case) We note that estimating E[log 2 |f -1 (R w )|] is done by generating yi's as in the special case, but with t ∈ [2, 3] as determined above, and while asking for the value of both Pr[Rw = yi] and Pr[Fn = yi] for all yi's, and afterwards requiring upper-bound proofs for one of these values depending on whether yi was sampled from Rw or Fn. These values will serve as basis for determining whether each yi is t-heavy or t-light, and will also yield an estimate of the probability that Rw is t-light.</p><p>Recall that the verifier accepts w if and only if all the foregoing checks (including the ones stated in the adaptive case) are satisfied.</p><p>Ignoring the small probability that we selected a bad threshold t as well as the small probability that we come across a query that is close to the threshold, we analyze the foregoing protocol as follows. We start by analyzing the queries yi's used in the sub-protocol for estimating</p><formula xml:id="formula_21">E[log 2 |f -1 (R w )|].</formula><p>We first note that, by virtue of the lower and upper bound proofs, for almost all queries yi's generated by Rw, the sizes of {r : R(w, r) = yi} must be approximately correct. Next, employing a reasoning as in the special case, it follows that for almost all t-light queries yi's we obtain correct estimates of the size of their f -image (i.e., we verify that almost all the sizes claimed by the prover for the |f -1 (yi)|'s are approximately correct). It follows that we correctly characterize almost all the t-light yi's generated by Rw as such. As for (almost all) t-heavy queries yi's generated by Rw, we may wrongly consider them t-light only if the prover has significantly overstated the size of their preimage, because we have a good estimate of {r : R(w, r) = y (k) i } for with a uniformly distributed member of f -1 (Fn). Here we rely on our ability to generate samples of Rw along with a uniformly distributed member of {r : R(w, r) = Rw}.</p><p>(almost all) these yi's. Recalling that an overstatement of |f -1 (y (k) i )| is detected with overwhelmingly high probability (by the lower-bound protocol), it follows that almost all t-heavy queries yi's generated by Rw are correctly characterized as such. Thus, the characterization of almost all yi's (generated by Rw) as t-light or t-heavy is correct, and so is the estimate of the probability that Rw is t-light. Recalling that for almost all the t-light yi's generated by Rw we have a correct estimate of |f -1 (yi)|, we conclude that the estimate of E[log 2 |f -1 (R w )|] is approximately correct.</p><p>Next we employ parts of the foregoing reasoning to the y i )| cannot be overstated), we conclude that we correctly characterize almost all t-heavy queries as such. The comparison to the estimated probability that Rw is t-light guarantees that the prover cannot claim too many t-light y (k) i 's as t-heavy, which implies that we have correctly characterized almost all y (k) i 's as t-light or t-heavy. Recalling that |f -1 (y (k) i )| can only be understated (due to the lowerbound proofs) and using the estimate of E[log 2 |f -1 (R w )|] as an approximate lower-bound, it follows that the claims made regarding almost all the |f -1 (y (k) i )|'s are approximately correct. Thus, as in the special case, the correctness of the reduction implies the completeness and soundness of the foregoing constant-round protocol. Remark 6. In contrast to Remark 4, dealing with general one-way functions (even in the non-adaptive case) requires referring to the average-case nature of the reduction; that is, we must use the hypothesis that the reduction yields the correct answer even in case that the inverting oracle fails on some inputs (as long as the measure of such inputs is adequately small). This average-case hypothesis is required since there exist reductions from N P to inverting in the worst-case some (general) polynomial-time computable function (see <ref type="bibr" target="#b18">[18,</ref><ref type="bibr">Chap. 2,</ref><ref type="bibr">Exer. 3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DISCUSSION: INTERPRETATIONS OF OUR NEGATIVE RESULTS</head><p>Negative results of the type obtained in this work (as well as in <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">11]</ref>) can be interpreted in several ways: The straightforward view is that such results narrow down the means by which one can base one-way functions on N Phardness. Namely, under the assumption that coN P is not contained in AM, our results show that (1) non-adaptive randomized reductions are not suitable for basing one-way functions on N P-hardness, and (2) that one-way functions based on N P-hardness can not be size verifiable (e.g., cannot be regular with an efficiently recognizable range).</p><p>Another interpretation is that these negative results are an indication that (worst-case) complexity assumptions regarding N P as a whole (i.e., N P ⊆ BPP) are not sufficient to base one-way functions on. But this does not rule out the possibility of basing one-way functions on the worstcase hardness of a subclass of N P (e.g., the conjecture that N P ∩ coN P ⊆ BPP). This is the case because our results (as previous ones) actually show that certain reductions of the (worst-case) decision problem of a set S to (averagecase) inverting of f imply that S ∈ AM ∩ coAM. But no contradiction is obtained if S belongs to N P ∩ coN P anyhow. Indeed, the decision problems related to lattices that are currently known to have worst-case to average-case reductions belong to N P ∩ coN P (cf. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b29">29]</ref> versus <ref type="bibr" target="#b1">[1]</ref>).</p><p>Yet another interpretation is that these negative results suggest that we should turn to a more relaxed notion of a reduction, which is uncommon in complexity theory and yet is applicable in the current context. We refer to "non black-box" reductions in which the reduction gets the code (of the program) of a potential probabilistic polynomial-time inverting algorithm (rather than black-box access to an arbitrary inverting oracle). The added power of such (security) reductions was demonstrated a few years ago by Barak <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref>.</p><p>Remark 7. Recall that Remark 4 asserts that, unless coN P ⊆ AM, there exist no reductions from N P to inverting in the worst-case a size-verifiable polynomial-time computable function. In contrast, it is known that reductions do exist from N P to inverting in the worst-case some (general) polynomial-time computable function (see <ref type="bibr" target="#b18">[18,</ref><ref type="bibr">Chap. 2,</ref><ref type="bibr">Exer. 3]</ref>). This yields a (structural complexity) separation between size-verifiable polynomial-time computable functions on one hand and general polynomial-time computable functions on the other hand, (assuming as usual coN P ⊆ AM).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the value of Pr[Rw = y (k) i ], or equivalently the size of {r : R(w, r) = y (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>i</head><label></label><figDesc>'s. Recalling that, for almost all queries y (k) i , we obtained correct estimates of the size of {r : R(w, r) = y (k) i } (and that |f -1 (y (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We let the verifier check that this value is sufficiently close to the claimed value of (1/qm)• Hence, combining these two items, the verifier gets a good estimate of the size of {r : R(w, r) = y Using a protocol as in the special case, the verifier obtains an estimate of E[log 2 |f -1 (R w )|], where R w denotes Rw conditioned on being t-light, and checks that this value is sufficiently close to the claimed average of log 2 |f -1 (y</figDesc><table><row><cell cols="4">q i=1 ing an understating of the size of almost all the sets m (k) i }|, thus prevent-k=1 log 2 |{r : R(w, r) = y</cell></row><row><cell cols="3">{r : R(w, r) = y</cell><cell>(k) i }.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(k) i } for all</cell></row><row><cell cols="4">but few (i, k)'s. That is, the verifier can confirm that</cell></row><row><cell cols="4">for almost all the (i, k)'s the claimed (by prover) size</cell></row><row><cell cols="4">of {r : R(w, r) = y</cell><cell>(k) i } is approximately correct.</cell></row><row><cell cols="4">• Using the claimed (by the prover) values of Pr[Rw =</cell></row><row><cell cols="4">y decisions regarding which of the y (k) i ] and Pr[Fn = y (k) i ], the verifier makes tentative (k) i 's is t-light.</cell></row><row><cell cols="4">Note that for most (i, k), the prover's claim about</cell></row><row><cell cols="4">Pr[Rw = y claim about Pr[Fn = y (k) i ] is approximately correct, whereas the (k) i ] can only be understated</cell></row><row><cell cols="4">(by virtue of the lower-bound protocol employed for</cell></row><row><cell cols="2">the set f -1 (y</cell><cell cols="2">(k) i )).</cell></row><row><cell></cell><cell cols="3">(k) i )|, taken only over t-light y</cell><cell>(k) i 's. In ad-</cell></row><row><cell cols="4">dition, the verifier checks that the fraction of t-light</cell></row><row><cell>y</cell><cell cols="3">(k) i 's (among all y</cell><cell>(k) i 's) approximates the probability</cell></row><row><cell cols="4">that Rw is t-light.</cell></row></table><note><p><p>11 </p>, we get an estimate of E[log 2 |{r : R(w, r) = Rw}|].</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We comment that the power of adaptive versus nonadaptive reductions has been studied in various works (e.g.,<ref type="bibr" target="#b16">[16</ref></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>, 24, 6]). It is known that if NP ⊆ BP E, then there exists a set in N P \ BPP that is adaptively random selfreducible but not non-adaptively random self-reducible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>That is, for a witness relation R that corresponds to some N P-set S = {x : ∃y (x, y) ∈ R}, we consider the sets R(x) = {y : (x, y) ∈ R} for various x ∈ S.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>In contrast, the standard definition of one-way function requires that any efficient inverting algorithm succeeds with negligible probability (i.e., probability that is smaller than 1/poly(n) on all but finitely many n's). Here we relax the security requirement in two ways (by requiring more of a successful inverting algorithm): first, we require that the inverting algorithm be successful on any input length (hence hardness only occurs i.o.), and second that the success probability exceeds 1/2 rather than an arbitrary small 1/poly(n) (hence the term "weak").</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For example, given an arbitrary reduction of L to inverting f , consider a modified reduction that tosses n additional coins ρ1, ..., ρn, issues n additional queries, and halts without output if and only if for i = 1, ..., n the i-th additional query is answered with the (ρi + 1)-st corresponding preimage (in lexicographic order). This reduction works with probability that is very close to the original one, but a cheating prover can always cause its emulation to halt without output.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>We stress that in both cases both choices can be made. We note that, when analyzing the completeness condition, one may prefer to analyze the deviation of the individual sums (for each i).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>This is the first (and only) place where we use the averagecase nature of the reduction R.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p><ref type="bibr" target="#b11">11</ref> In the simple case we got an estimate of E[log 2 |f -1 (Fn)|], while relying on our ability to generate samples of Fn along</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research of Adi Akavia was supported in part by NSF grant CCF0514167. The research of Oded Goldreich was partially supported by the Israel Science Foundation (grant No. 460/05). The research of Shafi Goldwasser was supported in part by NSF CNS-0430450, NSF CCF0514167, Sun Microsystems, and the Minerva Foundation. Dana Moshkovitz is grateful to Muli Safra for supporting her visit to MIT, where this research has been initiated.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lattice Problems in NP intersect coNP</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th FOCS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perfect Zero-Knowledge Languages can be Recognized in Two Rounds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hastad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th FOCS</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating hard instances of lattice problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th STOC</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On Basing One-Way Functions on NP-Hardness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akavia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moshkovitz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In preparations, to be posted on ECCC</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trading Group Theory for Randomness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th STOC</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stronger seperations for random-self-reducability, rounds, and advice</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laplante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Complexity</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="98" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How to Go Beyond the Black-Box Simulation Barrier</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd FOCS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constant-Round Coin-Tossing with a Man in the Middle or Realizing the Shared Random String Model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43th FOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the Theory of Average Case Complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="219" />
			<date type="published" when="1992-04">April 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How to Generate Cryptographically Strong Sequences of Pseudo-Random Bits</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preliminary version in 23rd FOCS</title>
		<imprint>
			<date type="published" when="1982">1984. 1982</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="850" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On worst-case to average-case reductions for NP problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th FOCS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relativized Cryptography</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brassard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th FOCS</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="383" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Security-preserving hardness-amplification for any regular one-way function In 31st STOC</title>
		<author>
			<persName><forename type="first">G</forename><surname>Di-Crescenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Complexity of Promise Problems with Applications to Public-Key Cryptography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacobi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. and Control</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="159" to="173" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random self-reducibility of complete sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fortnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="994" to="1005" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The power of adaptiveness and additional queries in random self-reductions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fortnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="158" to="174" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Complexity of Perfect Zero-Knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fortnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended abstract in 19th STOC</title>
		<imprint>
			<date type="published" when="1987">1989. 1987</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<title level="m">Foundation of Cryptography -Basic Tools</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Security Preserving Amplification of Hardness</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st FOCS</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the Existence of Pseudorandom Generators</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1163" to="1175" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On interactive proofs with a laconic provers</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Private Coins versus Public Coins in Interactive Proof Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended abstract in 18th STOC</title>
		<imprint>
			<date type="published" when="1986">1989. 1986</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing complexity assumptions for statistically-hiding commitment</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shaltiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurocrypt</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3494</biblScope>
			<biblScope unit="page" from="58" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">P-Selective Sets, and Reducing Search to Decision vs. Self-reducibility</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hemaspaandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="209" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Pseudorandom Generator from any One-way Function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hastad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1364" to="1396" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">No Better Ways to Generate Hard NP Instances than Picking Uniformly at Random</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st FOCS</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="812" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On The Efficiency Of Local Decoding Procedures For Error-Correcting Codes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd STOC</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m">Advances in Computing Research: a research annual</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</editor>
		<imprint>
			<publisher>Randomness and Computation</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Worst-case to Average-case Reductions Based on Gaussian Measures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Micciancio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th FOCS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Theory and Application of Trapdoor Functions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd FOCS</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
