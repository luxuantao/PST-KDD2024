<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-23">23 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@cs.cmu.com</email>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<email>simonyan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<email>yiming@cs.cmu.edu</email>
						</author>
						<title level="a" type="main">DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-23">23 Apr 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1806.09055v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Discovering state-of-the-art neural network architectures requires substantial effort of human experts. Recently, there has been a growing interest in developing algorithmic solutions to automate the manual process of architecture design. The automatically searched architectures have achieved highly competitive performance in tasks such as image classification <ref type="bibr" target="#b42">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b43">Zoph et al., 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b34">Real et al., 2018)</ref> and object detection <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref>.</p><p>The best existing architecture search algorithms are computationally demanding despite their remarkable performance. For example, obtaining a state-of-the-art architecture for CIFAR-10 and ImageNet required 2000 GPU days of reinforcement learning (RL) <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> or 3150 GPU days of evolution <ref type="bibr" target="#b34">(Real et al., 2018)</ref>. Several approaches for speeding up have been proposed, such as imposing a particular structure of the search space <ref type="bibr" target="#b22">(Liu et al., 2018b;</ref><ref type="bibr">a)</ref>, weights or performance prediction for each individual architecture <ref type="bibr" target="#b5">(Brock et al., 2018;</ref><ref type="bibr" target="#b3">Baker et al., 2018)</ref> and weight sharing/inheritance across multiple architectures <ref type="bibr" target="#b9">(Elsken et al., 2017;</ref><ref type="bibr" target="#b32">Pham et al., 2018b;</ref><ref type="bibr" target="#b6">Cai et al., 2018;</ref><ref type="bibr" target="#b4">Bender et al., 2018)</ref>, but the fundamental challenge of scalability remains. An inherent cause of inefficiency for the dominant approaches, e.g. based on RL, evolution, MCTS <ref type="bibr" target="#b29">(Negrinho &amp; Gordon, 2017)</ref>, SMBO <ref type="bibr" target="#b21">(Liu et al., 2018a)</ref> or Bayesian optimization <ref type="bibr" target="#b18">(Kandasamy et al., 2018)</ref>, is the fact that architecture search is treated as a black-box optimization problem over a discrete domain, which leads to a large number of architecture evaluations required.</p><p>In this work, we approach the problem from a different angle, and propose a method for efficient architecture search called DARTS (Differentiable ARchiTecture Search). Instead of searching over a discrete set of candidate architectures, we relax the search space to be continuous, so that the architecture can be optimized with respect to its validation set performance by gradient descent. The data efficiency of gradient-based optimization, as opposed to inefficient black-box search, allows DARTS to achieve competitive performance with the state of the art using orders of magnitude less computation resources. It also outperforms another recent efficient architecture search method, ENAS <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>. Notably, DARTS is simpler than many existing approaches as it does not involve controllers <ref type="bibr" target="#b42">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b2">Baker et al., 2017;</ref><ref type="bibr" target="#b43">Zoph et al., 2018;</ref><ref type="bibr" target="#b32">Pham et al., 2018b;</ref><ref type="bibr" target="#b40">Zhong et al., 2018)</ref>, hypernetworks <ref type="bibr" target="#b5">(Brock et al., 2018)</ref> or performance predictors <ref type="bibr" target="#b21">(Liu et al., 2018a</ref>), yet it is generic enough handle both convolutional and recurrent architectures.</p><p>The idea of searching architectures within a continuous domain is not new <ref type="bibr" target="#b35">(Saxena &amp; Verbeek, 2016;</ref><ref type="bibr" target="#b0">Ahmed &amp; Torresani, 2017;</ref><ref type="bibr" target="#b37">Veniat &amp; Denoyer, 2017;</ref><ref type="bibr">Shin et al., 2018)</ref>, but there are several major distinctions. While prior works seek to fine-tune a specific aspect of an architecture, such as filter shapes or branching patterns in a convolutional network, DARTS is able to learn high-performance architecture building blocks with complex graph topologies within a rich search space. Moreover, DARTS is not restricted to any specific architecture family, and is applicable to both convolutional and recurrent networks.</p><p>In our experiments (Sect. 3) we show that DARTS is able to design a convolutional cell that achieves 2.76 ± 0.09% test error on CIFAR-10 for image classification using 3.3M parameters, which is competitive with the state-of-the-art result by regularized evolution <ref type="bibr" target="#b34">(Real et al., 2018)</ref> obtained using three orders of magnitude more computation resources. The same convolutional cell also achieves 26.7% top-1 error when transferred to ImageNet (mobile setting), which is comparable to the best RL method <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref>. On the language modeling task, DARTS efficiently discovers a recurrent cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned LSTM <ref type="bibr" target="#b26">(Melis et al., 2018)</ref> and all the existing automatically searched cells based on NAS <ref type="bibr" target="#b42">(Zoph &amp; Le, 2017)</ref> and ENAS <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>.</p><p>Our contributions can be summarized as follows:</p><p>• We introduce a novel algorithm for differentiable network architecture search based on bilevel optimization, which is applicable to both convolutional and recurrent architectures.</p><p>• Through extensive experiments on image classification and language modeling tasks we show that gradient-based architecture search achieves highly competitive results on CIFAR-10 and outperforms the state of the art on PTB. This is a very interesting result, considering that so far the best architecture search methods used non-differentiable search techniques, e.g. based on RL <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> or evolution <ref type="bibr" target="#b34">(Real et al., 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2018b)</ref>.</p><p>• We achieve remarkable efficiency improvement (reducing the cost of architecture discovery to a few GPU days), which we attribute to the use of gradient-based optimization as opposed to non-differentiable search techniques.</p><p>• We show that the architectures learned by DARTS on CIFAR-10 and PTB are transferable to ImageNet and WikiText-2, respectively.</p><p>The implementation of DARTS is available at https://github.com/quark0/darts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DIFFERENTIABLE ARCHITECTURE SEARCH</head><p>We describe our search space in general form in Sect. 2.1, where the computation procedure for an architecture (or a cell in it) is represented as a directed acyclic graph. We then introduce a simple continuous relaxation scheme for our search space which leads to a differentiable learning objective for the joint optimization of the architecture and its weights (Sect. 2.2). Finally, we propose an approximation technique to make the algorithm computationally feasible and efficient (Sect. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SEARCH SPACE</head><p>Following <ref type="bibr" target="#b43">Zoph et al. (2018)</ref>; <ref type="bibr" target="#b34">Real et al. (2018)</ref>; <ref type="bibr" target="#b21">Liu et al. (2018a;</ref><ref type="bibr" target="#b33">b)</ref>, we search for a computation cell as the building block of the final architecture. The learned cell could either be stacked to form a convolutional network or recursively connected to form a recurrent network.</p><p>A cell is a directed acyclic graph consisting of an ordered sequence of N nodes. Each node x (i) is a latent representation (e.g. a feature map in convolutional networks) and each directed edge (i, j) is associated with some operation o (i,j) that transforms x (i) . We assume the cell to have two input nodes and a single output node. For convolutional cells, the input nodes are defined as the cell outputs in the previous two layers <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref>. For recurrent cells, these are defined as the input at the current step and the state carried from the previous step. The output of the cell is obtained by applying a reduction operation (e.g. concatenation) to all the intermediate nodes.</p><p>Each intermediate node is computed based on all of its predecessors: A special zero operation is also included to indicate a lack of connection between two nodes. The task of learning the cell therefore reduces to learning the operations on its edges.</p><formula xml:id="formula_0">x (j) = i&lt;j o (i,j) (x (i) ) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTINUOUS RELAXATION AND OPTIMIZATION</head><p>Let O be a set of candidate operations (e.g., convolution, max pooling, zero) where each operation represents some function o(•) to be applied to x (i) . To make the search space continuous, we relax the categorical choice of a particular operation to a softmax over all possible operations:</p><formula xml:id="formula_1">ō(i,j) (x) = o∈O exp(α (i,j) o ) o ∈O exp(α (i,j) o ) o(x)<label>(2)</label></formula><p>where the operation mixing weights for a pair of nodes (i, j) are parameterized by a vector α (i,j) of dimension |O|. The task of architecture search then reduces to learning a set of continuous variables α = α (i,j) , as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. At the end of search, a discrete architecture can be obtained by replacing each mixed operation ō(i,j) with the most likely operation, i.e., o (i,j) = argmax o∈O α (i,j) o . In the following, we refer to α as the (encoding of the) architecture.</p><p>After relaxation, our goal is to jointly learn the architecture α and the weights w within all the mixed operations (e.g. weights of the convolution filters). Analogous to architecture search using RL <ref type="bibr" target="#b42">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b43">Zoph et al., 2018;</ref><ref type="bibr" target="#b32">Pham et al., 2018b)</ref> or evolution <ref type="bibr" target="#b22">(Liu et al., 2018b;</ref><ref type="bibr" target="#b34">Real et al., 2018)</ref> where the validation set performance is treated as the reward or fitness, DARTS aims to optimize the validation loss, but using gradient descent.</p><p>Denote by L train and L val the training and the validation loss, respectively. Both losses are determined not only by the architecture α, but also the weights w in the network. The goal for architecture search is to find α * that minimizes the validation loss L val (w * , α * ), where the weights w * associated with the architecture are obtained by minimizing the training loss w * = argmin w L train (w, α * ).</p><p>This implies a bilevel optimization problem <ref type="bibr" target="#b1">(Anandalingam &amp; Friesz, 1992;</ref><ref type="bibr" target="#b7">Colson et al., 2007)</ref> with α as the upper-level variable and w as the lower-level variable:</p><formula xml:id="formula_2">min α L val (w * (α), α) (3) s.t. w * (α) = argmin w L train (w, α)<label>(4)</label></formula><p>The nested formulation also arises in gradient-based hyperparameter optimization <ref type="bibr" target="#b25">(Maclaurin et al., 2015;</ref><ref type="bibr" target="#b31">Pedregosa, 2016;</ref><ref type="bibr" target="#b11">Franceschi et al., 2018)</ref>, which is related in a sense that the architecture α could be viewed as a special type of hyperparameter, although its dimension is substantially higher than scalar-valued hyperparameters such as the learning rate, and it is harder to optimize.</p><p>Algorithm 1: DARTS -Differentiable Architecture Search Create a mixed operation ō(i,j) parametrized by α (i,j) for each edge (i, j) while not converged do 1. Update architecture α by descending ∇ α L val (w − ξ∇ w L train (w, α), α) (ξ = 0 if using first-order approximation) 2. Update weights w by descending ∇ w L train (w, α) Derive the final architecture based on the learned α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">APPROXIMATE ARCHITECTURE GRADIENT</head><p>Evaluating the architecture gradient exactly can be prohibitive due to the expensive inner optimization. We therefore propose a simple approximation scheme as follows:</p><formula xml:id="formula_3">∇ α L val (w * (α), α) (5) ≈∇ α L val (w − ξ∇ w L train (w, α), α)<label>(6)</label></formula><p>where w denotes the current weights maintained by the algorithm, and ξ is the learning rate for a step of inner optimization. The idea is to approximate w * (α) by adapting w using only a single training step, without solving the inner optimization (equation 4) completely by training until convergence. Related techniques have been used in meta-learning for model transfer <ref type="bibr" target="#b10">(Finn et al., 2017)</ref>, gradientbased hyperparameter tuning <ref type="bibr" target="#b24">(Luketina et al., 2016)</ref> and unrolled generative adversarial networks <ref type="bibr" target="#b28">(Metz et al., 2017)</ref>. Note equation 6 will reduce to ∇ α L val (w, α) if w is already a local optimum for the inner optimization and thus ∇ w L train (w, α) = 0.</p><p>The iterative procedure is outlined in Alg. 1. While we are not currently aware of the convergence guarantees for our optimization algorithm, in practice it is able to reach a fixed point with a suitable choice of ξ<ref type="foot" target="#foot_0">1</ref> . We also note that when momentum is enabled for weight optimisation, the one-step unrolled learning objective in equation 6 is modified accordingly and all of our analysis still applies.</p><p>Applying chain rule to the approximate architecture gradient (equation 6) yields</p><formula xml:id="formula_4">∇ α L val (w , α) − ξ∇ 2 α,w L train (w, α)∇ w L val (w , α)<label>(7)</label></formula><p>where w = w − ξ∇ w L train (w, α) denotes the weights for a one-step forward model. The expression above contains an expensive matrix-vector product in its second term. Fortunately, the complexity can be substantially reduced using the finite difference approximation. Let be a small scalar<ref type="foot" target="#foot_1">2</ref> and w ± = w ± ∇ w L val (w , α). Then:</p><formula xml:id="formula_5">∇ 2 α,w L train (w, α)∇ w L val (w , α) ≈ ∇ α L train (w + , α) − ∇ α L train (w − , α) 2 (8)</formula><p>Evaluating the finite difference requires only two forward passes for the weights and two backward passes for α, and the complexity is reduced from</p><formula xml:id="formula_6">O(|α||w|) to O(|α| + |w|).</formula><p>First-order Approximation When ξ = 0, the second-order derivative in equation 7 will disappear. In this case, the architecture gradient is given by ∇ α L val (w, α), corresponding to the simple heuristic of optimizing the validation loss by assuming the current w is the same as w * (α). This leads to some speed-up but empirically worse performance, according to our experimental results in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table">2</ref>. In the following, we refer to the case of ξ = 0 as the first-order approximation, and refer to the gradient formulation with ξ &gt; 0 as the second-order approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">DERIVING DISCRETE ARCHITECTURES</head><p>To form each node in the discrete architecture, we retain the top-k strongest operations (from distinct nodes) among all non-zero candidate operations collected from all the previous nodes. The strength of an operation is defined as</p><formula xml:id="formula_7">exp(α (i,j) o ) o ∈O exp(α (i,j) o )</formula><p>. To make our derived architecture comparable with  ) , w (0) ) = (2, −2). The analytical solution for the corresponding bilevel optimization problem is (α * , w * ) = (1, 1), which is highlighted in the red circle. The dashed red line indicates the feasible set where constraint equation 4 is satisfied exactly (namely, weights in w are optimal for the given architecture α). The example shows that a suitable choice of ξ helps to converge to a better local optimum.</p><p>those in the existing works, we use k = 2 for convolutional cells <ref type="bibr" target="#b43">(Zoph et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2018a;</ref><ref type="bibr" target="#b34">Real et al., 2018)</ref> and k = 1 for recurrent cells <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>.</p><p>The zero operations are excluded in the above for two reasons. First, we need exactly k non-zero incoming edges per node for fair comparison with the existing models. Second, the strength of the zero operations is underdetermined, as increasing the logits of zero operations only affects the scale of the resulting node representations, and does not affect the final classification outcome due to the presence of batch normalization <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND RESULTS</head><p>Our experiments on CIFAR-10 and PTB consist of two stages, architecture search (Sect. 3.1) and architecture evaluation (Sect. 3.2). In the first stage, we search for the cell architectures using DARTS, and determine the best cells based on their validation performance. In the second stage, we use these cells to construct larger architectures, which we train from scratch and report their performance on the test set. We also investigate the transferability of the best cells learned on CIFAR-10 and PTB by evaluating them on ImageNet and WikiText-2 (WT2) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ARCHITECTURE SEARCH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">SEARCHING FOR CONVOLUTIONAL CELLS ON CIFAR-10</head><p>We include the following operations in O: 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5 dilated separable convolutions, 3 × 3 max pooling, 3 × 3 average pooling, identity, and zero. All operations are of stride one (if applicable) and the convolved feature maps are padded to preserve their spatial resolution. We use the ReLU-Conv-BN order for convolutional operations, and each separable convolution is always applied twice <ref type="bibr" target="#b43">(Zoph et al., 2018;</ref><ref type="bibr" target="#b34">Real et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2018a)</ref>.</p><p>Our convolutional cell consists of N = 7 nodes, among which the output node is defined as the depthwise concatenation of all the intermediate nodes (input nodes excluded). The rest of the setup follows <ref type="bibr" target="#b43">Zoph et al. (2018)</ref>; <ref type="bibr" target="#b21">Liu et al. (2018a)</ref>; <ref type="bibr" target="#b34">Real et al. (2018)</ref>, where a network is then formed by stacking multiple cells together. The first and second nodes of cell k are set equal to the outputs of cell k − 2 and cell k − 1, respectively, and 1 × 1 convolutions are inserted as necessary. Cells located at the 1/3 and 2/3 of the total depth of the network are reduction cells, in which all the operations adjacent to the input nodes are of stride two. The architecture encoding therefore is (α normal , α reduce ), where α normal is shared by all the normal cells and α reduce is shared by all the reduction cells.</p><p>Detailed experimental setup for this section can be found in Sect. A.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">SEARCHING FOR RECURRENT CELLS ON PENN TREEBANK</head><p>Our set of available operations includes linear transformations followed by one of tanh, relu, sigmoid activations, as well as the identity mapping and the zero operation. The choice of these candidate operations follows Zoph &amp; Le (2017); <ref type="bibr" target="#b32">Pham et al. (2018b)</ref>.</p><p>Our recurrent cell consists of N = 12 nodes. The very first intermediate node is obtained by linearly transforming the two input nodes, adding up the results and then passing through a tanh activation Each architecture snapshot is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on PTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 times with different random seeds, and report the median and the best (per run) validation performance of the architectures over time. As references, we also report the results (under the same evaluation setup; with comparable number of parameters) of the best existing cells discovered using RL or evolution, including NASNet-A <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> (2000 GPU days), AmoebaNet-A (3150 GPU days) <ref type="bibr" target="#b34">(Real et al., 2018)</ref> and ENAS (0.5 GPU day) <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>.</p><p>function, as done in the ENAS cell <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>. The rest of the cell is learned. Other settings are similar to ENAS, where each operation is enhanced with a highway bypass <ref type="bibr" target="#b41">(Zilly et al., 2016)</ref> and the cell output is defined as the average of all the intermediate nodes. As in ENAS, we enable batch normalization in each node to prevent gradient explosion during architecture search, and disable it during architecture evaluation. Our recurrent network consists of only a single cell, i.e. we do not assume any repetitive patterns within the recurrent architecture.</p><p>Detailed experimental setup for this section can be found in Sect. A.1.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ARCHITECTURE EVALUATION</head><p>To determine the architecture for final evaluation, we run DARTS four times with different random seeds and pick the best cell based on its validation performance obtained by training from scratch for a short period (100 epochs on CIFAR-10 and 300 epochs on PTB). This is particularly important for recurrent cells, as the optimization outcomes can be initialization-sensitive (Fig. <ref type="figure" target="#fig_2">3</ref>).</p><p>To evaluate the selected architecture, we randomly initialize its weights (weights learned during the search process are discarded), train it from scratch, and report its performance on the test set. We note the test set is never used for architecture search or architecture selection.</p><p>Detailed experimental setup for architecture evaluation on CIFAR-10 and PTB can be found in Sect. A.  <ref type="bibr" target="#b40">(Zhong et al., 2018)</ref> 3.54 39.8 96 8 RL AmoebaNet-A <ref type="bibr" target="#b34">(Real et al., 2018)</ref> 3.34 ± 0.06 3.2 3150 19 evolution AmoebaNet-A + cutout (Real et al., 2018)  †  3.12 3.1 3150 19 evolution AmoebaNet-B + cutout <ref type="bibr" target="#b34">(Real et al., 2018)</ref> 2.55 ± 0.05 2.8 3150 19 evolution Hierarchical evolution <ref type="bibr" target="#b22">(Liu et al., 2018b)</ref> 3.75 ± 0.12 15.7 300 6 evolution PNAS <ref type="bibr" target="#b21">(Liu et al., 2018a)</ref> 3.41 ± 0.09 3.2 225 8 SMBO ENAS + cutout <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref> 2.89 4.6 0.5 6 RL ENAS + cutout <ref type="bibr">(Pham et al.,</ref>  * Obtained by repeating ENAS for 8 times using the code publicly released by the authors. The cell for final evaluation is chosen according to the same selection protocol as for DARTS. † Obtained by training the corresponding architectures using our setup. ‡ Best architecture among 24 samples according to the validation error after 100 training epochs.</p><p>Table <ref type="table">2</ref>: Comparison with state-of-the-art language models on PTB (lower perplexity is better). Note the search cost for DARTS does not include the selection cost (1 GPU day) or the final evaluation cost by training the selected architecture from scratch (3 GPU days).  <ref type="bibr" target="#b27">(Merity et al., 2018)</ref> 60.7 58.8 24 --manual LSTM + skip connections <ref type="bibr" target="#b26">(Melis et al., 2018)</ref> 60.9 58.3 24 --manual LSTM + 15 softmax experts <ref type="bibr" target="#b38">(Yang et al., 2018)</ref>    <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> 27.5 9.0 4.9 558 2000 RL AmoebaNet-A <ref type="bibr" target="#b34">(Real et al., 2018)</ref> 25.5 8.0 5.1 555 3150 evolution AmoebaNet-B <ref type="bibr" target="#b34">(Real et al., 2018)</ref> 26.0 8.5 5.3 555 3150 evolution AmoebaNet-C <ref type="bibr" target="#b34">(Real et al., 2018)</ref> 24.3 7.6 6.4 570 3150 evolution PNAS <ref type="bibr" target="#b21">(Liu et al., 2018a)</ref> 25 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RESULTS ANALYSIS</head><p>The CIFAR-10 results for convolutional architectures are presented in Table <ref type="table" target="#tab_0">1</ref>. Notably, DARTS achieved comparable results with the state of the art <ref type="bibr" target="#b43">(Zoph et al., 2018;</ref><ref type="bibr" target="#b34">Real et al., 2018)</ref> while using three orders of magnitude less computation resources (i.e. 1.5 or 4 GPU days vs 2000 GPU days for NASNet and 3150 GPU days for AmoebaNet). Moreover, with slightly longer search time, DARTS outperformed ENAS <ref type="bibr" target="#b32">(Pham et al., 2018b</ref>) by discovering cells with comparable error rates but less parameters. The longer search time is due to the fact that we have repeated the search process four times for cell selection. This practice is less important for convolutional cells however, because the performance of discovered architectures does not strongly depend on initialization (Fig. <ref type="figure" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative Optimization Strategies</head><p>To better understand the necessity of bilevel optimization, we investigated a simplistic search strategy, where α and w are jointly optimized over the union of the training and validation sets using coordinate descent. The resulting best convolutional cell (out of 4 runs) yielded 4.16 ± 0.16% test error using 3.1M parameters, which is worse than random search.</p><p>In the second experiment, we optimized α simultaneously with w (without alteration) using SGD, again over all the data available (training + validation). The resulting best cell yielded 3.56 ± 0.10% test error using 3.0M parameters. We hypothesize that these heuristics would cause α (analogous to hyperparameters) to overfit the training data, leading to poor generalization. Note that α is not directly optimized on the training set in DARTS.</p><p>Table <ref type="table">2</ref> presents the results for recurrent architectures on PTB, where a cell discovered by DARTS achieved the test perplexity of 55.7. This is on par with the state-of-the-art model enhanced by a mixture of softmaxes <ref type="bibr" target="#b38">(Yang et al., 2018)</ref>, and better than all the rest of the architectures that are either manually or automatically discovered. Note that our automatically searched cell outperforms the extensively tuned LSTM <ref type="bibr" target="#b26">(Melis et al., 2018)</ref>, demonstrating the importance of architecture search in addition to hyperparameter search. In terms of efficiency, the overall cost (4 runs in total) is within 1 GPU day, which is comparable to ENAS and significantly faster than NAS <ref type="bibr" target="#b42">(Zoph &amp; Le, 2017)</ref>.</p><p>It is also interesting to note that random search is competitive for both convolutional and recurrent models, which reflects the importance of the search space design. Nevertheless, with comparable or less search cost, DARTS is able to significantly improve upon random search in both cases (2.76 ± 0.09 vs 3.29 ± 0.15 on CIFAR-10; 55.7 vs 59.4 on PTB).</p><p>Results in Table <ref type="table" target="#tab_4">3</ref> show that the cell learned on CIFAR-10 is indeed transferable to ImageNet. It is worth noticing that DARTS achieves competitive performance with the state-of-the-art RL method <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> while using three orders of magnitude less computation resources.</p><p>Table <ref type="table">4</ref> shows that the cell identified by DARTS transfers to WT2 better than ENAS, although the overall results are less strong than those presented in Table <ref type="table">2</ref> for PTB. The weaker transferability between PTB and WT2 (as compared to that between CIFAR-10 and ImageNet) could be explained by the relatively small size of the source dataset (PTB) for architecture search. The issue of transferability could potentially be circumvented by directly optimizing the architecture on the task of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL DETAILS</head><p>A.1 ARCHITECTURE SEARCH A.1.1 CIFAR-10</p><p>Since the architecture will be varying throughout the search process, we always use batch-specific statistics for batch normalization rather than the global moving average. Learnable affine parameters in all batch normalizations are disabled during the search process to avoid rescaling the outputs of the candidate operations.</p><p>To carry out architecture search, we hold out half of the CIFAR-10 training data as the validation set. A small network of 8 cells is trained using DARTS for 50 epochs, with batch size 64 (for both the training and validation sets) and the initial number of channels 16. The numbers were chosen to ensure the network can fit into a single GPU. We use momentum SGD to optimize the weights w, with initial learning rate η w = 0.025 (annealed down to zero following a cosine schedule without restart <ref type="bibr" target="#b23">(Loshchilov &amp; Hutter, 2016)</ref>), momentum 0.9, and weight decay 3 × 10 −4 . We use zero initialization for architecture variables (the α's in both the normal and reduction cells), which implies equal amount of attention (after taking the softmax) over all possible ops. At the early stage this ensures weights in every candidate op to receive sufficient learning signal (more exploration). We use Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2014)</ref> as the optimizer for α, with initial learning rate η α = 3 × 10 −4 , momentum β = (0.5, 0.999) and weight decay 10 −<ref type="foot" target="#foot_2">3</ref> . The search takes one day on a single GPU 3 .</p><p>A.1.2 PTB For architecture search, both the embedding and the hidden sizes are set to 300. The linear transformation parameters across all incoming operations connected to the same node are shared (their shapes are all 300 × 300), as the algorithm always has the option to focus on one of the predecessors and mask away the others. Tying the weights leads to memory savings and faster computation, allowing us to train the continuous architecture using a single GPU. Learnable affine parameters in batch normalizations are disabled, as we did for convolutional cells. The network is then trained for 50 epochs using SGD without momentum, with learning rate η w = 20, batch size 256, BPTT length 35, and weight decay 5 × 10 −7 . We apply variational dropout <ref type="bibr" target="#b12">(Gal &amp; Ghahramani, 2016)</ref> of 0.2 to word embeddings, 0.75 to the cell input, and 0.25 to all the hidden nodes. A dropout of 0.75 is also applied to the output layer. Other training settings are identical to those in <ref type="bibr" target="#b27">Merity et al. (2018)</ref>; <ref type="bibr" target="#b38">Yang et al. (2018)</ref>. Similarly to the convolutional architectures, we use Adam for the optimization of α (initialized as zeros), with initial learning rate η α = 3 × 10 −3 , momentum β = (0.9, 0.999) and weight decay 10 −3 . The search takes 6 hours on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ARCHITECTURE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 CIFAR-10</head><p>A large network of 20 cells is trained for 600 epochs with batch size 96. The initial number of channels is increased from 16 to 36 to ensure our model size is comparable with other baselines in the literature (around 3M). Other hyperparameters remain the same as the ones used for architecture search. Following existing works <ref type="bibr" target="#b32">(Pham et al., 2018b;</ref><ref type="bibr" target="#b43">Zoph et al., 2018;</ref><ref type="bibr" target="#b21">Liu et al., 2018a;</ref><ref type="bibr" target="#b34">Real et al., 2018)</ref>, additional enhancements include cutout (DeVries &amp; Taylor, 2017), path dropout of probability 0.2 and auxiliary towers with weight 0.4. The training takes 1.5 days on a single GPU with our implementation in PyTorch <ref type="bibr" target="#b30">(Paszke et al., 2017)</ref>. Since the CIFAR results are subject to high variance even with exactly the same setup <ref type="bibr" target="#b22">(Liu et al., 2018b)</ref>, we report the mean and standard deviation of 10 independent runs for our full model.</p><p>To avoid any discrepancy between different implementations or training settings (e.g. the batch sizes), we incorporated the NASNet-A cell <ref type="bibr" target="#b43">(Zoph et al., 2018)</ref> and the AmoebaNet-A cell <ref type="bibr" target="#b34">(Real et al., 2018)</ref> into our training framework and reported their results under the same settings as our cells.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of DARTS: (a) Operations on the edges are initially unknown. (b) Continuous relaxation of the search space by placing a mixture of candidate operations on each edge. (c) Joint optimization of the mixing probabilities and the network weights by solving a bilevel optimization problem. (d) Inducing the final architecture from the learned mixing probabilities.</figDesc><graphic url="image-1.png" coords="3,157.50,81.86,296.99,134.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Learning dynamics of our iterative algorithm when L val (w, α) = αw − 2α + 1 and L train (w, α) = w 2 −2αw+α 2 , starting from (α (0) , w (0) ) = (2, −2). The analytical solution for the corresponding bilevel optimization problem is (α * , w * ) = (1, 1), which is highlighted in the red circle. The dashed red line indicates the feasible set where constraint equation 4 is satisfied exactly (namely, weights in w are optimal for the given architecture α). The example shows that a suitable choice of ξ helps to converge to a better local optimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells on Penn Treebank. We keep track of the most recent architectures over time. Each architecture snapshot is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on PTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 times with different random seeds, and report the median and the best (per run) validation performance of the architectures over time. As references, we also report the results (under the same evaluation setup; with comparable number of parameters) of the best existing cells discovered using RL or evolution, including NASNet-A<ref type="bibr" target="#b43">(Zoph et al., 2018</ref>) (2000 GPU days), AmoebaNet-A (3150 GPU days)<ref type="bibr" target="#b34">(Real et al., 2018)</ref> and ENAS (0.5 GPU day)<ref type="bibr" target="#b32">(Pham et al., 2018b)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 4: Normal cell learned on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>2.1 and Sect. A.2.2, respectively. Besides CIFAR-10 and PTB, we further investigated the transferability of our best convolutional cell (searched on CIFAR-10) and recurrent cell (searched on PTB) by evaluating them on ImageNet (mobile setting) and WikiText-2, respectively. More details of the transfer learning experiments can be found in Sect. A.2.3 and Sect. A.2.4. Comparison with state-of-the-art image classifiers on CIFAR-10 (lower error rate is better). Note the search cost for DARTS does not include the selection cost (1 GPU day) or the final evaluation cost by training the selected architecture from scratch (1.5 GPU days).</figDesc><table><row><cell>Architecture</cell><cell cols="4">Test Error Params Search Cost #ops (%) (M) (GPU days)</cell><cell>Search Method</cell></row><row><cell>DenseNet-BC (Huang et al., 2017)</cell><cell>3.46</cell><cell>25.6</cell><cell>-</cell><cell>-</cell><cell>manual</cell></row><row><cell>NASNet-A + cutout (Zoph et al., 2018)</cell><cell>2.65</cell><cell>3.3</cell><cell>2000</cell><cell>13</cell><cell>RL</cell></row><row><cell>NASNet-A + cutout (Zoph et al., 2018)  †</cell><cell>2.83</cell><cell>3.1</cell><cell>2000</cell><cell>13</cell><cell>RL</cell></row><row><cell>BlockQNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art image classifiers on ImageNet in the mobile setting.</figDesc><table><row><cell>Architecture</cell><cell cols="5">Test Error (%) Params +× Search Cost top-1 top-5 (M) (M) (GPU days)</cell><cell>Search Method</cell></row><row><cell>Inception-v1 (Szegedy et al., 2015)</cell><cell>30.2</cell><cell>10.1</cell><cell>6.6</cell><cell>1448</cell><cell>-</cell><cell>manual</cell></row><row><cell>MobileNet (Howard et al., 2017)</cell><cell>29.4</cell><cell>10.5</cell><cell>4.2</cell><cell>569</cell><cell>-</cell><cell>manual</cell></row><row><cell cols="2">ShuffleNet 2× (g = 3) (Zhang et al., 2017) 26.3</cell><cell>-</cell><cell>∼5</cell><cell>524</cell><cell>-</cell><cell>manual</cell></row><row><cell>NASNet-A (Zoph et al., 2018)</cell><cell>26.0</cell><cell>8.4</cell><cell>5.3</cell><cell>564</cell><cell>2000</cell><cell>RL</cell></row><row><cell>NASNet-B (Zoph et al., 2018)</cell><cell>27.2</cell><cell>8.7</cell><cell>5.3</cell><cell>488</cell><cell>2000</cell><cell>RL</cell></row><row><cell>NASNet-C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A simple working strategy is to set ξ equal to the learning rate for w's optimizer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We found = 0.01/ ∇ w L val (w , α) 2 to be sufficiently accurate in all of our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">All of our experiments were performed using NVIDIA GTX 1080Ti GPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We did not conduct extensive hyperparameter tuning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Zihang Dai, Hieu Pham and Zico Kolter for useful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LSTM + augmented loss <ref type="bibr" target="#b16">(Inan et al., 2017)</ref> 91.5 87.0 28 -manual LSTM + continuous cache pointer <ref type="bibr" target="#b13">(Grave et al., 2016)</ref> -68.9 --manual LSTM <ref type="bibr" target="#b27">(Merity et al., 2018)</ref> 69.1 66.0 33 -manual LSTM + skip connections <ref type="bibr" target="#b26">(Melis et al., 2018)</ref> 69.1 65.9 24 -manual LSTM + 15 softmax experts <ref type="bibr" target="#b38">(Yang et al., 2018)</ref> 66.0 63 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks. By searching in a continuous space, DARTS is able to match or outperform the state-of-the-art non-differentiable architecture search methods on image classification and language modeling tasks with remarkable efficiency improvement by several orders of magnitude.</p><p>There are many interesting directions to improve DARTS further. For example, the current method may suffer from discrepancies between the continuous architecture encoding and the derived discrete architecture. This could be alleviated, e.g., by annealing the softmax temperature (with a suitable schedule) to enforce one-hot selection. It would also be interesting to investigate performance-aware architecture derivation schemes based on the shared parameters learned during the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 PTB</head><p>A single-layer recurrent network with the discovered cell is trained until convergence with batch size 64 using averaged SGD (Polyak &amp; Juditsky, 1992) (ASGD), with learning rate η w = 20 and weight decay 8 × 10 −7 . To speedup, we start with SGD and trigger ASGD using the same protocol as in <ref type="bibr" target="#b38">Yang et al. (2018)</ref>; <ref type="bibr" target="#b27">Merity et al. (2018)</ref>. Both the embedding and the hidden sizes are set to 850 to ensure our model size is comparable with other baselines. The token-wise dropout on the embedding layer is set to 0.1. Other hyperparameters remain exactly the same as those for architecture search.</p><p>For fair comparison, we do not finetune our model at the end of the optimization, nor do we use any additional enhancements such as dynamic evaluation <ref type="bibr" target="#b20">(Krause et al., 2017)</ref> or continuous cache <ref type="bibr" target="#b13">(Grave et al., 2016)</ref>. The training takes 3 days on a single 1080Ti GPU with our PyTorch implementation. To account for implementation discrepancies, we also incorporated the ENAS cell <ref type="bibr" target="#b32">(Pham et al., 2018b)</ref> into our codebase and trained their network under the same setup as our discovered cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 IMAGENET</head><p>We consider the mobile setting where the input image size is 224×224 and the number of multiply-add operations in the model is restricted to be less than 600M.</p><p>A network of 14 cells is trained for 250 epochs with batch size 128, weight decay 3 × 10 −5 and initial SGD learning rate 0.1 (decayed by a factor of 0.97 after each epoch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 WIKITEXT-2</head><p>We use embedding and hidden sizes 700, weight decay 5 × 10 −7 , and hidden-node variational dropout 0.15. Other hyperparameters remain the same as in our PTB experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SEARCH WITH INCREASED DEPTH</head><p>To better understand the effect of depth for architecture search, we conducted architecture search on CIFAR-10 by increasing the number of cells in the stack from 8 to 20. The initial number of channels is reduced from 16 to 6 due to memory budget of a single GPU. All the other hyperparameters remain the same. The search cost doubles and the resulting cell achieves 2.88 ± 0.09% test error, which is slightly worse than 2.76 ± 0.09% obtained using a shallower network. This particular setup may have suffered from the enlarged discrepancy of the number of channels between architecture search and final evaluation. Moreover, searching with a deeper model might require different hyperparameters due to the increased number of layers to back-prop through.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPLEXITY ANALYSIS</head><p>In this section, we analyze the complexity of our search space for convolutional cells.</p><p>Each of our discretized cell allows 4 k=1</p><p>(k+1)k 2 × (7 2 ) ≈ 10 9 possible DAGs without considering graph isomorphism (recall we have 7 non-zero ops, 2 input nodes, 4 intermediate nodes with 2 predecessors each). Since we are jointly learning both normal and reduction cells, the total number of architectures is approximately (10 9 ) 2 = 10 18 . This is greater than the 5.6 × 10 14 of PNAS <ref type="bibr" target="#b21">(Liu et al., 2018a)</ref> which learns only a single type of cell.</p><p>Also note that we retained the top-2 predecessors per node only at the very end, and our continuous search space before this final discretization step is even larger. Specifically, each relaxed cell (a fully connected graph) contains 2 + 3 + 4 + 5 = 14 learnable edges, allowing (7 + 1) 14 ≈ 4 × 10 12 possible configurations (+1 to include the zero op indicating a lack of connection). Again, since we are learning both normal and reduction cells, the total number of architectures covered by the continuous space before discretization is (4 × 10 12 ) 2 ≈ 10 25 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09582</idno>
		<title level="m">Connectivity learning in multi-branch networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical optimization: An introduction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Anandalingam</surname></persName>
		</author>
		<author>
			<persName><surname>Friesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Colson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04528</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimilano</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04426</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07432</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable gradient-based tuning of continuous regularization hyperparameters</title>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2952" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeparchitect: Automatically designing and training deep architectures</title>
		<author>
			<persName><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Authors&apos; implementation of &quot;Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<ptr target="https://github.com/melodyguan/enas/tree/2734eb2657847f090e1bc5c51c2b9cbf0be51887" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016. 2018a. 2018-04-05</date>
		</imprint>
	</monogr>
	<note>Hyperparameter optimization with approximate gradient</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><forename type="middle">B</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Differentiable neural network architecture search</title>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<meeting><address><addrLine>Richard Shin, Charles Packer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="page" from="4053" to="4061" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jen-Hao Rick</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning time/memory-efficient deep architectures with budgeted super networks</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Veniat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00046</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck: a high-rank rnn language model</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
