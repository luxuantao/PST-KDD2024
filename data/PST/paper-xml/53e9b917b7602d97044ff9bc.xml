<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">H-DIBCO 2010 -Handwritten Document Image Binarization Competition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Pratikakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics and Telecommunications National Center for Scientific Research &quot;Demokritos&quot; GR</orgName>
								<orgName type="laboratory">Computational Intelligence Laboratory</orgName>
								<address>
									<addrLine>15310 Agia Paraskevi</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering Democritus</orgName>
								<orgName type="institution">University of Thrace</orgName>
								<address>
									<postCode>GR-67100</postCode>
									<settlement>Xanthi</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Basilis</forename><surname>Gatos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics and Telecommunications National Center for Scientific Research &quot;Demokritos&quot; GR</orgName>
								<orgName type="laboratory">Computational Intelligence Laboratory</orgName>
								<address>
									<addrLine>15310 Agia Paraskevi</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Ntirogiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics and Telecommunications National Center for Scientific Research &quot;Demokritos&quot; GR</orgName>
								<orgName type="laboratory">Computational Intelligence Laboratory</orgName>
								<address>
									<addrLine>15310 Agia Paraskevi</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">H-DIBCO 2010 -Handwritten Document Image Binarization Competition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB6116D07208DEC8B1C6716845E59B1F</idno>
					<idno type="DOI">10.1109/ICFHR.2010.118</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>handwritten document image</term>
					<term>binarization</term>
					<term>performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>H-DIBCO 2010 is the International Document Image Binarization Contest which is dedicated to handwritten document images organized in conjunction with ICFHR 2010 conference. The general objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures. This paper reports on the contest details including the evaluation measures used as well as the performance of the 17 submitted methods along with a short description of each method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Handwritten document image binarization contributes significantly in the success of the handwritten document image recognition challenging task. Motivated by this, it is imperative to create a framework for benchmarking purposes, i.e. a benchmarking dataset along with an objective evaluation methodology in order to capture the efficiency of current image binarization practices for handwritten document images. To this end, following the success of DIBCO 2009 <ref type="bibr" target="#b0">[1]</ref> organized in conjunction with ICDAR'09, the follow-up of this contest has been organized, namely the Handwritten Document Image Binarization Contest (H-DIBCO 2010) in the context of ICFHR 2010 conference. In this contest, we focused on the evaluation of document image binarization methods using a variety of scanned handwritten documents for which the corresponding binary ground truth image has been created. The authors of submitted methods had initially registered in the competition and downloaded representative document image samples along with the corresponding ground truth. At a next step, all registered participants were required to submit their binarization executable. After the evaluation of all candidate methods, the testing dataset (10 handwritten images with the associated ground truth) along with the evaluation software has been released as publicly available in the following link:</p><p>(http://www.iit.demokritos.gr/~bgat/H-DIBCO2010/benchmark).</p><p>The remainder of the paper is structured as follows: Each of the methods submitted to the competition is briefly described in Section II. The evaluation measures are detailed in Section III. Experimental results are shown in Section IV while in Section V conclusions are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS AND PARTICIPANTS</head><p>Sixteen (16) distinct research groups have participated in the competition with seventeen (17) different algorithms Either certain participants have submitted more than one algorithm or representatives from more than one research groups joined efforts in a single submission. Brief descriptions of the methods are given in the following (The order of appearance is based upon the order of submission of the algorithm). The proposed method consists of four main steps. First, image contrast which is evaluated by local maximum and minimum is used to select the high contrast points. Second, the stroke edges which are extracted using Canny's method are combined with those high contrast points to produce a better edge map. Third, the document image is binarized by a local threshold which is decided based on the constructed edge map. Finally, some post-processing work is applied to improve the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>Ben-Gurion University, Computer Science department, Israel (I. Bar-Yosef, K. Kedem, I. Dinstein):</p><p>The proposed approach is composed by several steps, mainly adaptive binarization, removal of false objects and accurate local region-based active contour. The initial binarization step is based on normalized image gradients and local intensity averaging. At the second step, we analyze each connected component whether it should be omitted or processed in the last phase. Finally, a fast and accurate active contour method is applied based on local region statistics.</p><p>3) South University of Toulon-Var, LSIS, UMR CNRS 6168, France (T. Lelore and F. Bouchara): The algorithm is composed of three different steps. First, text position is roughly estimated thanks to an edge detection approach. Next to the previously estimated text location, a clustering algorithm is applied in order to produce a three valued image (Text, Background, Unknown). Finally, a postprocessing step assigns a class to 'Unknown' pixels thanks to heuristic rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) EPITA Research and Development Laboratory (LRDE), France (T. Geraud, G. Lazzara):</head><p>The method is based on a multi-scale implementation of Sauvola's binarization <ref type="bibr" target="#b1">[2]</ref>. A post-processing is applied to remove small connected components and fill holes inside characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Synchromedia Laboratory, École de technologie supérieure,</head><p>Montréal, Québec, Canada (R.F. Moghaddam and M. Cheriet): A generalized multi-scale adaptive binarization method <ref type="bibr" target="#b2">[3]</ref> is implemented. The method uses the multi-level classifiers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. The key classifier is the estimated background. Thanks to automatic estimation of the parameters, the method is able to provide binarized document images without any need for human interaction. The estimated parameters are the average stroke width and the average line height. In order to obtain better results, post-processing steps are also performed. The algorithm used is the same as the one used in the DIBCO contest in the framework of ICDAR 09 <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b6">[7]</ref> and is based on the toggle mapping operator <ref type="bibr" target="#b7">[8]</ref>. The image I is mapped on two functions: the morphological erosion E of the image and the morphological dilation D of the image. Then, for each pixel, if the given pixel value is closer to the erosion, it is marked as 'background' and if the pixel is closer to the dilation it is marked as 'foreground'. To reduce noise in homogeneous regions, pixels whose erosion and dilation are too close are excluded from the analysis. In other words, every pixel p with the difference between the dilation and the erosion is under a threshold t is considered as included in an homogeneous region and it is excluded from the analysis. Pixels are then classified into three classes: 'foreground', 'background' and 'homogeneous'. Finally, 'homogeneous' regions are assigned to 'foreground' or 'background' according to the class of their boundaries. Quality of results highly depends on the choice of t and this value is difficult to choose. A hysteresis threshold is used in order to reduce the critical effect of the threshold parameter.</p><p>Since the version used in DIBCO contest, two main improvements are to be noticed. The first improvement is that color images are now segmented several times. Each channel and the luminance are segmented. The final result is the union of results from each channel. Segmenting only the luminance leads to missing some regions. The second improvement is that 'background' is estimated and removed. The choice of t is difficult to set and the value is the same for the whole document but document may not be homogeneous (especially background variation due to stains). The background is coarsely detected by a large opening and then removed from the image. The resulting document is more homogeneous and the use of a unique t for the whole document is less problematic. 7) Institute of Space Technology, Pakistan (K. Khurshid): In the proposed algorithm NICK, where the thresholding formula has been derived from the basic Niblack algorithm <ref type="bibr" target="#b8">[9]</ref>, binarization threshold is found out for each pixel by taking into account its neighbouring pixels in a sliding window using the formula in the following:</p><formula xml:id="formula_0">2 2 1 ( ) NP i i p m T m k NP = - = + ∑<label>(1)</label></formula><p>where , k is the NICK factor ranging between -0.2 and -0.1 p i = pixel value of gray scale image NP = number of pixels in the window m = mean gray value of the NP pixels During experiments it has been observed that one major advantage of NICK over Niblack is that it considerably improves binarization for 'white' and 'light' page images by shifting down the binarization threshold to ensure that no 'non-text' areas are taken erroneously as 'text'. The value of NICK factor k can vary from -0.1 to -0.2 depending upon the application requirement. Value of k close to -0.2 makes sure that noise is all but eliminated but characters can break a little bit, while with values close to -0.1, some noise pixels can be left but the text will be extracted crisply and unbroken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8) CMM, Mines Paristech, France (J. Hernandez):</head><p>This method is based on a morphological operator named ultimate opening (UO). The method consists of three steps: Firstly, ultimate openings of height and width attributes are carried out in order to extract the most contrasted structures in both horizontal and vertical directions. UO provides two pieces of information, contrast R(I) and size q(I). Then, a classical Otsu's binarization <ref type="bibr" target="#b9">[10]</ref> is performed from contrast output. Finally, we apply a post-processing step: eliminate small and isolated structures, and remove the connections to the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9) NifiSoft, Saint-Etienne, France (A. Hassaïne):</head><p>The proposed method classifies each pixel as 'foreground' or 'background' taking into account its global k-means segmentation, the otsu segmentation, the values of its neighbors in these two segmented images, the values of the basic morphological operations with several sizes and the values of gaussian filters with several sizes. Moreover, if the color information is present, the hue and saturation gradient are also taken into account. All these descriptors are combined using a logistic regression to perform the classification. Foreground objects are identified with an analysis of a modified version of the connected component tree (taking advantage of the hyper-connection theory to allow non-flat nodes). For each leaf of the tree, the background level is defined by a criterion on the evolution of the "area/gray level" curve <ref type="bibr" target="#b10">[11]</ref>. Then, the edges of the flattened image are detected using a Sobel operator and a global Otsu threshold.</p><p>The local threshold value is then determined by the pixel values of the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11) Brigham Young University, USA (O. Nina):</head><p>This method combines a recursive version of the Otsu algorithm after a preprocess step of background normalization and smoothing using a bilateral filter and a final despeckle step. The proposed method can automatically process both color and gray-level images. In the first step, the input image is converted to a gray-level image. Then, in several steps, the binarized version of the input is created. The core of the method is based on the multi-level classifiers. These classifiers are capable to extract and identify information on different levels from local to global. On each level, a set of parameters is used as the a priori information of the document image. In an automatic way, these parameters are estimated by analysis of the input image. In this method, the most important level is the content level. Among many classifiers on this level, stroke map classifier is the key one.</p><p>Stroke map tries to capture pixels on the document image which may belong to the text strokes. This is achieved by analysis of image structures around the target pixel. This kernel-based classifier depends on an a priori parameter, i.e. the estimated stroke width. This parameter is computed automatically at the beginning of the process. In order to obtain better results, removal of the noise pixels is also performed. Size of a partition: During the process of division, the height and width of each partition is divided by 2 provided each having a value of greater than 2. If one of either height or width reaches that limit then its value is 'frozen' and the other's value is divided by 2 for subsequent level of partitioning. When both height and width reaches the limit, further division of sub-images is stopped. B.</p><p>Standard deviation of grey values of pixels in each partition: For any partition at any level, the standard deviation of the grey values of the pixels in that partition is calculated. If it is very low compared to the 0 th level of standard deviation then the corresponding partition is not histogram equalized. Rather, if the grey value of a pixel in that partition is less than the mean grey value of that partition then its membership value is given as 0.4 (i.e. forcing it towards black) otherwise, its membership value is given as 0.8 (i.e. forcing it towards white).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16) Jean Monnet University St. Etienne, FRANCE (S. Karaoglu):</head><p>This methodology presents an innovative methodology for binarization which does not require any parameter tuning by the user; and can deal with degradations which occur due to shadows, non-uniform illumination, low-contrast, large signal-dependent noise, smear and strain. A pre-processing procedure based connected opening for image enhancement has been applied to suppress the dark structures in the image. Difference of gamma functions in approximation with Generalized Extreme Value Distribution has been used to realize the proper threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17) Technological Educational Institute (TEI) of Athens, Greece (A. Nikolaou):</head><p>This method is a naive/rough exploration of the idea to use Integral Images of Histograms for binarization. In this implementation we assumed that most problems arise from dark background pixels due to stains etc. Using the Integral Image of Histograms, we can obtain the histogram of any rectangular region in constant complexity. For each pixel, the otsu threshold of a square window with the size of the minimum dimension of the grayscale image is attributed. As long as it is classified as 'foreground' we reduce the window by a factor of 11/20 up to 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION MEASURES</head><p>For the evaluation, the measures used comprise an ensemble of measures that have been widely used for evaluation purposes. These measures consist of (a) F-Measure; (b) pseudo F-Measure; (c) PSNR; (d) Negative Rate Metric and (e) Misclassification Penalty Metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. F-Measure 2 Recall Precision Recall Precision</head><formula xml:id="formula_1">FM × × = +<label>(2)</label></formula><p>where Recall TP TP FN = + , Precision TP TP FP = + TP, FP, FN denote the True positive, False positive and False Negative values, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. pseudo F-Measure</head><p>This measure has been introduced in <ref type="bibr" target="#b11">[12]</ref>. It was motivated by the fact that each character has a unique silhouette which can be represented by its skeleton. In this respect, we assume that a perfect recall can be achieved in the case that each skeleton constituent of the ground truth has been detected. Compared with the typical F-Measure as presented in III.A, there exist a difference which concerns an alternate measure for recall, namely pseudo-Recall (p-Recall) which is based on the skeletonized ground truth image.</p><p>The skeletonized ground truth image is defined by the following equations: 0, background ( , )</p><p>1, text</p><formula xml:id="formula_2">SG x y ⎧ = ⎨ ⎩<label>(3)</label></formula><p>Taking into account the skeletonized ground truth image, we are able to automatically measure the performance of any binarization algorithm in terms of recall.</p><p>p-Recall is defined as the percentage of the skeletonized ground truth image SG that is detected in the resulting MxN binary image B. p-Recall is given by the following equation: , PSNR is a measure of how close is an image to another. Therefore, the higher the value of PSNR, the higher the similarity of the two MxN images is. We consider that the difference between foreground and background equals to C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Negative Rate Metric (NRM)</head><p>The negative rate metric NRM is based on the pixel-wise mismatches between the GT and prediction. It combines the false negative rate NR FN and the false positive rate NR FP . It is denoted as follows:</p><p>2</p><formula xml:id="formula_4">FN FP NR NR NRM + =<label>(7)</label></formula><p>where</p><formula xml:id="formula_5">FN FN FN TP N NR N N = + , FP FP FP TN N NR N N = +</formula><p>N TP denotes the number of true positives, N FP denotes the number of false positives, N TN denotes the number of true negatives, N FN denotes the number of false negatives.</p><p>In contrast to F-Measure and PSNR, the binarization quality is better for lower NRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Misclassification penalty metric (MPM)</head><p>The Misclassification penalty metric MPM evaluates the prediction against the Ground Truth (GT) on an object-byobject basis. Misclassification pixels are penalized by their distance from the ground truth object's border. The normalization factor D is the sum over all the pixel-tocontour distances of the GT object. A low MPM score denotes that the algorithm is good at identifying an object's boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The H-DIBCO testing dataset consists of 10 handwritten document images for which the associated ground truth was built for the evaluation following a semi-automatic procedure based on <ref type="bibr" target="#b11">[12]</ref>. Representative examples of the dataset along with the associated ground truth images are shown in Fig. <ref type="figure" target="#fig_8">1(a)</ref>,(e) and Fig. <ref type="figure" target="#fig_0">1</ref>(b),(f), respectively. The document images of this dataset originate from the collections of the Library of Congress <ref type="bibr" target="#b12">[13]</ref>. The selection of the images in the dataset was made so that should contain representative degradations which appear frequently (e.g. variable background intensity, shadows, smear, smudge, low contrast, bleed-through). The evaluation was based upon the five distinct measures presented in Section III. At Table <ref type="table" target="#tab_0">I</ref>, the detailed performance of each algorithm for each encountered measure is given. The final ranking as shown in Table <ref type="table" target="#tab_0">I</ref> was calculated after sorting the accumulated ranking value for all measures. Specifically, let R(i,j) be the rank of the i th method using the j th measure, where i = 1 … t, t denotes the number of the binarization techniques used in the evaluation and j=1…m, m denotes the number of the evaluation measures. As denoted in <ref type="bibr" target="#b8">(9)</ref>, for each binarization method, the final ranking S i is achieved by the five rankings summation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The H-DIBCO 2010 Handwritten Document Image Binarization Contest attracted 16 research groups that are currently active in document image analysis. The general objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures. This objective is fulfilled by firstly, providing short descriptions of each submitted algorithm, thus, enabling the interested researchers to be aware of the highly performing algorithms and be able to push forward the state of the art by a new more advanced approach. Secondly, the public availability of the testing dataset and the evaluation software permits further benchmarking and comparison with H-DIBCO results. The authors hope that this effort will stimulate fruitful discussions which will provide substantial aid towards advancing the state of the art in handwritten document image binarization. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )</head><label>1</label><figDesc>National University of Singapore &amp; Institute for Infocomm Research, Singapore (B. Su, S. Lu, C.L. Tan):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 )</head><label>6</label><figDesc>EPITA Research and Development Laboratory (LRDE) &amp; MINES ParisTech, Centre de morphologie mathématique, Mathématiques et Systèmes (CMM), Fontainebleau, France (J. Fabrizio, B. Marcotegui):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 )</head><label>10</label><figDesc>Université de Strasbourg, Laboratoire des Sciences de l'Image, de l'Informatique et de la Télédétection -Équipe Modèles, Images et Vision (MIV), France (B. Perret): The proposed algorithm is based on two steps: (i) background removal and (ii) adaptive thresholding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>12 )</head><label>12</label><figDesc>Synchromedia Laboratory, École de technologie supérieure, Montréal, Québec, Canada (D. Rivest-Henault, R.F. Moghaddam and M. Cheriet):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>13 )</head><label>13</label><figDesc>SMCC, Jadavpur University, Kolkata, India (A.F. Mollah): In the proposed method first, the given image is pre-processed and then, edges are extracted. Some edges are filtered out and the remaining is used to extract text regions from the original image. These text regions are partitioned and then a global binarization technique (i.e. Otsu) is applied on each such partition.14) SmithCollege, MA, USA (N.R. Howe): This algorithm is built upon recent work in figure/ground segmentation for video. It uses the Laplacian operator to assess the local likelihood of foreground and background labels, Canny edge detection to identify likely discontinuities, and a graph cut implementation to efficiently find the minimum energy solution of an objective function combining these concepts. 15) MCKV Institute of Engineering, Dept. of CSE, Howrah, India &amp; Jadavpur University, CSE Department, Kolkata, India (S. Saha, S. Basu, M. Nasipuri, D.K. Basu): In the proposed technique, histogram equalization is applied over the whole image as well as at different levels of localization or partition independently over the original grey values of the pixels in those localized areas. Each of these partitions is again subdivided into four partitions. At any level, whenever the image is histogram equalized, each pixel gets a new grey value increasing contrast with its neighbours. This new grey value divided by 255 provides a membership of greyness for each pixel. Membership value is an indication of the inclination of the pixel towards black or white. Each pixel gets a set of membership values for global operation as well as for different levels or depth of local operations. Ultimately, all the membership values of each pixel that are obtained for different levels are combined to get the net membership value for each pixel. Net membership value is the weighted average of all the membership values for each pixel with respect to its grey value. During calculation, membership values obtained from local histogram equalization are given more weight than membership values obtained from global histogram equalization. Each pixel is then binarized depending on whether the net membership value of that pixel crosses 0.5 or not. If it crosses 0.5 then it is decided as 'white' otherwise, it is decided as 'black'. For proper binarization, selecting the number of level or depth becomes an issue. It depends on two factors: A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>of the i th false negative and the j th false positive pixel from the contour of the text in the GT image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>best performance is achieved by two algorithms that have been equally performed taking into account the final ranking over all measures. The top ranked algorithms are : Algorithm 1 which has been submitted by B. Su, S. Lu and C.L. Tan as collaboration between the National University of Singapore and the Institute for Infocomm Research in Singapore and Algorithm 2 which has been submitted by I. Bar-Yosef, K. Kedem, I. Dinstein from the Ben-Gurion University in Israel. Example binarization results of those two algorithms are shown in Fig. 1(c),(g) and Fig. 1(d),(h), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a),(e) Representative original handwritten images included in the testing dataset; (b),(f) Ground truth image; (c),(g) Binarization results from Algorithm 1; (d),(h) Binarization results from Algorithm 2</figDesc><graphic coords="6,63.45,576.26,224.03,50.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>EVALUATION RESULTS WRT TO THE MEASURES USED FOR ALL METHODS SUBMITTED TO H-DIBCO 2010</figDesc><table><row><cell cols="2">Rank Method</cell><cell>FM (%)</cell><cell>p-FM (%)</cell><cell>PSNR</cell><cell>NRM (x10 -2 )</cell><cell>MPM (x10 -3 )</cell></row><row><cell></cell><cell>1</cell><cell>91,50</cell><cell>93,58</cell><cell>19,78</cell><cell>5,981</cell><cell>0,492</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>89,70</cell><cell>95,15</cell><cell>19,15</cell><cell>8,180</cell><cell>0,288</cell></row><row><cell>2</cell><cell>3</cell><cell>91,78</cell><cell>94,43</cell><cell>19,67</cell><cell>4,771</cell><cell>1,334</cell></row><row><cell>3</cell><cell>14</cell><cell>89,73</cell><cell>90,11</cell><cell>18,90</cell><cell>5,776</cell><cell>0,412</cell></row><row><cell>4</cell><cell>10</cell><cell>87,98</cell><cell>90,83</cell><cell>18,26</cell><cell>7,677</cell><cell>0,377</cell></row><row><cell>5</cell><cell>13</cell><cell>86,85</cell><cell>92,43</cell><cell>18,19</cell><cell>9,989</cell><cell>0,231</cell></row><row><cell>6</cell><cell>8</cell><cell>86,13</cell><cell>88,8</cell><cell>17,62</cell><cell>8,686</cell><cell>0,378</cell></row><row><cell>7</cell><cell>17</cell><cell>85,71</cell><cell>91,68</cell><cell>17,63</cell><cell>10,42</cell><cell>1,188</cell></row><row><cell>8</cell><cell>16</cell><cell>83,22</cell><cell>91,24</cell><cell>17,19</cell><cell>13,15</cell><cell>0,507</cell></row><row><cell>9</cell><cell>12</cell><cell>85,06</cell><cell>89,63</cell><cell>17,56</cell><cell>10,48</cell><cell>3,807</cell></row><row><cell></cell><cell>9</cell><cell>83,51</cell><cell>86,88</cell><cell>17,24</cell><cell>13,02</cell><cell>0,949</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>82,99</cell><cell>87,55</cell><cell>17,02</cell><cell>12,83</cell><cell>0,695</cell></row><row><cell>11</cell><cell>15</cell><cell>81,39</cell><cell>81,91</cell><cell>15,60</cell><cell>5,534</cell><cell>1,666</cell></row><row><cell>12</cell><cell>6</cell><cell>84,95</cell><cell>86,89</cell><cell>16,82</cell><cell>11,47</cell><cell>48,63</cell></row><row><cell>13</cell><cell>7</cell><cell>82,29</cell><cell>89,56</cell><cell>16,61</cell><cell>13,19</cell><cell>2,844</cell></row><row><cell>14</cell><cell>5</cell><cell>73,51</cell><cell>78,96</cell><cell>15,95</cell><cell>19,95</cell><cell>1,044</cell></row><row><cell>15</cell><cell>4</cell><cell>57,73</cell><cell>66,42</cell><cell>14,29</cell><cell>28,41</cell><cell>1,107</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-4221-8/10 $26.00 © 2010 IEEE DOI 10.1109/ICFHR.2010.118</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work has been partially funded by the European Community's Seventh Framework Programme under grant agreement n° 215064 (project IMPACT).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ntirogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<title level="m">10th International Conference on Document analysis and Recognition (ICDAR&apos;09)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07-26">2009. 2009. July. 26-29, 2009</date>
			<biblScope unit="page" from="1375" to="1382" />
		</imprint>
	</monogr>
	<note>ICDAR 2009 Document Image Binarization Contest</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive Document Image Binarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sauvola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-scale framework for adaptive binarization of degraded document images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2186" to="2198" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RSLDI: Restoration of single-sided low-quality document images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3355" to="3364" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">EFDM: Restoration of Single-sided Low-quality Document Images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
		<idno>ICFHR&apos;08</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Application of Multi-level Classifiers and Clustering for Automatic Word-spotting in Historical Document Images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
		<idno>ICDAR&apos;09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="511" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text segmentation in natural scenes using Toggle-Mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fabrizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP&apos;09)</title>
		<meeting><address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">November 7-10, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From pixels to features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<editor>J.C. Simon</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Elsevier</publisher>
			<pubPlace>North-Holland</pubPlace>
		</imprint>
	</monogr>
	<note>Toggle mappings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<title level="m">An Introduction to Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="115" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Thresholding Selection Method from Gray-level Histogram</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-03">Mar. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From hyperconnections to hypercomponent tree : Application to document image binarization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Slezak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Digital Geometry and Mathematical Morphology</title>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Objective Evaluation Methodology for Document Image Binarization Techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ntirogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th IAPR International Workshop on Document Analysis Systems (DAS 2008)</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Nara Prefectural New Public Hall</publisher>
			<date type="published" when="2008">September 17-19, 2008</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="http://memory.loc.gov/ammem" />
		<title level="m">Library of Congress</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
