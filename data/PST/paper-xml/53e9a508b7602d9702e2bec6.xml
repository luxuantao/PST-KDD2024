<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrowdReranking: Exploring Multiple Search Engines for Visual Search Reranking *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xshua@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CrowdReranking: Exploring Multiple Search Engines for Visual Search Reranking *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4EDD94B8D02DFBE18DF188F2BD704FB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval models Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Visual search</term>
					<term>search reranking</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing approaches to visual search reranking predominantly focus on mining information within the initial search results. However, the initial ranked list cannot provide enough cues for reranking by itself due to the typically unsatisfying visual search performance. This paper presents a new method for visual search reranking called CrowdReranking, which is characterized by mining relevant visual patterns from image search results of multiple search engines which are available on the Internet. Observing that different search engines might have different data sources for indexing and methods for ranking, it is reasonable to assume that there exist different search results yet certain common visual patterns relevant to a given query among those results. We first construct a set of visual words based on the local image patches collected from multiple image search engines. We then explicitly detect two kinds of visual patterns, i.e., salient and concurrent patterns, among the visual words. Theoretically, we formalize reranking as an optimization problem on the basis of the mined visual patterns and propose a close-form solution. Empirically, we conduct extensive experiments on several real-world search engines and one benchmark dataset, and show that the proposed CrowdReranking is superior to the state-of-the-art works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The explosive growth and widespread accessibility of community contributed media contents on the Internet have led to surge of research activity in visual search <ref type="bibr" target="#b11">[11]</ref>. Due to the great success of text search, most popular image and video search engines, such as Google <ref type="bibr">[5]</ref>, Yahoo! <ref type="bibr" target="#b23">[23]</ref>, and Live <ref type="bibr" target="#b16">[16]</ref>, build upon text search techniques by using the text information associated with media contents. This kind of visual search approach has proven unsatisfying as it entirely ignores the visual contents as a ranking signal.</p><p>To address this issue, search reranking has received increasing attention in recent years. It is defined as reordering visual documents based on multimodal cues to improve search performance. The documents might be images or video shots. The research on visual search reranking has proceeded along two dimensions from the perspective of the external knowledge used: self-reranking which only uses initial search results, and query-example-based reranking which leverages user-provided query examples. In this paper, we propose a new method, called CrowdReranking, by exploring multiple image and video search engines or sites which are available on the Internet.</p><p>The first dimension predominantly focuses on detecting the recurrent patterns solely in the initial search results, followed by using such patterns to perform reranking <ref type="bibr" target="#b6">[6]</ref> [7] <ref type="bibr" target="#b9">[9]</ref>  <ref type="bibr" target="#b12">[12]</ref>  <ref type="bibr" target="#b20">[20]</ref>. However, it is well-known that existing visual search engines do not have satisfying performance, mainly because of the noisy and even missing surrounding text. Therefore, the initial ranked list usually cannot provide enough cues to detect recurrent patterns for reranking. For example, it can be observed in Figure <ref type="figure" target="#fig_0">1</ref> that there are few relevant results in the top search results of TRECVID 2007 <ref type="bibr" target="#b15">[15]</ref>, Engine I, and Engine II<ref type="foot" target="#foot_0">1</ref> . It is difficult to achieve satisfying reranking if we solely mine information within the initial search results.</p><p>To address this issue, the second dimension leverages a few query examples to train the reranking models <ref type="bibr" target="#b10">[10]</ref>  <ref type="bibr" target="#b13">[13]</ref> [17] <ref type="bibr" target="#b24">[24]</ref>. The search performance can be improved due to the external knowledge derived from these examples. The model-based methods in this dimension assume the availability of a large collection of training samples. However, it is typical that training examples are too expensive to obtain as users are reluctant to provide enough query examples while searching.</p><p>On one hand, existing approaches have not been widely applied due to the very limited information they can mine for guiding the reranking process. On the other, there exists rich crowdsourcing knowledge available online that can be used for reranking. For example, there are a number of search engines (e.g., Google <ref type="bibr">[5]</ref>, Yahoo! <ref type="bibr" target="#b23">[23]</ref>, and Live <ref type="bibr" target="#b16">[16]</ref>) and social media sites (e.g., Flickr <ref type="bibr" target="#b4">[4]</ref>) supporting different kinds of visual search abilities. The following observations inspire the idea of leveraging search results from multiple visual search engines for reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Different search engines have different search results</head><p>as they might have different data sources and metadata for indexing, as well as different search and filtering methods for ranking, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Using search results from different engines can inform and complement the relevant visual information for each other. Thus, the reranking performance can be significantly improved due to the richer knowledge involved.</p><p>• Although a single search engine cannot always have enough cues for reranking, it is reasonable to assume that across the search results from multiple engines, there are common visual patterns relevant to a given query. For example, it would be easier to find the relevant visual patterns about "street market scene" among the results from multiple engines rather than each individual in Figure <ref type="figure" target="#fig_0">1</ref>. The repetition in a large fraction of the images is an important signal that can be used to infer a common "visual pattern" throughout the multiple sets.</p><p>Motivated by the above observations, we propose a new method for visual search reranking, called CrowdReranking. Rather than only uses a single search engine, CrowdReranking is characterized by mining relevant visual patterns from the search results of multiple search engines. Given a query, finding the representative visual patterns, as well as their relative strengths and relations in multiple sets of images is the basis of CrowdReranking proposed in this study. As local features have proven effective for visual recognition in a large-scale image set <ref type="bibr" target="#b3">[3]</ref> [9], we first construct a set of representative visual words based on the local image patches from multiple image search engines. We then explicitly detect two kinds of visual patterns relevant to the given query, i.e., salient and concurrent patterns, among the visual words. The salient pattern indicates the importance of each visual word, while the concurrent pattern expresses the interdependent relations among the visual words. The concurrent pattern, usually called context, is known to be informative for vision applications <ref type="bibr" target="#b22">[22]</ref>. Intuitively, if a visual word is with high importance for a given query, then other words cooccurring with it would be prioritized. Therefore, we adopt a graph propagation method like PageRank <ref type="bibr">[1] [9]</ref>, by treating visual words as pages and their concurrence as hyperlinks. The stationary probabilities over the PageRank graph are represented as the salient pattern, while the concurrent pattern is estimated based on the propagation of the weights of graph edges. We then formalize reranking as an optimization problem which maximally preserves the initial ranked list and simultaneously matches the reranked list against the learned visual patterns as much as possible.</p><p>The remainder of this paper is organized as follows. Section 2 introduces the CrowdReranking approach. Section 3 shows experiments, followed by conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CROWDRERANKING APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The objective of CrowdReranking is to mine certain visual patterns which are relevant to a given query from the search results of multiple search engines, then these patterns are used to obtain an optimal reranked document list which has the best match against the mined patterns. The flowchart of CrowdReranking is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. Given a textual query, an initial ranked list of visual documents (images or video shots) is obtained by text search technique based on image surrounding text or video transcripts. Meanwhile, this query is fed to multiple image and video search engines or sites (e.g., Google, Yahoo!, Live, and Flickr image and video search engines) to obtain different lists of search results. First, we detect a set of representative visual words by clustering the local features of image patches which are collected from the search results of multiple engines. We then construct a graph in which the visual words are nodes and the edges between the nodes are weighted by their concurrent relations. Through a propagation process which takes the initial ranks and the reliability of search engines into account, we can explicitly detect the relevant visual patterns, including salient and concurrent patterns. The reranking is then formalized as an optimization problem on the basis of the mined visual patterns, as well as the Bag-of-Words (BoW) representation of the initial ranked list. A close-form solution can be achieved to this optimization problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Formulation</head><p>Suppose we have a document set X with N documents to be reranked, where X = {x1, x2, . . . , xN }. Let ri and ri denote the initial ranking score (i.e., relevance) and reranking score for document xi. In the reranking problem, the initial ranking scores are to be preserved as they indicate the relevance information from text perspective. On the other hand, the reranked list should be consistent with the learned knowledge (i.e., visual patterns) from multiple search engines. Therefore, we can formulate the reranking problem by minimizing the following energy function:</p><formula xml:id="formula_0">E(r) = Dist(r, r) -λCons(r, K)<label>(1)</label></formula><p>where r = [r1, r2, . . . , rN ] T and r = [r1, r2, . . . , rN ] T . Dist(r, r) corresponds to the ranking distance, while Cons(r, K) corresponds to the consistence between the reranked list r and the learned knowledge K. K indicates the learned knowledge from multiple search engines which also corresponds to the mined visual patterns in this paper. The parameter λ tunes the contribution of knowledge K to the reranked list. When λ = 0, the reranked list r will be the same as the initial ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Pattern Mining</head><p>As we look for common visual patterns across different ranked lists of images, the pattern representations should be invariant to a variety of degradations (scale, orientation, global or local appearance, and so on). We adopt scaleinvariant feature transform (SIFT) descriptor with a Difference of Gaussian (DoG) interest point detector in this work, as it has proven to be effective for large-scale visual recognition <ref type="bibr" target="#b9">[9]</ref>  <ref type="bibr" target="#b14">[14]</ref>. The interest point is referred to as local salient patch in this paper, each associated with a 128-dimensional feature vector. We further adopt K-Means to cluster the similar patches into "visual words," and use Bag-of-Words (BoW) to represent each image <ref type="bibr" target="#b19">[19]</ref>. For a given query, the visual patterns K will be mined from the visual words collected from the search results of multiple search engines.</p><p>Specifically, we investigate two kinds of visual patterns in this work: salient and concurrent patterns. The salient pattern indicates the importance of each visual word, while concurrent pattern expresses the interdependent relations among the visual words. The premise of using concurrence as hyperlinks is that if a visual word is viewed important, then other co-occurring or similar visual words also might be of interest. For example, for a query "beach," visual words extracted at the "sea" patch is ranked high, then the co-occurring "sand" and "sky" patches should be also prioritized. Therefore, we adopt PageRank-like propagation framework and construct a graph with the visual words as nodes and co-occurrence between the visual words as hyperlinks. Suppose we have L visual words, the visual pattern K is expressed as the combination of salient pattern q and concurrent pattern C:</p><formula xml:id="formula_1">K K(q, C)<label>(2)</label></formula><p>where q = [q1, q2, . . . , qL] T is a L-dimensional vector with each element indicating the salience or importance of a visual word, and C = [cmn] (L×L) is a L × L matrix with each element indicating the hyperlink between two words. Let W (j) denote the set of words that contain patches connecting to the patches in word j, and P (i, j) denote the set of patches in word i connecting to the patches in word j, as shown in Figure <ref type="figure" target="#fig_3">3</ref>. The salience of word j after the k-th iteration, qj(k), is given in a way similar to PageRank <ref type="bibr" target="#b1">[1]</ref>:</p><formula xml:id="formula_2">qj(k) = εqj(0) + (1 -ε) X i∈W (j) |P (i, j)| P L k=1 |P (i, k)| qi(k -1)<label>(3)</label></formula><p>where |•| denotes the size of a set, ε (0 &lt; ε &lt; 1) is the weight balancing the initial and the propagated salience scores. qj(0) = P x j , x j denotes the normalized ranking score <ref type="bibr" target="#b7">[7]</ref> of the -th patch from word j in the initial ranked list. Accordingly, the concurrent pattern is given by the average weight between word i and j over the graph:</p><formula xml:id="formula_3">cij = " |P (i, j)| P L k=1 |P (i, k)| + |P (j, i)| P L k =1 |P (j, k )| « × 0.5<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reranking</head><p>This section discusses the ranking distance Dist(r, r) and consistence Cons(r, K) based on the mined knowledge K(q, C). Recently, some researchers have proposed various ranking distances, mainly including pointwise and pairwise distances as follows.</p><p>• Pointwise ranking distance <ref type="bibr" target="#b6">[6]</ref>:</p><formula xml:id="formula_4">Dist(r, r) = X n (rn -rn) 2<label>(5)</label></formula><p>• Pairwise ranking distance <ref type="bibr" target="#b20">[20]</ref>:</p><formula xml:id="formula_5">Dist(r, r) = X m,n " 1 - rm -rn rm -rn « 2<label>(6)</label></formula><p>For the consistency, most existing reranking methods solely use the visual consistency within the initial search results, assuming that visual documents similar in appearance are with similar ranks <ref type="bibr" target="#b6">[6]</ref>  <ref type="bibr" target="#b20">[20]</ref>. As we have mentioned, only mining within initial ranked list is not reasonable when there exist much more irrelevant documents than relevant ones. In this work, we leverage the mined visual pattern K to define a more suitable consistence.</p><p>Let fn = [fn1, fn2, . . . , fnL] T denote the BoW representation for image xn <ref type="bibr" target="#b19">[19]</ref>, the consistence is defined by</p><formula xml:id="formula_6">Cons (r, K) = X n X i qifni + X i,j cijfnifnj ! rn<label>(7)</label></formula><p>Let s = [s1, s2, . . . , sN ] T denote the vector with entries sn = P i qifni + P i,j cijfnifnj. s can be viewed as the cosine similarity between the visual representation of image xn and the mined visual patterns. Based on the two types of ranking distances, we integrate the above two ranking distances in equation ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_5">6</ref>), as well as the consistence in <ref type="bibr" target="#b7">(7)</ref>, to equation (1), and have the following two objective reranking functions.</p><p>• Reranking function using pointwise ranking distance:</p><formula xml:id="formula_7">min r  X n (rn -rn) 2 -λ X n " X i q i f ni + X i,j c ij f ni f nj " rn ff<label>(8)</label></formula><p>We called this optimization problem as pointwise miningbased reranking. We can obtain the solution of Equation ( <ref type="formula" target="#formula_7">8</ref>) as follows (proven in Appendix):</p><formula xml:id="formula_8">r = 1 2 (2r + λs) (9)</formula><p>Obviously, equation ( <ref type="formula">9</ref>) consists of two parts, i.e., r and s, which corresponds to the initial ranked list and the learned knowledge, respectively. Therefore, the pointwise reranking can be also viewed as the linear fusion between the initial ranked list and the ranked list learned from the online sources.</p><p>• Reranking function using pairwise ranking distance:</p><formula xml:id="formula_9">min r  X m,n " 1 - rm -rn rm -rn " 2 -λ X n " X i q i f ni + X i,j c ij f ni f nj " rn ff<label>(10)</label></formula><p>Algorithm 1 The CrowdReranking algorithm.</p><p>Input: The initial ranked list r = [r 1 , r2 , ..., rN ] T . Output: The reranked list r = [r 1 , r 2 , ..., r N ] T . Algorithm:</p><p>1: Collect data from multiple search engines by the same query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Extract SIFT features for each image and construct the visual words by K-Means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Calculate salient and concurrent patterns by equation ( <ref type="formula" target="#formula_2">3</ref>) and (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Obtain the reranked list according to equation ( <ref type="formula">9</ref>) or <ref type="bibr" target="#b11">(11)</ref>.</p><p>We called this optimization problem as pairwise miningbased reranking. The solution of equation ( <ref type="formula" target="#formula_9">10</ref>) with a constraint rN = 0 can be derived as (proven in Appendix):</p><formula xml:id="formula_10">r = 1 2 ∆-1 (2c + λs) (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>where c = 2(Ue) T , c and s are obtained by replacing the last element of c and s with zero, respectively. ∆ = D -U, where U = [umn] (N ×N ) denotes an anti-symmetric matrix with umn = 1 rm-rn , and D is a diagonal matrix with its (nn)-element dnn = P N n=1 umn. ∆ is obtained by replacing the last row of ∆ with [0, . . . , 0, 1] (1×N ) .</p><p>In equation <ref type="bibr" target="#b11">(11)</ref>, there are also two parts, i.e., ∆-1 c and ∆-1 s, in which ∆-1 c is solely determined by the initial ranked list, while ∆-1 s can be viewed as the learned knowledge biased by the initial ranked list. Therefore, the reranked list can be also viewed as the combination of the initial ranked list and the learned knowledge.</p><p>In summary, we get the flowchart of CrowdReranking in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We conducted experiments on two image datasets. One is real-world image data collected from three popular image search engines (i.e., Engine I, II, and III) and a photo sharing site (i.e., Engine IV, also called "Web set" in short). We selected 29 representative top queries from the query history of one popular search engine. These queries consist of a variety of types, including objects, people, event, entertainments, location, and time<ref type="foot" target="#foot_1">2</ref> . For each query, we collected about top 1,000 images returned by each search engine. After some parts of unaccessible searched images are filtered, the dataset contains 74,000 images in total, and the ranks of these images are kept as the initial ranked list.</p><p>The other dataset is the benchmark TRECVID 2007 test set <ref type="bibr" target="#b21">[21]</ref> (called "TV07 set" in short), which consists of 18,142 video shots. The search and reranking are performed on the basis of the keyframe and transcript for each shot. There are 24 text queries with their ground truth of relevance. The description of each query can be found in <ref type="bibr" target="#b21">[21]</ref>. The text   search results by Okapi BM25 <ref type="bibr" target="#b18">[18]</ref> were used as the initial ranked list in the following experiments.</p><p>We can find that most queries in the real dataset are represented as a general word or a simple phrase, while those in TV07 usually describe an event or a scene with much more words. The TV07 has much lower text search performance as the queries are more challenging. Therefore, the two datasets can be viewed as representative and complementary datasets in both research and real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodologies</head><p>For the Web set from the four sites, the relevance of each returned image to the corresponding query was manually labeled by three subjects on a 1-4 scales: (1) "irrelevant," (2) "fair," (3) "relevant," and (4) "excellent." The ground truth relevance of each image is the median scale of the three evaluations. We adopt NDCG as performance metric since it is widely used to deal with multiple relevance levels <ref type="bibr" target="#b8">[8]</ref>. Given a query q, the NDCG score at the depth d in the ranked documents is defined by</p><formula xml:id="formula_12">N DCG@d = Z d X d j=1 2 r j -1 log(1 + j) (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where r j is the rating of the j-th document, Z d is a normalization constant and is chosen so that a perfect ranking's N DCG@d value is 1. For TV07 set, we also used the NDCG to evaluate the performance based on the available relevance (i.e., two scales of "positive" or "negative").</p><p>In our experiments, for the Web set, we reranked the search result of each search engine by using the other three engines. For TV07 set, we used the mined visual patterns from all four search engines. We used top 100 images from each engine for visual pattern mining and top 500 images in the initial search results for reranking, since it is typical that there are very few relevant images after the top 500 search results. The number of visual words is empirically set to 2,000 <ref type="bibr" target="#b19">[19]</ref>.</p><p>To demonstrate the effectiveness of the proposed Crow-dReranking methods which include pointwise mining-based reranking and pairwise mining-based reranking, we compared with the following three state-of-the-art reranking methods.  For all these approaches, we select parameters according to their globally best performance setting in our experiments.</p><p>• Random walk reranking <ref type="bibr" target="#b6">[6]</ref>.</p><p>• Bayesian reranking <ref type="bibr" target="#b20">[20]</ref>. The first two are representative methods in the first research dimension for visual search reranking (i.e., self-ranking).</p><p>• NPRF reranking <ref type="bibr" target="#b24">[24]</ref>. The third is a representative method in the second research dimension (i.e., queryexample-based reranking). It uses the query examples as "positive" documents and randomly samples the low-rank images as "negative" to train the SVM models based on global image features. To fairly compare with our approach, we directly use the top-ranked images from the multiple search engines as the query examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Evaluation of Reranking Performance</head><p>The experimental results are shown in Figure <ref type="figure" target="#fig_5">4</ref> and 5, from which we can see that the proposed two reranking approaches outperform the others. Moreover, it can be observed that:</p><p>• The improvements of the proposed CrowdReranking over Random walk and Bayesian reranking methods indicate that mining external knowledge, especially the visual patterns from the search results of multiple search engines, can benefit reranking a lot.</p><p>• The superiority of the proposed CrowdReranking to NPRF-based reranking indicates that the mined visual patterns from the local visual words are more effective than the global features discovered solely from topranked images for reranking.</p><p>• NPRF reranking did not outperform Random walk reranking, Bayesian reranking, as well as CrowdReranking. The reasons are two-fold: (1) Rather than the object-related queries in the Web set, the queries in TV07 set are more specific to event and scene, which in general represent more diverse appearance. Therefore, it is difficult to discover the common visual patterns from external sources. In contrast, the proposed CrowdReranking which mines the patterns from visual words on the basis of local features can effectively leverage the external sources and thus achieve better performance.</p><p>(2) The "negative" samples randomly selected from the low-ranked images have diverse appearances, which makes the models in NPRF-based reranking do not have a good generalization ability.</p><p>• Among these reranking methods, Random walk and Bayesian reranking respectively has three parameters (i.e., the number of nearest-neighbors, the trade-off parameter and the scaling parameter in Gaussian kernel), and about 400 sets of parameters in total to select the optimal one; while NPRF has two parameters (i.e., C and γ in RBF kernel of SVM), and 70 sets of parameters in total. In addition, different sets of parameters are elaborately selected for different search engines. As a result, the robustness of these methods is limited. In contrast, the proposed CrowdReranking has only one parameter λ. From the Figure <ref type="figure" target="#fig_9">7</ref>, we can see that it reaches the peak at a relatively stable value for different search engines.</p><p>Furthermore, the performance improvements are consistent and stable-most queries are improved compared to the initial ranked lists and have better performance than the other methods, as shown in Figure <ref type="figure" target="#fig_8">6</ref>. The performance of some queries has significant improvement even their initial search results are extremely poor, such as the query of "(198) a door being opened" and "(217) a road taken from a moving vehicle through the front windshield." However, we can see that the performance of some queries degrades with the random walk and Bayesian reranking. This indicates that only mining within initial ranked list could not be always enough to obtain satisfying reranking results. On the other hand, NPRF-based reranking, which also mines the external online sources, is not stable in terms of reranking.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the top 10 images of different engines and reranking approaches. It is difficult to discover relevant visual patterns solely from the initial search results for the given query in TV07 set as there are only two samples somewhat relevant. However, based on the search results from multiple search engines, we can mine salient and concurrent visual patterns about the query. As a result, the relevant documents can be ranked higher in the reranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation of Trade-off Parameter λ</head><p>We also investigated the performance of CrowdReranking with different tradeoff parameter λ in equation <ref type="bibr" target="#b1">(1)</ref>. Figure <ref type="figure" target="#fig_9">7</ref> shows the performance of the pointwise and pairwise mining-based reranking methods with different λ in terms of N DCG@10. From the figures, we can see that the performance curve is like a "Λ" shape as λ increases.</p><p>• As shown in Figure <ref type="figure" target="#fig_9">7</ref> (a), the performance of pointwise mining-based reranking increases when λ increases and arrives at the peak at λ = 0.1 on Web set, while it reaches the peak at λ = 0.5 on TV07 set. As aforementioned, TV07 set has more challenging queries (i.e., long and complex queries) and thus has worse performance compared with Web set. Basically, a relatively larger λ would be more suitable for a worse initial search results as in TV07 set. It can be concluded that the trade-off parameter λ can be set according to the performance of initial search results. The same conclusion can be drwan in the pairwise mining-based reranking, as shown in Figure <ref type="figure" target="#fig_9">7</ref> (b).</p><p>• When λ goes to infinity, the reranking process relies almost entirely on the learned knowledge with the initial search results excluded. The reranking will reduce to the query-by-example (QBE) search problem and the performance will significantly degrade. From this observation, we can conclude that both the initial search results and external knowledge play important roles in the reranking. However, it remains an open problem on how to find an optimal λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we have proposed a novel visual reranking method by mining relevant visual patterns from the search results which are available from existing search engines on the Internet. To the best of our knowledge, the proposed CrowdReranking represents the first attempt towards leveraging crowdsourcing knowledge for visual reranking.</p><p>There are several open problems for further studies. First, the number of search engines or online resources is still limited in this work. It would be a promising topic to discover more search engines and sites and investigate how many engines and sites are enough for reranking. Second, most of current search engines are multilingual systems. We can use the results from multilingual systems for reranking. Third, the well-organized online knowledge like Wikipedia and Mediapedia can be also leveraged for visual reranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The exemplary top seven search results selected from TRECVID 2007 [21] and four search engines. The query is "street market scene" which is in the query list of TRECVID 2007 video search task. The green rectangles indicate relevant results. Note that the search results of TRECVID 2007 are obtained by our submission based on automatic text search [15]. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[ 0 .</head><label>0</label><figDesc>55, 0.82, 0.10, 0.63, 0.40, ... ] concurrent pattern 1.00, 0.44, 0.00, 0.30, 0.49, 0.53 0.44, 1.00, 0.36, 0.87, 0.24, 0.59 0.00, 0.36, 1.00, 0.70, 0.00, 0.00 0.30, 0.87, 0.70, 1.00, 0.33, 0.54 0.49, 0.24, 0.00, 0.33, 1.00, 0.58 0.53, 0.59, 0.00, 0.54, 0.58, 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The flowchart of CrowdReranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The computation of visual patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of reranking methods in terms of NDCG. Results in Web set are the average of three search engines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of different methods for two queries. (a) TRECVID 2007 query: "Find shots of hands at a keyboard typing or using a mouse." (b) Web query: "George W. Bush." [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of each query in Web set and TV07 set measured by N DCG@10. Results in Web set are the average of three search engines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance with different λ measured by NDCG@10. Note that "λ = 0" corresponds to the initial search without reranking.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use Engine I, II, III, and IV to represent four popular image and video search engines to preserve anonymity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The queries include: (1) animal, (2) beach, (3) Beijing Olympic 2008, (4) building, (5) car, (6) cat,<ref type="bibr" target="#b7">(7)</ref> clouds, (8) earth, (9) flower,<ref type="bibr" target="#b10">(10)</ref> fox,<ref type="bibr" target="#b11">(11)</ref> funny dog, (12) George W. Bush, (13) grape,<ref type="bibr" target="#b14">(14)</ref> hearts, (15) hello kitty, (16) hiking, (17) Mercedes logo,<ref type="bibr" target="#b18">(18)</ref> panda,<ref type="bibr" target="#b19">(19)</ref> sky, (20) statue of liberty, (21) sun,<ref type="bibr" target="#b22">(22)</ref> trees,<ref type="bibr" target="#b23">(23)</ref> wedding,<ref type="bibr" target="#b24">(24)</ref> white cat, (25) white house, (26) white house night, (27) winter, (28) yellow rose, and (29) zebra.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was performed when Yuan Liu was visiting Microsoft Research Asia as a research intern.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">APPENDIX 6.1 Pointwise mining-based reranking</head><p>Rewrite equation <ref type="bibr" target="#b8">(8)</ref> in the matrix way:</p><p>Then, taking derivatives and equate it to zero, we can obtain</p><p>Then, the solution is The solution of equation ( <ref type="formula">17</ref>) is non-unique since the Laplacian matrix ∆ is singular <ref type="bibr" target="#b2">[2]</ref>. Referring to <ref type="bibr" target="#b20">[20]</ref>, we also simply add a constraint rN = 0 where N is the length of r.</p><p>Thus we replace the last row of ∆ with [0, 0, . . . , 0, 1]1×N to obtain ∆, the last element of c and s with zero to obtain c and s, respectively. Then, the solution is r = 1 2 ∆-1 (2c + λs). <ref type="bibr" target="#b18">(18)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability analysis of interconnected nonlinear systems under matrix feedback</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Cremeant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Murra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Decision and Control</title>
		<meeting>IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning object categories from Google&apos;s image search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Flickr</surname></persName>
		</author>
		<ptr target="http://www.flickr.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="http://video.google.com/" />
		<title level="m">Google image and video search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video search reranking through random walk over document-level context graph</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia<address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video search reranking via information bottleneck principle</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia<address><addrLine>Santa Barbara, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">IR evaluation methods for retrieving highly relevant documents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekalainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PageRank for product image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International World Wide Web Conference</title>
		<meeting>International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A reranking approach for context-based concept fusion in video indexing and retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Image and Video Retrieval</title>
		<meeting>ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-based multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to video search rerank via pseudo preference feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia &amp; Expo</title>
		<meeting>IEEE International Conference on Multimedia &amp; Expo</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing video search reranking via minimum incremental information loss</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting>ACM International Workshop on Multimedia Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object recognition with informative features and linear classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MSRA-USTC-SJTU at TRECVID 2007: High-level feature extraction and search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC Video Retrieval Evaluation Online Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://www.live.com/?scope=videos/" />
		<title level="m">Microsoft Live image and video search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic concept-based query expansion and re-ranking for multimedia retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tesić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simple, proven approaches to text retrieval</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Jones</surname></persName>
		</author>
		<idno>TR356</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian video search reranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://www-nlpir.nist.gov/projects/trecvid/" />
		<title level="m">TRECVID</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A critical view of context</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="261" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://video.yahoo.com/" />
		<title level="m">Yahoo image and video search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimedia search with pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Image and Video Retrieval</title>
		<meeting>ACM International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
