<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Attention Prototypical Networks for Few-Shot Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengli</forename><surname>Sun</surname></persName>
							<email>slsun@ss.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
							<email>sunqingfeng@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
							<email>kezhou@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
							<email>lvtengchao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Attention Prototypical Networks for Few-Shot Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the current effective methods for text classification task are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available. In this paper, we propose a hierarchical attention prototypical networks (HAPN) for few-shot text classification. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space. We verify the effectiveness of our model on two standard benchmark fewshot text classification datasets -FewRel and CSID, and achieve the state-of-the-art performance. The visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances separately. In addition, our attention mechanism increases support set augmentability and accelerates convergence speed in the training stage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The dominant text classification models in deep learning <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015a;</ref><ref type="bibr" target="#b20">Yang et al., 2016;</ref><ref type="bibr" target="#b18">Wang et al., 2018)</ref> require a considerable amount of labeled data to learn a large number of parameters. However, such methods may have difficulty in learning the semantic space in the case that only few data are available. Few-shot learning has became an effective approach to solve this challenge, it can train a neural network with a few parameters using few data but achieve good performance. A typical example of this approach is prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017)</ref>, which averages the vector of few support instances as the class prototype and computes distance between target query and each prototype, then classify the query to the nearest prototype's class. However, prototypical networks is rough and does not consider the adverse effects of various noises in the data, which weakens the discrimination and expressiveness of the prototype.</p><p>In this paper, we propose a hierarchical attention prototypical networks for few-shot text classification by using attention mechanism in three levels. For feature level attention, we use convolutional neural networks to get the feature scores which is different for various classes. For word level attention, we adopt an attention mechanism to learn the importance of each word hidden state in an instance. For instance level multi cross attention, with the help of multi cross attention between support set and target query, we can determine the importance of different instances in the same class and enable the model to get a more discriminative prototype of each class.</p><p>In the actual scenario, we apply HAPN on intention detection of our open domain chatbots with different character. If we create a chatbot for old people, the user intentions will focus on children, health or expectation, so we can define specific intentions and supply related responses. And because of only few data are needed, we can expand the number of classes quickly. The model helps chatbot to identify user intentions precisely, makes the dialogue process smoother, more knowledgeable and more controllable.</p><p>There are three main parts of our contribution: first of all, we propose a hierarchical attention prototypical networks for few-shot text classification, then we achieve state-of-the-art performance on FewRel and CSID datasets, and the experiments prove our model is faster and more extensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Classification</head><p>Text Classification is an important task in Natural Language Processing, and many models are proposed to solve it. The traditional methods mainly focus on feature engineerings such as bagof-words or n-grams <ref type="bibr" target="#b19">(Wang and Manning, 2012)</ref> or SVMs <ref type="bibr" target="#b16">(Tang et al., 2015)</ref>. The neural network based methods like <ref type="bibr" target="#b7">Kim (2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-Shot Learning</head><p>Few-Shot Learning (FSL) aims to solve classification problems by training a classifier with few instances in each class, and it can apply to unseen classes. The early works aim to use transfer learning approaches, <ref type="bibr" target="#b1">Caruana (1994)</ref> and <ref type="bibr" target="#b0">Bengio (2011)</ref> adopt the target task from the pre-trained models. Then <ref type="bibr" target="#b8">Koch et al. (2015)</ref> explore a method for learning siamese neural networks which employs an unique structure to rank similarity between inputs. <ref type="bibr" target="#b17">Vinyals et al. (2016)</ref> use matching networks to map a small labeled support set and an unlabelled example to its label, and obviate the need for fine-tuning to adapt to new class types. Prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017</ref>) learns a metric space in which the model can perform well by computing distance between query and prototype representations of each class and classify the query to the nearest prototype's class. <ref type="bibr" target="#b15">Sung et al. (2018)</ref> propose a two-branch relation networks, which learns to compare query against few-shot labeled sample support data. Dual TriNet structure <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> can efficiently and directly augment multi-layer visual features to boost the few-shot classification.But all of the above works mainly concentrate on computer vision field, the research and applications in NLP field are extremely limited. Recently, <ref type="bibr" target="#b21">Yu et al. (2018)</ref> propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task such as intention classification, <ref type="bibr" target="#b5">Han et al. (2018)</ref> present a relation classification dataset -FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, <ref type="bibr" target="#b3">Gao et al. (2019)</ref> propose a hybrid attention-based prototypical networks for noisy few-shot relation classification. However, these methods do not consider mining semantic information or reducing the impact of noise more precisely. And in most of the realistic settings, we may increase the number of instances gradually, so model capacity needs more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>In few-shot text classification task, our goal is to learn a function : G(D, S, x) → y. D is the labeled data, we divide D into three parts: D train , D validation , and D test , and each part has specific label space. We use D train to optimize parameters, D validation to select best hyper parameters, and D test to evaluate the model.</p><p>The "episode" training strategy that <ref type="bibr" target="#b17">Vinyals et al. (2016)</ref> proposed has proved to be effective. For each training episode, we first sample a label set L from D train , then use L to sample the support set S and the query set Q, finally, we feed S and Q to the model and minimize the loss. If L includes N different classes and each class of S contains K instances, we call the target problem N -way K-shot learning. For this paper, we consider N = 5 or 10, and K = 5 or 10.</p><p>For exactly, in an episode, we are given a support set S S ={(x 1 1 , l 1 ), (x 2 1 , l 1 ), . . . , (x n 1 1 , l 1 ), ⋯,</p><formula xml:id="formula_0">(x 1 m , l m ), (x 2 m , l m ), . . . , (x nm m , l m )}, l 1 , l 2 , ⋯, l m ∈ L (1)</formula><p>consists of n i text instances for each class l i ∈ L, x j i means it is the j support instance belonging to calss l i , and instance x j i includes T i,j words {w 1 , w 2 , . . . , w T i,j }.</p><p>Then x is an unlabeled instance of query set Q to classify, and y ∈ L is the output label followed by the prediction of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Overview</head><p>The overall architecture of the Hierarchical Attention Prototypical Networks is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We introduce different components in the following subsections: Instance Encoder Each instance in support set or query set will be first represented to a input vector by transforming each word into embeddings. Considering the lightweight and speed of the model, we achieve this part with one layer convolutional neural networks (CNN). For ease of comparison, its details are the same as <ref type="bibr" target="#b5">Han et al. (2018)</ref> proposed. Hierarchical Attention In order to get more important information from rare data, we adopt a hi-erarchical attention mechanism. Feature level attention enhances or reduces the importance of different feature in each class, word level attention highlight the important words for meaning of the instance, and instance level multi cross attention can extract the important support instances for different query instances, these three attention mechanisms work together to improve the classification performance of our model. Prototypical Networks Prototypical networks compute a prototype vector as the representation of each class, and this vector is the mean vector of the embedded support instances belonging to its class. We compare the distance between all prototype vectors and a target query vector, then classify this query to the nearest one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Encoder</head><p>The instance encoder part consists of two layers: embedding layer and instance encoding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Embedding Layer</head><p>Given an instance x = {w t , w 2 , . . . , w T } with T words. We use an embedding matrix W E ,w t = W E w t to embed each word to a vector</p><formula xml:id="formula_1">{w 1 , w 2 , . . . , w T }, w t ∈ R d (2)</formula><p>where d is the word embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Encoding Layer</head><p>Following we apply a convolutional neural network <ref type="bibr" target="#b22">Zeng et al. (2014)</ref> as encoding layer to get the hidden annotations of each word by a convolution kernel with the window size m</p><formula xml:id="formula_2">h t = CNN(w t− m−1 2 , . . . , w t− m+1 2 )<label>(3)</label></formula><p>Especially, if the word w t has a position embedding p t , we should concat w t and p t</p><formula xml:id="formula_3">wp t = [w t ⊕ p t ] (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ⊕ is a concatation, the h t will be as follow</p><formula xml:id="formula_5">h t = CNN(wp t− m−1 2 , . . . , wp t− m+1 2 ) (5)</formula><p>Then, we aggregate all h t to get the overall representation of instance x</p><formula xml:id="formula_6">x = {h 1 , h 2 , . . . , h t } (6)</formula><p>Finally, we define those two layers as a comprehensive function</p><formula xml:id="formula_7">x = g θ (x)<label>(7)</label></formula><p>θ in this function are the networks parameters to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prototypical Networks</head><p>The prototypical networks <ref type="bibr" target="#b13">(Snell et al., 2017)</ref> has achieved excellent performance in few-shot image classification and few-shot text classification <ref type="bibr" target="#b5">(Han et al., 2018;</ref><ref type="bibr" target="#b3">Gao et al., 2019)</ref> tasks respectively, so our model is based on prototypical networks and aims to get promotion. The fundamental idea of prototypical networks is simple but efficient: we can use a prototype vector c i as the representative feature of class l i , each prototype vector can be calculated by averaging all the embedded instances in its support set</p><formula xml:id="formula_8">c i = 1 n i n i j=1 g θ (x j i )<label>(8)</label></formula><p>Then the probability distribution over the classes in L can be produced by a softmax function over distances between all prototypes vector and the target query q</p><formula xml:id="formula_9">p θ (y = l i q) = exp(−d(g θ (q), c i ) Σ L l=1 exp(−d(g θ (q), c l )<label>(9)</label></formula><p>As <ref type="bibr" target="#b13">Snell et al. (2017)</ref> mentioned, squared Euclidean distance is a reasonable choice, however, we will introduce a more effective method in section 4.4.1, which combines squared Euclidean distance with class feature scores, and achieves definite improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hierarchical Attention</head><p>We focus on sentence-level text classification in this work. The proposed model gets a feature scores vector and transfers the support set of each class into a vector representation, on which we build a classifier to perform few-shot text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Feature Level Attention</head><p>Obviously, the same dimension belonging to different classes has different importance when we calculate the euclidean distance. In other words, some feature dimensions are more discriminative for distinguishing specific class in the feature level space, and other features are confusing and useless at the same time.</p><p>So we apply a CNN-based feature attention mechanism similar to <ref type="bibr" target="#b3">Gao et al. (2019)</ref> proposed as a class feature extractor. It depends on all the instances in the support set of each class and will dynamiclly change with different classes. Given a support set S i ∈ R n i ×T ×d of class l i as the output of above instance encoder part</p><formula xml:id="formula_10">S i = {x 1 , x 2 , . . . , x n i } (10)</formula><p>we apply a max pooling layer over each instance in S i to get a new feature map S ci ∈ R n i ×d . Then we use three convolution layers to obtain λ i ∈ R d , which is the scores vector of class l i . The specific structure of above class feature extractor is shown in Table <ref type="table">1</ref>.</p><formula xml:id="formula_11">layer name kernel size stride output size pool T × 1 1 × 1 K × d × 1 conv 1 K × 1 1 × 1 K × d × 32 ReLU conv 2 K × 1 1 × 1 K × d × 64 ReLU conv 3 K × 1 K × 1 1 × d × 1 ReLU Table 1: Class feature extractor architecture</formula><p>So we get a new distance calculation method as follow</p><formula xml:id="formula_12">d(c i , q ′ ) = (c i − q ′ ) 2 ⋅ λ i<label>(11)</label></formula><p>where q ′ is the query vector passed through the word level attention mechanism which will be introduced in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Word Level Attention</head><p>The importance of different words to the meanings of an instance is unequal, thus it is worth pointing out which words are useful and which words are useless. Therefore, we apply an attention mechanism <ref type="bibr" target="#b20">(Yang et al., 2016)</ref> to get those important words and assemble them to compose a more informative instance vector s j , and the definitions are as follows</p><formula xml:id="formula_13">u j t = tanh(W w h j t + b w ) (12) v j t = u j t ⊺ u w (<label>13</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">α j t = exp(v j t ) Σ t exp(v j t )<label>(14)</label></formula><formula xml:id="formula_16">s j = t α j t h j t (<label>15</label></formula><formula xml:id="formula_17">)</formula><p>where h j t is the t hidden word embedding of instance x j , it was encoded through the instance encoder, and has the same hidden size with x j .</p><p>Firstly, the W w and b w followed by activation function tanh make up a MLP layer to transform h j t to the new hidden representation u j t . Immediately, we apply a dot product operation between u j t and a word level weight vector u w to compute similarity v j t as the importance weight of u j t . Then we use a softmax function to normalize v j t to α j t . Finally, we calculate the instance level vector s j through the weighted sum of α j t and h j t . As memory networks <ref type="bibr" target="#b14">(Sukhbaatar et al., 2015)</ref> proposed, u w can help us to select the important words in each instance, it will be randomly initialized at the beginning of the training stage, and be optimized together with the networks parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Instance Level Multi Cross Attention</head><p>The previous prototypical networks use the mean vector of support instances as the class prototype. Because of the diversity and lack of the support instances, the gap between each support vector and prototype maybe wide, meanwhile, different query instances can be expressed in several ways, so not every instance in a support set contributes equally to the class prototype when they face a target query instance. To highlight the importance of support instances which are useful clues to classify a query instance correctly, we propose a multi cross attention mechanism.</p><p>Given a support set S ′ i ∈ R n i ×d for class l i and a query vector q ′ ∈ R d , they are all encoded through the instance encoder and word level attention. We consider each support vector s j i in S ′ i has its own weight β j i to query q ′ . So the formula (8) will be rewritten as follow</p><formula xml:id="formula_18">c i = n i j=1 β j i s j i (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>where we define r j i = β j i s j i as the weighted prototype vector and the definitions of β j i are as follows</p><formula xml:id="formula_20">β j i = exp(γ j i ) Σ n i j=1 exp(γ j i )<label>(17)</label></formula><formula xml:id="formula_21">γ j i = sum{σ(f ϕ (mca))} (18) mca = [s j iφ ⊕ q ′ φ ⊕ τ 1 ⊕ τ 2 ] (19) τ 1 = s j iφ − q ′ φ , τ 2 = s j iφ ⊙ q ′ φ<label>(20)</label></formula><formula xml:id="formula_22">s j iφ = f φ (s j i ), q ′ φ = f φ (q ′ ) (21)</formula><p>where f φ is a linear layer, ⋅ is element-wise absolute value and ⊙ is element-wise product, we use these two operation to get the difference information τ 1 and τ 2 between s j i and q ′ , then concatenate them all as the multi cross attention information mca, then f ϕ (⋅) is a linear layer, σ(⋅) is a tanh activation function, sum{⋅} means a sum operation of all elements in the vector. Finally, γ j i is the weight of j instance in support set s i , and we use a softmax function to nomalize it to β j i . Through the multi cross attention mechanism, the prototype can pay more attention to those query-related support instances and improve the capacity of support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we will introduce the experiment results of our model. Firstly, we evaluate our model on FewRel dataset and CSID dataset, and achieve state-of-the-art results, our model outperforms the best baselines models by 1.11% and 1.64% respectively on 10 way 5 shot setting. Then we will show how our model works by case study and visualization of attention layers. We further demonstrate that the hierarchical attention increases the augmentability of support set and the convergence speed of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>FewRel Few-Shot Relation Classification <ref type="bibr" target="#b5">(Han et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>Firstly, we compare our model with several traditional models such as Finetune and kNN, Then we compare our model with five state-of-the-art fewshot learning models based on neural networks, they are MetaN <ref type="bibr" target="#b11">(Munkhdalai and Yu, 2017)</ref>, GNN <ref type="bibr" target="#b4">(Garcia and Bruna, 2018)</ref>, SNAIL <ref type="bibr" target="#b10">(Mishra et al., 2018)</ref>, Proto <ref type="bibr" target="#b13">(Snell et al., 2017)</ref> and PHATT <ref type="bibr" target="#b3">(Gao et al., 2019)</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation details</head><p>We compare our models with seven baselines, and the implementation details are as follows.</p><p>For FewRel dataset, we cite the results reported by <ref type="bibr" target="#b13">Snell et al. (2017)</ref> which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by <ref type="bibr" target="#b3">Gao et al. (2019)</ref> which includes Proto and PHATT. For a fair comparison, in our model, we use the same word embeddings and hyperparameters of instance encoder as PHATT proposed. In detail, we use the Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> consisting of 6B tokens and 400K vocabulary as our initialized word representation, and each word has a 50 dimensions vector. In addition, the position embedding dimension of a word is 10, the max length of each instance is 40. Finally, we evaluate all models on 5 way 5 shot and 10 way 5 shot settings.</p><p>For CSID dataset, we implement all above seven baseline models and our models. we use the Baidu Encyclopedia <ref type="bibr" target="#b9">(Li et al., 2018)</ref> as our initialized word representation, it includes 745M tokens and 5422K vocabulary, and each word has a 300d dimensions vector, the max length of each instance is 20. Finally, we evaluate all models on 5 way 5 shot, 5 way 10 shot, 10 way 5 shot and 10 way 10 shot settings.</p><p>For the Finetune and kNN baselines, they learn the parameters on the support set with the CNN encoder. For the neural networks based baselines, we use the same hyper parameters as <ref type="bibr" target="#b5">Han et al. (2018)</ref> proposed.</p><p>For our hierarchical attention prototypical networks, the window size of the CNN instance encoder is 3, the dimension of the hidden layer is 230, the learning rate is 0.1, the learning rate decay step is 3000 and the decay rate is 0.1. In addition, we train our model 12000 episodes and each episode consists of 20 classes.</p><p>In order to study the effects of different components, we refer to our models as HAPN-{FA,WA, IMCA}, FA indicates feature level attention, WA indicates word level attention and IMCA indicates instance level multi cross attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and analysis</head><p>The experimental accuracies on CSID and FewRel are shown in Tabel 2 and Table <ref type="table" target="#tab_3">4</ref> respectively. In this subsection, we will show the effects of hierarchical attention and support set augmentability of three Proto-based models and the convergence speed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Effects of hierarchical attention</head><p>Benefit from hierarchical attention, our model achieves excellent performance.</p><p>The case study of word level attention and instance level multi cross attention are shown in Table 3, this is a 2 way 3 shot task on FewRel dataset. The query instance is an instance of "mother" class in fact, and our model should classify it into "mother" class or "child" class. It is a difficult</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Word Attention IMCAS Support Set</p><p>(1) mother Cherie Gil is the daughter of Filipino actors Eddie Mesa and Rosemarie Gil, and sister of fellow actors, Michael de Mesa and the late Mark Gil.</p><p>When they reachedadulthood, Pelias and Neleus found their mother Tyro and then killed her stepmother, Sidero, for having mistreated her.</p><p>It was here that the Queen Consort Jetsun Pema gave birth to a son on 5 February 2016, Jigme Namgyel Wangchuck.</p><p>(   task because of there are many similarities between the expressions of two classes. With the help of word level attention, we highlight the importance of the word "daughter", which appears in the query instance and the first support instance of class "mother" at the same time, then this support instance get the highest attention score and contributes more to the prototype vector of "mother" class, finally our model can classify the query instance into the correct class in this confusing task. As shown in Figure <ref type="figure" target="#fig_3">2</ref>, by using the feature level attention, we also get the feature attention scores of "mother" class and "child" class respectively. The features with high scores have deep color, and the features with low scores have light color. Obviously, different classes may have different feature score vector, in other words, the same feature of different classes have different importance. So our feature level attention can highlight importance of the useful features and weaken the importance of the noise features, then the distance between the prototype vector and the query vector will measure the difference between them more efficiently.  We treat the final prototype embedding vector as the features of each instance, then we can get the distribution of features by principal pomponent analysis in feature space as shown in Figure <ref type="figure" target="#fig_4">3</ref>. As we can see, instances without hierarchical attention are more distributed and may cross with each other, but the instances with hierarchical attention are more centralized and discriminative, which proves that our model learns a better semantic space, which helps to distinguish confus-ing data.. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Augmentability of support set</head><p>More support instances can contribute more useful information to the prototype vector, meanwhile, more noise will be added in.</p><p>In this section, we define the support set augmentability (SSA) as the additive value of accuracy when we increase the same number of the support set for different models. So we compare our model's SSA with other models such as Proto and PHATT on the 10 way FewRel task, and the shot number ranges from 5 to 25.</p><p>By using the hierarchical attention, our model obtaines a strong robustness and can pay more attention to the important information of support set and reduce those negative effects of noisy data, thus as shown in Figure <ref type="figure" target="#fig_5">4</ref>, the support set augmentability of our model is larger than other models. Benefit from the above advantages, we can deploy our model in the cold start stage, and gradually accumulate labeled support data in practical applications, then improve the performance of the model day by day, and thus improve the utilization rate of few data in realistic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Convergence speed comparison</head><p>At the training stage, we also compare the convergence speed between Proto, PHATT, and HAPN on the 10 way 5 shot and 10 way 15 shot FewRel task. As shown in Figure <ref type="figure" target="#fig_7">5</ref>, our model can be optimized more quickly than the other models. From 10 way 5 shot task to 10 way 15 shot settings, the Proto model takes almost twice time to achieve 70% accuracy on validation set, in other words, the convergence speed will decrease sharply when we increase the number of support instances, but this problem can be effectively alleviated when we use hierarchical attention mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Previous few-shot learning models for text classification roughly apply text representations or neglect the noisy information. We propose to do hierarchical attention prototypical networks consisting of feature level, word level and instance level multi cross attention, which highlight the important information of few data and learn a more discriminative prototype representation. In the experiments, our model achieves the state-of-theart performance on FewRel and CSID datasets. HAPN not only increases support set augmentability but also accelerates convergence speed in the training stage.</p><p>In the future, we will contribute new text dataset to few-shot learning, explore better feature extrac-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hierarchical Attention Prototypical Networks architecture</figDesc><graphic url="image-1.png" coords="2,190.72,62.81,216.11,238.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>applies convolutional neural networks for sentence classification. Then, Johnson and Zhang (2015) use a one-hot word order CNN, and Zhang et al. (2015b) apply a character level CNN. C-LSTM (Zhou et al., 2015) combines CNN and RNN for sentence representation and text classification. Yang et al. (2016) explore the hierarchical structure of documents classification, they use a GRU-based attention to build representations of sentences and another GRU-based attention to aggregate them into a document representation. But above supervised learning methods require large-scale labeled data and can't classify unseen classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Feature attention scores of "mother" class (b) Feature attention scores of "child" class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Feature attention scores of different classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Instances distribution of embedding vector without hierarchical attention (a) and with hierarchical attention (b). The left blue points marked × are instances of "mother" class and the right orange points marked • are instances of "child" class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Support set augmentability of Proto, PHATT and HAPN on FewRel validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training Proto, PHATT and HAPN on FewRel dataset. Lines marked denote loss on the training set and lines marked △ denote accuracy on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Accuracies (%) of different models on the CSID dataset on four different settings.</figDesc><table><row><cell>is a new large-scale supervised</cell></row><row><cell>dataset 1 . It consists of 70000 instances on 100 re-</cell></row><row><cell>lations derived from Wikipedia, and each relation</cell></row><row><cell>includes 700 instances. It also marks the head and</cell></row><row><cell>tail entities in each instance, and the average num-</cell></row><row><cell>ber of tokens is 24.99. FewRel has 64 relations for</cell></row><row><cell>training, 16 relations for validation, and 20 rela-</cell></row><row><cell>tions for test separately.</cell></row><row><cell>CSID Character Studio Intention Detection is a</cell></row><row><cell>dataset extracted from a real-world open domain</cell></row><row><cell>chatbot. In character studio platform, this chatbot</cell></row><row><cell>should transform its character style sometime so it</cell></row><row><cell>can adapt to different user group and environment,</cell></row><row><cell>thus dialog query intention detection turns into an</cell></row><row><cell>important task. CSID consists of 24596 instances</cell></row><row><cell>for 128 intentions, and each intention includes 30</cell></row><row><cell>to 260 instances, the average number of tokens in</cell></row><row><cell>each instance is 11.52. We use 80, 18 and 30 inten-</cell></row><row><cell>tions for training, validation, and test respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) childIn 1421 Mehmed died and his son Murad II refused to honour his father's obligations to the Byzantines.Henry Norreys was a lifelong friend of Queen Elizabeth and was the father of six sons, who included Sir John Norreys, a famous English soldier.Jim Henson and his son Brian were impressed enough with Barron's style to offer him a job directing the pilot episode of "The Storyteller". Princess Dagmar of Demark, the daughter of Frederick VIII of Denmark and Louise of Sweden, lived on Kongestlund.</figDesc><table><row><cell></cell><cell>Query</cell></row><row><cell>(1) or (2)</cell><cell>From 1922 to 1963,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Visualization of word level and instance level multi cross attention scores (IMCAS) for 2 way 3 shot setting, the bold words are head entities and tail entities.</figDesc><table><row><cell>Model</cell><cell cols="2">5 Way 5 Shot 10 Way 5 Shot</cell></row><row><cell>Finetune  *  kNN  *  MetaN  *  GNN  *  SNAIL  *  Proto ◇ PHATT ◇</cell><cell>68.66 ± 0.41 68.77 ± 0.41 80.57 ± 0.48 81.28 ± 0.62 79.40 ± 0.22 89.05 ± 0.09 90.12 ± 0.04</cell><cell>55.04 ± 0.31 55.87 ± 0.31 69.23 ± 0.52 64.02 ± 0.77 68.33 ± 0.25 81.46 ± 0.13 83.05 ± 0.05</cell></row><row><cell>HAPN-FA</cell><cell>89.79 ± 0.13</cell><cell>82.47 ± 0.20</cell></row><row><cell>HAPN-WA</cell><cell>90.86 ± 0.12</cell><cell>83.79 ± 0.19</cell></row><row><cell cols="2">HAPN-IMCA 90.92 ± 0.11</cell><cell>84.07 ± 0.19</cell></row><row><cell>HAPN</cell><cell>91.02 ± 0.11</cell><cell>84.16 ± 0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracies (%) for 5 way 5 shot and 10 way 5 shot settings on FewRel test set.</figDesc><table /><note>* reported by<ref type="bibr" target="#b5">Han et al. (2018)</ref> and ◇ reported by<ref type="bibr" target="#b3">Gao et al. (2019)</ref>.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/thunlp/FewRel</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Sawyer Zeng and Yue Liu for providing valuable hardware support and useful advice, and thank Xuexiang Xu and Yang Bai for helping us test online FewRel dataset. This work is also supported by the National Key Research and Development Program of China (No. 2018YFB1402902 and No. 2018YFB1403002) and the Natural Science Foundation of Jiangsu Province (No. BK20151132).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tor networks and do some industrial application.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unsupervised and Transfer Learning -Workshop held at ICML 2011</title>
				<meeting><address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-02">2011. July 2, 2011</date>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning many related tasks at the same time with backpropagation</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic feature augmentation in few-shot learning</title>
		<author>
			<persName><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno>abs/1804.05298</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid attention-based prototypical networks for noisy few-shot relation classification</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu Xu Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
				<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter</title>
				<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015-05-31">2015. May 31 -June 5, 2015</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5822</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analogical reasoning on chinese morphological and semantic relations</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
				<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25">2014. 2014. October 25-29, 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017. 2017, 4-9 December 2017</date>
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<title level="m">Yara parser: A fast and accurate dependency parser. End-to-end memory networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
				<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18">2018. June 18-22. 2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. 2015. September 17-21, 2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint embedding of words and labels for text classification</title>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2321" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
				<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012-07-08">2012. July 8-14, 2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter</title>
				<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diverse few-shot text classification with multiple metrics</title>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="page" from="1206" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
				<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23">2014. August 23-29, 2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015a. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015b. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<idno>abs/1511.08630</idno>
		<title level="m">A C-LSTM neural network for text classification</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
