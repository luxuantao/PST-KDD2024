<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contents lists available at ScienceDirect Information Sciences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yangli-Ao</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Zheng</surname></persName>
							<email>rzheng@mcmaster.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing and Software</orgName>
								<orgName type="institution">McMaster University</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<email>zhuangfz@ics.ict.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Lab of Intelligen Information Processing</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution" key="instit2">ICT, CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>10 0 049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruisi</forename><surname>He</surname></persName>
							<email>ruisi.he@bjtu.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="laboratory">State Key Laboratory of Rail Traffic Control and Safety</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naixue</forename><surname>Xiong</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Northeastern State University</orgName>
								<address>
									<settlement>Tahlequah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contents lists available at ScienceDirect Information Sciences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.ins.2018.01.013</idno>
					<note type="submission">Received 7 April 2017 Revised 10 December 2017 Accepted 8 January 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Density-based clustering Density estimation K nearest neighbors Graph theory</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discovering clusters from a dataset with different shapes, densities, and scales is a known challenging problem in data clustering. In this paper, we propose the RElative COre MErge (RECOME) clustering algorithm. The core of RECOME is a novel density measure, i.e., Relative K nearest Neighbor Kernel Density (RNKD). RECOME identifies core objects with unit RNKD, and partitions non-core objects into atom clusters by successively following higherdensity neighbor relations toward core objects. Core objects and their corresponding atom clusters are then merged through α-reachable paths on a KNN graph. We discover that the number of clusters computed by RECOME is a step function of the α parameter with jump discontinuity on a small collection of values. A fast jump discontinuity discovery (FJDD) method is proposed based on graph theory. RECOME is evaluated on both synthetic datasets and real datasets. Experimental results indicate that RECOME is able to discover clusters with different shapes, densities, and scales. It outperforms six baseline methods on both synthetic datasets and real datasets. Moreover, FJDD is shown to be effective to extract the jump discontinuity set of parameter α for all tested datasets, which can ease the task of data exploration and parameter tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering, also known as unsupervised learning, is a process of discovery and exploration for investigating inherent and hidden structures within a large dataset <ref type="bibr" target="#b9">[10]</ref> . It has been extensively applied to a variety of tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref> . Many clustering algorithms have been proposed in different scientific disciplines <ref type="bibr" target="#b12">[13]</ref> , and these methods often differ in the selection of objective functions, probabilistic models or heuristics adopted. Nonetheless, two difficulties, how to choose appropriate clustering number and how to discover clusters of an arbitrary shape, are faced by most methods. Densitybased clustering approaches are characterized by aggregating mechanisms based on density <ref type="bibr" target="#b27">[28]</ref> . They can handle data with irregular shapes and determine clustering number automatically. Ester et al. <ref type="bibr" target="#b8">[9]</ref> and Sander et al. <ref type="bibr" target="#b34">[35]</ref> pioneered two densitybased methods, Density Based Spatial Clustering of Applications with Noise (DBSCAN) and Generalizing DBSCAN, to detect clusters in a spatial database according to density differences. Although both methods can detect clusters with different shapes, they face the challenge of choosing appropriate parameter values. Subsequently, many improved methods have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> . Recently, a novel density based clustering method, named Fast search-and-find of Density Peaks (FDP) <ref type="bibr" target="#b32">[33]</ref> , was proposed. This algorithm assumes that cluster centers are surrounded by neighbors with lower local density and that they are at a relatively large distance from any point with higher density. FDP can recognize clusters regardless of their shape and of the dimensionality of the space in which they are embedded, but it lacks an efficient quantitative criterion for judging cluster centers. Accordingly, approaches such as 3DC <ref type="bibr" target="#b22">[23]</ref> and STClu <ref type="bibr" target="#b41">[42]</ref> have been proposed to improve FDP.</p><p>Density-based clustering methods have the advantages of discovering clusters with arbitrary shapes and dealing with noisy data, but they face two challenges. First, traditional density measures are not adaptive to clusters with different densities. Second, performances of traditional methods (e.g., DBSCAN and FDP) are sensitive to parameters, and it is non-trivial to set these parameters properly for different datasets.</p><p>Aiming to address these challenges, we propose the RElative COre MErge (RECOME) clustering algorithm, which is based on two density measures: the K nearest Neighbor Kernel Density (NKD) and Relative K nearest Neighbor Kernel Density (RNKD). RECOME firstly identifies core objects corresponding to objects with RNKD equal 1. A core object and its descendants, which are defined by a directed relation (i.e., higher density nearest-neighbor ) based on the NKD, form an atom cluster . These atom clusters are then merged using a novel notion of α-connectivity on a KNN graph. RECOME has been evaluated using both synthetic datasets and real world datasets. Experimental results demonstrate that RECOME outperforms six baseline methods. Furthermore, we find that the clustering results of RECOME can be characterized by a step function of its parameter α, and therefore devise a fast jump discontinuity discovery (FJDD) algorithm to extract the small collection of jump discontinuity values. In summary, this work makes the following contributions.</p><p>1. We give a formal analysis showing that the density measure NKD enjoys some desirable properties. Furthermore, based on the NKD, we propose a new density measure RNKD, which is instrumental in detecting clusters with different densities. 2. RECOME can avoid the "decision graph fraud" problem <ref type="bibr" target="#b22">[23]</ref> of FDP and can handle clusters with different shapes, densities, and scales. Furthermore, RECOME has nearly linear computational complexity if the K nearest neighbors of each object are computed in advance.</p><p>3. FJDD can extract all jump discontinuity values of parameter α for any dataset in O(n log n ) time, where n is the number of objects. It will greatly benefit parameter selection in real-world applications.</p><p>This paper is organized as follows. Section 2 introduces the related work. Section 3 presents the new density measure RNKD and discusses the robustness of NKD and RNKD. Section 4 describes the proposed clustering method RECOME. Section 5 presents the auxiliary algorithm FJDD . Section 6 demonstrates experimental results. Finally, we conclude the paper in Section 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Existing clustering methods can be categorized into partitional methods, hierarchical methods, grid-based methods, graph-based methods, density-based methods, etc <ref type="bibr" target="#b9">[10]</ref> . Partitional methods such as K-means <ref type="bibr" target="#b26">[27]</ref> and K-medoids <ref type="bibr" target="#b15">[16]</ref> , divide data to a number of partitions and a certain quantitative measure of the "goodness" of the resulting clusters is maximized iteratively. Hierarchical clustering methods can be agglomerative (bottom-up) or divisive (top-down). An agglomerative clustering (e.g., AGNES <ref type="bibr" target="#b13">[14]</ref> ) starts with one object for each cluster and recursively merges two or more of the most appropriate clusters. A divisive clustering (e.g., DIANA <ref type="bibr" target="#b14">[15]</ref> ) starts with the dataset as one cluster and recursively splits the most appropriate cluster. The process continues until a stopping criterion is reached. Grid-based methods such as STING <ref type="bibr" target="#b42">[43]</ref> and CLIQUE <ref type="bibr" target="#b0">[1]</ref> , divide the original data space into grids, and then group the grids according to the statistical characters of objects in each grid. Graph-based methods, such as SCAN <ref type="bibr" target="#b43">[44]</ref> and spectral clustering <ref type="bibr" target="#b36">[37]</ref> , first construct a similarity graph from a dataset, and then utilize the notion of structural-context similarity or the eigenvalues of Laplacian matrix to generate clusters. Density-based methods (e.g., DBSCAN <ref type="bibr" target="#b8">[9]</ref> and DENCLUE <ref type="bibr" target="#b11">[12]</ref> ) first estimate the distribution density of objects in a feature space, and then recognize clusters as regions of high density separated by regions of lower density. In this paper, we focus on density-based methods because they are highly relevant to the proposed algorithm.</p><p>In <ref type="bibr" target="#b8">[9]</ref> , Ester et al. proposed the first density-based method DBSCAN. In DBSCAN, a cut-off density of an object o is defined as the number of objects falling inside a ball of radius centered at o . If the cut-off density of o is higher than a threshold, MinPts, o is regarded as a key object. When the distance between two key objects is less than , they are called densityreachable. Density-reachable key objects form basic clusters. A non-key object is assigned to a basic cluster if it is within distance to a key object in the respective cluster; otherwise, the non-key object is treated as noise. DBSCAN is sensitive to the choice of parameters and MinPts , and can hardly handle clusters with heterogeneous densities. To overcome these drawbacks, Ankerst et al. <ref type="bibr" target="#b2">[3]</ref> proposed an enhanced density-connected algorithm OPTICS. OPTICS provides a visual tool to help users find the cluster structure and determine the parameters. Although OPTICS reduces the subjectivity in a parameter estimation, when dealing with a complex dataset, it is also difficult to determine how many 's are needed to find potential clusters <ref type="bibr" target="#b6">[7]</ref> . The distance function on V . In the formal analysis, we assume it is a metric (i.e., satisfying non-negativity, symmetry and triangle inequality).</p><formula xml:id="formula_0">N ( u ) The K nearest neighbors set of u in V with respect to d ( • , • ). N i ( u ) The i -th nearest neighbor of u in V . d i ( u )</formula><p>The distance between u and N i ( u ), i.e., d ( u, N i ( u )).</p><p>Kernel density <ref type="bibr" target="#b38">[39]</ref> is a well-known alternative to cut-off density. It is continuous and less sensitive to parameter selection. DENCLUE <ref type="bibr" target="#b11">[12]</ref> is a method based on kernel density, in which the local peaks (i.e., local density maxima) of the kernel density function are used to define clusters. Then, each object is assigned to a cluster by a hill-climbing procedure. However, traditional kernel density methods tend to give biased estimation when handling clusters with different scales. To overcome this difficulty, KNN kernel density <ref type="bibr" target="#b24">[25]</ref> has been introduced in the KNN-kernel density-based Clustering (KNNC) <ref type="bibr" target="#b39">[40]</ref> . In our work, the proposed RNKD estimation is inspired by KNN kernel density with further improvement allowing the inclusion of low-density clusters.</p><p>Detecting clusters with heterogeneous densities is another challenge for density-based approaches <ref type="bibr" target="#b18">[19]</ref> . Some algorithms <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b30">[31]</ref> have been proposed to solve this problem. In <ref type="bibr" target="#b7">[8]</ref> , a Shared Nearest Neighbor (SNN) clustering algorithm was proposed. SNN first finds K nearest neighbors of each data object according to similarity, and then refines the similarity between pairs using the number of neighbors that the two objects share. Based on the new measure, a DBSCAN-like process is used to generate clusters. SNN has been shown the capacity in detecting clusters with complex distribution, whereas its excessive parameters and relatively high computational cost weaken its applicability in practice. DiscovEring c-clusters of different dEnsities (DECODE) <ref type="bibr" target="#b30">[31]</ref> assumes that the target dataset is generated by a series of point processes and tries to find clusters as connected regions of objects whose distances to their m -th nearest neighbor are similar. It has shown to be capable of determining the number of density types with little prior knowledge, but the high computational cost limits its application to large data. In <ref type="bibr" target="#b4">[5]</ref> , a density-based outlier detection approach was proposed. It relies on the local outlier factor (LOF) of each object, which is equal to the average of the ratios between the local density of an object and those of its K nearest neighbors. LOF can effectively distinguish outliers from normal clusters. However, it is not suitable for finding clusters with complex distribution. In this work, we introduce the novel density measure RNKD, which allows handling clusters with heterogeneous densities efficiently.</p><p>Rodriguez and Laio proposed a novel density-based clustering method by finding density peaks called FDP <ref type="bibr" target="#b32">[33]</ref> . FDP discovers clusters by a two-phase process. First, local density is computed for each object according to the number of objects in its d c neighborhood, and then a group decision method is applied to determine cluster centers, called density peaks. Second, remaining objects are assigned to the same cluster as its nearest neighbor with higher density. FDP is effective in finding clusters with different shapes. However, reasonable cluster centers are hard to determine when several density peaks exist in a cluster. In this work, RECOME adopts an agglomerative procedure to merge atom clusters (analogous to those resulting from "density peaks"), which is feasible even encountering clusters with multi-peaks.</p><p>Key notations used in the paper are listed in Table <ref type="table" target="#tab_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Relative KNN kernel density estimation</head><p>Though many density measures have been proposed, few considers the local relative density levels (will be shown in Section 3.1 ), which are crucial to detect clusters with various densities. In this section, we will first illustrate the disadvantage of classical density measures used in density clustering, and then introduce the proposed measure RNKD that homogenizes the density estimation across clusters with different densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Density estimation</head><p>The most commonly used density measure is cut-off density, which is defined as the number of objects in an -ball centered at the respective object. However, it is highly sensitive to the parameter . As shown in Fig. <ref type="figure" target="#fig_1">1</ref> (a), a small variation in can result in drastic differences in density estimation. Another classical measure is kernel density defined as</p><formula xml:id="formula_1">ρ(u ) = v ∈ V ker d (u, v ) h ,</formula><p>where ker (.) is usually a monotonically decreasing and continuous function, and h is a constant controlling the scale. Kernel density is continuous and less sensitive to parameter selection, but it tends to give biased estimation for objects in a smallsize cluster because it considers contributions of all objects in the dataset (see Fig. <ref type="figure" target="#fig_1">1 (b)</ref>).  Our proposed RNKD estimation is inspired by KNN kernel density <ref type="bibr" target="#b39">[40]</ref> , and only considers the objects in N ( u ) for the density estimation of object u . In addition, the density of an object should be positive and has a negative relation with the distances between itself and its neighbors. Thus, the K-nearest neighbor kernel density (NKD) of object u is defined as</p><formula xml:id="formula_2">ρ(u ) = θ v ∈ N(u ) exp − d (u, v ) σ , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where</p><formula xml:id="formula_4">σ = u ∈ V d K (u ) | V |</formula><p>is the mean of the distance between v and its K -th nearest neighbors, and θ is a normalizing factor.</p><p>NKD exhibits some good properties (as will be discussed in Section 3.2 ) and allows easy discrimination of outliers. However, it may mistake low-density clusters as outliers. One such example is shown in Fig. <ref type="figure" target="#fig_1">1 (c)</ref>, where low-density clusters on the right may be mistaken as noise. The key observation is that though the NKD of a low-density cluster is low compared to high-density clusters, its NKD is still comparatively high in its surroundings. This observation motivates us to introduce a new density measure, called RNKD as follows.</p><formula xml:id="formula_5">Definition 1. Given u ∈ V, RNKD of u , denoted by ρ * ( u ), is defined as ρ * (u ) = ρ(u ) max v ∈ N(u ) ∪{ u } { ρ(v ) } . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>From the definition, we can see RNKD is a normalized local relative density measure. Fig. <ref type="bibr">1 (d)</ref> shows that it can effectively accentuate both dense and sparse clusters, but diminish noises. Though the parameter K still needs to be tuned (the number of nearest neighbors), we will show that it is generally robust to K in Section 3.2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties of NKD and RNKD</head><p>As discussed in Section 3.1 , one shortcoming of cut-off density is its sensitivity to parameter . Additionally, even for a fixed , there can be significant discontinuity in the cut-off densities of two adjacent objects (see Fig. <ref type="figure" target="#fig_1">1 (e)</ref>), which will lead to undesirable results. Though non-parametric kernel density can avoid this gap, it may suffer from the problem illustrated in Fig. <ref type="figure" target="#fig_1">1 (b)</ref>. Next, we show that NKD exhibits local continuity. In particular, the ratio of ρ( u ) and ρ( v ) between neighbor points u and v are bounded on both sides.</p><formula xml:id="formula_7">Theorem 1. ∀ u, v ∈ V, u = v , we have | ρ(u ) −ρ(v ) | ρ(u )+ ρ(v ) &lt; 1 − exp − d(u, v ) σ . Proof. See appendix. Consequently, for any u, v ∈ V , the following results hold exp − d(u, v ) σ 2 − exp − d(u, v ) σ ≤ ρ(u ) ρ(v ) ≤ 2 − exp − d(u, v ) σ exp − d(u, v ) σ . (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>This implies that ρ( u ) and ρ( v ) are close to each other when d(u, v ) σ is small. In other words, NKD changes slowly in the interior of a cluster. Furthermore, (3) also shows that RNKD will be stable at a high level in the interior of a cluster.</p><p>Another important issue is how sensitive the density measures are to the choice of parameter K . Fig. <ref type="figure" target="#fig_2">2</ref> shows the heatmap and the distribution of NKD and RNKD on the synthetic dataset in Fig. <ref type="figure" target="#fig_1">1</ref> when</p><formula xml:id="formula_9">K valued from 0 . 4 | V | to | V | .</formula><p>From this figure, we can observe that, when K ≥ 0 . 5 | V | , both NKD and RNKD follow similar distributions despite varying of K . This indicates, for this representative dataset, NKD and RNKD are robust to the parameter K in the range [0 .</p><formula xml:id="formula_10">5 | V | , | V | ] .</formula><p>In Section 6.3 , we will show that the proposed RECOME algorithm also achieves good performance when K is in this range. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RECOME clustering algorithm</head><p>Now we are in the position to present RECOME based on NKD and RNKD. We first identify core objects corresponding to data points of peak relative density. These core objects serve as centers of sub-clusters, called atom clusters , which will be further merged through connected paths on a KNN graph. Thus, RECOME is, in essence, an agglomerative hierarchical clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Finding core objects and atom clusters</head><p>From the definition of RNKD, we know that there exist objects with unit RNKD. Such objects are good candidates for cluster centers as they have local maxima values in density. Formally,</p><formula xml:id="formula_11">Definition 2. An object u, u ∈ V , is called a core object if ρ * (u ) = 1 . Denote the set of core objects by O = { u | u ∈ V, ρ * (u ) = 1 } .</formula><p>For a non-core object u (namely, ρ * ( u ) &lt; 1), we define its Higher Density Nearest-neighbor (HDN), π ( u ) as</p><formula xml:id="formula_12">π (u ) = arg min ρ(v ) &gt;ρ(u ) , v ∈ V { d (u, v ) } . (4)</formula><p>By the definition of ρ * ( u ), we can see that π ( u ) exists in N ( u ). In other words, the distance between an object and its HDN is small enough for most objects. So, HDN can be seen as a discrete approximation of gradients, which are hard to compute directly.</p><p>HDN allows us to construct a directed graph G = (V, A ) , where each vertex is an object and a direct edge exists from a non-core object to its higher density nearest neighbor, namely,</p><formula xml:id="formula_13">A = { u, π (u ) | u ∈ V \ O } .</formula><p>In G, starting from any non-core object and following the directed edges, we will eventually reach a core object. In other words, G can be partitioned into trees with disjoint vertices, where each tree is rooted at a core object. A core object and its descendants thus form a cluster, called an atom cluster . Due to the bijective relation between an atom cluster and its core object, we use the two terms interchangeably. Formally, for a core object o ∈ O , the atom cluster rooted at o is given by,</p><formula xml:id="formula_14">{ o} ∪ { v ∈ V \ O | v is connected to o in G } . (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>Atom clusters form the basis of final clusters, however, they themselves tend to be too fine. A true cluster may consist of many atom clusters. This happens when many local maximal exist in one true cluster. Thus, a merging step is needed to selectively combine atom clusters into desirable clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Merging atom clusters</head><p>To merge the selected atom clusters to obtain a better clustering result, we treat each core object as the representative object of the atom cluster that it belongs to. The problem is thus transformed from merging atom clusters to merging core objects. To do so, we define the KNN graph as follows,</p><formula xml:id="formula_16">Definition 3. A KNN graph G K = (V, E)</formula><p>, is an undirected graph, where V consists of all objects in the given dataset and</p><formula xml:id="formula_17">E = { u, v | u ∈ N(v ) ∧ v ∈ N(u ) } .</formula><p>Therefore, two objects u and v are directly connected in graph G K if they are the top K nearest neighbors of each other.</p><p>Next, we define the notion of α-reachability between vertices in G K . Definition 4. Given α ∈ [0, 1] and u, v ∈ V , if there exists a path u, w 1 , . . . ,</p><formula xml:id="formula_18">w s , v in G K that satisfies ρ * (w i ) &gt; α, i = 1 , 2 , . . . s, then v is α-reachable from u , denoted by u α − → v .</formula><p>Clearly, α-reachability is reflexive, symmetric, and transitive for the core object set O in G K . Thus, it can be used as an equivalence relation among the core objects to divide them into equivalence classes. All atom clusters associated with core objects in the same equivalent class are merged into a single cluster. Alternatively, we can view α-reachability as a way to prune edges in G K . Core objects in the same equivalence class reside in the same connected component.</p><p>The choice of α is expected to affect the partition of equivalence classes. When α is small, say α = 0 , most core objects are merged together. On the other hand, when α is large, in the extreme case when α = 1 , every core object forms an equivalence class and atom clusters are the final clusters. The property and the selection of α will be analyzed in Section 6.3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Algorithm description and complexity analysis</head><p>To this end, we have presented the four main steps of the proposed RECOME clustering algorithm, i.e., computing NKD, calculating RNKD, discovering atom clusters, and merging atom clusters. Algorithm 1 summarizes the details of RECOME.</p><p>Let the number of objects be n = | V | . Computing the KNN set is the most time-consuming step with computational complexity of O(n 2 + Kn log n ) . Note that this step can be performed in parallel and accelerated using indexing structures such as kd-tree <ref type="bibr" target="#b5">[6]</ref> and R * -tree <ref type="bibr" target="#b3">[4]</ref> . Both ρ * ( u ) and π ( u ) can be obtained from its K nearest neighbors in O(K ) time. After obtaining the KNN set N ( u ) and ρ * ( u ) for each u , the KNN graph G K can be built by a linear scan of N ( u ). Merging core objects can be accomplished using depth-first-search on G K . The computation complexity of Algorithm 1 is thus O(n 2 + Kn log n ) . Specifically, when K is fixed, the KNN sets need to be computed only once and thus the time complexity will reduce to O(Kn ) for a different α. The space complexity of RECOME is O(Kn ) because it needs not to store the distance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fast jump discontinuity discovery for parameter α</head><p>As evident from the description of RECOME, parameter α is crucial to the clustering result for a fixed K . Take the example in Fig. <ref type="figure">3</ref> . Starting from the same set of core objects and atom clusters, different values of α may result in different numbers of clusters. In particular, as α increases, cluster granularity (i.e., the volume of clusters) decreases and cluster purity increases. Thus, a pertinent question is how to select a proper α. For ease of presentation, we suppose parameter K is fixed in this section. Core objects of the same color belong to the same final cluster.(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: RECOME clustering</head><formula xml:id="formula_19">Input : V, K, α Output : Clusters 1 for u ∈ V do 2 Compute the KNN set N(u ) of u ; 3 ρ(u ) = θ v ∈ N(u ) exp − d (u, v ) σ ; 4 end 5 for u ∈ V do 6 ρ * (u ) = ρ(u ) max v ∈ N(u ) ∪{ u } { ρ(v ) } ; 7 end 8 Let O = { u | u ∈ V, ρ * (u ) = 1 } ; 9 for u ∈ V \ O do 10 π (u ) = arg min ρ(v ) &gt;ρ(u ) , v ∈ V { d (u, v ) } ; 11 end 12 Construct the directed graph G = (V, A ) , where A = { u, π (u ) | u ∈ V \ O } ; 13 for o ∈ O do 14 Find the atom cluster C o w.r.t. o by C o = { o} ∪ { u ∈ V \ O | u is connected to o in G } ; 15 end 16 Construct the undirected KNN graph G K = (V, E) , where E = { u, v | u ∈ N(v ) ∧ v ∈ N(u ) } ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Problem formalization</head><p>We observe that though α varies continuously in the interval of [0, 1], the number of clusters computed by RECOME is a step function of α. For instance, in Fig. <ref type="figure">3</ref> , only five clustering outcomes are possible for the example dataset, corresponding to α in the ranges of [0, 0.6), [0.6, 0.7), [0.7, 0.8), [0.8, 0.9), and [0.9, 1). The numbers of resulting clusters are 1, 3, 6, 7, and 8, respectively. It is desirable to have a small collection of α values (or ranges) that affect the clustering result as the processes of parameter tuning by developers or parameter selection by domain experts can be simplified.</p><p>We formalize the above intuition by first introducing the notion of jump discontinuity set.</p><p>Definition 5. Given a data set V and an input parameter K , an ascending list As described in Section 4 , the clustering number is only related to the merging of core objects, so we only need to discuss the effect of α on the merging process. Recall that G K is the KNN graph of dataset V . Without loss of generality, we suppose G K is connected. Consider a path p = u ≡ w 0 , w 1 , w 2 , . . . , w s ≡ v . Its left-open path capacity is defined as c(p) = min i&gt; 0 ρ * (w i ) . Suppose there are l paths from u to v , denoted by p 1 , p 2 , . . . , p L . The left-open capacity between u and v is defined as c(u, v ) = max 1 ≤l≤L c(p l ) . An example is shown in Fig. <ref type="figure" target="#fig_6">4</ref> (a). In other words, v is α-reachable from u iff α &lt; c ( u, v ).</p><formula xml:id="formula_20">L = { α 1 , α 2 , . . . , α l } is called a jump disconti- nuity<label>(</label></formula><p>Furthermore, we have the following proposition.  p 2 , and p 3 ) between them. It can be seen that c(p 1 ) = 0 . 6 , c(p 2 ) = 0 . 5 , and c(p 3 ) = 0 . 8 , so we have c(u, v ) = max { 0 . 6 , 0 . 5 , 0 . 8 } = 0 . 8 . (b) Edge capacity .</p><formula xml:id="formula_21">Proposition 1. The JD set L equals { 0 } ∪ { c(u, v ) | u, v ∈ O, u = v } .</formula><p>Each edge is marked with its weight. There are two core objects (i.e., u and v ) and three paths (i.e., p 1 , p 2 , and p 3 ) between them. It can be seen that</p><formula xml:id="formula_22">c * (p 1 ) = 0 . 6 , c * (p 2 ) = 0 . 5 , and c * (p 3 ) = 0 . 8 , so we have c * (u, v ) = max { 0 . 6 , 0 . 5 , 0 . 8 } = 0 . 8 .</formula><p>Proof. See appendix.</p><p>Considering that 0 belongs to L trivially, in the following section, we will focus on finding out</p><formula xml:id="formula_23">L * { c(u, v ) | u, v ∈ O, u = v } .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Fast jump discontinuity discovery</head><p>Finding L * can be reduced to the problem All-Pairs Bottleneck Paths in vertex weighted graphs <ref type="bibr" target="#b35">[36]</ref> . Unfortunately, the state-of-the-art method for it has a time complexity of O(n 2 . 575 ) . To reduce the time complexity, we need to exploit the characteristics of our problem.</p><p>To transform our problem, we define the edge weight function w</p><formula xml:id="formula_24">( u, v ) = min { ρ * (u ) , ρ * (v ) } for u, v ∈ E ( G K ).</formula><p>For a path p = u ≡ w 0 , w 1 , w 2 , . . . , w s ≡ v , its path edge-capacity is defined as c * (p) = min i&gt; 0 w ( w i −1 , w i ) . Suppose that there are l paths from u to v , denoted by p 1 , p 2 , . . . , p L . The edge capacity between u and v is defined as c * (u, v ) = max 1 ≤l≤L c * (p l ) . An example is shown in Fig. <ref type="figure" target="#fig_6">4 (b)</ref>.</p><formula xml:id="formula_25">Proposition 2. ∀ u, v ∈ O, u = v , the left-open capacity between u, v equals the edge capacity c * ( u, v ) in the transformed graph, i.e., c(u, v ) = c * (u, v ) . Proposition 2 implies that L * { c(u, v ) | u, v ∈ O, u = v } = { c * (u, v ) | u, v ∈ O, u = v } . Therefore, it suffices to determine { c * (u, v ) | u, v ∈ O, u = v } .</formula><p>This problem can be reduced to the problem of All-Pairs Bottleneck Paths in edge weighted graphs (edge-APBP) <ref type="bibr" target="#b35">[36]</ref> . For edge-APBP, the following property holds: Theorem 2. Suppose G is a connected edge weighted graph and T is the maximum spanning tree of G. Then,</p><formula xml:id="formula_26">∀ u, v ∈ V (G ) = V (T ) , u = v , c * G (u, v ) = c * T (u, v ) .</formula><p>Proof. See <ref type="bibr" target="#b35">[36]</ref> .</p><p>c * G (u, v ) and c * T (u, v ) denote the edge capacity between u and v in G and T , respectively. Suppose the maximum spanning tree of G K is T K , and then according to Theorem 2 , we can extract L * from T K using Algorithm 2 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate RECOME over synthetic and real-world datasets, and compare it with six other representative algorithms. We introduce the experiment setup in Section 6.1 , and then present the experimental results and analysis in Section 6.2 . In Section 6.3 , we conduct the parameter analysis of RECOME. Our implementation uses Microsoft Visual C++ 2015 14.0.24720.00, and all experiments are conducted on a workstation (Windows 64 bit, 4 Intel 3.2 GHz processors, 4GB of RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment setup 6.1.1. Datasets</head><p>Two-dimensional synthetic datasets : Four representative datasets S1, S2, S3, and S4 are used. S1 comprises 50 0 0 objects and 15 Gaussian clusters. S2 is an unbalanced dataset, which contains 8 classes of different density and size, 6500 objects with 117 noisy objects injected. S3 is comprised of 6 classes of non-convex shape and 80 0 0 objects; S4 is the mixture of S2 and S3, which contains 14 classes of different shapes, densities, and scales. See Fig. <ref type="figure">5</ref> for details.</p><p>MNIST training set <ref type="bibr" target="#b21">[22]</ref> : It is a real-world dataset containing 60,0 0 0 examples of handwritten digits from 0 to 9, which has been widely used in data mining and machine learning. We select the subsets of different digits for clustering tasks, including M367 (all examples of digits "3, 6, 7"), M3467, M0:8 (all examples of digits from 0 to 8), M0:9, sM3467 (small edition of M3467 for the visualization, generated by randomly selecting 900 examples of each digit), and sM0:8.</p><p>Olivetti face database <ref type="bibr" target="#b33">[34]</ref> : It is a real-world dataset containing 40 subjects of human face and each contains 10 examples. This dataset poses a challenge to the density estimation since the real number of clusters is comparable with the number of objects in each cluster (10 different pictures for each people). Two subsets, face10 (the examples of first 10 subjects) and face40 (the examples of all 40 subjects), have been selected for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Baseline methods and settings</head><p>We select 6 representative density-based clustering algorithms as baseline methods, i.e., DBSCAN <ref type="bibr" target="#b8">[9]</ref> , SNN <ref type="bibr" target="#b7">[8]</ref> , KNNC <ref type="bibr" target="#b39">[40]</ref> , FDP <ref type="bibr" target="#b32">[33]</ref> , 3DC <ref type="bibr" target="#b22">[23]</ref> , STClu <ref type="bibr" target="#b41">[42]</ref> . Since many methods share a common density measure, in the implementation, we consider the two density measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The detailed settings of all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Density measure Other settings DBSCAN <ref type="bibr" target="#b8">[9]</ref> (i)</p><p>The parameter MinPts is determined by λ • ρ, where λ is searched in {0.05, 0.1, 0.2, 0.4, 0.8, 1.6} and ρ is the mean density. Each outlier is assigned to its nearest cluster. SNN <ref type="bibr" target="#b7">[8]</ref> (ii)</p><p>For remained parameters and MinPts , both of them are set to λK , where λ is chosen from {0.2, 0.4, 0.5, 0.6, 0.8}, according to the recommendation in <ref type="bibr" target="#b7">[8]</ref> . KNNC <ref type="bibr" target="#b39">[40]</ref> (ii) No other parameters. FDP <ref type="bibr" target="#b32">[33]</ref> (i)</p><p>The objects with top C t gamma values are chosen as cluster centers, where C t is the real class number. 3DC <ref type="bibr" target="#b22">[23]</ref> (i) No other parameters. STClu <ref type="bibr" target="#b41">[42]</ref> (ii) No other parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECOME (ii)</head><p>For parameter α, enumerate all JD values extracted by FJDD algorithm. Euclidean distance is specified as the distance function.</p><p>(i) cut-off density. The parameter is set to the value of the top β percent distance among all object pairs. We vary β value in {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} according to the recommendation in <ref type="bibr" target="#b32">[33]</ref> .</p><p>(ii) Density based on K nearest neighbors. The parameter K is searched from (0 , | V | ] with a step size of 10% of the range, where V is the number of objects. This range covers the recommended parameters of different clustering methods and is shown to provide a good trade-off between evaluating clustering performance and computation time.</p><p>The detailed settings of all methods are listed in Table <ref type="table">2</ref> . The best performance is reported for all methods by searching through the afore-mentioned parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">Performance metrics</head><p>For the two-dimensional synthetic datasets, due to the lack of ground truth labels, we compare clustering results visually. For real-world datasets, two metrics are calculated based on the ground truth of the datasets. One is the normalized mutual information (NMI) <ref type="bibr" target="#b37">[38]</ref> , which is one of the most widely used measures of clustering quality. Given a dataset V of size n , suppose there are C clusters and C t actual classes. Let n i , n ( j ) and n ( j) i denote the number of objects in cluster i , actual class j , and both cluster i and actual class j , respectively. Then, NMI can be computed by</p><formula xml:id="formula_27">NMI = C i =1 C t j=1 n ( j) i n log nn ( j) i n i n ( j) C i =1 n i n log n i n C t j=1 n ( j) n log n ( j) n .</formula><p>The other metric is the F value. Given a dataset V = { v 1 , v 2 , . . . , v n } with cluster labels { c 1 , c 2 , . . . , c n } and actual class labels { l 1 , l 2 , . . . , l n } , define</p><formula xml:id="formula_28">Correctness (v i , v j ) = 1 if l i = l j ⇔ c i = c j 0 otherwise .</formula><p>Then, define PB and RB as:</p><formula xml:id="formula_29">P B = 1 n n i =1 i = j,c i = c j Correctness (v i , v j ) |{ v j | i = j, c i = c j }| , RB = 1 n n i =1 i = j,l i = l j Correctness (v i , v j ) |{ v j | i = j, l i = l j }| .</formula><p>PB and RB refer to precision b-cubed and recall b-cubed <ref type="bibr" target="#b9">[10]</ref> , respectively. Then F value is computed by F = 2 ×PB ×RB PB ×RB . We use this measure because PB and RB have been shown superior than other indices <ref type="bibr" target="#b1">[2]</ref> .</p><p>Both NMI and F fall in [0, 1], and a higher value denotes better clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Results on synthetic dataset</head><p>Fig. <ref type="figure" target="#fig_12">6</ref> shows the clustering results on the two-dimensional datasets. From the last row of this figure, we can observe that, RECOME achieves desirable results for all the four datasets.</p><p>For dataset S1 (the first column) which contains only trivial Gaussian clusters, desirable results are achieved by all methods except for KNNC. For dataset S2 (the second column), which comprises unbalanced convex clusters and sparse noises, SNN, FDP and RECOME detect correct clusters. DBSCAN, KNNC and STClu overlook the clusters with little size. For dataset S3 (the third column), which is comprised of 6 clusters of nonconvex shape, a desirable result is output by RECOME. DBSCAN achieve a comparable and slightly poor performance. However, other methods rarely produce satisfactory results, mis-merge objects from different true clusters or subdivide a true cluster into different parts. For dataset S4 (the last column), which   </p><p>(e) 3DC contains more complex clusters and proposes a big challenge to all baseline methods, only RECOME correctly detects the 14 clusters in it. These results indicate that RECOME is robust to the structure of cluster and own the potential to simultaneously discover clusters with different shapes, density, and scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Results on MNIST datasets</head><p>The clustering results on the MNIST datasets are presented in Table <ref type="table" target="#tab_1">3</ref> and the visualization for sM3467 and sM0:8 are shown in Fig. <ref type="figure" target="#fig_13">7</ref> .</p><p>From Table <ref type="table" target="#tab_1">3</ref> , we can see that RECOME has the best performance for all datasets except M367, on which a comparable result is produced by STClu. SNN achieves slightly lower scores for almost all datasets than RECOME at much longer running time. Among all the algorithms, KNNC has the shortest running time for all datasets, but its performance is not steady-it achieves passable performance on M0:8 and M0:9 but poor performance on the remaining datasets. For FDP, we error on the conservative side by feeding it the true cluster number. Even so, RECOME achieves higher NMI and F values than FDP and the gap becomes obvious as the increasing of class number. Similar to FDP, the results of 3DC are not desirable for the datasets with many classes. The performance of STClu is unstable-it achieves comparable results on M367 to M3467 and moderate results on sM0:8 and M0:8, but fails to work well on M0:9. This may be due to it based on statistical testing, which is largely depended on data quality. Despite our best effort s in parameter tuning, DBSCAN fail to perform well over all datasets but M367.</p><p>As shown in Fig. <ref type="figure" target="#fig_13">7</ref> , for dataset sM3467, SNN and RECOME output almost correct results. FDP and STClu detect four main classes, but cluster excessive objects wrong classes. DBSCAN and 3DC fail to find the true cluster number for this dataset. For sM0:8, we can see that SNN and RECOME achieve the best performance but they both threat the two clusters of the digits "3" and "8" (the first column of the second row and the last column of the third row) as one. Besides, for the cluster of the digit "5", SNN divides it into several sub-clusters. In contrast, RECOME detects this cluster but merge it into other clusters. Unfortunately, all other methods fail to output satisfactory results for this dataset, detecting few meaningful clusters or attributing many objects into wrong clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3.">Results on Olivetti Face Database</head><p>The clustering results on face10 and face40 are presented in Table <ref type="table" target="#tab_2">4</ref> . The visualized results for face10 are shown in the rightmost column of Fig. <ref type="figure" target="#fig_13">7</ref> (The visualization for face40 can be found in the supplementary material).</p><p>For face10, we can see that RECOME gets the highest score on both NMI and F metrics from Table <ref type="table" target="#tab_2">4</ref> . The other methods with the exception of STClu achieve similar scores, which are slightly lower than RECOME. Though the correct cluster number is found by RECOME, as shown in Fig. <ref type="figure" target="#fig_13">7</ref> , it mistakenly merges two clusters (the second row) and subdivides a true cluster (the left subject of the third row) into two classes.</p><p>For dataset face40, as shown in Table <ref type="table" target="#tab_2">4</ref> , KNNC achieves the best result in NMI and F measures, but this comes at the cost of an abnormally large cluster number. RECOME and SNN get comparable scores lower than KNNC by one percent and two percent in the NMI value, respectively. Besides, DBSCAN and FDP achieve reasonable performance and have a gap of 0.06 in the F metrice compared with RECOME. Despite our best effort s in parameter tuning, 3DC and STClu fail to perform well for this dataset. This may be explained by statistical errors since there are only 10 pictures for each people as discussed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Parameter analysis</head><p>As discussed previously, parameters K and α play fundamental roles in RECOME. Specifically, K determines the density estimation and the structure of the KNN graph. The number of the final clusters are largely determined by α. In this section, we present quantitative results on the impact of parameters K and α on the performance of RECOME.</p><p>For a given dataset with | V | objects, the possible value of the number of K can vary from 1 to | V | − 1 . However, we find that when K is large enough (up to | V | ), increasing K has little or negative impacts on the performance of RECOME. Thus, in this set of experiments, K is taken values from (0 , | V | ] with a step size of | V | / 40 .  For parameter α, as discussed in Section 5 , only a few values will lead to different results. To validate this claim, we search α in the range [0, 1] with a step size of 0.01 to make a better showcase and a evident staircase can be observed from the presentation. To evaluate the clustering results, both NMI and F metrics are used. Next, we present impacts of parameters K and α on NMI and F values for all datasets.</p><p>For synthetic datasets, due to the lack of ground truth for S2, S3, and S4, we label them according to the results shown in Fig. <ref type="figure" target="#fig_12">6</ref> (g) as they agree with our intuitive understanding. As shown in Fig. <ref type="figure" target="#fig_14">8 (a)</ref>, for all the four datasets, both NMI and F approach one when K is above | V | / 4 (in the figure, k = 10 for K = √ V / 4) for appropriate α's, and then change very slowly despite the increase of K . Meanwhile, when K falls in the range [ | V | / 4 , | V | ] , α valued greater than 0.8 will lead to desirable results. In addition, it is noted that, compared with S3 and S4, the maximum NMI and F values can be attained easier for S1 and S2. This is due to the fact that clusters in both S1 and S2 are convex and well separated, but for the other two datasets, irregular shapes and scales make it difficult to detect the true clusters. Regardless of the complex shapes and scales, RECOME can find the correct cluster numbers and output the desirable results in all cases with properly selected parameters.</p><p>Fig. <ref type="figure" target="#fig_14">8 (e)-(j)</ref> show the influences of parameters K and α on performance for MNIST training set. We can see that, similar to the synthetic datasets, both NMI and F values plateau out as K take around | V | / 4 for all 6 datasets. At the same time, when K is in the interval</p><formula xml:id="formula_33">[ | V | / 2 , | V | ] ,</formula><p>the α value in the range [0.85, 0.95] gives the best performance. On the other hand, as the number of clusters increases, the clustering performance degrades. For example, NMI over 0.9 can be easily attained for M3467 but not for M0:9. This is because more clusters tend to have more complex sample distribution and more overlapping among different classes. Furthermore, compared with two-dimensional datasets, we can observe that the region in the α − K plot that reaches high NMI and F values shrinks. This is due to the fact that the MNIST training set with high dimension is far harder to be clustered well. However, RECOME still achieves better performance across almost all six datasets compared with baseline methods.</p><p>For the Olivetti Face Database, the trends NMI and F are different from that in the other datasets. Performance on face10 follows a step-wise pattern with regard to K due to the small sample size (i.e., | V | = 10 ). For face40, both the NMI and F values remain stable when K varies from | V | / 40 to | V | / 4 . Then, with the increasing of K , the two indices drop rapidly and become quite small. This is because the ground truth partition constitutes 40 clusters and only 10 pictures in each cluster. The statistical error of the estimated density on such a small set of pictures is large <ref type="bibr" target="#b32">[33]</ref> . Therefore, for datasets consisting of clusters with few objects, K should be carefully tuned.</p><p>In summary, the performance of RECOME is dependent on both parameters of K and α. For most datasets, K in the range of [ | V | / 2 , | V | ] leads to a stable condition. Under this condition, the α value that produces a good performance falls in the range of [0.8, 1]. This implies that RECOME is not very sensitive to small changes in parameters K and α. Furthermore, as discussed in Section 5 , the algorithm FJDD can quickly determine the JD set with tiny size. Thus, those who fall in [0.8, 1] can then be used as the candidate set for α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">FJDD validation</head><p>As discussed in Section 5 , the number of clusters produced by RECOME is a step function of α. In this experiment, we fix K at 0 . 5 | V | and search α in [0, 1] with a step size 0.01. The resulting cluster number as a function of α is plotted in Fig. <ref type="figure" target="#fig_16">9</ref> . It can be observed that all curves are step-wise.</p><p>We further verify the correctness of Algorithm 3 on all datasets used in this paper with the jump points shown in the magenta dashed lines. From Fig. <ref type="figure" target="#fig_16">9</ref> , it can be seen that FJDD extracts all jump discontinuity correctly for all datasets. Interesting, the number of jump discontinuity is less than 20 for all datasets, which means that to obtain all possible clustering results, only at most 20% of run times is needed compared with searching for α in [0, 1] with the step size 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented a new density-based clustering method RECOME. RECOME exploits a novel density measure RNKD to detect core objects with various densities and generate atom clusters. A merging strategy based on KNN graph has been introduced to refine the atom clusters. In addition, we discovered that the number of clusters obtained by RECOME is a monotonic step function of α, and therefore proposed an auxiliary algorithm FJDD to help end users to select parameter α.</p><p>Experiment evaluations showed that RECOME can discover clusters of different shapes, densities, and scales with parameters chosen in stable ranges. FJDD can significantly reduce the number of effective choices for the parameter α.</p><p>In future research, we will extend the current work in three aspects. Firstly, we will design a distributed RECOME in cluster-computing frameworks (e.g. Apache Spark) to handle large volume of data. Secondly, when facing massive data, the number of jump points can grow significantly. We will investigate how to quickly identify a relevant range of α. Lastly, it is attractive to design an automatic strategy for parameter selection to make RECOME parameter-free.</p><p>Base on Lemma 1 , we can prove Theorem 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>| ρ(u )</head><formula xml:id="formula_34">− ρ(v ) | = θ K k =1 exp − d k (u ) σ − K k =1 exp − d k (v ) σ ≤ θ K k =1</formula><formula xml:id="formula_35">− exp − d k (v ) σ ≤θ K k =1 exp − d k (h (k )) σ − exp − d k (h (k )) + d(u, v ) σ = 1 − exp − d(u, v ) σ θ K k =1 exp − d k (h (k )) σ &lt; 1 −exp − d(u, v ) σ θ K k =1 exp − d k (u ) σ + exp − d k (v ) σ = 1 − exp − d(u, v ) σ (ρ(u ) + ρ(v )) .</formula><p>Since ρ(u ) + ρ(v ) &gt; 0 , the result follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proof of Proposition 1</head><p>Proof. Denote A = { 0 } ∪ { c(u, v ) | u, v ∈ O, u = v } = { α 1 , α 2 , . . . , α t } , where 0 ≡ α 1 &lt; α 2 &lt; . . . &lt; α t . For any α with 0 ≤ α ≤ 1, let G &gt;α K denote the remaining graph after removing all non-core nodes with weights not larger than α from G K . In addition, let # (G &gt;α K ) denote the number of components containing at least one core object in G &gt;α K . The key of the proof is the following equation.</p><p>#(V, K, α i ) = #(G &gt;α i K ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B.1)</head><p>To prove the conclusion, only two things need to be shown. First, ∀ i = j, # (V, K, α i ) = #( V, K, α j ). Second, ∀ 0 ≤ α ≤ 1, ∃ α i ∈ A such that # (V, K, α) = #( V, K, α i ). Now we show the former. Obviously, #( V, K, α) is a non-decreasing function with respect to α, which implies that showing "∀ 1 ≤ i &lt; t , #( V, K, α i ) &lt; # (V, K, α i +1 ) " is enough. Besides, for any u, v ∈ O with u = v , "u and v are located in different components in G &gt;α i K " implies "u and v are located in different components in G &gt;α i +1 K ". On the other hand, from the definition of A , there exists a pair of core objects u and v such that u and v are connected in G ) . According to (B.1) , the conclusion follows. Now we show the latter. For any α with 0 ≤ α ≤ 1, suppose α i is the maximal element of A that is not greater than α. According to the definition of A , we have # (G &gt;α K ) = # (G &gt;α i K ) . Thus, according to (B.1) , the desirable result follows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of different density measures. (a) Sensitivity to in DBSCAN. (b) Non-parametric kernel density methods are sensitive to noise. (c) NKD estimates may mistake low-density clusters as noise. (d) RNKD allows discrimination of low-density clusters from noise. The temperature of data points in (c) and (d) indicates the NKD and RNKD, respectively.</figDesc><graphic url="image-4.png" coords="4,103.91,55.04,348.36,67.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Columns from left to right are heat-map of NKD, heat-map of RNKD, distribution of NKD, and distribution of RNKD, respectively. Rows from top to bottom show the situations when K takes the value from small to large.</figDesc><graphic url="image-5.png" coords="5,278.81,71.60,136.41,427.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>17 1 Fig. 3 .</head><label>1713</label><figDesc>Fig. 3. Example of merging core objects results for different values of α. KNN graph is shown and each object (in circle) is marked with its relative density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>JD) set if the number of resulting clusters from RECOME, #( V, K, α) is a step function of α with jump discontinuity at α 1 , α 2 , . . . , α l ∈ [0 , 1] from left to right. Obviously, #( V, K, α) is a non-decreasing function of α. By definition, each JD in L yields a unique clustering result. From all the JDs in L , we can produce all possible clusters using RECOME. Recall that O is the set of core objects and | O | is the maximum number of clusters attainable by RECOME. Trivially, | L | ≤ | O |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Left-open capacity . Each object (in circle) is marked with its relative density. There are two core objects (i.e., u and v ) and three paths (i.e., p 1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2 :3 for i from 1 to n − 1 do 4 Suppose u and v are the two ends of e i ; 5 Suppose 6 if 7 L 8 end 9 T K ← T K + e i ; 10 end</head><label>245678910</label><figDesc>Extracting-JD Input : T K Output : L * 1 Sort E(T K ) by edge weight decreasingly and suppose the result is { e 1 , e 2 , ..., e n −1 } ; 2 Set L * ← ∅ , T K ← T K − E(T K ) (remove all edges from T K ); C u and C v are the two components in T K that contain u and v , respectively; both C u and C v contain at least one core object then * ← L * ∪ { w (e i ) }; In the Algorithm 2 , the sorting step is most time consuming with computational complexity O(n log n ) . The Prim's maximum spanning tree algorithm has a time complexity of O(| E| + n log n ) , where | E| = O(Kn ) . Therefore, the computation complexity of Algorithm 3 is O(Kn + n log n ) . Finally, the Fast Jump Discontinuity Discovery algorithm is summarized in Algorithm 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 3 :12Fig. 5 .</head><label>35</label><figDesc>Fig. 5. Two-dimensional synthetic datasets.</figDesc><graphic url="image-6.png" coords="9,100.24,143.67,340.56,213.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Clustering results on two-dimensional synthetic datasets. Different colors are used to represent different output clusters.(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visualized clustering results on real datasets. Columns from left to right are visualizations for sM3467, sM0:8, and face10, respectively. Different clusters are marked by different colors.(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic url="image-7.png" coords="13,147.74,55.17,245.28,486.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a)-(d) The clustering performance on 2-D synthetic datasets as a function of K and α. (e)-(j) The clustering performance on MNIST training set as a function of K and α. (k)-(l) The clustering performance on Olivetti face database as a function of K and α.</figDesc><graphic url="image-9.png" coords="14,359.45,59.96,71.82,467.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The cluster number C as a step function of α when K = 0 . 5 | V | . The elements of JD set L produced by Algorithm 3 are marked by magenta dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>.</head><label></label><figDesc>Let h (k ) = arg min w ∈{ u, v } { d k (w ) } . According to Lemma 1 and the monotonicity of the exp function, we have θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>and v are located in the same component in G &gt;α i K " and "u and v are located in different components in G&gt;α i +1 K ". Therefore, we have # (G &gt;α i K ) &lt; # (G &gt;α i +1 K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Main notations used throughout the paper.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>| • |</cell><cell>Absolute value of a scalar or cardinality of a set.</cell></row></table><note>V Dataset. Lower case symbols u, v, w, u i , v i , w i denote elements of V . d ( • , • )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Performance comparison of the seven methods on the MNIST datasets. C is the cluster number identified by the algorithms (or utilized as prior knowledge for FDP) and C t gives the true cluster number.</figDesc><table><row><cell></cell><cell></cell><cell>DBSCAN</cell><cell>SNN</cell><cell>KNNC</cell><cell>FDP</cell><cell>3DC</cell><cell>STClu</cell><cell>RECOME</cell></row><row><cell>M367</cell><cell>C</cell><cell>3</cell><cell>3</cell><cell>6</cell><cell>(3)</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>( C t = 3 )</cell><cell>NMI</cell><cell>.89</cell><cell>.70</cell><cell>.77</cell><cell>.93</cell><cell>.93</cell><cell>.96</cell><cell>.98</cell></row><row><cell></cell><cell>F</cell><cell>.95</cell><cell>.79</cell><cell>.78</cell><cell>.97</cell><cell>.97</cell><cell>.98</cell><cell>.96</cell></row><row><cell></cell><cell>Time(s)</cell><cell>545</cell><cell>587</cell><cell>26</cell><cell>96</cell><cell>124</cell><cell>57</cell><cell>64</cell></row><row><cell>sM3467</cell><cell>C</cell><cell>1</cell><cell>5</cell><cell>13</cell><cell>(4)</cell><cell>3</cell><cell>5</cell><cell>4</cell></row><row><cell>( C t = 4 )</cell><cell>NMI</cell><cell>.00</cell><cell>.91</cell><cell>.72</cell><cell>.87</cell><cell>.76</cell><cell>.85</cell><cell>.94</cell></row><row><cell></cell><cell>F</cell><cell>.40</cell><cell>.95</cell><cell>.67</cell><cell>.92</cell><cell>.81</cell><cell>.88</cell><cell>.97</cell></row><row><cell></cell><cell>Time(s)</cell><cell>21</cell><cell>18</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>2</cell></row><row><cell>M3467</cell><cell>C</cell><cell>1</cell><cell>4</cell><cell>10</cell><cell>(4)</cell><cell>3</cell><cell>5</cell><cell>4</cell></row><row><cell>( C t = 4 )</cell><cell>NMI</cell><cell>.00</cell><cell>.90</cell><cell>.76</cell><cell>.91</cell><cell>.80</cell><cell>.90</cell><cell>.94</cell></row><row><cell></cell><cell>F</cell><cell>.40</cell><cell>.95</cell><cell>.74</cell><cell>.95</cell><cell>.83</cell><cell>.92</cell><cell>.97</cell></row><row><cell></cell><cell>Time(s)</cell><cell>956</cell><cell>1006</cell><cell>46</cell><cell>170</cell><cell>235</cell><cell>99</cell><cell>116</cell></row><row><cell>sM0:8</cell><cell>C</cell><cell>2</cell><cell>26</cell><cell>24</cell><cell>(9)</cell><cell>7</cell><cell>17</cell><cell>9</cell></row><row><cell>( C t = 9 )</cell><cell>NMI</cell><cell>.20</cell><cell>.76</cell><cell>.68</cell><cell>.53</cell><cell>.35</cell><cell>.68</cell><cell>.80</cell></row><row><cell></cell><cell>F</cell><cell>.33</cell><cell>.77</cell><cell>.54</cell><cell>.54</cell><cell>.42</cell><cell>.62</cell><cell>.80</cell></row><row><cell></cell><cell>Time(s)</cell><cell>106</cell><cell>95</cell><cell>5</cell><cell>16</cell><cell>25</cell><cell>10</cell><cell>14</cell></row><row><cell>M0:8</cell><cell>C</cell><cell>2</cell><cell>29</cell><cell>15</cell><cell>(9)</cell><cell>7</cell><cell>19</cell><cell>14</cell></row><row><cell>( C t = 9 )</cell><cell>NMI</cell><cell>.14</cell><cell>.78</cell><cell>.75</cell><cell>.47</cell><cell>.04</cell><cell>.73</cell><cell>.82</cell></row><row><cell></cell><cell>F</cell><cell>.30</cell><cell>.80</cell><cell>.69</cell><cell>.47</cell><cell>.22</cell><cell>.68</cell><cell>.84</cell></row><row><cell></cell><cell>Time(s)</cell><cell>5236</cell><cell>5409</cell><cell>190</cell><cell>1172</cell><cell>1886</cell><cell>1098</cell><cell>762</cell></row><row><cell>M0:9</cell><cell>C</cell><cell>2</cell><cell>40</cell><cell>20</cell><cell>(10)</cell><cell>8</cell><cell>2</cell><cell>12</cell></row><row><cell>( C t = 10 )</cell><cell>NMI</cell><cell>.09</cell><cell>.72</cell><cell>.74</cell><cell>.43</cell><cell>.29</cell><cell>.26</cell><cell>.80</cell></row><row><cell></cell><cell>F</cell><cell>.24</cell><cell>.71</cell><cell>.70</cell><cell>.41</cell><cell>.37</cell><cell>.19</cell><cell>.80</cell></row><row><cell></cell><cell>Time(s)</cell><cell>6528</cell><cell>6162</cell><cell>198</cell><cell>1721</cell><cell>2321</cell><cell>1123</cell><cell>918</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Performance comparison of the seven methods on the Olivetti face database, where C means the cluster number identified by algorithm (or utilized as prior knowledge, for FDP) and C t means the true cluster number.</figDesc><table><row><cell></cell><cell></cell><cell>DBSCAN</cell><cell>SNN</cell><cell>KNNC</cell><cell>FDP</cell><cell>3DC</cell><cell>STClu</cell><cell>RECOME</cell></row><row><cell>face10</cell><cell>C</cell><cell>11</cell><cell>10</cell><cell>19</cell><cell>(10)</cell><cell>9</cell><cell>2</cell><cell>10</cell></row><row><cell>( C t = 10 )</cell><cell>NMI</cell><cell>.87</cell><cell>.88</cell><cell>.87</cell><cell>.89</cell><cell>.83</cell><cell>.42</cell><cell>.94</cell></row><row><cell></cell><cell>F</cell><cell>.81</cell><cell>.81</cell><cell>.73</cell><cell>.82</cell><cell>.73</cell><cell>.32</cell><cell>.90</cell></row><row><cell>face40</cell><cell>C</cell><cell>42</cell><cell>50</cell><cell>66</cell><cell>(40)</cell><cell>12</cell><cell>2</cell><cell>37</cell></row><row><cell>( C t = 40 )</cell><cell>NMI</cell><cell>.80</cell><cell>.87</cell><cell>.89</cell><cell>.83</cell><cell>.54</cell><cell>.27</cell><cell>.88</cell></row><row><cell></cell><cell>F</cell><cell>.60</cell><cell>.64</cell><cell>.67</cell><cell>.60</cell><cell>.29</cell><cell>.09</cell><cell>.66</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partly supported by the National Natural Science Foundation of China (No. 61725101 , 61773361 , 61771037 ), Beijing Natural Science Foundation (No. J160 0 04 ), Shanghai Research Program (No. 17511102900) and National Science and Engineering Council, Canada.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Proof of Theorem 1</head><p>Lemma 1. ∀ u, v ∈ V, the difference between d i ( u ) and d</p><p>Proof. It is equivalent to show that the following holds,</p><p>) according to the nonnegativity of distance. On the other hand, we have</p><p>diction with the definition of d i ( u ). Thus inequality (A.2) holds.</p><p>Similar argument holds for the left-side of inequality (A.1) .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optics: ordering points to identify the clustering structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<editor>ACM SIGMOD</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The r * -tree: an efficient and robust access method for points and rectangles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Acm</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
	<note>20 0 0</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Building a balanced k-d tree in o</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<idno>CoRR abs/1410.5420</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhancing density-based clustering: Parameter reduction and outlier detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cassisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giugno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pigola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pulvirenti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ertöz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siam International Conference on Data Mining</title>
				<meeting><address><addrLine>San Francisco, Ca, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>ACM SIGKDD</publisher>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel-power-density-based algorithm for channel multipath components clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L A</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Molisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kristem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TWC.2017.2740206</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7138" to="7151" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data clustering: 50 years beyond k-means</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Agglomerative nesting (program agnes), in: Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="199" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<title level="m">Finding Groups in Data: An Introduction to Cluster Analysis</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="253" to="279" />
		</imprint>
	</monogr>
	<note>Divisive analysis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustering by means of medoids</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Data Analysis Based on the L1-norm &amp; Related Methods</title>
				<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="405" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A particle-and-density based evolutionary clustering method for dynamic networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving atm coverage area using density based clustering algorithm and voronoi diagrams</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Kisore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Koteswaraiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density-based clustering</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.30</idno>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip. Rev</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustering methods for agent distribution optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kubalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Indel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Staron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part C</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="86" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A clustering algorithm for stream data with lda-based unsupervised localized dimension reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laohakiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phimoltares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lursinsap</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="104C" to="123" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delta-density based clustering with a divide-and-conquer strategy: 3dc clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vdbscan: varied density based spatial clustering of applications with noise, in: 2007 International conference on service systems and service management</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A nonparametric estimate of a multivariate density function</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Loftsgaarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Quesenberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Math. Stat</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1049" to="1051" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast density-based clustering through dataset partition using graphics processing units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="94" to="112" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>1, Oakland, CA , USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spatial Clustering Methods in Data Mining: A Survey, Geographic data mining and knowledge discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Taylor and Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design of computationally efficient density-based clustering algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
				<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decode: a new method for discovering clusters of different densities in spatial data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jasra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2017.05.018</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="369" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">User clustering in a dynamic social network topic model for short text streams</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2017.05.018</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="102" to="116" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="92" to="106" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parameterisation of a stochastic model for human face identification</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Harter</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACV.1994.341300</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE Workshop on Applications of Computer Vision</title>
				<meeting>1994 IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Density-based clustering in spatial databases: The algorithm gdbscan and its applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="194" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">All-pairs bottleneck paths in vertex weighted graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
	<note>20 0 0</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cluster ensembles -a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variable kernel density estimation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Terrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Stat</title>
		<imprint>
			<biblScope unit="page" from="1236" to="1265" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knn-kernel density-based clustering for high-dimensional multivariate data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Buydens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="525" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based multiprototype competitive learning and its applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part C</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="934" to="946" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic clustering via outward statistical testing on density metrics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sting: A statistical information grid approach to spatial data mining</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Muntz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>VLDB</publisher>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scan: a structural clustering algorithm for networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yuruk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A J</forename><surname>Schweiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="824" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Color image segmentation using density-based clustering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2003.1221638</idno>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2003. ICME &apos;03. Proceedings. 2003 International Conference on</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A dirichlet multinomial mixture model-based approach for short text clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM SIGKDD</publisher>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spectral-spatial sparse subspace clustering for hyperspectral remote sensing images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3672" to="3684" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
